# Comparing `tmp/keras_cv-0.5.0-py3-none-any.whl.zip` & `tmp/keras_cv-0.5.1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,402 +1,416 @@
-Zip file size: 721593 bytes, number of entries: 400
--rw-r--r--  2.0 unx     1182 b- defN 23-May-10 00:10 keras_cv/__init__.py
--rw-r--r--  2.0 unx     2271 b- defN 23-May-10 00:10 keras_cv/conftest.py
--rw-r--r--  2.0 unx     1159 b- defN 23-May-10 00:10 keras_cv/version_check.py
--rw-r--r--  2.0 unx     1362 b- defN 23-May-10 00:10 keras_cv/version_check_test.py
--rw-r--r--  2.0 unx     1611 b- defN 23-May-10 00:10 keras_cv/bounding_box/__init__.py
--rw-r--r--  2.0 unx    18483 b- defN 23-May-10 00:10 keras_cv/bounding_box/converters.py
--rw-r--r--  2.0 unx     7134 b- defN 23-May-10 00:10 keras_cv/bounding_box/converters_test.py
--rw-r--r--  2.0 unx      909 b- defN 23-May-10 00:10 keras_cv/bounding_box/ensure_tensor.py
--rw-r--r--  2.0 unx     1443 b- defN 23-May-10 00:10 keras_cv/bounding_box/ensure_tensor_test.py
--rw-r--r--  2.0 unx     4035 b- defN 23-May-10 00:10 keras_cv/bounding_box/formats.py
--rw-r--r--  2.0 unx     6412 b- defN 23-May-10 00:10 keras_cv/bounding_box/iou.py
--rw-r--r--  2.0 unx     6130 b- defN 23-May-10 00:10 keras_cv/bounding_box/iou_test.py
--rw-r--r--  2.0 unx     3751 b- defN 23-May-10 00:10 keras_cv/bounding_box/mask_invalid_detections.py
--rw-r--r--  2.0 unx     3901 b- defN 23-May-10 00:10 keras_cv/bounding_box/mask_invalid_detections_test.py
--rw-r--r--  2.0 unx     3204 b- defN 23-May-10 00:10 keras_cv/bounding_box/to_dense.py
--rw-r--r--  2.0 unx     1147 b- defN 23-May-10 00:10 keras_cv/bounding_box/to_dense_test.py
--rw-r--r--  2.0 unx     3014 b- defN 23-May-10 00:10 keras_cv/bounding_box/to_ragged.py
--rw-r--r--  2.0 unx     2713 b- defN 23-May-10 00:10 keras_cv/bounding_box/to_ragged_test.py
--rw-r--r--  2.0 unx     7178 b- defN 23-May-10 00:10 keras_cv/bounding_box/utils.py
--rw-r--r--  2.0 unx     5569 b- defN 23-May-10 00:10 keras_cv/bounding_box/utils_test.py
--rw-r--r--  2.0 unx     3479 b- defN 23-May-10 00:10 keras_cv/bounding_box/validate_format.py
--rw-r--r--  2.0 unx     1581 b- defN 23-May-10 00:10 keras_cv/bounding_box/validate_format_test.py
--rw-r--r--  2.0 unx      652 b- defN 23-May-10 00:10 keras_cv/bounding_box_3d/__init__.py
--rw-r--r--  2.0 unx     1609 b- defN 23-May-10 00:10 keras_cv/bounding_box_3d/formats.py
--rw-r--r--  2.0 unx      727 b- defN 23-May-10 00:10 keras_cv/callbacks/__init__.py
--rw-r--r--  2.0 unx     4693 b- defN 23-May-10 00:10 keras_cv/callbacks/pycoco_callback.py
--rw-r--r--  2.0 unx     3263 b- defN 23-May-10 00:10 keras_cv/callbacks/pycoco_callback_test.py
--rw-r--r--  2.0 unx     6912 b- defN 23-May-10 00:10 keras_cv/callbacks/waymo_evaluation_callback.py
--rw-r--r--  2.0 unx     3413 b- defN 23-May-10 00:10 keras_cv/callbacks/waymo_evaluation_callback_test.py
--rw-r--r--  2.0 unx      936 b- defN 23-May-10 00:10 keras_cv/core/__init__.py
--rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/core/factor_sampler/__init__.py
--rw-r--r--  2.0 unx     1667 b- defN 23-May-10 00:10 keras_cv/core/factor_sampler/constant_factor_sampler.py
--rw-r--r--  2.0 unx      964 b- defN 23-May-10 00:10 keras_cv/core/factor_sampler/constant_factor_sampler_test.py
--rw-r--r--  2.0 unx     1343 b- defN 23-May-10 00:10 keras_cv/core/factor_sampler/factor_sampler.py
--rw-r--r--  2.0 unx     2501 b- defN 23-May-10 00:10 keras_cv/core/factor_sampler/normal_factor_sampler.py
--rw-r--r--  2.0 unx     1120 b- defN 23-May-10 00:10 keras_cv/core/factor_sampler/normal_factor_sampler_test_.py
--rw-r--r--  2.0 unx     2183 b- defN 23-May-10 00:10 keras_cv/core/factor_sampler/uniform_factor_sampler.py
--rw-r--r--  2.0 unx     1026 b- defN 23-May-10 00:10 keras_cv/core/factor_sampler/uniform_factor_sampler_test.py
--rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/custom_ops/__init__.py
--rw-r--r--  2.0 unx      625 b- defN 23-May-10 00:10 keras_cv/datasets/__init__.py
--rw-r--r--  2.0 unx      633 b- defN 23-May-10 00:10 keras_cv/datasets/imagenet/__init__.py
--rw-r--r--  2.0 unx     4439 b- defN 23-May-10 00:10 keras_cv/datasets/imagenet/load.py
--rw-r--r--  2.0 unx      635 b- defN 23-May-10 00:10 keras_cv/datasets/pascal_voc/__init__.py
--rw-r--r--  2.0 unx     3642 b- defN 23-May-10 00:10 keras_cv/datasets/pascal_voc/load.py
--rw-r--r--  2.0 unx    18789 b- defN 23-May-10 00:10 keras_cv/datasets/pascal_voc/segmentation.py
--rw-r--r--  2.0 unx    12179 b- defN 23-May-10 00:10 keras_cv/datasets/pascal_voc/segmentation_test.py
--rw-r--r--  2.0 unx     1103 b- defN 23-May-10 00:10 keras_cv/datasets/waymo/__init__.py
--rw-r--r--  2.0 unx     2935 b- defN 23-May-10 00:10 keras_cv/datasets/waymo/load.py
--rw-r--r--  2.0 unx     2114 b- defN 23-May-10 00:10 keras_cv/datasets/waymo/load_test.py
--rw-r--r--  2.0 unx     1980 b- defN 23-May-10 00:10 keras_cv/datasets/waymo/struct.py
--rw-r--r--  2.0 unx    27253 b- defN 23-May-10 00:10 keras_cv/datasets/waymo/transformer.py
--rw-r--r--  2.0 unx     6947 b- defN 23-May-10 00:10 keras_cv/datasets/waymo/transformer_test.py
--rw-r--r--  2.0 unx      783 b- defN 23-May-10 00:10 keras_cv/keypoint/__init__.py
--rw-r--r--  2.0 unx     6955 b- defN 23-May-10 00:10 keras_cv/keypoint/converters.py
--rw-r--r--  2.0 unx     5181 b- defN 23-May-10 00:10 keras_cv/keypoint/converters_test.py
--rw-r--r--  2.0 unx     1725 b- defN 23-May-10 00:10 keras_cv/keypoint/formats.py
--rw-r--r--  2.0 unx     1597 b- defN 23-May-10 00:10 keras_cv/keypoint/utils.py
--rw-r--r--  2.0 unx     2000 b- defN 23-May-10 00:10 keras_cv/keypoint/utils_test.py
--rw-r--r--  2.0 unx     6057 b- defN 23-May-10 00:10 keras_cv/layers/__init__.py
--rw-r--r--  2.0 unx     8828 b- defN 23-May-10 00:10 keras_cv/layers/feature_pyramid.py
--rw-r--r--  2.0 unx     5125 b- defN 23-May-10 00:10 keras_cv/layers/feature_pyramid_test.py
--rw-r--r--  2.0 unx     8076 b- defN 23-May-10 00:10 keras_cv/layers/fusedmbconv.py
--rw-r--r--  2.0 unx     2273 b- defN 23-May-10 00:10 keras_cv/layers/fusedmbconv_test.py
--rw-r--r--  2.0 unx     8349 b- defN 23-May-10 00:10 keras_cv/layers/mbconv.py
--rw-r--r--  2.0 unx     2216 b- defN 23-May-10 00:10 keras_cv/layers/mbconv_test.py
--rw-r--r--  2.0 unx    12344 b- defN 23-May-10 00:10 keras_cv/layers/serialization_test.py
--rw-r--r--  2.0 unx     6281 b- defN 23-May-10 00:10 keras_cv/layers/spatial_pyramid.py
--rw-r--r--  2.0 unx     1290 b- defN 23-May-10 00:10 keras_cv/layers/spatial_pyramid_test.py
--rw-r--r--  2.0 unx     5249 b- defN 23-May-10 00:10 keras_cv/layers/transformer_encoder.py
--rw-r--r--  2.0 unx     2135 b- defN 23-May-10 00:10 keras_cv/layers/transformer_encoder_test.py
--rw-r--r--  2.0 unx     7723 b- defN 23-May-10 00:10 keras_cv/layers/vit_layers.py
--rw-r--r--  2.0 unx     2886 b- defN 23-May-10 00:10 keras_cv/layers/vit_layers_test.py
--rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/__init__.py
--rw-r--r--  2.0 unx    11340 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/anchor_generator.py
--rw-r--r--  2.0 unx     6318 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/anchor_generator_test.py
--rw-r--r--  2.0 unx    11483 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/box_matcher.py
--rw-r--r--  2.0 unx     4939 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/box_matcher_test.py
--rw-r--r--  2.0 unx     5140 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/multi_class_non_max_suppression.py
--rw-r--r--  2.0 unx     1610 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/multi_class_non_max_suppression_test.py
--rw-r--r--  2.0 unx    15804 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/roi_align.py
--rw-r--r--  2.0 unx     9833 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/roi_generator.py
--rw-r--r--  2.0 unx     9256 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/roi_generator_test.py
--rw-r--r--  2.0 unx     6427 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/roi_pool.py
--rw-r--r--  2.0 unx     9712 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/roi_pool_test.py
--rw-r--r--  2.0 unx     9064 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/roi_sampler.py
--rw-r--r--  2.0 unx    11644 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/roi_sampler_test.py
--rw-r--r--  2.0 unx     9272 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/rpn_label_encoder.py
--rw-r--r--  2.0 unx     5631 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/rpn_label_encoder_test.py
--rw-r--r--  2.0 unx     3426 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/sampling.py
--rw-r--r--  2.0 unx     7277 b- defN 23-May-10 00:10 keras_cv/layers/object_detection/sampling_test.py
--rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/layers/object_detection_3d/__init__.py
--rw-r--r--  2.0 unx    16374 b- defN 23-May-10 00:10 keras_cv/layers/object_detection_3d/centernet_label_encoder.py
--rw-r--r--  2.0 unx     4863 b- defN 23-May-10 00:10 keras_cv/layers/object_detection_3d/centernet_label_encoder_test.py
--rw-r--r--  2.0 unx     8107 b- defN 23-May-10 00:10 keras_cv/layers/object_detection_3d/heatmap_decoder.py
--rw-r--r--  2.0 unx    10029 b- defN 23-May-10 00:10 keras_cv/layers/object_detection_3d/voxel_utils.py
--rw-r--r--  2.0 unx     2841 b- defN 23-May-10 00:10 keras_cv/layers/object_detection_3d/voxel_utils_test.py
--rw-r--r--  2.0 unx     9159 b- defN 23-May-10 00:10 keras_cv/layers/object_detection_3d/voxelization.py
--rw-r--r--  2.0 unx     4114 b- defN 23-May-10 00:10 keras_cv/layers/object_detection_3d/voxelization_test.py
--rw-r--r--  2.0 unx     4003 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/__init__.py
--rw-r--r--  2.0 unx    12754 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/aug_mix.py
--rw-r--r--  2.0 unx     2205 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/aug_mix_test.py
--rw-r--r--  2.0 unx     3509 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/auto_contrast.py
--rw-r--r--  2.0 unx     3331 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/auto_contrast_test.py
--rw-r--r--  2.0 unx    20504 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/base_image_augmentation_layer.py
--rw-r--r--  2.0 unx    10953 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/base_image_augmentation_layer_test.py
--rw-r--r--  2.0 unx     4559 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/channel_shuffle.py
--rw-r--r--  2.0 unx     4065 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/channel_shuffle_test.py
--rw-r--r--  2.0 unx     5992 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/cut_mix.py
--rw-r--r--  2.0 unx     5040 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/cut_mix_test.py
--rw-r--r--  2.0 unx     4929 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/equalization.py
--rw-r--r--  2.0 unx     2446 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/equalization_test.py
--rw-r--r--  2.0 unx     8025 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/fourier_mix.py
--rw-r--r--  2.0 unx     3447 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/fourier_mix_test.py
--rw-r--r--  2.0 unx     3841 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/grayscale.py
--rw-r--r--  2.0 unx     2820 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/grayscale_test.py
--rw-r--r--  2.0 unx     9733 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/grid_mask.py
--rw-r--r--  2.0 unx     3823 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/grid_mask_test.py
--rw-r--r--  2.0 unx    11349 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/jittered_resize.py
--rw-r--r--  2.0 unx     7360 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/jittered_resize_test.py
--rw-r--r--  2.0 unx     5019 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/maybe_apply.py
--rw-r--r--  2.0 unx     4634 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/maybe_apply_test.py
--rw-r--r--  2.0 unx     6100 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/mix_up.py
--rw-r--r--  2.0 unx     4581 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/mix_up_test.py
--rw-r--r--  2.0 unx    13424 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/mosaic.py
--rw-r--r--  2.0 unx     3726 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/mosaic_test.py
--rw-r--r--  2.0 unx     4237 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/posterization.py
--rw-r--r--  2.0 unx     3792 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/posterization_test.py
--rw-r--r--  2.0 unx     5343 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/ragged_image_test.py
--rw-r--r--  2.0 unx    10805 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/rand_augment.py
--rw-r--r--  2.0 unx     3758 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/rand_augment_test.py
--rw-r--r--  2.0 unx     4683 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_aspect_ratio.py
--rw-r--r--  2.0 unx     2277 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_aspect_ratio_test.py
--rw-r--r--  2.0 unx     4754 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_augmentation_pipeline.py
--rw-r--r--  2.0 unx     3340 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_augmentation_pipeline_test.py
--rw-r--r--  2.0 unx     5241 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_brightness.py
--rw-r--r--  2.0 unx     3337 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_brightness_test.py
--rw-r--r--  2.0 unx     4396 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_channel_shift.py
--rw-r--r--  2.0 unx     3964 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_channel_shift_test.py
--rw-r--r--  2.0 unx     4503 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_choice.py
--rw-r--r--  2.0 unx     3316 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_choice_test.py
--rw-r--r--  2.0 unx     3392 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_color_degeneration.py
--rw-r--r--  2.0 unx     2648 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_color_degeneration_test.py
--rw-r--r--  2.0 unx     6946 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_color_jitter.py
--rw-r--r--  2.0 unx     3902 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_color_jitter_test.py
--rw-r--r--  2.0 unx     4864 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_contrast.py
--rw-r--r--  2.0 unx     2213 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_contrast_test.py
--rw-r--r--  2.0 unx    10847 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_crop.py
--rw-r--r--  2.0 unx    11178 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_crop_and_resize.py
--rw-r--r--  2.0 unx    10241 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_crop_and_resize_test.py
--rw-r--r--  2.0 unx     9856 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_crop_test.py
--rw-r--r--  2.0 unx     7024 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_cutout.py
--rw-r--r--  2.0 unx     4978 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_cutout_test.py
--rw-r--r--  2.0 unx     9003 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_flip.py
--rw-r--r--  2.0 unx    10994 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_flip_test.py
--rw-r--r--  2.0 unx     4576 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_gaussian_blur.py
--rw-r--r--  2.0 unx     3245 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_gaussian_blur_test.py
--rw-r--r--  2.0 unx     5433 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_hue.py
--rw-r--r--  2.0 unx     4230 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_hue_test.py
--rw-r--r--  2.0 unx     3021 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_jpeg_quality.py
--rw-r--r--  2.0 unx     1984 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_jpeg_quality_test.py
--rw-r--r--  2.0 unx    12266 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_rotation.py
--rw-r--r--  2.0 unx     7395 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_rotation_test.py
--rw-r--r--  2.0 unx     4951 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_saturation.py
--rw-r--r--  2.0 unx     8248 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_saturation_test.py
--rw-r--r--  2.0 unx     5813 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_sharpness.py
--rw-r--r--  2.0 unx     2724 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_sharpness_test.py
--rw-r--r--  2.0 unx    12947 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_shear.py
--rw-r--r--  2.0 unx     9331 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_shear_test.py
--rw-r--r--  2.0 unx    11148 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_translation.py
--rw-r--r--  2.0 unx     8917 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_translation_test.py
--rw-r--r--  2.0 unx    10340 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_zoom.py
--rw-r--r--  2.0 unx     5859 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/random_zoom_test.py
--rw-r--r--  2.0 unx    11418 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/randomly_zoomed_crop.py
--rw-r--r--  2.0 unx     7266 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/randomly_zoomed_crop_test.py
--rw-r--r--  2.0 unx     4677 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/repeated_augmentation.py
--rw-r--r--  2.0 unx     1753 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/repeated_augmentation_test.py
--rw-r--r--  2.0 unx     2766 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/rescaling.py
--rw-r--r--  2.0 unx     2123 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/rescaling_test.py
--rw-r--r--  2.0 unx    12328 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/resizing.py
--rw-r--r--  2.0 unx    11545 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/resizing_test.py
--rw-r--r--  2.0 unx     6315 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/solarization.py
--rw-r--r--  2.0 unx     3152 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/solarization_test.py
--rw-r--r--  2.0 unx    19868 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/vectorized_base_image_augmentation_layer.py
--rw-r--r--  2.0 unx    20975 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/vectorized_base_image_augmentation_layer_test.py
--rw-r--r--  2.0 unx     5015 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/with_labels_test.py
--rw-r--r--  2.0 unx     5473 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/with_mixed_precision_test.py
--rw-r--r--  2.0 unx     4812 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing/with_segmentation_masks_test.py
--rw-r--r--  2.0 unx     1769 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/__init__.py
--rw-r--r--  2.0 unx     7987 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/base_augmentation_layer_3d.py
--rw-r--r--  2.0 unx     4789 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/base_augmentation_layer_3d_test.py
--rw-r--r--  2.0 unx     6272 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/frustum_random_dropping_points.py
--rw-r--r--  2.0 unx     5454 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/frustum_random_dropping_points_test.py
--rw-r--r--  2.0 unx     6969 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/frustum_random_point_feature_noise.py
--rw-r--r--  2.0 unx     8880 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/frustum_random_point_feature_noise_test.py
--rw-r--r--  2.0 unx     3815 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/global_random_dropping_points.py
--rw-r--r--  2.0 unx     5100 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/global_random_dropping_points_test.py
--rw-r--r--  2.0 unx     4458 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/global_random_flip.py
--rw-r--r--  2.0 unx     3275 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/global_random_flip_test.py
--rw-r--r--  2.0 unx     6016 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/global_random_rotation.py
--rw-r--r--  2.0 unx     3158 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/global_random_rotation_test.py
--rw-r--r--  2.0 unx     7090 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/global_random_scaling.py
--rw-r--r--  2.0 unx     4610 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/global_random_scaling_test.py
--rw-r--r--  2.0 unx     4733 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/global_random_translation.py
--rw-r--r--  2.0 unx     2935 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/global_random_translation_test.py
--rw-r--r--  2.0 unx    11464 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/group_points_by_bounding_boxes.py
--rw-r--r--  2.0 unx     7460 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/group_points_by_bounding_boxes_test.py
--rw-r--r--  2.0 unx    12606 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/random_copy_paste.py
--rw-r--r--  2.0 unx     8348 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/random_copy_paste_test.py
--rw-r--r--  2.0 unx     5418 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/random_drop_box.py
--rw-r--r--  2.0 unx    12338 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/random_drop_box_test.py
--rw-r--r--  2.0 unx     7322 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/swap_background.py
--rw-r--r--  2.0 unx    11087 b- defN 23-May-10 00:10 keras_cv/layers/preprocessing_3d/swap_background_test.py
--rw-r--r--  2.0 unx      868 b- defN 23-May-10 00:10 keras_cv/layers/regularization/__init__.py
--rw-r--r--  2.0 unx     2545 b- defN 23-May-10 00:10 keras_cv/layers/regularization/drop_path.py
--rw-r--r--  2.0 unx     2460 b- defN 23-May-10 00:10 keras_cv/layers/regularization/drop_path_test.py
--rw-r--r--  2.0 unx     8831 b- defN 23-May-10 00:10 keras_cv/layers/regularization/dropblock_2d.py
--rw-r--r--  2.0 unx     3653 b- defN 23-May-10 00:10 keras_cv/layers/regularization/dropblock_2d_test.py
--rw-r--r--  2.0 unx     4906 b- defN 23-May-10 00:10 keras_cv/layers/regularization/squeeze_excite.py
--rw-r--r--  2.0 unx     1882 b- defN 23-May-10 00:10 keras_cv/layers/regularization/squeeze_excite_test.py
--rw-r--r--  2.0 unx     2738 b- defN 23-May-10 00:10 keras_cv/layers/regularization/stochastic_depth.py
--rw-r--r--  2.0 unx     1771 b- defN 23-May-10 00:10 keras_cv/layers/regularization/stochastic_depth_test.py
--rw-r--r--  2.0 unx      989 b- defN 23-May-10 00:10 keras_cv/losses/__init__.py
--rw-r--r--  2.0 unx     4854 b- defN 23-May-10 00:10 keras_cv/losses/centernet_box_loss.py
--rw-r--r--  2.0 unx     1459 b- defN 23-May-10 00:10 keras_cv/losses/centernet_box_loss_test.py
--rw-r--r--  2.0 unx     4140 b- defN 23-May-10 00:10 keras_cv/losses/focal.py
--rw-r--r--  2.0 unx     2486 b- defN 23-May-10 00:10 keras_cv/losses/focal_test.py
--rw-r--r--  2.0 unx     7410 b- defN 23-May-10 00:10 keras_cv/losses/giou_loss.py
--rw-r--r--  2.0 unx     2541 b- defN 23-May-10 00:10 keras_cv/losses/giou_loss_test.py
--rw-r--r--  2.0 unx     4921 b- defN 23-May-10 00:10 keras_cv/losses/iou_loss.py
--rw-r--r--  2.0 unx     2521 b- defN 23-May-10 00:10 keras_cv/losses/iou_loss_test.py
--rw-r--r--  2.0 unx     4283 b- defN 23-May-10 00:10 keras_cv/losses/penalty_reduced_focal_loss.py
--rw-r--r--  2.0 unx     3744 b- defN 23-May-10 00:10 keras_cv/losses/penalty_reduced_focal_loss_test.py
--rw-r--r--  2.0 unx     2211 b- defN 23-May-10 00:10 keras_cv/losses/serialization_test.py
--rw-r--r--  2.0 unx     3468 b- defN 23-May-10 00:10 keras_cv/losses/simclr_loss.py
--rw-r--r--  2.0 unx     2145 b- defN 23-May-10 00:10 keras_cv/losses/simclr_loss_test.py
--rw-r--r--  2.0 unx     1885 b- defN 23-May-10 00:10 keras_cv/losses/smooth_l1.py
--rw-r--r--  2.0 unx     1225 b- defN 23-May-10 00:10 keras_cv/losses/smooth_l1_test.py
--rw-r--r--  2.0 unx      663 b- defN 23-May-10 00:10 keras_cv/metrics/__init__.py
--rw-r--r--  2.0 unx      719 b- defN 23-May-10 00:10 keras_cv/metrics/coco/__init__.py
--rw-r--r--  2.0 unx     8388 b- defN 23-May-10 00:10 keras_cv/metrics/coco/pycoco_wrapper.py
--rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/metrics/object_detection/__init__.py
--rw-r--r--  2.0 unx     9843 b- defN 23-May-10 00:10 keras_cv/metrics/object_detection/box_coco_metrics.py
--rw-r--r--  2.0 unx     4779 b- defN 23-May-10 00:10 keras_cv/metrics/object_detection/box_coco_metrics_test.py
--rw-r--r--  2.0 unx     4079 b- defN 23-May-10 00:10 keras_cv/models/__init__.py
--rw-r--r--  2.0 unx     6387 b- defN 23-May-10 00:10 keras_cv/models/task.py
--rw-r--r--  2.0 unx     1093 b- defN 23-May-10 00:10 keras_cv/models/utils.py
--rw-r--r--  2.0 unx     1133 b- defN 23-May-10 00:10 keras_cv/models/utils_test.py
--rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/models/__internal__/__init__.py
--rw-r--r--  2.0 unx     5966 b- defN 23-May-10 00:10 keras_cv/models/__internal__/unet.py
--rw-r--r--  2.0 unx     1693 b- defN 23-May-10 00:10 keras_cv/models/__internal__/unet_test.py
--rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/models/backbones/__init__.py
--rw-r--r--  2.0 unx     6264 b- defN 23-May-10 00:10 keras_cv/models/backbones/backbone.py
--rw-r--r--  2.0 unx     1839 b- defN 23-May-10 00:10 keras_cv/models/backbones/backbone_presets.py
--rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/models/backbones/csp_darknet/__init__.py
--rw-r--r--  2.0 unx    12000 b- defN 23-May-10 00:10 keras_cv/models/backbones/csp_darknet/csp_darknet_backbone.py
--rw-r--r--  2.0 unx     6518 b- defN 23-May-10 00:10 keras_cv/models/backbones/csp_darknet/csp_darknet_backbone_presets.py
--rw-r--r--  2.0 unx     4028 b- defN 23-May-10 00:10 keras_cv/models/backbones/csp_darknet/csp_darknet_backbone_presets_test.py
--rw-r--r--  2.0 unx     5552 b- defN 23-May-10 00:10 keras_cv/models/backbones/csp_darknet/csp_darknet_backbone_test.py
--rw-r--r--  2.0 unx    12186 b- defN 23-May-10 00:10 keras_cv/models/backbones/csp_darknet/csp_darknet_utils.py
--rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/models/backbones/efficientnet_v2/__init__.py
--rw-r--r--  2.0 unx     9316 b- defN 23-May-10 00:10 keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_aliases.py
--rw-r--r--  2.0 unx    12981 b- defN 23-May-10 00:10 keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone.py
--rw-r--r--  2.0 unx    19514 b- defN 23-May-10 00:10 keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_presets.py
--rw-r--r--  2.0 unx     2287 b- defN 23-May-10 00:10 keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_presets_test.py
--rw-r--r--  2.0 unx     8546 b- defN 23-May-10 00:10 keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_test.py
--rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/models/backbones/mobilenet_v3/__init__.py
--rw-r--r--  2.0 unx    15819 b- defN 23-May-10 00:10 keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone.py
--rw-r--r--  2.0 unx     6080 b- defN 23-May-10 00:10 keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_presets.py
--rw-r--r--  2.0 unx     1418 b- defN 23-May-10 00:10 keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_presets_test.py
--rw-r--r--  2.0 unx     3266 b- defN 23-May-10 00:10 keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_test.py
--rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/models/backbones/resnet_v1/__init__.py
--rw-r--r--  2.0 unx    17276 b- defN 23-May-10 00:10 keras_cv/models/backbones/resnet_v1/resnet_v1_backbone.py
--rw-r--r--  2.0 unx     5551 b- defN 23-May-10 00:10 keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_presets.py
--rw-r--r--  2.0 unx     3546 b- defN 23-May-10 00:10 keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_presets_test.py
--rw-r--r--  2.0 unx     6283 b- defN 23-May-10 00:10 keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_test.py
--rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/models/backbones/resnet_v2/__init__.py
--rw-r--r--  2.0 unx    18820 b- defN 23-May-10 00:10 keras_cv/models/backbones/resnet_v2/resnet_v2_backbone.py
--rw-r--r--  2.0 unx     5617 b- defN 23-May-10 00:10 keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_presets.py
--rw-r--r--  2.0 unx     3724 b- defN 23-May-10 00:10 keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_presets_test.py
--rw-r--r--  2.0 unx     5693 b- defN 23-May-10 00:10 keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_test.py
--rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/models/classification/__init__.py
--rw-r--r--  2.0 unx     4645 b- defN 23-May-10 00:10 keras_cv/models/classification/image_classifier.py
--rw-r--r--  2.0 unx     8454 b- defN 23-May-10 00:10 keras_cv/models/classification/image_classifier_presets.py
--rw-r--r--  2.0 unx     7098 b- defN 23-May-10 00:10 keras_cv/models/classification/image_classifier_test.py
--rw-r--r--  2.0 unx     4514 b- defN 23-May-10 00:10 keras_cv/models/legacy/__init__.py
--rw-r--r--  2.0 unx    14440 b- defN 23-May-10 00:10 keras_cv/models/legacy/convmixer.py
--rw-r--r--  2.0 unx     1835 b- defN 23-May-10 00:10 keras_cv/models/legacy/convmixer_test.py
--rw-r--r--  2.0 unx    20264 b- defN 23-May-10 00:10 keras_cv/models/legacy/convnext.py
--rw-r--r--  2.0 unx     2456 b- defN 23-May-10 00:10 keras_cv/models/legacy/convnext_test.py
--rw-r--r--  2.0 unx    11050 b- defN 23-May-10 00:10 keras_cv/models/legacy/darknet.py
--rw-r--r--  2.0 unx     1823 b- defN 23-May-10 00:10 keras_cv/models/legacy/darknet_test.py
--rw-r--r--  2.0 unx    13432 b- defN 23-May-10 00:10 keras_cv/models/legacy/densenet.py
--rw-r--r--  2.0 unx     2001 b- defN 23-May-10 00:10 keras_cv/models/legacy/densenet_test.py
--rw-r--r--  2.0 unx    22966 b- defN 23-May-10 00:10 keras_cv/models/legacy/efficientnet_lite.py
--rw-r--r--  2.0 unx     2173 b- defN 23-May-10 00:10 keras_cv/models/legacy/efficientnet_lite_test.py
--rw-r--r--  2.0 unx    29352 b- defN 23-May-10 00:10 keras_cv/models/legacy/efficientnet_v1.py
--rw-r--r--  2.0 unx     2265 b- defN 23-May-10 00:10 keras_cv/models/legacy/efficientnet_v1_test.py
--rw-r--r--  2.0 unx    14405 b- defN 23-May-10 00:10 keras_cv/models/legacy/mlp_mixer.py
--rw-r--r--  2.0 unx     2050 b- defN 23-May-10 00:10 keras_cv/models/legacy/mlp_mixer_test.py
--rw-r--r--  2.0 unx     6541 b- defN 23-May-10 00:10 keras_cv/models/legacy/models_test.py
--rw-r--r--  2.0 unx    45767 b- defN 23-May-10 00:10 keras_cv/models/legacy/regnet.py
--rw-r--r--  2.0 unx     2265 b- defN 23-May-10 00:10 keras_cv/models/legacy/regnetx_test.py
--rw-r--r--  2.0 unx     2265 b- defN 23-May-10 00:10 keras_cv/models/legacy/regnety_test.py
--rw-r--r--  2.0 unx     3662 b- defN 23-May-10 00:10 keras_cv/models/legacy/utils.py
--rw-r--r--  2.0 unx     2320 b- defN 23-May-10 00:10 keras_cv/models/legacy/utils_test.py
--rw-r--r--  2.0 unx     8144 b- defN 23-May-10 00:10 keras_cv/models/legacy/vgg16.py
--rw-r--r--  2.0 unx     1779 b- defN 23-May-10 00:10 keras_cv/models/legacy/vgg16_test.py
--rw-r--r--  2.0 unx     7103 b- defN 23-May-10 00:10 keras_cv/models/legacy/vgg19.py
--rw-r--r--  2.0 unx     1779 b- defN 23-May-10 00:10 keras_cv/models/legacy/vgg19_test.py
--rw-r--r--  2.0 unx    26159 b- defN 23-May-10 00:10 keras_cv/models/legacy/vit.py
--rw-r--r--  2.0 unx     2410 b- defN 23-May-10 00:10 keras_cv/models/legacy/vit_test.py
--rw-r--r--  2.0 unx     8729 b- defN 23-May-10 00:10 keras_cv/models/legacy/weights.py
--rw-r--r--  2.0 unx      651 b- defN 23-May-10 00:10 keras_cv/models/legacy/segmentation/__init__.py
--rw-r--r--  2.0 unx    12412 b- defN 23-May-10 00:10 keras_cv/models/legacy/segmentation/deeplab.py
--rw-r--r--  2.0 unx     5955 b- defN 23-May-10 00:10 keras_cv/models/legacy/segmentation/deeplab_test.py
--rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/models/object_detection/__init__.py
--rw-r--r--  2.0 unx     4329 b- defN 23-May-10 00:10 keras_cv/models/object_detection/__internal__.py
--rw-r--r--  2.0 unx     1904 b- defN 23-May-10 00:10 keras_cv/models/object_detection/__test_utils__.py
--rw-r--r--  2.0 unx     3490 b- defN 23-May-10 00:10 keras_cv/models/object_detection/predict_utils.py
--rw-r--r--  2.0 unx      898 b- defN 23-May-10 00:10 keras_cv/models/object_detection/retinanet/__init__.py
--rw-r--r--  2.0 unx     2501 b- defN 23-May-10 00:10 keras_cv/models/object_detection/retinanet/feature_pyramid.py
--rw-r--r--  2.0 unx     2841 b- defN 23-May-10 00:10 keras_cv/models/object_detection/retinanet/prediction_head.py
--rw-r--r--  2.0 unx    23051 b- defN 23-May-10 00:10 keras_cv/models/object_detection/retinanet/retinanet.py
--rw-r--r--  2.0 unx     9759 b- defN 23-May-10 00:10 keras_cv/models/object_detection/retinanet/retinanet_label_encoder.py
--rw-r--r--  2.0 unx     4594 b- defN 23-May-10 00:10 keras_cv/models/object_detection/retinanet/retinanet_label_encoder_test.py
--rw-r--r--  2.0 unx     1634 b- defN 23-May-10 00:10 keras_cv/models/object_detection/retinanet/retinanet_presets.py
--rw-r--r--  2.0 unx     7822 b- defN 23-May-10 00:10 keras_cv/models/object_detection/retinanet/retinanet_test.py
--rw-r--r--  2.0 unx      687 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolo_v8/__init__.py
--rw-r--r--  2.0 unx     7018 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolo_v8/yolo_v8_backbone.py
--rw-r--r--  2.0 unx     6577 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolo_v8/yolo_v8_backbone_presets.py
--rw-r--r--  2.0 unx    22502 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolo_v8/yolo_v8_detector.py
--rw-r--r--  2.0 unx     1595 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolo_v8/yolo_v8_detector_presets.py
--rw-r--r--  2.0 unx     4708 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolo_v8/yolo_v8_detector_test.py
--rw-r--r--  2.0 unx     2090 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolo_v8/yolo_v8_iou_loss.py
--rw-r--r--  2.0 unx    14891 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolo_v8/yolo_v8_label_encoder.py
--rw-r--r--  2.0 unx     2884 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolo_v8/yolo_v8_layers.py
--rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolox/__init__.py
--rw-r--r--  2.0 unx     3419 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolox/binary_crossentropy.py
--rw-r--r--  2.0 unx      954 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolox/layers/__init__.py
--rw-r--r--  2.0 unx     6313 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolox/layers/yolox_decoder.py
--rw-r--r--  2.0 unx     5423 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolox/layers/yolox_head.py
--rw-r--r--  2.0 unx     1859 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolox/layers/yolox_head_test.py
--rw-r--r--  2.0 unx     1923 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolox/layers/yolox_label_encoder.py
--rw-r--r--  2.0 unx     2986 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolox/layers/yolox_label_encoder_test.py
--rw-r--r--  2.0 unx     5198 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolox/layers/yolox_pafpn.py
--rw-r--r--  2.0 unx     1806 b- defN 23-May-10 00:10 keras_cv/models/object_detection/yolox/layers/yolox_pafpn_test.py
--rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/models/object_detection_3d/__init__.py
--rw-r--r--  2.0 unx    10038 b- defN 23-May-10 00:10 keras_cv/models/object_detection_3d/center_pillar.py
--rw-r--r--  2.0 unx     5577 b- defN 23-May-10 00:10 keras_cv/models/object_detection_3d/center_pillar_test.py
--rw-r--r--  2.0 unx     1324 b- defN 23-May-10 00:10 keras_cv/models/stable_diffusion/__init__.py
--rw-r--r--  2.0 unx     7025 b- defN 23-May-10 00:10 keras_cv/models/stable_diffusion/clip_tokenizer.py
--rw-r--r--  2.0 unx    17410 b- defN 23-May-10 00:10 keras_cv/models/stable_diffusion/constants.py
--rw-r--r--  2.0 unx     2690 b- defN 23-May-10 00:10 keras_cv/models/stable_diffusion/decoder.py
--rw-r--r--  2.0 unx    13271 b- defN 23-May-10 00:10 keras_cv/models/stable_diffusion/diffusion_model.py
--rw-r--r--  2.0 unx     2757 b- defN 23-May-10 00:10 keras_cv/models/stable_diffusion/image_encoder.py
--rw-r--r--  2.0 unx     7770 b- defN 23-May-10 00:10 keras_cv/models/stable_diffusion/noise_scheduler.py
--rw-r--r--  2.0 unx    19421 b- defN 23-May-10 00:10 keras_cv/models/stable_diffusion/stable_diffusion.py
--rw-r--r--  2.0 unx     2414 b- defN 23-May-10 00:10 keras_cv/models/stable_diffusion/stable_diffusion_test.py
--rw-r--r--  2.0 unx     6680 b- defN 23-May-10 00:10 keras_cv/models/stable_diffusion/text_encoder.py
--rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/models/stable_diffusion/__internal__/__init__.py
--rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/models/stable_diffusion/__internal__/layers/__init__.py
--rw-r--r--  2.0 unx     1948 b- defN 23-May-10 00:10 keras_cv/models/stable_diffusion/__internal__/layers/attention_block.py
--rw-r--r--  2.0 unx     1005 b- defN 23-May-10 00:10 keras_cv/models/stable_diffusion/__internal__/layers/padded_conv2d.py
--rw-r--r--  2.0 unx     1560 b- defN 23-May-10 00:10 keras_cv/models/stable_diffusion/__internal__/layers/resnet_block.py
--rw-r--r--  2.0 unx      624 b- defN 23-May-10 00:10 keras_cv/ops/__init__.py
--rw-r--r--  2.0 unx     1589 b- defN 23-May-10 00:10 keras_cv/ops/iou_3d.py
--rw-r--r--  2.0 unx     2242 b- defN 23-May-10 00:10 keras_cv/ops/iou_3d_test.py
--rw-r--r--  2.0 unx     1522 b- defN 23-May-10 00:10 keras_cv/point_cloud/__init__.py
--rw-r--r--  2.0 unx    18327 b- defN 23-May-10 00:10 keras_cv/point_cloud/point_cloud.py
--rw-r--r--  2.0 unx    14038 b- defN 23-May-10 00:10 keras_cv/point_cloud/point_cloud_test.py
--rw-r--r--  2.0 unx     7774 b- defN 23-May-10 00:10 keras_cv/point_cloud/within_box_3d_test.py
--rw-r--r--  2.0 unx      810 b- defN 23-May-10 00:10 keras_cv/training/__init__.py
--rw-r--r--  2.0 unx      584 b- defN 23-May-10 00:10 keras_cv/training/contrastive/__init__.py
--rw-r--r--  2.0 unx     9414 b- defN 23-May-10 00:10 keras_cv/training/contrastive/contrastive_trainer.py
--rw-r--r--  2.0 unx     6082 b- defN 23-May-10 00:10 keras_cv/training/contrastive/contrastive_trainer_test.py
--rw-r--r--  2.0 unx     3197 b- defN 23-May-10 00:10 keras_cv/training/contrastive/simclr_trainer.py
--rw-r--r--  2.0 unx     1609 b- defN 23-May-10 00:10 keras_cv/training/contrastive/simclr_trainer_test.py
--rw-r--r--  2.0 unx     1408 b- defN 23-May-10 00:10 keras_cv/utils/__init__.py
--rw-r--r--  2.0 unx     2080 b- defN 23-May-10 00:10 keras_cv/utils/conditional_imports.py
--rw-r--r--  2.0 unx     2474 b- defN 23-May-10 00:10 keras_cv/utils/conv_utils.py
--rw-r--r--  2.0 unx     3105 b- defN 23-May-10 00:10 keras_cv/utils/fill_utils.py
--rw-r--r--  2.0 unx    11265 b- defN 23-May-10 00:10 keras_cv/utils/fill_utils_test.py
--rw-r--r--  2.0 unx    14465 b- defN 23-May-10 00:10 keras_cv/utils/preprocessing.py
--rw-r--r--  2.0 unx     2303 b- defN 23-May-10 00:10 keras_cv/utils/preprocessing_test.py
--rw-r--r--  2.0 unx     1802 b- defN 23-May-10 00:10 keras_cv/utils/python_utils.py
--rw-r--r--  2.0 unx     2843 b- defN 23-May-10 00:10 keras_cv/utils/resource_loader.py
--rw-r--r--  2.0 unx     4731 b- defN 23-May-10 00:10 keras_cv/utils/target_gather.py
--rw-r--r--  2.0 unx     5346 b- defN 23-May-10 00:10 keras_cv/utils/target_gather_test.py
--rw-r--r--  2.0 unx     3628 b- defN 23-May-10 00:10 keras_cv/utils/test_utils.py
--rw-r--r--  2.0 unx      991 b- defN 23-May-10 00:10 keras_cv/utils/to_numpy.py
--rw-r--r--  2.0 unx     2813 b- defN 23-May-10 00:10 keras_cv/utils/train.py
--rw-r--r--  2.0 unx      754 b- defN 23-May-10 00:10 keras_cv/visualization/__init__.py
--rw-r--r--  2.0 unx     5497 b- defN 23-May-10 00:10 keras_cv/visualization/draw_bounding_boxes.py
--rw-r--r--  2.0 unx     6120 b- defN 23-May-10 00:10 keras_cv/visualization/plot_bounding_box_gallery.py
--rw-r--r--  2.0 unx     3945 b- defN 23-May-10 00:10 keras_cv/visualization/plot_image_gallery.py
--rw-r--r--  2.0 unx    11412 b- defN 23-May-10 00:11 keras_cv-0.5.0.dist-info/LICENSE
--rw-r--r--  2.0 unx    10792 b- defN 23-May-10 00:11 keras_cv-0.5.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-May-10 00:11 keras_cv-0.5.0.dist-info/WHEEL
--rw-r--r--  2.0 unx        9 b- defN 23-May-10 00:11 keras_cv-0.5.0.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    41631 b- defN 23-May-10 00:11 keras_cv-0.5.0.dist-info/RECORD
-400 files, 2334301 bytes uncompressed, 653439 bytes compressed:  72.0%
+Zip file size: 742026 bytes, number of entries: 414
+-rw-r--r--  2.0 unx     1182 b- defN 23-Jul-07 12:41 keras_cv/__init__.py
+-rw-r--r--  2.0 unx     2354 b- defN 23-Jul-07 12:41 keras_cv/conftest.py
+-rw-r--r--  2.0 unx     1159 b- defN 23-Jul-07 12:41 keras_cv/version_check.py
+-rw-r--r--  2.0 unx     1362 b- defN 23-Jul-07 12:41 keras_cv/version_check_test.py
+-rw-r--r--  2.0 unx     1662 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/__init__.py
+-rw-r--r--  2.0 unx    18483 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/converters.py
+-rw-r--r--  2.0 unx     7134 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/converters_test.py
+-rw-r--r--  2.0 unx      909 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/ensure_tensor.py
+-rw-r--r--  2.0 unx     1443 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/ensure_tensor_test.py
+-rw-r--r--  2.0 unx     4035 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/formats.py
+-rw-r--r--  2.0 unx     9144 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/iou.py
+-rw-r--r--  2.0 unx     6130 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/iou_test.py
+-rw-r--r--  2.0 unx     3751 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/mask_invalid_detections.py
+-rw-r--r--  2.0 unx     3901 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/mask_invalid_detections_test.py
+-rw-r--r--  2.0 unx     3204 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/to_dense.py
+-rw-r--r--  2.0 unx     1147 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/to_dense_test.py
+-rw-r--r--  2.0 unx     3014 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/to_ragged.py
+-rw-r--r--  2.0 unx     2713 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/to_ragged_test.py
+-rw-r--r--  2.0 unx     7178 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/utils.py
+-rw-r--r--  2.0 unx     5569 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/utils_test.py
+-rw-r--r--  2.0 unx     3479 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/validate_format.py
+-rw-r--r--  2.0 unx     1581 b- defN 23-Jul-07 12:41 keras_cv/bounding_box/validate_format_test.py
+-rw-r--r--  2.0 unx      652 b- defN 23-Jul-07 12:41 keras_cv/bounding_box_3d/__init__.py
+-rw-r--r--  2.0 unx     1609 b- defN 23-Jul-07 12:41 keras_cv/bounding_box_3d/formats.py
+-rw-r--r--  2.0 unx      727 b- defN 23-Jul-07 12:41 keras_cv/callbacks/__init__.py
+-rw-r--r--  2.0 unx     4693 b- defN 23-Jul-07 12:41 keras_cv/callbacks/pycoco_callback.py
+-rw-r--r--  2.0 unx     3323 b- defN 23-Jul-07 12:41 keras_cv/callbacks/pycoco_callback_test.py
+-rw-r--r--  2.0 unx     6910 b- defN 23-Jul-07 12:41 keras_cv/callbacks/waymo_evaluation_callback.py
+-rw-r--r--  2.0 unx     3413 b- defN 23-Jul-07 12:41 keras_cv/callbacks/waymo_evaluation_callback_test.py
+-rw-r--r--  2.0 unx      936 b- defN 23-Jul-07 12:41 keras_cv/core/__init__.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/core/factor_sampler/__init__.py
+-rw-r--r--  2.0 unx     1667 b- defN 23-Jul-07 12:41 keras_cv/core/factor_sampler/constant_factor_sampler.py
+-rw-r--r--  2.0 unx      964 b- defN 23-Jul-07 12:41 keras_cv/core/factor_sampler/constant_factor_sampler_test.py
+-rw-r--r--  2.0 unx     1343 b- defN 23-Jul-07 12:41 keras_cv/core/factor_sampler/factor_sampler.py
+-rw-r--r--  2.0 unx     2501 b- defN 23-Jul-07 12:41 keras_cv/core/factor_sampler/normal_factor_sampler.py
+-rw-r--r--  2.0 unx     1120 b- defN 23-Jul-07 12:41 keras_cv/core/factor_sampler/normal_factor_sampler_test_.py
+-rw-r--r--  2.0 unx     2183 b- defN 23-Jul-07 12:41 keras_cv/core/factor_sampler/uniform_factor_sampler.py
+-rw-r--r--  2.0 unx     1026 b- defN 23-Jul-07 12:41 keras_cv/core/factor_sampler/uniform_factor_sampler_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/custom_ops/__init__.py
+-rw-r--r--  2.0 unx      625 b- defN 23-Jul-07 12:41 keras_cv/datasets/__init__.py
+-rw-r--r--  2.0 unx      633 b- defN 23-Jul-07 12:41 keras_cv/datasets/imagenet/__init__.py
+-rw-r--r--  2.0 unx     4439 b- defN 23-Jul-07 12:41 keras_cv/datasets/imagenet/load.py
+-rw-r--r--  2.0 unx      635 b- defN 23-Jul-07 12:41 keras_cv/datasets/pascal_voc/__init__.py
+-rw-r--r--  2.0 unx     3642 b- defN 23-Jul-07 12:41 keras_cv/datasets/pascal_voc/load.py
+-rw-r--r--  2.0 unx    18789 b- defN 23-Jul-07 12:41 keras_cv/datasets/pascal_voc/segmentation.py
+-rw-r--r--  2.0 unx    12179 b- defN 23-Jul-07 12:41 keras_cv/datasets/pascal_voc/segmentation_test.py
+-rw-r--r--  2.0 unx     1103 b- defN 23-Jul-07 12:41 keras_cv/datasets/waymo/__init__.py
+-rw-r--r--  2.0 unx     2935 b- defN 23-Jul-07 12:41 keras_cv/datasets/waymo/load.py
+-rw-r--r--  2.0 unx     2114 b- defN 23-Jul-07 12:41 keras_cv/datasets/waymo/load_test.py
+-rw-r--r--  2.0 unx     1980 b- defN 23-Jul-07 12:41 keras_cv/datasets/waymo/struct.py
+-rw-r--r--  2.0 unx    27253 b- defN 23-Jul-07 12:41 keras_cv/datasets/waymo/transformer.py
+-rw-r--r--  2.0 unx     6947 b- defN 23-Jul-07 12:41 keras_cv/datasets/waymo/transformer_test.py
+-rw-r--r--  2.0 unx      783 b- defN 23-Jul-07 12:41 keras_cv/keypoint/__init__.py
+-rw-r--r--  2.0 unx     6955 b- defN 23-Jul-07 12:41 keras_cv/keypoint/converters.py
+-rw-r--r--  2.0 unx     5181 b- defN 23-Jul-07 12:41 keras_cv/keypoint/converters_test.py
+-rw-r--r--  2.0 unx     1725 b- defN 23-Jul-07 12:41 keras_cv/keypoint/formats.py
+-rw-r--r--  2.0 unx     1597 b- defN 23-Jul-07 12:41 keras_cv/keypoint/utils.py
+-rw-r--r--  2.0 unx     2000 b- defN 23-Jul-07 12:41 keras_cv/keypoint/utils_test.py
+-rw-r--r--  2.0 unx     6103 b- defN 23-Jul-07 12:41 keras_cv/layers/__init__.py
+-rw-r--r--  2.0 unx     8828 b- defN 23-Jul-07 12:41 keras_cv/layers/feature_pyramid.py
+-rw-r--r--  2.0 unx     5125 b- defN 23-Jul-07 12:41 keras_cv/layers/feature_pyramid_test.py
+-rw-r--r--  2.0 unx     8076 b- defN 23-Jul-07 12:41 keras_cv/layers/fusedmbconv.py
+-rw-r--r--  2.0 unx     2273 b- defN 23-Jul-07 12:41 keras_cv/layers/fusedmbconv_test.py
+-rw-r--r--  2.0 unx     8349 b- defN 23-Jul-07 12:41 keras_cv/layers/mbconv.py
+-rw-r--r--  2.0 unx     2216 b- defN 23-Jul-07 12:41 keras_cv/layers/mbconv_test.py
+-rw-r--r--  2.0 unx    12061 b- defN 23-Jul-07 12:41 keras_cv/layers/serialization_test.py
+-rw-r--r--  2.0 unx     6281 b- defN 23-Jul-07 12:41 keras_cv/layers/spatial_pyramid.py
+-rw-r--r--  2.0 unx     1290 b- defN 23-Jul-07 12:41 keras_cv/layers/spatial_pyramid_test.py
+-rw-r--r--  2.0 unx     5249 b- defN 23-Jul-07 12:41 keras_cv/layers/transformer_encoder.py
+-rw-r--r--  2.0 unx     2135 b- defN 23-Jul-07 12:41 keras_cv/layers/transformer_encoder_test.py
+-rw-r--r--  2.0 unx     7723 b- defN 23-Jul-07 12:41 keras_cv/layers/vit_layers.py
+-rw-r--r--  2.0 unx     2886 b- defN 23-Jul-07 12:41 keras_cv/layers/vit_layers_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/__init__.py
+-rw-r--r--  2.0 unx    11406 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/anchor_generator.py
+-rw-r--r--  2.0 unx     6422 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/anchor_generator_test.py
+-rw-r--r--  2.0 unx    11483 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/box_matcher.py
+-rw-r--r--  2.0 unx     4939 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/box_matcher_test.py
+-rw-r--r--  2.0 unx     5140 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/multi_class_non_max_suppression.py
+-rw-r--r--  2.0 unx     1610 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/multi_class_non_max_suppression_test.py
+-rw-r--r--  2.0 unx    16185 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/roi_align.py
+-rw-r--r--  2.0 unx     9833 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/roi_generator.py
+-rw-r--r--  2.0 unx     9256 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/roi_generator_test.py
+-rw-r--r--  2.0 unx     6427 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/roi_pool.py
+-rw-r--r--  2.0 unx     9712 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/roi_pool_test.py
+-rw-r--r--  2.0 unx     9064 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/roi_sampler.py
+-rw-r--r--  2.0 unx    11643 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/roi_sampler_test.py
+-rw-r--r--  2.0 unx     9272 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/rpn_label_encoder.py
+-rw-r--r--  2.0 unx     5631 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/rpn_label_encoder_test.py
+-rw-r--r--  2.0 unx     3426 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/sampling.py
+-rw-r--r--  2.0 unx     7277 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection/sampling_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection_3d/__init__.py
+-rw-r--r--  2.0 unx    16098 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection_3d/centernet_label_encoder.py
+-rw-r--r--  2.0 unx     4743 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection_3d/centernet_label_encoder_test.py
+-rw-r--r--  2.0 unx     8107 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection_3d/heatmap_decoder.py
+-rw-r--r--  2.0 unx    10029 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection_3d/voxel_utils.py
+-rw-r--r--  2.0 unx     2841 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection_3d/voxel_utils_test.py
+-rw-r--r--  2.0 unx     9159 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection_3d/voxelization.py
+-rw-r--r--  2.0 unx     4114 b- defN 23-Jul-07 12:41 keras_cv/layers/object_detection_3d/voxelization_test.py
+-rw-r--r--  2.0 unx     3914 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/__init__.py
+-rw-r--r--  2.0 unx    12754 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/aug_mix.py
+-rw-r--r--  2.0 unx     2580 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/aug_mix_test.py
+-rw-r--r--  2.0 unx     3509 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/auto_contrast.py
+-rw-r--r--  2.0 unx     3331 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/auto_contrast_test.py
+-rw-r--r--  2.0 unx    20504 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/base_image_augmentation_layer.py
+-rw-r--r--  2.0 unx    10953 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/base_image_augmentation_layer_test.py
+-rw-r--r--  2.0 unx     4559 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/channel_shuffle.py
+-rw-r--r--  2.0 unx     4065 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/channel_shuffle_test.py
+-rw-r--r--  2.0 unx     5992 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/cut_mix.py
+-rw-r--r--  2.0 unx     5040 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/cut_mix_test.py
+-rw-r--r--  2.0 unx     4929 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/equalization.py
+-rw-r--r--  2.0 unx     2446 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/equalization_test.py
+-rw-r--r--  2.0 unx     8025 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/fourier_mix.py
+-rw-r--r--  2.0 unx     3447 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/fourier_mix_test.py
+-rw-r--r--  2.0 unx     3841 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/grayscale.py
+-rw-r--r--  2.0 unx     2820 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/grayscale_test.py
+-rw-r--r--  2.0 unx     9733 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/grid_mask.py
+-rw-r--r--  2.0 unx     3823 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/grid_mask_test.py
+-rw-r--r--  2.0 unx    11349 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/jittered_resize.py
+-rw-r--r--  2.0 unx     7360 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/jittered_resize_test.py
+-rw-r--r--  2.0 unx     7461 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/mix_up.py
+-rw-r--r--  2.0 unx     6180 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/mix_up_test.py
+-rw-r--r--  2.0 unx    13424 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/mosaic.py
+-rw-r--r--  2.0 unx     3726 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/mosaic_test.py
+-rw-r--r--  2.0 unx     4237 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/posterization.py
+-rw-r--r--  2.0 unx     3792 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/posterization_test.py
+-rw-r--r--  2.0 unx     5101 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/ragged_image_test.py
+-rw-r--r--  2.0 unx    10805 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/rand_augment.py
+-rw-r--r--  2.0 unx     3758 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/rand_augment_test.py
+-rw-r--r--  2.0 unx     5023 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_apply.py
+-rw-r--r--  2.0 unx     4646 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_apply_test.py
+-rw-r--r--  2.0 unx     4683 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_aspect_ratio.py
+-rw-r--r--  2.0 unx     2277 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_aspect_ratio_test.py
+-rw-r--r--  2.0 unx     4754 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_augmentation_pipeline.py
+-rw-r--r--  2.0 unx     3340 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_augmentation_pipeline_test.py
+-rw-r--r--  2.0 unx     5241 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_brightness.py
+-rw-r--r--  2.0 unx     3337 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_brightness_test.py
+-rw-r--r--  2.0 unx     4396 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_channel_shift.py
+-rw-r--r--  2.0 unx     3964 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_channel_shift_test.py
+-rw-r--r--  2.0 unx     4503 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_choice.py
+-rw-r--r--  2.0 unx     3316 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_choice_test.py
+-rw-r--r--  2.0 unx     3392 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_color_degeneration.py
+-rw-r--r--  2.0 unx     2648 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_color_degeneration_test.py
+-rw-r--r--  2.0 unx     6946 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_color_jitter.py
+-rw-r--r--  2.0 unx     3902 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_color_jitter_test.py
+-rw-r--r--  2.0 unx     4864 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_contrast.py
+-rw-r--r--  2.0 unx     2213 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_contrast_test.py
+-rw-r--r--  2.0 unx    10846 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_crop.py
+-rw-r--r--  2.0 unx    11178 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_crop_and_resize.py
+-rw-r--r--  2.0 unx    10241 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_crop_and_resize_test.py
+-rw-r--r--  2.0 unx     9856 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_crop_test.py
+-rw-r--r--  2.0 unx     7024 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_cutout.py
+-rw-r--r--  2.0 unx     4978 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_cutout_test.py
+-rw-r--r--  2.0 unx     9003 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_flip.py
+-rw-r--r--  2.0 unx    10994 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_flip_test.py
+-rw-r--r--  2.0 unx     4576 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_gaussian_blur.py
+-rw-r--r--  2.0 unx     3245 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_gaussian_blur_test.py
+-rw-r--r--  2.0 unx     5433 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_hue.py
+-rw-r--r--  2.0 unx     4230 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_hue_test.py
+-rw-r--r--  2.0 unx     3021 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_jpeg_quality.py
+-rw-r--r--  2.0 unx     1984 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_jpeg_quality_test.py
+-rw-r--r--  2.0 unx    12271 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_rotation.py
+-rw-r--r--  2.0 unx     7395 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_rotation_test.py
+-rw-r--r--  2.0 unx     4951 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_saturation.py
+-rw-r--r--  2.0 unx     8248 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_saturation_test.py
+-rw-r--r--  2.0 unx     5813 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_sharpness.py
+-rw-r--r--  2.0 unx     2724 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_sharpness_test.py
+-rw-r--r--  2.0 unx    13111 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_shear.py
+-rw-r--r--  2.0 unx     9331 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_shear_test.py
+-rw-r--r--  2.0 unx    11148 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_translation.py
+-rw-r--r--  2.0 unx     8917 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_translation_test.py
+-rw-r--r--  2.0 unx    10340 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_zoom.py
+-rw-r--r--  2.0 unx     5859 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/random_zoom_test.py
+-rw-r--r--  2.0 unx     4677 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/repeated_augmentation.py
+-rw-r--r--  2.0 unx     1753 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/repeated_augmentation_test.py
+-rw-r--r--  2.0 unx     2766 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/rescaling.py
+-rw-r--r--  2.0 unx     2123 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/rescaling_test.py
+-rw-r--r--  2.0 unx    15342 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/resizing.py
+-rw-r--r--  2.0 unx    12313 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/resizing_test.py
+-rw-r--r--  2.0 unx     6315 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/solarization.py
+-rw-r--r--  2.0 unx     3152 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/solarization_test.py
+-rw-r--r--  2.0 unx    19868 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/vectorized_base_image_augmentation_layer.py
+-rw-r--r--  2.0 unx    20977 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/vectorized_base_image_augmentation_layer_test.py
+-rw-r--r--  2.0 unx     4773 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/with_labels_test.py
+-rw-r--r--  2.0 unx     5231 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/with_mixed_precision_test.py
+-rw-r--r--  2.0 unx     4885 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing/with_segmentation_masks_test.py
+-rw-r--r--  2.0 unx     1904 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/__init__.py
+-rw-r--r--  2.0 unx    10608 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/base_augmentation_layer_3d.py
+-rw-r--r--  2.0 unx     4789 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/base_augmentation_layer_3d_test.py
+-rw-r--r--  2.0 unx     3623 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/input_format_test.py
+-rw-r--r--  2.0 unx      172 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/__init__.py
+-rw-r--r--  2.0 unx     5860 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/frustum_random_dropping_points.py
+-rw-r--r--  2.0 unx     5063 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/frustum_random_dropping_points_test.py
+-rw-r--r--  2.0 unx     6556 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/frustum_random_point_feature_noise.py
+-rw-r--r--  2.0 unx     8474 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/frustum_random_point_feature_noise_test.py
+-rw-r--r--  2.0 unx     3403 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/global_random_dropping_points.py
+-rw-r--r--  2.0 unx     4709 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/global_random_dropping_points_test.py
+-rw-r--r--  2.0 unx     4046 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/global_random_flip.py
+-rw-r--r--  2.0 unx     2879 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/global_random_flip_test.py
+-rw-r--r--  2.0 unx     5604 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/global_random_rotation.py
+-rw-r--r--  2.0 unx     2753 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/global_random_rotation_test.py
+-rw-r--r--  2.0 unx     6678 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/global_random_scaling.py
+-rw-r--r--  2.0 unx     4205 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/global_random_scaling_test.py
+-rw-r--r--  2.0 unx     4321 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/global_random_translation.py
+-rw-r--r--  2.0 unx     2530 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/global_random_translation_test.py
+-rw-r--r--  2.0 unx    11112 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/group_points_by_bounding_boxes.py
+-rw-r--r--  2.0 unx     7069 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/group_points_by_bounding_boxes_test.py
+-rw-r--r--  2.0 unx    12254 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/random_copy_paste.py
+-rw-r--r--  2.0 unx     7952 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/random_copy_paste_test.py
+-rw-r--r--  2.0 unx     5006 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/random_drop_box.py
+-rw-r--r--  2.0 unx    11932 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/random_drop_box_test.py
+-rw-r--r--  2.0 unx     6970 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/swap_background.py
+-rw-r--r--  2.0 unx    10691 b- defN 23-Jul-07 12:41 keras_cv/layers/preprocessing_3d/waymo/swap_background_test.py
+-rw-r--r--  2.0 unx      868 b- defN 23-Jul-07 12:41 keras_cv/layers/regularization/__init__.py
+-rw-r--r--  2.0 unx     2545 b- defN 23-Jul-07 12:41 keras_cv/layers/regularization/drop_path.py
+-rw-r--r--  2.0 unx     2460 b- defN 23-Jul-07 12:41 keras_cv/layers/regularization/drop_path_test.py
+-rw-r--r--  2.0 unx     8831 b- defN 23-Jul-07 12:41 keras_cv/layers/regularization/dropblock_2d.py
+-rw-r--r--  2.0 unx     3653 b- defN 23-Jul-07 12:41 keras_cv/layers/regularization/dropblock_2d_test.py
+-rw-r--r--  2.0 unx     4906 b- defN 23-Jul-07 12:41 keras_cv/layers/regularization/squeeze_excite.py
+-rw-r--r--  2.0 unx     1882 b- defN 23-Jul-07 12:41 keras_cv/layers/regularization/squeeze_excite_test.py
+-rw-r--r--  2.0 unx     2738 b- defN 23-Jul-07 12:41 keras_cv/layers/regularization/stochastic_depth.py
+-rw-r--r--  2.0 unx     1771 b- defN 23-Jul-07 12:41 keras_cv/layers/regularization/stochastic_depth_test.py
+-rw-r--r--  2.0 unx     1036 b- defN 23-Jul-07 12:41 keras_cv/losses/__init__.py
+-rw-r--r--  2.0 unx     4854 b- defN 23-Jul-07 12:41 keras_cv/losses/centernet_box_loss.py
+-rw-r--r--  2.0 unx     1459 b- defN 23-Jul-07 12:41 keras_cv/losses/centernet_box_loss_test.py
+-rw-r--r--  2.0 unx     3548 b- defN 23-Jul-07 12:41 keras_cv/losses/ciou_loss.py
+-rw-r--r--  2.0 unx     3042 b- defN 23-Jul-07 12:41 keras_cv/losses/ciou_loss_test.py
+-rw-r--r--  2.0 unx     4140 b- defN 23-Jul-07 12:41 keras_cv/losses/focal.py
+-rw-r--r--  2.0 unx     2486 b- defN 23-Jul-07 12:41 keras_cv/losses/focal_test.py
+-rw-r--r--  2.0 unx     7410 b- defN 23-Jul-07 12:41 keras_cv/losses/giou_loss.py
+-rw-r--r--  2.0 unx     2541 b- defN 23-Jul-07 12:41 keras_cv/losses/giou_loss_test.py
+-rw-r--r--  2.0 unx     4921 b- defN 23-Jul-07 12:41 keras_cv/losses/iou_loss.py
+-rw-r--r--  2.0 unx     2521 b- defN 23-Jul-07 12:41 keras_cv/losses/iou_loss_test.py
+-rw-r--r--  2.0 unx     4283 b- defN 23-Jul-07 12:41 keras_cv/losses/penalty_reduced_focal_loss.py
+-rw-r--r--  2.0 unx     3744 b- defN 23-Jul-07 12:41 keras_cv/losses/penalty_reduced_focal_loss_test.py
+-rw-r--r--  2.0 unx     2211 b- defN 23-Jul-07 12:41 keras_cv/losses/serialization_test.py
+-rw-r--r--  2.0 unx     3468 b- defN 23-Jul-07 12:41 keras_cv/losses/simclr_loss.py
+-rw-r--r--  2.0 unx     2145 b- defN 23-Jul-07 12:41 keras_cv/losses/simclr_loss_test.py
+-rw-r--r--  2.0 unx     1885 b- defN 23-Jul-07 12:41 keras_cv/losses/smooth_l1.py
+-rw-r--r--  2.0 unx     1225 b- defN 23-Jul-07 12:41 keras_cv/losses/smooth_l1_test.py
+-rw-r--r--  2.0 unx      663 b- defN 23-Jul-07 12:41 keras_cv/metrics/__init__.py
+-rw-r--r--  2.0 unx      719 b- defN 23-Jul-07 12:41 keras_cv/metrics/coco/__init__.py
+-rw-r--r--  2.0 unx     8421 b- defN 23-Jul-07 12:41 keras_cv/metrics/coco/pycoco_wrapper.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/metrics/object_detection/__init__.py
+-rw-r--r--  2.0 unx    10152 b- defN 23-Jul-07 12:41 keras_cv/metrics/object_detection/box_coco_metrics.py
+-rw-r--r--  2.0 unx     9331 b- defN 23-Jul-07 12:41 keras_cv/metrics/object_detection/box_coco_metrics_test.py
+-rw-r--r--  2.0 unx     4472 b- defN 23-Jul-07 12:41 keras_cv/models/__init__.py
+-rw-r--r--  2.0 unx     7295 b- defN 23-Jul-07 12:41 keras_cv/models/task.py
+-rw-r--r--  2.0 unx     1855 b- defN 23-Jul-07 12:41 keras_cv/models/utils.py
+-rw-r--r--  2.0 unx     1133 b- defN 23-Jul-07 12:41 keras_cv/models/utils_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/models/__internal__/__init__.py
+-rw-r--r--  2.0 unx     5966 b- defN 23-Jul-07 12:41 keras_cv/models/__internal__/unet.py
+-rw-r--r--  2.0 unx     1693 b- defN 23-Jul-07 12:41 keras_cv/models/__internal__/unet_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/__init__.py
+-rw-r--r--  2.0 unx     6535 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/backbone.py
+-rw-r--r--  2.0 unx     2236 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/backbone_presets.py
+-rw-r--r--  2.0 unx      821 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/test_backbone_presets.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/csp_darknet/__init__.py
+-rw-r--r--  2.0 unx    12495 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/csp_darknet/csp_darknet_backbone.py
+-rw-r--r--  2.0 unx     6518 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/csp_darknet/csp_darknet_backbone_presets.py
+-rw-r--r--  2.0 unx     4028 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/csp_darknet/csp_darknet_backbone_presets_test.py
+-rw-r--r--  2.0 unx     5644 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/csp_darknet/csp_darknet_backbone_test.py
+-rw-r--r--  2.0 unx    12186 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/csp_darknet/csp_darknet_utils.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/densenet/__init__.py
+-rw-r--r--  2.0 unx     4818 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/densenet/densenet_aliases.py
+-rw-r--r--  2.0 unx     7928 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/densenet/densenet_backbone.py
+-rw-r--r--  2.0 unx     3952 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/densenet/densenet_backbone_presets.py
+-rw-r--r--  2.0 unx     3681 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/densenet/densenet_backbone_presets_test.py
+-rw-r--r--  2.0 unx     4853 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/densenet/densenet_backbone_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/efficientnet_v2/__init__.py
+-rw-r--r--  2.0 unx     9316 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_aliases.py
+-rw-r--r--  2.0 unx    12987 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone.py
+-rw-r--r--  2.0 unx    19514 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_presets.py
+-rw-r--r--  2.0 unx     2299 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_presets_test.py
+-rw-r--r--  2.0 unx     7678 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/mobilenet_v3/__init__.py
+-rw-r--r--  2.0 unx     3931 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_aliases.py
+-rw-r--r--  2.0 unx    12382 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone.py
+-rw-r--r--  2.0 unx     6080 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_presets.py
+-rw-r--r--  2.0 unx     2580 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_presets_test.py
+-rw-r--r--  2.0 unx     3830 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/resnet_v1/__init__.py
+-rw-r--r--  2.0 unx     6506 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/resnet_v1/resnet_v1_aliases.py
+-rw-r--r--  2.0 unx    11614 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/resnet_v1/resnet_v1_backbone.py
+-rw-r--r--  2.0 unx     5551 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_presets.py
+-rw-r--r--  2.0 unx     3550 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_presets_test.py
+-rw-r--r--  2.0 unx     5815 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/resnet_v2/__init__.py
+-rw-r--r--  2.0 unx     6666 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/resnet_v2/resnet_v2_aliases.py
+-rw-r--r--  2.0 unx    13017 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/resnet_v2/resnet_v2_backbone.py
+-rw-r--r--  2.0 unx     5617 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_presets.py
+-rw-r--r--  2.0 unx     3723 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_presets_test.py
+-rw-r--r--  2.0 unx     5226 b- defN 23-Jul-07 12:41 keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/models/classification/__init__.py
+-rw-r--r--  2.0 unx     4645 b- defN 23-Jul-07 12:41 keras_cv/models/classification/image_classifier.py
+-rw-r--r--  2.0 unx     8454 b- defN 23-Jul-07 12:41 keras_cv/models/classification/image_classifier_presets.py
+-rw-r--r--  2.0 unx     7220 b- defN 23-Jul-07 12:41 keras_cv/models/classification/image_classifier_test.py
+-rw-r--r--  2.0 unx     4346 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/__init__.py
+-rw-r--r--  2.0 unx    14440 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/convmixer.py
+-rw-r--r--  2.0 unx     1835 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/convmixer_test.py
+-rw-r--r--  2.0 unx    20264 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/convnext.py
+-rw-r--r--  2.0 unx     2456 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/convnext_test.py
+-rw-r--r--  2.0 unx    11050 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/darknet.py
+-rw-r--r--  2.0 unx     1823 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/darknet_test.py
+-rw-r--r--  2.0 unx    22320 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/efficientnet_lite.py
+-rw-r--r--  2.0 unx     2173 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/efficientnet_lite_test.py
+-rw-r--r--  2.0 unx    29352 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/efficientnet_v1.py
+-rw-r--r--  2.0 unx     2265 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/efficientnet_v1_test.py
+-rw-r--r--  2.0 unx    14405 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/mlp_mixer.py
+-rw-r--r--  2.0 unx     2050 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/mlp_mixer_test.py
+-rw-r--r--  2.0 unx     6602 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/models_test.py
+-rw-r--r--  2.0 unx    45767 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/regnet.py
+-rw-r--r--  2.0 unx     2265 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/regnetx_test.py
+-rw-r--r--  2.0 unx     2265 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/regnety_test.py
+-rw-r--r--  2.0 unx     3662 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/utils.py
+-rw-r--r--  2.0 unx     2320 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/utils_test.py
+-rw-r--r--  2.0 unx     8144 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/vgg16.py
+-rw-r--r--  2.0 unx     1779 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/vgg16_test.py
+-rw-r--r--  2.0 unx     7103 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/vgg19.py
+-rw-r--r--  2.0 unx     1892 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/vgg19_test.py
+-rw-r--r--  2.0 unx    26159 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/vit.py
+-rw-r--r--  2.0 unx     2410 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/vit_test.py
+-rw-r--r--  2.0 unx     8708 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/weights.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/object_detection/__init__.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/object_detection/faster_rcnn/__init__.py
+-rw-r--r--  2.0 unx    23865 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/object_detection/faster_rcnn/faster_rcnn.py
+-rw-r--r--  2.0 unx     3914 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/object_detection/faster_rcnn/faster_rcnn_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/segmentation/__init__.py
+-rw-r--r--  2.0 unx    12412 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/segmentation/deeplab.py
+-rw-r--r--  2.0 unx     5686 b- defN 23-Jul-07 12:41 keras_cv/models/legacy/segmentation/deeplab_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/__init__.py
+-rw-r--r--  2.0 unx     4329 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/__internal__.py
+-rw-r--r--  2.0 unx     1904 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/__test_utils__.py
+-rw-r--r--  2.0 unx     3694 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/predict_utils.py
+-rw-r--r--  2.0 unx      898 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/retinanet/__init__.py
+-rw-r--r--  2.0 unx     2510 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/retinanet/feature_pyramid.py
+-rw-r--r--  2.0 unx     2841 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/retinanet/prediction_head.py
+-rw-r--r--  2.0 unx    24260 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/retinanet/retinanet.py
+-rw-r--r--  2.0 unx     9759 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/retinanet/retinanet_label_encoder.py
+-rw-r--r--  2.0 unx     4594 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/retinanet/retinanet_label_encoder_test.py
+-rw-r--r--  2.0 unx     1672 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/retinanet/retinanet_presets.py
+-rw-r--r--  2.0 unx    11212 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/retinanet/retinanet_test.py
+-rw-r--r--  2.0 unx      687 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolo_v8/__init__.py
+-rw-r--r--  2.0 unx     7027 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolo_v8/yolo_v8_backbone.py
+-rw-r--r--  2.0 unx     6577 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolo_v8/yolo_v8_backbone_presets.py
+-rw-r--r--  2.0 unx    23732 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolo_v8/yolo_v8_detector.py
+-rw-r--r--  2.0 unx     1598 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolo_v8/yolo_v8_detector_presets.py
+-rw-r--r--  2.0 unx     8675 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolo_v8/yolo_v8_detector_test.py
+-rw-r--r--  2.0 unx    16358 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolo_v8/yolo_v8_label_encoder.py
+-rw-r--r--  2.0 unx     2884 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolo_v8/yolo_v8_layers.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolox/__init__.py
+-rw-r--r--  2.0 unx     3419 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolox/binary_crossentropy.py
+-rw-r--r--  2.0 unx      954 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolox/layers/__init__.py
+-rw-r--r--  2.0 unx     6313 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolox/layers/yolox_decoder.py
+-rw-r--r--  2.0 unx     5423 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolox/layers/yolox_head.py
+-rw-r--r--  2.0 unx     1859 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolox/layers/yolox_head_test.py
+-rw-r--r--  2.0 unx     1923 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolox/layers/yolox_label_encoder.py
+-rw-r--r--  2.0 unx     2986 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolox/layers/yolox_label_encoder_test.py
+-rw-r--r--  2.0 unx     5198 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolox/layers/yolox_pafpn.py
+-rw-r--r--  2.0 unx     1806 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection/yolox/layers/yolox_pafpn_test.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection_3d/__init__.py
+-rw-r--r--  2.0 unx    10038 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection_3d/center_pillar.py
+-rw-r--r--  2.0 unx     5577 b- defN 23-Jul-07 12:41 keras_cv/models/object_detection_3d/center_pillar_test.py
+-rw-r--r--  2.0 unx     1324 b- defN 23-Jul-07 12:41 keras_cv/models/stable_diffusion/__init__.py
+-rw-r--r--  2.0 unx     7025 b- defN 23-Jul-07 12:41 keras_cv/models/stable_diffusion/clip_tokenizer.py
+-rw-r--r--  2.0 unx    17410 b- defN 23-Jul-07 12:41 keras_cv/models/stable_diffusion/constants.py
+-rw-r--r--  2.0 unx     2690 b- defN 23-Jul-07 12:41 keras_cv/models/stable_diffusion/decoder.py
+-rw-r--r--  2.0 unx    13271 b- defN 23-Jul-07 12:41 keras_cv/models/stable_diffusion/diffusion_model.py
+-rw-r--r--  2.0 unx     2757 b- defN 23-Jul-07 12:41 keras_cv/models/stable_diffusion/image_encoder.py
+-rw-r--r--  2.0 unx     7771 b- defN 23-Jul-07 12:41 keras_cv/models/stable_diffusion/noise_scheduler.py
+-rw-r--r--  2.0 unx    19440 b- defN 23-Jul-07 12:41 keras_cv/models/stable_diffusion/stable_diffusion.py
+-rw-r--r--  2.0 unx     2414 b- defN 23-Jul-07 12:41 keras_cv/models/stable_diffusion/stable_diffusion_test.py
+-rw-r--r--  2.0 unx     6680 b- defN 23-Jul-07 12:41 keras_cv/models/stable_diffusion/text_encoder.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/models/stable_diffusion/__internal__/__init__.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/models/stable_diffusion/__internal__/layers/__init__.py
+-rw-r--r--  2.0 unx     1948 b- defN 23-Jul-07 12:41 keras_cv/models/stable_diffusion/__internal__/layers/attention_block.py
+-rw-r--r--  2.0 unx     1005 b- defN 23-Jul-07 12:41 keras_cv/models/stable_diffusion/__internal__/layers/padded_conv2d.py
+-rw-r--r--  2.0 unx     1560 b- defN 23-Jul-07 12:41 keras_cv/models/stable_diffusion/__internal__/layers/resnet_block.py
+-rw-r--r--  2.0 unx      624 b- defN 23-Jul-07 12:41 keras_cv/ops/__init__.py
+-rw-r--r--  2.0 unx     1589 b- defN 23-Jul-07 12:41 keras_cv/ops/iou_3d.py
+-rw-r--r--  2.0 unx     2243 b- defN 23-Jul-07 12:41 keras_cv/ops/iou_3d_test.py
+-rw-r--r--  2.0 unx     1522 b- defN 23-Jul-07 12:41 keras_cv/point_cloud/__init__.py
+-rw-r--r--  2.0 unx    18327 b- defN 23-Jul-07 12:41 keras_cv/point_cloud/point_cloud.py
+-rw-r--r--  2.0 unx    14038 b- defN 23-Jul-07 12:41 keras_cv/point_cloud/point_cloud_test.py
+-rw-r--r--  2.0 unx     7774 b- defN 23-Jul-07 12:41 keras_cv/point_cloud/within_box_3d_test.py
+-rw-r--r--  2.0 unx      810 b- defN 23-Jul-07 12:41 keras_cv/training/__init__.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jul-07 12:41 keras_cv/training/contrastive/__init__.py
+-rw-r--r--  2.0 unx     9871 b- defN 23-Jul-07 12:41 keras_cv/training/contrastive/contrastive_trainer.py
+-rw-r--r--  2.0 unx     6332 b- defN 23-Jul-07 12:41 keras_cv/training/contrastive/contrastive_trainer_test.py
+-rw-r--r--  2.0 unx     3199 b- defN 23-Jul-07 12:41 keras_cv/training/contrastive/simclr_trainer.py
+-rw-r--r--  2.0 unx     1779 b- defN 23-Jul-07 12:41 keras_cv/training/contrastive/simclr_trainer_test.py
+-rw-r--r--  2.0 unx     1408 b- defN 23-Jul-07 12:41 keras_cv/utils/__init__.py
+-rw-r--r--  2.0 unx     2080 b- defN 23-Jul-07 12:41 keras_cv/utils/conditional_imports.py
+-rw-r--r--  2.0 unx     2474 b- defN 23-Jul-07 12:41 keras_cv/utils/conv_utils.py
+-rw-r--r--  2.0 unx     3105 b- defN 23-Jul-07 12:41 keras_cv/utils/fill_utils.py
+-rw-r--r--  2.0 unx    11265 b- defN 23-Jul-07 12:41 keras_cv/utils/fill_utils_test.py
+-rw-r--r--  2.0 unx    14465 b- defN 23-Jul-07 12:41 keras_cv/utils/preprocessing.py
+-rw-r--r--  2.0 unx     2303 b- defN 23-Jul-07 12:41 keras_cv/utils/preprocessing_test.py
+-rw-r--r--  2.0 unx     1803 b- defN 23-Jul-07 12:41 keras_cv/utils/python_utils.py
+-rw-r--r--  2.0 unx     2843 b- defN 23-Jul-07 12:41 keras_cv/utils/resource_loader.py
+-rw-r--r--  2.0 unx     4731 b- defN 23-Jul-07 12:41 keras_cv/utils/target_gather.py
+-rw-r--r--  2.0 unx     5346 b- defN 23-Jul-07 12:41 keras_cv/utils/target_gather_test.py
+-rw-r--r--  2.0 unx     3628 b- defN 23-Jul-07 12:41 keras_cv/utils/test_utils.py
+-rw-r--r--  2.0 unx      991 b- defN 23-Jul-07 12:41 keras_cv/utils/to_numpy.py
+-rw-r--r--  2.0 unx     2813 b- defN 23-Jul-07 12:41 keras_cv/utils/train.py
+-rw-r--r--  2.0 unx      860 b- defN 23-Jul-07 12:41 keras_cv/visualization/__init__.py
+-rw-r--r--  2.0 unx     5497 b- defN 23-Jul-07 12:41 keras_cv/visualization/draw_bounding_boxes.py
+-rw-r--r--  2.0 unx     6120 b- defN 23-Jul-07 12:41 keras_cv/visualization/plot_bounding_box_gallery.py
+-rw-r--r--  2.0 unx     5899 b- defN 23-Jul-07 12:41 keras_cv/visualization/plot_image_gallery.py
+-rw-r--r--  2.0 unx     4718 b- defN 23-Jul-07 12:41 keras_cv/visualization/plot_segmentation_mask_gallery.py
+-rw-r--r--  2.0 unx    11853 b- defN 23-Jul-07 12:42 keras_cv-0.5.1.dist-info/LICENSE
+-rw-r--r--  2.0 unx    10791 b- defN 23-Jul-07 12:42 keras_cv-0.5.1.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jul-07 12:42 keras_cv-0.5.1.dist-info/WHEEL
+-rw-r--r--  2.0 unx        9 b- defN 23-Jul-07 12:42 keras_cv-0.5.1.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    43358 b- defN 23-Jul-07 12:42 keras_cv-0.5.1.dist-info/RECORD
+414 files, 2398381 bytes uncompressed, 670946 bytes compressed:  72.0%
```

## zipnote {}

```diff
@@ -351,20 +351,14 @@
 
 Filename: keras_cv/layers/preprocessing/jittered_resize.py
 Comment: 
 
 Filename: keras_cv/layers/preprocessing/jittered_resize_test.py
 Comment: 
 
-Filename: keras_cv/layers/preprocessing/maybe_apply.py
-Comment: 
-
-Filename: keras_cv/layers/preprocessing/maybe_apply_test.py
-Comment: 
-
 Filename: keras_cv/layers/preprocessing/mix_up.py
 Comment: 
 
 Filename: keras_cv/layers/preprocessing/mix_up_test.py
 Comment: 
 
 Filename: keras_cv/layers/preprocessing/mosaic.py
@@ -384,14 +378,20 @@
 
 Filename: keras_cv/layers/preprocessing/rand_augment.py
 Comment: 
 
 Filename: keras_cv/layers/preprocessing/rand_augment_test.py
 Comment: 
 
+Filename: keras_cv/layers/preprocessing/random_apply.py
+Comment: 
+
+Filename: keras_cv/layers/preprocessing/random_apply_test.py
+Comment: 
+
 Filename: keras_cv/layers/preprocessing/random_aspect_ratio.py
 Comment: 
 
 Filename: keras_cv/layers/preprocessing/random_aspect_ratio_test.py
 Comment: 
 
 Filename: keras_cv/layers/preprocessing/random_augmentation_pipeline.py
@@ -510,20 +510,14 @@
 
 Filename: keras_cv/layers/preprocessing/random_zoom.py
 Comment: 
 
 Filename: keras_cv/layers/preprocessing/random_zoom_test.py
 Comment: 
 
-Filename: keras_cv/layers/preprocessing/randomly_zoomed_crop.py
-Comment: 
-
-Filename: keras_cv/layers/preprocessing/randomly_zoomed_crop_test.py
-Comment: 
-
 Filename: keras_cv/layers/preprocessing/repeated_augmentation.py
 Comment: 
 
 Filename: keras_cv/layers/preprocessing/repeated_augmentation_test.py
 Comment: 
 
 Filename: keras_cv/layers/preprocessing/rescaling.py
@@ -564,78 +558,84 @@
 
 Filename: keras_cv/layers/preprocessing_3d/base_augmentation_layer_3d.py
 Comment: 
 
 Filename: keras_cv/layers/preprocessing_3d/base_augmentation_layer_3d_test.py
 Comment: 
 
-Filename: keras_cv/layers/preprocessing_3d/frustum_random_dropping_points.py
+Filename: keras_cv/layers/preprocessing_3d/input_format_test.py
+Comment: 
+
+Filename: keras_cv/layers/preprocessing_3d/waymo/__init__.py
 Comment: 
 
-Filename: keras_cv/layers/preprocessing_3d/frustum_random_dropping_points_test.py
+Filename: keras_cv/layers/preprocessing_3d/waymo/frustum_random_dropping_points.py
 Comment: 
 
-Filename: keras_cv/layers/preprocessing_3d/frustum_random_point_feature_noise.py
+Filename: keras_cv/layers/preprocessing_3d/waymo/frustum_random_dropping_points_test.py
 Comment: 
 
-Filename: keras_cv/layers/preprocessing_3d/frustum_random_point_feature_noise_test.py
+Filename: keras_cv/layers/preprocessing_3d/waymo/frustum_random_point_feature_noise.py
 Comment: 
 
-Filename: keras_cv/layers/preprocessing_3d/global_random_dropping_points.py
+Filename: keras_cv/layers/preprocessing_3d/waymo/frustum_random_point_feature_noise_test.py
 Comment: 
 
-Filename: keras_cv/layers/preprocessing_3d/global_random_dropping_points_test.py
+Filename: keras_cv/layers/preprocessing_3d/waymo/global_random_dropping_points.py
 Comment: 
 
-Filename: keras_cv/layers/preprocessing_3d/global_random_flip.py
+Filename: keras_cv/layers/preprocessing_3d/waymo/global_random_dropping_points_test.py
 Comment: 
 
-Filename: keras_cv/layers/preprocessing_3d/global_random_flip_test.py
+Filename: keras_cv/layers/preprocessing_3d/waymo/global_random_flip.py
 Comment: 
 
-Filename: keras_cv/layers/preprocessing_3d/global_random_rotation.py
+Filename: keras_cv/layers/preprocessing_3d/waymo/global_random_flip_test.py
 Comment: 
 
-Filename: keras_cv/layers/preprocessing_3d/global_random_rotation_test.py
+Filename: keras_cv/layers/preprocessing_3d/waymo/global_random_rotation.py
 Comment: 
 
-Filename: keras_cv/layers/preprocessing_3d/global_random_scaling.py
+Filename: keras_cv/layers/preprocessing_3d/waymo/global_random_rotation_test.py
 Comment: 
 
-Filename: keras_cv/layers/preprocessing_3d/global_random_scaling_test.py
+Filename: keras_cv/layers/preprocessing_3d/waymo/global_random_scaling.py
 Comment: 
 
-Filename: keras_cv/layers/preprocessing_3d/global_random_translation.py
+Filename: keras_cv/layers/preprocessing_3d/waymo/global_random_scaling_test.py
 Comment: 
 
-Filename: keras_cv/layers/preprocessing_3d/global_random_translation_test.py
+Filename: keras_cv/layers/preprocessing_3d/waymo/global_random_translation.py
 Comment: 
 
-Filename: keras_cv/layers/preprocessing_3d/group_points_by_bounding_boxes.py
+Filename: keras_cv/layers/preprocessing_3d/waymo/global_random_translation_test.py
 Comment: 
 
-Filename: keras_cv/layers/preprocessing_3d/group_points_by_bounding_boxes_test.py
+Filename: keras_cv/layers/preprocessing_3d/waymo/group_points_by_bounding_boxes.py
 Comment: 
 
-Filename: keras_cv/layers/preprocessing_3d/random_copy_paste.py
+Filename: keras_cv/layers/preprocessing_3d/waymo/group_points_by_bounding_boxes_test.py
 Comment: 
 
-Filename: keras_cv/layers/preprocessing_3d/random_copy_paste_test.py
+Filename: keras_cv/layers/preprocessing_3d/waymo/random_copy_paste.py
 Comment: 
 
-Filename: keras_cv/layers/preprocessing_3d/random_drop_box.py
+Filename: keras_cv/layers/preprocessing_3d/waymo/random_copy_paste_test.py
 Comment: 
 
-Filename: keras_cv/layers/preprocessing_3d/random_drop_box_test.py
+Filename: keras_cv/layers/preprocessing_3d/waymo/random_drop_box.py
 Comment: 
 
-Filename: keras_cv/layers/preprocessing_3d/swap_background.py
+Filename: keras_cv/layers/preprocessing_3d/waymo/random_drop_box_test.py
 Comment: 
 
-Filename: keras_cv/layers/preprocessing_3d/swap_background_test.py
+Filename: keras_cv/layers/preprocessing_3d/waymo/swap_background.py
+Comment: 
+
+Filename: keras_cv/layers/preprocessing_3d/waymo/swap_background_test.py
 Comment: 
 
 Filename: keras_cv/layers/regularization/__init__.py
 Comment: 
 
 Filename: keras_cv/layers/regularization/drop_path.py
 Comment: 
@@ -666,14 +666,20 @@
 
 Filename: keras_cv/losses/centernet_box_loss.py
 Comment: 
 
 Filename: keras_cv/losses/centernet_box_loss_test.py
 Comment: 
 
+Filename: keras_cv/losses/ciou_loss.py
+Comment: 
+
+Filename: keras_cv/losses/ciou_loss_test.py
+Comment: 
+
 Filename: keras_cv/losses/focal.py
 Comment: 
 
 Filename: keras_cv/losses/focal_test.py
 Comment: 
 
 Filename: keras_cv/losses/giou_loss.py
@@ -753,14 +759,17 @@
 
 Filename: keras_cv/models/backbones/backbone.py
 Comment: 
 
 Filename: keras_cv/models/backbones/backbone_presets.py
 Comment: 
 
+Filename: keras_cv/models/backbones/test_backbone_presets.py
+Comment: 
+
 Filename: keras_cv/models/backbones/csp_darknet/__init__.py
 Comment: 
 
 Filename: keras_cv/models/backbones/csp_darknet/csp_darknet_backbone.py
 Comment: 
 
 Filename: keras_cv/models/backbones/csp_darknet/csp_darknet_backbone_presets.py
@@ -771,14 +780,32 @@
 
 Filename: keras_cv/models/backbones/csp_darknet/csp_darknet_backbone_test.py
 Comment: 
 
 Filename: keras_cv/models/backbones/csp_darknet/csp_darknet_utils.py
 Comment: 
 
+Filename: keras_cv/models/backbones/densenet/__init__.py
+Comment: 
+
+Filename: keras_cv/models/backbones/densenet/densenet_aliases.py
+Comment: 
+
+Filename: keras_cv/models/backbones/densenet/densenet_backbone.py
+Comment: 
+
+Filename: keras_cv/models/backbones/densenet/densenet_backbone_presets.py
+Comment: 
+
+Filename: keras_cv/models/backbones/densenet/densenet_backbone_presets_test.py
+Comment: 
+
+Filename: keras_cv/models/backbones/densenet/densenet_backbone_test.py
+Comment: 
+
 Filename: keras_cv/models/backbones/efficientnet_v2/__init__.py
 Comment: 
 
 Filename: keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_aliases.py
 Comment: 
 
 Filename: keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone.py
@@ -792,14 +819,17 @@
 
 Filename: keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_test.py
 Comment: 
 
 Filename: keras_cv/models/backbones/mobilenet_v3/__init__.py
 Comment: 
 
+Filename: keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_aliases.py
+Comment: 
+
 Filename: keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone.py
 Comment: 
 
 Filename: keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_presets.py
 Comment: 
 
 Filename: keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_presets_test.py
@@ -807,14 +837,17 @@
 
 Filename: keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_test.py
 Comment: 
 
 Filename: keras_cv/models/backbones/resnet_v1/__init__.py
 Comment: 
 
+Filename: keras_cv/models/backbones/resnet_v1/resnet_v1_aliases.py
+Comment: 
+
 Filename: keras_cv/models/backbones/resnet_v1/resnet_v1_backbone.py
 Comment: 
 
 Filename: keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_presets.py
 Comment: 
 
 Filename: keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_presets_test.py
@@ -822,14 +855,17 @@
 
 Filename: keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_test.py
 Comment: 
 
 Filename: keras_cv/models/backbones/resnet_v2/__init__.py
 Comment: 
 
+Filename: keras_cv/models/backbones/resnet_v2/resnet_v2_aliases.py
+Comment: 
+
 Filename: keras_cv/models/backbones/resnet_v2/resnet_v2_backbone.py
 Comment: 
 
 Filename: keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_presets.py
 Comment: 
 
 Filename: keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_presets_test.py
@@ -867,20 +903,14 @@
 
 Filename: keras_cv/models/legacy/darknet.py
 Comment: 
 
 Filename: keras_cv/models/legacy/darknet_test.py
 Comment: 
 
-Filename: keras_cv/models/legacy/densenet.py
-Comment: 
-
-Filename: keras_cv/models/legacy/densenet_test.py
-Comment: 
-
 Filename: keras_cv/models/legacy/efficientnet_lite.py
 Comment: 
 
 Filename: keras_cv/models/legacy/efficientnet_lite_test.py
 Comment: 
 
 Filename: keras_cv/models/legacy/efficientnet_v1.py
@@ -930,14 +960,26 @@
 
 Filename: keras_cv/models/legacy/vit_test.py
 Comment: 
 
 Filename: keras_cv/models/legacy/weights.py
 Comment: 
 
+Filename: keras_cv/models/legacy/object_detection/__init__.py
+Comment: 
+
+Filename: keras_cv/models/legacy/object_detection/faster_rcnn/__init__.py
+Comment: 
+
+Filename: keras_cv/models/legacy/object_detection/faster_rcnn/faster_rcnn.py
+Comment: 
+
+Filename: keras_cv/models/legacy/object_detection/faster_rcnn/faster_rcnn_test.py
+Comment: 
+
 Filename: keras_cv/models/legacy/segmentation/__init__.py
 Comment: 
 
 Filename: keras_cv/models/legacy/segmentation/deeplab.py
 Comment: 
 
 Filename: keras_cv/models/legacy/segmentation/deeplab_test.py
@@ -993,17 +1035,14 @@
 
 Filename: keras_cv/models/object_detection/yolo_v8/yolo_v8_detector_presets.py
 Comment: 
 
 Filename: keras_cv/models/object_detection/yolo_v8/yolo_v8_detector_test.py
 Comment: 
 
-Filename: keras_cv/models/object_detection/yolo_v8/yolo_v8_iou_loss.py
-Comment: 
-
 Filename: keras_cv/models/object_detection/yolo_v8/yolo_v8_label_encoder.py
 Comment: 
 
 Filename: keras_cv/models/object_detection/yolo_v8/yolo_v8_layers.py
 Comment: 
 
 Filename: keras_cv/models/object_detection/yolox/__init__.py
@@ -1179,23 +1218,26 @@
 
 Filename: keras_cv/visualization/plot_bounding_box_gallery.py
 Comment: 
 
 Filename: keras_cv/visualization/plot_image_gallery.py
 Comment: 
 
-Filename: keras_cv-0.5.0.dist-info/LICENSE
+Filename: keras_cv/visualization/plot_segmentation_mask_gallery.py
+Comment: 
+
+Filename: keras_cv-0.5.1.dist-info/LICENSE
 Comment: 
 
-Filename: keras_cv-0.5.0.dist-info/METADATA
+Filename: keras_cv-0.5.1.dist-info/METADATA
 Comment: 
 
-Filename: keras_cv-0.5.0.dist-info/WHEEL
+Filename: keras_cv-0.5.1.dist-info/WHEEL
 Comment: 
 
-Filename: keras_cv-0.5.0.dist-info/top_level.txt
+Filename: keras_cv-0.5.1.dist-info/top_level.txt
 Comment: 
 
-Filename: keras_cv-0.5.0.dist-info/RECORD
+Filename: keras_cv-0.5.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## keras_cv/__init__.py

```diff
@@ -29,8 +29,8 @@
 from keras_cv import utils
 from keras_cv import visualization
 from keras_cv.core import ConstantFactorSampler
 from keras_cv.core import FactorSampler
 from keras_cv.core import NormalFactorSampler
 from keras_cv.core import UniformFactorSampler
 
-__version__ = "0.5.0"
+__version__ = "0.5.1"
```

## keras_cv/conftest.py

```diff
@@ -57,11 +57,13 @@
     )
     skip_extra_large = pytest.mark.skipif(
         not run_extra_large_tests, reason="need --run_extra_large option to run"
     )
     for item in items:
         if "keras_format" in item.name:
             item.add_marker(skip_keras_saving_test)
+        if "tf_format" in item.name:
+            item.add_marker(skip_extra_large)
         if "large" in item.keywords:
             item.add_marker(skip_large)
         if "extra_large" in item.keywords:
             item.add_marker(skip_extra_large)
```

## keras_cv/bounding_box/__init__.py

```diff
@@ -18,14 +18,15 @@
 from keras_cv.bounding_box.ensure_tensor import ensure_tensor
 from keras_cv.bounding_box.formats import CENTER_XYWH
 from keras_cv.bounding_box.formats import REL_XYXY
 from keras_cv.bounding_box.formats import REL_YXYX
 from keras_cv.bounding_box.formats import XYWH
 from keras_cv.bounding_box.formats import XYXY
 from keras_cv.bounding_box.formats import YXYX
+from keras_cv.bounding_box.iou import compute_ciou
 from keras_cv.bounding_box.iou import compute_iou
 from keras_cv.bounding_box.mask_invalid_detections import (
     mask_invalid_detections,
 )
 from keras_cv.bounding_box.to_dense import to_dense
 from keras_cv.bounding_box.to_ragged import to_ragged
 from keras_cv.bounding_box.utils import as_relative
```

## keras_cv/bounding_box/iou.py

```diff
@@ -8,14 +8,16 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Contains functions to compute ious of bounding boxes."""
+import math
+
 import tensorflow as tf
 
 from keras_cv import bounding_box
 
 
 def _compute_area(box):
     """Computes area for bounding boxes
@@ -168,7 +170,79 @@
     boxes1_mask = tf.less(tf.reduce_max(boxes1, axis=-1, keepdims=True), 0.0)
     boxes2_mask = tf.less(tf.reduce_max(boxes2, axis=-1, keepdims=True), 0.0)
     background_mask = tf.logical_or(
         boxes1_mask, tf.transpose(boxes2_mask, perm)
     )
     iou_lookup_table = tf.where(background_mask, mask_val_t, res)
     return iou_lookup_table
+
+
+def compute_ciou(box1, box2, bounding_box_format, eps=1e-7):
+    """
+    Computes the Complete IoU (CIoU) between two bounding boxes or between
+    two batches of bounding boxes.
+
+    CIoU loss is an extension of GIoU loss, which further improves the IoU
+    optimization for object detection. CIoU loss not only penalizes the
+    bounding box coordinates but also considers the aspect ratio and center
+    distance of the boxes. The length of the last dimension should be 4 to
+    represent the bounding boxes.
+
+    Args:
+        box1 (tf.Tensor): Tensor representing the first bounding box with
+            shape (..., 4).
+        box2 (tf.Tensor): Tensor representing the second bounding box with
+            shape (..., 4).
+        bounding_box_format: a case-insensitive string (for example, "xyxy").
+            Each bounding box is defined by these 4 values. For detailed
+            information on the supported formats, see the [KerasCV bounding box
+            documentation](https://keras.io/api/keras_cv/bounding_box/formats/).
+        eps (float, optional): A small value to avoid division by zero. Default
+            is 1e-7.
+
+    Returns:
+        tf.Tensor: The CIoU distance between the two bounding boxes.
+    """
+    target_format = "xyxy"
+    if bounding_box.is_relative(bounding_box_format):
+        target_format = bounding_box.as_relative(target_format)
+
+    box1 = bounding_box.convert_format(
+        box1, source=bounding_box_format, target=target_format
+    )
+
+    box2 = bounding_box.convert_format(
+        box2, source=bounding_box_format, target=target_format
+    )
+    b1_x1, b1_y1, b1_x2, b1_y2 = tf.split(box1, 4, axis=-1)
+    b2_x1, b2_y1, b2_x2, b2_y2 = tf.split(box2, 4, axis=-1)
+    w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + eps
+    w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + eps
+
+    # Intersection area
+    inter = tf.math.maximum(
+        tf.math.minimum(b1_x2, b2_x2) - tf.math.maximum(b1_x1, b2_x1), 0
+    ) * tf.math.maximum(
+        tf.math.minimum(b1_y2, b2_y2) - tf.math.maximum(b1_y1, b2_y1), 0
+    )
+
+    # Union Area
+    union = w1 * h1 + w2 * h2 - inter + eps
+
+    # IoU
+    iou = inter / union
+
+    cw = tf.math.maximum(b1_x2, b2_x2) - tf.math.minimum(
+        b1_x1, b2_x1
+    )  # convex (smallest enclosing box) width
+    ch = tf.math.maximum(b1_y2, b2_y2) - tf.math.minimum(
+        b1_y1, b2_y1
+    )  # convex height
+    c2 = cw**2 + ch**2 + eps  # convex diagonal squared
+    rho2 = (
+        (b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2
+        + (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2
+    ) / 4  # center dist ** 2
+    v = tf.pow((4 / math.pi**2) * (tf.atan(w2 / h2) - tf.atan(w1 / h1)), 2)
+    alpha = v / (v - iou + (1 + eps))
+
+    return iou - (rho2 / c2 + v * alpha)
```

## keras_cv/callbacks/pycoco_callback_test.py

```diff
@@ -27,14 +27,15 @@
 class PyCOCOCallbackTest(tf.test.TestCase):
     @pytest.fixture(autouse=True)
     def cleanup_global_session(self):
         # Code before yield runs before the test
         yield
         keras.backend.clear_session()
 
+    @pytest.mark.large  # Fit is slow, so mark these large.
     def test_model_fit_retinanet(self):
         model = keras_cv.models.RetinaNet(
             num_classes=10,
             bounding_box_format="xywh",
             backbone=keras_cv.models.ResNet50V2Backbone(),
         )
         # all metric formats must match
```

## keras_cv/callbacks/waymo_evaluation_callback.py

```diff
@@ -79,15 +79,15 @@
 
     def on_epoch_end(self, epoch, logs=None):
         logs = logs or {}
 
         gt, preds = self._eval_dataset(self.val_data)
         self.evaluator.update_state(gt, preds)
 
-        metrics = self.evaluator.evaluate()
+        metrics = self.evaluator.result()
 
         metrics_dict = {
             "average_precision_vehicle_l1": metrics.average_precision[0],
             "average_precision_vehicle_l2": metrics.average_precision[1],
             "average_precision_ped_l1": metrics.average_precision[2],
             "average_precision_ped_l2": metrics.average_precision[3],
         }
```

## keras_cv/layers/__init__.py

```diff
@@ -36,19 +36,19 @@
 from keras_cv.layers.preprocessing.channel_shuffle import ChannelShuffle
 from keras_cv.layers.preprocessing.cut_mix import CutMix
 from keras_cv.layers.preprocessing.equalization import Equalization
 from keras_cv.layers.preprocessing.fourier_mix import FourierMix
 from keras_cv.layers.preprocessing.grayscale import Grayscale
 from keras_cv.layers.preprocessing.grid_mask import GridMask
 from keras_cv.layers.preprocessing.jittered_resize import JitteredResize
-from keras_cv.layers.preprocessing.maybe_apply import MaybeApply
 from keras_cv.layers.preprocessing.mix_up import MixUp
 from keras_cv.layers.preprocessing.mosaic import Mosaic
 from keras_cv.layers.preprocessing.posterization import Posterization
 from keras_cv.layers.preprocessing.rand_augment import RandAugment
+from keras_cv.layers.preprocessing.random_apply import RandomApply
 from keras_cv.layers.preprocessing.random_aspect_ratio import RandomAspectRatio
 from keras_cv.layers.preprocessing.random_augmentation_pipeline import (
     RandomAugmentationPipeline,
 )
 from keras_cv.layers.preprocessing.random_brightness import RandomBrightness
 from keras_cv.layers.preprocessing.random_channel_shift import (
     RandomChannelShift,
@@ -72,51 +72,54 @@
 from keras_cv.layers.preprocessing.random_jpeg_quality import RandomJpegQuality
 from keras_cv.layers.preprocessing.random_rotation import RandomRotation
 from keras_cv.layers.preprocessing.random_saturation import RandomSaturation
 from keras_cv.layers.preprocessing.random_sharpness import RandomSharpness
 from keras_cv.layers.preprocessing.random_shear import RandomShear
 from keras_cv.layers.preprocessing.random_translation import RandomTranslation
 from keras_cv.layers.preprocessing.random_zoom import RandomZoom
-from keras_cv.layers.preprocessing.randomly_zoomed_crop import (
-    RandomlyZoomedCrop,
-)
 from keras_cv.layers.preprocessing.repeated_augmentation import (
     RepeatedAugmentation,
 )
 from keras_cv.layers.preprocessing.rescaling import Rescaling
 from keras_cv.layers.preprocessing.resizing import Resizing
 from keras_cv.layers.preprocessing.solarization import Solarization
 from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
     VectorizedBaseImageAugmentationLayer,
 )
-from keras_cv.layers.preprocessing_3d.frustum_random_dropping_points import (
+from keras_cv.layers.preprocessing_3d.waymo.frustum_random_dropping_points import (  # noqa: E501
     FrustumRandomDroppingPoints,
 )
-from keras_cv.layers.preprocessing_3d.frustum_random_point_feature_noise import (  # noqa: E501
+from keras_cv.layers.preprocessing_3d.waymo.frustum_random_point_feature_noise import (  # noqa: E501
     FrustumRandomPointFeatureNoise,
 )
-from keras_cv.layers.preprocessing_3d.global_random_dropping_points import (
+from keras_cv.layers.preprocessing_3d.waymo.global_random_dropping_points import (  # noqa: E501
     GlobalRandomDroppingPoints,
 )
-from keras_cv.layers.preprocessing_3d.global_random_flip import GlobalRandomFlip
-from keras_cv.layers.preprocessing_3d.global_random_rotation import (
+from keras_cv.layers.preprocessing_3d.waymo.global_random_flip import (
+    GlobalRandomFlip,
+)
+from keras_cv.layers.preprocessing_3d.waymo.global_random_rotation import (
     GlobalRandomRotation,
 )
-from keras_cv.layers.preprocessing_3d.global_random_scaling import (
+from keras_cv.layers.preprocessing_3d.waymo.global_random_scaling import (
     GlobalRandomScaling,
 )
-from keras_cv.layers.preprocessing_3d.global_random_translation import (
+from keras_cv.layers.preprocessing_3d.waymo.global_random_translation import (
     GlobalRandomTranslation,
 )
-from keras_cv.layers.preprocessing_3d.group_points_by_bounding_boxes import (
+from keras_cv.layers.preprocessing_3d.waymo.group_points_by_bounding_boxes import (  # noqa: E501
     GroupPointsByBoundingBoxes,
 )
-from keras_cv.layers.preprocessing_3d.random_copy_paste import RandomCopyPaste
-from keras_cv.layers.preprocessing_3d.random_drop_box import RandomDropBox
-from keras_cv.layers.preprocessing_3d.swap_background import SwapBackground
+from keras_cv.layers.preprocessing_3d.waymo.random_copy_paste import (
+    RandomCopyPaste,
+)
+from keras_cv.layers.preprocessing_3d.waymo.random_drop_box import RandomDropBox
+from keras_cv.layers.preprocessing_3d.waymo.swap_background import (
+    SwapBackground,
+)
 from keras_cv.layers.regularization.drop_path import DropPath
 from keras_cv.layers.regularization.dropblock_2d import DropBlock2D
 from keras_cv.layers.regularization.squeeze_excite import SqueezeAndExcite2D
 from keras_cv.layers.regularization.stochastic_depth import StochasticDepth
 from keras_cv.layers.spatial_pyramid import SpatialPyramidPooling
 from keras_cv.layers.transformer_encoder import TransformerEncoder
 from keras_cv.layers.vit_layers import PatchingAndEmbedding
```

## keras_cv/layers/serialization_test.py

```diff
@@ -150,24 +150,14 @@
             {
                 "target_size": (224, 224),
                 "crop_area_factor": (0.8, 1.0),
                 "aspect_ratio_factor": (3 / 4, 4 / 3),
             },
         ),
         (
-            "RandomlyZoomedCrop",
-            cv_layers.RandomlyZoomedCrop,
-            {
-                "height": 224,
-                "width": 224,
-                "zoom_factor": (0.8, 1.0),
-                "aspect_ratio_factor": (3 / 4, 4 / 3),
-            },
-        ),
-        (
             "DropBlock2D",
             cv_layers.DropBlock2D,
             {"rate": 0.1, "block_size": (7, 7), "seed": 1234},
         ),
         (
             "StochasticDepth",
             cv_layers.StochasticDepth,
@@ -187,16 +177,16 @@
             "DropPath",
             cv_layers.DropPath,
             {
                 "rate": 0.2,
             },
         ),
         (
-            "MaybeApply",
-            cv_layers.MaybeApply,
+            "RandomApply",
+            cv_layers.RandomApply,
             {
                 "rate": 0.5,
                 "layer": None,
                 "seed": 1234,
             },
         ),
         (
```

## keras_cv/layers/object_detection/anchor_generator.py

```diff
@@ -258,19 +258,23 @@
         half_anchor_heights = tf.reshape(0.5 * anchor_heights, [1, 1, -1])
         half_anchor_widths = tf.reshape(0.5 * anchor_widths, [1, 1, -1])
 
         stride = tf.cast(self.stride, tf.float32)
         # make sure range of `cx` is within limit of `image_width` with
         # `stride`, also for sizes where `image_width % stride != 0`.
         # [W]
-        cx = tf.range(0.5 * stride, (image_width // stride) * stride, stride)
+        cx = tf.range(
+            0.5 * stride, tf.math.ceil(image_width / stride) * stride, stride
+        )
         # make sure range of `cy` is within limit of `image_height` with
         # `stride`, also for sizes where `image_height % stride != 0`.
         # [H]
-        cy = tf.range(0.5 * stride, (image_height // stride) * stride, stride)
+        cy = tf.range(
+            0.5 * stride, tf.math.ceil(image_height / stride) * stride, stride
+        )
         # [H, W]
         cx_grid, cy_grid = tf.meshgrid(cx, cy)
         # [H, W, 1]
         cx_grid = tf.expand_dims(cx_grid, axis=-1)
         cy_grid = tf.expand_dims(cy_grid, axis=-1)
 
         y_min = tf.reshape(cy_grid - half_anchor_heights, (-1,))
```

## keras_cv/layers/object_detection/anchor_generator_test.py

```diff
@@ -67,19 +67,20 @@
             aspect_ratios=aspect_ratios,
             scales=scales,
             strides=strides,
         )
         boxes = anchor_generator(image=image)
         boxes = tf.concat(list(boxes.values()), axis=0)
 
-        expected_box_shapes = (
-            (image_shape[0] // tf.constant(strides))
-            * (image_shape[1] // tf.constant(strides))
+        expected_box_shapes = tf.cast(
+            tf.math.ceil(image_shape[0] / tf.constant(strides))
+            * tf.math.ceil(image_shape[1] / tf.constant(strides))
             * len(scales)
-            * len(aspect_ratios)
+            * len(aspect_ratios),
+            tf.int32,
         )
 
         sum_expected_shape = (expected_box_shapes.numpy().sum(), 4)
         self.assertEqual(boxes.shape, sum_expected_shape)
 
     @parameterized.parameters(
         ((640, 480, 3),),
@@ -98,19 +99,20 @@
             aspect_ratios=aspect_ratios,
             scales=scales,
             strides=strides,
         )
         boxes = anchor_generator(image_shape=image_shape)
         boxes = tf.concat(list(boxes.values()), axis=0)
 
-        expected_box_shapes = (
-            (image_shape[0] // tf.constant(strides))
-            * (image_shape[1] // tf.constant(strides))
+        expected_box_shapes = tf.cast(
+            tf.math.ceil(image_shape[0] / tf.constant(strides))
+            * tf.math.ceil(image_shape[1] / tf.constant(strides))
             * len(scales)
-            * len(aspect_ratios)
+            * len(aspect_ratios),
+            tf.int32,
         )
 
         sum_expected_shape = (expected_box_shapes.numpy().sum(), 4)
         self.assertEqual(boxes.shape, sum_expected_shape)
 
     def test_hand_crafted_aspect_ratios(self):
         strides = [4]
```

## keras_cv/layers/object_detection/roi_align.py

```diff
@@ -174,32 +174,38 @@
 
     Generate the (output_size, output_size) set of pixels for each input box
     by first locating the box into the correct feature level, and then cropping
     and resizing it using the corresponding feature map of that level.
 
     Args:
       features: A dictionary with key as pyramid level and value as features.
+        The pyramid level keys need to be represented by strings like so:
+        "P2", "P3", "P4", and so on.
         The features are in shape of [batch_size, height_l, width_l,
         num_filters].
       boxes: A 3-D Tensor of shape [batch_size, num_boxes, 4]. Each row
         represents a box with [y1, x1, y2, x2] in un-normalized coordinates.
       output_size: A scalar to indicate the output crop size.
       sample_offset: a float number in [0, 1] indicates the subpixel sample
         offset from grid point.
 
     Returns:
       A 5-D tensor representing feature crop of shape
       [batch_size, num_boxes, output_size, output_size, num_filters].
     """
 
     with tf.name_scope("multilevel_crop_and_resize"):
-        levels = list(features.keys())
-        min_level = int(min(levels))
-        max_level = int(max(levels))
-        features_shape = tf.shape(features[min_level])
+        levels_str = list(features.keys())
+        # Levels are represented by strings with a prefix "P" to represent
+        # pyramid levels. The integer level can be obtained by looking at
+        # the value that follows the "P".
+        levels = [int(level_str[1:]) for level_str in levels_str]
+        min_level = min(levels)
+        max_level = max(levels)
+        features_shape = tf.shape(features[f"P{min_level}"])
         batch_size, max_feature_height, max_feature_width, num_filters = (
             features_shape[0],
             features_shape[1],
             features_shape[2],
             features_shape[3],
         )
 
@@ -207,21 +213,21 @@
 
         # Stack feature pyramid into a features_all of shape
         # [batch_size, levels, height, width, num_filters].
         features_all = []
         feature_heights = []
         feature_widths = []
         for level in range(min_level, max_level + 1):
-            shape = features[level].get_shape().as_list()
+            shape = features[f"P{level}"].get_shape().as_list()
             feature_heights.append(shape[1])
             feature_widths.append(shape[2])
             # Concat tensor of [batch_size, height_l * width_l, num_filters] for
             # each level.
             features_all.append(
-                tf.reshape(features[level], [batch_size, -1, num_filters])
+                tf.reshape(features[f"P{level}"], [batch_size, -1, num_filters])
             )
         features_r2 = tf.reshape(tf.concat(features_all, 1), [-1, num_filters])
 
         # Calculate height_l * width_l for each level.
         level_dim_sizes = [
             feature_widths[i] * feature_heights[i]
             for i in range(len(feature_widths))
@@ -373,15 +379,15 @@
     """Performs ROIAlign for the second stage processing."""
 
     def __init__(
         self,
         bounding_box_format,
         target_size=7,
         sample_offset: float = 0.5,
-        **kwargs
+        **kwargs,
     ):
         """
         Generates ROI Aligner.
 
         Args:
           bounding_box_format: the input format for boxes.
           crop_size: An `int` of the output size of the cropped features.
```

## keras_cv/layers/object_detection/roi_sampler_test.py

```diff
@@ -43,15 +43,15 @@
         )
         gt_boxes = gt_boxes[tf.newaxis, ...]
         gt_classes = tf.constant([[2, 10, -1]], dtype=tf.int32)
         gt_classes = gt_classes[..., tf.newaxis]
         _, sampled_gt_boxes, _, sampled_gt_classes, _ = roi_sampler(
             rois, gt_boxes, gt_classes
         )
-        # given we only choose 1 positive sample, and `append_labesl` is False,
+        # given we only choose 1 positive sample, and `append_label` is False,
         # only the 2nd ROI is chosen.
         expected_gt_boxes = tf.constant(
             [[0.0, 0.0, 0, 0.0], [0.0, 0.0, 0, 0.0]]
         )
         expected_gt_boxes = expected_gt_boxes[tf.newaxis, ...]
         # only the 2nd ROI is chosen, and the negative ROI is mapped to 0.
         expected_gt_classes = tf.constant([[10], [0]], dtype=tf.int32)
```

## keras_cv/layers/object_detection_3d/centernet_label_encoder.py

```diff
@@ -53,24 +53,22 @@
     return mesh * voxel_size
 
 
 def compute_heatmap(
     box_3d: tf.Tensor,
     box_mask: tf.Tensor,
     voxel_size: Sequence[float],
-    min_radius: Sequence[float],
     max_radius: Sequence[float],
 ) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor]:
     """Compute heatmap for boxes.
 
     Args:
       box_3d: 3d boxes in xyz format, vehicle frame, [B, boxes, 7].
       box_mask: box masking, [B, boxes]
       voxel_size: the size on each voxel dimension (xyz)
-      min_radius: the minimum radius on each voxel dimension (xyz)
       max_radius: the maximum radius on each voxel dimension (xyz)
 
     Returns:
       point_xyz: the point location w.r.t. vehicle frame, [B, boxes,
         max_voxels_per_box, 3]
       mask: point mask, [B, boxes, max_voxels_per_box]
       heatmap: the returned heatmap w.r.t box frame, [B, boxes,
@@ -330,34 +328,31 @@
     This layer takes the box locations, box classes and box masks, voxelizes
     and compute the Gaussian radius for each box, then computes class specific
     heatmap for classification and class specific box offset w.r.t to feature
     map for regression.
 
     Args:
       voxel_size: the x, y, z dimension (in meters) of each voxel.
-      min_radius: minimum Gaussian radius in each dimension in meters.
       max_radius: maximum Gaussian radius in each dimension in meters.
       spatial_size: the x, y, z boundary of voxels
       num_classes: number of object classes.
       top_k_heatmap: A sequence of integers, top k for each class. Can be None.
     """
 
     def __init__(
         self,
         voxel_size: Sequence[float],
-        min_radius: Sequence[float],
         max_radius: Sequence[float],
         spatial_size: Sequence[float],
         num_classes: int,
         top_k_heatmap: Sequence[int],
         **kwargs,
     ):
         super().__init__(**kwargs)
         self._voxel_size = voxel_size
-        self._min_radius = min_radius
         self._max_radius = max_radius
         self._spatial_size = spatial_size
         self._num_classes = num_classes
         self._top_k_heatmap = top_k_heatmap
 
     def call(self, inputs):
         """
@@ -386,15 +381,14 @@
         # point_xyz - [B, num_boxes * max_num_voxels_per_box, 3]
         # heatmap - [B, num_boxes * max_num_voxels_per_box]
         # compute localized heatmap around its radius.
         point_xyz, point_mask, heatmap, box_id = compute_heatmap(
             box_3d,
             box_mask,
             self._voxel_size,
-            self._min_radius,
             self._max_radius,
         )
         # heatmap - [B, H, W, Z]
         # scatter the localized heatmap to global heatmap in vehicle frame.
         dense_heatmap, dense_box_id = scatter_to_dense_heatmap(
             point_xyz,
             point_mask,
```

## keras_cv/layers/object_detection_3d/centernet_label_encoder_test.py

```diff
@@ -20,15 +20,14 @@
 )
 
 
 class CenterNetLabelEncoderTest(tf.test.TestCase):
     def test_voxelization_output_shape_no_z(self):
         layer = CenterNetLabelEncoder(
             voxel_size=[0.1, 0.1, 1000],
-            min_radius=[0.8, 0.8, 0.0],
             max_radius=[8.0, 8.0, 0.0],
             spatial_size=[-20, 20, -20, 20, -20, 20],
             num_classes=2,
             top_k_heatmap=[10, 20],
         )
         box_3d = tf.random.uniform(
             shape=[2, 100, 7], minval=-5, maxval=5, dtype=tf.float32
@@ -53,15 +52,14 @@
         # last dimension only has x, y
         self.assertEqual(output["class_1"]["top_k_index"].shape, [2, 10, 2])
         self.assertEqual(output["class_2"]["top_k_index"].shape, [2, 20, 2])
 
     def test_voxelization_output_shape_with_z(self):
         layer = CenterNetLabelEncoder(
             voxel_size=[0.1, 0.1, 10],
-            min_radius=[0.8, 0.8, 0.0],
             max_radius=[8.0, 8.0, 0.0],
             spatial_size=[-20, 20, -20, 20, -20, 20],
             num_classes=2,
             top_k_heatmap=[10, 20],
         )
         box_3d = tf.random.uniform(
             shape=[2, 100, 7], minval=-5, maxval=5, dtype=tf.float32
@@ -86,15 +84,14 @@
         # last dimension has x, y, z
         self.assertEqual(output["class_1"]["top_k_index"].shape, [2, 10, 3])
         self.assertEqual(output["class_2"]["top_k_index"].shape, [2, 20, 3])
 
     def test_voxelization_output_shape_missing_topk(self):
         layer = CenterNetLabelEncoder(
             voxel_size=[0.1, 0.1, 1000],
-            min_radius=[0.8, 0.8, 0.0],
             max_radius=[8.0, 8.0, 0.0],
             spatial_size=[-20, 20, -20, 20, -20, 20],
             num_classes=2,
             top_k_heatmap=[10, 0],
         )
         box_3d = tf.random.uniform(
             shape=[2, 100, 7], minval=-5, maxval=5, dtype=tf.float32
```

## keras_cv/layers/preprocessing/__init__.py

```diff
@@ -27,19 +27,19 @@
 from keras_cv.layers.preprocessing.channel_shuffle import ChannelShuffle
 from keras_cv.layers.preprocessing.cut_mix import CutMix
 from keras_cv.layers.preprocessing.equalization import Equalization
 from keras_cv.layers.preprocessing.fourier_mix import FourierMix
 from keras_cv.layers.preprocessing.grayscale import Grayscale
 from keras_cv.layers.preprocessing.grid_mask import GridMask
 from keras_cv.layers.preprocessing.jittered_resize import JitteredResize
-from keras_cv.layers.preprocessing.maybe_apply import MaybeApply
 from keras_cv.layers.preprocessing.mix_up import MixUp
 from keras_cv.layers.preprocessing.mosaic import Mosaic
 from keras_cv.layers.preprocessing.posterization import Posterization
 from keras_cv.layers.preprocessing.rand_augment import RandAugment
+from keras_cv.layers.preprocessing.random_apply import RandomApply
 from keras_cv.layers.preprocessing.random_aspect_ratio import RandomAspectRatio
 from keras_cv.layers.preprocessing.random_augmentation_pipeline import (
     RandomAugmentationPipeline,
 )
 from keras_cv.layers.preprocessing.random_brightness import RandomBrightness
 from keras_cv.layers.preprocessing.random_channel_shift import (
     RandomChannelShift,
@@ -63,17 +63,14 @@
 from keras_cv.layers.preprocessing.random_jpeg_quality import RandomJpegQuality
 from keras_cv.layers.preprocessing.random_rotation import RandomRotation
 from keras_cv.layers.preprocessing.random_saturation import RandomSaturation
 from keras_cv.layers.preprocessing.random_sharpness import RandomSharpness
 from keras_cv.layers.preprocessing.random_shear import RandomShear
 from keras_cv.layers.preprocessing.random_translation import RandomTranslation
 from keras_cv.layers.preprocessing.random_zoom import RandomZoom
-from keras_cv.layers.preprocessing.randomly_zoomed_crop import (
-    RandomlyZoomedCrop,
-)
 from keras_cv.layers.preprocessing.repeated_augmentation import (
     RepeatedAugmentation,
 )
 from keras_cv.layers.preprocessing.rescaling import Rescaling
 from keras_cv.layers.preprocessing.resizing import Resizing
 from keras_cv.layers.preprocessing.solarization import Solarization
 from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
```

## keras_cv/layers/preprocessing/aug_mix_test.py

```diff
@@ -73,7 +73,20 @@
         xs = layer(xs)
         self.assertEqual(xs.shape, [2, 512, 512, 3])
 
         # greyscale
         xs = tf.ones((2, 512, 512, 1))
         xs = layer(xs)
         self.assertEqual(xs.shape, [2, 512, 512, 1])
+
+    def test_many_augmentations(self):
+        layer = preprocessing.AugMix([0, 255], chain_depth=[25, 26])
+
+        # RGB
+        xs = tf.ones((2, 512, 512, 3))
+        xs = layer(xs)
+        self.assertEqual(xs.shape, [2, 512, 512, 3])
+
+        # greyscale
+        xs = tf.ones((2, 512, 512, 1))
+        xs = layer(xs)
+        self.assertEqual(xs.shape, [2, 512, 512, 1])
```

## keras_cv/layers/preprocessing/mix_up.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
@@ -65,26 +65,32 @@
         return sample_alpha / (sample_alpha + sample_beta)
 
     def _batch_augment(self, inputs):
         self._validate_inputs(inputs)
         images = inputs.get("images", None)
         labels = inputs.get("labels", None)
         bounding_boxes = inputs.get("bounding_boxes", None)
+        segmentation_masks = inputs.get("segmentation_masks", None)
         images, lambda_sample, permutation_order = self._mixup(images)
         if labels is not None:
             labels = self._update_labels(
                 labels, lambda_sample, permutation_order
             )
             inputs["labels"] = labels
         if bounding_boxes is not None:
             bounding_boxes = self._update_bounding_boxes(
                 bounding_boxes, permutation_order
             )
             inputs["bounding_boxes"] = bounding_boxes
         inputs["images"] = images
+        if segmentation_masks is not None:
+            segmentation_masks = self._update_segmentation_masks(
+                segmentation_masks, lambda_sample, permutation_order
+            )
+            inputs["segmentation_masks"] = segmentation_masks
         return inputs
 
     def _augment(self, inputs):
         raise ValueError(
             "MixUp received a single image to `call`. The layer relies on "
             "combining multiple examples, and as such will not behave as "
             "expected. Please call the layer with 2 or more samples."
@@ -126,36 +132,66 @@
         boxes, classes = bounding_boxes["boxes"], bounding_boxes["classes"]
         boxes_for_mixup = tf.gather(boxes, permutation_order)
         classes_for_mixup = tf.gather(classes, permutation_order)
         boxes = tf.concat([boxes, boxes_for_mixup], axis=1)
         classes = tf.concat([classes, classes_for_mixup], axis=1)
         return {"boxes": boxes, "classes": classes}
 
+    def _update_segmentation_masks(
+        self, segmentation_masks, lambda_sample, permutation_order
+    ):
+        lambda_sample = tf.reshape(lambda_sample, [-1, 1, 1, 1])
+
+        segmentation_masks_for_mixup = tf.gather(
+            segmentation_masks, permutation_order
+        )
+
+        segmentation_masks = (
+            lambda_sample * segmentation_masks
+            + (1.0 - lambda_sample) * segmentation_masks_for_mixup
+        )
+
+        return segmentation_masks
+
     def _validate_inputs(self, inputs):
         images = inputs.get("images", None)
         labels = inputs.get("labels", None)
         bounding_boxes = inputs.get("bounding_boxes", None)
+        segmentation_masks = inputs.get("segmentation_masks", None)
 
-        if images is None or (labels is None and bounding_boxes is None):
+        if images is None or (
+            labels is None
+            and bounding_boxes is None
+            and segmentation_masks is None
+        ):
             raise ValueError(
                 "MixUp expects inputs in a dictionary with format "
                 '{"images": images, "labels": labels}. or'
-                '{"images": images, "bounding_boxes": bounding_boxes}'
+                '{"images": images, "bounding_boxes": bounding_boxes}. or'
+                '{"images": images, "segmentation_masks": segmentation_masks}. '
                 f"Got: inputs = {inputs}."
             )
 
         if labels is not None and not labels.dtype.is_floating:
             raise ValueError(
                 f"MixUp received labels with type {labels.dtype}. "
                 "Labels must be of type float."
             )
 
         if bounding_boxes is not None:
             _ = bounding_box.validate_format(bounding_boxes)
 
+        if segmentation_masks is not None:
+            if len(segmentation_masks.shape) != 4:
+                raise ValueError(
+                    "MixUp expects shape of segmentation_masks as "
+                    "[batch, h, w, num_classes]. "
+                    f"Got: shape = {segmentation_masks.shape}. "
+                )
+
     def get_config(self):
         config = {
             "alpha": self.alpha,
             "seed": self.seed,
         }
         base_config = super().get_config()
         return dict(list(base_config.items()) + list(config.items()))
```

## keras_cv/layers/preprocessing/mix_up_test.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2023 The KerasCV Authors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
@@ -28,35 +28,48 @@
 
         # randomly sample bounding boxes
         ys_bounding_boxes = {
             "boxes": tf.random.uniform((2, 3, 4), 0, 1),
             "classes": tf.random.uniform((2, 3), 0, 1),
         }
 
+        # randomly sample segmentation mask
+        ys_segmentation_masks = tf.cast(
+            tf.stack(
+                [2 * tf.ones((512, 512)), tf.ones((512, 512))],
+                axis=0,
+            ),
+            tf.uint8,
+        )
+        ys_segmentation_masks = tf.one_hot(ys_segmentation_masks, 3)
+
         layer = MixUp()
         # mixup on labels
         outputs = layer(
             {
                 "images": xs,
                 "labels": ys_labels,
                 "bounding_boxes": ys_bounding_boxes,
+                "segmentation_masks": ys_segmentation_masks,
             }
         )
-        xs, ys_labels, ys_bounding_boxes = (
+        xs, ys_labels, ys_bounding_boxes, ys_segmentation_masks = (
             outputs["images"],
             outputs["labels"],
             outputs["bounding_boxes"],
+            outputs["segmentation_masks"],
         )
 
         self.assertEqual(xs.shape, [2, 512, 512, 3])
         self.assertEqual(ys_labels.shape, [2, 10])
         self.assertEqual(ys_bounding_boxes["boxes"].shape, [2, 6, 4])
         self.assertEqual(ys_bounding_boxes["classes"].shape, [2, 6])
+        self.assertEqual(ys_segmentation_masks.shape, [2, 512, 512, 3])
 
-    def test_mix_up_call_results(self):
+    def test_mix_up_call_results_with_labels(self):
         xs = tf.cast(
             tf.stack(
                 [2 * tf.ones((4, 4, 3)), tf.ones((4, 4, 3))],
                 axis=0,
             ),
             tf.float32,
         )
@@ -70,14 +83,48 @@
         self.assertNotAllClose(xs, 1.0)
         self.assertNotAllClose(xs, 2.0)
 
         # No labels should still be close to their originals
         self.assertNotAllClose(ys, 1.0)
         self.assertNotAllClose(ys, 0.0)
 
+    def test_mix_up_call_results_with_masks(self):
+        xs = tf.cast(
+            tf.stack(
+                [2 * tf.ones((4, 4, 3)), tf.ones((4, 4, 3))],
+                axis=0,
+            ),
+            tf.float32,
+        )
+        ys_segmentation_masks = tf.cast(
+            tf.stack(
+                [2 * tf.ones((4, 4)), tf.ones((4, 4))],
+                axis=0,
+            ),
+            tf.uint8,
+        )
+        ys_segmentation_masks = tf.one_hot(ys_segmentation_masks, 3)
+
+        layer = MixUp()
+        outputs = layer(
+            {"images": xs, "segmentation_masks": ys_segmentation_masks}
+        )
+        xs, ys_segmentation_masks = (
+            outputs["images"],
+            outputs["segmentation_masks"],
+        )
+
+        # None of the individual values should still be close to 1 or 0
+        self.assertNotAllClose(xs, 1.0)
+        self.assertNotAllClose(xs, 2.0)
+
+        # No masks should still be close to their originals
+        self.assertNotAllClose(ys_segmentation_masks, 1.0)
+        self.assertNotAllClose(ys_segmentation_masks, 0.0)
+
     def test_in_tf_function(self):
         xs = tf.cast(
             tf.stack(
                 [2 * tf.ones((4, 4, 3)), tf.ones((4, 4, 3))],
                 axis=0,
             ),
             tf.float32,
```

## keras_cv/layers/preprocessing/ragged_image_test.py

```diff
@@ -118,24 +118,14 @@
         {
             "target_size": (224, 224),
             "crop_area_factor": (0.8, 1.0),
             "aspect_ratio_factor": (3 / 4, 4 / 3),
         },
     ),
     (
-        "RandomlyZoomedCrop",
-        layers.RandomlyZoomedCrop,
-        {
-            "height": 224,
-            "width": 224,
-            "zoom_factor": (0.8, 1.0),
-            "aspect_ratio_factor": (3 / 4, 4 / 3),
-        },
-    ),
-    (
         "Resizing",
         layers.Resizing,
         {
             "height": 224,
             "width": 224,
         },
     ),
```

## keras_cv/layers/preprocessing/random_crop.py

```diff
@@ -18,15 +18,15 @@
 
 from keras_cv import bounding_box
 from keras_cv.layers.preprocessing.vectorized_base_image_augmentation_layer import (  # noqa: E501
     VectorizedBaseImageAugmentationLayer,
 )
 
 # In order to support both unbatched and batched inputs, the horizontal
-# and verticle axis is reverse indexed
+# and vertical axis is reverse indexed
 H_AXIS = -3
 W_AXIS = -2
 
 
 @keras.utils.register_keras_serializable(package="keras_cv")
 class RandomCrop(VectorizedBaseImageAugmentationLayer):
     """A preprocessing layer which randomly crops images.
@@ -35,15 +35,15 @@
     size.
 
     If an input image is smaller than the target size, the input will be
     resized and cropped to return the largest possible window in the image that
     matches the target aspect ratio.
 
     Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and
-    of interger or floating point dtype.
+    of integer or floating point dtype.
 
     Input shape:
         3D (unbatched) or 4D (batched) tensor with shape:
         `(..., height, width, channels)`, in `"channels_last"` format.
 
     Output shape:
         3D (unbatched) or 4D (batched) tensor with shape:
```

## keras_cv/layers/preprocessing/random_rotation.py

```diff
@@ -177,15 +177,15 @@
                 tf.stack([boxes[:, :, 0], boxes[:, :, 3]], axis=2),
             ],
             axis=2,
         )
         # point_x : x coordinates of all corners of the bounding box
         point_xs = tf.gather(points, [0], axis=3)
         point_x_offsets = tf.cast((point_xs - origin_x), dtype=tf.float32)
-        # point_y : y cordinates of all corners of the bounding box
+        # point_y : y coordinates of all corners of the bounding box
         point_ys = tf.gather(points, [1], axis=3)
         point_y_offsets = tf.cast((point_ys - origin_y), dtype=tf.float32)
         # rotated bounding box coordinates
         # new_x : new position of x coordinates of corners of bounding box
         new_x = (
             origin_x
             + tf.multiply(tf.cos(angles), point_x_offsets)
@@ -197,17 +197,17 @@
             + tf.multiply(tf.sin(angles), point_x_offsets)
             + tf.multiply(tf.cos(angles), point_y_offsets)
         )
         # rotated bounding box coordinates
         out = tf.concat([new_x, new_y], axis=3)
         # find readjusted coordinates of bounding box to represent it in corners
         # format
-        min_cordinates = tf.math.reduce_min(out, axis=2)
-        max_cordinates = tf.math.reduce_max(out, axis=2)
-        boxes = tf.concat([min_cordinates, max_cordinates], axis=2)
+        min_coordinates = tf.math.reduce_min(out, axis=2)
+        max_coordinates = tf.math.reduce_max(out, axis=2)
+        boxes = tf.concat([min_coordinates, max_coordinates], axis=2)
 
         bounding_boxes = bounding_boxes.copy()
         bounding_boxes["boxes"] = boxes
         bounding_boxes = bounding_box.clip_to_image(
             bounding_boxes,
             bounding_box_format="xyxy",
             images=raw_images,
@@ -247,15 +247,15 @@
                     "`segmentation_classes`. `segmentation_classes` was not "
                     f"specified, and mask has shape {segmentation_masks.shape}"
                 )
             rotated_mask = self._rotate_images(
                 segmentation_masks, transformations
             )
             # Round because we are in one-hot encoding, and we may have
-            # pixels with ambugious value due to floating point math for
+            # pixels with ambiguous value due to floating point math for
             # rotation.
             return tf.round(rotated_mask)
 
     def _rotate_images(self, images, transformations):
         images = preprocessing_utils.ensure_tensor(images, self.compute_dtype)
         original_shape = images.shape
         image_shape = tf.shape(images)
```

## keras_cv/layers/preprocessing/random_shear.py

```diff
@@ -315,14 +315,19 @@
             source="rel_xyxy",
             target=self.bounding_box_format,
             images=images,
             dtype=self.compute_dtype,
         )
         return bounding_boxes
 
+    @staticmethod
+    def _format_transform(transform):
+        transform = tf.convert_to_tensor(transform, dtype=tf.float32)
+        return transform[tf.newaxis]
+
     def get_config(self):
         config = super().get_config()
         config.update(
             {
                 "x_factor": self.x_factor,
                 "y_factor": self.y_factor,
                 "interpolation": self.interpolation,
```

## keras_cv/layers/preprocessing/resizing.py

```diff
@@ -20,15 +20,21 @@
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 
 H_AXIS = -3
 W_AXIS = -2
 
-supported_keys = ["images", "labels", "targets", "bounding_boxes"]
+supported_keys = [
+    "images",
+    "labels",
+    "targets",
+    "bounding_boxes",
+    "segmentation_masks",
+]
 
 
 class Resizing(BaseImageAugmentationLayer):
     """A preprocessing layer which resizes images.
 
     This layer resizes an image input to a target height and width. The input
     should be a 4D (batched) or 3D (unbatched) tensor in `"channels_last"`
@@ -109,14 +115,15 @@
             shape=(self.height, self.width, images.shape[-1]),
             dtype=self.compute_dtype,
         )
 
     def _augment(self, inputs):
         images = inputs.get("images", None)
         bounding_boxes = inputs.get("bounding_boxes", None)
+        segmentation_masks = inputs.get("segmentation_masks", None)
 
         if images is not None:
             images = tf.expand_dims(images, axis=0)
             inputs["images"] = images
 
         if bounding_boxes is not None:
             bounding_boxes = bounding_boxes.copy()
@@ -124,14 +131,18 @@
                 bounding_boxes["classes"], axis=0
             )
             bounding_boxes["boxes"] = tf.expand_dims(
                 bounding_boxes["boxes"], axis=0
             )
             inputs["bounding_boxes"] = bounding_boxes
 
+        if segmentation_masks is not None:
+            segmentation_masks = tf.expand_dims(segmentation_masks, axis=0)
+            inputs["segmentation_masks"] = segmentation_masks
+
         outputs = self._batch_augment(inputs)
 
         if images is not None:
             images = tf.squeeze(outputs["images"], axis=0)
             inputs["images"] = images
 
         if bounding_boxes is not None:
@@ -139,32 +150,47 @@
                 outputs["bounding_boxes"]["classes"], axis=0
             )
             outputs["bounding_boxes"]["boxes"] = tf.squeeze(
                 outputs["bounding_boxes"]["boxes"], axis=0
             )
             inputs["bounding_boxes"] = outputs["bounding_boxes"]
 
+        if segmentation_masks is not None:
+            segmentation_masks = tf.squeeze(
+                outputs["segmentation_masks"], axis=0
+            )
+            inputs["segmentation_masks"] = segmentation_masks
+
         return inputs
 
     def _resize_with_distortion(self, inputs):
         images = inputs.get("images", None)
+        segmentation_masks = inputs.get("segmentation_masks", None)
 
         size = [self.height, self.width]
         images = tf.image.resize(
             images, size=size, method=self._interpolation_method
         )
         images = tf.cast(images, self.compute_dtype)
 
+        if segmentation_masks is not None:
+            segmentation_masks = tf.image.resize(
+                segmentation_masks, size=size, method="nearest"
+            )
+
         inputs["images"] = images
+        inputs["segmentation_masks"] = segmentation_masks
+
         return inputs
 
     def _resize_with_pad(self, inputs):
         def resize_single_with_pad_to_aspect(x):
             image = x.get("images", None)
             bounding_boxes = x.get("bounding_boxes", None)
+            segmentation_masks = x.get("segmentation_masks", None)
 
             # images must be dense-able at this point.
             if isinstance(image, tf.RaggedTensor):
                 image = image.to_tensor()
 
             img_size = tf.shape(image)
             img_height = tf.cast(img_size[H_AXIS], self.compute_dtype)
@@ -213,35 +239,60 @@
                 )
             inputs["images"] = image
 
             if bounding_boxes is not None:
                 inputs["bounding_boxes"] = keras_cv.bounding_box.to_ragged(
                     bounding_boxes
                 )
+
+            if segmentation_masks is not None:
+                segmentation_masks = tf.image.resize(
+                    segmentation_masks,
+                    size=(target_height, target_width),
+                    method="nearest",
+                )
+                segmentation_masks = tf.image.pad_to_bounding_box(
+                    tf.cast(segmentation_masks, dtype="float32"),
+                    0,
+                    0,
+                    self.height,
+                    self.width,
+                )
+                inputs["segmentation_masks"] = segmentation_masks
+
             return inputs
 
         size_as_shape = tf.TensorShape((self.height, self.width))
         shape = size_as_shape + inputs["images"].shape[-1:]
         img_spec = tf.TensorSpec(shape, self.compute_dtype)
         fn_output_signature = {"images": img_spec}
 
         bounding_boxes = inputs.get("bounding_boxes", None)
         if bounding_boxes is not None:
             boxes_spec = self._compute_bounding_box_signature(bounding_boxes)
             fn_output_signature["bounding_boxes"] = boxes_spec
 
+        segmentation_masks = inputs.get("segmentation_masks", None)
+        if segmentation_masks is not None:
+            seg_map_shape = (
+                size_as_shape + inputs["segmentation_masks"].shape[-1:]
+            )
+            seg_map_spec = tf.TensorSpec(seg_map_shape, self.compute_dtype)
+            fn_output_signature["segmentation_masks"] = seg_map_spec
+
         return tf.map_fn(
             resize_single_with_pad_to_aspect,
             inputs,
             fn_output_signature=fn_output_signature,
         )
 
     def _resize_with_crop(self, inputs):
         images = inputs.get("images", None)
         bounding_boxes = inputs.get("bounding_boxes", None)
+        segmentation_masks = inputs.get("segmentation_masks", None)
         if bounding_boxes is not None:
             raise ValueError(
                 "Resizing(crop_to_aspect_ratio=True) does not support "
                 "bounding box inputs. Please use `pad_to_aspect_ratio=True` "
                 "when processing bounding boxes with Resizing()."
             )
         inputs["images"] = images
@@ -251,32 +302,62 @@
         # efficiently on float32 unless interpolation is nearest, in which case
         # output type matches input type.
         if self.interpolation == "nearest":
             input_dtype = self.compute_dtype
         else:
             input_dtype = tf.float32
 
-        def resize_with_crop_to_aspect(x):
+        def resize_with_crop_to_aspect(x, interpolation_method):
             if isinstance(x, tf.RaggedTensor):
                 x = x.to_tensor()
             return keras.preprocessing.image.smart_resize(
-                x, size=size, interpolation=self._interpolation_method
+                x,
+                size=size,
+                interpolation=interpolation_method,
             )
 
+        def resize_with_crop_to_aspect_images(x):
+            return resize_with_crop_to_aspect(
+                x, interpolation_method=self._interpolation_method
+            )
+
+        def resize_with_crop_to_aspect_masks(x):
+            return resize_with_crop_to_aspect(x, interpolation_method="nearest")
+
         if isinstance(images, tf.RaggedTensor):
             size_as_shape = tf.TensorShape(size)
             shape = size_as_shape + images.shape[-1:]
             spec = tf.TensorSpec(shape, input_dtype)
             images = tf.map_fn(
-                resize_with_crop_to_aspect, images, fn_output_signature=spec
+                resize_with_crop_to_aspect_images,
+                images,
+                fn_output_signature=spec,
             )
         else:
-            images = resize_with_crop_to_aspect(images)
+            images = resize_with_crop_to_aspect_images(images)
 
         inputs["images"] = images
+
+        if segmentation_masks is not None:
+            if isinstance(segmentation_masks, tf.RaggedTensor):
+                size_as_shape = tf.TensorShape(size)
+                shape = size_as_shape + segmentation_masks.shape[-1:]
+                spec = tf.TensorSpec(shape, input_dtype)
+                segmentation_masks = tf.map_fn(
+                    resize_with_crop_to_aspect_masks,
+                    segmentation_masks,
+                    fn_output_signature=spec,
+                )
+            else:
+                segmentation_masks = resize_with_crop_to_aspect_masks(
+                    segmentation_masks
+                )
+
+            inputs["segmentation_masks"] = segmentation_masks
+
         return inputs
 
     def _check_inputs(self, inputs):
         for key in inputs:
             if key not in supported_keys:
                 raise ValueError(
                     "Resizing() currently only supports keys "
```

## keras_cv/layers/preprocessing/resizing_test.py

```diff
@@ -302,7 +302,30 @@
             outputs["images"].shape.as_list(),
         )
 
         self.assertAllEqual(outputs["images"][1][:, :8, :], tf.ones((16, 8, 3)))
         self.assertAllEqual(
             outputs["images"][1][:, -8:, :], tf.zeros((16, 8, 3))
         )
+
+    def test_resize_with_mask(self):
+        input_images = np.random.normal(size=(2, 4, 4, 3))
+        seg_masks = np.random.uniform(
+            low=0.0, high=3.0, size=(2, 4, 4, 3)
+        ).astype("int32")
+        inputs = {
+            "images": input_images,
+            "segmentation_masks": seg_masks,
+        }
+
+        layer = cv_layers.Resizing(2, 2)
+        outputs = layer(inputs)
+
+        expected_output_images = tf.image.resize(input_images, size=(2, 2))
+        expected_output_seg_masks = tf.image.resize(
+            seg_masks, size=(2, 2), method="nearest"
+        )
+
+        self.assertAllEqual(expected_output_images, outputs["images"])
+        self.assertAllEqual(
+            expected_output_seg_masks, outputs["segmentation_masks"]
+        )
```

## keras_cv/layers/preprocessing/vectorized_base_image_augmentation_layer_test.py

```diff
@@ -525,15 +525,15 @@
             ]
         )
         add_layer = VectorizedRandomAddLayer(fixed_value=0.5)
         add_layer.force_output_dense_images = True
         result = add_layer(images)
         self.assertTrue(isinstance(result, tf.Tensor))
 
-    def test_converts_ragged_to_dense_segmention_masks(self):
+    def test_converts_ragged_to_dense_segmentation_masks(self):
         images = tf.ragged.stack(
             [
                 np.random.random(size=(8, 8, 3)).astype("float32"),
                 np.random.random(size=(16, 8, 3)).astype("float32"),
             ]
         )
         segmentation_masks = tf.ragged.stack(
```

## keras_cv/layers/preprocessing/with_labels_test.py

```diff
@@ -33,24 +33,14 @@
         "Resizing",
         layers.Resizing,
         {
             "height": 224,
             "width": 224,
         },
     ),
-    (
-        "RandomlyZoomedCrop",
-        layers.RandomlyZoomedCrop,
-        {
-            "height": 224,
-            "width": 224,
-            "zoom_factor": (0.8, 1.0),
-            "aspect_ratio_factor": (3 / 4, 4 / 3),
-        },
-    ),
     ("Grayscale", layers.Grayscale, {}),
     ("GridMask", layers.GridMask, {}),
     (
         "Posterization",
         layers.Posterization,
         {"bits": 3, "value_range": (0, 255)},
     ),
```

## keras_cv/layers/preprocessing/with_mixed_precision_test.py

```diff
@@ -27,24 +27,14 @@
         layers.RandomCropAndResize,
         {
             "target_size": (224, 224),
             "crop_area_factor": (0.8, 1.0),
             "aspect_ratio_factor": (3 / 4, 4 / 3),
         },
     ),
-    (
-        "RandomlyZoomedCrop",
-        layers.RandomlyZoomedCrop,
-        {
-            "height": 224,
-            "width": 224,
-            "zoom_factor": (0.8, 1.0),
-            "aspect_ratio_factor": (3 / 4, 4 / 3),
-        },
-    ),
     ("Grayscale", layers.Grayscale, {}),
     ("GridMask", layers.GridMask, {}),
     (
         "Posterization",
         layers.Posterization,
         {"bits": 3, "value_range": (0, 255)},
     ),
```

## keras_cv/layers/preprocessing/with_segmentation_masks_test.py

```diff
@@ -78,14 +78,15 @@
     ("RandomSaturation", preprocessing.RandomSaturation, {"factor": 0.5}),
     (
         "RandomSharpness",
         preprocessing.RandomSharpness,
         {"factor": 0.5, "value_range": (0, 255)},
     ),
     ("Solarization", preprocessing.Solarization, {"value_range": (0, 255)}),
+    ("Resizing", preprocessing.Resizing, {"height": 512, "width": 512}),
 ]
 
 
 class WithSegmentationMasksTest(tf.test.TestCase, parameterized.TestCase):
     @parameterized.named_parameters(*TEST_CONFIGURATIONS)
     def test_can_run_with_segmentation_masks(self, layer_cls, init_args):
         num_classes = 10
```

## keras_cv/layers/preprocessing_3d/__init__.py

```diff
@@ -11,32 +11,38 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from keras_cv.layers.preprocessing_3d.base_augmentation_layer_3d import (
     BaseAugmentationLayer3D,
 )
-from keras_cv.layers.preprocessing_3d.frustum_random_dropping_points import (
+from keras_cv.layers.preprocessing_3d.waymo.frustum_random_dropping_points import (  # noqa: E501
     FrustumRandomDroppingPoints,
 )
-from keras_cv.layers.preprocessing_3d.frustum_random_point_feature_noise import (  # noqa: E501
+from keras_cv.layers.preprocessing_3d.waymo.frustum_random_point_feature_noise import (  # noqa: E501
     FrustumRandomPointFeatureNoise,
 )
-from keras_cv.layers.preprocessing_3d.global_random_dropping_points import (
+from keras_cv.layers.preprocessing_3d.waymo.global_random_dropping_points import (  # noqa: E501
     GlobalRandomDroppingPoints,
 )
-from keras_cv.layers.preprocessing_3d.global_random_flip import GlobalRandomFlip
-from keras_cv.layers.preprocessing_3d.global_random_rotation import (
+from keras_cv.layers.preprocessing_3d.waymo.global_random_flip import (
+    GlobalRandomFlip,
+)
+from keras_cv.layers.preprocessing_3d.waymo.global_random_rotation import (
     GlobalRandomRotation,
 )
-from keras_cv.layers.preprocessing_3d.global_random_scaling import (
+from keras_cv.layers.preprocessing_3d.waymo.global_random_scaling import (
     GlobalRandomScaling,
 )
-from keras_cv.layers.preprocessing_3d.global_random_translation import (
+from keras_cv.layers.preprocessing_3d.waymo.global_random_translation import (
     GlobalRandomTranslation,
 )
-from keras_cv.layers.preprocessing_3d.group_points_by_bounding_boxes import (
+from keras_cv.layers.preprocessing_3d.waymo.group_points_by_bounding_boxes import (  # noqa: E501
     GroupPointsByBoundingBoxes,
 )
-from keras_cv.layers.preprocessing_3d.random_copy_paste import RandomCopyPaste
-from keras_cv.layers.preprocessing_3d.random_drop_box import RandomDropBox
-from keras_cv.layers.preprocessing_3d.swap_background import SwapBackground
+from keras_cv.layers.preprocessing_3d.waymo.random_copy_paste import (
+    RandomCopyPaste,
+)
+from keras_cv.layers.preprocessing_3d.waymo.random_drop_box import RandomDropBox
+from keras_cv.layers.preprocessing_3d.waymo.swap_background import (
+    SwapBackground,
+)
```

## keras_cv/layers/preprocessing_3d/base_augmentation_layer_3d.py

```diff
@@ -161,29 +161,44 @@
         Returns:
           Any type of object, which will be forwarded to `augment_point_clouds`,
           and `augment_bounding_box` as the `transformation` parameter.
         """
         return None
 
     def call(self, inputs):
-        point_clouds = inputs[POINT_CLOUDS]
-        bounding_boxes = inputs[BOUNDING_BOXES]
+        if "3d_boxes" in inputs.keys():
+            # TODO(ianstenbit): Consider using the better format internally
+            # (in the KPL implementations) instead of wrapping it at call time.
+            point_clouds, bounding_boxes = convert_from_model_format(inputs)
+            inputs = {
+                POINT_CLOUDS: point_clouds,
+                BOUNDING_BOXES: bounding_boxes,
+            }
+            use_model_format = True
+        else:
+            point_clouds = inputs[POINT_CLOUDS]
+            bounding_boxes = inputs[BOUNDING_BOXES]
+            use_model_format = False
         if point_clouds.shape.rank == 3 and bounding_boxes.shape.rank == 3:
-            return self._augment(inputs)
+            outputs = self._augment(inputs)
         elif point_clouds.shape.rank == 4 and bounding_boxes.shape.rank == 4:
-            return self._batch_augment(inputs)
+            outputs = self._batch_augment(inputs)
         else:
             raise ValueError(
                 "Point clouds augmentation layers are expecting inputs "
                 "point clouds and bounding boxes to be rank 3D (Frame, Point, "
                 "Feature) or 4D (Batch, Frame, Point, Feature) tensors. Got "
                 "shape: {} and {}".format(
                     point_clouds.shape, bounding_boxes.shape
                 )
             )
+        if use_model_format:
+            return convert_to_model_format(outputs)
+        else:
+            return outputs
 
     def _augment(self, inputs):
         point_clouds = inputs.get(POINT_CLOUDS, None)
         bounding_boxes = inputs.get(BOUNDING_BOXES, None)
         transformation = self.get_random_transformation(
             point_clouds=point_clouds,
             bounding_boxes=bounding_boxes,
@@ -199,7 +214,78 @@
         # preserve any additional inputs unmodified by this layer.
         for key in inputs.keys() - result.keys():
             result[key] = inputs[key]
         return result
 
     def _batch_augment(self, inputs):
         return self._map_fn(self._augment, inputs)
+
+
+def convert_to_model_format(inputs):
+    point_clouds = {
+        "point_xyz": inputs["point_clouds"][..., :3],
+        "point_feature": inputs["point_clouds"][..., 3:-1],
+        "point_mask": tf.cast(inputs["point_clouds"][..., -1], tf.bool),
+    }
+    boxes = {
+        "boxes": inputs["bounding_boxes"][..., :7],
+        "classes": inputs["bounding_boxes"][..., 7],
+        "mask": tf.cast(inputs["bounding_boxes"][..., 8], tf.bool),
+    }
+
+    # Special case for when we have a difficulty field
+    if inputs["bounding_boxes"].shape[-1] > 8:
+        boxes["difficulty"] = inputs["bounding_boxes"][..., -1]
+
+    return {
+        "point_clouds": point_clouds,
+        "3d_boxes": boxes,
+    }
+
+
+def convert_from_model_format(inputs):
+    point_clouds = tf.concat(
+        [
+            inputs["point_clouds"]["point_xyz"],
+            inputs["point_clouds"]["point_feature"],
+            tf.expand_dims(
+                tf.cast(
+                    inputs["point_clouds"]["point_mask"],
+                    inputs["point_clouds"]["point_xyz"].dtype,
+                ),
+                axis=-1,
+            ),
+        ],
+        axis=-1,
+    )
+
+    box_tensors = [
+        inputs["3d_boxes"]["boxes"],
+        tf.expand_dims(
+            tf.cast(
+                inputs["3d_boxes"]["classes"], inputs["3d_boxes"]["boxes"].dtype
+            ),
+            axis=-1,
+        ),
+        tf.expand_dims(
+            tf.cast(
+                inputs["3d_boxes"]["mask"], inputs["3d_boxes"]["boxes"].dtype
+            ),
+            axis=-1,
+        ),
+    ]
+
+    # Special case for when we have a difficulty field
+    if "difficulty" in inputs["3d_boxes"].keys():
+        box_tensors.append(
+            tf.expand_dims(
+                tf.cast(
+                    inputs["3d_boxes"]["difficulty"],
+                    inputs["3d_boxes"]["boxes"].dtype,
+                ),
+                axis=-1,
+            )
+        )
+
+    boxes = tf.concat(box_tensors, axis=-1)
+
+    return point_clouds, boxes
```

## keras_cv/losses/__init__.py

```diff
@@ -9,14 +9,15 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from keras_cv.losses.centernet_box_loss import CenterNetBoxLoss
+from keras_cv.losses.ciou_loss import CIoULoss
 from keras_cv.losses.focal import FocalLoss
 from keras_cv.losses.giou_loss import GIoULoss
 from keras_cv.losses.iou_loss import IoULoss
 from keras_cv.losses.penalty_reduced_focal_loss import (
     BinaryPenaltyReducedFocalCrossEntropy,
 )
 from keras_cv.losses.simclr_loss import SimCLRLoss
```

## keras_cv/metrics/coco/pycoco_wrapper.py

```diff
@@ -54,15 +54,15 @@
 
     def __init__(self, gt_dataset=None):
         """Instantiates a COCO-style API object.
         Args:
           eval_type: either 'box' or 'mask'.
           annotation_file: a JSON file that stores annotations of the eval
             dataset. This is required if `gt_dataset` is not provided.
-          gt_dataset: the groundtruth eval datatset in COCO API format.
+          gt_dataset: the groundtruth eval dataset in COCO API format.
         """
         assert_pycocotools_installed("PyCOCOWrapper")
         COCO.__init__(self, annotation_file=None)
         self._eval_type = "box"
         if gt_dataset:
             self.dataset = gt_dataset
             self.createIndex()
@@ -121,26 +121,26 @@
     return new_boxes
 
 
 def _convert_predictions_to_coco_annotations(predictions):
     coco_predictions = []
     num_batches = len(predictions["source_id"])
     for i in range(num_batches):
-        predictions["detection_boxes"][i] = _yxyx_to_xywh(
-            predictions["detection_boxes"][i]
-        )
         batch_size = predictions["source_id"][i].shape[0]
         for j in range(batch_size):
             max_num_detections = predictions["num_detections"][i][j]
+            predictions["detection_boxes"][i][j] = _yxyx_to_xywh(
+                predictions["detection_boxes"][i][j]
+            )
             for k in range(max_num_detections):
                 ann = {}
                 ann["image_id"] = predictions["source_id"][i][j]
-                ann["category_id"] = predictions["detection_classes"][i][j, k]
-                ann["bbox"] = predictions["detection_boxes"][i][j, k]
-                ann["score"] = predictions["detection_scores"][i][j, k]
+                ann["category_id"] = predictions["detection_classes"][i][j][k]
+                ann["bbox"] = predictions["detection_boxes"][i][j][k]
+                ann["score"] = predictions["detection_scores"][i][j][k]
                 coco_predictions.append(ann)
 
     for i, ann in enumerate(coco_predictions):
         ann["id"] = i + 1
 
     return coco_predictions
 
@@ -148,35 +148,35 @@
 def _convert_groundtruths_to_coco_dataset(groundtruths, label_map=None):
     source_ids = np.concatenate(groundtruths["source_id"], axis=0)
     gt_images = [{"id": i} for i in source_ids]
 
     gt_annotations = []
     num_batches = len(groundtruths["source_id"])
     for i in range(num_batches):
-        max_num_instances = groundtruths["classes"][i].shape[1]
+        max_num_instances = max(x.shape[0] for x in groundtruths["classes"][i])
         batch_size = groundtruths["source_id"][i].shape[0]
         for j in range(batch_size):
             num_instances = groundtruths["num_detections"][i][j]
             if num_instances > max_num_instances:
                 num_instances = max_num_instances
             for k in range(int(num_instances)):
                 ann = {}
                 ann["image_id"] = groundtruths["source_id"][i][j]
                 ann["iscrowd"] = 0
-                ann["category_id"] = int(groundtruths["classes"][i][j, k])
+                ann["category_id"] = int(groundtruths["classes"][i][j][k])
                 boxes = groundtruths["boxes"][i]
                 ann["bbox"] = [
-                    float(boxes[j, k, 1]),
-                    float(boxes[j, k, 0]),
-                    float(boxes[j, k, 3] - boxes[j, k, 1]),
-                    float(boxes[j, k, 2] - boxes[j, k, 0]),
+                    float(boxes[j][k][1]),
+                    float(boxes[j][k][0]),
+                    float(boxes[j][k][3] - boxes[j][k][1]),
+                    float(boxes[j][k][2] - boxes[j][k][0]),
                 ]
                 ann["area"] = float(
-                    (boxes[j, k, 3] - boxes[j, k, 1])
-                    * (boxes[j, k, 2] - boxes[j, k, 0])
+                    (boxes[j][k][3] - boxes[j][k][1])
+                    * (boxes[j][k][2] - boxes[j][k][0])
                 )
                 gt_annotations.append(ann)
 
     for i, ann in enumerate(gt_annotations):
         ann["id"] = i + 1
 
     if label_map:
```

## keras_cv/metrics/object_detection/box_coco_metrics.py

```diff
@@ -219,14 +219,21 @@
         if self.name.startswith("box_coco_metrics"):
             return ""
         return self.name + "_"
 
     def update_state(self, y_true, y_pred, sample_weight=None):
         self._eval_step_count += 1
 
+        if isinstance(y_true["boxes"], tf.RaggedTensor) != isinstance(
+            y_pred["boxes"], tf.RaggedTensor
+        ):
+            # Make sure we have same ragged/dense status for y_true and y_pred
+            y_true = bounding_box.to_dense(y_true)
+            y_pred = bounding_box.to_dense(y_pred)
+
         self.ground_truths.append(y_true)
         self.predictions.append(y_pred)
 
         # Compute on first step, so we don't have an inconsistent list of
         # metrics in our train_step() results. This will just populate the
         # metrics with `0.0` until we get to `evaluate_freq`.
         if self._eval_step_count % self.evaluate_freq == 0:
```

## keras_cv/metrics/object_detection/box_coco_metrics_test.py

```diff
@@ -133,7 +133,135 @@
         suite.update_state(y_true, y_pred)
         metrics = suite.result()
 
         for metric in golden_metrics:
             self.assertAlmostEqual(
                 metrics["coco_metrics_" + metric], golden_metrics[metric]
             )
+
+    def test_coco_metric_suite_ragged_prediction(self):
+        suite = BoxCOCOMetrics(bounding_box_format="xyxy", evaluate_freq=1)
+        ragged_bounding_boxes = {
+            # shape: (2, (2, 1), 4)
+            "boxes": tf.ragged.constant(
+                [
+                    [[10, 10, 20, 20], [100, 100, 150, 150]],  # small, medium
+                    [[200, 200, 400, 400]],  # large
+                ],
+                ragged_rank=1,
+                dtype=tf.float32,
+            ),
+            "classes": tf.ragged.constant(
+                [[0, 1], [2]],
+                ragged_rank=1,
+                dtype=tf.float32,
+            ),
+            "confidence": tf.ragged.constant(
+                [[0.7, 0.8], [0.9]],
+                ragged_rank=1,
+                dtype=tf.float32,
+            ),
+        }
+        different_ragged_bounding_boxes = {
+            # shape: (2, (2, 3), 4)
+            "boxes": tf.ragged.constant(
+                [
+                    [[10, 10, 25, 25], [100, 105, 155, 155]],
+                    [[200, 200, 450, 450], [1, 1, 5, 5], [50, 50, 300, 300]],
+                ],
+                ragged_rank=1,
+                dtype=tf.float32,
+            ),
+            "classes": tf.ragged.constant(
+                [[0, 1], [2, 3, 3]],
+                ragged_rank=1,
+                dtype=tf.float32,
+            ),
+            "confidence": tf.ragged.constant(
+                [[0.7, 0.8], [0.9, 0.7, 0.7]],
+                ragged_rank=1,
+                dtype=tf.float32,
+            ),
+        }
+
+        suite.update_state(
+            ragged_bounding_boxes,
+            bounding_box.to_dense(ragged_bounding_boxes),
+        )
+        metrics = suite.result()
+        for metric in metrics:
+            # The metrics will be all 1.0 because the predictions and ground
+            # truth values are identical.
+            self.assertEqual(metrics[metric], 1.0)
+
+        suite.reset_state()
+        suite.update_state(
+            ragged_bounding_boxes,
+            bounding_box.to_dense(different_ragged_bounding_boxes),
+        )
+        metrics = suite.result()
+        for metric in metrics:
+            # The metrics will not be 1.0 because the predictions and ground
+            # truth values are completely different.
+            self.assertNotEqual(metrics[metric], 1.0)
+
+    def test_coco_metric_suite_ragged_labels(self):
+        suite = BoxCOCOMetrics(bounding_box_format="xyxy", evaluate_freq=1)
+        ragged_bounding_boxes = {
+            # shape: (2, (2, 1), 4)
+            "boxes": tf.ragged.constant(
+                [
+                    [[10, 10, 20, 20], [100, 100, 150, 150]],  # small, medium
+                    [[200, 200, 400, 400]],  # large
+                ],
+                ragged_rank=1,
+                dtype=tf.float32,
+            ),
+            "classes": tf.ragged.constant(
+                [[0, 1], [2]],
+                ragged_rank=1,
+                dtype=tf.float32,
+            ),
+            "confidence": tf.ragged.constant(
+                [[0.7, 0.8], [0.9]],
+                ragged_rank=1,
+                dtype=tf.float32,
+            ),
+        }
+        different_ragged_bounding_boxes = {
+            # shape: (2, (2, 3), 4)
+            "boxes": tf.ragged.constant(
+                [
+                    [[10, 10, 25, 25], [100, 105, 155, 155]],
+                    [[200, 200, 450, 450], [1, 1, 5, 5], [50, 50, 300, 300]],
+                ],
+                ragged_rank=1,
+                dtype=tf.float32,
+            ),
+            "classes": tf.ragged.constant(
+                [[0, 1], [2, 3, 3]],
+                ragged_rank=1,
+                dtype=tf.float32,
+            ),
+            "confidence": tf.ragged.constant(
+                [[0.7, 0.8], [0.9, 0.7, 0.7]],
+                ragged_rank=1,
+                dtype=tf.float32,
+            ),
+        }
+
+        suite.update_state(ragged_bounding_boxes, ragged_bounding_boxes)
+        metrics = suite.result()
+        for metric in metrics:
+            # The metrics will be all 1.0 because the predictions and ground
+            # truth values are identical.
+            self.assertEqual(metrics[metric], 1.0)
+
+        suite.reset_state()
+        suite.update_state(
+            ragged_bounding_boxes, different_ragged_bounding_boxes
+        )
+        metrics = suite.result()
+        for metric in metrics:
+            # The metrics will not be 1.0 because the predictions and ground
+            # truth values are completely different.
+            self.assertNotEqual(metrics[metric], 1.0)
```

## keras_cv/models/__init__.py

```diff
@@ -8,14 +8,15 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from keras_cv.models import legacy
 from keras_cv.models.backbones.csp_darknet.csp_darknet_backbone import (
     CSPDarkNetBackbone,
 )
 from keras_cv.models.backbones.csp_darknet.csp_darknet_backbone import (
     CSPDarkNetLBackbone,
 )
 from keras_cv.models.backbones.csp_darknet.csp_darknet_backbone import (
@@ -26,14 +27,26 @@
 )
 from keras_cv.models.backbones.csp_darknet.csp_darknet_backbone import (
     CSPDarkNetTinyBackbone,
 )
 from keras_cv.models.backbones.csp_darknet.csp_darknet_backbone import (
     CSPDarkNetXLBackbone,
 )
+from keras_cv.models.backbones.densenet.densenet_aliases import (
+    DenseNet121Backbone,
+)
+from keras_cv.models.backbones.densenet.densenet_aliases import (
+    DenseNet169Backbone,
+)
+from keras_cv.models.backbones.densenet.densenet_aliases import (
+    DenseNet201Backbone,
+)
+from keras_cv.models.backbones.densenet.densenet_backbone import (
+    DenseNetBackbone,
+)
 from keras_cv.models.backbones.efficientnet_v2.efficientnet_v2_aliases import (
     EfficientNetV2B0Backbone,
 )
 from keras_cv.models.backbones.efficientnet_v2.efficientnet_v2_aliases import (
     EfficientNetV2B1Backbone,
 )
 from keras_cv.models.backbones.efficientnet_v2.efficientnet_v2_aliases import (
@@ -50,54 +63,54 @@
 )
 from keras_cv.models.backbones.efficientnet_v2.efficientnet_v2_aliases import (
     EfficientNetV2MBackbone,
 )
 from keras_cv.models.backbones.efficientnet_v2.efficientnet_v2_aliases import (
     EfficientNetV2SBackbone,
 )
-from keras_cv.models.backbones.mobilenet_v3.mobilenet_v3_backbone import (
-    MobileNetV3Backbone,
-)
-from keras_cv.models.backbones.mobilenet_v3.mobilenet_v3_backbone import (
+from keras_cv.models.backbones.mobilenet_v3.mobilenet_v3_aliases import (
     MobileNetV3LargeBackbone,
 )
-from keras_cv.models.backbones.mobilenet_v3.mobilenet_v3_backbone import (
+from keras_cv.models.backbones.mobilenet_v3.mobilenet_v3_aliases import (
     MobileNetV3SmallBackbone,
 )
-from keras_cv.models.backbones.resnet_v1.resnet_v1_backbone import (
+from keras_cv.models.backbones.mobilenet_v3.mobilenet_v3_backbone import (
+    MobileNetV3Backbone,
+)
+from keras_cv.models.backbones.resnet_v1.resnet_v1_aliases import (
     ResNet18Backbone,
 )
-from keras_cv.models.backbones.resnet_v1.resnet_v1_backbone import (
+from keras_cv.models.backbones.resnet_v1.resnet_v1_aliases import (
     ResNet34Backbone,
 )
-from keras_cv.models.backbones.resnet_v1.resnet_v1_backbone import (
+from keras_cv.models.backbones.resnet_v1.resnet_v1_aliases import (
     ResNet50Backbone,
 )
-from keras_cv.models.backbones.resnet_v1.resnet_v1_backbone import (
+from keras_cv.models.backbones.resnet_v1.resnet_v1_aliases import (
     ResNet101Backbone,
 )
-from keras_cv.models.backbones.resnet_v1.resnet_v1_backbone import (
+from keras_cv.models.backbones.resnet_v1.resnet_v1_aliases import (
     ResNet152Backbone,
 )
 from keras_cv.models.backbones.resnet_v1.resnet_v1_backbone import (
     ResNetBackbone,
 )
-from keras_cv.models.backbones.resnet_v2.resnet_v2_backbone import (
+from keras_cv.models.backbones.resnet_v2.resnet_v2_aliases import (
     ResNet18V2Backbone,
 )
-from keras_cv.models.backbones.resnet_v2.resnet_v2_backbone import (
+from keras_cv.models.backbones.resnet_v2.resnet_v2_aliases import (
     ResNet34V2Backbone,
 )
-from keras_cv.models.backbones.resnet_v2.resnet_v2_backbone import (
+from keras_cv.models.backbones.resnet_v2.resnet_v2_aliases import (
     ResNet50V2Backbone,
 )
-from keras_cv.models.backbones.resnet_v2.resnet_v2_backbone import (
+from keras_cv.models.backbones.resnet_v2.resnet_v2_aliases import (
     ResNet101V2Backbone,
 )
-from keras_cv.models.backbones.resnet_v2.resnet_v2_backbone import (
+from keras_cv.models.backbones.resnet_v2.resnet_v2_aliases import (
     ResNet152V2Backbone,
 )
 from keras_cv.models.backbones.resnet_v2.resnet_v2_backbone import (
     ResNetV2Backbone,
 )
 from keras_cv.models.classification.image_classifier import ImageClassifier
 from keras_cv.models.object_detection.retinanet.retinanet import RetinaNet
```

## keras_cv/models/task.py

```diff
@@ -52,38 +52,43 @@
         # vanilla `keras.Model`. We override it to get a subclass instance back.
         if "backbone" in config and isinstance(config["backbone"], dict):
             config["backbone"] = keras.layers.deserialize(config["backbone"])
         return cls(**config)
 
     @classproperty
     def presets(cls):
-        """Dictionary of preset names and configurations."""
+        """Dictionary of preset names and configs."""
         return {}
 
     @classproperty
     def presets_with_weights(cls):
-        """Dictionary of preset names and configurations that include
-        weights."""
+        """Dictionary of preset names and configs that include weights."""
         return {}
 
     @classproperty
+    def presets_without_weights(cls):
+        """Dictionary of preset names and configs that don't include weights."""
+        return {
+            preset: cls.presets[preset]
+            for preset in set(cls.presets) - set(cls.presets_with_weights)
+        }
+
+    @classproperty
     def backbone_presets(cls):
-        """Dictionary of preset names and configurations for compatible
-        backbones."""
+        """Dictionary of preset names and configs for compatible backbones."""
         return {}
 
     @classmethod
     def from_preset(
         cls,
         preset,
         load_weights=None,
         **kwargs,
     ):
-        """Instantiate {{model_name}} model from preset architecture and
-        weights.
+        """Instantiate {{model_name}} model from preset config and weights.
 
         Args:
             preset: string. Must be one of "{{preset_names}}".
                 If looking for a preset with pretrained weights, choose one of
                 "{{preset_with_weights_names}}".
             load_weights: Whether to load pre-trained weights into model.
                 Defaults to `None`, which follows whether the preset has
@@ -148,14 +153,28 @@
             cache_subdir=os.path.join("models", preset),
             file_hash=metadata["weights_hash"],
         )
 
         model.load_weights(weights)
         return model
 
+    @property
+    def layers(self):
+        # Some of our task models don't use the Backbone directly, but create
+        # a feature extractor from it. In these cases, we don't want to count
+        # the `backbone` as a layer, because it will be included in the model
+        # summary and saves weights despite not being part of the model graph.
+        layers = super().layers
+        if hasattr(self, "_backbone") and self.backbone in layers:
+            # We know that the backbone is not part of the graph if it has no
+            # inbound nodes.
+            if len(self.backbone._inbound_nodes) == 0:
+                layers.remove(self.backbone)
+        return layers
+
     def __init_subclass__(cls, **kwargs):
         # Use __init_subclass__ to set up a correct docstring for from_preset.
         super().__init_subclass__(**kwargs)
 
         # If the subclass does not define from_preset, assign a wrapper so that
         # each class can have a distinct docstring.
         if "from_preset" not in cls.__dict__:
```

## keras_cv/models/utils.py

```diff
@@ -10,19 +10,45 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 """Utility functions for models"""
 
+from keras import backend
 from keras import layers
 from tensorflow import keras
 
 
 def parse_model_inputs(input_shape, input_tensor):
     if input_tensor is None:
         return layers.Input(shape=input_shape)
     else:
         if not keras.backend.is_keras_tensor(input_tensor):
             return layers.Input(tensor=input_tensor, shape=input_shape)
         else:
             return input_tensor
+
+
+def correct_pad_downsample(inputs, kernel_size):
+    """Returns a tuple for zero-padding for 2D convolution with downsampling.
+
+    Args:
+        inputs: Input tensor.
+        kernel_size: An integer or tuple/list of 2 integers.
+
+    Returns:
+        A tuple.
+    """
+    img_dim = 1
+    input_size = backend.int_shape(inputs)[img_dim : (img_dim + 2)]
+    if isinstance(kernel_size, int):
+        kernel_size = (kernel_size, kernel_size)
+    if input_size[0] is None:
+        adjust = (1, 1)
+    else:
+        adjust = (1 - input_size[0] % 2, 1 - input_size[1] % 2)
+    correct = (kernel_size[0] // 2, kernel_size[1] // 2)
+    return (
+        (correct[0] - adjust[0], correct[0]),
+        (correct[1] - adjust[1], correct[1]),
+    )
```

## keras_cv/models/backbones/backbone.py

```diff
@@ -44,32 +44,38 @@
     def from_config(cls, config):
         # The default `from_config()` for functional models will return a
         # vanilla `keras.Model`. We override it to get a subclass instance back.
         return cls(**config)
 
     @classproperty
     def presets(cls):
-        """Dictionary of preset names and configurations."""
+        """Dictionary of preset names and configs."""
         return {}
 
     @classproperty
     def presets_with_weights(cls):
-        """Dictionary of preset names and configurations that include
-        weights."""
+        """Dictionary of preset names and configs that include weights."""
         return {}
 
+    @classproperty
+    def presets_without_weights(cls):
+        """Dictionary of preset names and configs that don't include weights."""
+        return {
+            preset: cls.presets[preset]
+            for preset in set(cls.presets) - set(cls.presets_with_weights)
+        }
+
     @classmethod
     def from_preset(
         cls,
         preset,
         load_weights=None,
         **kwargs,
     ):
-        """Instantiate {{model_name}} model from preset architecture and
-        weights.
+        """Instantiate {{model_name}} model from preset config and weights.
 
         Args:
             preset: string. Must be one of "{{preset_names}}".
                 If looking for a preset with pretrained weights, choose one of
                 "{{preset_with_weights_names}}".
             load_weights: Whether to load pre-trained weights into model.
                 Defaults to `None`, which follows whether the preset has
@@ -154,26 +160,26 @@
                 preset_with_weights_names='", "'.join(cls.presets_with_weights),
             )(cls.from_preset.__func__)
 
     @property
     def pyramid_level_inputs(self):
         """Intermediate model outputs for feature extraction.
 
-        Format is a dictionary with int as key and layer name as value.
-        The int key represent the level of the feature output. A typical feature
-        pyramid has five levels corresponding to scales P3, P4, P5, P6, P7 in
-        the backbone. Scale Pn represents a feature map 2^n times smaller in
-        width and height than the input image.
+        Format is a dictionary with string as key and layer name as value.
+        The string key represents the level of the feature output. A typical
+        feature pyramid has five levels corresponding to scales "P3", "P4",
+        "P5", "P6", "P7" in the backbone. Scale Pn represents a feature map 2^n
+        times smaller in width and height than the input image.
 
         Example:
         ```python
         {
-            3: 'v2_stack_1_block4_out',
-            4: 'v2_stack_2_block6_out',
-            5: 'v2_stack_3_block3_out',
+            'P3': 'v2_stack_1_block4_out',
+            'P4': 'v2_stack_2_block6_out',
+            'P5': 'v2_stack_3_block3_out',
         }
         ```
         """
         return self._pyramid_level_inputs
 
     @pyramid_level_inputs.setter
     def pyramid_level_inputs(self, value):
```

## keras_cv/models/backbones/backbone_presets.py

```diff
@@ -11,34 +11,40 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """All Backbone presets"""
 
 from keras_cv.models.backbones.csp_darknet import csp_darknet_backbone_presets
+from keras_cv.models.backbones.densenet import densenet_backbone_presets
 from keras_cv.models.backbones.efficientnet_v2 import (
     efficientnet_v2_backbone_presets,
 )
 from keras_cv.models.backbones.mobilenet_v3 import mobilenet_v3_backbone_presets
 from keras_cv.models.backbones.resnet_v1 import resnet_v1_backbone_presets
 from keras_cv.models.backbones.resnet_v2 import resnet_v2_backbone_presets
+from keras_cv.models.object_detection.yolo_v8 import yolo_v8_backbone_presets
 
 backbone_presets_no_weights = {
     **resnet_v1_backbone_presets.backbone_presets_no_weights,
     **resnet_v2_backbone_presets.backbone_presets_no_weights,
     **mobilenet_v3_backbone_presets.backbone_presets_no_weights,
     **csp_darknet_backbone_presets.backbone_presets_no_weights,
     **efficientnet_v2_backbone_presets.backbone_presets_no_weights,
+    **densenet_backbone_presets.backbone_presets_no_weights,
+    **yolo_v8_backbone_presets.backbone_presets_no_weights,
 }
 
 backbone_presets_with_weights = {
     **resnet_v1_backbone_presets.backbone_presets_with_weights,
     **resnet_v2_backbone_presets.backbone_presets_with_weights,
     **mobilenet_v3_backbone_presets.backbone_presets_with_weights,
     **csp_darknet_backbone_presets.backbone_presets_with_weights,
     **efficientnet_v2_backbone_presets.backbone_presets_with_weights,
+    **densenet_backbone_presets.backbone_presets_with_weights,
+    **yolo_v8_backbone_presets.backbone_presets_with_weights,
 }
 
 backbone_presets = {
     **backbone_presets_no_weights,
     **backbone_presets_with_weights,
 }
```

## keras_cv/models/backbones/csp_darknet/csp_darknet_backbone.py

```diff
@@ -140,15 +140,15 @@
             x = CrossStagePartial(
                 channels,
                 num_bottlenecks=depth,
                 use_depthwise=use_depthwise,
                 residual=(index != len(stackwise_depth) - 1),
                 name=f"dark{index + 2}_csp",
             )(x)
-            pyramid_level_inputs[index + 2] = x.node.layer.name
+            pyramid_level_inputs[f"P{index + 2}"] = x.node.layer.name
 
         super().__init__(inputs=inputs, outputs=x, **kwargs)
         self.pyramid_level_inputs = pyramid_level_inputs
 
         self.stackwise_channels = stackwise_channels
         self.stackwise_depth = stackwise_depth
         self.include_rescaling = include_rescaling
@@ -263,14 +263,20 @@
         return CSPDarkNetBackbone.from_preset("csp_darknet_s", **kwargs)
 
     @classproperty
     def presets(cls):
         """Dictionary of preset names and configurations."""
         return {}
 
+    @classproperty
+    def presets_with_weights(cls):
+        """Dictionary of preset names and configurations that include
+        weights."""
+        return {}
+
 
 class CSPDarkNetMBackbone(CSPDarkNetBackbone):
     def __new__(
         cls,
         include_rescaling=True,
         input_shape=(None, None, 3),
         input_tensor=None,
@@ -287,14 +293,20 @@
         return CSPDarkNetBackbone.from_preset("csp_darknet_m", **kwargs)
 
     @classproperty
     def presets(cls):
         """Dictionary of preset names and configurations."""
         return {}
 
+    @classproperty
+    def presets_with_weights(cls):
+        """Dictionary of preset names and configurations that include
+        weights."""
+        return {}
+
 
 class CSPDarkNetLBackbone(CSPDarkNetBackbone):
     def __new__(
         cls,
         include_rescaling=True,
         input_shape=(None, None, 3),
         input_tensor=None,
@@ -345,14 +357,20 @@
         return CSPDarkNetBackbone.from_preset("csp_darknet_xl", **kwargs)
 
     @classproperty
     def presets(cls):
         """Dictionary of preset names and configurations."""
         return {}
 
+    @classproperty
+    def presets_with_weights(cls):
+        """Dictionary of preset names and configurations that include
+        weights."""
+        return {}
+
 
 setattr(
     CSPDarkNetTinyBackbone,
     "__doc__",
     ALIAS_DOCSTRING.format(
         name="Tiny",
         stackwise_channels="[48, 96, 192, 384]",
```

## keras_cv/models/backbones/csp_darknet/csp_darknet_backbone_test.py

```diff
@@ -10,14 +10,15 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import os
 
+import pytest
 import tensorflow as tf
 from absl.testing import parameterized
 from tensorflow import keras
 
 from keras_cv.models.backbones.csp_darknet import csp_darknet_backbone
 from keras_cv.utils.train import get_feature_extractor
 
@@ -46,14 +47,15 @@
         )
         model(self.input_batch)
 
     @parameterized.named_parameters(
         ("tf_format", "tf", "model"),
         ("keras_format", "keras_v3", "model.keras"),
     )
+    @pytest.mark.large  # Saving is slow, so mark these large.
     def test_saved_model(self, save_format, filename):
         model = csp_darknet_backbone.CSPDarkNetBackbone(
             stackwise_channels=[48, 96, 192, 384],
             stackwise_depth=[1, 3, 3, 1],
             include_rescaling=True,
         )
         model_output = model(self.input_batch)
@@ -70,14 +72,15 @@
         restored_output = restored_model(self.input_batch)
         self.assertAllClose(model_output, restored_output)
 
     @parameterized.named_parameters(
         ("tf_format", "tf", "model"),
         ("keras_format", "keras_v3", "model.keras"),
     )
+    @pytest.mark.large  # Saving is slow, so mark these large.
     def test_saved_alias_model(self, save_format, filename):
         model = csp_darknet_backbone.CSPDarkNetLBackbone()
         model_output = model(self.input_batch)
         save_path = os.path.join(self.get_temp_dir(), filename)
         model.save(save_path, save_format=save_format)
         restored_model = keras.models.load_model(save_path)
 
@@ -87,56 +90,62 @@
             restored_model, csp_darknet_backbone.CSPDarkNetBackbone
         )
 
         # Check that output matches.
         restored_output = restored_model(self.input_batch)
         self.assertAllClose(model_output, restored_output)
 
-    def test_create_backbone_model_from_alias_model(self):
-        model = csp_darknet_backbone.CSPDarkNetLBackbone(
-            include_rescaling=False
-        )
+    def test_feature_pyramid_inputs(self):
+        model = csp_darknet_backbone.CSPDarkNetLBackbone()
         backbone_model = get_feature_extractor(
             model,
             model.pyramid_level_inputs.values(),
             model.pyramid_level_inputs.keys(),
         )
-        inputs = keras.Input(shape=[224, 224, 3])
+        input_size = 256
+        inputs = tf.keras.Input(shape=[input_size, input_size, 3])
         outputs = backbone_model(inputs)
-        # CSPDarkNet backbone has 4 level of features (2 ~ 5)
-        self.assertLen(outputs, 4)
-        self.assertEquals(list(outputs.keys()), [2, 3, 4, 5])
-        self.assertEquals(outputs[2].shape, [None, 56, 56, 128])
-        self.assertEquals(outputs[3].shape, [None, 28, 28, 256])
-        self.assertEquals(outputs[4].shape, [None, 14, 14, 512])
-        self.assertEquals(outputs[5].shape, [None, 7, 7, 1024])
-
-    def test_create_backbone_model_with_level_config(self):
-        model = csp_darknet_backbone.CSPDarkNetBackbone(
-            stackwise_channels=[48, 96, 192, 384],
-            stackwise_depth=[1, 3, 3, 1],
-            include_rescaling=True,
+        levels = ["P2", "P3", "P4", "P5"]
+        self.assertEquals(list(outputs.keys()), levels)
+        self.assertEquals(
+            outputs["P2"].shape,
+            [None, input_size // 2**2, input_size // 2**2, 128],
+        )
+        self.assertEquals(
+            outputs["P3"].shape,
+            [None, input_size // 2**3, input_size // 2**3, 256],
+        )
+        self.assertEquals(
+            outputs["P4"].shape,
+            [None, input_size // 2**4, input_size // 2**4, 512],
+        )
+        self.assertEquals(
+            outputs["P5"].shape,
+            [None, input_size // 2**5, input_size // 2**5, 1024],
         )
-        levels = [3, 4]
-        layer_names = [model.pyramid_level_inputs[level] for level in [3, 4]]
-        backbone_model = get_feature_extractor(model, layer_names, levels)
-        inputs = keras.Input(shape=[256, 256, 3])
-        outputs = backbone_model(inputs)
-        self.assertLen(outputs, 2)
-        self.assertEquals(list(outputs.keys()), [3, 4])
-        self.assertEquals(outputs[3].shape, [None, 32, 32, 96])
-        self.assertEquals(outputs[4].shape, [None, 16, 16, 192])
 
     @parameterized.named_parameters(
         ("Tiny", csp_darknet_backbone.CSPDarkNetTinyBackbone),
         ("S", csp_darknet_backbone.CSPDarkNetSBackbone),
         ("M", csp_darknet_backbone.CSPDarkNetMBackbone),
         ("L", csp_darknet_backbone.CSPDarkNetLBackbone),
         ("XL", csp_darknet_backbone.CSPDarkNetXLBackbone),
     )
     def test_specific_arch_forward_pass(self, arch_class):
         backbone = arch_class()
         backbone(tf.random.uniform(shape=[2, 256, 256, 3]))
 
+    @parameterized.named_parameters(
+        ("Tiny", csp_darknet_backbone.CSPDarkNetTinyBackbone),
+        ("S", csp_darknet_backbone.CSPDarkNetSBackbone),
+        ("M", csp_darknet_backbone.CSPDarkNetMBackbone),
+        ("L", csp_darknet_backbone.CSPDarkNetLBackbone),
+        ("XL", csp_darknet_backbone.CSPDarkNetXLBackbone),
+    )
+    def test_specific_arch_presets(self, arch_class):
+        self.assertDictEqual(
+            arch_class.presets, arch_class.presets_with_weights
+        )
+
 
 if __name__ == "__main__":
     tf.test.main()
```

## keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone.py

```diff
@@ -253,15 +253,15 @@
         self.depth_coefficient = depth_coefficient
         self.skip_connection_dropout = skip_connection_dropout
         self.depth_divisor = depth_divisor
         self.min_depth = min_depth
         self.activation = activation
         self.input_tensor = input_tensor
         self.pyramid_level_inputs = {
-            i + 1: name for i, name in enumerate(pyramid_level_inputs)
+            f"P{i + 1}": name for i, name in enumerate(pyramid_level_inputs)
         }
         self.stackwise_kernel_sizes = stackwise_kernel_sizes
         self.stackwise_num_repeats = stackwise_num_repeats
         self.stackwise_input_filters = stackwise_input_filters
         self.stackwise_output_filters = stackwise_output_filters
         self.stackwise_expansion_ratios = stackwise_expansion_ratios
         self.stackwise_squeeze_and_excite_ratios = (
```

## keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_presets_test.py

```diff
@@ -44,16 +44,16 @@
         model(input_data)
 
     def test_efficientnet_feature_extractor(self):
         model = EfficientNetV2SBackbone(
             include_rescaling=False,
             input_shape=[256, 256, 3],
         )
-        levels = [3, 4]
-        layer_names = [model.pyramid_level_inputs[level] for level in [3, 4]]
+        levels = ["P3", "P4"]
+        layer_names = [model.pyramid_level_inputs[level] for level in levels]
         backbone_model = get_feature_extractor(model, layer_names, levels)
         inputs = tf.keras.Input(shape=[256, 256, 3])
         outputs = backbone_model(inputs)
         self.assertLen(outputs, 2)
-        self.assertEquals(list(outputs.keys()), [3, 4])
-        self.assertEquals(outputs[3].shape[:3], [None, 32, 32])
-        self.assertEquals(outputs[4].shape[:3], [None, 16, 16])
+        self.assertEquals(list(outputs.keys()), levels)
+        self.assertEquals(outputs["P3"].shape[:3], [None, 32, 32])
+        self.assertEquals(outputs["P4"].shape[:3], [None, 16, 16])
```

## keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_test.py

```diff
@@ -10,14 +10,15 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import os
 
+import pytest
 import tensorflow as tf
 from absl.testing import parameterized
 from tensorflow import keras
 
 from keras_cv.models.backbones.efficientnet_v2.efficientnet_v2_aliases import (
     EfficientNetV2SBackbone,
 )
@@ -81,14 +82,15 @@
         )
         model(self.input_batch)
 
     @parameterized.named_parameters(
         ("tf_format", "tf", "model"),
         ("keras_format", "keras_v3", "model.keras"),
     )
+    @pytest.mark.large  # Saving is slow, so mark these large.
     def test_saved_model(self, save_format, filename):
         model = EfficientNetV2Backbone(
             stackwise_kernel_sizes=[3, 3, 3, 3, 3, 3],
             stackwise_num_repeats=[2, 4, 4, 6, 9, 15],
             stackwise_input_filters=[24, 24, 48, 64, 128, 160],
             stackwise_output_filters=[24, 48, 64, 128, 160, 256],
             stackwise_expansion_ratios=[1, 4, 4, 4, 6, 6],
@@ -118,14 +120,15 @@
         restored_output = restored_model(self.input_batch)
         self.assertAllClose(model_output, restored_output)
 
     @parameterized.named_parameters(
         ("tf_format", "tf", "model"),
         ("keras_format", "keras_v3", "model.keras"),
     )
+    @pytest.mark.large  # Saving is slow, so mark these large.
     def test_saved_alias_model(self, save_format, filename):
         model = EfficientNetV2SBackbone()
         model_output = model(self.input_batch)
         save_path = os.path.join(self.get_temp_dir(), filename)
         model.save(save_path, save_format=save_format)
         restored_model = keras.models.load_model(save_path)
 
@@ -133,63 +136,46 @@
         # Note that these aliases serialized as the base class
         self.assertIsInstance(restored_model, EfficientNetV2Backbone)
 
         # Check that output matches.
         restored_output = restored_model(self.input_batch)
         self.assertAllClose(model_output, restored_output)
 
-    def test_create_backbone_model_from_alias_model(self):
-        model = EfficientNetV2SBackbone(
-            include_rescaling=False,
-        )
+    def test_feature_pyramid_inputs(self):
+        model = EfficientNetV2SBackbone()
         backbone_model = get_feature_extractor(
             model,
             model.pyramid_level_inputs.values(),
             model.pyramid_level_inputs.keys(),
         )
-        inputs = keras.Input(shape=[256, 256, 3])
+        input_size = 256
+        inputs = tf.keras.Input(shape=[input_size, input_size, 3])
         outputs = backbone_model(inputs)
-        # EfficientNetV2S backbone has 4 level of features (1 ~ 5)
-        self.assertLen(outputs, 5)
-        self.assertEquals(list(outputs.keys()), [1, 2, 3, 4, 5])
-        self.assertEquals(outputs[2].shape, [None, 64, 64, 48])
-        self.assertEquals(outputs[3].shape, [None, 32, 32, 64])
-        self.assertEquals(outputs[4].shape, [None, 16, 16, 160])
-        self.assertEquals(outputs[5].shape, [None, 8, 8, 1280])
-
-    def test_create_backbone_model_with_level_config(self):
-        model = EfficientNetV2Backbone(
-            stackwise_kernel_sizes=[3, 3, 3, 3, 3, 3],
-            stackwise_num_repeats=[2, 4, 4, 6, 9, 15],
-            stackwise_input_filters=[24, 24, 48, 64, 128, 160],
-            stackwise_output_filters=[24, 48, 64, 128, 160, 256],
-            stackwise_expansion_ratios=[1, 4, 4, 4, 6, 6],
-            stackwise_squeeze_and_excite_ratios=[0.0, 0.0, 0, 0.25, 0.25, 0.25],
-            stackwise_strides=[1, 2, 2, 2, 1, 2],
-            stackwise_conv_types=[
-                "fused",
-                "fused",
-                "fused",
-                "unfused",
-                "unfused",
-                "unfused",
-            ],
-            width_coefficient=1.0,
-            depth_coefficient=1.0,
-            include_rescaling=True,
+        levels = ["P1", "P2", "P3", "P4", "P5"]
+        self.assertEquals(list(outputs.keys()), levels)
+        self.assertEquals(
+            outputs["P1"].shape,
+            [None, input_size // 2**1, input_size // 2**1, 24],
+        )
+        self.assertEquals(
+            outputs["P2"].shape,
+            [None, input_size // 2**2, input_size // 2**2, 48],
+        )
+        self.assertEquals(
+            outputs["P3"].shape,
+            [None, input_size // 2**3, input_size // 2**3, 64],
+        )
+        self.assertEquals(
+            outputs["P4"].shape,
+            [None, input_size // 2**4, input_size // 2**4, 160],
+        )
+        self.assertEquals(
+            outputs["P5"].shape,
+            [None, input_size // 2**5, input_size // 2**5, 1280],
         )
-        levels = [3, 4]
-        layer_names = [model.pyramid_level_inputs[level] for level in [3, 4]]
-        backbone_model = get_feature_extractor(model, layer_names, levels)
-        inputs = keras.Input(shape=[256, 256, 3])
-        outputs = backbone_model(inputs)
-        self.assertLen(outputs, 2)
-        self.assertEquals(list(outputs.keys()), [3, 4])
-        self.assertEquals(outputs[3].shape, [None, 32, 32, 64])
-        self.assertEquals(outputs[4].shape, [None, 16, 16, 160])
 
     @parameterized.named_parameters(
         ("one_channel", 1),
         ("four_channels", 4),
     )
     def test_application_variable_input_channels(self, num_channels):
         model = EfficientNetV2Backbone(
```

## keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone.py

```diff
@@ -21,15 +21,14 @@
 """  # noqa: E501
 
 import copy
 
 from tensorflow import keras
 from tensorflow.keras import backend
 from tensorflow.keras import layers
-from tensorflow.keras.utils import custom_object_scope
 
 from keras_cv import layers as cv_layers
 from keras_cv.models import utils
 from keras_cv.models.backbones.backbone import Backbone
 from keras_cv.models.backbones.mobilenet_v3.mobilenet_v3_backbone_presets import (  # noqa: E501
     backbone_presets,
 )
@@ -39,182 +38,14 @@
 from keras_cv.utils.python_utils import classproperty
 
 CHANNEL_AXIS = -1
 BN_EPSILON = 1e-3
 BN_MOMENTUM = 0.999
 
 
-def correct_pad_downsample(inputs, kernel_size):
-    """Returns a tuple for zero-padding for 2D convolution with downsampling.
-
-    Args:
-      inputs: Input tensor.
-      kernel_size: An integer
-
-    Returns:
-      A tuple.
-    """
-    input_size = backend.int_shape(inputs)[1:3]
-    kernel_size = (kernel_size, kernel_size)
-
-    if input_size[0] is None:
-        adjust = (1, 1)
-    else:
-        adjust = (1 - input_size[0] % 2, 1 - input_size[1] % 2)
-
-    correct = (kernel_size[0] // 2, kernel_size[1] // 2)
-
-    return (
-        (correct[0] - adjust[0], correct[0]),
-        (correct[1] - adjust[1], correct[1]),
-    )
-
-
-def adjust_channels(x, divisor=8, min_value=None):
-    """Ensure that all layers have a channel number divisible by the `divisor`.
-
-    Args:
-        x: integer, input value.
-        divisor: integer, the value by which a channel number should be
-            divisible, defaults to 8.
-        min_value: float, optional minimum value for the new tensor. If None,
-            defaults to value of divisor.
-
-    Returns:
-        the updated input scalar.
-    """
-
-    if min_value is None:
-        min_value = divisor
-
-    new_x = max(min_value, int(x + divisor / 2) // divisor * divisor)
-
-    # make sure that round down does not go down by more than 10%.
-    if new_x < 0.9 * x:
-        new_x += divisor
-    return new_x
-
-
-def apply_hard_sigmoid(x):
-    activation = layers.ReLU(6.0)
-    return activation(x + 3.0) * (1.0 / 6.0)
-
-
-def apply_hard_swish(x):
-    return layers.Multiply()([x, apply_hard_sigmoid(x)])
-
-
-def apply_inverted_res_block(
-    x,
-    expansion,
-    filters,
-    kernel_size,
-    stride,
-    se_ratio,
-    activation,
-    expansion_index,
-):
-    """An Inverted Residual Block.
-
-    Args:
-        x: input tensor.
-        expansion: integer, the expansion ratio, multiplied with infilters to
-            get the minimum value passed to adjust_channels.
-        filters: integer, number of filters for convolution layer.
-        kernel_size: integer, the kernel size for DepthWise Convolutions.
-        stride: integer, the stride length for DepthWise Convolutions.
-        se_ratio: float, ratio for bottleneck filters. Number of bottleneck
-            filters = filters * se_ratio.
-        activation: the activation layer to use.
-        expansion_index: integer, a unique identification if you want to use
-            expanded convolutions. If greater than 0, an additional Conv+BN
-            layer is added after the expanded convolutional layer.
-
-    Returns:
-        the updated input tensor.
-    """
-    if isinstance(activation, str):
-        if activation == "hard_swish":
-            activation = apply_hard_swish
-        else:
-            activation = keras.activations.get(activation)
-
-    shortcut = x
-    prefix = "expanded_conv/"
-    infilters = backend.int_shape(x)[CHANNEL_AXIS]
-
-    if expansion_index > 0:
-        prefix = f"expanded_conv_{expansion_index}/"
-
-        x = layers.Conv2D(
-            adjust_channels(infilters * expansion),
-            kernel_size=1,
-            padding="same",
-            use_bias=False,
-            name=prefix + "expand",
-        )(x)
-        x = layers.BatchNormalization(
-            axis=CHANNEL_AXIS,
-            epsilon=BN_EPSILON,
-            momentum=BN_MOMENTUM,
-            name=prefix + "expand/BatchNorm",
-        )(x)
-        x = activation(x)
-
-    if stride == 2:
-        x = layers.ZeroPadding2D(
-            padding=correct_pad_downsample(x, kernel_size),
-            name=prefix + "depthwise/pad",
-        )(x)
-
-    x = layers.DepthwiseConv2D(
-        kernel_size,
-        strides=stride,
-        padding="same" if stride == 1 else "valid",
-        use_bias=False,
-        name=prefix + "depthwise",
-    )(x)
-    x = layers.BatchNormalization(
-        axis=CHANNEL_AXIS,
-        epsilon=BN_EPSILON,
-        momentum=BN_MOMENTUM,
-        name=prefix + "depthwise/BatchNorm",
-    )(x)
-    x = activation(x)
-
-    if se_ratio:
-        with custom_object_scope({"hard_sigmoid": apply_hard_sigmoid}):
-            se_filters = adjust_channels(infilters * expansion)
-            x = cv_layers.SqueezeAndExcite2D(
-                filters=se_filters,
-                bottleneck_filters=adjust_channels(se_filters * se_ratio),
-                squeeze_activation="relu",
-                excite_activation="hard_sigmoid",
-            )(x)
-
-    x = layers.Conv2D(
-        filters,
-        kernel_size=1,
-        padding="same",
-        use_bias=False,
-        name=prefix + "project",
-    )(x)
-    x = layers.BatchNormalization(
-        axis=CHANNEL_AXIS,
-        epsilon=BN_EPSILON,
-        momentum=BN_MOMENTUM,
-        name=prefix + "project/BatchNorm",
-    )(x)
-
-    if stride == 1 and infilters == filters:
-        x = layers.Add(name=prefix + "Add")([shortcut, x])
-
-    return x
-
-
 @keras.utils.register_keras_serializable(package="keras_cv.models")
 class MobileNetV3Backbone(Backbone):
     """Instantiates the MobileNetV3 architecture.
 
     References:
         - [Searching for MobileNetV3](https://arxiv.org/pdf/1905.02244.pdf)
         (ICCV 2019)
@@ -297,29 +128,31 @@
             axis=CHANNEL_AXIS,
             epsilon=BN_EPSILON,
             momentum=BN_MOMENTUM,
             name="Conv/BatchNorm",
         )(x)
         x = apply_hard_swish(x)
 
-        pyramid_level_inputs = {}
+        pyramid_level_inputs = []
         for stack_index in range(len(stackwise_filters)):
+            if stackwise_stride[stack_index] != 1:
+                pyramid_level_inputs.append(x.node.layer.name)
             x = apply_inverted_res_block(
                 x,
                 expansion=stackwise_expansion[stack_index],
                 filters=adjust_channels(
                     (stackwise_filters[stack_index]) * alpha
                 ),
                 kernel_size=stackwise_kernel_size[stack_index],
                 stride=stackwise_stride[stack_index],
                 se_ratio=stackwise_se_ratio[stack_index],
                 activation=stackwise_activation[stack_index],
                 expansion_index=stack_index,
             )
-            pyramid_level_inputs[stack_index + 2] = x.node.layer.name
+        pyramid_level_inputs.append(x.node.layer.name)
 
         last_conv_ch = adjust_channels(backend.int_shape(x)[CHANNEL_AXIS] * 6)
 
         x = layers.Conv2D(
             last_conv_ch,
             kernel_size=1,
             padding="same",
@@ -332,15 +165,17 @@
             momentum=BN_MOMENTUM,
             name="Conv_1/BatchNorm",
         )(x)
         x = apply_hard_swish(x)
 
         super().__init__(inputs=inputs, outputs=x, **kwargs)
 
-        self.pyramid_level_inputs = pyramid_level_inputs
+        self.pyramid_level_inputs = {
+            f"P{i + 1}": name for i, name in enumerate(pyramid_level_inputs)
+        }
         self.stackwise_expansion = stackwise_expansion
         self.stackwise_filters = stackwise_filters
         self.stackwise_kernel_size = stackwise_kernel_size
         self.stackwise_stride = stackwise_stride
         self.stackwise_se_ratio = stackwise_se_ratio
         self.stackwise_activation = stackwise_activation
         self.include_rescaling = include_rescaling
@@ -373,108 +208,157 @@
     @classproperty
     def presets_with_weights(cls):
         """Dictionary of preset names and configurations that include
         weights."""
         return copy.deepcopy(backbone_presets_with_weights)
 
 
-ALIAS_DOCSTRING = """MobileNetV3Backbone model with {num_layers} layers.
+class HardSigmoidActivation(layers.Layer):
+    def __init__(self):
+        super().__init__()
 
-    References:
-        - [Searching for MobileNetV3](https://arxiv.org/abs/1905.02244)
-        - [Based on the Original keras.applications MobileNetv3](https://github.com/keras-team/keras/blob/master/keras/applications/mobilenet_v3.py)
+    def call(self, x):
+        return apply_hard_sigmoid(x)
 
-    For transfer learning use cases, make sure to read the
-    [guide to transfer learning & fine-tuning](https://keras.io/guides/transfer_learning/).
+    def get_config(self):
+        return super().get_config()
+
+
+def adjust_channels(x, divisor=8, min_value=None):
+    """Ensure that all layers have a channel number divisible by the `divisor`.
 
     Args:
-        include_rescaling: bool, whether to rescale the inputs. If set to
-            True, inputs will be passed through a `Rescaling(scale=1 / 255)`
-            layer. Defaults to True.
-        input_shape: optional shape tuple, defaults to (None, None, 3).
-        input_tensor: optional Keras tensor (i.e., output of `layers.Input()`)
-            to use as image input for the model.
+        x: integer, input value.
+        divisor: integer, the value by which a channel number should be
+            divisible, defaults to 8.
+        min_value: float, optional minimum value for the new tensor. If None,
+            defaults to value of divisor.
 
-    Examples:
-    ```python
-    input_data = tf.ones(shape=(8, 224, 224, 3))
+    Returns:
+        the updated input scalar.
+    """
 
-    # Randomly initialized backbone
-    model = {name}Backbone()
-    output = model(input_data)
-    ```
-"""  # noqa: E501
+    if min_value is None:
+        min_value = divisor
 
+    new_x = max(min_value, int(x + divisor / 2) // divisor * divisor)
 
-class MobileNetV3SmallBackbone(MobileNetV3Backbone):
-    def __new__(
-        cls,
-        include_rescaling=True,
-        input_shape=(None, None, 3),
-        input_tensor=None,
-        **kwargs,
-    ):
-        # Pack args in kwargs
-        kwargs.update(
-            {
-                "include_rescaling": include_rescaling,
-                "input_shape": input_shape,
-                "input_tensor": input_tensor,
-            }
-        )
-        return MobileNetV3Backbone.from_preset("mobilenet_v3_small", **kwargs)
+    # make sure that round down does not go down by more than 10%.
+    if new_x < 0.9 * x:
+        new_x += divisor
+    return new_x
 
-    @classproperty
-    def presets(cls):
-        """Dictionary of preset names and configurations."""
-        return {}
 
-    @classproperty
-    def presets_with_weights(cls):
-        """Dictionary of preset names and configurations."""
-        return {}
+def apply_hard_sigmoid(x):
+    activation = layers.ReLU(6.0)
+    return activation(x + 3.0) * (1.0 / 6.0)
 
 
-class MobileNetV3LargeBackbone(MobileNetV3Backbone):
-    def __new__(
-        cls,
-        include_rescaling=True,
-        input_shape=(None, None, 3),
-        input_tensor=None,
-        **kwargs,
-    ):
-        # Pack args in kwargs
-        kwargs.update(
-            {
-                "include_rescaling": include_rescaling,
-                "input_shape": input_shape,
-                "input_tensor": input_tensor,
-            }
-        )
-        return MobileNetV3Backbone.from_preset("mobilenet_v3_large", **kwargs)
+def apply_hard_swish(x):
+    return layers.Multiply()([x, apply_hard_sigmoid(x)])
 
-    @classproperty
-    def presets(cls):
-        """Dictionary of preset names and configurations."""
-        return {
-            "mobilenet_v3_large_imagenet": copy.deepcopy(
-                backbone_presets["mobilenet_v3_large_imagenet"]
-            ),
-        }
 
-    @classproperty
-    def presets_with_weights(cls):
-        """Dictionary of preset names and configurations that include
-        weights."""
-        return cls.presets
+def apply_inverted_res_block(
+    x,
+    expansion,
+    filters,
+    kernel_size,
+    stride,
+    se_ratio,
+    activation,
+    expansion_index,
+):
+    """An Inverted Residual Block.
 
+    Args:
+        x: input tensor.
+        expansion: integer, the expansion ratio, multiplied with infilters to
+            get the minimum value passed to adjust_channels.
+        filters: integer, number of filters for convolution layer.
+        kernel_size: integer, the kernel size for DepthWise Convolutions.
+        stride: integer, the stride length for DepthWise Convolutions.
+        se_ratio: float, ratio for bottleneck filters. Number of bottleneck
+            filters = filters * se_ratio.
+        activation: the activation layer to use.
+        expansion_index: integer, a unique identification if you want to use
+            expanded convolutions. If greater than 0, an additional Conv+BN
+            layer is added after the expanded convolutional layer.
 
-setattr(
-    MobileNetV3LargeBackbone,
-    "__doc__",
-    ALIAS_DOCSTRING.format(name="MobileNetV3Large", num_layers="28"),
-)
-setattr(
-    MobileNetV3SmallBackbone,
-    "__doc__",
-    ALIAS_DOCSTRING.format(name="MobileNetV3Small", num_layers="14"),
-)
+    Returns:
+        the updated input tensor.
+    """
+    if isinstance(activation, str):
+        if activation == "hard_swish":
+            activation = apply_hard_swish
+        else:
+            activation = keras.activations.get(activation)
+
+    shortcut = x
+    prefix = "expanded_conv/"
+    infilters = backend.int_shape(x)[CHANNEL_AXIS]
+
+    if expansion_index > 0:
+        prefix = f"expanded_conv_{expansion_index}/"
+
+        x = layers.Conv2D(
+            adjust_channels(infilters * expansion),
+            kernel_size=1,
+            padding="same",
+            use_bias=False,
+            name=prefix + "expand",
+        )(x)
+        x = layers.BatchNormalization(
+            axis=CHANNEL_AXIS,
+            epsilon=BN_EPSILON,
+            momentum=BN_MOMENTUM,
+            name=prefix + "expand/BatchNorm",
+        )(x)
+        x = activation(x)
+
+    if stride == 2:
+        x = layers.ZeroPadding2D(
+            padding=utils.correct_pad_downsample(x, kernel_size),
+            name=prefix + "depthwise/pad",
+        )(x)
+
+    x = layers.DepthwiseConv2D(
+        kernel_size,
+        strides=stride,
+        padding="same" if stride == 1 else "valid",
+        use_bias=False,
+        name=prefix + "depthwise",
+    )(x)
+    x = layers.BatchNormalization(
+        axis=CHANNEL_AXIS,
+        epsilon=BN_EPSILON,
+        momentum=BN_MOMENTUM,
+        name=prefix + "depthwise/BatchNorm",
+    )(x)
+    x = activation(x)
+
+    if se_ratio:
+        se_filters = adjust_channels(infilters * expansion)
+        x = cv_layers.SqueezeAndExcite2D(
+            filters=se_filters,
+            bottleneck_filters=adjust_channels(se_filters * se_ratio),
+            squeeze_activation="relu",
+            excite_activation=HardSigmoidActivation(),
+        )(x)
+
+    x = layers.Conv2D(
+        filters,
+        kernel_size=1,
+        padding="same",
+        use_bias=False,
+        name=prefix + "project",
+    )(x)
+    x = layers.BatchNormalization(
+        axis=CHANNEL_AXIS,
+        epsilon=BN_EPSILON,
+        momentum=BN_MOMENTUM,
+        name=prefix + "project/BatchNorm",
+    )(x)
+
+    if stride == 1 and infilters == filters:
+        x = layers.Add(name=prefix + "Add")([shortcut, x])
+
+    return x
```

## keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_presets_test.py

```diff
@@ -19,14 +19,40 @@
 from absl.testing import parameterized
 
 from keras_cv.models.backbones.mobilenet_v3.mobilenet_v3_backbone import (
     MobileNetV3Backbone,
 )
 
 
+@pytest.mark.large
+class MobileNetV3PresetSmokeTest(tf.test.TestCase):
+    """
+    A smoke test for MobileNetV3 presets we run continuously.
+    This only tests the smallest weights we have available. Run with:
+    `pytest keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_presets_test.py --run_large`
+    """  # noqa: E501
+
+    def setUp(self):
+        self.input_batch = tf.ones(shape=(8, 224, 224, 3))
+
+    def test_backbone_output(self):
+        model = MobileNetV3Backbone.from_preset("mobilenet_v3_large_imagenet")
+        outputs = model(self.input_batch)
+
+        # The forward pass from a preset should be stable!
+        # This test should catch cases where we unintentionally change our
+        # network code in a way that would invalidate our preset weights.
+        # We should only update these numbers if we are updating a weights
+        # file, or have found a discrepancy with the upstream source.
+        outputs = outputs[0, 0, 0, :5]
+        expected = [0.27, 0.01, 0.29, 0.08, -0.12]
+        # Keep a high tolerance, so we are robust to different hardware.
+        self.assertAllClose(outputs, expected, atol=0.01, rtol=0.01)
+
+
 @pytest.mark.extra_large
 class MobileNetV3PresetFullTest(tf.test.TestCase, parameterized.TestCase):
     """
     Test the full enumeration of our preset.
     This tests every preset for MobileNetV3 and is only run manually.
     Run with:
     `pytest keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_presets_test.py --run_extra_large`
```

## keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_test.py

```diff
@@ -10,23 +10,24 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import os
 
+import pytest
 import tensorflow as tf
 from absl.testing import parameterized
 from tensorflow import keras
 
-from keras_cv.models.backbones.mobilenet_v3.mobilenet_v3_backbone import (
-    MobileNetV3Backbone,
+from keras_cv.models.backbones.mobilenet_v3.mobilenet_v3_aliases import (
+    MobileNetV3SmallBackbone,
 )
 from keras_cv.models.backbones.mobilenet_v3.mobilenet_v3_backbone import (
-    MobileNetV3SmallBackbone,
+    MobileNetV3Backbone,
 )
 from keras_cv.utils.train import get_feature_extractor
 
 
 class MobileNetV3BackboneTest(tf.test.TestCase, parameterized.TestCase):
     def setUp(self):
         self.input_batch = tf.ones(shape=(2, 224, 224, 3))
@@ -43,42 +44,61 @@
         )
         model(self.input_batch)
 
     @parameterized.named_parameters(
         ("tf_format", "tf", "model"),
         ("keras_format", "keras_v3", "model.keras"),
     )
+    @pytest.mark.large  # Saving is slow, so mark these large.
     def test_saved_model(self, save_format, filename):
         model = MobileNetV3SmallBackbone()
         model_output = model(self.input_batch)
         save_path = os.path.join(self.get_temp_dir(), filename)
         model.save(save_path, save_format=save_format)
         restored_model = keras.models.load_model(save_path)
 
         # Check we got the real object back.
         self.assertIsInstance(restored_model, MobileNetV3Backbone)
 
         # Check that output matches.
         restored_output = restored_model(self.input_batch)
         self.assertAllClose(model_output, restored_output)
 
-    def test_create_backbone_model_with_level_config(self):
-        model = MobileNetV3SmallBackbone(
-            include_rescaling=False,
-            input_shape=[256, 256, 3],
+    def test_feature_pyramid_inputs(self):
+        model = MobileNetV3SmallBackbone()
+        backbone_model = get_feature_extractor(
+            model,
+            model.pyramid_level_inputs.values(),
+            model.pyramid_level_inputs.keys(),
         )
-        levels = [4, 5]
-        layer_names = [model.pyramid_level_inputs[level] for level in [4, 5]]
-        backbone_model = get_feature_extractor(model, layer_names, levels)
-        inputs = tf.keras.Input(shape=[256, 256, 3])
+        input_size = 256
+        inputs = tf.keras.Input(shape=[input_size, input_size, 3])
         outputs = backbone_model(inputs)
-        self.assertLen(outputs, 2)
-        self.assertEquals(list(outputs.keys()), [4, 5])
-        self.assertEquals(outputs[4].shape, [None, 32, 32, 24])
-        self.assertEquals(outputs[5].shape, [None, 16, 16, 40])
+        levels = ["P1", "P2", "P3", "P4", "P5"]
+        self.assertEquals(list(outputs.keys()), levels)
+        self.assertEquals(
+            outputs["P1"].shape,
+            [None, input_size // 2**1, input_size // 2**1, 16],
+        )
+        self.assertEquals(
+            outputs["P2"].shape,
+            [None, input_size // 2**2, input_size // 2**2, 16],
+        )
+        self.assertEquals(
+            outputs["P3"].shape,
+            [None, input_size // 2**3, input_size // 2**3, 24],
+        )
+        self.assertEquals(
+            outputs["P4"].shape,
+            [None, input_size // 2**4, input_size // 2**4, 48],
+        )
+        self.assertEquals(
+            outputs["P5"].shape,
+            [None, input_size // 2**5, input_size // 2**5, 96],
+        )
 
     @parameterized.named_parameters(
         ("one_channel", 1),
         ("four_channels", 4),
     )
     def test_application_variable_input_channels(self, num_channels):
         model = MobileNetV3SmallBackbone(
```

## keras_cv/models/backbones/resnet_v1/resnet_v1_backbone.py

```diff
@@ -7,15 +7,15 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""ResNet models for KerasCV.
+"""ResNet backbone model.
 Reference:
   - [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)
     (CVPR 2015)
   - [Based on the original keras.applications ResNet](https://github.com/keras-team/keras/blob/master/keras/applications/resnet.py)  # noqa: E501
 """
 
 import copy
@@ -34,14 +34,150 @@
 )
 from keras_cv.utils.python_utils import classproperty
 
 BN_AXIS = 3
 BN_EPSILON = 1.001e-5
 
 
+@keras.utils.register_keras_serializable(package="keras_cv.models")
+class ResNetBackbone(Backbone):
+    """Instantiates the ResNet architecture.
+
+    Reference:
+        - [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)
+
+    The difference in ResNetV1 and ResNetV2 rests in the structure of their
+    individual building blocks. In ResNetV2, the batch normalization and
+    ReLU activation precede the convolution layers, as opposed to ResNetV1 where
+    the batch normalization and ReLU activation are applied after the
+    convolution layers.
+
+    For transfer learning use cases, make sure to read the
+    [guide to transfer learning & fine-tuning](https://keras.io/guides/transfer_learning/).
+
+    Args:
+        stackwise_filters: list of ints, number of filters for each stack in
+            the model.
+        stackwise_blocks: list of ints, number of blocks for each stack in the
+            model.
+        stackwise_strides: list of ints, stride for each stack in the model.
+        include_rescaling: bool, whether to rescale the inputs. If set
+            to `True`, inputs will be passed through a `Rescaling(1/255.0)`
+            layer.
+        input_shape: optional shape tuple, defaults to (None, None, 3).
+        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)
+            to use as image input for the model.
+        block_type: string, one of "basic_block" or "block". The block type to
+            stack. Use "basic_block" for ResNet18 and ResNet34.
+
+    Examples:
+    ```python
+    input_data = tf.ones(shape=(8, 224, 224, 3))
+
+    # Pretrained backbone
+    model = keras_cv.models.ResNetBackbone.from_preset("resnet50_imagenet")
+    output = model(input_data)
+
+    # Randomly initialized backbone with a custom config
+    model = ResNetBackbone(
+        stackwise_filters=[64, 128, 256, 512],
+        stackwise_blocks=[2, 2, 2, 2],
+        stackwise_strides=[1, 2, 2, 2],
+        include_rescaling=False,
+    )
+    output = model(input_data)
+    ```
+    """  # noqa: E501
+
+    def __init__(
+        self,
+        *,
+        stackwise_filters,
+        stackwise_blocks,
+        stackwise_strides,
+        include_rescaling,
+        input_shape=(None, None, 3),
+        input_tensor=None,
+        block_type="block",
+        **kwargs,
+    ):
+        inputs = utils.parse_model_inputs(input_shape, input_tensor)
+        x = inputs
+
+        if include_rescaling:
+            x = layers.Rescaling(1 / 255.0)(x)
+
+        x = layers.Conv2D(
+            64, 7, strides=2, use_bias=False, padding="same", name="conv1_conv"
+        )(x)
+
+        x = layers.BatchNormalization(
+            axis=BN_AXIS, epsilon=BN_EPSILON, name="conv1_bn"
+        )(x)
+        x = layers.Activation("relu", name="conv1_relu")(x)
+
+        x = layers.MaxPooling2D(
+            3, strides=2, padding="same", name="pool1_pool"
+        )(x)
+
+        num_stacks = len(stackwise_filters)
+
+        pyramid_level_inputs = {}
+        for stack_index in range(num_stacks):
+            x = apply_stack(
+                x,
+                filters=stackwise_filters[stack_index],
+                blocks=stackwise_blocks[stack_index],
+                stride=stackwise_strides[stack_index],
+                block_type=block_type,
+                first_shortcut=(block_type == "block" or stack_index > 0),
+                name=f"v2_stack_{stack_index}",
+            )
+            pyramid_level_inputs[f"P{stack_index + 2}"] = x.node.layer.name
+
+        # Create model.
+        super().__init__(inputs=inputs, outputs=x, **kwargs)
+
+        # All references to `self` below this line
+        self.pyramid_level_inputs = pyramid_level_inputs
+        self.stackwise_filters = stackwise_filters
+        self.stackwise_blocks = stackwise_blocks
+        self.stackwise_strides = stackwise_strides
+        self.include_rescaling = include_rescaling
+        self.input_tensor = input_tensor
+        self.block_type = block_type
+
+    def get_config(self):
+        config = super().get_config()
+        config.update(
+            {
+                "stackwise_filters": self.stackwise_filters,
+                "stackwise_blocks": self.stackwise_blocks,
+                "stackwise_strides": self.stackwise_strides,
+                "include_rescaling": self.include_rescaling,
+                # Remove batch dimension from `input_shape`
+                "input_shape": self.input_shape[1:],
+                "input_tensor": self.input_tensor,
+                "block_type": self.block_type,
+            }
+        )
+        return config
+
+    @classproperty
+    def presets(cls):
+        """Dictionary of preset names and configurations."""
+        return copy.deepcopy(backbone_presets)
+
+    @classproperty
+    def presets_with_weights(cls):
+        """Dictionary of preset names and configurations that include
+        weights."""
+        return copy.deepcopy(backbone_presets_with_weights)
+
+
 def apply_basic_block(
     x, filters, kernel_size=3, stride=1, conv_shortcut=True, name=None
 ):
     """A basic residual block (v1).
 
     Args:
         x: input tensor.
@@ -215,337 +351,7 @@
         conv_shortcut=first_shortcut,
     )
     for i in range(2, blocks + 1):
         x = block_fn(
             x, filters, conv_shortcut=False, name=name + "_block" + str(i)
         )
     return x
-
-
-@keras.utils.register_keras_serializable(package="keras_cv.models")
-class ResNetBackbone(Backbone):
-    """Instantiates the ResNet architecture.
-
-    Reference:
-        - [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)
-
-    The difference in ResNetV1 and ResNetV2 rests in the structure of their
-    individual building blocks. In ResNetV2, the batch normalization and
-    ReLU activation precede the convolution layers, as opposed to ResNetV1 where
-    the batch normalization and ReLU activation are applied after the
-    convolution layers.
-
-    For transfer learning use cases, make sure to read the
-    [guide to transfer learning & fine-tuning](https://keras.io/guides/transfer_learning/).
-
-    Args:
-        stackwise_filters: list of ints, number of filters for each stack in
-            the model.
-        stackwise_blocks: list of ints, number of blocks for each stack in the
-            model.
-        stackwise_strides: list of ints, stride for each stack in the model.
-        include_rescaling: bool, whether to rescale the inputs. If set
-            to `True`, inputs will be passed through a `Rescaling(1/255.0)`
-            layer.
-        input_shape: optional shape tuple, defaults to (None, None, 3).
-        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)
-            to use as image input for the model.
-        block_type: string, one of "basic_block" or "block". The block type to
-            stack. Use "basic_block" for ResNet18 and ResNet34.
-
-    Examples:
-    ```python
-    input_data = tf.ones(shape=(8, 224, 224, 3))
-
-    # Pretrained backbone
-    model = keras_cv.models.ResNetBackbone.from_preset("resnet50_imagenet")
-    output = model(input_data)
-
-    # Randomly initialized backbone with a custom config
-    model = ResNetBackbone(
-        stackwise_filters=[64, 128, 256, 512],
-        stackwise_blocks=[2, 2, 2, 2],
-        stackwise_strides=[1, 2, 2, 2],
-        include_rescaling=False,
-    )
-    output = model(input_data)
-    ```
-    """  # noqa: E501
-
-    def __init__(
-        self,
-        *,
-        stackwise_filters,
-        stackwise_blocks,
-        stackwise_strides,
-        include_rescaling,
-        input_shape=(None, None, 3),
-        input_tensor=None,
-        block_type="block",
-        **kwargs,
-    ):
-        inputs = utils.parse_model_inputs(input_shape, input_tensor)
-        x = inputs
-
-        if include_rescaling:
-            x = layers.Rescaling(1 / 255.0)(x)
-
-        x = layers.Conv2D(
-            64, 7, strides=2, use_bias=False, padding="same", name="conv1_conv"
-        )(x)
-
-        x = layers.BatchNormalization(
-            axis=BN_AXIS, epsilon=BN_EPSILON, name="conv1_bn"
-        )(x)
-        x = layers.Activation("relu", name="conv1_relu")(x)
-
-        x = layers.MaxPooling2D(
-            3, strides=2, padding="same", name="pool1_pool"
-        )(x)
-
-        num_stacks = len(stackwise_filters)
-
-        pyramid_level_inputs = {}
-        for stack_index in range(num_stacks):
-            x = apply_stack(
-                x,
-                filters=stackwise_filters[stack_index],
-                blocks=stackwise_blocks[stack_index],
-                stride=stackwise_strides[stack_index],
-                block_type=block_type,
-                first_shortcut=(block_type == "block" or stack_index > 0),
-                name=f"v2_stack_{stack_index}",
-            )
-            pyramid_level_inputs[stack_index + 2] = x.node.layer.name
-
-        # Create model.
-        super().__init__(inputs=inputs, outputs=x, **kwargs)
-
-        # All references to `self` below this line
-        self.pyramid_level_inputs = pyramid_level_inputs
-        self.stackwise_filters = stackwise_filters
-        self.stackwise_blocks = stackwise_blocks
-        self.stackwise_strides = stackwise_strides
-        self.include_rescaling = include_rescaling
-        self.input_tensor = input_tensor
-        self.block_type = block_type
-
-    def get_config(self):
-        config = super().get_config()
-        config.update(
-            {
-                "stackwise_filters": self.stackwise_filters,
-                "stackwise_blocks": self.stackwise_blocks,
-                "stackwise_strides": self.stackwise_strides,
-                "include_rescaling": self.include_rescaling,
-                # Remove batch dimension from `input_shape`
-                "input_shape": self.input_shape[1:],
-                "input_tensor": self.input_tensor,
-                "block_type": self.block_type,
-            }
-        )
-        return config
-
-    @classproperty
-    def presets(cls):
-        """Dictionary of preset names and configurations."""
-        return copy.deepcopy(backbone_presets)
-
-    @classproperty
-    def presets_with_weights(cls):
-        """Dictionary of preset names and configurations that include
-        weights."""
-        return copy.deepcopy(backbone_presets_with_weights)
-
-
-ALIAS_DOCSTRING = """ResNetBackbone (V1) model with {num_layers} layers.
-
-    Reference:
-        - [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)
-
-    The difference in ResNetV1 and ResNetV2 rests in the structure of their
-    individual building blocks. In ResNetV2, the batch normalization and
-    ReLU activation precede the convolution layers, as opposed to ResNetV1 where
-    the batch normalization and ReLU activation are applied after the
-    convolution layers.
-
-    For transfer learning use cases, make sure to read the
-    [guide to transfer learning & fine-tuning](https://keras.io/guides/transfer_learning/).
-
-    Args:
-        include_rescaling: bool, whether to rescale the inputs. If set
-            to `True`, inputs will be passed through a `Rescaling(1/255.0)`
-            layer.
-        input_shape: optional shape tuple, defaults to (None, None, 3).
-        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)
-            to use as image input for the model.
-
-    Examples:
-    ```python
-    input_data = tf.ones(shape=(8, 224, 224, 3))
-
-    # Randomly initialized backbone
-    model = ResNet{num_layers}Backbone()
-    output = model(input_data)
-    ```
-"""  # noqa: E501
-
-
-class ResNet18Backbone(ResNetBackbone):
-    def __new__(
-        cls,
-        include_rescaling=True,
-        input_shape=(None, None, 3),
-        input_tensor=None,
-        **kwargs,
-    ):
-        # Pack args in kwargs
-        kwargs.update(
-            {
-                "include_rescaling": include_rescaling,
-                "input_shape": input_shape,
-                "input_tensor": input_tensor,
-            }
-        )
-        return ResNetBackbone.from_preset("resnet18", **kwargs)
-
-    @classproperty
-    def presets(cls):
-        """Dictionary of preset names and configurations."""
-        return {}
-
-    @classproperty
-    def presets_with_weights(cls):
-        """Dictionary of preset names and configurations that include
-        weights."""
-        return {}
-
-
-class ResNet34Backbone(ResNetBackbone):
-    def __new__(
-        cls,
-        include_rescaling=True,
-        input_shape=(None, None, 3),
-        input_tensor=None,
-        **kwargs,
-    ):
-        # Pack args in kwargs
-        kwargs.update(
-            {
-                "include_rescaling": include_rescaling,
-                "input_shape": input_shape,
-                "input_tensor": input_tensor,
-            }
-        )
-        return ResNetBackbone.from_preset("resnet34", **kwargs)
-
-    @classproperty
-    def presets(cls):
-        """Dictionary of preset names and configurations."""
-        return {}
-
-    @classproperty
-    def presets_with_weights(cls):
-        """Dictionary of preset names and configurations that include
-        weights."""
-        return {}
-
-
-class ResNet50Backbone(ResNetBackbone):
-    def __new__(
-        cls,
-        include_rescaling=True,
-        input_shape=(None, None, 3),
-        input_tensor=None,
-        **kwargs,
-    ):
-        # Pack args in kwargs
-        kwargs.update(
-            {
-                "include_rescaling": include_rescaling,
-                "input_shape": input_shape,
-                "input_tensor": input_tensor,
-            }
-        )
-        return ResNetBackbone.from_preset("resnet50", **kwargs)
-
-    @classproperty
-    def presets(cls):
-        """Dictionary of preset names and configurations."""
-        return {
-            "resnet50_imagenet": copy.deepcopy(
-                backbone_presets["resnet50_imagenet"]
-            ),
-        }
-
-    @classproperty
-    def presets_with_weights(cls):
-        """Dictionary of preset names and configurations that include
-        weights."""
-        return cls.presets
-
-
-class ResNet101Backbone(ResNetBackbone):
-    def __new__(
-        cls,
-        include_rescaling=True,
-        input_shape=(None, None, 3),
-        input_tensor=None,
-        **kwargs,
-    ):
-        # Pack args in kwargs
-        kwargs.update(
-            {
-                "include_rescaling": include_rescaling,
-                "input_shape": input_shape,
-                "input_tensor": input_tensor,
-            }
-        )
-        return ResNetBackbone.from_preset("resnet101", **kwargs)
-
-    @classproperty
-    def presets(cls):
-        """Dictionary of preset names and configurations."""
-        return {}
-
-    @classproperty
-    def presets_with_weights(cls):
-        """Dictionary of preset names and configurations that include
-        weights."""
-        return {}
-
-
-class ResNet152Backbone(ResNetBackbone):
-    def __new__(
-        self,
-        include_rescaling=True,
-        input_shape=(None, None, 3),
-        input_tensor=None,
-        **kwargs,
-    ):
-        # Pack args in kwargs
-        kwargs.update(
-            {
-                "include_rescaling": include_rescaling,
-                "input_shape": input_shape,
-                "input_tensor": input_tensor,
-            }
-        )
-        return ResNetBackbone.from_preset("resnet152", **kwargs)
-
-    @classproperty
-    def presets(cls):
-        """Dictionary of preset names and configurations."""
-        return {}
-
-    @classproperty
-    def presets_with_weights(cls):
-        """Dictionary of preset names and configurations that include
-        weights."""
-        return {}
-
-
-setattr(ResNet18Backbone, "__doc__", ALIAS_DOCSTRING.format(num_layers=18))
-setattr(ResNet34Backbone, "__doc__", ALIAS_DOCSTRING.format(num_layers=34))
-setattr(ResNet50Backbone, "__doc__", ALIAS_DOCSTRING.format(num_layers=50))
-setattr(ResNet101Backbone, "__doc__", ALIAS_DOCSTRING.format(num_layers=101))
-setattr(ResNet152Backbone, "__doc__", ALIAS_DOCSTRING.format(num_layers=152))
```

## keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_presets_test.py

```diff
@@ -12,15 +12,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """Tests for loading pretrained model presets."""
 
 import pytest
 import tensorflow as tf
 
-from keras_cv.models.backbones.resnet_v1.resnet_v1_backbone import (
+from keras_cv.models.backbones.resnet_v1.resnet_v1_aliases import (
     ResNet50Backbone,
 )
 from keras_cv.models.backbones.resnet_v1.resnet_v1_backbone import (
     ResNetBackbone,
 )
 
 
@@ -79,15 +79,15 @@
             ResNetBackbone.from_preset("resnet50", load_weights=True)
 
 
 @pytest.mark.extra_large
 class ResNetPresetFullTest(tf.test.TestCase):
     """
     Test the full enumeration of our preset.
-    This every presets for ResNet and is only run manually.
+    This tests every preset for ResNet and is only run manually.
     Run with:
     `pytest keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_presets_test.py --run_extra_large`  # noqa: E501
     """
 
     def test_load_resnet(self):
         input_data = tf.ones(shape=(2, 224, 224, 3))
         for preset in ResNetBackbone.presets:
```

## keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_test.py

```diff
@@ -10,28 +10,29 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import os
 
+import pytest
 import tensorflow as tf
 from absl.testing import parameterized
 from tensorflow import keras
 
-from keras_cv.models.backbones.resnet_v1.resnet_v1_backbone import (
+from keras_cv.models.backbones.resnet_v1.resnet_v1_aliases import (
     ResNet18Backbone,
 )
-from keras_cv.models.backbones.resnet_v1.resnet_v1_backbone import (
+from keras_cv.models.backbones.resnet_v1.resnet_v1_aliases import (
     ResNet50Backbone,
 )
-from keras_cv.models.backbones.resnet_v1.resnet_v1_backbone import (
+from keras_cv.models.backbones.resnet_v1.resnet_v1_aliases import (
     ResNet101Backbone,
 )
-from keras_cv.models.backbones.resnet_v1.resnet_v1_backbone import (
+from keras_cv.models.backbones.resnet_v1.resnet_v1_aliases import (
     ResNet152Backbone,
 )
 from keras_cv.models.backbones.resnet_v1.resnet_v1_backbone import (
     ResNetBackbone,
 )
 from keras_cv.utils.train import get_feature_extractor
 
@@ -62,14 +63,15 @@
         )
         model(self.input_batch)
 
     @parameterized.named_parameters(
         ("tf_format", "tf", "model"),
         ("keras_format", "keras_v3", "model.keras"),
     )
+    @pytest.mark.large  # Saving is slow, so mark these large.
     def test_saved_model(self, save_format, filename):
         model = ResNetBackbone(
             stackwise_filters=[64, 128, 256, 512],
             stackwise_blocks=[2, 2, 2, 2],
             stackwise_strides=[1, 2, 2, 2],
             include_rescaling=False,
         )
@@ -85,14 +87,15 @@
         restored_output = restored_model(self.input_batch)
         self.assertAllClose(model_output, restored_output)
 
     @parameterized.named_parameters(
         ("tf_format", "tf", "model"),
         ("keras_format", "keras_v3", "model.keras"),
     )
+    @pytest.mark.large  # Saving is slow, so mark these large.
     def test_saved_alias_model(self, save_format, filename):
         model = ResNet50Backbone()
         model_output = model(self.input_batch)
         save_path = os.path.join(self.get_temp_dir(), filename)
         model.save(save_path, save_format=save_format)
         restored_model = keras.models.load_model(save_path)
 
@@ -100,50 +103,42 @@
         # Note that these aliases serialized as the base class
         self.assertIsInstance(restored_model, ResNetBackbone)
 
         # Check that output matches.
         restored_output = restored_model(self.input_batch)
         self.assertAllClose(model_output, restored_output)
 
-    def test_create_backbone_model_from_alias_model(self):
-        model = ResNet50Backbone(
-            include_rescaling=False,
-        )
+    def test_feature_pyramid_inputs(self):
+        model = ResNet50Backbone()
         backbone_model = get_feature_extractor(
             model,
             model.pyramid_level_inputs.values(),
             model.pyramid_level_inputs.keys(),
         )
-        inputs = keras.Input(shape=[256, 256, 3])
+        input_size = 256
+        inputs = tf.keras.Input(shape=[input_size, input_size, 3])
         outputs = backbone_model(inputs)
-        # Resnet50 backbone has 4 level of features (2 ~ 5)
-        self.assertLen(outputs, 4)
-        self.assertEquals(list(outputs.keys()), [2, 3, 4, 5])
-        self.assertEquals(outputs[2].shape, [None, 64, 64, 256])
-        self.assertEquals(outputs[3].shape, [None, 32, 32, 512])
-        self.assertEquals(outputs[4].shape, [None, 16, 16, 1024])
-        self.assertEquals(outputs[5].shape, [None, 8, 8, 2048])
-
-    def test_create_backbone_model_with_level_config(self):
-        model = ResNetBackbone(
-            stackwise_filters=[64, 128, 256, 512],
-            stackwise_blocks=[2, 2, 2, 2],
-            stackwise_strides=[1, 2, 2, 2],
-            include_rescaling=False,
-            input_shape=[256, 256, 3],
+        levels = ["P2", "P3", "P4", "P5"]
+        self.assertEquals(list(outputs.keys()), levels)
+        self.assertEquals(
+            outputs["P2"].shape,
+            [None, input_size // 2**2, input_size // 2**2, 256],
+        )
+        self.assertEquals(
+            outputs["P3"].shape,
+            [None, input_size // 2**3, input_size // 2**3, 512],
+        )
+        self.assertEquals(
+            outputs["P4"].shape,
+            [None, input_size // 2**4, input_size // 2**4, 1024],
+        )
+        self.assertEquals(
+            outputs["P5"].shape,
+            [None, input_size // 2**5, input_size // 2**5, 2048],
         )
-        levels = [3, 4]
-        layer_names = [model.pyramid_level_inputs[level] for level in [3, 4]]
-        backbone_model = get_feature_extractor(model, layer_names, levels)
-        inputs = keras.Input(shape=[256, 256, 3])
-        outputs = backbone_model(inputs)
-        self.assertLen(outputs, 2)
-        self.assertEquals(list(outputs.keys()), [3, 4])
-        self.assertEquals(outputs[3].shape, [None, 32, 32, 512])
-        self.assertEquals(outputs[4].shape, [None, 16, 16, 1024])
 
     @parameterized.named_parameters(
         ("one_channel", 1),
         ("four_channels", 4),
     )
     def test_application_variable_input_channels(self, num_channels):
         # ResNet50 model
```

## keras_cv/models/backbones/resnet_v2/resnet_v2_backbone.py

```diff
@@ -7,15 +7,15 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""ResNet models for Keras.
+"""ResNet backbone model.
 Reference:
   - [Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027) (ECCV 2016)
   - [Based on the original keras.applications ResNet](https://github.com/keras-team/keras/blob/master/keras/applications/resnet_v2.py)
 """  # noqa: E501
 
 import copy
 
@@ -33,14 +33,164 @@
 )
 from keras_cv.utils.python_utils import classproperty
 
 BN_AXIS = 3
 BN_EPSILON = 1.001e-5
 
 
+@keras.utils.register_keras_serializable(package="keras_cv.models")
+class ResNetV2Backbone(Backbone):
+    """Instantiates the ResNetV2 architecture.
+
+    Reference:
+        - [Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027) (ECCV 2016)
+
+    The difference in Resnet and ResNetV2 rests in the structure of their
+    individual building blocks. In ResNetV2, the batch normalization and
+    ReLU activation precede the convolution layers, as opposed to ResNetV1 where
+    the batch normalization and ReLU activation are applied after the
+    convolution layers.
+
+    For transfer learning use cases, make sure to read the
+    [guide to transfer learning & fine-tuning](https://keras.io/guides/transfer_learning/).
+
+    Args:
+        stackwise_filters: list of ints, number of filters for each stack in
+            the model.
+        stackwise_blocks: list of ints, number of blocks for each stack in the
+            model.
+        stackwise_strides: list of ints, stride for each stack in the model.
+        include_rescaling: bool, whether to rescale the inputs. If set
+            to `True`, inputs will be passed through a `Rescaling(1/255.0)`
+            layer.
+        stackwise_dilations: list of ints, dilation for each stack in the
+            model. If `None` (default), dilation will not be used.
+        input_shape: optional shape tuple, defaults to (None, None, 3).
+        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)
+            to use as image input for the model.
+        block_type: string, one of "basic_block" or "block". The block type to
+            stack. Use "basic_block" for smaller models like ResNet18 and
+            ResNet34.
+
+    Examples:
+    ```python
+    input_data = tf.ones(shape=(8, 224, 224, 3))
+
+    # Pretrained backbone
+    model = keras_cv.models.ResNetV2Backbone.from_preset("resnet50_v2_imagenet")
+    output = model(input_data)
+
+    # Randomly initialized backbone with a custom config
+    model = ResNetV2Backbone(
+        stackwise_filters=[64, 128, 256, 512],
+        stackwise_blocks=[2, 2, 2, 2],
+        stackwise_strides=[1, 2, 2, 2],
+        include_rescaling=False,
+    )
+    output = model(input_data)
+    ```
+    """  # noqa: E501
+
+    def __init__(
+        self,
+        *,
+        stackwise_filters,
+        stackwise_blocks,
+        stackwise_strides,
+        include_rescaling,
+        stackwise_dilations=None,
+        input_shape=(None, None, 3),
+        input_tensor=None,
+        block_type="block",
+        **kwargs,
+    ):
+        inputs = utils.parse_model_inputs(input_shape, input_tensor)
+        x = inputs
+
+        if include_rescaling:
+            x = layers.Rescaling(1 / 255.0)(x)
+
+        x = layers.Conv2D(
+            64,
+            7,
+            strides=2,
+            use_bias=True,
+            padding="same",
+            name="conv1_conv",
+        )(x)
+
+        x = layers.MaxPooling2D(
+            3, strides=2, padding="same", name="pool1_pool"
+        )(x)
+
+        num_stacks = len(stackwise_filters)
+        if stackwise_dilations is None:
+            stackwise_dilations = [1] * num_stacks
+
+        pyramid_level_inputs = {}
+        for stack_index in range(num_stacks):
+            x = apply_stack(
+                x,
+                filters=stackwise_filters[stack_index],
+                blocks=stackwise_blocks[stack_index],
+                stride=stackwise_strides[stack_index],
+                dilations=stackwise_dilations[stack_index],
+                block_type=block_type,
+                first_shortcut=(block_type == "block" or stack_index > 0),
+                name=f"v2_stack_{stack_index}",
+            )
+            pyramid_level_inputs[f"P{stack_index + 2}"] = x.node.layer.name
+
+        x = layers.BatchNormalization(
+            axis=BN_AXIS, epsilon=BN_EPSILON, name="post_bn"
+        )(x)
+        x = layers.Activation("relu", name="post_relu")(x)
+
+        # Create model.
+        super().__init__(inputs=inputs, outputs=x, **kwargs)
+
+        # All references to `self` below this line
+        self.pyramid_level_inputs = pyramid_level_inputs
+        self.stackwise_filters = stackwise_filters
+        self.stackwise_blocks = stackwise_blocks
+        self.stackwise_strides = stackwise_strides
+        self.include_rescaling = include_rescaling
+        self.stackwise_dilations = stackwise_dilations
+        self.input_tensor = input_tensor
+        self.block_type = block_type
+
+    def get_config(self):
+        config = super().get_config()
+        config.update(
+            {
+                "stackwise_filters": self.stackwise_filters,
+                "stackwise_blocks": self.stackwise_blocks,
+                "stackwise_strides": self.stackwise_strides,
+                "include_rescaling": self.include_rescaling,
+                # Remove batch dimension from `input_shape`
+                "input_shape": self.input_shape[1:],
+                "stackwise_dilations": self.stackwise_dilations,
+                "input_tensor": self.input_tensor,
+                "block_type": self.block_type,
+            }
+        )
+        return config
+
+    @classproperty
+    def presets(cls):
+        """Dictionary of preset names and configurations."""
+        return copy.deepcopy(backbone_presets)
+
+    @classproperty
+    def presets_with_weights(cls):
+        """Dictionary of preset names and configurations that include
+        weights."""
+        return copy.deepcopy(backbone_presets_with_weights)
+
+
 def apply_basic_block(
     x,
     filters,
     kernel_size=3,
     stride=1,
     dilation=1,
     conv_shortcut=False,
@@ -249,371 +399,7 @@
         x,
         filters,
         stride=stride,
         dilation=dilations,
         name=name + "_block" + str(blocks),
     )
     return x
-
-
-@keras.utils.register_keras_serializable(package="keras_cv.models")
-class ResNetV2Backbone(Backbone):
-    """Instantiates the ResNetV2 architecture.
-
-    Reference:
-        - [Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027) (ECCV 2016)
-
-    The difference in Resnet and ResNetV2 rests in the structure of their
-    individual building blocks. In ResNetV2, the batch normalization and
-    ReLU activation precede the convolution layers, as opposed to ResNetV1 where
-    the batch normalization and ReLU activation are applied after the
-    convolution layers.
-
-    For transfer learning use cases, make sure to read the
-    [guide to transfer learning & fine-tuning](https://keras.io/guides/transfer_learning/).
-
-    Args:
-        stackwise_filters: list of ints, number of filters for each stack in
-            the model.
-        stackwise_blocks: list of ints, number of blocks for each stack in the
-            model.
-        stackwise_strides: list of ints, stride for each stack in the model.
-        include_rescaling: bool, whether to rescale the inputs. If set
-            to `True`, inputs will be passed through a `Rescaling(1/255.0)`
-            layer.
-        stackwise_dilations: list of ints, dilation for each stack in the
-            model. If `None` (default), dilation will not be used.
-        input_shape: optional shape tuple, defaults to (None, None, 3).
-        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)
-            to use as image input for the model.
-        block_type: string, one of "basic_block" or "block". The block type to
-            stack. Use "basic_block" for smaller models like ResNet18 and
-            ResNet34.
-
-    Examples:
-    ```python
-    input_data = tf.ones(shape=(8, 224, 224, 3))
-
-    # Pretrained backbone
-    model = keras_cv.models.ResNetV2Backbone.from_preset("resnet50_v2_imagenet")
-    output = model(input_data)
-
-    # Randomly initialized backbone with a custom config
-    model = ResNetV2Backbone(
-        stackwise_filters=[64, 128, 256, 512],
-        stackwise_blocks=[2, 2, 2, 2],
-        stackwise_strides=[1, 2, 2, 2],
-        include_rescaling=False,
-    )
-    output = model(input_data)
-    ```
-    """  # noqa: E501
-
-    def __init__(
-        self,
-        *,
-        stackwise_filters,
-        stackwise_blocks,
-        stackwise_strides,
-        include_rescaling,
-        stackwise_dilations=None,
-        input_shape=(None, None, 3),
-        input_tensor=None,
-        block_type="block",
-        **kwargs,
-    ):
-        inputs = utils.parse_model_inputs(input_shape, input_tensor)
-        x = inputs
-
-        if include_rescaling:
-            x = layers.Rescaling(1 / 255.0)(x)
-
-        x = layers.Conv2D(
-            64,
-            7,
-            strides=2,
-            use_bias=True,
-            padding="same",
-            name="conv1_conv",
-        )(x)
-
-        x = layers.MaxPooling2D(
-            3, strides=2, padding="same", name="pool1_pool"
-        )(x)
-
-        num_stacks = len(stackwise_filters)
-        if stackwise_dilations is None:
-            stackwise_dilations = [1] * num_stacks
-
-        pyramid_level_inputs = {}
-        for stack_index in range(num_stacks):
-            x = apply_stack(
-                x,
-                filters=stackwise_filters[stack_index],
-                blocks=stackwise_blocks[stack_index],
-                stride=stackwise_strides[stack_index],
-                dilations=stackwise_dilations[stack_index],
-                block_type=block_type,
-                first_shortcut=(block_type == "block" or stack_index > 0),
-                name=f"v2_stack_{stack_index}",
-            )
-            pyramid_level_inputs[stack_index + 2] = x.node.layer.name
-
-        x = layers.BatchNormalization(
-            axis=BN_AXIS, epsilon=BN_EPSILON, name="post_bn"
-        )(x)
-        x = layers.Activation("relu", name="post_relu")(x)
-
-        # Create model.
-        super().__init__(inputs=inputs, outputs=x, **kwargs)
-
-        # All references to `self` below this line
-        self.pyramid_level_inputs = pyramid_level_inputs
-        self.stackwise_filters = stackwise_filters
-        self.stackwise_blocks = stackwise_blocks
-        self.stackwise_strides = stackwise_strides
-        self.include_rescaling = include_rescaling
-        self.stackwise_dilations = stackwise_dilations
-        self.input_tensor = input_tensor
-        self.block_type = block_type
-
-    def get_config(self):
-        config = super().get_config()
-        config.update(
-            {
-                "stackwise_filters": self.stackwise_filters,
-                "stackwise_blocks": self.stackwise_blocks,
-                "stackwise_strides": self.stackwise_strides,
-                "include_rescaling": self.include_rescaling,
-                # Remove batch dimension from `input_shape`
-                "input_shape": self.input_shape[1:],
-                "stackwise_dilations": self.stackwise_dilations,
-                "input_tensor": self.input_tensor,
-                "block_type": self.block_type,
-            }
-        )
-        return config
-
-    @classproperty
-    def presets(cls):
-        """Dictionary of preset names and configurations."""
-        return copy.deepcopy(backbone_presets)
-
-    @classproperty
-    def presets_with_weights(cls):
-        """Dictionary of preset names and configurations that include
-        weights."""
-        return copy.deepcopy(backbone_presets_with_weights)
-
-
-ALIAS_DOCSTRING = """ResNetV2Backbone model with {num_layers} layers.
-
-    Reference:
-        - [Identity Mappings in Deep Residual Networks](https://arxiv.org/abs/1603.05027) (ECCV 2016)
-
-    The difference in ResNet and ResNetV2 rests in the structure of their
-    individual building blocks. In ResNetV2, the batch normalization and
-    ReLU activation precede the convolution layers, as opposed to ResNetV1 where
-    the batch normalization and ReLU activation are applied after the
-    convolution layers.
-
-    For transfer learning use cases, make sure to read the
-    [guide to transfer learning & fine-tuning](https://keras.io/guides/transfer_learning/).
-
-    Args:
-        include_rescaling: bool, whether to rescale the inputs. If set
-            to `True`, inputs will be passed through a `Rescaling(1/255.0)`
-            layer.
-        input_shape: optional shape tuple, defaults to (None, None, 3).
-        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)
-            to use as image input for the model.
-
-    Examples:
-    ```python
-    input_data = tf.ones(shape=(8, 224, 224, 3))
-
-    # Randomly initialized backbone
-    model = ResNet{num_layers}V2Backbone()
-    output = model(input_data)
-    ```
-"""  # noqa: E501
-
-
-class ResNet18V2Backbone(ResNetV2Backbone):
-    def __new__(
-        cls,
-        include_rescaling=True,
-        input_shape=(None, None, 3),
-        input_tensor=None,
-        **kwargs,
-    ):
-        # Pack args in kwargs
-        kwargs.update(
-            {
-                "include_rescaling": include_rescaling,
-                "input_shape": input_shape,
-                "input_tensor": input_tensor,
-            }
-        )
-        return ResNetV2Backbone.from_preset("resnet18_v2", **kwargs)
-
-    @classproperty
-    def presets(cls):
-        """Dictionary of preset names and configurations."""
-        return {}
-
-    @classproperty
-    def presets_with_weights(cls):
-        """Dictionary of preset names and configurations that include
-        weights."""
-        return {}
-
-
-class ResNet34V2Backbone(ResNetV2Backbone):
-    def __new__(
-        cls,
-        include_rescaling=True,
-        input_shape=(None, None, 3),
-        input_tensor=None,
-        **kwargs,
-    ):
-        # Pack args in kwargs
-        kwargs.update(
-            {
-                "include_rescaling": include_rescaling,
-                "input_shape": input_shape,
-                "input_tensor": input_tensor,
-            }
-        )
-        return ResNetV2Backbone.from_preset("resnet34_v2", **kwargs)
-
-    @classproperty
-    def presets(cls):
-        """Dictionary of preset names and configurations."""
-        return {}
-
-    @classproperty
-    def presets_with_weights(cls):
-        """Dictionary of preset names and configurations that include
-        weights."""
-        return {}
-
-
-class ResNet50V2Backbone(ResNetV2Backbone):
-    def __new__(
-        cls,
-        include_rescaling=True,
-        input_shape=(None, None, 3),
-        input_tensor=None,
-        **kwargs,
-    ):
-        # Pack args in kwargs
-        kwargs.update(
-            {
-                "include_rescaling": include_rescaling,
-                "input_shape": input_shape,
-                "input_tensor": input_tensor,
-            }
-        )
-        return ResNetV2Backbone.from_preset("resnet50_v2", **kwargs)
-
-    @classproperty
-    def presets(cls):
-        """Dictionary of preset names and configurations."""
-        return {
-            "resnet50_v2_imagenet": copy.deepcopy(
-                backbone_presets["resnet50_v2_imagenet"]
-            ),
-        }
-
-    @classproperty
-    def presets_with_weights(cls):
-        """Dictionary of preset names and configurations that include
-        weights."""
-        return cls.presets
-
-
-class ResNet101V2Backbone(ResNetV2Backbone):
-    def __new__(
-        cls,
-        include_rescaling=True,
-        input_shape=(None, None, 3),
-        input_tensor=None,
-        **kwargs,
-    ):
-        # Pack args in kwargs
-        kwargs.update(
-            {
-                "include_rescaling": include_rescaling,
-                "input_shape": input_shape,
-                "input_tensor": input_tensor,
-            }
-        )
-        return ResNetV2Backbone.from_preset("resnet101_v2", **kwargs)
-
-    @classproperty
-    def presets(cls):
-        """Dictionary of preset names and configurations."""
-        return {}
-
-    @classproperty
-    def presets_with_weights(cls):
-        """Dictionary of preset names and configurations that include
-        weights."""
-        return {}
-
-
-class ResNet152V2Backbone(ResNetV2Backbone):
-    def __new__(
-        cls,
-        include_rescaling=True,
-        input_shape=(None, None, 3),
-        input_tensor=None,
-        **kwargs,
-    ):
-        # Pack args in kwargs
-        kwargs.update(
-            {
-                "include_rescaling": include_rescaling,
-                "input_shape": input_shape,
-                "input_tensor": input_tensor,
-            }
-        )
-        return ResNetV2Backbone.from_preset("resnet152_v2", **kwargs)
-
-    @classproperty
-    def presets(cls):
-        """Dictionary of preset names and configurations."""
-        return {}
-
-    @classproperty
-    def presets_with_weights(cls):
-        """Dictionary of preset names and configurations that include
-        weights."""
-        return {}
-
-
-setattr(
-    ResNet18V2Backbone,
-    "__doc__",
-    ALIAS_DOCSTRING.format(num_layers=18),
-)
-setattr(
-    ResNet34V2Backbone,
-    "__doc__",
-    ALIAS_DOCSTRING.format(num_layers=34),
-)
-setattr(
-    ResNet50V2Backbone,
-    "__doc__",
-    ALIAS_DOCSTRING.format(num_layers=50),
-)
-setattr(
-    ResNet101V2Backbone,
-    "__doc__",
-    ALIAS_DOCSTRING.format(num_layers=101),
-)
-setattr(
-    ResNet152V2Backbone,
-    "__doc__",
-    ALIAS_DOCSTRING.format(num_layers=152),
-)
```

## keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_presets_test.py

```diff
@@ -13,15 +13,15 @@
 # limitations under the License.
 """Tests for loading pretrained model presets."""
 
 import pytest
 import tensorflow as tf
 from absl.testing import parameterized
 
-from keras_cv.models.backbones.resnet_v2.resnet_v2_backbone import (
+from keras_cv.models.backbones.resnet_v2.resnet_v2_aliases import (
     ResNet50V2Backbone,
 )
 from keras_cv.models.backbones.resnet_v2.resnet_v2_backbone import (
     ResNetV2Backbone,
 )
```

## keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_test.py

```diff
@@ -10,19 +10,20 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import os
 
+import pytest
 import tensorflow as tf
 from absl.testing import parameterized
 from tensorflow import keras
 
-from keras_cv.models.backbones.resnet_v2.resnet_v2_backbone import (
+from keras_cv.models.backbones.resnet_v2.resnet_v2_aliases import (
     ResNet50V2Backbone,
 )
 from keras_cv.models.backbones.resnet_v2.resnet_v2_backbone import (
     ResNetV2Backbone,
 )
 from keras_cv.utils.train import get_feature_extractor
 
@@ -53,14 +54,15 @@
         )
         model(self.input_batch)
 
     @parameterized.named_parameters(
         ("tf_format", "tf", "model"),
         ("keras_format", "keras_v3", "model.keras"),
     )
+    @pytest.mark.large  # Saving is slow, so mark these large.
     def test_saved_model(self, save_format, filename):
         model = ResNetV2Backbone(
             stackwise_filters=[64, 128, 256, 512],
             stackwise_blocks=[2, 2, 2, 2],
             stackwise_strides=[1, 2, 2, 2],
             include_rescaling=False,
         )
@@ -76,14 +78,15 @@
         restored_output = restored_model(self.input_batch)
         self.assertAllClose(model_output, restored_output)
 
     @parameterized.named_parameters(
         ("tf_format", "tf", "model"),
         ("keras_format", "keras_v3", "model.keras"),
     )
+    @pytest.mark.large  # Saving is slow, so mark these large.
     def test_saved_alias_model(self, save_format, filename):
         model = ResNet50V2Backbone()
         model_output = model(self.input_batch)
         save_path = os.path.join(self.get_temp_dir(), filename)
         model.save(save_path, save_format=save_format)
         restored_model = keras.models.load_model(save_path)
 
@@ -91,50 +94,42 @@
         # Note that these aliases serialized as the base class
         self.assertIsInstance(restored_model, ResNetV2Backbone)
 
         # Check that output matches.
         restored_output = restored_model(self.input_batch)
         self.assertAllClose(model_output, restored_output)
 
-    def test_create_backbone_model_from_alias_model(self):
-        model = ResNet50V2Backbone(
-            include_rescaling=False,
-        )
+    def test_feature_pyramid_inputs(self):
+        model = ResNet50V2Backbone()
         backbone_model = get_feature_extractor(
             model,
             model.pyramid_level_inputs.values(),
             model.pyramid_level_inputs.keys(),
         )
-        inputs = keras.Input(shape=[256, 256, 3])
+        input_size = 256
+        inputs = tf.keras.Input(shape=[input_size, input_size, 3])
         outputs = backbone_model(inputs)
-        # Resnet50 backbone has 4 level of features (2 ~ 5)
-        self.assertLen(outputs, 4)
-        self.assertEquals(list(outputs.keys()), [2, 3, 4, 5])
-        self.assertEquals(outputs[2].shape, [None, 64, 64, 256])
-        self.assertEquals(outputs[3].shape, [None, 32, 32, 512])
-        self.assertEquals(outputs[4].shape, [None, 16, 16, 1024])
-        self.assertEquals(outputs[5].shape, [None, 8, 8, 2048])
-
-    def test_create_backbone_model_with_level_config(self):
-        model = ResNetV2Backbone(
-            stackwise_filters=[64, 128, 256, 512],
-            stackwise_blocks=[2, 2, 2, 2],
-            stackwise_strides=[1, 2, 2, 2],
-            include_rescaling=False,
-            input_shape=[256, 256, 3],
+        levels = ["P2", "P3", "P4", "P5"]
+        self.assertEquals(list(outputs.keys()), levels)
+        self.assertEquals(
+            outputs["P2"].shape,
+            [None, input_size // 2**2, input_size // 2**2, 256],
+        )
+        self.assertEquals(
+            outputs["P3"].shape,
+            [None, input_size // 2**3, input_size // 2**3, 512],
+        )
+        self.assertEquals(
+            outputs["P4"].shape,
+            [None, input_size // 2**4, input_size // 2**4, 1024],
+        )
+        self.assertEquals(
+            outputs["P5"].shape,
+            [None, input_size // 2**5, input_size // 2**5, 2048],
         )
-        levels = [3, 4]
-        layer_names = [model.pyramid_level_inputs[level] for level in [3, 4]]
-        backbone_model = get_feature_extractor(model, layer_names, levels)
-        inputs = keras.Input(shape=[256, 256, 3])
-        outputs = backbone_model(inputs)
-        self.assertLen(outputs, 2)
-        self.assertEquals(list(outputs.keys()), [3, 4])
-        self.assertEquals(outputs[3].shape, [None, 32, 32, 512])
-        self.assertEquals(outputs[4].shape, [None, 16, 16, 1024])
 
     @parameterized.named_parameters(
         ("one_channel", 1),
         ("four_channels", 4),
     )
     def test_application_variable_input_channels(self, num_channels):
         # ResNet50 model
```

## keras_cv/models/classification/image_classifier_test.py

```diff
@@ -17,15 +17,15 @@
 import os
 
 import pytest
 import tensorflow as tf
 from absl.testing import parameterized
 from tensorflow import keras
 
-from keras_cv.models.backbones.resnet_v2.resnet_v2_backbone import (
+from keras_cv.models.backbones.resnet_v2.resnet_v2_aliases import (
     ResNet18V2Backbone,
 )
 from keras_cv.models.classification.image_classifier import ImageClassifier
 
 
 class ImageClassifierTest(tf.test.TestCase, parameterized.TestCase):
     def setUp(self):
@@ -40,14 +40,15 @@
             num_classes=2,
         )
         model(self.input_batch)
 
     @parameterized.named_parameters(
         ("jit_compile_false", False), ("jit_compile_true", True)
     )
+    @pytest.mark.large  # Fit is slow, so mark these large.
     def test_classifier_fit(self, jit_compile):
         model = ImageClassifier(
             backbone=ResNet18V2Backbone(),
             num_classes=2,
         )
         model.compile(
             loss="categorical_crossentropy",
@@ -76,14 +77,15 @@
                 pooling="clowntown",
             )
 
     @parameterized.named_parameters(
         ("tf_format", "tf", "model"),
         ("keras_format", "keras_v3", "model.keras"),
     )
+    @pytest.mark.large  # Saving is slow, so mark these large.
     def test_saved_model(self, save_format, filename):
         model = ImageClassifier(
             backbone=ResNet18V2Backbone(),
             num_classes=2,
         )
         model_output = model(self.input_batch)
         save_path = os.path.join(self.get_temp_dir(), filename)
```

## keras_cv/models/legacy/__init__.py

```diff
@@ -20,17 +20,14 @@
 from keras_cv.models.legacy.convnext import ConvNeXtBase
 from keras_cv.models.legacy.convnext import ConvNeXtLarge
 from keras_cv.models.legacy.convnext import ConvNeXtSmall
 from keras_cv.models.legacy.convnext import ConvNeXtTiny
 from keras_cv.models.legacy.convnext import ConvNeXtXLarge
 from keras_cv.models.legacy.darknet import DarkNet21
 from keras_cv.models.legacy.darknet import DarkNet53
-from keras_cv.models.legacy.densenet import DenseNet121
-from keras_cv.models.legacy.densenet import DenseNet169
-from keras_cv.models.legacy.densenet import DenseNet201
 from keras_cv.models.legacy.efficientnet_lite import EfficientNetLiteB0
 from keras_cv.models.legacy.efficientnet_lite import EfficientNetLiteB1
 from keras_cv.models.legacy.efficientnet_lite import EfficientNetLiteB2
 from keras_cv.models.legacy.efficientnet_lite import EfficientNetLiteB3
 from keras_cv.models.legacy.efficientnet_lite import EfficientNetLiteB4
 from keras_cv.models.legacy.efficientnet_v1 import EfficientNetB0
 from keras_cv.models.legacy.efficientnet_v1 import EfficientNetB1
```

## keras_cv/models/legacy/efficientnet_lite.py

```diff
@@ -27,14 +27,15 @@
 import tensorflow as tf
 from keras import backend
 from keras import layers
 from tensorflow import keras
 
 from keras_cv.models.legacy import utils
 from keras_cv.models.legacy.weights import parse_weights
+from keras_cv.models.utils import correct_pad_downsample
 
 DEFAULT_BLOCKS_ARGS = [
     {
         "kernel_size": 3,
         "repeats": 1,
         "filters_in": 32,
         "filters_out": 16,
@@ -160,39 +161,14 @@
     Returns:
         A `keras.Model` instance.
 """  # noqa: E501
 
 BN_AXIS = 3
 
 
-def correct_pad(inputs, kernel_size):
-    """Returns a tuple for zero-padding for 2D convolution with downsampling.
-
-    Args:
-        inputs: Input tensor.
-        kernel_size: An integer or tuple/list of 2 integers.
-
-    Returns:
-        A tuple.
-    """
-    img_dim = 1
-    input_size = backend.int_shape(inputs)[img_dim : (img_dim + 2)]
-    if isinstance(kernel_size, int):
-        kernel_size = (kernel_size, kernel_size)
-    if input_size[0] is None:
-        adjust = (1, 1)
-    else:
-        adjust = (1 - input_size[0] % 2, 1 - input_size[1] % 2)
-    correct = (kernel_size[0] // 2, kernel_size[1] // 2)
-    return (
-        (correct[0] - adjust[0], correct[0]),
-        (correct[1] - adjust[1], correct[1]),
-    )
-
-
 def round_filters(filters, depth_divisor, width_coefficient):
     """Round number of filters based on depth multiplier."""
     filters *= width_coefficient
     new_filters = max(
         depth_divisor,
         int(filters + depth_divisor / 2) // depth_divisor * depth_divisor,
     )
@@ -254,15 +230,15 @@
         x = layers.Activation(activation, name=name + "expand_activation")(x)
     else:
         x = inputs
 
     # Depthwise Convolution
     if strides == 2:
         x = layers.ZeroPadding2D(
-            padding=correct_pad(x, kernel_size),
+            padding=correct_pad_downsample(x, kernel_size),
             name=name + "dwconv_pad",
         )(x)
         conv_pad = "valid"
     else:
         conv_pad = "same"
     x = layers.DepthwiseConv2D(
         kernel_size,
@@ -406,15 +382,15 @@
         x = img_input
 
         if include_rescaling:
             # Use common rescaling strategy across keras_cv
             x = layers.Rescaling(1.0 / 255.0)(x)
 
         x = layers.ZeroPadding2D(
-            padding=correct_pad(x, 3), name="stem_conv_pad"
+            padding=correct_pad_downsample(x, 3), name="stem_conv_pad"
         )(x)
         x = layers.Conv2D(
             32,
             3,
             strides=2,
             padding="valid",
             use_bias=False,
```

## keras_cv/models/legacy/models_test.py

```diff
@@ -30,15 +30,15 @@
         # Code before yield runs before the test
         yield
         keras.backend.clear_session()
 
     def _test_application_base(self, app, _, args):
         # Can be instantiated with default arguments
         model = app(
-            include_top=True, num_classes=1000, include_rescaling=False, **args
+            include_top=True, num_classes=10, include_rescaling=False, **args
         )
 
         # Can be serialized and deserialized
         config = model.get_config()
         reconstructed_model = model.__class__.from_config(config)
         self.assertEqual(len(model.weights), len(reconstructed_model.weights))
 
@@ -158,14 +158,15 @@
         x = backbone(x)
 
         backbone_output = backbone.get_layer(index=-1).output
 
         model = keras.Model(inputs=inputs, outputs=[backbone_output])
         model.compile()
 
+    @pytest.mark.large  # Saving is slow, so mark these large.
     def _test_model_serialization(self, app, _, args, save_format, filename):
         model = app(include_rescaling=True, include_top=False, **args)
         input_batch = tf.ones(shape=(16, 224, 224, 3))
         model_output = model(input_batch)
         save_path = os.path.join(self.get_temp_dir(), filename)
         model.save(save_path, save_format=save_format)
         restored_model = keras.models.load_model(save_path)
```

## keras_cv/models/legacy/vgg19_test.py

```diff
@@ -8,28 +8,31 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import pytest
 import tensorflow as tf
 from absl.testing import parameterized
 
 from keras_cv.models.legacy import vgg19
 
 from .models_test import ModelsTest
 
 MODEL_LIST = [
     (vgg19.VGG19, 512, {}),
 ]
 
 
 class VGG19Test(ModelsTest, tf.test.TestCase, parameterized.TestCase):
+    # VGG19 has 143M parameters and likes to OOM on our GCB instance.
     @parameterized.parameters(*MODEL_LIST)
+    @pytest.mark.extra_large
     def test_application_base(self, app, _, args):
         super()._test_application_base(app, _, args)
 
     @parameterized.parameters(*MODEL_LIST)
     def test_application_with_rescaling(self, app, last_dim, args):
         super()._test_application_with_rescaling(app, last_dim, args)
```

## keras_cv/models/legacy/weights.py

```diff
@@ -7,34 +7,34 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 import tensorflow as tf
-from keras.utils import data_utils
+from keras import utils
 
 
 def parse_weights(weights, include_top, model_type):
     if not weights:
         return weights
     if weights.startswith("gs://"):
         weights = weights.replace("gs://", "https://storage.googleapis.com/")
-        return data_utils.get_file(
+        return utils.get_file(
             origin=weights,
             cache_subdir="models",
         )
     if tf.io.gfile.exists(weights):
         return weights
     if weights in ALIASES[model_type]:
         weights = ALIASES[model_type][weights]
     if weights in WEIGHTS_CONFIG[model_type]:
         if not include_top:
             weights = weights + "-notop"
-        return data_utils.get_file(
+        return utils.get_file(
             origin=f"{BASE_PATH}/{model_type}/{weights}.h5",
             cache_subdir="models",
             file_hash=WEIGHTS_CONFIG[model_type][weights],
         )
 
     raise ValueError(
         "The `weights` argument should be either `None`, a the path to the "
```

## keras_cv/models/legacy/segmentation/__init__.py

```diff
@@ -7,9 +7,7 @@
 #     https://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-
-from keras_cv.models.legacy.segmentation.deeplab import DeepLabV3
```

## keras_cv/models/legacy/segmentation/deeplab_test.py

```diff
@@ -14,91 +14,93 @@
 
 import pytest
 import tensorflow as tf
 import tensorflow_datasets as tfds
 from tensorflow import keras
 
 from keras_cv.models import ResNet50V2Backbone
-from keras_cv.models.legacy import segmentation
+from keras_cv.models.legacy.segmentation.deeplab import DeepLabV3
 
 
 class DeeplabTest(tf.test.TestCase):
     def test_deeplab_model_construction_with_preconfigured_setting(self):
         backbone = ResNet50V2Backbone(
             input_shape=[64, 64, 3],
         )
-        model = segmentation.DeepLabV3(num_classes=11, backbone=backbone)
+        model = DeepLabV3(num_classes=11, backbone=backbone)
         input_image = tf.random.uniform(shape=[1, 64, 64, 3])
         output = model(input_image, training=True)
 
         self.assertEquals(output["output"].shape, [1, 64, 64, 11])
 
     def test_greyscale_input(self):
         backbone = ResNet50V2Backbone(
             input_shape=[64, 64, 1],
         )
-        model = segmentation.DeepLabV3(num_classes=11, backbone=backbone)
+        model = DeepLabV3(num_classes=11, backbone=backbone)
         input_image = tf.random.uniform(shape=[1, 64, 64, 1])
         output = model(input_image, training=True)
 
         self.assertEquals(output["output"].shape, [1, 64, 64, 11])
 
     def test_missing_input_shapes(self):
         with self.assertRaisesRegex(
             ValueError,
             "Input shapes for both the backbone and DeepLabV3 "
             "cannot be `None`.",
         ):
             backbone = ResNet50V2Backbone()
-            segmentation.DeepLabV3(num_classes=11, backbone=backbone)
+            DeepLabV3(num_classes=11, backbone=backbone)
 
     def test_deeplab_model_with_components(self):
         backbone = ResNet50V2Backbone(
             input_shape=[64, 64, 3],
         )
-        model = segmentation.DeepLabV3(
+        model = DeepLabV3(
             num_classes=11,
             backbone=backbone,
         )
 
         input_image = tf.random.uniform(shape=[1, 64, 64, 3])
         output = model(input_image, training=True)
 
         self.assertEquals(output["output"].shape, [1, 64, 64, 11])
 
     def test_mixed_precision(self):
         keras.mixed_precision.set_global_policy("mixed_float16")
         backbone = ResNet50V2Backbone(
             input_shape=[64, 64, 3],
         )
-        model = segmentation.DeepLabV3(
+        model = DeepLabV3(
             num_classes=11,
             backbone=backbone,
         )
         input_image = tf.random.uniform(shape=[1, 64, 64, 3])
         output = model(input_image, training=True)
 
         self.assertEquals(output["output"].dtype, tf.float32)
         keras.mixed_precision.set_global_policy("float32")
 
     def test_invalid_backbone_model(self):
         with self.assertRaisesRegex(
             ValueError, "Argument `backbone` must be a `keras.layers.Layer`"
         ):
-            segmentation.DeepLabV3(
+            DeepLabV3(
                 num_classes=11,
                 backbone=tf.Module(),
             )
 
     @pytest.mark.extra_large
     def test_model_train(self):
+        target_size = [96, 96]
+
         backbone = ResNet50V2Backbone(
-            input_shape=[384, 384, 3],
+            input_shape=target_size + [3],
         )
-        model = segmentation.DeepLabV3(num_classes=1, backbone=backbone)
+        model = DeepLabV3(num_classes=1, backbone=backbone)
 
         gcs_data_pattern = "gs://caltech_birds2011_mask/0.1.1/*.tfrecord*"
         features = tfds.features.FeaturesDict(
             {
                 "bbox": tfds.features.BBoxFeature(),
                 "image": tfds.features.Image(
                     shape=(None, None, 3), dtype=tf.uint8
@@ -116,30 +118,25 @@
         AUTO = tf.data.AUTOTUNE
         ignore_order = tf.data.Options()
         ignore_order.experimental_deterministic = False
         ds = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)
         ds = ds.with_options(ignore_order)
         ds = ds.map(features.deserialize_example, num_parallel_calls=AUTO)
 
-        target_size = [384, 384]
-        output_res = [96, 96]
-        num_images = 11788
-
-        image_resizing = keras.layers.Resizing(target_size[1], target_size[0])
-        labels_resizing = keras.layers.Resizing(output_res[1], output_res[0])
+        resizing = keras.layers.Resizing(target_size[1], target_size[0])
 
         def resize_images_and_masks(data):
             image = tf.image.convert_image_dtype(
                 data["image"], dtype=tf.float32
             )
-            data["image"] = image_resizing(image)
+            data["image"] = resizing(image)
             # WARNING: assumes processing unbatched
             mask = data["segmentation_mask"]
             mask = tf.image.convert_image_dtype(mask, dtype=tf.float32)
-            data["segmentation_mask"] = labels_resizing(mask)
+            data["segmentation_mask"] = resizing(mask)
             return data
 
         def keep_image_and_mask_only(data):
             return data["image"], data["segmentation_mask"]
 
         dataset = ds
         dataset = dataset.map(resize_images_and_masks)
@@ -150,20 +147,19 @@
             tf.data.experimental.dense_to_ragged_batch(batch_size)
         )
         training_dataset = training_dataset.repeat()
 
         epochs = 1
         model.compile(
             optimizer="adam",
-            loss=keras.losses.BinaryCrossentropy(from_logits=True),
+            loss=keras.losses.BinaryCrossentropy(from_logits=False),
             metrics=["accuracy"],
         )
 
         model.fit(
-            training_dataset,
+            training_dataset.take(10),
             epochs=epochs,
-            steps_per_epoch=num_images // batch_size,
         )
 
 
 if __name__ == "__main__":
     tf.test.main()
```

## keras_cv/models/object_detection/predict_utils.py

```diff
@@ -9,17 +9,23 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import tensorflow as tf
-from keras.engine.training import _minimum_control_deps
-from keras.engine.training import reduce_per_replica
-from keras.utils import tf_utils
+
+try:
+    from keras.src.engine.training import _minimum_control_deps
+    from keras.src.engine.training import reduce_per_replica
+    from keras.src.utils import tf_utils
+except ImportError:
+    from keras.engine.training import _minimum_control_deps
+    from keras.engine.training import reduce_per_replica
+    from keras.utils import tf_utils
 
 
 def make_predict_function(model, force=False):
     if model.predict_function is not None and not force:
         return model.predict_function
 
     def step_function(iterator):
```

## keras_cv/models/object_detection/retinanet/feature_pyramid.py

```diff
@@ -30,17 +30,17 @@
         self.conv_c5_3x3 = keras.layers.Conv2D(256, 3, 1, "same")
         self.conv_c6_3x3 = keras.layers.Conv2D(256, 3, 2, "same")
         self.conv_c7_3x3 = keras.layers.Conv2D(256, 3, 2, "same")
         self.upsample_2x = keras.layers.UpSampling2D(2)
 
     def call(self, inputs, training=False):
         if isinstance(inputs, dict):
-            c3_output = inputs[3]
-            c4_output = inputs[4]
-            c5_output = inputs[5]
+            c3_output = inputs["P3"]
+            c4_output = inputs["P4"]
+            c5_output = inputs["P5"]
         else:
             c3_output, c4_output, c5_output = inputs
         p3_output = self.conv_c3_1x1(c3_output, training=training)
         p4_output = self.conv_c4_1x1(c4_output, training=training)
         p5_output = self.conv_c5_1x1(c5_output, training=training)
         p4_output = p4_output + self.upsample_2x(p5_output, training=training)
         p3_output = p3_output + self.upsample_2x(p4_output, training=training)
```

## keras_cv/models/object_detection/retinanet/retinanet.py

```diff
@@ -89,17 +89,18 @@
         num_classes: the number of classes in your dataset excluding the
             background class. Classes should be represented by integers in the
             range [0, num_classes).
         bounding_box_format: The format of bounding boxes of input dataset.
             Refer
             [to the keras.io docs](https://keras.io/api/keras_cv/bounding_box/formats/)
             for more details on supported bounding box formats.
-        backbone: `keras.Model`. Must implement the `pyramid_level_inputs`
-            property with keys 3, 4, and 5 and layer names as values. A somewhat
-            sensible backbone to use in many cases is the:
+        backbone: `keras.Model`. If the default `feature_pyramid` is used,
+            must implement the `pyramid_level_inputs` property with keys "P3", "P4",
+            and "P5" and layer names as values. A somewhat sensible backbone
+            to use in many cases is the:
             `keras_cv.models.ResNetBackbone.from_preset("resnet50_imagenet")`
         anchor_generator: (Optional) a `keras_cv.layers.AnchorGenerator`. If
             provided, the anchor generator will be passed to both the
             `label_encoder` and the `prediction_decoder`. Only to be used when
             both `label_encoder` and `prediction_decoder` are both `None`.
             Defaults to an anchor generator with the parameterization:
             `strides=[2**i for i in range(3, 8)]`,
@@ -115,14 +116,18 @@
             argument.
         prediction_decoder: (Optional)  A `keras.layers.Layer` that is
             responsible for transforming RetinaNet predictions into usable
             bounding box Tensors. If not provided, a default is provided. The
             default `prediction_decoder` layer is a
             `keras_cv.layers.MultiClassNonMaxSuppression` layer, which uses
             a Non-Max Suppression for box pruning.
+        feature_pyramid: (Optional) A `keras.layers.Layer` that produces
+            a list of 4D feature maps (batch dimension included)
+            when called on the pyramid-level outputs of the `backbone`.
+            If not provided, the reference implementation from the paper will be used.
         classification_head: (Optional) A `keras.Layer` that performs
             classification of the bounding boxes. If not provided, a simple
             ConvNet with 3 layers will be used.
         box_head: (Optional) A `keras.Layer` that performs regression of the
             bounding boxes. If not provided, a simple ConvNet with 3 layers
             will be used.
     """  # noqa: E501
@@ -131,50 +136,50 @@
         self,
         backbone,
         num_classes,
         bounding_box_format,
         anchor_generator=None,
         label_encoder=None,
         prediction_decoder=None,
+        feature_pyramid=None,
         classification_head=None,
         box_head=None,
         **kwargs,
     ):
-        if anchor_generator is not None and (
-            prediction_decoder or label_encoder
-        ):
+        if anchor_generator is not None and label_encoder is not None:
             raise ValueError(
                 "`anchor_generator` is only to be provided when "
-                "both `label_encoder` and `prediction_decoder` are both "
-                f"`None`. Received `anchor_generator={anchor_generator}` "
-                f"`label_encoder={label_encoder}`, "
-                f"`prediction_decoder={prediction_decoder}`. To customize the "
-                "behavior of the anchor_generator inside of a custom "
-                "`label_encoder` or custom `prediction_decoder` you should "
+                "`label_encoder` is `None`. Received `anchor_generator="
+                f"{anchor_generator}`, label_encoder={label_encoder}`. To "
+                "customize the behavior of the anchor_generator inside of a "
+                "custom `label_encoder` you should provide both to `RetinaNet`"
                 "provide both to `RetinaNet`, and ensure that the "
                 "`anchor_generator` provided to both is identical"
             )
-        anchor_generator = (
-            anchor_generator
-            or RetinaNet.default_anchor_generator(bounding_box_format)
-        )
-        label_encoder = label_encoder or RetinaNetLabelEncoder(
-            bounding_box_format=bounding_box_format,
-            anchor_generator=anchor_generator,
-            box_variance=BOX_VARIANCE,
-        )
 
-        extractor_levels = [3, 4, 5]
+        if label_encoder is None:
+            anchor_generator = (
+                anchor_generator
+                or RetinaNet.default_anchor_generator(bounding_box_format)
+            )
+
+            label_encoder = RetinaNetLabelEncoder(
+                bounding_box_format=bounding_box_format,
+                anchor_generator=anchor_generator,
+                box_variance=BOX_VARIANCE,
+            )
+
+        extractor_levels = ["P3", "P4", "P5"]
         extractor_layer_names = [
             backbone.pyramid_level_inputs[i] for i in extractor_levels
         ]
         feature_extractor = get_feature_extractor(
             backbone, extractor_layer_names, extractor_levels
         )
-        feature_pyramid = FeaturePyramid()
+        feature_pyramid = feature_pyramid or FeaturePyramid()
 
         prior_probability = keras.initializers.Constant(
             -np.log((1 - 0.01) / 0.01)
         )
         classification_head = classification_head or PredictionHead(
             output_filters=9 * num_classes,
             bias_initializer=prior_probability,
@@ -213,15 +218,15 @@
 
         super().__init__(
             inputs=inputs,
             outputs=outputs,
             **kwargs,
         )
         self.label_encoder = label_encoder
-        self.anchor_generator = anchor_generator
+        self.anchor_generator = label_encoder.anchor_generator
         self.bounding_box_format = bounding_box_format
         self.num_classes = num_classes
         self.backbone = backbone
 
         self.feature_extractor = feature_extractor
         self._prediction_decoder = (
             prediction_decoder
@@ -430,14 +435,24 @@
             "box": box_pred,
             "classification": cls_pred,
         }
         sample_weights = {
             "box": box_weights,
             "classification": cls_weights,
         }
+        zero_weight = {
+            "box": tf.zeros_like(box_weights),
+            "classification": tf.zeros_like(cls_weights),
+        }
+
+        sample_weights = tf.cond(
+            normalizer == 0,
+            lambda: zero_weight,
+            lambda: sample_weights,
+        )
         return super().compute_loss(
             x=x, y=y_true, y_pred=y_pred, sample_weight=sample_weights
         )
 
     def train_step(self, data):
         x, y = unpack_input(data)
         y_for_label_encoder = bounding_box.convert_format(
@@ -518,18 +533,32 @@
     def get_config(self):
         return {
             "num_classes": self.num_classes,
             "bounding_box_format": self.bounding_box_format,
             "backbone": keras.utils.serialize_keras_object(self.backbone),
             "label_encoder": self.label_encoder,
             "prediction_decoder": self._prediction_decoder,
-            "classification_head": self.classification_head,
-            "box_head": self.box_head,
+            "classification_head": keras.utils.serialize_keras_object(
+                self.classification_head
+            ),
+            "box_head": keras.utils.serialize_keras_object(self.box_head),
         }
 
+    @classmethod
+    def from_config(cls, config):
+        if "box_head" in config and isinstance(config["box_head"], dict):
+            config["box_head"] = keras.layers.deserialize(config["box_head"])
+        if "classification_head" in config and isinstance(
+            config["classification_head"], dict
+        ):
+            config["classification_head"] = keras.layers.deserialize(
+                config["classification_head"]
+            )
+        return super().from_config(config)
+
     @classproperty
     def presets(cls):
         """Dictionary of preset names and configurations."""
         return copy.deepcopy({**backbone_presets, **retinanet_presets})
 
     @classproperty
     def presets_with_weights(cls):
```

## keras_cv/models/object_detection/retinanet/retinanet_presets.py

```diff
@@ -29,11 +29,11 @@
         },
         "config": {
             "backbone": resnet_v1_backbone_presets.backbone_presets["resnet50"],
             # 21 used as an implicit background class marginally improves
             # performance.
             "num_classes": 20,
         },
-        "weights_url": "https://storage.googleapis.com/keras-cv/models/retinanet/pascal_voc/resnet50-v3.weights.h5",  # noqa: E501
-        "weights_hash": "84f51edc5d669109187b9c60edee1e55",
+        "weights_url": "https://storage.googleapis.com/keras-cv/models/retinanet/pascal_voc/resnet50-v4.h5",  # noqa: E501
+        "weights_hash": "961f6f9a03c869900d61ae74a9f0c31c6439b4f81f13593f5c16c4733061fbac",  # noqa: E501
     },
 }
```

## keras_cv/models/object_detection/retinanet/retinanet_test.py

```diff
@@ -8,76 +8,101 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import copy
 import os
 
 import pytest
 import tensorflow as tf
+from absl.testing import parameterized
 from tensorflow import keras
 from tensorflow.keras import optimizers
 
 import keras_cv
+from keras_cv.models.backbones.test_backbone_presets import (
+    test_backbone_presets,
+)
 from keras_cv.models.object_detection.__test_utils__ import (
     _create_bounding_box_dataset,
 )
+from keras_cv.models.object_detection.retinanet import RetinaNetLabelEncoder
 
 
-class RetinaNetTest(tf.test.TestCase):
+class RetinaNetTest(tf.test.TestCase, parameterized.TestCase):
     @pytest.fixture(autouse=True)
     def cleanup_global_session(self):
         # Code before yield runs before the test
         tf.config.set_soft_device_placement(False)
         yield
         # Reset soft device placement to not interfere with other unit test
         # files
         tf.config.set_soft_device_placement(True)
         keras.backend.clear_session()
 
     def test_retinanet_construction(self):
         retinanet = keras_cv.models.RetinaNet(
             num_classes=20,
             bounding_box_format="xywh",
-            backbone=keras_cv.models.ResNet50V2Backbone(),
+            backbone=keras_cv.models.ResNet18V2Backbone(),
         )
         retinanet.compile(
             classification_loss="focal",
             box_loss="smoothl1",
             optimizer="adam",
         )
 
-        # TODO(lukewood) uncomment when using keras_cv.models.ResNet50
+        # TODO(lukewood) uncomment when using keras_cv.models.ResNet18
         # self.assertIsNotNone(retinanet.backbone.get_layer(name="rescaling"))
         # TODO(lukewood): test compile with the FocalLoss class
 
-    @pytest.mark.skipif(
-        "INTEGRATION" not in os.environ or os.environ["INTEGRATION"] != "true",
-        reason="Takes a long time to run, only runs when INTEGRATION "
-        "environment variable is set. To run the test please run: \n"
-        "`INTEGRATION=true pytest keras_cv/",
-    )
+    def test_retinanet_recompilation_without_metrics(self):
+        retinanet = keras_cv.models.RetinaNet(
+            num_classes=20,
+            bounding_box_format="xywh",
+            backbone=keras_cv.models.ResNet18V2Backbone(),
+        )
+        retinanet.compile(
+            classification_loss="focal",
+            box_loss="smoothl1",
+            optimizer="adam",
+            metrics=[
+                keras_cv.metrics.BoxCOCOMetrics(
+                    bounding_box_format="center_xywh", evaluate_freq=20
+                )
+            ],
+        )
+        self.assertIsNotNone(retinanet._user_metrics)
+        retinanet.compile(
+            classification_loss="focal",
+            box_loss="smoothl1",
+            optimizer="adam",
+            metrics=None,
+        )
+
+        self.assertIsNone(retinanet._user_metrics)
+
+    @pytest.mark.large  # Fit is slow, so mark these large.
     def test_retinanet_call(self):
         retinanet = keras_cv.models.RetinaNet(
             num_classes=20,
             bounding_box_format="xywh",
-            backbone=keras_cv.models.ResNet50V2Backbone(),
+            backbone=keras_cv.models.ResNet18V2Backbone(),
         )
         images = tf.random.uniform((2, 512, 512, 3))
         _ = retinanet(images)
         _ = retinanet.predict(images)
 
     def test_wrong_logits(self):
         retinanet = keras_cv.models.RetinaNet(
             num_classes=2,
             bounding_box_format="xywh",
-            backbone=keras_cv.models.ResNet50V2Backbone(),
+            backbone=keras_cv.models.ResNet18V2Backbone(),
         )
 
         with self.assertRaisesRegex(
             ValueError,
             "from_logits",
         ):
             retinanet.compile(
@@ -86,37 +111,20 @@
                     from_logits=False, reduction="none"
                 ),
                 box_loss=keras_cv.losses.SmoothL1Loss(
                     l1_cutoff=1.0, reduction="none"
                 ),
             )
 
-    def test_no_metrics(self):
-        retinanet = keras_cv.models.RetinaNet(
-            num_classes=2,
-            bounding_box_format="xywh",
-            backbone=keras_cv.models.ResNet50V2Backbone(),
-        )
-
-        retinanet.compile(
-            optimizer=optimizers.SGD(learning_rate=0.25),
-            classification_loss=keras_cv.losses.FocalLoss(
-                from_logits=True, reduction="none"
-            ),
-            box_loss=keras_cv.losses.SmoothL1Loss(
-                l1_cutoff=1.0, reduction="none"
-            ),
-        )
-
     def test_weights_contained_in_trainable_variables(self):
         bounding_box_format = "xywh"
         retinanet = keras_cv.models.RetinaNet(
             num_classes=2,
             bounding_box_format=bounding_box_format,
-            backbone=keras_cv.models.ResNet50V2Backbone(),
+            backbone=keras_cv.models.ResNet18V2Backbone(),
         )
         retinanet.backbone.trainable = False
         retinanet.compile(
             optimizer=optimizers.Adam(),
             classification_loss=keras_cv.losses.FocalLoss(
                 from_logits=True, reduction="none"
             ),
@@ -130,20 +138,50 @@
         _ = retinanet(xs)
         variable_names = [x.name for x in retinanet.trainable_variables]
         # classification_head
         self.assertIn("prediction_head/conv2d_8/kernel:0", variable_names)
         # box_head
         self.assertIn("prediction_head_1/conv2d_12/kernel:0", variable_names)
 
+    @pytest.mark.large  # Fit is slow, so mark these large.
+    def test_no_nans(self):
+        retina_net = keras_cv.models.RetinaNet(
+            num_classes=2,
+            bounding_box_format="xywh",
+            backbone=keras_cv.models.ResNet18V2Backbone(),
+        )
+
+        retina_net.compile(
+            optimizer=optimizers.Adam(),
+            classification_loss="focal",
+            box_loss="smoothl1",
+        )
+
+        # only a -1 box
+        xs = tf.ones((1, 512, 512, 3), tf.float32)
+        ys = {
+            "classes": tf.constant([[-1]], tf.float32),
+            "boxes": tf.constant([[[0, 0, 0, 0]]], tf.float32),
+        }
+        ds = tf.data.Dataset.from_tensor_slices((xs, ys))
+        ds = ds.repeat(2)
+        ds = ds.batch(2)
+        retina_net.fit(ds, epochs=1)
+
+        weights = retina_net.get_weights()
+        for weight in weights:
+            self.assertFalse(tf.math.reduce_any(tf.math.is_nan(weight)))
+
+    @pytest.mark.large  # Fit is slow, so mark these large.
     def test_weights_change(self):
         bounding_box_format = "xywh"
         retinanet = keras_cv.models.RetinaNet(
             num_classes=2,
             bounding_box_format=bounding_box_format,
-            backbone=keras_cv.models.ResNet50V2Backbone(),
+            backbone=keras_cv.models.ResNet18V2Backbone(),
         )
 
         retinanet.compile(
             optimizer=optimizers.Adam(),
             classification_loss=keras_cv.losses.FocalLoss(
                 from_logits=True, reduction="none"
             ),
@@ -178,43 +216,107 @@
             original_box_head_weights, box_head_after_fit_weights
         ):
             self.assertNotAllClose(w1, w2)
 
         for w1, w2 in zip(original_fpn_weights, fpn_after_fit):
             self.assertNotAllClose(w1, w2)
 
-    def test_serialization(self):
-        # TODO(haifengj): Reuse test code from
-        # ModelTest._test_model_serialization.
+    @parameterized.named_parameters(
+        ("tf_format", "tf", "model"),
+        ("keras_format", "keras_v3", "model.keras"),
+    )
+    @pytest.mark.large  # Saving is slow, so mark these large.
+    def test_saved_model(self, save_format, filename):
         model = keras_cv.models.RetinaNet(
             num_classes=20,
             bounding_box_format="xywh",
-            backbone=keras_cv.models.ResNet50V2Backbone(),
+            backbone=keras_cv.models.ResNet18V2Backbone(),
         )
-        serialized_1 = keras.utils.serialize_keras_object(model)
-        restored = keras.utils.deserialize_keras_object(
-            copy.deepcopy(serialized_1)
+        input_batch = tf.ones(shape=(2, 224, 224, 3))
+        model_output = model(input_batch)
+        save_path = os.path.join(self.get_temp_dir(), filename)
+        model.save(save_path, save_format=save_format)
+        restored_model = keras.models.load_model(save_path)
+
+        # Check we got the real object back.
+        self.assertIsInstance(restored_model, keras_cv.models.RetinaNet)
+
+        # Check that output matches.
+        restored_output = restored_model(input_batch)
+        self.assertAllClose(model_output, restored_output)
+
+    def test_call_with_custom_label_encoder(self):
+        anchor_generator = (
+            keras_cv.models.RetinaNet.default_anchor_generator("xywh"),
+        )
+        model = keras_cv.models.RetinaNet(
+            num_classes=20,
+            bounding_box_format="xywh",
+            backbone=keras_cv.models.ResNet18V2Backbone(),
+            label_encoder=RetinaNetLabelEncoder(
+                bounding_box_format="xywh",
+                anchor_generator=anchor_generator,
+                box_variance=[0.1, 0.1, 0.2, 0.2],
+            ),
         )
-        serialized_2 = keras.utils.serialize_keras_object(restored)
-        self.assertEqual(serialized_1, serialized_2)
+        model(tf.ones(shape=(2, 224, 224, 3)))
 
 
 @pytest.mark.large
-class RetinaNetSmokeTest(tf.test.TestCase):
-    def test_backbone_preset_weight_loading(self):
-        # Check that backbone preset weights loaded correctly
-        # TODO(lukewood): need to forward pass test once proper weights are
-        # implemented
-        keras_cv.models.RetinaNet.from_preset(
-            "resnet50_v2_imagenet",
+class RetinaNetSmokeTest(tf.test.TestCase, parameterized.TestCase):
+    @parameterized.named_parameters(
+        *[(preset, preset) for preset in test_backbone_presets]
+    )
+    def test_backbone_preset(self, preset):
+        model = keras_cv.models.RetinaNet.from_preset(
+            preset,
             num_classes=20,
             bounding_box_format="xywh",
         )
+        xs, _ = _create_bounding_box_dataset(bounding_box_format="xywh")
+        output = model(xs)
+
+        # 4 represents number of parameters in a box
+        # 49104 is the number of anchors for a 512x512 image
+        self.assertEqual(output["box"].shape, (xs.shape[0], 49104, 4))
 
     def test_full_preset_weight_loading(self):
-        # Check that backbone preset weights loaded correctly
-        # TODO(lukewood): need to forward pass test once proper weights are
-        # implemented
-        keras_cv.models.RetinaNet.from_preset(
+        model = keras_cv.models.RetinaNet.from_preset(
             "retinanet_resnet50_pascalvoc",
             bounding_box_format="xywh",
         )
+        xs = tf.ones((1, 512, 512, 3), tf.float32)
+        output = model(xs)
+
+        expected_box = tf.constant(
+            [-1.2427993, 0.05179548, -1.9953268, 0.32456252]
+        )
+        self.assertAllClose(output["box"][0, 123, :], expected_box, atol=1e-5)
+
+        expected_class = tf.constant(
+            [
+                -8.387445,
+                -7.891776,
+                -8.14204,
+                -8.117359,
+                -7.2517176,
+                -7.906804,
+                -7.0910635,
+                -8.295824,
+                -6.5567474,
+                -7.086027,
+                -6.3826647,
+                -7.960227,
+                -7.556676,
+                -8.28963,
+                -6.526232,
+                -7.071624,
+                -6.9687414,
+                -6.6398506,
+                -8.598567,
+                -6.484198,
+            ]
+        )
+        expected_class = tf.reshape(expected_class, (20,))
+        self.assertAllClose(
+            output["classification"][0, 123], expected_class, atol=1e-5
+        )
```

## keras_cv/models/object_detection/yolo_v8/yolo_v8_backbone.py

```diff
@@ -147,15 +147,15 @@
             kernel_size=3,
             strides=2,
             activation=activation,
             name="stem_2",
         )
 
         """ blocks """
-        pyramid_level_inputs = {1: x.node.layer.name}
+        pyramid_level_inputs = {"P1": x.node.layer.name}
         for stack_id, (channel, depth) in enumerate(
             zip(stackwise_channels, stackwise_depth)
         ):
             stack_name = f"stack{stack_id + 1}"
             if stack_id >= 1:
                 x = apply_conv_bn(
                     x,
@@ -176,15 +176,15 @@
             if stack_id == len(stackwise_depth) - 1:
                 x = apply_spatial_pyramid_pooling_fast(
                     x,
                     pool_size=5,
                     activation=activation,
                     name=f"{stack_name}_spp_fast",
                 )
-            pyramid_level_inputs[stack_id + 2] = x.node.layer.name
+            pyramid_level_inputs[f"P{stack_id + 2}"] = x.node.layer.name
 
         super().__init__(inputs=inputs, outputs=x, **kwargs)
         self.pyramid_level_inputs = pyramid_level_inputs
         self.stackwise_channels = stackwise_channels
         self.stackwise_depth = stackwise_depth
         self.include_rescaling = include_rescaling
         self.activation = activation
```

## keras_cv/models/object_detection/yolo_v8/yolo_v8_detector.py

```diff
@@ -8,33 +8,32 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import copy
+import warnings
 
 import tensorflow as tf
 from keras import layers
 from tensorflow import keras
 
 import keras_cv
 from keras_cv import bounding_box
+from keras_cv.losses.ciou_loss import CIoULoss
 from keras_cv.models.backbones.backbone_presets import backbone_presets
 from keras_cv.models.backbones.backbone_presets import (
     backbone_presets_with_weights,
 )
 from keras_cv.models.object_detection import predict_utils
 from keras_cv.models.object_detection.__internal__ import unpack_input
 from keras_cv.models.object_detection.yolo_v8.yolo_v8_detector_presets import (
     yolo_v8_detector_presets,
 )
-from keras_cv.models.object_detection.yolo_v8.yolo_v8_iou_loss import (
-    YOLOV8IoULoss,
-)
 from keras_cv.models.object_detection.yolo_v8.yolo_v8_label_encoder import (
     YOLOV8LabelEncoder,
 )
 from keras_cv.models.object_detection.yolo_v8.yolo_v8_layers import (
     apply_conv_bn,
 )
 from keras_cv.models.object_detection.yolo_v8.yolo_v8_layers import (
@@ -54,20 +53,20 @@
 ):
     """Gets anchor points for YOLOV8.
 
     YOLOV8 uses anchor points representing the center of proposed boxes, and
     matches ground truth boxes to anchors based on center points.
 
     Args:
-        image_shape: tuple or list of two integers representing the heigh and
+        image_shape: tuple or list of two integers representing the height and
             width of input images, respectively.
         strides: tuple of list of integers, the size of the strides across the
             image size that should be used to create anchors.
         base_anchors: tuple or list of two integers representing the offset from
-            (0,0) to start creating the center of anchor boxes, releative to the
+            (0,0) to start creating the center of anchor boxes, relative to the
             stride. For example, using the default (0.5, 0.5) creates the first
             anchor box for each stride such that its center is half of a stride
             from the edge of the image.
 
     Returns:
         A tuple of anchor centerpoints and anchor strides. Multiplying the
         two together will yield the centerpoints in absolute x,y format.
@@ -319,27 +318,28 @@
 
 
 @keras.utils.register_keras_serializable(package="keras_cv")
 class YOLOV8Detector(Task):
     """Implements the YOLOV8 architecture for object detection.
 
     Args:
+        backbone: `keras.Model`, must implement the `pyramid_level_inputs`
+            property with keys "P2", "P3", and "P4" and layer names as values.
+            A sensible backbone to use is the `keras_cv.models.YOLOV8Backbone`.
         num_classes: integer, the number of classes in your dataset excluding the
             background class. Classes should be represented by integers in the
             range [0, num_classes).
         bounding_box_format: string, the format of bounding boxes of input dataset.
             Refer
             [to the keras.io docs](https://keras.io/api/keras_cv/bounding_box/formats/)
             for more details on supported bounding box formats.
-        backbone: `keras.Model`, must implement the `pyramid_level_inputs`
-            property with keys 2, 3, and 4 and layer names as values. A
-            sensible backbone to use is the `keras_cv.models.YOLOV8Backbone`.
         fpn_depth: integer, a specification of the depth of the CSP blocks in
             the Feature Pyramid Network. This is usually 1, 2, or 3, depending
-            on the size of your YOLOV8Detector model.
+            on the size of your YOLOV8Detector model. We recommend using 3 for
+            "yolo_v8_l_backbone" and "yolo_v8_xl_backbone". Defaults to 2.
         label_encoder: (Optional)  A `YOLOV8LabelEncoder` that is
             responsible for transforming input boxes into trainable labels for
             YOLOV8Detector. If not provided, a default is provided.
         prediction_decoder: (Optional)  A `keras.layers.Layer` that is
             responsible for transforming YOLOV8 predictions into usable
             bounding boxes. If not provided, a default is provided. The
             default `prediction_decoder` layer is a
@@ -373,33 +373,33 @@
 
     # Get predictions using the model
     model.predict(images)
 
     # Train model
     model.compile(
         classification_loss='binary_crossentropy',
-        box_loss='iou',
+        box_loss='ciou',
         optimizer=tf.optimizers.SGD(global_clipnorm=10.0),
         jit_compile=False,
     )
     model.fit(images, labels)
     ```
     """  # noqa: E501
 
     def __init__(
         self,
+        backbone,
         num_classes,
         bounding_box_format,
-        backbone,
-        fpn_depth,
+        fpn_depth=2,
         label_encoder=None,
         prediction_decoder=None,
         **kwargs,
     ):
-        extractor_levels = [3, 4, 5]
+        extractor_levels = ["P3", "P4", "P5"]
         extractor_layer_names = [
             backbone.pyramid_level_inputs[i] for i in extractor_levels
         ]
         feature_extractor = get_feature_extractor(
             backbone, extractor_layer_names, extractor_levels
         )
 
@@ -421,15 +421,15 @@
             [outputs["classes"]]
         )
 
         outputs = {"boxes": boxes, "classes": scores}
         super().__init__(inputs=images, outputs=outputs, **kwargs)
 
         self.bounding_box_format = bounding_box_format
-        self.prediction_decoder = (
+        self._prediction_decoder = (
             prediction_decoder
             or keras_cv.layers.MultiClassNonMaxSuppression(
                 bounding_box_format=bounding_box_format,
                 from_logits=False,
                 confidence_threshold=0.2,
                 iou_threshold=0.7,
             )
@@ -454,35 +454,41 @@
 
         `compile()` mirrors the standard Keras `compile()` method, but has one
         key distinction -- two losses must be provided: `box_loss` and
         `classification_loss`.
 
         Args:
             box_loss: a Keras loss to use for box offset regression. A
-                preconfigured loss is provided when the string "iou" is passed.
+                preconfigured loss is provided when the string "ciou" is passed.
             classification_loss: a Keras loss to use for box classification. A
                 preconfigured loss is provided when the string
                 "binary_crossentropy" is passed.
             box_loss_weight: (optional) float, a scaling factor for the box
                 loss. Defaults to 7.5.
             classification_loss_weight: (optional) float, a scaling factor for
                 the classification loss. Defaults to 0.5.
             kwargs: most other `keras.Model.compile()` arguments are supported
                 and propagated to the `keras.Model` class.
         """
         if metrics is not None:
             raise ValueError("User metrics not yet supported for YOLOV8")
 
         if isinstance(box_loss, str):
-            if box_loss == "iou":
-                box_loss = YOLOV8IoULoss(reduction="sum")
+            if box_loss == "ciou":
+                box_loss = CIoULoss(bounding_box_format="xyxy", reduction="sum")
+            elif box_loss == "iou":
+                warnings.warn(
+                    "YOLOV8 recommends using CIoU loss, but was configured to "
+                    "use standard IoU. Consider using `box_loss='ciou'` "
+                    "instead."
+                )
             else:
                 raise ValueError(
                     f"Invalid box loss for YOLOV8Detector: {box_loss}. Box "
-                    "loss should be a keras.Loss or the string 'iou'."
+                    "loss should be a keras.Loss or the string 'ciou'."
                 )
         if isinstance(classification_loss, str):
             if classification_loss == "binary_crossentropy":
                 classification_loss = keras.losses.BinaryCrossentropy(
                     reduction="sum"
                 )
             else:
@@ -602,22 +608,42 @@
         )
 
         return self.prediction_decoder(box_preds, scores)
 
     def make_predict_function(self, force=False):
         return predict_utils.make_predict_function(self, force=force)
 
+    @property
+    def prediction_decoder(self):
+        return self._prediction_decoder
+
+    @prediction_decoder.setter
+    def prediction_decoder(self, prediction_decoder):
+        if prediction_decoder.bounding_box_format != self.bounding_box_format:
+            raise ValueError(
+                "Expected `prediction_decoder` and YOLOV8Detector to "
+                "use the same `bounding_box_format`, but got "
+                "`prediction_decoder.bounding_box_format="
+                f"{prediction_decoder.bounding_box_format}`, and "
+                "`self.bounding_box_format="
+                f"{self.bounding_box_format}`."
+            )
+        self._prediction_decoder = prediction_decoder
+        self.make_predict_function(force=True)
+        self.make_train_function(force=True)
+        self.make_test_function(force=True)
+
     def get_config(self):
         return {
             "num_classes": self.num_classes,
             "bounding_box_format": self.bounding_box_format,
             "fpn_depth": self.fpn_depth,
             "backbone": keras.utils.serialize_keras_object(self.backbone),
             "label_encoder": self.label_encoder,
-            "prediction_decoder": self.prediction_decoder,
+            "prediction_decoder": self._prediction_decoder,
         }
 
     @classproperty
     def presets(cls):
         """Dictionary of preset names and configurations."""
         return copy.deepcopy({**backbone_presets, **yolo_v8_detector_presets})
```

## keras_cv/models/object_detection/yolo_v8/yolo_v8_detector_presets.py

```diff
@@ -29,11 +29,11 @@
         "config": {
             "backbone": yolo_v8_backbone_presets.backbone_presets[
                 "yolo_v8_m_backbone"
             ],
             "num_classes": 20,
             "fpn_depth": 2,
         },
-        "weights_url": "https://storage.googleapis.com/keras-cv/models/yolov8/pascal_voc/yolov8_m.h5",  # noqa: E501
-        "weights_hash": "e641690aec205a3ca1ea730ea362ddc36c8b4a5abcebb6a23b18cbc9c091316d",  # noqa: E501
+        "weights_url": "https://storage.googleapis.com/keras-cv/models/yolov8/pascal_voc/yolov8_m_v1.h5",  # noqa: E501
+        "weights_hash": "2891fbd66f71e0b9da0cb02ef3afbccb819e1b8f18204157f643f4ec058a71a8",  # noqa: E501
     },
 }
```

## keras_cv/models/object_detection/yolo_v8/yolo_v8_detector_test.py

```diff
@@ -8,47 +8,100 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import copy
+import os
 
 import pytest
 import tensorflow as tf
+from absl.testing import parameterized
 from tensorflow import keras
 
 import keras_cv
+from keras_cv import bounding_box
+from keras_cv.models.backbones.test_backbone_presets import (
+    test_backbone_presets,
+)
 from keras_cv.models.object_detection.__test_utils__ import (
     _create_bounding_box_dataset,
 )
 from keras_cv.models.object_detection.yolo_v8.yolo_v8_detector_presets import (
     yolo_v8_detector_presets,
 )
 
 
-class YOLOV8DetectorTest(tf.test.TestCase):
+class YOLOV8DetectorTest(tf.test.TestCase, parameterized.TestCase):
+    @pytest.mark.large  # Fit is slow, so mark these large.
     def test_fit(self):
         bounding_box_format = "xywh"
         yolo = keras_cv.models.YOLOV8Detector(
             num_classes=2,
             fpn_depth=1,
             bounding_box_format=bounding_box_format,
             backbone=keras_cv.models.YOLOV8Backbone.from_preset(
                 "yolo_v8_xs_backbone"
             ),
         )
 
         yolo.compile(
             optimizer="adam",
             classification_loss="binary_crossentropy",
-            box_loss="iou",
+            box_loss="ciou",
+        )
+        xs, ys = _create_bounding_box_dataset(bounding_box_format)
+
+        yolo.fit(x=xs, y=ys, epochs=1)
+
+    @pytest.mark.large  # Fit is slow, so mark these large.
+    def test_fit_with_ragged_tensors(self):
+        bounding_box_format = "xywh"
+        yolo = keras_cv.models.YOLOV8Detector(
+            num_classes=2,
+            fpn_depth=1,
+            bounding_box_format=bounding_box_format,
+            backbone=keras_cv.models.YOLOV8Backbone.from_preset(
+                "yolo_v8_xs_backbone"
+            ),
+        )
+
+        yolo.compile(
+            optimizer="adam",
+            classification_loss="binary_crossentropy",
+            box_loss="ciou",
         )
         xs, ys = _create_bounding_box_dataset(bounding_box_format)
+        ys = bounding_box.to_ragged(ys)
+
+        yolo.fit(x=xs, y=ys, epochs=1)
+
+    @pytest.mark.large  # Fit is slow, so mark these large.
+    def test_fit_with_no_valid_gt_bbox(self):
+        bounding_box_format = "xywh"
+        yolo = keras_cv.models.YOLOV8Detector(
+            num_classes=1,
+            fpn_depth=1,
+            bounding_box_format=bounding_box_format,
+            backbone=keras_cv.models.YOLOV8Backbone.from_preset(
+                "yolo_v8_xs_backbone"
+            ),
+        )
+
+        yolo.compile(
+            optimizer="adam",
+            classification_loss="binary_crossentropy",
+            box_loss="ciou",
+        )
+        xs, ys = _create_bounding_box_dataset(bounding_box_format)
+        # Make all bounding_boxes invalid and filter out them
+        ys["classes"] = -tf.ones_like(ys["classes"])
+        ys = bounding_box.to_ragged(ys)
+
         yolo.fit(x=xs, y=ys, epochs=1)
 
     def test_trainable_weight_count(self):
         yolo = keras_cv.models.YOLOV8Detector(
             num_classes=2,
             fpn_depth=1,
             bounding_box_format="xywh",
@@ -77,36 +130,95 @@
                 box_loss="bad_loss", classification_loss="binary_crossentropy"
             )
 
         with self.assertRaisesRegex(
             ValueError,
             "Invalid classification loss",
         ):
-            yolo.compile(box_loss="iou", classification_loss="bad_loss")
+            yolo.compile(box_loss="ciou", classification_loss="bad_loss")
 
-    def test_serialization(self):
+    @parameterized.named_parameters(
+        ("tf_format", "tf", "model"),
+        ("keras_format", "keras_v3", "model.keras"),
+    )
+    @pytest.mark.large  # Saving is slow, so mark these large.
+    def test_saved_model(self, save_format, filename):
         model = keras_cv.models.YOLOV8Detector(
             num_classes=20,
             bounding_box_format="xywh",
             fpn_depth=1,
             backbone=keras_cv.models.YOLOV8Backbone.from_preset(
                 "yolo_v8_xs_backbone"
             ),
         )
-        serialized_1 = keras.utils.serialize_keras_object(model)
-        restored = keras.utils.deserialize_keras_object(
-            copy.deepcopy(serialized_1)
+        xs, _ = _create_bounding_box_dataset("xywh")
+        model_output = model(xs)
+        save_path = os.path.join(self.get_temp_dir(), filename)
+        model.save(save_path, save_format=save_format)
+        restored_model = keras.models.load_model(save_path)
+
+        # Check we got the real object back.
+        self.assertIsInstance(restored_model, keras_cv.models.YOLOV8Detector)
+
+        # Check that output matches.
+        restored_output = restored_model(xs)
+        self.assertAllClose(model_output, restored_output)
+
+    def test_update_prediction_decoder(self):
+        yolo = keras_cv.models.YOLOV8Detector(
+            num_classes=2,
+            fpn_depth=1,
+            bounding_box_format="xywh",
+            backbone=keras_cv.models.YOLOV8Backbone.from_preset(
+                "yolo_v8_s_backbone"
+            ),
+            prediction_decoder=keras_cv.layers.MultiClassNonMaxSuppression(
+                bounding_box_format="xywh",
+                from_logits=False,
+                confidence_threshold=0.0,
+                iou_threshold=1.0,
+            ),
+        )
+
+        image = tf.ones((1, 512, 512, 3))
+
+        outputs = yolo.predict(image)
+        # We predicted at least 1 box with confidence_threshold 0
+        self.assertGreater(outputs["boxes"]._values.shape[0], 0)
+
+        yolo.prediction_decoder = keras_cv.layers.MultiClassNonMaxSuppression(
+            bounding_box_format="xywh",
+            from_logits=False,
+            confidence_threshold=1.0,
+            iou_threshold=1.0,
         )
-        serialized_2 = keras.utils.serialize_keras_object(restored)
-        self.assertEqual(serialized_1, serialized_2)
+
+        outputs = yolo.predict(image)
+        # We predicted no boxes with confidence threshold 1
+        self.assertEqual(outputs["boxes"]._values.shape[0], 0)
 
 
 @pytest.mark.large
-class YOLOV8DetectorSmokeTest(tf.test.TestCase):
-    # TODO(ianstenbit): Update this test to use a KerasCV-trained preset.
+class YOLOV8DetectorSmokeTest(tf.test.TestCase, parameterized.TestCase):
+    @parameterized.named_parameters(
+        *[(preset, preset) for preset in test_backbone_presets]
+    )
+    def test_backbone_preset(self, preset):
+        model = keras_cv.models.YOLOV8Detector.from_preset(
+            preset,
+            num_classes=20,
+            bounding_box_format="xywh",
+        )
+        xs, _ = _create_bounding_box_dataset(bounding_box_format="xywh")
+        output = model(xs)
+
+        # 64 represents number of parameters in a box
+        # 5376 is the number of anchors for a 512x512 image
+        self.assertEqual(output["boxes"].shape, (xs.shape[0], 5376, 64))
+
     def test_preset_with_forward_pass(self):
         model = keras_cv.models.YOLOV8Detector.from_preset(
             "yolo_v8_m_pascalvoc",
             bounding_box_format="xywh",
         )
 
         image = tf.ones((1, 512, 512, 3))
```

## keras_cv/models/object_detection/yolo_v8/yolo_v8_label_encoder.py

```diff
@@ -15,15 +15,16 @@
 and is adapted from https://github.com/ultralytics/ultralytics/blob/main/ultralytics/yolo/utils/tal.py
 """  # noqa: E501
 
 import tensorflow as tf
 from tensorflow import keras
 from tensorflow.keras import layers
 
-from keras_cv.models.object_detection.yolo_v8.yolo_v8_iou_loss import bbox_iou
+from keras_cv import bounding_box
+from keras_cv.bounding_box.iou import compute_ciou
 
 
 def select_highest_overlaps(mask_pos, overlaps, max_num_boxes):
     """Break ties when two GT boxes match to the same anchor.
 
     Picks the GT box with the highest IoU.
     """
@@ -34,15 +35,16 @@
         fg_mask, mask_pos, overlaps, max_num_boxes
     ):
         mask_multi_gts = tf.repeat(
             tf.expand_dims(fg_mask, axis=1) > 1, max_num_boxes, axis=1
         )  # (b, max_num_boxes, num_anchors)
         max_overlaps_idx = tf.argmax(overlaps, axis=1)  # (b, num_anchors)
         is_max_overlaps = tf.one_hot(
-            max_overlaps_idx, max_num_boxes
+            max_overlaps_idx,
+            tf.cast(max_num_boxes, dtype=tf.int32),  # tf.one_hot must use int32
         )  # (b, num_anchors, max_num_boxes)
         is_max_overlaps = tf.cast(
             tf.transpose(is_max_overlaps, perm=(0, 2, 1)), overlaps.dtype
         )  # (b, max_num_boxes, num_anchors)
         mask_pos = tf.where(
             mask_multi_gts, is_max_overlaps, mask_pos
         )  # (b, max_num_boxes, num_anchors)
@@ -66,15 +68,15 @@
 
     Returns:
         a boolean mask Tensor of shape (batch_size, num_gt_boxes, num_anchors)
         where the value is `True` if the anchor point falls inside the gt box,
         and `False` otherwise.
     """
     n_anchors = xy_centers.shape[0]
-    bs, n_boxes, _ = gt_bboxes.shape
+    n_boxes = tf.shape(gt_bboxes)[1]
 
     left_top, right_bottom = tf.split(
         tf.reshape(gt_bboxes, (-1, 1, 4)), 2, axis=-1
     )
     bbox_deltas = tf.reshape(
         tf.concat(
             [
@@ -117,15 +119,15 @@
     def __init__(
         self,
         num_classes,
         max_anchor_matches=10,
         alpha=0.5,
         beta=6.0,
         epsilon=1e-9,
-        **kwargs
+        **kwargs,
     ):
         super().__init__(**kwargs)
         self.max_anchor_matches = max_anchor_matches
         self.num_classes = num_classes
         self.alpha = alpha
         self.beta = beta
         self.epsilon = epsilon
@@ -158,57 +160,83 @@
                 - A Float Tensor of shape (batch_size, num_anchors, num_classes)
                     representing class targets for the model.
                 - A Boolean Tensor of shape (batch_size, num_anchors)
                     representing whether each anchor was a match with a ground
                     truth box. Anchors that didn't match with a ground truth
                     box should be excluded from both class and box losses.
         """
-        max_num_boxes = gt_bboxes.shape[1]
+        if isinstance(gt_bboxes, tf.RaggedTensor):
+            dense_bounding_boxes = bounding_box.to_dense(
+                {"boxes": gt_bboxes, "classes": gt_labels},
+            )
+            gt_bboxes = dense_bounding_boxes["boxes"]
+            gt_labels = dense_bounding_boxes["classes"]
 
-        mask_pos, align_metric, overlaps = self.get_pos_mask(
-            pd_scores,
-            pd_bboxes,
-            gt_labels,
-            gt_bboxes,
-            anc_points,
-            mask_gt,
-            max_num_boxes,
-        )
+        if isinstance(mask_gt, tf.RaggedTensor):
+            mask_gt = mask_gt.to_tensor()
 
-        target_gt_idx, fg_mask, mask_pos = select_highest_overlaps(
-            mask_pos, overlaps, max_num_boxes
-        )
+        max_num_boxes = tf.cast(tf.shape(gt_bboxes)[1], dtype=tf.int64)
 
-        target_bboxes, target_scores = self.get_targets(
-            gt_labels, gt_bboxes, target_gt_idx, fg_mask, max_num_boxes
-        )
+        def encode_to_targets(
+            pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt
+        ):
+            mask_pos, align_metric, overlaps = self.get_pos_mask(
+                pd_scores,
+                pd_bboxes,
+                gt_labels,
+                gt_bboxes,
+                anc_points,
+                mask_gt,
+                max_num_boxes,
+            )
 
-        align_metric *= mask_pos
-        pos_align_metrics = tf.reduce_max(
-            align_metric, axis=-1, keepdims=True
-        )  # b, max_num_boxes
-        pos_overlaps = tf.reduce_max(
-            overlaps * mask_pos, axis=-1, keepdims=True
-        )  # b, max_num_boxes
-        norm_align_metric = tf.expand_dims(
-            tf.reduce_max(
-                align_metric
-                * pos_overlaps
-                / (pos_align_metrics + self.epsilon),
-                axis=-2,
-            ),
-            axis=-1,
-        )
-        target_scores = target_scores * norm_align_metric
+            target_gt_idx, fg_mask, mask_pos = select_highest_overlaps(
+                mask_pos, overlaps, max_num_boxes
+            )
+
+            target_bboxes, target_scores = self.get_targets(
+                gt_labels, gt_bboxes, target_gt_idx, fg_mask, max_num_boxes
+            )
+
+            align_metric *= mask_pos
+            pos_align_metrics = tf.reduce_max(
+                align_metric, axis=-1, keepdims=True
+            )  # b, max_num_boxes
+            pos_overlaps = tf.reduce_max(
+                overlaps * mask_pos, axis=-1, keepdims=True
+            )  # b, max_num_boxes
+            norm_align_metric = tf.expand_dims(
+                tf.reduce_max(
+                    align_metric
+                    * pos_overlaps
+                    / (pos_align_metrics + self.epsilon),
+                    axis=-2,
+                ),
+                axis=-1,
+            )
+            target_scores = target_scores * norm_align_metric
+
+            # No need to compute gradients for these, as they're all targets
+            return (
+                tf.stop_gradient(target_bboxes),
+                tf.stop_gradient(target_scores),
+                tf.stop_gradient(tf.cast(fg_mask, tf.bool)),
+            )
 
-        # No need to compute gradients for these, as they're all targets
-        return (
-            tf.stop_gradient(target_bboxes),
-            tf.stop_gradient(target_scores),
-            tf.stop_gradient(tf.cast(fg_mask, tf.bool)),
+        # return zeros if no gt boxes are present
+        return tf.cond(
+            max_num_boxes > 0,
+            lambda: encode_to_targets(
+                pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt
+            ),
+            lambda: (
+                tf.zeros_like(pd_bboxes),
+                tf.zeros_like(pd_scores),
+                tf.zeros_like(pd_scores[..., 0], dtype=tf.bool),
+            ),
         )
 
     def get_pos_mask(
         self,
         pd_scores,
         pd_bboxes,
         gt_labels,
@@ -275,30 +303,33 @@
         na = pd_bboxes.shape[-2]
         mask_gt = tf.cast(mask_gt, tf.bool)  # b, max_num_boxes, num_anchors
 
         ind_1 = tf.cast(gt_labels, tf.int64)
         pd_scores = tf.gather(
             pd_scores, tf.math.maximum(ind_1, 0), axis=-1, batch_dims=1
         )
-        pd_scores = tf.where(ind_1[:, tf.newaxis, :] >= 0, pd_scores, 0)
+        pd_scores = tf.where(ind_1[:, tf.newaxis, :] >= 0, pd_scores, 0.0)
         pd_scores = tf.transpose(pd_scores, perm=(0, 2, 1))
 
-        bbox_scores = tf.where(mask_gt, pd_scores, 0)
+        bbox_scores = tf.where(mask_gt, pd_scores, 0.0)
 
         pd_boxes = tf.repeat(
             tf.expand_dims(pd_bboxes, axis=1), max_num_boxes, axis=1
         )
 
         gt_boxes = tf.repeat(tf.expand_dims(gt_bboxes, axis=2), na, axis=2)
 
-        iou = tf.squeeze(bbox_iou(gt_boxes, pd_boxes), axis=-1)
-        iou = tf.where(iou > 0, iou, 0)
+        iou = tf.squeeze(
+            compute_ciou(gt_boxes, pd_boxes, bounding_box_format="xyxy"),
+            axis=-1,
+        )
+        iou = tf.where(iou > 0, iou, 0.0)
 
         iou = tf.reshape(iou, (-1, max_num_boxes, na))
-        overlaps = tf.where(mask_gt, iou, 0)
+        overlaps = tf.where(mask_gt, iou, 0.0)
 
         align_metric = tf.math.pow(bbox_scores, self.alpha) * tf.math.pow(
             overlaps, self.beta
         )
         return align_metric, overlaps
 
     def select_topk_candidates(self, metrics, topk_mask):
@@ -349,15 +380,15 @@
         """
 
         batch_ind = tf.range(tf.shape(gt_labels)[0], dtype=tf.int64)[
             ..., tf.newaxis
         ]
         target_gt_idx = target_gt_idx + batch_ind * max_num_boxes
 
-        gt_bboxes = tf.reshape(gt_bboxes, (-1, gt_bboxes.shape[1], 4))
+        gt_bboxes = tf.reshape(gt_bboxes, (-1, tf.shape(gt_bboxes)[1], 4))
 
         target_labels = tf.gather(
             tf.reshape(tf.cast(gt_labels, tf.int64), (-1,)), target_gt_idx
         )  # (b, num_anchors)
 
         # assigned target boxes, (b, max_num_boxes, 4) -> (b, num_anchors)
         target_bboxes = tf.gather(
@@ -372,14 +403,19 @@
         fg_scores_mask = tf.repeat(
             fg_mask[:, :, tf.newaxis], self.num_classes, axis=2
         )  # (b, num_anchors, num_classes)
         target_scores = tf.where(fg_scores_mask > 0, target_scores, 0)
 
         return target_bboxes, target_scores
 
+    def count_params(self):
+        # The label encoder has no weights, so we short-circuit the weight
+        # counting to avoid having to `build` this layer unnecessarily.
+        return 0
+
     def get_config(self):
         config = {
             "max_anchor_matches": self.max_anchor_matches,
             "num_classes": self.num_classes,
             "alpha": self.alpha,
             "beta": self.beta,
             "epsilon": self.epsilon,
```

## keras_cv/models/stable_diffusion/noise_scheduler.py

```diff
@@ -27,15 +27,15 @@
         beta_end: the final `beta` value.
         beta_schedule: the beta schedule, a mapping from a beta range to a
             sequence of betas for stepping the model. Choose from `linear` or
             `quadratic`.
         betas: a complete set of betas, in lieu of using one of the existing
             schedules.
         variance_type: options to clip the variance used when adding noise to
-            the denoised sample. Choose from `fixed_small`, `fixed_small_log`,
+            the de-noised sample. Choose from `fixed_small`, `fixed_small_log`,
             `fixed_large`, `fixed_large_log`, `learned` or `learned_range`.
         clip_sample: option to clip predicted sample between -1 and 1 for
             numerical stability.
     """
 
     def __init__(
         self,
```

## keras_cv/models/stable_diffusion/stable_diffusion.py

```diff
@@ -89,15 +89,15 @@
 
     def encode_text(self, prompt):
         """Encodes a prompt into a latent text encoding.
 
         The encoding produced by this method should be used as the
         `encoded_text` parameter of `StableDiffusion.generate_image`. Encoding
         text separately from generating an image can be used to arbitrarily
-        modify the text encoding priot to image generation, e.g. for walking
+        modify the text encoding prior to image generation, e.g. for walking
         between two prompts.
 
         Args:
             prompt: a string to encode, must be 77 tokens or shorter.
 
         Example:
 
@@ -476,15 +476,15 @@
         img_width=512,
         jit_compile=False,
     ):
         super().__init__(img_height, img_width, jit_compile)
         print(
             "By using this model checkpoint, you acknowledge that its usage is "
             "subject to the terms of the CreativeML Open RAIL++-M license at "
-            "https://github.com/Stability-AI/stablediffusion/main/LICENSE-MODEL"
+            "https://github.com/Stability-AI/stablediffusion/blob/main/LICENSE-MODEL"  # noqa: E501
         )
 
     @property
     def text_encoder(self):
         """text_encoder returns the text encoder with pretrained weights.
         Can be overriden for tasks like textual inversion where the text encoder
         needs to be modified.
```

## keras_cv/ops/iou_3d_test.py

```diff
@@ -30,15 +30,15 @@
     )
     def testOpCall(self):
         # Predicted boxes:
         # 0: a 2x2x2 box centered at 0,0,0, rotated 0 degrees
         # 1: a 2x2x2 box centered at 1,1,1, rotated 135 degrees
         # Ground Truth boxes:
         # 0: a 2x2x2 box centered at 1,1,1, rotated 45 degrees
-        #    (idential to predicted box 1)
+        #    (identical to predicted box 1)
         # 1: a 2x2x2 box centered at 1,1,1, rotated 0 degrees
         box_preds = [[0, 0, 0, 2, 2, 2, 0], [1, 1, 1, 2, 2, 2, 3 * math.pi / 4]]
         box_gt = [[1, 1, 1, 2, 2, 2, math.pi / 4], [1, 1, 1, 2, 2, 2, 0]]
 
         # Predicted box 0 and both ground truth boxes overlap by 1/8th of the
         # box. Therefore, IiU is 1/15.
         # Predicted box 1 is the same as ground truth box 0, therefore IoU is 1.
```

## keras_cv/training/contrastive/contrastive_trainer.py

```diff
@@ -40,18 +40,20 @@
 
     Returns:
       A `keras.Model` instance.
 
 
     Usage:
     ```python
-    encoder = keras_cv.models.DenseNet121(
-        include_rescaling=True,
-        include_top=False,
-        pooling="avg")
+    encoder = keras.Sequential(
+        [
+            DenseNet121Backbone(include_rescaling=False),
+            layers.GlobalAveragePooling2D(name="avg_pool"),
+        ],
+    )
     augmenter = keras_cv.layers.preprocessing.RandomFlip()
     projector = keras.layers.Dense(64)
     probe = keras_cv.training.ContrastiveTrainer.linear_probe(num_classes=10)
 
     trainer = keras_cv.training.ContrastiveTrainer(
         encoder=encoder,
         augmenter=augmenter,
@@ -103,14 +105,18 @@
                 "exactly 2 augmenters."
             )
 
         self.augmenters = (
             augmenter if type(augmenter) is tuple else (augmenter, augmenter)
         )
         self.encoder = encoder
+        # Check to see if the projector is being shared or are distinct.
+        self._is_shared_projector = (
+            True if not isinstance(projector, tuple) else False
+        )
         self.projectors = (
             projector if type(projector) is tuple else (projector, projector)
         )
         self.probe = probe
 
         self.loss_metric = keras.metrics.Mean(name="loss")
 
@@ -224,27 +230,31 @@
 
             loss = self.compiled_loss(
                 projections_0,
                 projections_1,
                 regularization_losses=self.encoder.losses,
             )
 
+        # If the projector is shared, then take the trainable weights of just
+        # one of the projectors in the tuple. If not, use both the projectors.
+        projector_weights = (
+            self.projectors[0].trainable_weights
+            if self._is_shared_projector
+            else self.projectors[0].trainable_weights
+            + self.projectors[1].trainable_weights
+        )
         gradients = tape.gradient(
             loss,
-            self.encoder.trainable_weights
-            + self.projectors[0].trainable_weights
-            + self.projectors[1].trainable_weights,
+            self.encoder.trainable_weights + projector_weights,
         )
 
         self.optimizer.apply_gradients(
             zip(
                 gradients,
-                self.encoder.trainable_weights
-                + self.projectors[0].trainable_weights
-                + self.projectors[1].trainable_weights,
+                self.encoder.trainable_weights + projector_weights,
             )
         )
         self.loss_metric.update_state(loss)
 
         if self.probe:
             if labels is None:
                 raise ValueError(
```

## keras_cv/training/contrastive/contrastive_trainer_test.py

```diff
@@ -8,26 +8,30 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import pytest
 import tensorflow as tf
 from tensorflow import keras
 from tensorflow.keras import layers
 from tensorflow.keras import metrics
 from tensorflow.keras import optimizers
 
 from keras_cv.layers import preprocessing
 from keras_cv.losses import SimCLRLoss
-from keras_cv.models.legacy import DenseNet121
+from keras_cv.models import DenseNet121Backbone
 from keras_cv.training import ContrastiveTrainer
 
 
+# TODO(jbischof): revisit "extra_large" tag once development resumes.
+# These tests are currently some of the slowest in our repo.
+@pytest.mark.extra_large
 class ContrastiveTrainerTest(tf.test.TestCase):
     def test_probe_requires_probe_optimizer(self):
         trainer = ContrastiveTrainer(
             encoder=self.build_encoder(),
             augmenter=self.build_augmenter(),
             projector=self.build_projector(),
             probe=self.build_probe(),
@@ -125,15 +129,15 @@
         with self.assertRaises(NotImplementedError):
             trainer(tf.ones((1, 50, 50, 3)))
 
     def test_encoder_must_have_flat_output(self):
         with self.assertRaises(ValueError):
             _ = ContrastiveTrainer(
                 # A DenseNet without pooling does not have a flat output
-                encoder=DenseNet121(include_rescaling=False, include_top=False),
+                encoder=DenseNet121Backbone(include_rescaling=False),
                 augmenter=self.build_augmenter(),
                 projector=self.build_projector(),
                 probe=None,
             )
 
     def test_with_multiple_augmenters_and_projectors(self):
         augmenter0 = preprocessing.RandomFlip("horizontal")
@@ -160,16 +164,19 @@
 
         trainer_without_probing.fit(images)
 
     def build_augmenter(self):
         return preprocessing.RandomFlip("horizontal")
 
     def build_encoder(self):
-        return DenseNet121(
-            include_rescaling=False, include_top=False, pooling="avg"
+        return keras.Sequential(
+            [
+                DenseNet121Backbone(include_rescaling=False),
+                layers.GlobalAveragePooling2D(name="avg_pool"),
+            ],
         )
 
     def build_projector(self):
         return layers.Dense(128)
 
     def build_probe(self, num_classes=20):
         return layers.Dense(num_classes)
```

## keras_cv/training/contrastive/simclr_trainer.py

```diff
@@ -70,19 +70,19 @@
             [
                 preprocessing.RandomFlip("horizontal"),
                 preprocessing.RandomCropAndResize(
                     target_size=(height, width),
                     crop_area_factor=crop_area_factor,
                     aspect_ratio_factor=aspect_ratio_factor,
                 ),
-                preprocessing.MaybeApply(
+                preprocessing.RandomApply(
                     preprocessing.Grayscale(output_channels=3),
                     rate=grayscale_rate,
                 ),
-                preprocessing.MaybeApply(
+                preprocessing.RandomApply(
                     preprocessing.RandomColorJitter(
                         value_range=value_range,
                         brightness_factor=brightness_factor,
                         contrast_factor=contrast_factor,
                         saturation_factor=saturation_factor,
                         hue_factor=hue_factor,
                     ),
```

## keras_cv/training/contrastive/simclr_trainer_test.py

```diff
@@ -8,25 +8,29 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import pytest
 import tensorflow as tf
 from tensorflow import keras
 from tensorflow.keras import layers
 from tensorflow.keras import optimizers
 
 from keras_cv.losses import SimCLRLoss
 from keras_cv.models import ResNet50V2Backbone
 from keras_cv.training import SimCLRAugmenter
 from keras_cv.training import SimCLRTrainer
 
 
+# TODO(jbischof): revisit "extra_large" tag once development resumes.
+# These tests are currently some of the slowest in our repo.
+@pytest.mark.extra_large
 class SimCLRTrainerTest(tf.test.TestCase):
     def test_train_without_probing(self):
         simclr_without_probing = SimCLRTrainer(
             self.build_encoder(),
             augmenter=SimCLRAugmenter(value_range=(0, 255)),
         )
```

## keras_cv/utils/python_utils.py

```diff
@@ -25,15 +25,15 @@
     """Format a python docstring using a dictionary of replacements.
 
     This decorator can be placed on a function, class or method to format it's
     docstring with python variables.
 
     The decorator will replace any double bracketed variable with a kwargs
     value passed to the decorator itself. For example
-    `@format_docstring(name="foo")` will replace any occurance of `{{name}}` in
+    `@format_docstring(name="foo")` will replace any occurrence of `{{name}}` in
     the docstring with the string literal `foo`.
     """
 
     def decorate(obj):
         doc = obj.__doc__
         # We use `str.format()` to replace variables in the docstring, but use
         # double brackets, e.g. {{var}}, to mark format strings. So we need to
```

## keras_cv/visualization/__init__.py

```diff
@@ -12,7 +12,10 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from keras_cv.visualization.plot_bounding_box_gallery import (
     plot_bounding_box_gallery,
 )
 from keras_cv.visualization.plot_image_gallery import plot_image_gallery
+from keras_cv.visualization.plot_segmentation_mask_gallery import (
+    plot_segmentation_mask_gallery,
+)
```

## keras_cv/visualization/plot_image_gallery.py

```diff
@@ -8,30 +8,66 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import math
+
+import numpy as np
+import tensorflow as tf
+
 import keras_cv
 from keras_cv import utils
 from keras_cv.utils import assert_matplotlib_installed
 
 try:
     import matplotlib.pyplot as plt
 except:
     plt = None
 
 
+def _extract_image_batch(images, num_images, batch_size):
+    def unpack_images(inputs):
+        return inputs["image"]
+
+    num_batches_required = math.ceil(num_images / batch_size)
+
+    if isinstance(images, tf.data.Dataset):
+        images = images.map(unpack_images)
+
+        if batch_size == 1:
+            images = images.ragged_batch(num_batches_required)
+            sample = next(iter(images.take(1)))
+        else:
+            sample = next(iter(images.take(num_batches_required)))
+
+        return sample
+
+    else:
+        if len(images.shape) != 4:
+            raise ValueError(
+                "`plot_images_gallery()` requires you to "
+                "batch your `np.array` samples together."
+            )
+        else:
+            num_samples = (
+                num_images if num_images <= batch_size else num_batches_required
+            )
+            sample = images[:num_samples, ...]
+    return sample
+
+
 def plot_image_gallery(
     images,
     value_range,
-    rows=3,
-    cols=3,
     scale=2,
+    rows=None,
+    cols=None,
     path=None,
     show=None,
     transparent=True,
     dpi=60,
     legend_handles=None,
 ):
     """Displays a gallery of images.
@@ -41,82 +77,108 @@
     train_ds = tfds.load(
         "cats_vs_dogs",
         split="train",
         with_info=False,
         shuffle_files=True,
     )
 
-
-    def unpackage_tfds_inputs(inputs):
-        return inputs["image"]
-
-    train_ds = train_ds.map(unpackage_tfds_inputs)
-    train_ds = train_ds.apply(tf.data.experimental.dense_to_ragged_batch(16))
-
     keras_cv.visualization.plot_image_gallery(
-        next(iter(train_ds.take(1))),
+        train_ds,
         value_range=(0, 255),
         scale=3,
-        rows=2,
-        cols=2,
     )
     ```
 
     ![example gallery](https://i.imgur.com/r0ndse0.png)
 
     Args:
-        images: a Tensor or NumPy array containing images to show in the
-            gallery.
+        images: a Tensor, `tf.data.Dataset` or NumPy array containing images
+            to show in the gallery. Note: If using a `tf.data.Dataset`,
+            images should be present in the `FeaturesDict` under
+            the key `image`.
         value_range: value range of the images. Common examples include
             `(0, 255)` and `(0, 1)`.
-        rows: number of rows in the gallery to show.
-        cols: number of columns in the gallery to show.
         scale: how large to scale the images in the gallery
+        rows: (Optional) number of rows in the gallery to show.
+            Required if inputs are unbatched.
+        cols: (Optional) number of columns in the gallery to show.
+            Required if inputs are unbatched.
         path: (Optional) path to save the resulting gallery to.
         show: (Optional) whether to show the gallery of images.
         transparent: (Optional) whether to give the image a transparent
             background, defaults to `True`.
         dpi: (Optional) the dpi to pass to matplotlib.savefig(), defaults to
             `60`.
         legend_handles: (Optional) matplotlib.patches List of legend handles.
             I.e. passing: `[patches.Patch(color='red', label='mylabel')]` will
             produce a legend with a single red patch and the label 'mylabel'.
+
     """
     assert_matplotlib_installed("plot_bounding_box_gallery")
 
     if path is None and show is None:
         # Default to showing the image
         show = True
     if path is not None and show:
         raise ValueError(
             "plot_gallery() expects either `path` to be set, or `show` "
             "to be true."
         )
 
-    fig = plt.figure(figsize=(cols * scale, rows * scale))
-    fig.tight_layout()  # Or equivalently,  "plt.tight_layout()"
-    plt.subplots_adjust(wspace=0, hspace=0)
-    plt.margins(x=0, y=0)
-    plt.axis("off")
+    if isinstance(images, tf.data.Dataset):
+        sample = next(iter(images.take(1)))
+        batch_size = (
+            sample["image"].shape[0] if len(sample["image"].shape) == 4 else 1
+        )  # batch_size from within passed `tf.data.Dataset`
+    else:
+        batch_size = (
+            images.shape[0] if len(images.shape) == 4 else 1
+        )  # batch_size from np.array or single image
+
+    rows = rows or int(math.ceil(math.sqrt(batch_size)))
+    cols = cols or int(math.ceil(batch_size // rows))
+
+    num_images = rows * cols
+    images = _extract_image_batch(images, num_images, batch_size)
+
+    # Generate subplots
+    fig, axes = plt.subplots(
+        nrows=rows,
+        ncols=cols,
+        figsize=(cols * scale, rows * scale),
+        frameon=False,
+        layout="tight",
+        squeeze=True,
+        sharex="row",
+        sharey="col",
+    )
+    fig.subplots_adjust(wspace=0, hspace=0)
+
+    if isinstance(axes, np.ndarray) and len(axes.shape) == 1:
+        expand_axis = 0 if rows == 1 else -1
+        axes = np.expand_dims(axes, expand_axis)
 
     if legend_handles is not None:
         fig.legend(handles=legend_handles, loc="lower center")
 
+    # Perform image range transform
     images = keras_cv.utils.transform_value_range(
         images, original_range=value_range, target_range=(0, 255)
     )
     images = utils.to_numpy(images)
-    images = images.astype(int)
+
     for row in range(rows):
         for col in range(cols):
             index = row * cols + col
-            plt.subplot(rows, cols, index + 1)
-            plt.imshow(images[index].astype("uint8"))
-            plt.axis("off")
-            plt.margins(x=0, y=0)
+            current_axis = (
+                axes[row, col] if isinstance(axes, np.ndarray) else axes
+            )
+            current_axis.imshow(images[index].astype("uint8"))
+            current_axis.margins(x=0, y=0)
+            current_axis.axis("off")
 
     if path is None and not show:
         return
     if path is not None:
         plt.savefig(
             fname=path,
             pad_inches=0,
```

## Comparing `keras_cv/layers/preprocessing/maybe_apply.py` & `keras_cv/layers/preprocessing/random_apply.py`

 * *Files 8% similar despite different names*

```diff
@@ -16,15 +16,15 @@
 
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
 
 
 @keras.utils.register_keras_serializable(package="keras_cv")
-class MaybeApply(BaseImageAugmentationLayer):
+class RandomApply(BaseImageAugmentationLayer):
     """Apply provided layer to random elements in a batch.
 
     Args:
         layer: a keras `Layer` or `BaseImageAugmentationLayer`. This layer will
             be applied to randomly chosen samples in a batch. Layer should not
             modify the size of provided inputs.
         rate: controls the frequency of applying the layer. 1.0 means all
@@ -61,16 +61,16 @@
     #        [[0.38977218, 0.80855536],
     #         [0.6040567 , 0.10502195]],
     #
     #        [[0.51828027, 0.12730157],
     #         [0.288486  , 0.252975  ]]], dtype=float32)>
 
     # Apply the layer with 50% probability:
-    maybe_apply = MaybeApply(layer=zero_out, rate=0.5, seed=1234)
-    outputs = maybe_apply(images)
+    random_apply = RandomApply(layer=zero_out, rate=0.5, seed=1234)
+    outputs = random_apply(images)
     print(outputs[..., 0])
     # <tf.Tensor: shape=(5, 2, 2), dtype=float32, numpy=
     # array([[[0.        , 0.        ],
     #         [0.        , 0.        ]],
     #
     #        [[0.34717774, 0.73199546],
     #         [0.56369007, 0.9769211 ]],
```

## Comparing `keras_cv/layers/preprocessing/maybe_apply_test.py` & `keras_cv/layers/preprocessing/random_apply_test.py`

 * *Files 7% similar despite different names*

```diff
@@ -16,15 +16,15 @@
 from absl.testing import parameterized
 from tensorflow import keras
 
 from keras_cv import layers
 from keras_cv.layers.preprocessing.base_image_augmentation_layer import (
     BaseImageAugmentationLayer,
 )
-from keras_cv.layers.preprocessing.maybe_apply import MaybeApply
+from keras_cv.layers.preprocessing.random_apply import RandomApply
 
 
 class ZeroOut(BaseImageAugmentationLayer):
     """Zero out all entries, for testing purposes."""
 
     def __init__(self):
         super(ZeroOut, self).__init__()
@@ -32,26 +32,26 @@
     def augment_image(self, image, transformation=None, **kwargs):
         return 0 * image
 
     def augment_label(self, label, transformation=None, **kwargs):
         return 0 * label
 
 
-class MaybeApplyTest(tf.test.TestCase, parameterized.TestCase):
+class RandomApplyTest(tf.test.TestCase, parameterized.TestCase):
     rng = tf.random.Generator.from_seed(seed=1234)
 
     @parameterized.parameters([-0.5, 1.7])
     def test_raises_error_on_invalid_rate_parameter(self, invalid_rate):
         with self.assertRaises(ValueError):
-            MaybeApply(rate=invalid_rate, layer=ZeroOut())
+            RandomApply(rate=invalid_rate, layer=ZeroOut())
 
     def test_works_with_batched_input(self):
         batch_size = 32
         dummy_inputs = self.rng.uniform(shape=(batch_size, 224, 224, 3))
-        layer = MaybeApply(rate=0.5, layer=ZeroOut(), seed=1234)
+        layer = RandomApply(rate=0.5, layer=ZeroOut(), seed=1234)
 
         outputs = layer(dummy_inputs)
         num_zero_inputs = self._num_zero_batches(dummy_inputs)
         num_zero_outputs = self._num_zero_batches(outputs)
 
         self.assertEqual(num_zero_inputs, 0)
         self.assertLess(num_zero_outputs, batch_size)
@@ -59,70 +59,70 @@
 
     def test_works_with_batchwise_layers(self):
         batch_size = 32
         dummy_inputs = self.rng.uniform(shape=(batch_size, 224, 224, 3))
         dummy_outputs = self.rng.uniform(shape=(batch_size,))
         inputs = {"images": dummy_inputs, "labels": dummy_outputs}
         layer = layers.CutMix()
-        layer = layers.MaybeApply(layer, rate=0.5, batchwise=True)
+        layer = layers.RandomApply(layer, rate=0.5, batchwise=True)
         _ = layer(inputs)
 
     @staticmethod
     def _num_zero_batches(images):
         num_batches = tf.shape(images)[0]
         num_non_zero_batches = tf.math.count_nonzero(
             tf.math.count_nonzero(images, axis=[1, 2, 3]), dtype=tf.int32
         )
         return num_batches - num_non_zero_batches
 
     def test_inputs_unchanged_with_zero_rate(self):
         dummy_inputs = self.rng.uniform(shape=(32, 224, 224, 3))
-        layer = MaybeApply(rate=0.0, layer=ZeroOut())
+        layer = RandomApply(rate=0.0, layer=ZeroOut())
 
         outputs = layer(dummy_inputs)
 
         self.assertAllClose(outputs, dummy_inputs)
 
     def test_all_inputs_changed_with_rate_equal_to_one(self):
         dummy_inputs = self.rng.uniform(shape=(32, 224, 224, 3))
-        layer = MaybeApply(rate=1.0, layer=ZeroOut())
+        layer = RandomApply(rate=1.0, layer=ZeroOut())
 
         outputs = layer(dummy_inputs)
 
         self.assertAllEqual(outputs, tf.zeros_like(dummy_inputs))
 
     def test_works_with_single_image(self):
         dummy_inputs = self.rng.uniform(shape=(224, 224, 3))
-        layer = MaybeApply(rate=1.0, layer=ZeroOut())
+        layer = RandomApply(rate=1.0, layer=ZeroOut())
 
         outputs = layer(dummy_inputs)
 
         self.assertAllEqual(outputs, tf.zeros_like(dummy_inputs))
 
     def test_can_modify_label(self):
         dummy_inputs = self.rng.uniform(shape=(32, 224, 224, 3))
         dummy_labels = tf.ones(shape=(32, 2))
-        layer = MaybeApply(rate=1.0, layer=ZeroOut())
+        layer = RandomApply(rate=1.0, layer=ZeroOut())
 
         outputs = layer({"images": dummy_inputs, "labels": dummy_labels})
 
         self.assertAllEqual(outputs["labels"], tf.zeros_like(dummy_labels))
 
     def test_works_with_native_keras_layers(self):
         dummy_inputs = self.rng.uniform(shape=(32, 224, 224, 3))
         zero_out = keras.layers.Lambda(lambda x: {"images": 0 * x["images"]})
-        layer = MaybeApply(rate=1.0, layer=zero_out)
+        layer = RandomApply(rate=1.0, layer=zero_out)
 
         outputs = layer(dummy_inputs)
 
         self.assertAllEqual(outputs, tf.zeros_like(dummy_inputs))
 
     def test_works_with_xla(self):
         dummy_inputs = self.rng.uniform(shape=(32, 224, 224, 3))
         # auto_vectorize=True will crash XLA
-        layer = MaybeApply(rate=0.5, layer=ZeroOut(), auto_vectorize=False)
+        layer = RandomApply(rate=0.5, layer=ZeroOut(), auto_vectorize=False)
 
         @tf.function(jit_compile=True)
         def apply(x):
             return layer(x)
 
         apply(dummy_inputs)
```

## Comparing `keras_cv/layers/preprocessing_3d/frustum_random_dropping_points.py` & `keras_cv/layers/preprocessing_3d/waymo/frustum_random_dropping_points.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,20 +1,10 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2022 Waymo LLC.
 #
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+# Licensed under the terms in https://github.com/keras-team/keras-cv/blob/master/keras_cv/layers/preprocessing_3d/waymo/LICENSE  # noqa: E501
 
 import tensorflow as tf
 from tensorflow import keras
 
 from keras_cv import point_cloud
 from keras_cv.layers.preprocessing_3d import base_augmentation_layer_3d
```

## Comparing `keras_cv/layers/preprocessing_3d/frustum_random_dropping_points_test.py` & `keras_cv/layers/preprocessing_3d/waymo/frustum_random_dropping_points_test.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,25 +1,16 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2022 Waymo LLC.
 #
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+# Licensed under the terms in https://github.com/keras-team/keras-cv/blob/master/keras_cv/layers/preprocessing_3d/waymo/LICENSE  # noqa: E501
+
 import numpy as np
 import tensorflow as tf
 
 from keras_cv.layers.preprocessing_3d import base_augmentation_layer_3d
-from keras_cv.layers.preprocessing_3d.frustum_random_dropping_points import (
+from keras_cv.layers.preprocessing_3d.waymo.frustum_random_dropping_points import (  # noqa: E501
     FrustumRandomDroppingPoints,
 )
 
 POINT_CLOUDS = base_augmentation_layer_3d.POINT_CLOUDS
 BOUNDING_BOXES = base_augmentation_layer_3d.BOUNDING_BOXES
```

## Comparing `keras_cv/layers/preprocessing_3d/frustum_random_point_feature_noise.py` & `keras_cv/layers/preprocessing_3d/waymo/frustum_random_point_feature_noise.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,20 +1,10 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2022 Waymo LLC.
 #
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+# Licensed under the terms in https://github.com/keras-team/keras-cv/blob/master/keras_cv/layers/preprocessing_3d/waymo/LICENSE  # noqa: E501
 
 import tensorflow as tf
 from tensorflow import keras
 
 from keras_cv import point_cloud
 from keras_cv.layers.preprocessing_3d import base_augmentation_layer_3d
 
@@ -31,15 +21,15 @@
     """A preprocessing layer which randomly add noise to point features within a
     randomly generated frustum during training.
 
     This layer will randomly select a point from the point cloud as the center
     of a frustum then generate a frustum based on r_distance, theta_width, and
     phi_width. Uniformly sampled features noise from [1-max_noise_level,
     1+max_noise_level] will be multiplied to points inside the selected frustum.
-    Here, we perturbe point features other than (x, y, z, class). The
+    Here, we perturb point features other than (x, y, z, class). The
     point_clouds tensor shape must be specific and cannot be dynamic. During
     inference time, the output will be identical to input. Call the layer with
     `training=True` to add noise to the input points.
 
     Input shape:
       point_clouds: 3D (multi frames) float32 Tensor with shape
         [num of frames, num of points, num of point features].
```

## Comparing `keras_cv/layers/preprocessing_3d/frustum_random_point_feature_noise_test.py` & `keras_cv/layers/preprocessing_3d/waymo/frustum_random_point_feature_noise_test.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,27 +1,17 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2022 Waymo LLC.
 #
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+# Licensed under the terms in https://github.com/keras-team/keras-cv/blob/master/keras_cv/layers/preprocessing_3d/waymo/LICENSE  # noqa: E501
 
 import numpy as np
 import tensorflow as tf
 from tensorflow import keras
 
 from keras_cv.layers.preprocessing_3d import base_augmentation_layer_3d
-from keras_cv.layers.preprocessing_3d.frustum_random_point_feature_noise import (  # noqa: E501
+from keras_cv.layers.preprocessing_3d.waymo.frustum_random_point_feature_noise import (  # noqa: E501
     FrustumRandomPointFeatureNoise,
 )
 
 POINT_CLOUDS = base_augmentation_layer_3d.POINT_CLOUDS
 BOUNDING_BOXES = base_augmentation_layer_3d.BOUNDING_BOXES
 POINTCLOUD_LABEL_INDEX = base_augmentation_layer_3d.POINTCLOUD_LABEL_INDEX
```

## Comparing `keras_cv/layers/preprocessing_3d/global_random_dropping_points.py` & `keras_cv/layers/preprocessing_3d/waymo/global_random_dropping_points.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,20 +1,10 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2022 Waymo LLC.
 #
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+# Licensed under the terms in https://github.com/keras-team/keras-cv/blob/master/keras_cv/layers/preprocessing_3d/waymo/LICENSE  # noqa: E501
 
 import tensorflow as tf
 from tensorflow import keras
 
 from keras_cv.layers.preprocessing_3d import base_augmentation_layer_3d
 
 POINT_CLOUDS = base_augmentation_layer_3d.POINT_CLOUDS
```

## Comparing `keras_cv/layers/preprocessing_3d/global_random_dropping_points_test.py` & `keras_cv/layers/preprocessing_3d/waymo/global_random_dropping_points_test.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,25 +1,16 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2022 Waymo LLC.
 #
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+# Licensed under the terms in https://github.com/keras-team/keras-cv/blob/master/keras_cv/layers/preprocessing_3d/waymo/LICENSE  # noqa: E501
+
 import numpy as np
 import tensorflow as tf
 
 from keras_cv.layers.preprocessing_3d import base_augmentation_layer_3d
-from keras_cv.layers.preprocessing_3d.global_random_dropping_points import (
+from keras_cv.layers.preprocessing_3d.waymo.global_random_dropping_points import (  # noqa: E501
     GlobalRandomDroppingPoints,
 )
 
 POINT_CLOUDS = base_augmentation_layer_3d.POINT_CLOUDS
 BOUNDING_BOXES = base_augmentation_layer_3d.BOUNDING_BOXES
```

## Comparing `keras_cv/layers/preprocessing_3d/global_random_flip.py` & `keras_cv/layers/preprocessing_3d/waymo/global_random_flip.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,20 +1,10 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2022 Waymo LLC.
 #
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+# Licensed under the terms in https://github.com/keras-team/keras-cv/blob/master/keras_cv/layers/preprocessing_3d/waymo/LICENSE  # noqa: E501
 
 import tensorflow as tf
 from tensorflow import keras
 
 from keras_cv.bounding_box_3d import CENTER_XYZ_DXDYDZ_PHI
 from keras_cv.layers.preprocessing_3d import base_augmentation_layer_3d
 from keras_cv.point_cloud import wrap_angle_radians
```

## Comparing `keras_cv/layers/preprocessing_3d/global_random_flip_test.py` & `keras_cv/layers/preprocessing_3d/waymo/global_random_flip_test.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,25 +1,18 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2022 Waymo LLC.
 #
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+# Licensed under the terms in https://github.com/keras-team/keras-cv/blob/master/keras_cv/layers/preprocessing_3d/waymo/LICENSE  # noqa: E501
+
 import numpy as np
 import tensorflow as tf
 
 from keras_cv.layers.preprocessing_3d import base_augmentation_layer_3d
-from keras_cv.layers.preprocessing_3d.global_random_flip import GlobalRandomFlip
+from keras_cv.layers.preprocessing_3d.waymo.global_random_flip import (
+    GlobalRandomFlip,
+)
 
 POINT_CLOUDS = base_augmentation_layer_3d.POINT_CLOUDS
 BOUNDING_BOXES = base_augmentation_layer_3d.BOUNDING_BOXES
 
 
 class GlobalRandomFlipTest(tf.test.TestCase):
     def test_augment_random_point_clouds_and_bounding_boxes(self):
```

## Comparing `keras_cv/layers/preprocessing_3d/global_random_rotation.py` & `keras_cv/layers/preprocessing_3d/waymo/global_random_rotation.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,20 +1,10 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2022 Waymo LLC.
 #
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+# Licensed under the terms in https://github.com/keras-team/keras-cv/blob/master/keras_cv/layers/preprocessing_3d/waymo/LICENSE  # noqa: E501
 
 import tensorflow as tf
 from tensorflow import keras
 
 from keras_cv.bounding_box_3d import CENTER_XYZ_DXDYDZ_PHI
 from keras_cv.layers.preprocessing_3d import base_augmentation_layer_3d
 from keras_cv.point_cloud import coordinate_transform
```

## Comparing `keras_cv/layers/preprocessing_3d/global_random_rotation_test.py` & `keras_cv/layers/preprocessing_3d/waymo/global_random_rotation_test.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,25 +1,16 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2022 Waymo LLC.
 #
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+# Licensed under the terms in https://github.com/keras-team/keras-cv/blob/master/keras_cv/layers/preprocessing_3d/waymo/LICENSE  # noqa: E501
+
 import numpy as np
 import tensorflow as tf
 
 from keras_cv.layers.preprocessing_3d import base_augmentation_layer_3d
-from keras_cv.layers.preprocessing_3d.global_random_rotation import (
+from keras_cv.layers.preprocessing_3d.waymo.global_random_rotation import (
     GlobalRandomRotation,
 )
 
 POINT_CLOUDS = base_augmentation_layer_3d.POINT_CLOUDS
 BOUNDING_BOXES = base_augmentation_layer_3d.BOUNDING_BOXES
```

## Comparing `keras_cv/layers/preprocessing_3d/global_random_scaling.py` & `keras_cv/layers/preprocessing_3d/waymo/global_random_scaling.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,20 +1,10 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2022 Waymo LLC.
 #
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+# Licensed under the terms in https://github.com/keras-team/keras-cv/blob/master/keras_cv/layers/preprocessing_3d/waymo/LICENSE  # noqa: E501
 
 import tensorflow as tf
 from tensorflow import keras
 
 from keras_cv.bounding_box_3d import CENTER_XYZ_DXDYDZ_PHI
 from keras_cv.layers.preprocessing_3d import base_augmentation_layer_3d
```

## Comparing `keras_cv/layers/preprocessing_3d/global_random_scaling_test.py` & `keras_cv/layers/preprocessing_3d/waymo/global_random_scaling_test.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,25 +1,16 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2022 Waymo LLC.
 #
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+# Licensed under the terms in https://github.com/keras-team/keras-cv/blob/master/keras_cv/layers/preprocessing_3d/waymo/LICENSE  # noqa: E501
+
 import numpy as np
 import tensorflow as tf
 
 from keras_cv.layers.preprocessing_3d import base_augmentation_layer_3d
-from keras_cv.layers.preprocessing_3d.global_random_scaling import (
+from keras_cv.layers.preprocessing_3d.waymo.global_random_scaling import (
     GlobalRandomScaling,
 )
 
 POINT_CLOUDS = base_augmentation_layer_3d.POINT_CLOUDS
 BOUNDING_BOXES = base_augmentation_layer_3d.BOUNDING_BOXES
```

## Comparing `keras_cv/layers/preprocessing_3d/global_random_translation.py` & `keras_cv/layers/preprocessing_3d/waymo/global_random_translation.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,20 +1,10 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2022 Waymo LLC.
 #
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+# Licensed under the terms in https://github.com/keras-team/keras-cv/blob/master/keras_cv/layers/preprocessing_3d/waymo/LICENSE  # noqa: E501
 
 import tensorflow as tf
 from tensorflow import keras
 
 from keras_cv.bounding_box_3d import CENTER_XYZ_DXDYDZ_PHI
 from keras_cv.layers.preprocessing_3d import base_augmentation_layer_3d
 from keras_cv.point_cloud import coordinate_transform
```

## Comparing `keras_cv/layers/preprocessing_3d/group_points_by_bounding_boxes.py` & `keras_cv/layers/preprocessing_3d/waymo/group_points_by_bounding_boxes.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,20 +1,10 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2022 Waymo LLC.
 #
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+# Licensed under the terms in https://github.com/keras-team/keras-cv/blob/master/keras_cv/layers/preprocessing_3d/waymo/LICENSE  # noqa: E501
 
 import tensorflow as tf
 from tensorflow import keras
 
 from keras_cv.bounding_box_3d import CENTER_XYZ_DXDYDZ_PHI
 from keras_cv.layers.preprocessing_3d import base_augmentation_layer_3d
 from keras_cv.point_cloud import group_points_by_boxes
@@ -239,14 +229,15 @@
                 OBJECT_POINT_CLOUDS: object_point_clouds,
                 OBJECT_BOUNDING_BOXES: object_bounding_boxes,
             }
         )
         return result
 
     def call(self, inputs):
+        # TODO(ianstenbit): Support the model input format.
         point_clouds = inputs[POINT_CLOUDS]
         bounding_boxes = inputs[BOUNDING_BOXES]
         if point_clouds.shape.rank == 3 and bounding_boxes.shape.rank == 3:
             return self._augment(inputs)
         elif point_clouds.shape.rank == 4 and bounding_boxes.shape.rank == 4:
             batch = point_clouds.get_shape().as_list()[0]
             object_point_clouds_list = []
```

## Comparing `keras_cv/layers/preprocessing_3d/group_points_by_bounding_boxes_test.py` & `keras_cv/layers/preprocessing_3d/waymo/random_copy_paste_test.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,207 +1,235 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2022 Waymo LLC.
 #
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+# Licensed under the terms in https://github.com/keras-team/keras-cv/blob/master/keras_cv/layers/preprocessing_3d/waymo/LICENSE  # noqa: E501
+
 import os
 
 import numpy as np
 import pytest
 import tensorflow as tf
 
 from keras_cv.layers.preprocessing_3d import base_augmentation_layer_3d
-from keras_cv.layers.preprocessing_3d.group_points_by_bounding_boxes import (
-    GroupPointsByBoundingBoxes,
+from keras_cv.layers.preprocessing_3d.waymo.random_copy_paste import (
+    RandomCopyPaste,
 )
 
 POINT_CLOUDS = base_augmentation_layer_3d.POINT_CLOUDS
 BOUNDING_BOXES = base_augmentation_layer_3d.BOUNDING_BOXES
 OBJECT_POINT_CLOUDS = base_augmentation_layer_3d.OBJECT_POINT_CLOUDS
 OBJECT_BOUNDING_BOXES = base_augmentation_layer_3d.OBJECT_BOUNDING_BOXES
 
 
-class GroupPointsByBoundingBoxesTest(tf.test.TestCase):
+class RandomCopyPasteTest(tf.test.TestCase):
+    @pytest.mark.skipif(
+        "TEST_CUSTOM_OPS" not in os.environ
+        or os.environ["TEST_CUSTOM_OPS"] != "true",
+        reason="Requires binaries compiled from source",
+    )
     def test_augment_point_clouds_and_bounding_boxes(self):
-        add_layer = GroupPointsByBoundingBoxes(
+        add_layer = RandomCopyPaste(
             label_index=1,
-            min_points_per_bounding_boxes=1,
-            max_points_per_bounding_boxes=2,
+            min_paste_bounding_boxes=1,
+            max_paste_bounding_boxes=1,
         )
+        # point_clouds: 3D (multi frames) float32 Tensor with shape
+        # [num of frames, num of points, num of point features].
+        # The first 5 features are [x, y, z, class, range].
         point_clouds = np.array(
             [
                 [
                     [0, 1, 2, 3, 4],
                     [10, 1, 2, 3, 4],
                     [0, -1, 2, 3, 4],
                     [100, 100, 2, 3, 4],
+                    [0, 0, 0, 0, 0],
                 ]
             ]
             * 2
         ).astype("float32")
+        # bounding_boxes: 3D (multi frames) float32 Tensor with shape
+        # [num of frames, num of boxes, num of box features].
+        # The first 8 features are [x, y, z, dx, dy, dz, phi, box class].
         bounding_boxes = np.array(
             [
                 [
                     [0, 0, 0, 4, 4, 4, 0, 1],
-                    [10, 1, 2, 2, 2, 2, 0, 1],
                     [20, 20, 20, 1, 1, 1, 0, 1],
+                    [0, 0, 0, 0, 0, 0, 0, 0],
+                    [0, 0, 0, 0, 0, 0, 0, 0],
+                ]
+            ]
+            * 2
+        ).astype("float32")
+        object_point_clouds = np.array(
+            [
+                [
+                    [[0, 1, 2, 3, 4], [0, 1, 1, 3, 4]],
+                    [[100, 101, 2, 3, 4], [0, 0, 0, 0, 0]],
+                ]
+            ]
+            * 2
+        ).astype("float32")
+        object_bounding_boxes = np.array(
+            [
+                [
+                    [0, 0, 1, 4, 4, 4, 0, 1],
+                    [100, 100, 2, 5, 5, 5, 0, 1],
                 ]
             ]
             * 2
         ).astype("float32")
         inputs = {
             POINT_CLOUDS: point_clouds,
             BOUNDING_BOXES: bounding_boxes,
-            "dummy_item": np.random.uniform(size=(2, 2, 2)),
+            OBJECT_POINT_CLOUDS: object_point_clouds,
+            OBJECT_BOUNDING_BOXES: object_bounding_boxes,
         }
         outputs = add_layer(inputs)
-        object_point_clouds = np.array(
+        # The first object bounding box [0, 0, 1, 4, 4, 4, 0, 1] overlaps with
+        # existing bounding box [0, 0, 0, 4, 4, 4, 0, 1], thus not used.
+        # The second object bounding box [100, 100, 2, 5, 5, 5, 0, 1] and object
+        # point clouds [100, 101, 2, 3, 4] are pasted.
+        augmented_point_clouds = np.array(
             [
                 [
-                    [[0, 1, 2, 3, 4], [0, -1, 2, 3, 4]],
-                    [[10, 1, 2, 3, 4], [0, 0, 0, 0, 0]],
+                    [100, 101, 2, 3, 4],
+                    [0, 1, 2, 3, 4],
+                    [10, 1, 2, 3, 4],
+                    [0, -1, 2, 3, 4],
+                    [0, 0, 0, 0, 0],
                 ]
             ]
             * 2
         ).astype("float32")
-        object_bounding_boxes = np.array(
-            [[[0, 0, 0, 4, 4, 4, 0, 1], [10, 1, 2, 2, 2, 2, 0, 1]]] * 2
+        augmented_bounding_boxes = np.array(
+            [
+                [
+                    [100, 100, 2, 5, 5, 5, 0, 1],
+                    [0, 0, 0, 4, 4, 4, 0, 1],
+                    [20, 20, 20, 1, 1, 1, 0, 1],
+                    [0, 0, 0, 0, 0, 0, 0, 0],
+                ]
+            ]
+            * 2
         ).astype("float32")
-        self.assertAllClose(inputs[POINT_CLOUDS], outputs[POINT_CLOUDS])
-        self.assertAllClose(inputs[BOUNDING_BOXES], outputs[BOUNDING_BOXES])
-        self.assertAllClose(inputs["dummy_item"], outputs["dummy_item"])
-        # Sort the point clouds due to the orders of points are different when
-        # using Tensorflow and Metal+Tensorflow (MAC).
-        outputs[OBJECT_POINT_CLOUDS] = tf.sort(
-            outputs[OBJECT_POINT_CLOUDS], axis=-2
+        self.assertAllClose(
+            inputs[OBJECT_POINT_CLOUDS], outputs[OBJECT_POINT_CLOUDS]
         )
-        object_point_clouds = tf.sort(object_point_clouds, axis=-2)
-        self.assertAllClose(outputs[OBJECT_POINT_CLOUDS], object_point_clouds)
         self.assertAllClose(
-            outputs[OBJECT_BOUNDING_BOXES], object_bounding_boxes
+            inputs[OBJECT_BOUNDING_BOXES], outputs[OBJECT_BOUNDING_BOXES]
         )
+        self.assertAllClose(outputs[POINT_CLOUDS], augmented_point_clouds)
+        self.assertAllClose(outputs[BOUNDING_BOXES], augmented_bounding_boxes)
 
+    @pytest.mark.skipif(
+        "TEST_CUSTOM_OPS" not in os.environ
+        or os.environ["TEST_CUSTOM_OPS"] != "true",
+        reason="Requires binaries compiled from source",
+    )
     def test_augment_batch_point_clouds_and_bounding_boxes(self):
-        add_layer = GroupPointsByBoundingBoxes(
+        add_layer = RandomCopyPaste(
             label_index=1,
-            min_points_per_bounding_boxes=1,
-            max_points_per_bounding_boxes=2,
+            min_paste_bounding_boxes=1,
+            max_paste_bounding_boxes=1,
         )
         point_clouds = np.array(
             [
                 [
                     [
                         [0, 1, 2, 3, 4],
                         [10, 1, 2, 3, 4],
                         [0, -1, 2, 3, 4],
                         [100, 100, 2, 3, 4],
+                        [0, 0, 0, 0, 0],
                     ]
                 ]
                 * 2
             ]
             * 3
         ).astype("float32")
         bounding_boxes = np.array(
             [
                 [
                     [
                         [0, 0, 0, 4, 4, 4, 0, 1],
-                        [10, 1, 2, 2, 2, 2, 0, 1],
                         [20, 20, 20, 1, 1, 1, 0, 1],
+                        [0, 0, 0, 0, 0, 0, 0, 0],
+                        [0, 0, 0, 0, 0, 0, 0, 0],
                     ]
                 ]
                 * 2
             ]
             * 3
         ).astype("float32")
-        inputs = {POINT_CLOUDS: point_clouds, BOUNDING_BOXES: bounding_boxes}
-        outputs = add_layer(inputs)
         object_point_clouds = np.array(
             [
                 [
-                    [[0, 1, 2, 3, 4], [0, -1, 2, 3, 4]],
-                    [[10, 1, 2, 3, 4], [0, 0, 0, 0, 0]],
+                    [
+                        [[0, 1, 2, 3, 4], [0, 1, 1, 3, 4]],
+                        [[100, 101, 2, 3, 4], [0, 0, 0, 0, 0]],
+                    ]
                 ]
-                * 3
+                * 2
             ]
-            * 2
+            * 3
         ).astype("float32")
         object_bounding_boxes = np.array(
-            [[[0, 0, 0, 4, 4, 4, 0, 1], [10, 1, 2, 2, 2, 2, 0, 1]] * 3] * 2
-        ).astype("float32")
-        self.assertAllClose(inputs[POINT_CLOUDS], outputs[POINT_CLOUDS])
-        self.assertAllClose(inputs[BOUNDING_BOXES], outputs[BOUNDING_BOXES])
-        # Sort the point clouds due to the orders of points are different when
-        # using Tensorflow and Metal+Tensorflow (MAC).
-        outputs[OBJECT_POINT_CLOUDS] = tf.sort(
-            outputs[OBJECT_POINT_CLOUDS], axis=-2
-        )
-        object_point_clouds = tf.sort(object_point_clouds, axis=-2)
-        self.assertAllClose(outputs[OBJECT_POINT_CLOUDS], object_point_clouds)
-        self.assertAllClose(
-            outputs[OBJECT_BOUNDING_BOXES], object_bounding_boxes
-        )
-
-    @pytest.mark.skipif(
-        "TEST_CUSTOM_OPS" not in os.environ
-        or os.environ["TEST_CUSTOM_OPS"] != "true",
-        reason="Requires binaries compiled from source",
-    )
-    def test_augment_point_clouds_and_bounding_boxes_v2(self):
-        add_layer = GroupPointsByBoundingBoxes(
-            label_index=1,
-            min_points_per_bounding_boxes=1,
-            max_points_per_bounding_boxes=2,
-        )
-        point_clouds = np.array(
             [
                 [
-                    [0, 1, 2, 3, 4],
-                    [10, 1, 2, 3, 4],
-                    [0, -1, 2, 3, 4],
-                    [100, 100, 2, 3, 4],
+                    [
+                        [0, 0, 1, 4, 4, 4, 0, 1],
+                        [100, 100, 2, 5, 5, 5, 0, 1],
+                    ]
                 ]
+                * 2
             ]
-            * 2
+            * 3
         ).astype("float32")
-        bounding_boxes = np.array(
+        inputs = {
+            POINT_CLOUDS: point_clouds,
+            BOUNDING_BOXES: bounding_boxes,
+            OBJECT_POINT_CLOUDS: object_point_clouds,
+            OBJECT_BOUNDING_BOXES: object_bounding_boxes,
+        }
+        outputs = add_layer(inputs)
+        # The first object bounding box [0, 0, 1, 4, 4, 4, 0, 1] overlaps with
+        # existing bounding box [0, 0, 0, 4, 4, 4, 0, 1], thus not used.
+        # The second object bounding box [100, 100, 2, 5, 5, 5, 0, 1] and object
+        # point clouds [100, 101, 2, 3, 4] are pasted.
+        augmented_point_clouds = np.array(
             [
                 [
-                    [0, 0, 0, 4, 4, 4, 0, 1],
-                    [10, 1, 2, 2, 2, 2, 0, 1],
-                    [20, 20, 20, 1, 1, 1, 0, 1],
+                    [
+                        [100, 101, 2, 3, 4],
+                        [0, 1, 2, 3, 4],
+                        [10, 1, 2, 3, 4],
+                        [0, -1, 2, 3, 4],
+                        [0, 0, 0, 0, 0],
+                    ]
                 ]
+                * 2
             ]
-            * 2
+            * 3
         ).astype("float32")
-        point_clouds = tf.convert_to_tensor(point_clouds)
-        bounding_boxes = tf.convert_to_tensor(bounding_boxes)
-        outputs = add_layer.augment_point_clouds_bounding_boxes_v2(
-            point_clouds=point_clouds, bounding_boxes=bounding_boxes
-        )
-        object_point_clouds, object_bounding_boxes = outputs[0], outputs[1]
-        expected_object_point_clouds = np.array(
+        augmented_bounding_boxes = np.array(
             [
                 [
-                    [[0, 1, 2, 3, 4], [0, -1, 2, 3, 4]],
-                    [[10, 1, 2, 3, 4], [0, 0, 0, 0, 0]],
+                    [
+                        [100, 100, 2, 5, 5, 5, 0, 1],
+                        [0, 0, 0, 4, 4, 4, 0, 1],
+                        [20, 20, 20, 1, 1, 1, 0, 1],
+                        [0, 0, 0, 0, 0, 0, 0, 0],
+                    ]
                 ]
+                * 2
             ]
-            * 2
-        ).astype("float32")
-        expected_object_bounding_boxes = np.array(
-            [[[0, 0, 0, 4, 4, 4, 0, 1], [10, 1, 2, 2, 2, 2, 0, 1]]] * 2
+            * 3
         ).astype("float32")
         self.assertAllClose(
-            expected_object_point_clouds, object_point_clouds.to_tensor()
+            inputs[OBJECT_POINT_CLOUDS], outputs[OBJECT_POINT_CLOUDS]
         )
         self.assertAllClose(
-            expected_object_bounding_boxes, object_bounding_boxes.to_tensor()
+            inputs[OBJECT_BOUNDING_BOXES], outputs[OBJECT_BOUNDING_BOXES]
         )
+        self.assertAllClose(outputs[POINT_CLOUDS], augmented_point_clouds)
+        self.assertAllClose(outputs[BOUNDING_BOXES], augmented_bounding_boxes)
```

## Comparing `keras_cv/layers/preprocessing_3d/random_copy_paste.py` & `keras_cv/layers/preprocessing_3d/waymo/random_copy_paste.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,20 +1,10 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2022 Waymo LLC.
 #
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+# Licensed under the terms in https://github.com/keras-team/keras-cv/blob/master/keras_cv/layers/preprocessing_3d/waymo/LICENSE  # noqa: E501
 
 import tensorflow as tf
 from tensorflow import keras
 
 from keras_cv.bounding_box_3d import CENTER_XYZ_DXDYDZ_PHI
 from keras_cv.layers.preprocessing_3d import base_augmentation_layer_3d
 from keras_cv.ops import iou_3d
@@ -258,14 +248,15 @@
         )
         result.update(
             {POINT_CLOUDS: point_clouds, BOUNDING_BOXES: bounding_boxes}
         )
         return result
 
     def call(self, inputs):
+        # TODO(ianstenbit): Support the model input format.
         point_clouds = inputs[POINT_CLOUDS]
         bounding_boxes = inputs[BOUNDING_BOXES]
         if point_clouds.shape.rank == 3 and bounding_boxes.shape.rank == 3:
             return self._augment(inputs)
         elif point_clouds.shape.rank == 4 and bounding_boxes.shape.rank == 4:
             batch = point_clouds.get_shape().as_list()[0]
             point_clouds_list = []
```

## Comparing `keras_cv/layers/preprocessing_3d/random_drop_box.py` & `keras_cv/layers/preprocessing_3d/waymo/random_drop_box.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,20 +1,10 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2022 Waymo LLC.
 #
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+# Licensed under the terms in https://github.com/keras-team/keras-cv/blob/master/keras_cv/layers/preprocessing_3d/waymo/LICENSE  # noqa: E501
 
 import tensorflow as tf
 from tensorflow import keras
 
 from keras_cv.layers.preprocessing_3d import base_augmentation_layer_3d
 from keras_cv.point_cloud import is_within_any_box3d
```

## Comparing `keras_cv/layers/preprocessing_3d/random_drop_box_test.py` & `keras_cv/layers/preprocessing_3d/waymo/random_drop_box_test.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,27 +1,17 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2022 Waymo LLC.
 #
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+# Licensed under the terms in https://github.com/keras-team/keras-cv/blob/master/keras_cv/layers/preprocessing_3d/waymo/LICENSE  # noqa: E501
 
 import numpy as np
 import tensorflow as tf
 from tensorflow import keras
 
 from keras_cv.layers.preprocessing_3d import base_augmentation_layer_3d
-from keras_cv.layers.preprocessing_3d.random_drop_box import RandomDropBox
+from keras_cv.layers.preprocessing_3d.waymo.random_drop_box import RandomDropBox
 
 POINT_CLOUDS = base_augmentation_layer_3d.POINT_CLOUDS
 BOUNDING_BOXES = base_augmentation_layer_3d.BOUNDING_BOXES
 ADDITIONAL_POINT_CLOUDS = base_augmentation_layer_3d.ADDITIONAL_POINT_CLOUDS
 ADDITIONAL_BOUNDING_BOXES = base_augmentation_layer_3d.ADDITIONAL_BOUNDING_BOXES
```

## Comparing `keras_cv/layers/preprocessing_3d/swap_background.py` & `keras_cv/layers/preprocessing_3d/waymo/swap_background.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,20 +1,10 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2022 Waymo LLC.
 #
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+# Licensed under the terms in https://github.com/keras-team/keras-cv/blob/master/keras_cv/layers/preprocessing_3d/waymo/LICENSE  # noqa: E501
 
 import tensorflow as tf
 from tensorflow import keras
 
 from keras_cv.bounding_box_3d import CENTER_XYZ_DXDYDZ_PHI
 from keras_cv.layers.preprocessing_3d import base_augmentation_layer_3d
 from keras_cv.point_cloud import is_within_any_box3d
@@ -53,14 +43,15 @@
     Output shape:
       A tuple of two Tensors (point_clouds, bounding_boxes) with the same shape
       as input Tensors.
 
     """
 
     def __init__(self, **kwargs):
+        # TODO(ianstenbit): Support the model input format.
         super().__init__(**kwargs)
         self.auto_vectorize = False
 
     def get_config(self):
         return {}
 
     def get_random_transformation(
```

## Comparing `keras_cv/layers/preprocessing_3d/swap_background_test.py` & `keras_cv/layers/preprocessing_3d/waymo/swap_background_test.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,25 +1,18 @@
-# Copyright 2022 The KerasCV Authors
+# Copyright 2022 Waymo LLC.
 #
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     https://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
+# Licensed under the terms in https://github.com/keras-team/keras-cv/blob/master/keras_cv/layers/preprocessing_3d/waymo/LICENSE  # noqa: E501
+
 import numpy as np
 import tensorflow as tf
 
 from keras_cv.layers.preprocessing_3d import base_augmentation_layer_3d
-from keras_cv.layers.preprocessing_3d.swap_background import SwapBackground
+from keras_cv.layers.preprocessing_3d.waymo.swap_background import (
+    SwapBackground,
+)
 
 POINT_CLOUDS = base_augmentation_layer_3d.POINT_CLOUDS
 BOUNDING_BOXES = base_augmentation_layer_3d.BOUNDING_BOXES
 ADDITIONAL_POINT_CLOUDS = base_augmentation_layer_3d.ADDITIONAL_POINT_CLOUDS
 ADDITIONAL_BOUNDING_BOXES = base_augmentation_layer_3d.ADDITIONAL_BOUNDING_BOXES
```

## Comparing `keras_cv-0.5.0.dist-info/LICENSE` & `keras_cv-0.5.1.dist-info/LICENSE`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,12 @@
-Copyright 2022 The KerasCV Authors. All rights reserved.
+Copyright © 2023 The KerasCV Authors
+All code in this repository excluding the code located in
+keras_cv/layers/preprocessing_3d/waymo is licensed under the Apache License,
+Version 2.0. The code appearing in the keras_cv/layers/preprocessing_3d/waymo
+folder is licensed under terms appearing below.
 
                                  Apache License
                            Version 2.0, January 2004
                         http://www.apache.org/licenses/
 
    TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
 
@@ -184,20 +188,25 @@
       replaced with your own identifying information. (Don't include
       the brackets!)  The text should be enclosed in the appropriate
       comment syntax for the file format. We also recommend that a
       file or class name and description of purpose be included on the
       same "printed page" as the copyright notice for easier
       identification within third-party archives.
 
-   Copyright 2015, The TensorFlow Authors.
+   Copyright 2023 The KerasCV Authors
 
    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at
 
        http://www.apache.org/licenses/LICENSE-2.0
 
    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
+
+# The following applies only to the code appearing in
+# keras_cv/layers/preprocessing_3d/waymo
+
+License: https://github.com/keras-team/keras-cv/blob/master/keras_cv/layers/preprocessing_3d/waymo/LICENSE
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## Comparing `keras_cv-0.5.0.dist-info/METADATA` & `keras_cv-0.5.1.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: keras-cv
-Version: 0.5.0
+Version: 0.5.1
 Summary: Industry-strength computer Vision extensions for Keras.
 Home-page: https://github.com/keras-team/keras-cv
 Author: Keras team
 Author-email: keras-cv@google.com
 License: Apache License 2.0
 Classifier: Programming Language :: Python
 Classifier: Programming Language :: Python :: 3.7
@@ -148,15 +148,15 @@
 
 We would like to leverage/outsource the Keras community not only for bug reporting,
 but also for active development for feature delivery. To achieve this, here is the predefined
 process for how to contribute to this repository:
 
 1) Contributors are always welcome to help us fix an issue, add tests, better documentation. 
 2) If contributors would like to create a backbone, we usually require a pre-trained weight set
-with the model for one dataset as the first PR, and a training script as a follow-up. The training script will preferrably help us reproduce the results claimed from paper. The backbone should be generic but the training script can contain paper specific parameters such as learning rate schedules and weight decays. The training script will be used to produce leaderboard results. 
+with the model for one dataset as the first PR, and a training script as a follow-up. The training script will preferably help us reproduce the results claimed from paper. The backbone should be generic but the training script can contain paper specific parameters such as learning rate schedules and weight decays. The training script will be used to produce leaderboard results. 
 Exceptions apply to large transformer-based models which are difficult to train. If this is the case,
 contributors should let us know so the team can help in training the model or providing GCP resources.
 3) If contributors would like to create a meta arch, please try to be aligned with our roadmap and create a PR for design review to make sure the meta arch is modular.
 4) If contributors would like to create a new input formatting which is not in our roadmap for the next 6 months, e.g., keypoint, please create an issue and ask for a sponsor.
 5) If contributors would like to support a new task which is not in our roadmap for the next 6 months, e.g., 3D reconstruction, please create an issue and ask for a sponsor.
 
 Thank you to all of our wonderful contributors!
```

## Comparing `keras_cv-0.5.0.dist-info/RECORD` & `keras_cv-0.5.1.dist-info/RECORD`

 * *Files 4% similar despite different names*

```diff
@@ -1,18 +1,18 @@
-keras_cv/__init__.py,sha256=nC27SQ3bDpMketN8Ais4afv-oWdEtD8igF1plTACZxo,1182
-keras_cv/conftest.py,sha256=S4niR-IK05iFT_hWO9FuvmXX3FXyV2O1FwRv0DcpsXE,2271
+keras_cv/__init__.py,sha256=6wteSYLAzYLXim8lP7-B-HWtDEcF3h52iHt7IbpxP2M,1182
+keras_cv/conftest.py,sha256=1GK8v29KpFjexH0wwxWEKCANkQAlrOTER6Jw_E9z6DM,2354
 keras_cv/version_check.py,sha256=LqMks3U07vEn9lmEZOoBvTgRl7_Nee-6rt_bkUP8VKM,1159
 keras_cv/version_check_test.py,sha256=rV0HuMJEAQOoE8embKlWKMLFMB8dY4WbYBxFdJ-jbjU,1362
-keras_cv/bounding_box/__init__.py,sha256=ZSWjUIwDIAlQf6uKq5H-XUD1U8JwJDSZb0HN-vSdpag,1611
+keras_cv/bounding_box/__init__.py,sha256=coBAs1UK0Mcj7FgEEFtse6TQWNZ8kQSys2axUoTq_34,1662
 keras_cv/bounding_box/converters.py,sha256=zdOwi1MtCv8E6WbXgnBRghxLDvJWCCrFZXrhSMFcCdI,18483
 keras_cv/bounding_box/converters_test.py,sha256=vhV7ip5QMgsL27zu5GWsSvRgv6z1jlRV0FyZ9mYD4Oo,7134
 keras_cv/bounding_box/ensure_tensor.py,sha256=6d7lbfv4-PKeY4yfpvphMClNdw2g49WohBGekzXXoHk,909
 keras_cv/bounding_box/ensure_tensor_test.py,sha256=y_8eepWMJ5Y2QRHOD8J3FDDT9hZoMTlDf_5Znehumvo,1443
 keras_cv/bounding_box/formats.py,sha256=ClUExYPKQ9pEnPMhgmc48sC0PDpPnqHkzPq_GqlNuWs,4035
-keras_cv/bounding_box/iou.py,sha256=x9udflg92_5F5GDUM6FOpEUTYMSSFv1kHDFpVe2kEZY,6412
+keras_cv/bounding_box/iou.py,sha256=PCrhI5Lr5cBBW4xEs0WawVre27sYgEYq68UVLxYiIrE,9144
 keras_cv/bounding_box/iou_test.py,sha256=H4_6OzNVEQ-pbRestVmGds85R8mFZOS1Z9S0wj7Krsc,6130
 keras_cv/bounding_box/mask_invalid_detections.py,sha256=-kCZNPvMCWhCUSxF2GGSLxOfgzwDxxqCeLffgiWhGwE,3751
 keras_cv/bounding_box/mask_invalid_detections_test.py,sha256=tMexLag8SQ5BaWY8Ti8lJJYrnay0ovkTun0xc2dA06U,3901
 keras_cv/bounding_box/to_dense.py,sha256=GrDNFiDdJGIzJUIMOS0zjZFkfKf1J_XUjfwgxqo-PBE,3204
 keras_cv/bounding_box/to_dense_test.py,sha256=wjUtUn8AhyjRAHHpBLt3wFmwpXvy2CFc6qhL_orOXr4,1147
 keras_cv/bounding_box/to_ragged.py,sha256=w0idLsXjuySRvxtIdjuonfn1fAvyeSAGZJ0yZNcGieA,3014
 keras_cv/bounding_box/to_ragged_test.py,sha256=w5P8uupWyEcdGt8TG-OeTCc_UhJQPMHrlc-rKWabVxs,2713
@@ -20,16 +20,16 @@
 keras_cv/bounding_box/utils_test.py,sha256=oqOp4QULsLwwO0F4bpZmc1QqIhi3N8qZgRWpwlfJqCI,5569
 keras_cv/bounding_box/validate_format.py,sha256=IrdFqLw5a-ENw3g1LTO5V-VFY30M1VA7ZKg-vaB0viM,3479
 keras_cv/bounding_box/validate_format_test.py,sha256=ruHJk2_ioqZnymkCBf8xCZPv07GlZ7IxP0B2oh6Q1O8,1581
 keras_cv/bounding_box_3d/__init__.py,sha256=hokR_WW7-_7UBm4ebdib6MjQpuDHoe1TU-fHFfi4ZPE,652
 keras_cv/bounding_box_3d/formats.py,sha256=_VLtI5w8ksveB-eJ4l1Dselw1C0fCF1fA4-myYbt28Y,1609
 keras_cv/callbacks/__init__.py,sha256=GaURZvzP51cgtturjtiGm0Kuov_0NsLWAMDvT7_ECB0,727
 keras_cv/callbacks/pycoco_callback.py,sha256=JWt8UcdesSoNSY9ZvS6NZqTBBvd2pPr1vjh5H9xgxqo,4693
-keras_cv/callbacks/pycoco_callback_test.py,sha256=SjiKTIhjkuzeC1FKAOPOJvNvIY3X7UbUuUYmEAK7kyU,3263
-keras_cv/callbacks/waymo_evaluation_callback.py,sha256=3nwLcbEwCBFkrs-FoURCRibivNHowyWi3OECDK2S4PI,6912
+keras_cv/callbacks/pycoco_callback_test.py,sha256=l_kO7q_zDyUXuF59InM-33WKwztIeBWtaao6XXagKm4,3323
+keras_cv/callbacks/waymo_evaluation_callback.py,sha256=VOfyEoWoyUsgzX47WH49Tn39lKjZCtTVCsK1D9c9awI,6910
 keras_cv/callbacks/waymo_evaluation_callback_test.py,sha256=A9eEyqeETWt_FIwSel43VRGWmGntxNfM7m4FNdtxOrY,3413
 keras_cv/core/__init__.py,sha256=CKYvfvZOXfcuH9XwYxt1XD-aR316P6mfXuYJSkHoAww,936
 keras_cv/core/factor_sampler/__init__.py,sha256=hDBbodQWKeVBL86IY1jpd0BqfnzOp7jtjmghJqQOSqk,584
 keras_cv/core/factor_sampler/constant_factor_sampler.py,sha256=yyn__-MmJAqVTJeExFvY6NVkSij0fO--bLVsp1hqvf8,1667
 keras_cv/core/factor_sampler/constant_factor_sampler_test.py,sha256=2BzWrYJXlQZLT6OJwqb52qtQOXHA8srLGvqBcPhu3gI,964
 keras_cv/core/factor_sampler/factor_sampler.py,sha256=PYBNGRE1SxHKZ6AdgP5TrxyAxc0pQmIIVfkfNFDQ1Bo,1343
 keras_cv/core/factor_sampler/normal_factor_sampler.py,sha256=wzBfrG2aXszrR7bob0cPQaDbZ8vLNLwMXLiT8vYQz6c,2501
@@ -52,57 +52,57 @@
 keras_cv/datasets/waymo/transformer_test.py,sha256=tmTtyO_B5ra3RFB6stssf2wTOatOmaceaeK2QcEXTb0,6947
 keras_cv/keypoint/__init__.py,sha256=r64NpD6b0BT0CS_hmpr_60Ah74pXgyYANss1x2bO2r0,783
 keras_cv/keypoint/converters.py,sha256=VKklqsoBwwtEmfwqOHBe0DCZc55orzbC1-xvC6V-YGk,6955
 keras_cv/keypoint/converters_test.py,sha256=qpgRbTtaH5oUe18sj9itOR1_mmU-4jcyXUnUDKMdZjc,5181
 keras_cv/keypoint/formats.py,sha256=b7vVRK9ePdC9lMeLNOlIRg_lnq8anuDsHu6o8Lq8XJU,1725
 keras_cv/keypoint/utils.py,sha256=H5SCMC0WWm8rclaAJr7pIHzitQuljbZ8FxCR4GYfLu8,1597
 keras_cv/keypoint/utils_test.py,sha256=_pcpWKS6zI5rj_vxM8PsaLRy7R2n5JA11hE4aaan25E,2000
-keras_cv/layers/__init__.py,sha256=gpmQu4OcLwLHAJFLkBO23eBnkMom6m9oKbfhWwiPhNI,6057
+keras_cv/layers/__init__.py,sha256=_IjAl9WZ2BcIKZyQq8Ef81NB0MDlnNfFxz-BR_a9rlg,6103
 keras_cv/layers/feature_pyramid.py,sha256=KcNUCNRTcJwjXVbbvUUN4EY3oPQ4pZZbZ9hEMdnZqo4,8828
 keras_cv/layers/feature_pyramid_test.py,sha256=BwO_c3GFiITgxsdUJ4xN-EelrQkmoof-pvrOV-7XUGI,5125
 keras_cv/layers/fusedmbconv.py,sha256=MqLvyOR5wjs4kdlkWvPzdgFd1PogH5z2xJwh80xVDpk,8076
 keras_cv/layers/fusedmbconv_test.py,sha256=aEfM3bhsqI2CY3TrxJOpYliCMxGnQSly7wz3_vNmpCg,2273
 keras_cv/layers/mbconv.py,sha256=6YbzUBk19egc7L4qcTSgezhYJ5mhWmz3ON761irZKfM,8349
 keras_cv/layers/mbconv_test.py,sha256=gx0ylTG9BL1kopIh8EgIYpOILNQ8ck4Amp-skYKVzi0,2216
-keras_cv/layers/serialization_test.py,sha256=41RwgyRNRa6NN3vImEmLzJOLr9GB7ypvYVnSQqYeaAQ,12344
+keras_cv/layers/serialization_test.py,sha256=_3G017HBVBDrgkgAgI1M0wXy9oF4PCf6OQuqFTnm7Dc,12061
 keras_cv/layers/spatial_pyramid.py,sha256=moO2kwHzMORqv8KL25-dNStPXCKc6rqm0EsJLVreq3E,6281
 keras_cv/layers/spatial_pyramid_test.py,sha256=pcCIG967qGmE5VI7hfrln59VRCzk7-WySFpXU4zfrQU,1290
 keras_cv/layers/transformer_encoder.py,sha256=VG_j7yNYWfy0V8noJAXHOSiIGhW2FQ62g5az42Jk8aw,5249
 keras_cv/layers/transformer_encoder_test.py,sha256=OoZF6xLbZSrQC7L0COMyiXcow8pVFX256o6DsjvB2U4,2135
 keras_cv/layers/vit_layers.py,sha256=eDe99utjMz9CrZYtWmAVwj6u3Q1BKqwgjmRvbW2VQT8,7723
 keras_cv/layers/vit_layers_test.py,sha256=ykwjmxfMuCiJAYZf46W_0oQt-wEgQ8LRhWSYnQ30zvQ,2886
 keras_cv/layers/object_detection/__init__.py,sha256=hDBbodQWKeVBL86IY1jpd0BqfnzOp7jtjmghJqQOSqk,584
-keras_cv/layers/object_detection/anchor_generator.py,sha256=5Ezt0HR8TcZ82RoPsxswqUsPJDDqf4RGE1t-cnsUX7s,11340
-keras_cv/layers/object_detection/anchor_generator_test.py,sha256=KfqFHUU3zpmfIT9LCsxXbw_wN58GELkbWvrbKi_blWw,6318
+keras_cv/layers/object_detection/anchor_generator.py,sha256=nng-JAWtBjkkzH51LA_ArBWF5O011mvxvZsSHj1pJ0E,11406
+keras_cv/layers/object_detection/anchor_generator_test.py,sha256=P3A4cZEoKZM_15_YaM8x3KZjD5Q5DH5-qQGXM6JX1ac,6422
 keras_cv/layers/object_detection/box_matcher.py,sha256=OYNhkXpRr5SDMbD-MMI936RhO-v11sIMT4CYmSN-QkM,11483
 keras_cv/layers/object_detection/box_matcher_test.py,sha256=jR2m1KFh_9g875-qksmjZdsghlJSR7ppw3slJhLXq68,4939
 keras_cv/layers/object_detection/multi_class_non_max_suppression.py,sha256=_QoL0dolU5xv0J2Zz1_UdlD308m7ptXWq7nnwwQgdzE,5140
 keras_cv/layers/object_detection/multi_class_non_max_suppression_test.py,sha256=yW5cuqoJ4xzhWfElEYiGD-XE6aRg5aLHDeR7eAiWV8U,1610
-keras_cv/layers/object_detection/roi_align.py,sha256=BWYhDlFcccUmciUcik1nbB3ekj1UEyHRyPedFZy1bHM,15804
+keras_cv/layers/object_detection/roi_align.py,sha256=oQ9Jfgipum_MTSvBUIACXSJe-Hg5A35vy1RYdHewKJ4,16185
 keras_cv/layers/object_detection/roi_generator.py,sha256=SiVVMlrBtCuj55oBr9zkJWeYXX04p8kug4RwMWubjp4,9833
 keras_cv/layers/object_detection/roi_generator_test.py,sha256=gGwxAoefwEziGq3KYL4hYv_oV9qPRmcM6OQXCX4Omkk,9256
 keras_cv/layers/object_detection/roi_pool.py,sha256=cIYIH76gWvGtDKgmZ8NqUA4l-Sw58V9Xktp11wkN9gM,6427
 keras_cv/layers/object_detection/roi_pool_test.py,sha256=E4lKzwcuOG8y_2a6EykriLJG6Fngkk_X4GIJjdeFHfc,9712
 keras_cv/layers/object_detection/roi_sampler.py,sha256=w_0TA2Vtlgx-CGUIiEZRgg1zyn4sfiak9tUj29s43_M,9064
-keras_cv/layers/object_detection/roi_sampler_test.py,sha256=gj0H4nYgWFj1xpAn9mBPllJ4PWYGCveig5BsziGh3Bw,11644
+keras_cv/layers/object_detection/roi_sampler_test.py,sha256=5T53jKZ9JahLBRE0CaxENTo1jTWSMY36lb8V_SIp-nw,11643
 keras_cv/layers/object_detection/rpn_label_encoder.py,sha256=AnMNPwSr_v9tPC5rfHO4--U24gL5BDZRS22kyGX9bpc,9272
 keras_cv/layers/object_detection/rpn_label_encoder_test.py,sha256=wH2PrfMfhp2HZ3f2QI-tmlmykC8schjLfsb2k6IcoIU,5631
 keras_cv/layers/object_detection/sampling.py,sha256=YCckkFqLp71zp85JZ1DZHGtqho8W7hE2fg7IYvmovsE,3426
 keras_cv/layers/object_detection/sampling_test.py,sha256=NoDiwuAm2WqPX4l_qPtPkVw4lM4sjGNC0XleNQ9LoWo,7277
 keras_cv/layers/object_detection_3d/__init__.py,sha256=hDBbodQWKeVBL86IY1jpd0BqfnzOp7jtjmghJqQOSqk,584
-keras_cv/layers/object_detection_3d/centernet_label_encoder.py,sha256=LbKJrnMXTc28yqDrRhqmNHLoZL5vTVIXHlZXO9uh2qk,16374
-keras_cv/layers/object_detection_3d/centernet_label_encoder_test.py,sha256=MVzXwchPiaQCL_3RHfwtkpOXu7ChndiqBwbGCd4Coqw,4863
+keras_cv/layers/object_detection_3d/centernet_label_encoder.py,sha256=nUEL8w6C1q_6lk96M7CfcZLrD9LxvcQX-aQCBy8C-Mo,16098
+keras_cv/layers/object_detection_3d/centernet_label_encoder_test.py,sha256=WVkv-iNp984PZukuokvG2VaXOdqUwn5ywD_hUhVSRao,4743
 keras_cv/layers/object_detection_3d/heatmap_decoder.py,sha256=tCaA5MEmKvhERcrtVrDc0iVZvL73Y5_1ZxK2hmr18eY,8107
 keras_cv/layers/object_detection_3d/voxel_utils.py,sha256=AM1czJfqLqaXrRjHy8XROHJcXKAA-ZfTRPtoZBAHsEc,10029
 keras_cv/layers/object_detection_3d/voxel_utils_test.py,sha256=bFR-xIGe2fda3klY13bcz4ZJtLyT_T_ita1BzZjWf9k,2841
 keras_cv/layers/object_detection_3d/voxelization.py,sha256=iYTadgC2h3yNqnwg-fpa7G62kG5rIfKLSerzP1Bccv8,9159
 keras_cv/layers/object_detection_3d/voxelization_test.py,sha256=9b9MO1RhUZXZ_1-WLcGbgahL8XLYEslLpsZvFtmdkf8,4114
-keras_cv/layers/preprocessing/__init__.py,sha256=YDXIE8StGdSaEPkRgkNKbZBv7mGZAK3gyZijgpIvE0Q,4003
+keras_cv/layers/preprocessing/__init__.py,sha256=Xt9CHiVH4BNqiGu5iaYiDagJEGoyoXVjnLaPBfa5g9A,3914
 keras_cv/layers/preprocessing/aug_mix.py,sha256=TkkcX2Hnmp7SeHVT_q5dlchMZM38a_-Z3J8IVOZ9eMc,12754
-keras_cv/layers/preprocessing/aug_mix_test.py,sha256=ZXY3L4eYZKesaDV4aRa5zYP9bf49rmuEntBggpnADvo,2205
+keras_cv/layers/preprocessing/aug_mix_test.py,sha256=qyGIT2A4BN0bC8bPYKyJi51Pl_ayEbDlX-EIuKHZmnQ,2580
 keras_cv/layers/preprocessing/auto_contrast.py,sha256=8TOYm11omxHoR6cdfVj0YjeIbdxLZYgkAY2uJUAe_p0,3509
 keras_cv/layers/preprocessing/auto_contrast_test.py,sha256=h9q40em7oa9MVH-J05MOYM9-9oXmCnJGQ9T_cwYfCa0,3331
 keras_cv/layers/preprocessing/base_image_augmentation_layer.py,sha256=_zz2ltzbTw555AkwRXcCMU24QBNiHdHD9ICLZIKni5E,20504
 keras_cv/layers/preprocessing/base_image_augmentation_layer_test.py,sha256=FFcoKKdpAkgBNWYwvLHg-v6wTfZhyJFhtJeF9amUXEI,10953
 keras_cv/layers/preprocessing/channel_shuffle.py,sha256=Vl8do16_OhmFfNQht7-wT6sYxEjLBDLazvcFZdOPOOU,4559
 keras_cv/layers/preprocessing/channel_shuffle_test.py,sha256=Or2CYMmxNoJ2HWUMt7IanqmcHtQrfYCcljL6nwm-MXc,4065
 keras_cv/layers/preprocessing/cut_mix.py,sha256=d41F7OQPTwShcZIHxYq0lsq7eAhj8SPzDzDou87ldMs,5992
@@ -113,25 +113,25 @@
 keras_cv/layers/preprocessing/fourier_mix_test.py,sha256=2D6YCdpq120gB9uHkRItdN4v5jYgNbfN7699NtV7Jug,3447
 keras_cv/layers/preprocessing/grayscale.py,sha256=mM8jwsRqxsDb016u7LaOImUU823udd59xMu6Byj3GAc,3841
 keras_cv/layers/preprocessing/grayscale_test.py,sha256=JTVymTD9mBBh5Zu-Akkf9Pck9Rg-2kPwXcMmdRjN4Kc,2820
 keras_cv/layers/preprocessing/grid_mask.py,sha256=qeyFEi9_mb6YFHayr8MJoLxRKpQaJHqPD9xRvPpXj6o,9733
 keras_cv/layers/preprocessing/grid_mask_test.py,sha256=RFsl45u6Yi_nG4lorqDQz6FJahRF1YzaZOcrXBsJmHU,3823
 keras_cv/layers/preprocessing/jittered_resize.py,sha256=R_FVyRid5n-DNh6KNbpkiMapdTpcMMKHIlI2uLfySWE,11349
 keras_cv/layers/preprocessing/jittered_resize_test.py,sha256=1ahFxImZ_XDBnYT7v3BNa9VTlankWdsSgbLpH6enlEo,7360
-keras_cv/layers/preprocessing/maybe_apply.py,sha256=ZuolFxkYzDReMNjBQrkiBGLLmyAuawgfRQmijdgmDvc,5019
-keras_cv/layers/preprocessing/maybe_apply_test.py,sha256=PUiV4DITtMM1zVohrkaGIPuHCZQZTTwf9MDHoWW3rSs,4634
-keras_cv/layers/preprocessing/mix_up.py,sha256=rGyP0c0by3iHA1kEu9s56uaJNfGE4OLXffe7B4B4rco,6100
-keras_cv/layers/preprocessing/mix_up_test.py,sha256=AStUGw7CmFgQviVAOz5Bva4oijJL-jTZRlloijhUh4o,4581
+keras_cv/layers/preprocessing/mix_up.py,sha256=dUbD5TaBzl-TLADc1hbB6ZpA8ypNXxeMEJZ3gLFaRP8,7461
+keras_cv/layers/preprocessing/mix_up_test.py,sha256=G2boDs1ybCP7h2Jr2EMVwLgNcOtJvjCvfrSze6umdTw,6180
 keras_cv/layers/preprocessing/mosaic.py,sha256=u4ms7qWnSQ_a-Ms9KwlfVXQ84V-oxhBsEQFKd8ltijg,13424
 keras_cv/layers/preprocessing/mosaic_test.py,sha256=TbxJbwGqMkp3O_ilSuJNH1_gQC2-s7lj3HamHz41vX4,3726
 keras_cv/layers/preprocessing/posterization.py,sha256=z_i8-SeHd20hppTrWlzrtggPfgf8145ZGkVDJnaczIQ,4237
 keras_cv/layers/preprocessing/posterization_test.py,sha256=GX4UvVllEYKcquYouF7n0MdCBtsz-1VRiWolN7CY5Mo,3792
-keras_cv/layers/preprocessing/ragged_image_test.py,sha256=bCqlH0kqevWdHuiTIHh0xf918DT1u61bn61-CawKGyQ,5343
+keras_cv/layers/preprocessing/ragged_image_test.py,sha256=Go_f7A5Pvokdv3_fB2lFrRspcC-IXzGW2n9vnqKU0LI,5101
 keras_cv/layers/preprocessing/rand_augment.py,sha256=GhSySn_Gv2WNlO0SP2a91lwjOTWNFhHXXxi43KICNTw,10805
 keras_cv/layers/preprocessing/rand_augment_test.py,sha256=Zy-VCR9TXY2yF8ycMe2SGaWG5vT5rUMco1uglqg1Ndk,3758
+keras_cv/layers/preprocessing/random_apply.py,sha256=xcjzf2o3FtEgRvWwR5xJuCpX1OVO9tKqGWGYKHqBWOQ,5023
+keras_cv/layers/preprocessing/random_apply_test.py,sha256=jnZlTqYo9aXk7DhzkMIaOC5YefxnsLbre3O_Hjon1S0,4646
 keras_cv/layers/preprocessing/random_aspect_ratio.py,sha256=cSDj28L__z33lxtDyGRc2YzovjoSJmKaq3iInKPbq6o,4683
 keras_cv/layers/preprocessing/random_aspect_ratio_test.py,sha256=R6kULrL5FggsQO88gYJhC0dIqz3FpDV6InLoL830aLw,2277
 keras_cv/layers/preprocessing/random_augmentation_pipeline.py,sha256=GmnTpiS7DW8vzbbs-PrFWN1VIIEH_kGSFYg6-t_Bojk,4754
 keras_cv/layers/preprocessing/random_augmentation_pipeline_test.py,sha256=Y255n0YDCPFHd1rwFmKexba4PR6r5lQ3IUAzNjrS9oI,3340
 keras_cv/layers/preprocessing/random_brightness.py,sha256=YKj4hVwfm-5EPytYt2mOT241nnf3Dic2NU0C4QqvMpI,5241
 keras_cv/layers/preprocessing/random_brightness_test.py,sha256=JMN9MdzDj1zoqP4ykKuPg_LMfz99TLF3IAsfRT88xzg,3337
 keras_cv/layers/preprocessing/random_channel_shift.py,sha256=5BpP2n-P7XQoD4l0f6b-o3i7qJry44WKj4HRxpZ906g,4396
@@ -140,92 +140,94 @@
 keras_cv/layers/preprocessing/random_choice_test.py,sha256=rRJs_svcz6T7sx-WTXEMivjwKGQqv0wemOgkss1bns0,3316
 keras_cv/layers/preprocessing/random_color_degeneration.py,sha256=53KaEbV3Ele0x4yIc3zKRxVRH0QbbbZiiJdCoftOwbE,3392
 keras_cv/layers/preprocessing/random_color_degeneration_test.py,sha256=CGNpf979r5Zyfo05Ipgr4vBOwRKoj30atWLlSBtfBGs,2648
 keras_cv/layers/preprocessing/random_color_jitter.py,sha256=Z6TYcA-38KHwAUB57mZ1UsLpWwvNgCDyIIAr3KNowb8,6946
 keras_cv/layers/preprocessing/random_color_jitter_test.py,sha256=Z7aXMk74mCWWmXwFE46DButJpQiBPoP7yJUNqfk4QJU,3902
 keras_cv/layers/preprocessing/random_contrast.py,sha256=AcNGGlgJK-W-sg9gfJOUOo3uLzxORIEI4FLGlTw3zFM,4864
 keras_cv/layers/preprocessing/random_contrast_test.py,sha256=NVTvsRThc3WeKVz8-hMPuK5wsia_4Hkw8u0G4pzyXF8,2213
-keras_cv/layers/preprocessing/random_crop.py,sha256=Zl8ZEi_RIEUnGdPyfQ3oOuVPukH19ys3K0tYKQ_-2nk,10847
+keras_cv/layers/preprocessing/random_crop.py,sha256=e71xkckFIon5K-vWriIjLqFl3ML9WYrGeobwzkrhHL4,10846
 keras_cv/layers/preprocessing/random_crop_and_resize.py,sha256=uVwAHrc4B_Ls1amzy4LHjUMuHHapJaHG20PWQCNUUso,11178
 keras_cv/layers/preprocessing/random_crop_and_resize_test.py,sha256=9UgxIClGEAuJWF_6Ny9NMCkS3mf8Hjcoha35ehhykvs,10241
 keras_cv/layers/preprocessing/random_crop_test.py,sha256=KDSXAex569_MhVVxzRAN4Vc_rXmZgoWLOTk2WuaJG8Q,9856
 keras_cv/layers/preprocessing/random_cutout.py,sha256=BGA0uDt-Lxmmns1RzELY7mM_d03JVvEOAxCftUUniaY,7024
 keras_cv/layers/preprocessing/random_cutout_test.py,sha256=9isiv_0Vj3nvZAXgxgFCoB-9iNFvlyAZ9Oiiqf-AR4s,4978
 keras_cv/layers/preprocessing/random_flip.py,sha256=R850U1PqpMhkuQ7v6vAaL4Yj_nrzo29GgfZhX-3tbkg,9003
 keras_cv/layers/preprocessing/random_flip_test.py,sha256=pjTDGOywZR19vItJ4rwNTyNbxIRB3w-8yJQpP5LUvFg,10994
 keras_cv/layers/preprocessing/random_gaussian_blur.py,sha256=Qi_xh53gz0mU3idr5pjKRdPzX07dAcPKJNeejpdQt8c,4576
 keras_cv/layers/preprocessing/random_gaussian_blur_test.py,sha256=qKXMbClcRqadaO_4Zs97G-kWMTGta2tju9J04xeNOAQ,3245
 keras_cv/layers/preprocessing/random_hue.py,sha256=GohDofOhXEHu6Pb3mP4F4IwVxZmzgfqLik28H4Bozo0,5433
 keras_cv/layers/preprocessing/random_hue_test.py,sha256=upvOGOXGsfdBkmNdkQ9E9_A0d1zInJjVDpe_BAIwIqc,4230
 keras_cv/layers/preprocessing/random_jpeg_quality.py,sha256=OJg6ldVju9p2SfEg3BzP3lpebEW4DI6S-jOlEf4EXfY,3021
 keras_cv/layers/preprocessing/random_jpeg_quality_test.py,sha256=C5MwpgOivDbwKJyh8iUjqXdBnpfdlaPDwTt6AVCumKc,1984
-keras_cv/layers/preprocessing/random_rotation.py,sha256=jXciaVvLmVok_dqFSEEcqt1Ca9GdY5pIrW48zz7desY,12266
+keras_cv/layers/preprocessing/random_rotation.py,sha256=XHJBSulMI2GnqKUq2WDoxIoqc9Gy0gHwehqaRXxi_x0,12271
 keras_cv/layers/preprocessing/random_rotation_test.py,sha256=p6uwKrYEcirGUWiW-R9g-q-4UPiCNqcfwzqCWVQAQgY,7395
 keras_cv/layers/preprocessing/random_saturation.py,sha256=0nHv80-IaL_xdY3ftHLKsHYj6OoujRyhQwrUM9C6nUo,4951
 keras_cv/layers/preprocessing/random_saturation_test.py,sha256=ojtam1LOHjSBr89i3ZpOE2fQWSIR7o8PZsmk2HOMzyI,8248
 keras_cv/layers/preprocessing/random_sharpness.py,sha256=leperIQmL7LsDO-IOHJG3iX04hlBpvk4IYWzA4TPARY,5813
 keras_cv/layers/preprocessing/random_sharpness_test.py,sha256=0Il3x6WsuvI13U-UWglj_zvqYwnGG_wV8whCLJVDII8,2724
-keras_cv/layers/preprocessing/random_shear.py,sha256=k1ljbr-qij1Ck3qRrVZQRjkh1HOyPbiw4jH9chJWZZ0,12947
+keras_cv/layers/preprocessing/random_shear.py,sha256=sgnCm_TJkAzj9KVWEiItzA3KLfdBVVJYxuERLrZ3Sus,13111
 keras_cv/layers/preprocessing/random_shear_test.py,sha256=hDSaeI1uNB6YVV1_d54yhSkrZfPgqCh76oAaaAbc1ok,9331
 keras_cv/layers/preprocessing/random_translation.py,sha256=IGRJaJoqj3F9uqx996koQFv8pv7EwC6Ui5nc0SYVwAg,11148
 keras_cv/layers/preprocessing/random_translation_test.py,sha256=14K-lrHIAZvKty3Je8VE0QUs_RfZ7Rb7VwNLf4mlWGw,8917
 keras_cv/layers/preprocessing/random_zoom.py,sha256=AsHgGYsffTG_ohlDsnzMZ4mlD1gh-8SQoNAiVD9Yd80,10340
 keras_cv/layers/preprocessing/random_zoom_test.py,sha256=98ug5ZDgBI7Bq84s97tt22Y7WOA9I5r1j_qNaxNj2-Q,5859
-keras_cv/layers/preprocessing/randomly_zoomed_crop.py,sha256=iqeQXooCQQNxzZZkybxTS7wI1zwQ3--UhVhgGwMubR4,11418
-keras_cv/layers/preprocessing/randomly_zoomed_crop_test.py,sha256=OWeI-ix2Kj465GSyeDRIJYcIR3Lbj_a1DftQW9WPUNM,7266
 keras_cv/layers/preprocessing/repeated_augmentation.py,sha256=xtZy7qGY4hhrXKjzG6tCubGjOqaqVHtYoO5qjKozm-Q,4677
 keras_cv/layers/preprocessing/repeated_augmentation_test.py,sha256=HzlBGQOAv0rwGaYiVhpHmU50Q808zlrcpqBaYe2bBLE,1753
 keras_cv/layers/preprocessing/rescaling.py,sha256=hor0dxiHPGHqak6HIGKjqsDvd7XHHg2WuPmEy2Giz70,2766
 keras_cv/layers/preprocessing/rescaling_test.py,sha256=G7pJGDkFGTSqnKxpLT9VoP4433uzQGo2y9LIud8SJdI,2123
-keras_cv/layers/preprocessing/resizing.py,sha256=iUnreT-7qUFmEx3_UUhstrg33-VxDDsm3sxp2CJC_LE,12328
-keras_cv/layers/preprocessing/resizing_test.py,sha256=5C1uUl7xBidiUvylV3O_XkNSNZqNbLVqW1RVth0Y6-M,11545
+keras_cv/layers/preprocessing/resizing.py,sha256=Z45iRZQM1lskXrafaNwAcDu7WYhRenUnR2Ix9x7tVKE,15342
+keras_cv/layers/preprocessing/resizing_test.py,sha256=ZpfkIFRVOw9ss8uIJtMvsN_s2eV4oqodV-9V52J2kRc,12313
 keras_cv/layers/preprocessing/solarization.py,sha256=Spnuyvh3DinbV_g5cknlN7Pd5I3poQetZzJpT0EwDoU,6315
 keras_cv/layers/preprocessing/solarization_test.py,sha256=Ef4RgDgxYhrl7vOxlVjc3cbp1NF2-O74eiQyDHTzJ_c,3152
 keras_cv/layers/preprocessing/vectorized_base_image_augmentation_layer.py,sha256=x7ECy-QlzzZFeZ80CDp9-_3A2hKn6iqWSz1MwrhHlPg,19868
-keras_cv/layers/preprocessing/vectorized_base_image_augmentation_layer_test.py,sha256=9qnDF7HCCuUA6V1MAixPI1rFrXcia0tcdbAzr_NLUio,20975
-keras_cv/layers/preprocessing/with_labels_test.py,sha256=Ud8yOAZfLhJZjWiVAGdCIpeZ4q6u1F8uL5BrkMq2SjM,5015
-keras_cv/layers/preprocessing/with_mixed_precision_test.py,sha256=jyBM56EamQMRRyYTG4pYKcdU_T1BDtc-d3swFTATTAw,5473
-keras_cv/layers/preprocessing/with_segmentation_masks_test.py,sha256=K_qHjT4UjtLpa4QB1RAGLYiaQ33BKIgdWWYI17dDIw0,4812
-keras_cv/layers/preprocessing_3d/__init__.py,sha256=mLC9cXKiViLfAooARXo4kaQd2_e-nNgtemC3PMX3A_c,1769
-keras_cv/layers/preprocessing_3d/base_augmentation_layer_3d.py,sha256=DufaGhg6MZ8WQ5qm3q8fCNNGXZj5UCB6toBNjTBzdr8,7987
+keras_cv/layers/preprocessing/vectorized_base_image_augmentation_layer_test.py,sha256=LD1rg52LEsxEXlaUQh3fkii4DNywGr0EYbRqXuQoq9Q,20977
+keras_cv/layers/preprocessing/with_labels_test.py,sha256=nKTZKj40UhG28oV7z2rBBpIFgdGx97jwjSUgcR3SZjk,4773
+keras_cv/layers/preprocessing/with_mixed_precision_test.py,sha256=XYb4YQyP-rUR-zIT5enD7TaybbA-9EYK3vXQ7Ibp0vc,5231
+keras_cv/layers/preprocessing/with_segmentation_masks_test.py,sha256=xdaI5yqWo9zTTVT1rT31ZGjCmJlOMljc-1WA_PY96Z4,4885
+keras_cv/layers/preprocessing_3d/__init__.py,sha256=6_AQQLdMdzl63Ro_eEGOIeXe7Ilxjoezm5jaz1AOE7g,1904
+keras_cv/layers/preprocessing_3d/base_augmentation_layer_3d.py,sha256=4kp2UVUukA-PpE2BIxqnG6r5EIPVq6QSWK99i1bpGek,10608
 keras_cv/layers/preprocessing_3d/base_augmentation_layer_3d_test.py,sha256=XfLsnLcSPcBMIyyyaZO6aWiL0cKBcUsfLoUUI6fAAZU,4789
-keras_cv/layers/preprocessing_3d/frustum_random_dropping_points.py,sha256=PxFqNFQJmnsBAVm4_75FO9V1UbnaSJfqVRlxrK3CV1o,6272
-keras_cv/layers/preprocessing_3d/frustum_random_dropping_points_test.py,sha256=IQVnS3_3Za-Zr8kLeENmoZ8HuxkbtkpiINqg8IO5baI,5454
-keras_cv/layers/preprocessing_3d/frustum_random_point_feature_noise.py,sha256=CtepB7NmW9ZvvE3I-kB36_oD-nSgq7ScgBwFeiLefNU,6969
-keras_cv/layers/preprocessing_3d/frustum_random_point_feature_noise_test.py,sha256=iaNAoXuPZbQ1t9btj1BVhubeSnEu6rMYKz7s78nNQPw,8880
-keras_cv/layers/preprocessing_3d/global_random_dropping_points.py,sha256=jZnfwKVongkpNKdB5aIDvCm5Qk2zI9QZ-d2MZY9YZFU,3815
-keras_cv/layers/preprocessing_3d/global_random_dropping_points_test.py,sha256=ZPQSLp60wv9MCZ8aZo1N4vqPGGxdr1j6CpsulQ9XHiQ,5100
-keras_cv/layers/preprocessing_3d/global_random_flip.py,sha256=HGsp8ryh8gT5nbfyxhvHLKckk5AVdwuFN8W1AG7tqzU,4458
-keras_cv/layers/preprocessing_3d/global_random_flip_test.py,sha256=JVUubAZfV_stcVetRkc5HoZqYX7iG7hadIcBEj0Leag,3275
-keras_cv/layers/preprocessing_3d/global_random_rotation.py,sha256=L4vihhuyDC1j-ahvbOkdKtCTCxxfQWo8-Gys19KLbLo,6016
-keras_cv/layers/preprocessing_3d/global_random_rotation_test.py,sha256=QCA7ipaULvBg6G782d_luaEFvvTywEm6TVckN6J3r04,3158
-keras_cv/layers/preprocessing_3d/global_random_scaling.py,sha256=24TOJSNvawKSnEddvuHPoX97InpraJFKff8fADasXUY,7090
-keras_cv/layers/preprocessing_3d/global_random_scaling_test.py,sha256=AyoPd0L9AGA2cOfV6ZB0-yPtjCAERFsKTDTWWobZVCg,4610
-keras_cv/layers/preprocessing_3d/global_random_translation.py,sha256=kOMMkvlufo7o2p8VyIDWiRRDrhFx2zauSmxB-7F46Uc,4733
-keras_cv/layers/preprocessing_3d/global_random_translation_test.py,sha256=9w8wkJ6xAmm_oXVucVciREZPJ9CDWSDkcw6sQlcxYYU,2935
-keras_cv/layers/preprocessing_3d/group_points_by_bounding_boxes.py,sha256=d2Bm_noNfaxDBN6C4K-Dg0093ZacmOXoFgJAak4QAvA,11464
-keras_cv/layers/preprocessing_3d/group_points_by_bounding_boxes_test.py,sha256=g40goot1tHDIzrRxnIQGqG4PqaZL2PmAJlqrqZSLU74,7460
-keras_cv/layers/preprocessing_3d/random_copy_paste.py,sha256=xbg0vj7ssyKmutP9mJleifhxT2TAKUCRLbjpucQSRKM,12606
-keras_cv/layers/preprocessing_3d/random_copy_paste_test.py,sha256=z1_NuYcD0wVBSmiQJMfaDjRL9eJUzS3joml5COZeguc,8348
-keras_cv/layers/preprocessing_3d/random_drop_box.py,sha256=rKylWskYaztEYt-F8U9PSxbZPl8-JQP20YaJoVfn0_U,5418
-keras_cv/layers/preprocessing_3d/random_drop_box_test.py,sha256=YWaRfY0DXe8mDB2cwNxmCrdnhnI0WihPGxlW2nas80o,12338
-keras_cv/layers/preprocessing_3d/swap_background.py,sha256=fpNNBNtLFzYRhk9ODVfAkFwVVL71oaQrk-evHyLIEFw,7322
-keras_cv/layers/preprocessing_3d/swap_background_test.py,sha256=uCIkasmXCl8CWE3787e2tqwxll1AWRWwWS8zgvKhsEs,11087
+keras_cv/layers/preprocessing_3d/input_format_test.py,sha256=MO5-1Me2sSkKMaJoAeLfjgBH6-jnI3f8g1fwj_r5TLc,3623
+keras_cv/layers/preprocessing_3d/waymo/__init__.py,sha256=4vu_MIXcDp6eo2TBan5WGw5AlGt13XhBn2cIF3VrQGc,172
+keras_cv/layers/preprocessing_3d/waymo/frustum_random_dropping_points.py,sha256=KnDKn1UG8r4m5zA-7-_3NcUFiLLO7yCQ7P92V-7VbIE,5860
+keras_cv/layers/preprocessing_3d/waymo/frustum_random_dropping_points_test.py,sha256=IKGJEAZlKEgimrPTFR2ajYvaCgUCRrEjs42scB7R08g,5063
+keras_cv/layers/preprocessing_3d/waymo/frustum_random_point_feature_noise.py,sha256=bQq5rztZVMOpPxbSelvF_I_ZVj9QZSCCF7N9H4GqVDw,6556
+keras_cv/layers/preprocessing_3d/waymo/frustum_random_point_feature_noise_test.py,sha256=6unfiwpPXNusI5MaCNL096POORNS-aiM_LtlBFWK_RU,8474
+keras_cv/layers/preprocessing_3d/waymo/global_random_dropping_points.py,sha256=rCEKj0-GmFdjA1vIet5mPFG2J0rlAGL5S43u4zo7lsc,3403
+keras_cv/layers/preprocessing_3d/waymo/global_random_dropping_points_test.py,sha256=oaeGfK32XvScPs9xc0KAVgDwfJNtVtGyGIjurQ6lS9E,4709
+keras_cv/layers/preprocessing_3d/waymo/global_random_flip.py,sha256=ByFCzY7UdaCSOaBx1N8vH58nhINrtI__xGlQE0PVJM0,4046
+keras_cv/layers/preprocessing_3d/waymo/global_random_flip_test.py,sha256=Zm7cr1_ncoSxAHrrP0xkcmafggJFR9lRSrsXWNMK8dw,2879
+keras_cv/layers/preprocessing_3d/waymo/global_random_rotation.py,sha256=pGUVvH2RrtXnvlFhwxMalJxrvmWqKiAnu502SkRqvGA,5604
+keras_cv/layers/preprocessing_3d/waymo/global_random_rotation_test.py,sha256=Adp-Y-Eyk43kIhmPTzwsLFlNgU8v8oTqJhpGShXSVJg,2753
+keras_cv/layers/preprocessing_3d/waymo/global_random_scaling.py,sha256=avswNvmJ_OBvZothpZQ7XvPnNCjG4ewqC5GV2ewyL_Q,6678
+keras_cv/layers/preprocessing_3d/waymo/global_random_scaling_test.py,sha256=psBbBzBTzVI9WsI9XCrhUtRXRNi_wi1_t9mz3IUZ35A,4205
+keras_cv/layers/preprocessing_3d/waymo/global_random_translation.py,sha256=5_Vhg4sQ3shCyx8_6vZ6WlAxCvds1mjP1PMCT9RklhI,4321
+keras_cv/layers/preprocessing_3d/waymo/global_random_translation_test.py,sha256=-EzymV1caqDPK-2uBYPNhlfyi3pAR2Q4_XpuxOsdsgc,2530
+keras_cv/layers/preprocessing_3d/waymo/group_points_by_bounding_boxes.py,sha256=Goc5i4Ciqfn3Z-vqxYBcxNj0VExVvNlNmtdG0lmjoh4,11112
+keras_cv/layers/preprocessing_3d/waymo/group_points_by_bounding_boxes_test.py,sha256=6WpBTGDVaLTrBfBDtguaCTQ7cAa9NYGPzEavpzqqKyg,7069
+keras_cv/layers/preprocessing_3d/waymo/random_copy_paste.py,sha256=qllwuzm8EKCHIsFbbk0tRUJ8S2VrAisHa65Q0lh2iKc,12254
+keras_cv/layers/preprocessing_3d/waymo/random_copy_paste_test.py,sha256=Etzxo-hXFCf7exuO_M9zedOBbJdRPVHqeF4cqamlt3s,7952
+keras_cv/layers/preprocessing_3d/waymo/random_drop_box.py,sha256=V5pUUw3_94xWOc_-qGDslnis2gP22gHpqpl5SMPDmV0,5006
+keras_cv/layers/preprocessing_3d/waymo/random_drop_box_test.py,sha256=raqlrFbxHdryTMgV68tPpnXQKeO5EuNgN_y2I1jOrGE,11932
+keras_cv/layers/preprocessing_3d/waymo/swap_background.py,sha256=K8DUmGQ2J4RQwXld46ES_6A6Wk7lgYS8zGeoj0cFXqU,6970
+keras_cv/layers/preprocessing_3d/waymo/swap_background_test.py,sha256=svSIWtMDvu0kjeCfVdeJNz5mHP-DOOLv8j3uTc3bgK4,10691
 keras_cv/layers/regularization/__init__.py,sha256=puQMT3krCCm0-Z6_1N296zpAZGe5FSIv-6ILRtjkG-I,868
 keras_cv/layers/regularization/drop_path.py,sha256=ev56dTYJHlchvq4i4rfTBjgjFpJGMD97_WC6igt-4kE,2545
 keras_cv/layers/regularization/drop_path_test.py,sha256=xD2WretcOvKTSZNWwUTyPXh1XKjCPcBoYiP7hHBbXEk,2460
 keras_cv/layers/regularization/dropblock_2d.py,sha256=Z4ExqwQTyjFFZu1pW6caIrvWU0MnyreIzQVvnJ6qMJs,8831
 keras_cv/layers/regularization/dropblock_2d_test.py,sha256=fGYKqszc-Hd-FHuH2YK6hwXgiGGux25aDBW8BaUnnbw,3653
 keras_cv/layers/regularization/squeeze_excite.py,sha256=1Zwhgk6mJ4Bx5cptlnVXzMOYuY9hW-UXHu3tCUBpsTo,4906
 keras_cv/layers/regularization/squeeze_excite_test.py,sha256=bK9QHsOJUe4ZIL0kaGCdjDbA9BP7LbnEkOcsIFhAzdM,1882
 keras_cv/layers/regularization/stochastic_depth.py,sha256=rRRFkhV9jPpKTLZhjUXaLMHvAePB5GsKxGnD1aiBMwE,2738
 keras_cv/layers/regularization/stochastic_depth_test.py,sha256=8ow6STkWtfuAUZGo7C1ExIFKyH32yhWDN0un9BtNXR4,1771
-keras_cv/losses/__init__.py,sha256=-hBSkcsNJDpcrCGPj8hJhLxZ4hqwU5T4SVArb9cla3E,989
+keras_cv/losses/__init__.py,sha256=1gzlHvlAbdbI_ZFRyZuaW7NalhfTDpUdA7jwT9hMWEc,1036
 keras_cv/losses/centernet_box_loss.py,sha256=fG4Gg5i_GYTI2p9NDcKJavdl9kdeBEtjpddYjotQWm8,4854
 keras_cv/losses/centernet_box_loss_test.py,sha256=ypeCas7EODJmq-AzJ5MDjSj3K5wY1wli3gejWIewbzw,1459
+keras_cv/losses/ciou_loss.py,sha256=VueR_eGStcSfgeHNEiUEXkl9VHppOTDm3woJJ3tQaM4,3548
+keras_cv/losses/ciou_loss_test.py,sha256=2Vmu66GQR_T6gOelRs6jHY5sHlibyLrJdxesQAIKZug,3042
 keras_cv/losses/focal.py,sha256=QMmhZ8bInDwOgFCrLHYnzfVX0lqr-IdmjMcDTvOQoxM,4140
 keras_cv/losses/focal_test.py,sha256=LUKm7_GCCFKNM3xDnFtRCN20aMHYJeX6JQPWoXQMlm4,2486
 keras_cv/losses/giou_loss.py,sha256=8vJ9wCwNXYOifbDeCNOhZ4LveizR_EReykqG08KAZZA,7410
 keras_cv/losses/giou_loss_test.py,sha256=YklDUDDqzqh1fjyqfgfrP1fBeiNK1ue_JIyVM6FOtos,2541
 keras_cv/losses/iou_loss.py,sha256=pUnEwz_QcVHTnTgDSvnQz9oNcLzoKBYnxh2190gGDNQ,4921
 keras_cv/losses/iou_loss_test.py,sha256=md0cjQUuSTvkuFgw4RAFOY2Ptqzdb_pcLtTrsv4d4B8,2521
 keras_cv/losses/penalty_reduced_focal_loss.py,sha256=tMfcAclWiPyEeiGd6ux2z1Idt3V-n3pSzvikiOua30k,4283
@@ -233,110 +235,121 @@
 keras_cv/losses/serialization_test.py,sha256=xn8e8IB0DN5z3Q1YUTqtfzqC_XSeDBxhpHsglqFpMaE,2211
 keras_cv/losses/simclr_loss.py,sha256=V369MolbCdw4cYDTUSOC2pJbWVGQlH6dTaAGagTBw78,3468
 keras_cv/losses/simclr_loss_test.py,sha256=theY3nyff-eBEmkUdL7Q_PA_78aCLeL8U-e5nypZGUQ,2145
 keras_cv/losses/smooth_l1.py,sha256=TM_J-KT0GJpTp4xCGY3GXuQxFnfatic-YQfFK0cHNu4,1885
 keras_cv/losses/smooth_l1_test.py,sha256=1a0AjxK3AXPD760xoPnqjlbvV6xTyWuE_Xtya0c_RhU,1225
 keras_cv/metrics/__init__.py,sha256=X8Cshzprx-J3MNKQUgGCP4STOucojE5oUGFtM-lKisY,663
 keras_cv/metrics/coco/__init__.py,sha256=7ZP_01BNrYxXkrI7kCTLsrsIxuG5lYjHwS-HzlPz0r4,719
-keras_cv/metrics/coco/pycoco_wrapper.py,sha256=vGeZ_DNWdvMuf6-0yJ6CJhNuCh7QfnFPivlCEeyTrt4,8388
+keras_cv/metrics/coco/pycoco_wrapper.py,sha256=UxBRxijNGSz7dM1xlt7u7VN9tSA3tp5hmezv2GHGSuc,8421
 keras_cv/metrics/object_detection/__init__.py,sha256=lTRkdYrWWKYepTfT5vmWF-0n517fEtcg1tPvLWVzyXU,584
-keras_cv/metrics/object_detection/box_coco_metrics.py,sha256=uwOkK8pItymx8UVfzzivPC4ucTSFUYqJQp2rSX4KImk,9843
-keras_cv/metrics/object_detection/box_coco_metrics_test.py,sha256=EPfi7oFW8td_blhOPyFZbabuaC2ZUNA5DkFbIYdRBYk,4779
-keras_cv/models/__init__.py,sha256=wKQaKi3G3YMDKhqCgRC97MLC8L6WDXO995GcuzkpDkc,4079
-keras_cv/models/task.py,sha256=zlFlwK7xS6qNEQnyHPv4eh430qxb3CXiScLb248wAcg,6387
-keras_cv/models/utils.py,sha256=bxmxfjGFVQM6aO-Fh9GjrORe6Ne1Tbv5aBGz1h-Ub-I,1093
+keras_cv/metrics/object_detection/box_coco_metrics.py,sha256=4p1nPEXNRorARYtcr4CTacSBOyo7Fc5B6O1XusGnj30,10152
+keras_cv/metrics/object_detection/box_coco_metrics_test.py,sha256=VkBnTqexNQp-B0ae8xx2K3VKpImmhUy03B31VtjMvCI,9331
+keras_cv/models/__init__.py,sha256=bvCoed7A_RVZuWDWo-B9xVqWXTa_Q-kV4oO3g9tiVI0,4472
+keras_cv/models/task.py,sha256=i1UyQKvaBHfT28A4-x4-Em8wT6bnZx-gtkcz538kABc,7295
+keras_cv/models/utils.py,sha256=EVf7sMYHJWJ4ETwuVlle42U2otaDabZ4RAFr4WBk9Fk,1855
 keras_cv/models/utils_test.py,sha256=ZXmz_ngH9DYrU-axCv3IYx7RCL-iwzC8Oc_uJ_Q_eQc,1133
 keras_cv/models/__internal__/__init__.py,sha256=hDBbodQWKeVBL86IY1jpd0BqfnzOp7jtjmghJqQOSqk,584
 keras_cv/models/__internal__/unet.py,sha256=6V7gRexsVAHdweVeI13zCz4NONheX_wUchaF6j73C3E,5966
 keras_cv/models/__internal__/unet_test.py,sha256=snnjgT9KzqeL1kpsyphVTg15JBw6oudgTvQYJKylrEE,1693
 keras_cv/models/backbones/__init__.py,sha256=lTRkdYrWWKYepTfT5vmWF-0n517fEtcg1tPvLWVzyXU,584
-keras_cv/models/backbones/backbone.py,sha256=9L_X4xHL8K1JAlf-W_mp62YJdr54fWJq57kXkyuz93E,6264
-keras_cv/models/backbones/backbone_presets.py,sha256=X6rXopPLnojShudOek_S9k6lxv7pUAlngXotrDZQOCk,1839
+keras_cv/models/backbones/backbone.py,sha256=JsLD8ryc_Zr00OjapRnUB-KZ6IGFkyuoc-s-SYUApns,6535
+keras_cv/models/backbones/backbone_presets.py,sha256=U2R3kTb3Fj5k2TbClRGizuUmsy8iUao0m6iaSKuwUHo,2236
+keras_cv/models/backbones/test_backbone_presets.py,sha256=fe5tweiyqijqo8-XOaA6723IZK9mEfP_ixEYeFDtYZA,821
 keras_cv/models/backbones/csp_darknet/__init__.py,sha256=lTRkdYrWWKYepTfT5vmWF-0n517fEtcg1tPvLWVzyXU,584
-keras_cv/models/backbones/csp_darknet/csp_darknet_backbone.py,sha256=6_8ht4jOygQU_Hi1PrwQXIiZzmqNhh54c3W0vZY1R7U,12000
+keras_cv/models/backbones/csp_darknet/csp_darknet_backbone.py,sha256=yf4uJL0AqzbUy5YmPNKJvTJpKFZ-6Vl0L-QWT_eXsh4,12495
 keras_cv/models/backbones/csp_darknet/csp_darknet_backbone_presets.py,sha256=FZrBXbztCR94xvAOJiAHck8uiwQBAZD9ULG7Dt70qQ8,6518
 keras_cv/models/backbones/csp_darknet/csp_darknet_backbone_presets_test.py,sha256=0uQolv9drvvg2vQncrG7rpxkwR27UpJ6K164AzH2lMo,4028
-keras_cv/models/backbones/csp_darknet/csp_darknet_backbone_test.py,sha256=QkzTRvM4MC7FAECiH5ZMBDNOoXaIanjs7aEXRR7gxCU,5552
+keras_cv/models/backbones/csp_darknet/csp_darknet_backbone_test.py,sha256=ODzZlZt4jFEbuISKE80vFtNj9MDz9dSkIyKwOzLyoZI,5644
 keras_cv/models/backbones/csp_darknet/csp_darknet_utils.py,sha256=XnoHxxNt8PMVim-DRNI5synXqYcTia-rG0pjCOIAFqo,12186
+keras_cv/models/backbones/densenet/__init__.py,sha256=lTRkdYrWWKYepTfT5vmWF-0n517fEtcg1tPvLWVzyXU,584
+keras_cv/models/backbones/densenet/densenet_aliases.py,sha256=Zwf-6Yb3rk5juCYBAMR-wh0oT-N0xFt6lzaM2jLQeK0,4818
+keras_cv/models/backbones/densenet/densenet_backbone.py,sha256=HdMIR00V3eWMd1Bw0RaZ89jVnfHwAiYdG27tOkrLRrs,7928
+keras_cv/models/backbones/densenet/densenet_backbone_presets.py,sha256=13Bw7Q-k5_IMS-_2MDuXop5o7A3ED_lpEL7e-ZeemUs,3952
+keras_cv/models/backbones/densenet/densenet_backbone_presets_test.py,sha256=URYk-s9faxmBCicXjx65xP8QLSyeSOPvD79cXGspXyI,3681
+keras_cv/models/backbones/densenet/densenet_backbone_test.py,sha256=m-VlHJi8Fc0wWHyprC3ypIO6JANS_mB4OwpQHsu2CBQ,4853
 keras_cv/models/backbones/efficientnet_v2/__init__.py,sha256=lTRkdYrWWKYepTfT5vmWF-0n517fEtcg1tPvLWVzyXU,584
 keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_aliases.py,sha256=y1WusTXHwRAAeqVFUzFG9JzaJjSiDxQNl_HDZshjU9g,9316
-keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone.py,sha256=m5Z8CyHr5yoZ5Uq_n8eln0tUc0EIrQ9qNAQc7wLS_DU,12981
+keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone.py,sha256=B5lxQaPzYDkRL88V3zXHZOLXgEnaedWZLxqK6xWz3RE,12987
 keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_presets.py,sha256=ycIWMYZa47gfzCh_Y3bEE4K7Sx0xyI6WRgGkRp2hPlw,19514
-keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_presets_test.py,sha256=LeHz66FhoocjX5Z_QeqOMIMqCVuGz4fhEvp6EyKP3tY,2287
-keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_test.py,sha256=rpVbuUH0RKKZIt4n_sb0BlxAEjJCQGjK_bK2vysRYFs,8546
+keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_presets_test.py,sha256=xFa4fTwqQYLAfXg8iHyHMcwt1ZfNOPHlwm-c1AdrdSc,2299
+keras_cv/models/backbones/efficientnet_v2/efficientnet_v2_backbone_test.py,sha256=G3W8SuZdCGHDO31oKX7molTFArdOoIdyKphFlh98ahs,7678
 keras_cv/models/backbones/mobilenet_v3/__init__.py,sha256=lTRkdYrWWKYepTfT5vmWF-0n517fEtcg1tPvLWVzyXU,584
-keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone.py,sha256=5ipz58pvHco8j2FfYBUZFhSSxQD-drF2RSqzpfy7QJw,15819
+keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_aliases.py,sha256=uKF4QOdvtsIVjLXaYmTZyRzWF85jyJgTC5vArtUvrD4,3931
+keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone.py,sha256=4aKYUG8Jkda_MPsbOGehtu95xHal-ddoveEo_-eNbzw,12382
 keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_presets.py,sha256=JuGNiVNnaAO-a6W6PRKOgGjtcCL_nQ--DW6D_beX9aw,6080
-keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_presets_test.py,sha256=b0xR8W5_lv2dvrfNYyAapkB1_pcsWd82isUz3bN8zLM,1418
-keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_test.py,sha256=K7VMUD1dMSG9i3uyRXi-INxpujOK0hHAuYZFVeVGF_E,3266
+keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_presets_test.py,sha256=joVC_BqMT1gedLevcC18_MlPpKWh3_VvkuosV6mYguM,2580
+keras_cv/models/backbones/mobilenet_v3/mobilenet_v3_backbone_test.py,sha256=MlNbqA-3tOKI5AyvOF3HLPRy3Drtqnv6hdPggN7NitA,3830
 keras_cv/models/backbones/resnet_v1/__init__.py,sha256=lTRkdYrWWKYepTfT5vmWF-0n517fEtcg1tPvLWVzyXU,584
-keras_cv/models/backbones/resnet_v1/resnet_v1_backbone.py,sha256=PPAnblqvDnygkkVm5c0htdBgIROq_VSYhN4RBEa29Zo,17276
+keras_cv/models/backbones/resnet_v1/resnet_v1_aliases.py,sha256=OTQ8L4lY9SYGAWAguJ1iPTCsJWP1l1dqxyOqpoGnsFk,6506
+keras_cv/models/backbones/resnet_v1/resnet_v1_backbone.py,sha256=A8WO1j70Th3DvrsPe55VMId9ovlRkx9ZECcxKnmFSt4,11614
 keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_presets.py,sha256=uOyWXpElHcRTx81YgsHTyBY0s0jhS5VsJgT5eUYrUUU,5551
-keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_presets_test.py,sha256=PE1bcOh6AZUT5y0J4SO-ps3n1NIzOP2kzlS5WLVkUAE,3546
-keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_test.py,sha256=f9MViEhHeVkGi4eMdmhEulXAssfjcSTfoG7oZwY86-w,6283
+keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_presets_test.py,sha256=PJpWg44TkYC_x3OVG-jJX2uhmiaWYfgmlpghCkTnfpM,3550
+keras_cv/models/backbones/resnet_v1/resnet_v1_backbone_test.py,sha256=vm5nLcAKBzRXJd0t0LofNTlSyCIKJnThTviRGFHtXns,5815
 keras_cv/models/backbones/resnet_v2/__init__.py,sha256=lTRkdYrWWKYepTfT5vmWF-0n517fEtcg1tPvLWVzyXU,584
-keras_cv/models/backbones/resnet_v2/resnet_v2_backbone.py,sha256=37HW8D8oDRUNxYx5lT-AFgOiSkcer909ZlW-51qUvD8,18820
+keras_cv/models/backbones/resnet_v2/resnet_v2_aliases.py,sha256=Mr1qd1Kua7Zg4zm1khifLL2eZSWWaRO9q4eb9tWqPo4,6666
+keras_cv/models/backbones/resnet_v2/resnet_v2_backbone.py,sha256=hAxgCEvYHmjVJu3kRyKgF4w8MMDJCu8hZa9WbIlaLz4,13017
 keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_presets.py,sha256=EdRbAgMOKA2d7XtfDSjSNhrML2GSUoD86PH0_ExVTKI,5617
-keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_presets_test.py,sha256=BAEXQWNTa3pDAGDcLKlG4GALB6kSktiuDbc4sDx1vnE,3724
-keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_test.py,sha256=FhKMy-ZNbGP_z-bVzgHJsmKE9iIWpZ32qe069LKoBiI,5693
+keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_presets_test.py,sha256=4CmM1ukbWtuLmIi5VmV52H28L5gedmlIsJM7qQxwg_Y,3723
+keras_cv/models/backbones/resnet_v2/resnet_v2_backbone_test.py,sha256=-6yujuia0APYNrT9KW9lPcvV1KeOTmiqXyl_2yH2Abw,5226
 keras_cv/models/classification/__init__.py,sha256=lTRkdYrWWKYepTfT5vmWF-0n517fEtcg1tPvLWVzyXU,584
 keras_cv/models/classification/image_classifier.py,sha256=5gD2fAYb1HvIg039DXXD0sxbBljLiW3uApqCBRu_3qQ,4645
 keras_cv/models/classification/image_classifier_presets.py,sha256=FDv2YJGhr0JXHCCNJV9ZSb5OzrJqwF3rPZg_uA_myII,8454
-keras_cv/models/classification/image_classifier_test.py,sha256=kBh-jP4ZSKIbIcAUlrMYoRuyXSQlzN5uVxE3J9ShaEE,7098
-keras_cv/models/legacy/__init__.py,sha256=rk6LqZiXqzFqhIPLmflZWDKcktnVID6kZyfqZyw-MHw,4514
+keras_cv/models/classification/image_classifier_test.py,sha256=_Giogfe-bLrdAIZelY1-eXcEOcAFe9cwaRkN531166o,7220
+keras_cv/models/legacy/__init__.py,sha256=1ST822kpTlv-yJrfnHv6X_REqFAKQ8AiC-36Pow7kE8,4346
 keras_cv/models/legacy/convmixer.py,sha256=zF65eULXe1Jy2-iWsunOSX_lSDSwZMfqaxXw1B27ogY,14440
 keras_cv/models/legacy/convmixer_test.py,sha256=MCnQ2eEDwoSoESr50kUeJJZPosSM6CNRnx5QOoQnWXI,1835
 keras_cv/models/legacy/convnext.py,sha256=n1hJWSkLatMtciMQDCX5FG0eQpaykDMwvLErzAJageU,20264
 keras_cv/models/legacy/convnext_test.py,sha256=rBr9ka92Vd75jTkkYMCtyXvDkuFfitv9ItA3iU_L-mA,2456
 keras_cv/models/legacy/darknet.py,sha256=vo3TIrrp29sLJbyk0Vz_buCu6iu-Oa_JIbrIcq0cNiw,11050
 keras_cv/models/legacy/darknet_test.py,sha256=7QkF6BeAUTnEe1_pxmP6TwapjXKHKaGZqWIx27zCLK8,1823
-keras_cv/models/legacy/densenet.py,sha256=SCcWz8nGD6E_MjYzMf4Hg5xt4OEYxro7yF3t5A6Tx2Y,13432
-keras_cv/models/legacy/densenet_test.py,sha256=2LubeApvKim80ToW11xg9-LgHgQh-En90GAfWWcsWUY,2001
-keras_cv/models/legacy/efficientnet_lite.py,sha256=THvk6uVdDAncvFBy5sFixRCv4FlYuY-IMrHRmsyIEJA,22966
+keras_cv/models/legacy/efficientnet_lite.py,sha256=V0rj857y8tot-IF-0BZpuLmd7bWJgTHYj7ri7VRDSJo,22320
 keras_cv/models/legacy/efficientnet_lite_test.py,sha256=c1dk95sxGp73etRBFqYmk0qWzNFlo5Bijpr0874B5Jk,2173
 keras_cv/models/legacy/efficientnet_v1.py,sha256=vdRTVKLp0H2Pu_71HdhAdmT_JPFVMhza1ZtDjNCigl8,29352
 keras_cv/models/legacy/efficientnet_v1_test.py,sha256=6JmMd-_YcBhhanmqyZC4XH8zaibodPRlyfCgIGMuVQY,2265
 keras_cv/models/legacy/mlp_mixer.py,sha256=Pyp7WOxdR_9gAlkDf2FTjkihCsvJ8bMp0vU4qvbViWk,14405
 keras_cv/models/legacy/mlp_mixer_test.py,sha256=MZ7Z1ywObeloCxxSJF3yAceRWjtdPoF4molEqRMEe4w,2050
-keras_cv/models/legacy/models_test.py,sha256=-Ddf1-7SpXjdrGHORQXrRGrUq8r4nzLEHqntR-gQr40,6541
+keras_cv/models/legacy/models_test.py,sha256=e_9YRqtl1xOZewYoLXC5EG3pLLjWrOLetzcCT5ulEOg,6602
 keras_cv/models/legacy/regnet.py,sha256=wUaUYRp9n3M5J1AN7q7Ex9Bfw5D0HnoErw9ALYUG-dQ,45767
 keras_cv/models/legacy/regnetx_test.py,sha256=MENWVbPqTLY8IYUncRm3JjGAvn5MxYzPH-6TmbtNgAM,2265
 keras_cv/models/legacy/regnety_test.py,sha256=L8DKBqOtZuaTBeYGl9A3zUVE0yFnz7MiOORAyxK4dJQ,2265
 keras_cv/models/legacy/utils.py,sha256=MbjgT50Vuu1BCXxCnnZ5csFKB1F8N5DAJfQLAVDalXI,3662
 keras_cv/models/legacy/utils_test.py,sha256=-7Y1EQrxsi58gDXk_8w9q0plJWaoPs3j4AQlPXtdRqU,2320
 keras_cv/models/legacy/vgg16.py,sha256=Z-zUhy-sG1U-e6hMLD0UIFXlwrcPpKy79FWbXizuOuc,8144
 keras_cv/models/legacy/vgg16_test.py,sha256=wLZVSnrS_SiZbfWC_rUc4B-7lHXadIIVCSPE2b-AFxg,1779
 keras_cv/models/legacy/vgg19.py,sha256=qf87g78qYqAayPYR1DAYmy50ZsexBsI7OUQhXuktgF0,7103
-keras_cv/models/legacy/vgg19_test.py,sha256=F5hhA6I4PTWfOduDD7uoCaBD3FGug_6g6u_NnD_aFLc,1779
+keras_cv/models/legacy/vgg19_test.py,sha256=ssWaUd6cTXuk9E0KIDd-211l_LP5tKGQROh8SHU3lvc,1892
 keras_cv/models/legacy/vit.py,sha256=cRFLOH-s5qcTD7qikyOqq_HpgGqF476_qFIop8HnTe4,26159
 keras_cv/models/legacy/vit_test.py,sha256=PxknM25tlUyhDujfC3UuHR1hL55zf9OyIKQAbjBO7Tc,2410
-keras_cv/models/legacy/weights.py,sha256=G1i7XgRp2Y3kLOIEhfSAsL4F7eaqcKz-ms-FeXJX6to,8729
-keras_cv/models/legacy/segmentation/__init__.py,sha256=QGl4z6VIl5fUvjTceVVNuvYtvNiAcb-cVNehtmbbXjo,651
+keras_cv/models/legacy/weights.py,sha256=K7dt1GnBqp8QPYIwDpVwW4pYjo-9nm6UmiXzuapQMAA,8708
+keras_cv/models/legacy/object_detection/__init__.py,sha256=hDBbodQWKeVBL86IY1jpd0BqfnzOp7jtjmghJqQOSqk,584
+keras_cv/models/legacy/object_detection/faster_rcnn/__init__.py,sha256=lTRkdYrWWKYepTfT5vmWF-0n517fEtcg1tPvLWVzyXU,584
+keras_cv/models/legacy/object_detection/faster_rcnn/faster_rcnn.py,sha256=QW4vJw65ntFJzI3-rbz6p4DBBDcqyT_JALfmAwbvHog,23865
+keras_cv/models/legacy/object_detection/faster_rcnn/faster_rcnn_test.py,sha256=ngoVKO5r__PijXbKyhlrPkpjL7idBKLkkI9ne6T2bc4,3914
+keras_cv/models/legacy/segmentation/__init__.py,sha256=hDBbodQWKeVBL86IY1jpd0BqfnzOp7jtjmghJqQOSqk,584
 keras_cv/models/legacy/segmentation/deeplab.py,sha256=fXM6hHVSmtMhQr0bQH19OnaUDkJaanNDW3vHa0Nu-GM,12412
-keras_cv/models/legacy/segmentation/deeplab_test.py,sha256=3CCny5u-YOTj5cmwfMJCM9td22NzIT2Viut2cr7Hk3Y,5955
+keras_cv/models/legacy/segmentation/deeplab_test.py,sha256=mIgZ_08UdfjlxLcm61tJzZe2TgN2zYI5qmfRdfVz3iE,5686
 keras_cv/models/object_detection/__init__.py,sha256=hDBbodQWKeVBL86IY1jpd0BqfnzOp7jtjmghJqQOSqk,584
 keras_cv/models/object_detection/__internal__.py,sha256=cjd5ZmGvv4OZ7BYySc2cvOFen2drOJgDNLPO2_edW1o,4329
 keras_cv/models/object_detection/__test_utils__.py,sha256=Eo0Vb9x12_33rXHlus3x6h7FLIhutbicZzp_X1RGQlQ,1904
-keras_cv/models/object_detection/predict_utils.py,sha256=HY5yshO7QaYDj2i-4EHHnnHc-Iq7tpe85Ez3SDD9o10,3490
+keras_cv/models/object_detection/predict_utils.py,sha256=KOhXJW13_cP4KLP01fKJcUZh_wHTnRd67LoGR6S4OQw,3694
 keras_cv/models/object_detection/retinanet/__init__.py,sha256=IwrDTskFA_9eIuGebwJ4-0_B8KanlWA_PRyi3Q-2M58,898
-keras_cv/models/object_detection/retinanet/feature_pyramid.py,sha256=de3zyqbuDHzsrjJZV2RhBRQI15sugICNv7V7AjCyaI4,2501
+keras_cv/models/object_detection/retinanet/feature_pyramid.py,sha256=_wc9219mN25L8OpH8LJ7fleWSLEvCd7DQUB_7qPPFRU,2510
 keras_cv/models/object_detection/retinanet/prediction_head.py,sha256=-SACAmX69oyM6h4dqD5ySV-9-aKC84DhrvNFzbwHmXs,2841
-keras_cv/models/object_detection/retinanet/retinanet.py,sha256=kKfINrlrZBAGL4A5wuckpnkeXALSOIQndpmRiGRzIY4,23051
+keras_cv/models/object_detection/retinanet/retinanet.py,sha256=RVjc8MFD-8OX29yA-jomQVEo1IR0IOJJIm-LGsZx2XU,24260
 keras_cv/models/object_detection/retinanet/retinanet_label_encoder.py,sha256=1sB0cQRysPVi5vnnbseoBSmGgw_NNHG0Xd1hyYrYgtg,9759
 keras_cv/models/object_detection/retinanet/retinanet_label_encoder_test.py,sha256=Vnr-OqPPphll3duyKmuhKYueSlgfDm2qBINzWw3OkaI,4594
-keras_cv/models/object_detection/retinanet/retinanet_presets.py,sha256=0o93_QiM3Llyn1fkbpRMYIut9GsNKRw2YO2h9vHTWcQ,1634
-keras_cv/models/object_detection/retinanet/retinanet_test.py,sha256=2YmmMIMp_opViDqVobfo87q7FWFqFAVvRoGIPwclasA,7822
+keras_cv/models/object_detection/retinanet/retinanet_presets.py,sha256=k1G33_DMT_CXQy-F4GrEWGDeo9sJhzivMmDoPVGNQN4,1672
+keras_cv/models/object_detection/retinanet/retinanet_test.py,sha256=_eqvoWZ0AfcnsEJIX3Sbl2EsINvyHXUFI4g0vM-EvPs,11212
 keras_cv/models/object_detection/yolo_v8/__init__.py,sha256=eUL-SD8orSL53eH7fPXWySgO2rHkJzkT3y8rL4llKtk,687
-keras_cv/models/object_detection/yolo_v8/yolo_v8_backbone.py,sha256=H6-jUCoTzmwvDHZmi-JDf9iYYCkbB_PAn-gs7Kw9KZU,7018
+keras_cv/models/object_detection/yolo_v8/yolo_v8_backbone.py,sha256=z2llJGcJSi7sN8MpWjNuFwa3PZyVoeSULJvOEiD6JS8,7027
 keras_cv/models/object_detection/yolo_v8/yolo_v8_backbone_presets.py,sha256=LIXCwGg4xJHdibMFXB2m_KPQZLG7xbypFzZk1XfIsUc,6577
-keras_cv/models/object_detection/yolo_v8/yolo_v8_detector.py,sha256=SJOWcaBIXIWszDln2tr-bEnYlrXkBat_E0tlEPG_gS4,22502
-keras_cv/models/object_detection/yolo_v8/yolo_v8_detector_presets.py,sha256=JTXcX7vZ3WJuTFVtVxN5V1TPJ_wR10EBSRFUeMqTprQ,1595
-keras_cv/models/object_detection/yolo_v8/yolo_v8_detector_test.py,sha256=aDM_zTRdzukNZBTvSreKazUJpfV0Cz51en0dljTBUPI,4708
-keras_cv/models/object_detection/yolo_v8/yolo_v8_iou_loss.py,sha256=_7TEgmiHm00KxuoBPSSdp2rkzWE6kicXRU8QPXIe5PE,2090
-keras_cv/models/object_detection/yolo_v8/yolo_v8_label_encoder.py,sha256=w3iauBbrc0CsbMHt2XCQr4mJbD6ncdt9Pq6XTiJZpYo,14891
+keras_cv/models/object_detection/yolo_v8/yolo_v8_detector.py,sha256=JGRKLjRnkInmHjXiEpBlStAdzefv0j-NPhlR56-b8-0,23732
+keras_cv/models/object_detection/yolo_v8/yolo_v8_detector_presets.py,sha256=Zqgwo0OkNWD2QyP2RItss733MvPXpXfteKMKQmGjgPM,1598
+keras_cv/models/object_detection/yolo_v8/yolo_v8_detector_test.py,sha256=a3PBLijH3NUBEJrO1tGL84HiAckEpIuorJbx0Jx3w0o,8675
+keras_cv/models/object_detection/yolo_v8/yolo_v8_label_encoder.py,sha256=xQc73HuHaOK84_Z9l-518uCemcTiS6in6ackDoxRNps,16358
 keras_cv/models/object_detection/yolo_v8/yolo_v8_layers.py,sha256=DHmCqzwBwYi_lWuqLn8FUwGMNsXQ1I03pbzOucE3ktY,2884
 keras_cv/models/object_detection/yolox/__init__.py,sha256=lTRkdYrWWKYepTfT5vmWF-0n517fEtcg1tPvLWVzyXU,584
 keras_cv/models/object_detection/yolox/binary_crossentropy.py,sha256=HcGHiKXtAXufxvXy7FR6wOKdmXx0GODyH74xtUAJrzg,3419
 keras_cv/models/object_detection/yolox/layers/__init__.py,sha256=SRs1LNrQrUKFqBMxfq2syl2gdClaqXZdHEDJ7f35L2k,954
 keras_cv/models/object_detection/yolox/layers/yolox_decoder.py,sha256=Qvlf1j8jBYpL9iZmK_6tWKgKbR1v0dmhqWmLdxH8eOg,6313
 keras_cv/models/object_detection/yolox/layers/yolox_head.py,sha256=mHO6fiwqQ-4ZZlUA2_uPnBeLFPLGk5nr-iKl8TNtprE,5423
 keras_cv/models/object_detection/yolox/layers/yolox_head_test.py,sha256=6R9-KhIpJ7skYmmjz41vsNMKVokKAg9FrBN8oflUXts,1859
@@ -349,52 +362,53 @@
 keras_cv/models/object_detection_3d/center_pillar_test.py,sha256=70X28vklYDrir7sEHKvC1_ZitnUHmiccHUyUNPJUYE4,5577
 keras_cv/models/stable_diffusion/__init__.py,sha256=_8wBV8X3b3r40BjkJXJqaYSU7JfclxRz8mVo81507uQ,1324
 keras_cv/models/stable_diffusion/clip_tokenizer.py,sha256=v-V0BQzodwjL-Akfhnc0-ywzbA6wrESKEuejwkSld7E,7025
 keras_cv/models/stable_diffusion/constants.py,sha256=AcJgrYp5XM_kckkSxEtVHfL9_a2LWxySq-08DOWZnyM,17410
 keras_cv/models/stable_diffusion/decoder.py,sha256=hUbi8IfQdjn4tBTv8sHH-mRcHHywcCr8SSeaU2pF_4g,2690
 keras_cv/models/stable_diffusion/diffusion_model.py,sha256=5dW0HHFj8oPj0k4erJzNYl1wd01oBlVJ_wE_eOSZZEA,13271
 keras_cv/models/stable_diffusion/image_encoder.py,sha256=NVC2v-ClmpSoeZQWG7870nQmiAyGycoEYzYhmoN0z6Q,2757
-keras_cv/models/stable_diffusion/noise_scheduler.py,sha256=3kjdFSIuRa2odHRlJJB9qOBaNOguHKkz0aIoCrN23_c,7770
-keras_cv/models/stable_diffusion/stable_diffusion.py,sha256=XQYP_QuPZNblo6Y-gq1jLL6G3Kob3kDbOJL7dPb8MZ4,19421
+keras_cv/models/stable_diffusion/noise_scheduler.py,sha256=gbLYcuRCvfZMlzk74yZIW_mf7QAxUxtU3vLYTBk0ze0,7771
+keras_cv/models/stable_diffusion/stable_diffusion.py,sha256=P9iwEOJr0rb_fbPq3Wqvv1jewLjYCS9XlZUSCBNtuB8,19440
 keras_cv/models/stable_diffusion/stable_diffusion_test.py,sha256=vwjDgMpJ14CmB-S1i10Oeh1THoELUNafMk2lp5CQAl8,2414
 keras_cv/models/stable_diffusion/text_encoder.py,sha256=yaXriTtB3vVeReZxZpNk5Y0xRD6JhEWbZkckgXxiWRM,6680
 keras_cv/models/stable_diffusion/__internal__/__init__.py,sha256=hDBbodQWKeVBL86IY1jpd0BqfnzOp7jtjmghJqQOSqk,584
 keras_cv/models/stable_diffusion/__internal__/layers/__init__.py,sha256=hDBbodQWKeVBL86IY1jpd0BqfnzOp7jtjmghJqQOSqk,584
 keras_cv/models/stable_diffusion/__internal__/layers/attention_block.py,sha256=sdDVdeS51RI1n5mNX4glafyHlaW2LkU5PgAVBmrVmgA,1948
 keras_cv/models/stable_diffusion/__internal__/layers/padded_conv2d.py,sha256=z3AO3IHvkhcfhU6om6729QqqIHhR6hwwVhek5s8jtcA,1005
 keras_cv/models/stable_diffusion/__internal__/layers/resnet_block.py,sha256=SOvE8_rMAHZIYrDxNRGTvfJa-W0mPljG5l7kCWF2t-o,1560
 keras_cv/ops/__init__.py,sha256=ygu6MeZEtohuDvOabajq4E-FHQ7xOI2O_3Z-3JjRwNs,624
 keras_cv/ops/iou_3d.py,sha256=Eshs7-xaC7iS6u0uE09_r8akjwE9-lXFOGI7VUxTcVk,1589
-keras_cv/ops/iou_3d_test.py,sha256=P36M1YrbNVpFJKB4dGUdgAHt79OZ4CeE7SIPBU7luuw,2242
+keras_cv/ops/iou_3d_test.py,sha256=1H6zonW9koQPDaZ9klpemzu3ZuqfL_5kuPaEBdOFX-A,2243
 keras_cv/point_cloud/__init__.py,sha256=u8QJ5UeFdH4x4XEk0sgG6zX0PRCPo1QlnsDESNCEm-Y,1522
 keras_cv/point_cloud/point_cloud.py,sha256=wTRpBWV010EEy_GwP-yib1YEQrqtdkRt5CYxcnVL28k,18327
 keras_cv/point_cloud/point_cloud_test.py,sha256=ygXBsoK5ADFZfow9c1mqtOFp86dbGRahOWn79g9sJKw,14038
 keras_cv/point_cloud/within_box_3d_test.py,sha256=vovw5NQR5AWQdmYf_g6E7hjRUoiwbUJzO0PRMQVe_WQ,7774
 keras_cv/training/__init__.py,sha256=zKTTo5nwGqrjrKUlSCvH5A64ypUlIVO_LqpTTqWKrgE,810
 keras_cv/training/contrastive/__init__.py,sha256=hDBbodQWKeVBL86IY1jpd0BqfnzOp7jtjmghJqQOSqk,584
-keras_cv/training/contrastive/contrastive_trainer.py,sha256=cJLddEMPwBM5-YizBG5PlIRQxCcYaebYbstro2RbBWM,9414
-keras_cv/training/contrastive/contrastive_trainer_test.py,sha256=v1JWSoF70AZl3_IQIfVSVcYDTSfpBx3NbYUMjvarCkA,6082
-keras_cv/training/contrastive/simclr_trainer.py,sha256=lZREjE7gqKfsplgbivqan4baLA6KTzblDujVGD3JcfQ,3197
-keras_cv/training/contrastive/simclr_trainer_test.py,sha256=uqsYPR2z29O6LqSdIf-mv8v5yFe0hEl1JsvFSjeeNWw,1609
+keras_cv/training/contrastive/contrastive_trainer.py,sha256=lhTTlQFJ7ArJQVJWOEipFd29ueLr0UwXb15eajDamQQ,9871
+keras_cv/training/contrastive/contrastive_trainer_test.py,sha256=0xsp9OcJ17Sg9ohVH56XcHa5sZdXsr_eT--vtqkdOK4,6332
+keras_cv/training/contrastive/simclr_trainer.py,sha256=H1WC5YqjlMF3yNnwRONkNWECTSWBOi-P63RxBWCmBAI,3199
+keras_cv/training/contrastive/simclr_trainer_test.py,sha256=yBFyhIs5APIXJIBfVMf3Yk2J8JJEdXm7Yc53rLWQzIw,1779
 keras_cv/utils/__init__.py,sha256=T29RuoUBnjaqMUw6d6_6PHf-SKQY0lvI7uOB7zyMw9Y,1408
 keras_cv/utils/conditional_imports.py,sha256=tm1Vn8ntsmw1eW2ewoZ-DRKTj2rTXipoHTamKrrI1pw,2080
 keras_cv/utils/conv_utils.py,sha256=bHkcdIJrC2XDByFQ6Lx52rKruXBJLkSq30erTSkeOko,2474
 keras_cv/utils/fill_utils.py,sha256=umSUsnPbqznINNhXjBRB-grPvdufXsbni8N4UuI_r8Y,3105
 keras_cv/utils/fill_utils_test.py,sha256=gJ8oq1yZIfBQyMpI61QnPKW4oujcJI19us_ghBVauHA,11265
 keras_cv/utils/preprocessing.py,sha256=yEXLCfwNnPlnnVD77hlmr3ZkZOtxlOIz3aAbz2pM21s,14465
 keras_cv/utils/preprocessing_test.py,sha256=eFL6dlg9kFJ2zuHYutvz8qyNy9Skiqr8ZpLZql_7LPs,2303
-keras_cv/utils/python_utils.py,sha256=d1ILxSZg7zQN5tu0OZnYg9wOfmat4P8A3DadX-1MRhc,1802
+keras_cv/utils/python_utils.py,sha256=s8B82a4Pm_JZS9cwBSxh4htiRuu-VN4-wVhOA3lVr10,1803
 keras_cv/utils/resource_loader.py,sha256=LYsQkuTV30JJ1lIRfOdlhBhP2MrswTNMJ5p5YAgL1ts,2843
 keras_cv/utils/target_gather.py,sha256=b9gN5J6Y8fA1iuYAY3RtuUnrb79XfFO8rLjJGbHDvsM,4731
 keras_cv/utils/target_gather_test.py,sha256=-Gp8CBl1maluIpJbW7IT6djyOlIY7V6vCW2FkSMkvDo,5346
 keras_cv/utils/test_utils.py,sha256=InZNiCk3HMZZ2PqTf1U_lThmXKYiPxcUhoYjxqxddAg,3628
 keras_cv/utils/to_numpy.py,sha256=Wa9tvz3RKEmIBdsaej5zzlADWkpVWSk_xL2lIAMOvMY,991
 keras_cv/utils/train.py,sha256=EsvRoaRvgeO-gaZVjQqTIBXCymOYC30MiaNg59y0G4Q,2813
-keras_cv/visualization/__init__.py,sha256=h4QC2aOrQu4TmHFV44FpJQlVWncQIG5wh3PDprsFGpM,754
+keras_cv/visualization/__init__.py,sha256=1R7vjZY2B7a0iPPNk94us8y-LVnGQPUtKe5Il_WYQfY,860
 keras_cv/visualization/draw_bounding_boxes.py,sha256=YqZGsARY1B47RRNsC75M50AUI0P-hCkCtNFGT5MLUH8,5497
 keras_cv/visualization/plot_bounding_box_gallery.py,sha256=yzuZVIBWLfhYqopbGPZjehPOaUYgrPdr-c2sxvl1jso,6120
-keras_cv/visualization/plot_image_gallery.py,sha256=Bbnrm1QvUCmUvnXXqbuOX9_ozOwrdBL1-fsIc5sEeEg,3945
-keras_cv-0.5.0.dist-info/LICENSE,sha256=2zOp5rCyz7FJvND3_eb184w-K3tdLuJy_3lhQ58OVn0,11412
-keras_cv-0.5.0.dist-info/METADATA,sha256=1Jz8LBcVJhmEMbPv2BfMh337cVyN04Xc-kRt8ww3luY,10792
-keras_cv-0.5.0.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-keras_cv-0.5.0.dist-info/top_level.txt,sha256=uKfJekc1tcN8PbI4ul-NL7P0z6JrS58MuciUKPPRKxY,9
-keras_cv-0.5.0.dist-info/RECORD,,
+keras_cv/visualization/plot_image_gallery.py,sha256=96k2GOZzhMwkDAaIb-ZDqnUryI3JhUeQi-NmMy5hzW0,5899
+keras_cv/visualization/plot_segmentation_mask_gallery.py,sha256=hVFZXFmRXCkQ978cFgoJTueNLMrSjvKHfDiqrybzduE,4718
+keras_cv-0.5.1.dist-info/LICENSE,sha256=66xNyvXuVtDPh1pQjz-nT4O11zccCbCa8aU56W9m2-Q,11853
+keras_cv-0.5.1.dist-info/METADATA,sha256=8A6kYXcJY1Y4m5uKXeX8zmtaUETMHXjXHPbKIDj_FxI,10791
+keras_cv-0.5.1.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+keras_cv-0.5.1.dist-info/top_level.txt,sha256=uKfJekc1tcN8PbI4ul-NL7P0z6JrS58MuciUKPPRKxY,9
+keras_cv-0.5.1.dist-info/RECORD,,
```

