# Comparing `tmp/azureml_automl_dnn_vision-1.52.0-py3-none-any.whl.zip` & `tmp/azureml_automl_dnn_vision-1.52.0.post1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,195 +1,195 @@
-Zip file size: 441751 bytes, number of entries: 193
--rw-rw-rw-  2.0 fat      299 b- defN 23-Jun-26 15:32 azureml/__init__.py
--rw-rw-rw-  2.0 fat      311 b- defN 23-Jun-26 15:32 azureml/automl/__init__.py
--rw-rw-rw-  2.0 fat      315 b- defN 23-Jun-26 15:32 azureml/automl/dnn/__init__.py
--rw-rw-rw-  2.0 fat      682 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/__init__.py
--rw-rw-rw-  2.0 fat      226 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/__init__.py
--rw-rw-rw-  2.0 fat    14072 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/runner.py
--rw-rw-rw-  2.0 fat      229 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/common/__init__.py
--rw-rw-rw-  2.0 fat    27028 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/common/classification_utils.py
--rw-rw-rw-  2.0 fat     7587 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/common/constants.py
--rw-rw-rw-  2.0 fat     2573 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/common/transforms.py
--rw-rw-rw-  2.0 fat      208 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/inference/__init__.py
--rw-rw-rw-  2.0 fat    23903 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/inference/score.py
--rw-rw-rw-  2.0 fat      210 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/io/__init__.py
--rw-rw-rw-  2.0 fat      398 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/io/read/__init__.py
--rw-rw-rw-  2.0 fat     4919 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/io/read/dataloader.py
--rw-rw-rw-  2.0 fat    22399 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/io/read/dataset_wrappers.py
--rw-rw-rw-  2.0 fat     5507 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/io/read/utils.py
--rw-rw-rw-  2.0 fat      237 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/io/write/__init__.py
--rw-rw-rw-  2.0 fat     2613 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/io/write/featurize_script.py
--rw-rw-rw-  2.0 fat     6433 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/io/write/score_script.py
--rw-rw-rw-  2.0 fat      973 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/io/write/score_script_utils.py
--rw-rw-rw-  2.0 fat      429 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/models/__init__.py
--rw-rw-rw-  2.0 fat    11491 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/models/base_model_wrapper.py
--rw-rw-rw-  2.0 fat    30503 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/models/classification_model_wrappers.py
--rw-rw-rw-  2.0 fat      221 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/trainer/__init__.py
--rw-rw-rw-  2.0 fat     1203 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/trainer/criterion.py
--rw-rw-rw-  2.0 fat    38271 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/classification/trainer/train.py
--rw-rw-rw-  2.0 fat    85645 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/NOTICE
--rw-rw-rw-  2.0 fat      263 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/__init__.py
--rw-rw-rw-  2.0 fat     1903 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/aml_dataset_base_wrapper.py
--rw-rw-rw-  2.0 fat    19557 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/artifacts_utils.py
--rw-rw-rw-  2.0 fat     2025 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/average_meter.py
--rw-rw-rw-  2.0 fat     2162 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/base_model_factory.py
--rw-rw-rw-  2.0 fat     1231 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/base_model_settings.py
--rw-rw-rw-  2.0 fat    22734 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/constants.py
--rw-rw-rw-  2.0 fat     2826 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/data_utils.py
--rw-rw-rw-  2.0 fat     3184 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/dataloaders.py
--rw-rw-rw-  2.0 fat    18976 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/dataset_helper.py
--rw-rw-rw-  2.0 fat    21108 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/distributed_utils.py
--rw-rw-rw-  2.0 fat      672 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/errors.py
--rw-rw-rw-  2.0 fat     2379 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/exceptions.py
--rw-rw-rw-  2.0 fat     2528 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/logging_utils.py
--rw-rw-rw-  2.0 fat    25039 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/model_export_utils.py
--rw-rw-rw-  2.0 fat    14730 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/parameters.py
--rw-rw-rw-  2.0 fat    10208 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/prediction_dataset.py
--rw-rw-rw-  2.0 fat    37122 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/pretrained_model_utilities.py
--rw-rw-rw-  2.0 fat     3200 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/sku_validation.py
--rw-rw-rw-  2.0 fat     9887 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/system_meter.py
--rw-rw-rw-  2.0 fat     5477 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/tiling_dataset_element.py
--rw-rw-rw-  2.0 fat     6399 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/tiling_utils.py
--rw-rw-rw-  2.0 fat     1039 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/torch_utils.py
--rw-rw-rw-  2.0 fat     2994 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/training_state.py
--rw-rw-rw-  2.0 fat    73199 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/utils.py
--rw-rw-rw-  2.0 fat      237 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/mlflow/__init__.py
--rw-rw-rw-  2.0 fat     8165 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/mlflow/mlflow_model_wrapper.py
--rw-rw-rw-  2.0 fat      272 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/trainer/__init__.py
--rw-rw-rw-  2.0 fat     7720 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/trainer/lrschedule.py
--rw-rw-rw-  2.0 fat     3336 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/trainer/lrschedule_parameters.py
--rw-rw-rw-  2.0 fat     9241 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/trainer/optimize.py
--rw-rw-rw-  2.0 fat     4337 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/common/trainer/optimize_parameters.py
--rw-rw-rw-  2.0 fat      229 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/explainability/__init__.py
--rw-rw-rw-  2.0 fat     2638 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/explainability/constants.py
--rw-rw-rw-  2.0 fat    16285 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/explainability/methods.py
--rw-rw-rw-  2.0 fat     8627 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/explainability/utils.py
--rw-rw-rw-  2.0 fat     6437 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/explainability/xrai_utils.py
--rw-rw-rw-  2.0 fat      321 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/metrics/__init__.py
--rw-rw-rw-  2.0 fat     6643 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/metrics/automl_classification_metrics.py
--rw-rw-rw-  2.0 fat     4480 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/metrics/classification_metrics.py
--rw-rw-rw-  2.0 fat      240 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/__init__.py
--rw-rw-rw-  2.0 fat    20822 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/runner.py
--rw-rw-rw-  2.0 fat      243 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/common/__init__.py
--rw-rw-rw-  2.0 fat     9636 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/common/augmentations.py
--rw-rw-rw-  2.0 fat     7356 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/common/boundingbox.py
--rw-rw-rw-  2.0 fat     3009 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/common/coco_eval_box_converter.py
--rw-rw-rw-  2.0 fat    11089 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/common/constants.py
--rw-rw-rw-  2.0 fat    11945 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/common/masktools.py
--rw-rw-rw-  2.0 fat    28085 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/common/object_detection_utils.py
--rw-rw-rw-  2.0 fat     1632 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/common/od_training_state.py
--rw-rw-rw-  2.0 fat     4891 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/common/parameters.py
--rw-rw-rw-  2.0 fat    26057 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/common/tiling_helper.py
--rw-rw-rw-  2.0 fat      238 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/data/__init__.py
--rw-rw-rw-  2.0 fat     4605 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/data/dataset_wrappers.py
--rw-rw-rw-  2.0 fat    40467 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/data/datasets.py
--rw-rw-rw-  2.0 fat     6110 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/data/loaders.py
--rw-rw-rw-  2.0 fat    12574 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/data/object_annotation.py
--rw-rw-rw-  2.0 fat     4985 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/data/tiling_distributed_sampler.py
--rw-rw-rw-  2.0 fat    11231 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/data/utils.py
--rw-rw-rw-  2.0 fat      240 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/eval/__init__.py
--rw-rw-rw-  2.0 fat     8380 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/eval/cocotools.py
--rw-rw-rw-  2.0 fat    22337 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/eval/incremental_voc_evaluator.py
--rw-rw-rw-  2.0 fat    19785 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/eval/metric_computation_utils.py
--rw-rw-rw-  2.0 fat    14609 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/eval/object_detection_instance_segmentation_evaluator.py
--rw-rw-rw-  2.0 fat     5220 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/eval/utils.py
--rw-rw-rw-  2.0 fat      239 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/models/__init__.py
--rw-rw-rw-  2.0 fat    14422 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/models/base_model_wrapper.py
--rw-rw-rw-  2.0 fat     5102 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/models/detection.py
--rw-rw-rw-  2.0 fat    14240 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/models/instance_segmentation_model_wrappers.py
--rw-rw-rw-  2.0 fat    28844 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/models/object_detection_model_wrappers.py
--rw-rw-rw-  2.0 fat      238 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/trainer/__init__.py
--rw-rw-rw-  2.0 fat     3651 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/trainer/criterion.py
--rw-rw-rw-  2.0 fat    25255 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/trainer/train.py
--rw-rw-rw-  2.0 fat      257 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/writers/__init__.py
--rw-rw-rw-  2.0 fat    15885 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/writers/score.py
--rw-rw-rw-  2.0 fat     5041 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/writers/score_script.py
--rw-rw-rw-  2.0 fat     1411 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection/writers/score_script_utils.py
--rw-rw-rw-  2.0 fat      241 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/__init__.py
--rw-rw-rw-  2.0 fat    14496 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/runner.py
--rw-rw-rw-  2.0 fat      243 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/common/__init__.py
--rw-rw-rw-  2.0 fat     7128 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/common/constants.py
--rw-rw-rw-  2.0 fat     1382 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/common/od_yolo_training_state.py
--rw-rw-rw-  2.0 fat      254 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/data/__init__.py
--rw-rw-rw-  2.0 fat    12848 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/data/datasets.py
--rw-rw-rw-  2.0 fat     9783 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/data/utils.py
--rw-rw-rw-  2.0 fat      245 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/eval/__init__.py
--rw-rw-rw-  2.0 fat     4430 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/eval/yolo_evaluator.py
--rw-rw-rw-  2.0 fat      239 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/models/__init__.py
--rw-rw-rw-  2.0 fat     6097 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/models/common.py
--rw-rw-rw-  2.0 fat     9858 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/models/yolo.py
--rw-rw-rw-  2.0 fat    11041 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/models/yolo_wrapper.py
--rw-rw-rw-  2.0 fat     1501 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/models/yolov5.3.0l.yaml
--rw-rw-rw-  2.0 fat     1503 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/models/yolov5.3.0m.yaml
--rw-rw-rw-  2.0 fat     1503 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/models/yolov5.3.0s.yaml
--rw-rw-rw-  2.0 fat     1503 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/models/yolov5.3.0x.yaml
--rw-rw-rw-  2.0 fat      238 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/trainer/__init__.py
--rw-rw-rw-  2.0 fat    28280 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/trainer/train.py
--rw-rw-rw-  2.0 fat      242 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/utils/__init__.py
--rw-rw-rw-  2.0 fat     3060 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/utils/ema.py
--rw-rw-rw-  2.0 fat     3799 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/utils/torch_utils.py
--rw-rw-rw-  2.0 fat    41566 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/utils/utils.py
--rw-rw-rw-  2.0 fat      257 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/writers/__init__.py
--rw-rw-rw-  2.0 fat    14388 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/writers/score.py
--rw-rw-rw-  2.0 fat     3292 b- defN 23-Jun-26 15:32 azureml/automl/dnn/vision/object_detection_yolo/writers/score_script.py
--rw-rw-rw-  2.0 fat      183 b- defN 23-Jun-26 15:32 tests/__init__.py
--rw-rw-rw-  2.0 fat      183 b- defN 23-Jun-26 15:32 tests/classification_tests/__init__.py
--rw-rw-rw-  2.0 fat      426 b- defN 23-Jun-26 15:32 tests/classification_tests/aml_dataset_mock.py
--rw-rw-rw-  2.0 fat      915 b- defN 23-Jun-26 15:32 tests/classification_tests/conftest.py
--rw-rw-rw-  2.0 fat    14353 b- defN 23-Jun-26 15:32 tests/classification_tests/test_classification_local_run.py
--rw-rw-rw-  2.0 fat     5320 b- defN 23-Jun-26 15:32 tests/classification_tests/test_classification_resume_local_run.py
--rw-rw-rw-  2.0 fat     3457 b- defN 23-Jun-26 15:32 tests/classification_tests/test_classification_trainer.py
--rw-rw-rw-  2.0 fat      906 b- defN 23-Jun-26 15:32 tests/classification_tests/test_classification_utils.py
--rw-rw-rw-  2.0 fat    11342 b- defN 23-Jun-26 15:32 tests/classification_tests/test_classification_xai_methods.py
--rw-rw-rw-  2.0 fat     2169 b- defN 23-Jun-26 15:32 tests/classification_tests/test_dataloaders.py
--rw-rw-rw-  2.0 fat    19599 b- defN 23-Jun-26 15:32 tests/classification_tests/test_dataset_wrappers.py
--rw-rw-rw-  2.0 fat    13726 b- defN 23-Jun-26 15:32 tests/classification_tests/test_inference_model_wrapper.py
--rw-rw-rw-  2.0 fat    19780 b- defN 23-Jun-26 15:32 tests/classification_tests/test_model_wrappers.py
--rw-rw-rw-  2.0 fat    11694 b- defN 23-Jun-26 15:32 tests/classification_tests/test_prediction_dataset.py
--rw-rw-rw-  2.0 fat     3067 b- defN 23-Jun-26 15:32 tests/classification_tests/test_pretrained_model_factory.py
--rw-rw-rw-  2.0 fat      183 b- defN 23-Jun-26 15:32 tests/common/__init__.py
--rw-rw-rw-  2.0 fat     1237 b- defN 23-Jun-26 15:32 tests/common/aml_dataset_mock.py
--rw-rw-rw-  2.0 fat     5177 b- defN 23-Jun-26 15:32 tests/common/run_mock.py
--rw-rw-rw-  2.0 fat    10438 b- defN 23-Jun-26 15:32 tests/common/test_aml_dataset_helper.py
--rw-rw-rw-  2.0 fat    16892 b- defN 23-Jun-26 15:32 tests/common/test_artifacts_utils.py
--rw-rw-rw-  2.0 fat    61650 b- defN 23-Jun-26 15:32 tests/common/test_common_methods.py
--rw-rw-rw-  2.0 fat    16927 b- defN 23-Jun-26 15:32 tests/common/test_distributed_utils.py
--rw-rw-rw-  2.0 fat     4295 b- defN 23-Jun-26 15:32 tests/common/test_mlflow_model_wrapper.py
--rw-rw-rw-  2.0 fat    10586 b- defN 23-Jun-26 15:32 tests/common/test_model_export_utils.py
--rw-rw-rw-  2.0 fat     6720 b- defN 23-Jun-26 15:32 tests/common/test_pretrained_model_utilities.py
--rw-rw-rw-  2.0 fat     6606 b- defN 23-Jun-26 15:32 tests/common/test_runner_default_args.py
--rw-rw-rw-  2.0 fat     5624 b- defN 23-Jun-26 15:32 tests/common/test_training_state.py
--rw-rw-rw-  2.0 fat     6315 b- defN 23-Jun-26 15:32 tests/common/utils.py
--rw-rw-rw-  2.0 fat      183 b- defN 23-Jun-26 15:32 tests/object_detection_tests/__init__.py
--rw-rw-rw-  2.0 fat      430 b- defN 23-Jun-26 15:32 tests/object_detection_tests/aml_dataset_mock.py
--rw-rw-rw-  2.0 fat      711 b- defN 23-Jun-26 15:32 tests/object_detection_tests/conftest.py
--rw-rw-rw-  2.0 fat     5755 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_augmentations.py
--rw-rw-rw-  2.0 fat     4323 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_coco_eval_box_converter.py
--rw-rw-rw-  2.0 fat     1604 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_cocotools.py
--rw-rw-rw-  2.0 fat     9815 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_dataset_wrappers.py
--rw-rw-rw-  2.0 fat    73835 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_datasets.py
--rw-rw-rw-  2.0 fat   100617 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_incremental_voc_evaluator.py
--rw-rw-rw-  2.0 fat     6193 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_maskutils.py
--rw-rw-rw-  2.0 fat    26704 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_metric_computation_utils.py
--rw-rw-rw-  2.0 fat    15744 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_model_wrappers.py
--rw-rw-rw-  2.0 fat    14068 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_object_annotation.py
--rw-rw-rw-  2.0 fat    20800 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_object_detection_instance_segmentation_evaluator.py
--rw-rw-rw-  2.0 fat     4296 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_object_detection_local_run.py
--rw-rw-rw-  2.0 fat    25852 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_object_detection_utils.py
--rw-rw-rw-  2.0 fat     2728 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_pretrained_model_factory.py
--rw-rw-rw-  2.0 fat     5550 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_scoring.py
--rw-rw-rw-  2.0 fat     5677 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_secondary_model_wrappers.py
--rw-rw-rw-  2.0 fat     5050 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_tiling_distributed_sampler.py
--rw-rw-rw-  2.0 fat    49608 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_tiling_helper.py
--rw-rw-rw-  2.0 fat     9531 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_tiling_utils.py
--rw-rw-rw-  2.0 fat     1280 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_trainer_criterion.py
--rw-rw-rw-  2.0 fat     4133 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_yolo_trainer_train.py
--rw-rw-rw-  2.0 fat     3710 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_yolo_utils.py
--rw-rw-rw-  2.0 fat      773 b- defN 23-Jun-26 15:32 tests/object_detection_tests/test_yolo_wrapper.py
--rw-rw-rw-  2.0 fat     1088 b- defN 23-Jun-26 15:32 tests/object_detection_tests/utils.py
--rw-rw-rw-  2.0 fat     1021 b- defN 23-Jun-26 15:39 azureml_automl_dnn_vision-1.52.0.dist-info/LICENSE.txt
--rw-rw-rw-  2.0 fat     1932 b- defN 23-Jun-26 15:39 azureml_automl_dnn_vision-1.52.0.dist-info/METADATA
--rw-rw-rw-  2.0 fat       97 b- defN 23-Jun-26 15:39 azureml_automl_dnn_vision-1.52.0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        1 b- defN 23-Jun-26 15:39 azureml_automl_dnn_vision-1.52.0.dist-info/namespace_packages.txt
--rw-rw-rw-  2.0 fat       14 b- defN 23-Jun-26 15:39 azureml_automl_dnn_vision-1.52.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat    21809 b- defN 23-Jun-26 15:39 azureml_automl_dnn_vision-1.52.0.dist-info/RECORD
-193 files, 1913185 bytes uncompressed, 405355 bytes compressed:  78.8%
+Zip file size: 437894 bytes, number of entries: 193
+-rw-rw-rw-  2.0 fat      299 b- defN 23-Jul-07 02:06 azureml/__init__.py
+-rw-rw-rw-  2.0 fat      311 b- defN 23-Jul-07 02:07 azureml/automl/__init__.py
+-rw-rw-rw-  2.0 fat      315 b- defN 23-Jul-07 02:08 azureml/automl/dnn/__init__.py
+-rw-rw-rw-  2.0 fat      682 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/__init__.py
+-rw-rw-rw-  2.0 fat      226 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/classification/__init__.py
+-rw-rw-rw-  2.0 fat    14072 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/classification/runner.py
+-rw-rw-rw-  2.0 fat      229 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/classification/common/__init__.py
+-rw-rw-rw-  2.0 fat    27028 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/classification/common/classification_utils.py
+-rw-rw-rw-  2.0 fat     7587 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/classification/common/constants.py
+-rw-rw-rw-  2.0 fat     2573 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/classification/common/transforms.py
+-rw-rw-rw-  2.0 fat      208 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/classification/inference/__init__.py
+-rw-rw-rw-  2.0 fat    23903 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/classification/inference/score.py
+-rw-rw-rw-  2.0 fat      210 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/classification/io/__init__.py
+-rw-rw-rw-  2.0 fat      398 b- defN 23-Jul-07 02:12 azureml/automl/dnn/vision/classification/io/read/__init__.py
+-rw-rw-rw-  2.0 fat     4919 b- defN 23-Jul-07 02:12 azureml/automl/dnn/vision/classification/io/read/dataloader.py
+-rw-rw-rw-  2.0 fat    22399 b- defN 23-Jul-07 02:12 azureml/automl/dnn/vision/classification/io/read/dataset_wrappers.py
+-rw-rw-rw-  2.0 fat     5507 b- defN 23-Jul-07 02:12 azureml/automl/dnn/vision/classification/io/read/utils.py
+-rw-rw-rw-  2.0 fat      237 b- defN 23-Jul-07 02:12 azureml/automl/dnn/vision/classification/io/write/__init__.py
+-rw-rw-rw-  2.0 fat     2613 b- defN 23-Jul-07 02:12 azureml/automl/dnn/vision/classification/io/write/featurize_script.py
+-rw-rw-rw-  2.0 fat     6433 b- defN 23-Jul-07 02:12 azureml/automl/dnn/vision/classification/io/write/score_script.py
+-rw-rw-rw-  2.0 fat      973 b- defN 23-Jul-07 02:12 azureml/automl/dnn/vision/classification/io/write/score_script_utils.py
+-rw-rw-rw-  2.0 fat      429 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/classification/models/__init__.py
+-rw-rw-rw-  2.0 fat    11491 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/classification/models/base_model_wrapper.py
+-rw-rw-rw-  2.0 fat    30503 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/classification/models/classification_model_wrappers.py
+-rw-rw-rw-  2.0 fat      221 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/classification/trainer/__init__.py
+-rw-rw-rw-  2.0 fat     1203 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/classification/trainer/criterion.py
+-rw-rw-rw-  2.0 fat    38271 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/classification/trainer/train.py
+-rw-rw-rw-  2.0 fat    85645 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/common/NOTICE
+-rw-rw-rw-  2.0 fat      263 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/common/__init__.py
+-rw-rw-rw-  2.0 fat     1903 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/common/aml_dataset_base_wrapper.py
+-rw-rw-rw-  2.0 fat    19557 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/common/artifacts_utils.py
+-rw-rw-rw-  2.0 fat     2025 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/common/average_meter.py
+-rw-rw-rw-  2.0 fat     2162 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/common/base_model_factory.py
+-rw-rw-rw-  2.0 fat     1231 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/common/base_model_settings.py
+-rw-rw-rw-  2.0 fat    22598 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/common/constants.py
+-rw-rw-rw-  2.0 fat     2826 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/common/data_utils.py
+-rw-rw-rw-  2.0 fat     3184 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/common/dataloaders.py
+-rw-rw-rw-  2.0 fat    18976 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/common/dataset_helper.py
+-rw-rw-rw-  2.0 fat    21108 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/common/distributed_utils.py
+-rw-rw-rw-  2.0 fat      672 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/common/errors.py
+-rw-rw-rw-  2.0 fat     2379 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/common/exceptions.py
+-rw-rw-rw-  2.0 fat     2528 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/common/logging_utils.py
+-rw-rw-rw-  2.0 fat    25039 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/common/model_export_utils.py
+-rw-rw-rw-  2.0 fat    14730 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/common/parameters.py
+-rw-rw-rw-  2.0 fat    10208 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/common/prediction_dataset.py
+-rw-rw-rw-  2.0 fat    37122 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/common/pretrained_model_utilities.py
+-rw-rw-rw-  2.0 fat     3200 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/common/sku_validation.py
+-rw-rw-rw-  2.0 fat     9887 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/common/system_meter.py
+-rw-rw-rw-  2.0 fat     5477 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/common/tiling_dataset_element.py
+-rw-rw-rw-  2.0 fat     6399 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/common/tiling_utils.py
+-rw-rw-rw-  2.0 fat     1039 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/common/torch_utils.py
+-rw-rw-rw-  2.0 fat     2994 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/common/training_state.py
+-rw-rw-rw-  2.0 fat    70662 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/common/utils.py
+-rw-rw-rw-  2.0 fat      237 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/common/mlflow/__init__.py
+-rw-rw-rw-  2.0 fat     8165 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/common/mlflow/mlflow_model_wrapper.py
+-rw-rw-rw-  2.0 fat      272 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/common/trainer/__init__.py
+-rw-rw-rw-  2.0 fat     7720 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/common/trainer/lrschedule.py
+-rw-rw-rw-  2.0 fat     3336 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/common/trainer/lrschedule_parameters.py
+-rw-rw-rw-  2.0 fat     9241 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/common/trainer/optimize.py
+-rw-rw-rw-  2.0 fat     4337 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/common/trainer/optimize_parameters.py
+-rw-rw-rw-  2.0 fat      229 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/explainability/__init__.py
+-rw-rw-rw-  2.0 fat     2638 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/explainability/constants.py
+-rw-rw-rw-  2.0 fat    16285 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/explainability/methods.py
+-rw-rw-rw-  2.0 fat     8627 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/explainability/utils.py
+-rw-rw-rw-  2.0 fat     6437 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/explainability/xrai_utils.py
+-rw-rw-rw-  2.0 fat      321 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/metrics/__init__.py
+-rw-rw-rw-  2.0 fat     6643 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/metrics/automl_classification_metrics.py
+-rw-rw-rw-  2.0 fat     4480 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/metrics/classification_metrics.py
+-rw-rw-rw-  2.0 fat      240 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/object_detection/__init__.py
+-rw-rw-rw-  2.0 fat    20822 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/object_detection/runner.py
+-rw-rw-rw-  2.0 fat      243 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection/common/__init__.py
+-rw-rw-rw-  2.0 fat     9636 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection/common/augmentations.py
+-rw-rw-rw-  2.0 fat     7356 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection/common/boundingbox.py
+-rw-rw-rw-  2.0 fat     3009 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection/common/coco_eval_box_converter.py
+-rw-rw-rw-  2.0 fat    11089 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection/common/constants.py
+-rw-rw-rw-  2.0 fat    11945 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection/common/masktools.py
+-rw-rw-rw-  2.0 fat    27385 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection/common/object_detection_utils.py
+-rw-rw-rw-  2.0 fat     1632 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection/common/od_training_state.py
+-rw-rw-rw-  2.0 fat     4891 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection/common/parameters.py
+-rw-rw-rw-  2.0 fat    26057 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection/common/tiling_helper.py
+-rw-rw-rw-  2.0 fat      238 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection/data/__init__.py
+-rw-rw-rw-  2.0 fat     4605 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection/data/dataset_wrappers.py
+-rw-rw-rw-  2.0 fat    40467 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection/data/datasets.py
+-rw-rw-rw-  2.0 fat     6110 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection/data/loaders.py
+-rw-rw-rw-  2.0 fat    12574 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection/data/object_annotation.py
+-rw-rw-rw-  2.0 fat     4985 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection/data/tiling_distributed_sampler.py
+-rw-rw-rw-  2.0 fat    11231 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection/data/utils.py
+-rw-rw-rw-  2.0 fat      240 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection/eval/__init__.py
+-rw-rw-rw-  2.0 fat     8380 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection/eval/cocotools.py
+-rw-rw-rw-  2.0 fat    21506 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection/eval/incremental_voc_evaluator.py
+-rw-rw-rw-  2.0 fat    17354 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection/eval/metric_computation_utils.py
+-rw-rw-rw-  2.0 fat    14609 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection/eval/object_detection_instance_segmentation_evaluator.py
+-rw-rw-rw-  2.0 fat     5220 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection/eval/utils.py
+-rw-rw-rw-  2.0 fat      239 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection/models/__init__.py
+-rw-rw-rw-  2.0 fat    14422 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection/models/base_model_wrapper.py
+-rw-rw-rw-  2.0 fat     5102 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection/models/detection.py
+-rw-rw-rw-  2.0 fat    14240 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection/models/instance_segmentation_model_wrappers.py
+-rw-rw-rw-  2.0 fat    28844 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection/models/object_detection_model_wrappers.py
+-rw-rw-rw-  2.0 fat      238 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection/trainer/__init__.py
+-rw-rw-rw-  2.0 fat     3651 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection/trainer/criterion.py
+-rw-rw-rw-  2.0 fat    25255 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection/trainer/train.py
+-rw-rw-rw-  2.0 fat      257 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection/writers/__init__.py
+-rw-rw-rw-  2.0 fat    15885 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection/writers/score.py
+-rw-rw-rw-  2.0 fat     5041 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection/writers/score_script.py
+-rw-rw-rw-  2.0 fat     1411 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection/writers/score_script_utils.py
+-rw-rw-rw-  2.0 fat      241 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/object_detection_yolo/__init__.py
+-rw-rw-rw-  2.0 fat    14496 b- defN 23-Jul-07 02:10 azureml/automl/dnn/vision/object_detection_yolo/runner.py
+-rw-rw-rw-  2.0 fat      243 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection_yolo/common/__init__.py
+-rw-rw-rw-  2.0 fat     7128 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection_yolo/common/constants.py
+-rw-rw-rw-  2.0 fat     1382 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection_yolo/common/od_yolo_training_state.py
+-rw-rw-rw-  2.0 fat      254 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection_yolo/data/__init__.py
+-rw-rw-rw-  2.0 fat    12848 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection_yolo/data/datasets.py
+-rw-rw-rw-  2.0 fat     9783 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection_yolo/data/utils.py
+-rw-rw-rw-  2.0 fat      245 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection_yolo/eval/__init__.py
+-rw-rw-rw-  2.0 fat     4430 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection_yolo/eval/yolo_evaluator.py
+-rw-rw-rw-  2.0 fat      239 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection_yolo/models/__init__.py
+-rw-rw-rw-  2.0 fat     6097 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection_yolo/models/common.py
+-rw-rw-rw-  2.0 fat     9858 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection_yolo/models/yolo.py
+-rw-rw-rw-  2.0 fat    11041 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection_yolo/models/yolo_wrapper.py
+-rw-rw-rw-  2.0 fat     1501 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection_yolo/models/yolov5.3.0l.yaml
+-rw-rw-rw-  2.0 fat     1503 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection_yolo/models/yolov5.3.0m.yaml
+-rw-rw-rw-  2.0 fat     1503 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection_yolo/models/yolov5.3.0s.yaml
+-rw-rw-rw-  2.0 fat     1503 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection_yolo/models/yolov5.3.0x.yaml
+-rw-rw-rw-  2.0 fat      238 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection_yolo/trainer/__init__.py
+-rw-rw-rw-  2.0 fat    28280 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection_yolo/trainer/train.py
+-rw-rw-rw-  2.0 fat      242 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection_yolo/utils/__init__.py
+-rw-rw-rw-  2.0 fat     3060 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection_yolo/utils/ema.py
+-rw-rw-rw-  2.0 fat     3799 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection_yolo/utils/torch_utils.py
+-rw-rw-rw-  2.0 fat    41566 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection_yolo/utils/utils.py
+-rw-rw-rw-  2.0 fat      257 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection_yolo/writers/__init__.py
+-rw-rw-rw-  2.0 fat    14388 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection_yolo/writers/score.py
+-rw-rw-rw-  2.0 fat     3292 b- defN 23-Jul-07 02:11 azureml/automl/dnn/vision/object_detection_yolo/writers/score_script.py
+-rw-rw-rw-  2.0 fat      183 b- defN 23-Jul-07 02:06 tests/__init__.py
+-rw-rw-rw-  2.0 fat      183 b- defN 23-Jul-07 02:07 tests/classification_tests/__init__.py
+-rw-rw-rw-  2.0 fat      426 b- defN 23-Jul-07 02:07 tests/classification_tests/aml_dataset_mock.py
+-rw-rw-rw-  2.0 fat      915 b- defN 23-Jul-07 02:07 tests/classification_tests/conftest.py
+-rw-rw-rw-  2.0 fat    14353 b- defN 23-Jul-07 02:07 tests/classification_tests/test_classification_local_run.py
+-rw-rw-rw-  2.0 fat     5320 b- defN 23-Jul-07 02:07 tests/classification_tests/test_classification_resume_local_run.py
+-rw-rw-rw-  2.0 fat     3457 b- defN 23-Jul-07 02:07 tests/classification_tests/test_classification_trainer.py
+-rw-rw-rw-  2.0 fat      906 b- defN 23-Jul-07 02:07 tests/classification_tests/test_classification_utils.py
+-rw-rw-rw-  2.0 fat    11342 b- defN 23-Jul-07 02:07 tests/classification_tests/test_classification_xai_methods.py
+-rw-rw-rw-  2.0 fat     2169 b- defN 23-Jul-07 02:07 tests/classification_tests/test_dataloaders.py
+-rw-rw-rw-  2.0 fat    19599 b- defN 23-Jul-07 02:07 tests/classification_tests/test_dataset_wrappers.py
+-rw-rw-rw-  2.0 fat    13726 b- defN 23-Jul-07 02:07 tests/classification_tests/test_inference_model_wrapper.py
+-rw-rw-rw-  2.0 fat    19780 b- defN 23-Jul-07 02:07 tests/classification_tests/test_model_wrappers.py
+-rw-rw-rw-  2.0 fat    11694 b- defN 23-Jul-07 02:07 tests/classification_tests/test_prediction_dataset.py
+-rw-rw-rw-  2.0 fat     3067 b- defN 23-Jul-07 02:07 tests/classification_tests/test_pretrained_model_factory.py
+-rw-rw-rw-  2.0 fat      183 b- defN 23-Jul-07 02:07 tests/common/__init__.py
+-rw-rw-rw-  2.0 fat     1237 b- defN 23-Jul-07 02:07 tests/common/aml_dataset_mock.py
+-rw-rw-rw-  2.0 fat     5177 b- defN 23-Jul-07 02:07 tests/common/run_mock.py
+-rw-rw-rw-  2.0 fat    10438 b- defN 23-Jul-07 02:07 tests/common/test_aml_dataset_helper.py
+-rw-rw-rw-  2.0 fat    16892 b- defN 23-Jul-07 02:07 tests/common/test_artifacts_utils.py
+-rw-rw-rw-  2.0 fat    58368 b- defN 23-Jul-07 02:07 tests/common/test_common_methods.py
+-rw-rw-rw-  2.0 fat    16927 b- defN 23-Jul-07 02:07 tests/common/test_distributed_utils.py
+-rw-rw-rw-  2.0 fat     4295 b- defN 23-Jul-07 02:07 tests/common/test_mlflow_model_wrapper.py
+-rw-rw-rw-  2.0 fat    10586 b- defN 23-Jul-07 02:07 tests/common/test_model_export_utils.py
+-rw-rw-rw-  2.0 fat     6720 b- defN 23-Jul-07 02:07 tests/common/test_pretrained_model_utilities.py
+-rw-rw-rw-  2.0 fat     6606 b- defN 23-Jul-07 02:07 tests/common/test_runner_default_args.py
+-rw-rw-rw-  2.0 fat     5624 b- defN 23-Jul-07 02:07 tests/common/test_training_state.py
+-rw-rw-rw-  2.0 fat     6315 b- defN 23-Jul-07 02:07 tests/common/utils.py
+-rw-rw-rw-  2.0 fat      183 b- defN 23-Jul-07 02:07 tests/object_detection_tests/__init__.py
+-rw-rw-rw-  2.0 fat      430 b- defN 23-Jul-07 02:07 tests/object_detection_tests/aml_dataset_mock.py
+-rw-rw-rw-  2.0 fat      711 b- defN 23-Jul-07 02:07 tests/object_detection_tests/conftest.py
+-rw-rw-rw-  2.0 fat     5755 b- defN 23-Jul-07 02:07 tests/object_detection_tests/test_augmentations.py
+-rw-rw-rw-  2.0 fat     4323 b- defN 23-Jul-07 02:07 tests/object_detection_tests/test_coco_eval_box_converter.py
+-rw-rw-rw-  2.0 fat     1604 b- defN 23-Jul-07 02:07 tests/object_detection_tests/test_cocotools.py
+-rw-rw-rw-  2.0 fat     9815 b- defN 23-Jul-07 02:07 tests/object_detection_tests/test_dataset_wrappers.py
+-rw-rw-rw-  2.0 fat    73835 b- defN 23-Jul-07 02:07 tests/object_detection_tests/test_datasets.py
+-rw-rw-rw-  2.0 fat    78443 b- defN 23-Jul-07 02:07 tests/object_detection_tests/test_incremental_voc_evaluator.py
+-rw-rw-rw-  2.0 fat     6193 b- defN 23-Jul-07 02:07 tests/object_detection_tests/test_maskutils.py
+-rw-rw-rw-  2.0 fat    21122 b- defN 23-Jul-07 02:07 tests/object_detection_tests/test_metric_computation_utils.py
+-rw-rw-rw-  2.0 fat    15744 b- defN 23-Jul-07 02:07 tests/object_detection_tests/test_model_wrappers.py
+-rw-rw-rw-  2.0 fat    14068 b- defN 23-Jul-07 02:07 tests/object_detection_tests/test_object_annotation.py
+-rw-rw-rw-  2.0 fat    18676 b- defN 23-Jul-07 02:07 tests/object_detection_tests/test_object_detection_instance_segmentation_evaluator.py
+-rw-rw-rw-  2.0 fat     4296 b- defN 23-Jul-07 02:07 tests/object_detection_tests/test_object_detection_local_run.py
+-rw-rw-rw-  2.0 fat    21358 b- defN 23-Jul-07 02:07 tests/object_detection_tests/test_object_detection_utils.py
+-rw-rw-rw-  2.0 fat     2728 b- defN 23-Jul-07 02:07 tests/object_detection_tests/test_pretrained_model_factory.py
+-rw-rw-rw-  2.0 fat     5550 b- defN 23-Jul-07 02:07 tests/object_detection_tests/test_scoring.py
+-rw-rw-rw-  2.0 fat     5677 b- defN 23-Jul-07 02:07 tests/object_detection_tests/test_secondary_model_wrappers.py
+-rw-rw-rw-  2.0 fat     5050 b- defN 23-Jul-07 02:07 tests/object_detection_tests/test_tiling_distributed_sampler.py
+-rw-rw-rw-  2.0 fat    49608 b- defN 23-Jul-07 02:07 tests/object_detection_tests/test_tiling_helper.py
+-rw-rw-rw-  2.0 fat     9531 b- defN 23-Jul-07 02:07 tests/object_detection_tests/test_tiling_utils.py
+-rw-rw-rw-  2.0 fat     1280 b- defN 23-Jul-07 02:07 tests/object_detection_tests/test_trainer_criterion.py
+-rw-rw-rw-  2.0 fat     4133 b- defN 23-Jul-07 02:07 tests/object_detection_tests/test_yolo_trainer_train.py
+-rw-rw-rw-  2.0 fat     3710 b- defN 23-Jul-07 02:07 tests/object_detection_tests/test_yolo_utils.py
+-rw-rw-rw-  2.0 fat      773 b- defN 23-Jul-07 02:07 tests/object_detection_tests/test_yolo_wrapper.py
+-rw-rw-rw-  2.0 fat     1088 b- defN 23-Jul-07 02:07 tests/object_detection_tests/utils.py
+-rw-rw-rw-  2.0 fat     1021 b- defN 23-Jul-07 02:18 azureml_automl_dnn_vision-1.52.0.post1.dist-info/LICENSE.txt
+-rw-rw-rw-  2.0 fat     1938 b- defN 23-Jul-07 02:18 azureml_automl_dnn_vision-1.52.0.post1.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       97 b- defN 23-Jul-07 02:18 azureml_automl_dnn_vision-1.52.0.post1.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        1 b- defN 23-Jul-07 02:18 azureml_automl_dnn_vision-1.52.0.post1.dist-info/namespace_packages.txt
+-rw-rw-rw-  2.0 fat       14 b- defN 23-Jul-07 02:18 azureml_automl_dnn_vision-1.52.0.post1.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat    21844 b- defN 23-Jul-07 02:18 azureml_automl_dnn_vision-1.52.0.post1.dist-info/RECORD
+193 files, 1868935 bytes uncompressed, 401426 bytes compressed:  78.5%
```

## zipnote {}

```diff
@@ -555,26 +555,26 @@
 
 Filename: tests/object_detection_tests/test_yolo_wrapper.py
 Comment: 
 
 Filename: tests/object_detection_tests/utils.py
 Comment: 
 
-Filename: azureml_automl_dnn_vision-1.52.0.dist-info/LICENSE.txt
+Filename: azureml_automl_dnn_vision-1.52.0.post1.dist-info/LICENSE.txt
 Comment: 
 
-Filename: azureml_automl_dnn_vision-1.52.0.dist-info/METADATA
+Filename: azureml_automl_dnn_vision-1.52.0.post1.dist-info/METADATA
 Comment: 
 
-Filename: azureml_automl_dnn_vision-1.52.0.dist-info/WHEEL
+Filename: azureml_automl_dnn_vision-1.52.0.post1.dist-info/WHEEL
 Comment: 
 
-Filename: azureml_automl_dnn_vision-1.52.0.dist-info/namespace_packages.txt
+Filename: azureml_automl_dnn_vision-1.52.0.post1.dist-info/namespace_packages.txt
 Comment: 
 
-Filename: azureml_automl_dnn_vision-1.52.0.dist-info/top_level.txt
+Filename: azureml_automl_dnn_vision-1.52.0.post1.dist-info/top_level.txt
 Comment: 
 
-Filename: azureml_automl_dnn_vision-1.52.0.dist-info/RECORD
+Filename: azureml_automl_dnn_vision-1.52.0.post1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## azureml/automl/dnn/vision/common/constants.py

```diff
@@ -82,16 +82,14 @@
     WEIGHTED = 'weighted'
     MEAN_AVERAGE_PRECISION = 'mean_average_precision'
     CONFUSION_MATRIX = 'confusion_matrix'
     AUTOML_CLASSIFICATION_EVAL_METRICS = 'automl_classification_eval_metrics'
     AUTOML_CLASSIFICATION_TRAIN_METRICS = 'automl_classification_train_metrics'
     COCO_METRICS = 'coco_metrics'
     PER_LABEL_METRICS = 'per_label_metrics'
-    PRECISIONS_PER_SCORE_THRESHOLD = 'precisions_per_score_threshold'
-    RECALLS_PER_SCORE_THRESHOLD = 'recalls_per_score_threshold'
     IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS = 'image_level_binary_classifier_metrics'
     CONFUSION_MATRICES_PER_SCORE_THRESHOLD = 'confusion_matrices_per_score_threshold'
     CLASS_NAME = 'class_name'
     CLASS_LABELS = 'class_labels'
     MATRIX = 'matrix'
     DATA = 'data'
     AVERAGE = 'average'
```

## azureml/automl/dnn/vision/common/utils.py

```diff
@@ -262,15 +262,16 @@
             if img is None:
                 raise AutoMLVisionDataException("cv2.imread returned None")
             if tile is not None:
                 img = img[int(tile.top_left_y): int(tile.bottom_right_y),
                           int(tile.top_left_x): int(tile.bottom_right_x)]
             return img
         else:
-            image = Image.open(image_url).convert("RGB")
+            with Image.open(image_url) as original_image:
+                image = original_image.convert("RGB")
             if tile is not None:
                 image = image.crop(tile.as_tuple())
             return image
     except Exception as ex:
         if ignore_data_errors:
             msg = "Exception occurred when trying to read the image. This file will be ignored."
             logger.warning(msg)
@@ -671,15 +672,14 @@
     """
 
     # The pycocotools package prints all scores from coco metrics, so printing coco_metrics would be redundant.
     METRICS_EXCLUDED_FROM_PRINTING = [MetricsLiterals.COCO_METRICS]
 
     # A select set of metrics need custom logging.
     METRICS_WITH_CUSTOM_LOGGING = [
-        MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD, MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD,
         MetricsLiterals.PER_LABEL_METRICS, MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS,
         MetricsLiterals.CONFUSION_MATRICES_PER_SCORE_THRESHOLD
     ]
 
     if azureml_run is None:
         raise AutoMLVisionTrainingException("Cannot log metrics to Run History since azureml_run is None",
                                             has_pii=False)
@@ -742,107 +742,64 @@
     :type metrics: Dict[str,Any]
     :param azureml_run: The run object.
     :type azureml_run: azureml.core.run
     :param class_names: List of class names
     :type class_names: List[str]
     """
 
-    UNDEFINED_VALUE = -1.0
     MISSED_CLASS_NAME = "Missed"
     NO_VALUE = "N/A"
 
-    def _add_train_suffix(name: str, is_train: bool) -> str:
-        """Add relevant suffix to metric name if computed on training set."""
-        return name + ("_train" if is_train else "")
-
-    def _log_pr_curve(
-            class_name: Optional[str], precisions_per_score_threshold: Dict[float, float],
-            recalls_per_score_threshold: Dict[float, float], is_train: bool
-    ) -> None:
-        # Log under a name that includes a. the _train suffix if present in the original metric name and b. the class
-        # name if the PR curve is class specific.
-        logged_metric_name = _add_train_suffix("pr_curve", is_train) + ("" if class_name is None else "_" + class_name)
-
-        # Go through sorted score thresholds and log the corresponding precision and recall values.
-        sorted_score_thresholds = sorted(
-            set(precisions_per_score_threshold.keys()).union(recalls_per_score_threshold.keys()), reverse=True
-        )
-        for st in sorted_score_thresholds:
-            # Skip score thresholds with invalid precision or recall values.
-            p = precisions_per_score_threshold.get(st, UNDEFINED_VALUE)
-            r = recalls_per_score_threshold.get(st, UNDEFINED_VALUE)
-            if (p != UNDEFINED_VALUE) and (r != UNDEFINED_VALUE):
-                azureml_run.log_row(logged_metric_name, recall=r, precision=p, score_threshold=st)
-
-    def _log_cm(
-            confusion_matrices_per_score_threshold: Dict[float, List[List[Any]]], is_train: bool
-    ) -> None:
-        # Go through sorted score thresholds and log the corresponding confusion matrices.
-        sorted_score_thresholds = sorted(confusion_matrices_per_score_threshold.keys())
-        for st in sorted_score_thresholds:
-            # Skip empty confusion matrices.
-            cm = confusion_matrices_per_score_threshold[st]
-            if len(cm) == 0:
-                continue
-
-            # Format the confusion matrix in scikit format, ie append a row to make it square and add a class. The
-            # matrix ends up being (C+1)x(C+1), with the last class called 'Missed'.
-            fcm = {
-                "schema_type": "confusion_matrix",
-                "schema_version": "1.0.0",
-                "data": {
-                    "class_labels": class_names + [MISSED_CLASS_NAME],
-                    "matrix": cm + [[NO_VALUE for _ in range(len(cm[0]))]]
-                }
-            }
+    for metric_name in metrics:
+        metric_name_stem = metric_name[:-len("_train")] if metric_name.endswith("_train") else metric_name
 
-            # Log under a name that includes a. the _train suffix if present in the original metric name and b. the
-            # score threshold.
-            logged_metric_name = _add_train_suffix("confusion_matrix", is_train) + "_score_threshold_{}".format(st)
-            azureml_run.log_confusion_matrix(logged_metric_name, fcm)
-
-    for is_train in [False, True]:
-        # Global PR curve.
-        metric_name1 = _add_train_suffix(MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD, is_train)
-        metric_name2 = _add_train_suffix(MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD, is_train)
-        if (metric_name1 in metrics) and (metric_name2 in metrics):
-            # Log table with score thresholds, precisions and recalls (global, regardless of class).
-            _log_pr_curve(None, metrics[metric_name1], metrics[metric_name2], is_train)
-
-        # Per class precision, recall, AP and PR curve.
-        metric_name = _add_train_suffix(MetricsLiterals.PER_LABEL_METRICS, is_train)
-        if metric_name in metrics:
+        if metric_name_stem == MetricsLiterals.PER_LABEL_METRICS:
             for class_index, class_metrics in metrics[metric_name].items():
-                # Log table with precision, recall and AP for the current class.
                 azureml_run.log_row(
                     metric_name, class_name=class_names[class_index],
                     precision=class_metrics[MetricsLiterals.PRECISION], recall=class_metrics[MetricsLiterals.RECALL],
                     average_precision=class_metrics[MetricsLiterals.AVERAGE_PRECISION]
                 )
 
-                # Log table with score thresholds, precisions and recalls for the current class.
-                _log_pr_curve(
-                    class_names[class_index], class_metrics[MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD],
-                    class_metrics[MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD], is_train
-                )
-
-        # Image level precision, recall and AP.
-        metric_name = _add_train_suffix(MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS, is_train)
-        if metric_name in metrics:
+        if metric_name_stem == MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS:
             image_level_metrics = metrics[metric_name]
             azureml_run.log_row(
                 metric_name, precision=image_level_metrics[MetricsLiterals.PRECISION],
                 recall=image_level_metrics[MetricsLiterals.RECALL],
                 average_precision=image_level_metrics[MetricsLiterals.AVERAGE_PRECISION]
             )
 
-        # Confusion matrices.
-        metric_name = _add_train_suffix(MetricsLiterals.CONFUSION_MATRICES_PER_SCORE_THRESHOLD, is_train)
-        if metric_name in metrics:
-            _log_cm(metrics[metric_name], is_train)
+        if metric_name_stem == MetricsLiterals.CONFUSION_MATRICES_PER_SCORE_THRESHOLD:
+            confusion_matrices_per_score_threshold = metrics[metric_name]
+
+            # Go through sorted score thresholds and log the corresponding confusion matrices.
+            sorted_score_thresholds = sorted(confusion_matrices_per_score_threshold.keys())
+            for st in sorted_score_thresholds:
+                # Skip empty confusion matrices.
+                cm = confusion_matrices_per_score_threshold[st]
+                if len(cm) == 0:
+                    continue
+
+                # Format the confusion matrix in scikit format, ie append a row to make it square and add a class. The
+                # matrix ends up being (C+1)x(C+1), with the last class called 'Missed'.
+                fcm = {
+                    "schema_type": "confusion_matrix",
+                    "schema_version": "1.0.0",
+                    "data": {
+                        "class_labels": class_names + [MISSED_CLASS_NAME],
+                        "matrix": cm + [[NO_VALUE for _ in range(len(cm[0]))]]
+                    }
+                }
+
+                # Log confusion matrix under a name that includes a. the _train suffix if present in the original
+                # metric name and b. the score threshold.
+                logged_metric_name = "confusion_matrix{}_score_threshold_{}".format(
+                    "_train" if metric_name.endswith("_train") else "", st
+                )
+                azureml_run.log_confusion_matrix(logged_metric_name, fcm)
 
 
 def log_verbose_metrics_to_rh(train_time: float, epoch_time: AverageMeter,
                               train_sys_meter: SystemMeter, valid_sys_meter: SystemMeter,
                               azureml_run: Run) -> None:
     """Logs verbose metrics to run history at the end of training.
 
@@ -1510,15 +1467,15 @@
         return
 
     stream_image_files = advanced_settings.get(SettingsLiterals.STREAM_IMAGE_FILES)
     automl_settings[SettingsLiterals.STREAM_IMAGE_FILES] = True if stream_image_files is True else False
     automl_settings[SettingsLiterals.APPLY_AUTOML_TRAIN_AUGMENTATIONS] = bool(
         advanced_settings.get(SettingsLiterals.APPLY_AUTOML_TRAIN_AUGMENTATIONS, True))
     automl_settings[SettingsLiterals.APPLY_MOSAIC_FOR_YOLO] = bool(
-        advanced_settings.get(SettingsLiterals.APPLY_MOSAIC_FOR_YOLO, True))
+        advanced_settings.get(SettingsLiterals.APPLY_MOSAIC_FOR_YOLO , True))
 
 
 def set_run_traits(azureml_run: Run, settings: Dict[str, Any]) -> None:
     """Sets traits on the AzureML run. Run traits are propagted to cold path logs. App Insights and Kusto logs are
     only stored for 30 days, but cold path logs are stored for a longer period of time. Setting traits enables
     querying run characteristics over a longer window of time.
```

## azureml/automl/dnn/vision/object_detection/common/object_detection_utils.py

```diff
@@ -261,88 +261,77 @@
 
 
 def _update_with_voc_metrics(current_metrics, cumulative_per_label_metrics, voc_metrics, is_train=False):
     """
     Update the current metrics and the cumulative metrics according to the VOC metrics dictionary.
     """
 
-    # Information controling how each VOC metric updates the current and cumulative metrics.
-    MetricInformation = namedtuple(
-        "MetricInformation", ["type", "group", "updates_current"], defaults=[None, None, True]
-    )
+    MetricInformation = namedtuple("MetricInformation", ["type", "is_base", "updates_current"])
     METRIC_INFORMATION_BY_NAME = {
-        MetricsLiterals.PRECISION: MetricInformation("scalar", "base"),
-        MetricsLiterals.RECALL: MetricInformation("scalar", "base"),
-        MetricsLiterals.AVERAGE_PRECISION: MetricInformation("scalar", "base", updates_current=False),
-        MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: MetricInformation("dictionary", "extended_base"),
-        MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: MetricInformation("dictionary", "extended_base"),
-        MetricsLiterals.PER_LABEL_METRICS: MetricInformation("per_label", "extra"),
-        MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS: MetricInformation("image_level", "extra"),
-        MetricsLiterals.CONFUSION_MATRICES_PER_SCORE_THRESHOLD: MetricInformation(
-            "matrix_per_score_threshold", "extra"
+        MetricsLiterals.PRECISION: MetricInformation("scalar", is_base=True, updates_current=True),
+        MetricsLiterals.RECALL: MetricInformation("scalar", is_base=True, updates_current=True),
+        MetricsLiterals.AVERAGE_PRECISION: MetricInformation("scalar", is_base=True, updates_current=False),
+        MetricsLiterals.PER_LABEL_METRICS: MetricInformation("per_label", is_base=False, updates_current=True),
+        MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS: MetricInformation(
+            "image_level", is_base=False, updates_current=True
         ),
+        MetricsLiterals.CONFUSION_MATRICES_PER_SCORE_THRESHOLD: MetricInformation(
+            "per_score_threshold", is_base=False, updates_current=True
+        )
     }
 
-    def _add_suffix(name):
-        """Add suffix to metric name if computed on training set."""
-
+    def _alter_name(name):
         return name + ("_train" if is_train else "")
 
-    def _select_and_round(m, m_type):
-        """Select metric fields to be logged and round their numeric values."""
-
-        if m_type == "scalar":
-            if isinstance(m, torch.Tensor):
-                m = m.item()
-            return round(m, 5)
-
-        if m_type == "dictionary":
-            return {_select_and_round(k, "scalar"): _select_and_round(v, "scalar") for k, v in m.items()}
+    def _round(x, value_type):
+        if value_type == "scalar":
+            if isinstance(x, torch.Tensor):
+                x = x.item()
+            return round(x, 5)
 
-        if m_type == "per_label":
+        if value_type == "per_label":
             return {
                 label_index: {
-                    metric_name: _select_and_round(metrics[metric_name], metric_information.type)
+                    metric_name: _round(metrics[metric_name], "scalar")
                     for metric_name, metric_information in METRIC_INFORMATION_BY_NAME.items()
-                    if metric_information.group in {"base", "extended_base"}
+                    if metric_information.is_base
                 }
-                for label_index, metrics in m.items()
+                for label_index, metrics in x.items()
             }
 
-        if m_type == "image_level":
+        if value_type == "image_level":
             return {
-                metric_name: _select_and_round(m[metric_name], metric_information.type)
-                for metric_name, metric_information in METRIC_INFORMATION_BY_NAME.items()
-                if metric_information.group == "base"
+                metric_name: _round(metric_value, "scalar")
+                for metric_name, metric_value in x.items()
             }
 
-        if m_type == "matrix_per_score_threshold":
+        if value_type == "per_score_threshold":
             return {
-                _select_and_round(score_threshold, "scalar"): [
-                    [_select_and_round(column, "scalar") for column in row] for row in confusion_matrix
+                _round(score_threshold, "scalar"): [
+                    [_round(column, "scalar") for column in row] for row in confusion_matrix
                 ]
-                for score_threshold, confusion_matrix in m.items()
+                for score_threshold, confusion_matrix in x.items()
             }
 
     # Set the current metrics: precision, recall, per label, image level and confusion matrix metrics.
     for metric_name, metric_information in METRIC_INFORMATION_BY_NAME.items():
         if metric_information.updates_current and (metric_name in voc_metrics):
-            metric_name_with_suffix = _add_suffix(metric_name)
+            altered_metric_name = _alter_name(metric_name)
             metric_value = voc_metrics[metric_name]
-            current_metrics[metric_name_with_suffix] = _select_and_round(metric_value, metric_information.type)
+            current_metrics[altered_metric_name] = _round(metric_value, metric_information.type)
 
     # Update the cumulative per-label metrics. Use label index instead of label name due to pii.
-    for label_index, metrics in current_metrics[_add_suffix(MetricsLiterals.PER_LABEL_METRICS)].items():
+    for label_index, metrics in current_metrics[_alter_name(MetricsLiterals.PER_LABEL_METRICS)].items():
         # If entry does not exist, initialize to empty dictionary.
         if label_index not in cumulative_per_label_metrics:
             cumulative_per_label_metrics[label_index] = {}
 
         # Go through base metrics: precision, recall and average precision.
         for metric_name, metric_information in METRIC_INFORMATION_BY_NAME.items():
-            if metric_information.group == "base":
+            if metric_information.is_base:
                 # If entry does not exist, initialize to empty list.
                 if metric_name not in cumulative_per_label_metrics[label_index]:
                     cumulative_per_label_metrics[label_index][metric_name] = []
 
                 # Accumulate current metric value.
                 cumulative_per_label_metrics[label_index][metric_name].append(metrics[metric_name])
```

## azureml/automl/dnn/vision/object_detection/eval/incremental_voc_evaluator.py

```diff
@@ -14,17 +14,14 @@
 from azureml.automl.dnn.vision.object_detection.eval.metric_computation_utils import calculate_confusion_matrices, \
     calculate_pr_metrics, match_objects, EPSILON, UNASSIGNED
 
 
 logger = get_logger(__name__)
 
 
-# TODO: refactor `compute_metrics()` and `_evaluate_image()` to reduce complexity and to abstract the accumulation
-# parts more.
-
 class IncrementalVocEvaluator:
     """
     Incremental VOC-style evaluation for object detection and instance segmentation.
 
     Suggested flow: make new object at beginning of evaluation, call `evaluate_batch()` after each batch, and
     eventually call `compute_metrics()` to get the final evaluation results.
     Users must specify whether the task is object detection or instance segmentation, the number of classes and the
@@ -181,79 +178,71 @@
         :rtype: dict with precision, recall, mean_average_precision, per_label_metrics, image_level_binary_classifier,
             confusion_matrices_per_score_thresholds keys
         """
 
         # Initialize the per class metrics to empty.
         metrics_per_class = {}
 
-        # Initialize the number of ground truth objects across all classes.
-        num_gt_objects = 0
-
         # Initialize the lists with all labels, scores and image indexes of predicted objects.
-        all_tp_fp_labels = [np.zeros((0,), dtype=np.uint8)]
-        all_scores = [np.zeros((0,))]
-        all_image_indexes = [np.zeros((0,), dtype=np.uint32)]
+        all_tp_fp_labels = []
+        all_scores = []
+        all_image_indexes = []
 
         # Go through each class and calculate the metrics for its objects (e.g. AP).
         for c in range(self._num_classes):
             # Get the labels and scores of predicted objects across all images.
             tp_fp_labels = np.concatenate(self._tp_fp_labels_per_class[c])
             scores = np.concatenate(self._scores_per_class[c])
 
             # Calculate metrics for the objects in the current class.
             metrics_per_class[c] = calculate_pr_metrics(
                 self._num_gt_objects_per_class[c], tp_fp_labels, scores, None,
                 self._use_voc_11_point_metric, self.UNDEFINED_METRIC_VALUE
             )
 
-            # Accumulate the number of ground truth objects for the current class.
-            num_gt_objects += self._num_gt_objects_per_class[c]
-
-            # Accumulate the lists with labels, scores and image indexes of predicted objects for the current class.
-            all_tp_fp_labels.extend(self._tp_fp_labels_per_class[c])
-            all_scores.extend(self._scores_per_class[c])
-            all_image_indexes.extend(self._image_indexes_per_class[c])
-
-        # Calculate metrics for all objects regardless of class.
-        metrics = calculate_pr_metrics(
-            num_gt_objects, np.concatenate(all_tp_fp_labels), np.concatenate(all_scores), None,
-            self._use_voc_11_point_metric, self.UNDEFINED_METRIC_VALUE
-        )
+            if self._task_is_detection:
+                # Accumulate the per class lists with all labels, scores and image indexes of predicted objects.
+                all_tp_fp_labels.extend(self._tp_fp_labels_per_class[c])
+                all_scores.extend(self._scores_per_class[c])
+                all_image_indexes.extend(self._image_indexes_per_class[c])
 
-        # Calculate the mean over all classes for the last precision, last recall and AP (-> mAP) metrics.
+        # Calculate the mean over all classes for the last precision, last recall and AP (=>mAP) metrics.
         object_level_metrics = {
             MetricsLiterals.PER_LABEL_METRICS: metrics_per_class,
             MetricsLiterals.PRECISION: self._calculate_metric_mean_over_classes(
                 metrics_per_class, MetricsLiterals.PRECISION
             ),
             MetricsLiterals.RECALL: self._calculate_metric_mean_over_classes(
                 metrics_per_class, MetricsLiterals.RECALL
             ),
             MetricsLiterals.MEAN_AVERAGE_PRECISION: self._calculate_metric_mean_over_classes(
                 metrics_per_class, MetricsLiterals.AVERAGE_PRECISION
-            ),
-            MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: metrics[MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD],
-            MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: metrics[MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD]
+            )
         }
 
         if self._task_is_detection:
             # Image level metrics and confusion matrices for object detection.
 
             # Calculate the image level metrics.
             image_level_metrics = {
                 MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS: calculate_pr_metrics(
-                    self._num_images_with_gt_objects, np.concatenate(all_tp_fp_labels), np.concatenate(all_scores),
-                    np.concatenate(all_image_indexes), False, self.UNDEFINED_METRIC_VALUE
+                    self._num_images_with_gt_objects,
+                    np.concatenate([np.zeros((0,), dtype=np.uint8)] + all_tp_fp_labels),
+                    np.concatenate([np.zeros((0,))] + all_scores),
+                    np.concatenate([np.zeros((0,), dtype=np.uint32)] + all_image_indexes),
+                    False,
+                    self.UNDEFINED_METRIC_VALUE
                 )
             }
 
             # Calculate the confusion matrices at representative scores.
             confusion_matrix_metrics = {
                 MetricsLiterals.CONFUSION_MATRICES_PER_SCORE_THRESHOLD: calculate_confusion_matrices(
-                    self._num_gt_objects_per_class, np.concatenate(self._all_matched_classes_and_scores)
+                    self._num_gt_objects_per_class,
+                    np.concatenate([np.zeros((0, 3), dtype=np.float32)] + self._all_matched_classes_and_scores)
                 )
             }
 
         else:
             # No image level metrics or confusion matrices for instance segmentation.
             image_level_metrics = {}
             confusion_matrix_metrics = {}
@@ -434,14 +423,12 @@
         :return: Mean metric value.
         :rtype: float
         """
 
         # Get the list of values of a metric across classes.
         values = [metrics_per_class[c][metric_name] for c in range(self._num_classes)]
 
-        # Calculate the mean of valid values. If no valid values, return undefined value.
+        # Calculate the mean of valid values.
         valid_values = [v for v in values if v != self.UNDEFINED_METRIC_VALUE]
-        if len(valid_values) == 0:
-            return self.UNDEFINED_METRIC_VALUE
         average_value = sum(valid_values) / (len(valid_values) + EPSILON)
 
         return average_value
```

## azureml/automl/dnn/vision/object_detection/eval/metric_computation_utils.py

```diff
@@ -12,18 +12,16 @@
 from azureml.automl.dnn.vision.common.exceptions import AutoMLVisionSystemException
 from azureml.automl.dnn.vision.common.logging_utils import get_logger
 
 
 # Codes for TP, FP, other.
 _TP_CODE, _FP_CODE, _OTHER_CODE = 1, 0, 2
 
-# Score thresholds at which to compute precision-recall pairs and confusion matrices. Since confusion matrices can be
-# very large, they are computed on a smaller/coarser set of thresholds than the precision-recall pairs.
-_SCORE_THRESHOLDS_FINE = [(i / 100.0) for i in range(100)]
-_SCORE_THRESHOLDS_COARSE = [(i / 10.0) for i in range(10)]
+# Score thresholds at which to compute confusion matrices.
+_SCORE_THRESHOLDS = [(i / 10.0) for i in range(10)]
 
 # Code for predicted objects not assigned to ground truth objects.
 UNASSIGNED = -1
 
 # Constant to avoid division by 0.
 EPSILON = 1E-9
 
@@ -179,103 +177,91 @@
                 tp_fp_labels[predicted_index] = _OTHER_CODE
 
     return tp_fp_labels, predicted_assignment
 
 
 def calculate_pr_metrics(m, tp_fp_labels, scores, image_indexes, use_voc_11_point_metric, undefined_value):
     """
-    Calculate metrics related to the PR curve, eg AP.
+    Calculate AP, highest recall and precision at highest recall given TP/FP labels, scores and image indexes.
 
-    Calculate average precision (area under PR curve), highest recall and precision at the highest recall, and
-    precision-recall values for fixed score thresholds from TP/FP labels, scores and image indexes.
-    If image indexes are provided, then the image level metrics are computed. If no image indexes are provided
-    (parameter set to `None`), then the regular object level metrics are computed.
+    If the image indexes are set to `None`, then the regular object level metrics are computed. If valid image indexes
+    are provided, then the image level metrics are computed.
 
     :param m: Number of ground truth objects/images with ground truth objects.
     :type m: int
     :param tp_fp_labels: Labels for predicted objects.
     :type tp_fp_labels: numpy.ndarray
     :param scores: Scores for predicted objects.
     :type scores: numpy.ndarray
     :param image_indexes: The indexes of the images the predicted objects belong to.
     :type image_indexes: Optional[numpy.ndarray]
     :param use_voc_11_point_metric: Whether to use the 11 point computation style.
     :type use_voc_11_point_metric: bool
     :param undefined_value: Value to use when a metric is undefined.
     :type undefined_value: float
-    :return: AP, highest recall, precision@highest recall, precision-recall values at fixed thresholds.
-    :rtype: dict with AP, highest recall, precision@highest recall, precision-recall pairs
+    :return: AP, highest recall, precision@highest recall.
+    :rtype: dict with precision, recall, AP
     """
 
     if image_indexes is None:
         # Get the number of predicted objects.
         n = len(tp_fp_labels)
     else:
         # Get the number of images with predicted objects.
         n = len(np.unique(image_indexes))
 
     # If there are no ground truth objects/images with ground truth objects and no predicted objects/images with
-    # predicted objects, then the AP, precision and recall, and precision-recall pairs are undefined.
-    undefined_value_per_score_threshold = {st: undefined_value for st in _SCORE_THRESHOLDS_FINE}
+    # predicted objects, AP, precision and recall are undefined.
     if (m == 0) and (n == 0):
         return {
             MetricsLiterals.AVERAGE_PRECISION: undefined_value,
             MetricsLiterals.PRECISION: undefined_value,
-            MetricsLiterals.RECALL: undefined_value,
-            MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: undefined_value_per_score_threshold,
-            MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: undefined_value_per_score_threshold,
+            MetricsLiterals.RECALL: undefined_value
         }
     # If there are no ground truth objects/images with ground truth objects but predicted objects/images with predicted
-    # objects exist, then the AP and recall are undefined, precision is 0, and precision-recall pairs are 0-undefined.
-    zero_per_score_threshold = {st: 0.0 for st in _SCORE_THRESHOLDS_FINE}
+    # objects exist, AP and recall are undefined and precision is 0.
     if m == 0:
         return {
             MetricsLiterals.AVERAGE_PRECISION: undefined_value,
             MetricsLiterals.PRECISION: 0.0,
-            MetricsLiterals.RECALL: undefined_value,
-            MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: zero_per_score_threshold,
-            MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: undefined_value_per_score_threshold,
+            MetricsLiterals.RECALL: undefined_value
         }
     # If ground truth objects/images with ground truth objects exist but there are no predicted objects/images with
-    # predicted objects, then the AP and recall are 0, precision is undefined, and precision-recall pairs are
-    # undefined-0.
+    # predicted objects, AP and recall are 0 and precision is undefined.
     if n == 0:
         return {
             MetricsLiterals.AVERAGE_PRECISION: 0.0,
             MetricsLiterals.PRECISION: undefined_value,
-            MetricsLiterals.RECALL: 0.0,
-            MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: undefined_value_per_score_threshold,
-            MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: zero_per_score_threshold,
+            MetricsLiterals.RECALL: 0.0
         }
 
-    # Get the indexes that sort the scores and the prediction labels sorted by scores.
-    indexes_scores_inc = np.argsort(scores)
-    indexes_scores_dec = indexes_scores_inc[::-1]
-    labels_sorted_by_score_dec = tp_fp_labels[indexes_scores_dec]
+    # Get the predictions in decreasing order by score.
+    indexes_scores_decreasing = np.argsort(scores)[::-1]
+    labels_sorted_by_score_desc = tp_fp_labels[indexes_scores_decreasing]
 
     if image_indexes is None:
         # Count the true positive and the false positive objects for each score threshold.
-        cum_tp = np.cumsum(labels_sorted_by_score_dec == _TP_CODE)
-        cum_fp = np.cumsum(labels_sorted_by_score_dec == _FP_CODE)
+        cum_tp = np.cumsum(labels_sorted_by_score_desc == _TP_CODE)
+        cum_fp = np.cumsum(labels_sorted_by_score_desc == _FP_CODE)
 
     else:
         # Get the image indexes in decreasing order by score.
-        image_indexes_sorted_by_score_dec = image_indexes[indexes_scores_dec]
+        image_indexes_sorted_by_score_desc = image_indexes[indexes_scores_decreasing]
 
         # Count the true positive and the false positive images for each score threshold. This has to be done in a for
         # loop with custom logic to obtain the image counts from object level information.
 
         # Initialize the counts to all zero and sets of images to empty.
         cum_tp = np.zeros((len(image_indexes),))
         cum_fp = np.zeros((len(image_indexes),))
         tp_images = set()
         fp_images = set()
 
         # Go through each score threshold and incrementally update the sets of true positive and false positive images.
-        for k, (c, i) in enumerate(zip(labels_sorted_by_score_dec, image_indexes_sorted_by_score_dec)):
+        for k, (c, i) in enumerate(zip(labels_sorted_by_score_desc, image_indexes_sorted_by_score_desc)):
             # Update the sets of true positives and false positives: if at least one true positive object exists in an
             # image, then the image becomes a true positive. If only false positive objects are present, then the image
             # is a false positive.
             if c == _TP_CODE:
                 tp_images.add(i)
                 fp_images.discard(i)
             elif c == _FP_CODE:
@@ -291,34 +277,19 @@
 
     # Calculate the area under the PR curve.
     if use_voc_11_point_metric:
         average_precision = _map_score_voc_11_point_metric(precisions, recalls)
     else:
         average_precision = _map_score_voc_auc(precisions, recalls)
 
-    # Get the precision and recall values at the thresholds in the fine set. Each threshold is binary searched in the
-    # sorted set of scores, and the precision and recall values for the score greater or equal to the threshold are
-    # used. If the threshold is greater than all the scores, the precision is set to undefined and the recall to zero.
-    threshold_indexes = len(scores) - 1 - np.searchsorted(
-        scores, _SCORE_THRESHOLDS_FINE, side="left", sorter=indexes_scores_inc
-    )
-    precisions_at_thresholds = np.concatenate((precisions, [undefined_value]))[threshold_indexes]
-    recalls_at_thresholds = np.concatenate((recalls, [0.0]))[threshold_indexes]
-
     # TODO: add F1 score.
     return {
         MetricsLiterals.AVERAGE_PRECISION: average_precision,
         MetricsLiterals.PRECISION: precisions[-1],
-        MetricsLiterals.RECALL: recalls[-1],
-        MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: {
-            st: precisions_at_thresholds[i] for i, st in enumerate(_SCORE_THRESHOLDS_FINE)
-        },
-        MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: {
-            st: recalls_at_thresholds[i] for i, st in enumerate(_SCORE_THRESHOLDS_FINE)
-        }
+        MetricsLiterals.RECALL: recalls[-1]
     }
 
 
 def calculate_confusion_matrices(num_gt_objects_per_class, matched_classes_and_scores):
     """Calculate the confusion matrices at fixed score thresholds.
 
     The confusion matrix is of size Cx(C+1) where C is the number of classes. The element at (i,j) where j<=C
@@ -349,15 +320,15 @@
     # Sort the matches in descending order of scores.
     indexes_scores_decreasing = np.argsort(matched_classes_and_scores[:, 2])
     matched_classes_and_scores = matched_classes_and_scores[indexes_scores_decreasing, :]
 
     # Go through the score thresholds in descending order and through the matches in descending order of score.
     confusion_matrices_per_score_threshold = {}
     score_index = len(matched_classes_and_scores) - 1
-    for score_threshold in sorted(_SCORE_THRESHOLDS_COARSE, reverse=True):
+    for score_threshold in sorted(_SCORE_THRESHOLDS, reverse=True):
         # Update the confusion matrix with the matches with score >= the current score threshold.
         while score_index >= 0:
             # Get the ground truth object class, predicted object class and score.
             gt_class, predicted_class, score = matched_classes_and_scores[score_index]
             gt_class, predicted_class = int(gt_class), int(predicted_class)
 
             # The matches with scores less than the score threshold do not count towards the confusion matrix for the
```

## tests/common/test_common_methods.py

```diff
@@ -1021,90 +1021,51 @@
     mock_run.return_value = None
     mock_log_row.return_value = None
 
     metrics = {
         MetricsLiterals.PRECISION: 0.7,
         MetricsLiterals.RECALL: 0.8,
         MetricsLiterals.MEAN_AVERAGE_PRECISION: 0.9,
-        MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: {0.25: 0.33, 0.5: 0.5, 0.75: 0.66},
-        MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: {0.25: 0.66, 0.5: 0.5, 0.75: 0.33},
         MetricsLiterals.PER_LABEL_METRICS: {
-            0: {
-                "precision": 0.1, "recall": 0.2, "average_precision": 0.3,
-                "precisions_per_score_threshold": {0.1: 1.0, 0.15: 1.0},
-                "recalls_per_score_threshold": {0.1: 1.0, 0.15: 1.0},
-            },
-            1: {
-                "precision": 0.2, "recall": 0.3, "average_precision": 0.4,
-                "precisions_per_score_threshold": {0.2: 1.0, 0.25: 1.0},
-                "recalls_per_score_threshold": {0.2: 1.0, 0.25: 1.0},
-            },
-            2: {
-                "precision": 0.3, "recall": 0.4, "average_precision": 0.5,
-                "precisions_per_score_threshold": {0.3: 1.0, 0.35: 1.0},
-                "recalls_per_score_threshold": {0.3: 1.0, 0.35: 1.0},
-            },
+            0: {"precision": 0.1, "recall": 0.2, "average_precision": 0.3},
+            1: {"precision": 0.2, "recall": 0.3, "average_precision": 0.4},
+            2: {"precision": 0.3, "recall": 0.4, "average_precision": 0.5},
         },
         MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS: {
             "precision": 0.5, "recall": 0.6, "average_precision": 0.7
         },
         MetricsLiterals.CONFUSION_MATRICES_PER_SCORE_THRESHOLD: {
             0.1: [[3, 2, 3, 2], [4, 3, 4, 3], [5, 4, 5, 4]],
             0.2: [[2, 2, 3, 3], [3, 3, 4, 4], [4, 4, 5, 5]],
             0.3: [[1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]],
         }
     }
     if include_training:
-        metrics[MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD + "_train"] = {0.15: 0.33, 0.4: 0.5, 0.65: 0.66}
-        metrics[MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD + "_train"] = {0.15: 0.66, 0.4: 0.5, 0.65: 0.33}
         metrics[MetricsLiterals.PER_LABEL_METRICS + "_train"] = {
-            0: {
-                "precision": 0.2, "recall": 0.3, "average_precision": 0.4,
-                "precisions_per_score_threshold": {0.1: 0.5, 0.15: 0.5},
-                "recalls_per_score_threshold": {0.1: 0.5, 0.15: 0.5},
-            },
-            1: {
-                "precision": 0.3, "recall": 0.4, "average_precision": 0.5,
-                "precisions_per_score_threshold": {0.2: 0.5, 0.25: 0.5},
-                "recalls_per_score_threshold": {0.2: 0.5, 0.25: 0.5}
-            },
-            2: {
-                "precision": 0.4, "recall": 0.5, "average_precision": 0.6,
-                "precisions_per_score_threshold": {0.3: 0.5, 0.35: 0.5},
-                "recalls_per_score_threshold": {0.3: 0.5, 0.35: 0.5},
-            },
+            0: {"precision": 0.2, "recall": 0.3, "average_precision": 0.4},
+            1: {"precision": 0.3, "recall": 0.4, "average_precision": 0.5},
+            2: {"precision": 0.4, "recall": 0.5, "average_precision": 0.6},
         }
         metrics[MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS + "_train"] = {
             "precision": 0.6, "recall": 0.7, "average_precision": 0.8
         }
         metrics[MetricsLiterals.CONFUSION_MATRICES_PER_SCORE_THRESHOLD + "_train"] = {
             0.1: [[4, 3, 4, 3], [5, 4, 5, 4], [6, 5, 6, 5]],
             0.2: [[3, 3, 4, 4], [4, 4, 5, 5], [5, 5, 6, 6]],
             0.3: [[2, 3, 4, 5], [3, 4, 5, 6], [4, 5, 6, 7]],
         }
 
     utils.log_detailed_object_detection_metrics(metrics, mock_run, ["dog", "cat", "axolotl"])
 
-    global_pr_calls = [
-        call("pr_curve", recall=0.66, precision=0.33, score_threshold=0.25),
-        call("pr_curve", recall=0.5, precision=0.5, score_threshold=0.5),
-        call("pr_curve", recall=0.33, precision=0.66, score_threshold=0.75)
-    ]
     per_label_calls = [
         call(MetricsLiterals.PER_LABEL_METRICS, class_name="dog", precision=0.1, recall=0.2, average_precision=0.3),
-        call("pr_curve_dog", recall=1.0, precision=1.0, score_threshold=0.1),
-        call("pr_curve_dog", recall=1.0, precision=1.0, score_threshold=0.15),
         call(MetricsLiterals.PER_LABEL_METRICS, class_name="cat", precision=0.2, recall=0.3, average_precision=0.4),
-        call("pr_curve_cat", recall=1.0, precision=1.0, score_threshold=0.2),
-        call("pr_curve_cat", recall=1.0, precision=1.0, score_threshold=0.25),
         call(
             MetricsLiterals.PER_LABEL_METRICS, class_name="axolotl", precision=0.3, recall=0.4, average_precision=0.5
-        ),
-        call("pr_curve_axolotl", recall=1.0, precision=1.0, score_threshold=0.3),
-        call("pr_curve_axolotl", recall=1.0, precision=1.0, score_threshold=0.35),
+        )
     ]
     image_level_calls = [
         call(MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS, precision=0.5, recall=0.6, average_precision=0.7)
     ]
     confusion_matrix_calls = [
         call(
             "confusion_matrix_score_threshold_0.1",
@@ -1137,38 +1098,27 @@
                     "class_labels": ["dog", "cat", "axolotl", "Missed"],
                     "matrix": [[1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6], ["N/A", "N/A", "N/A", "N/A"]]
                 }
             }
         )
     ]
     if include_training:
-        global_pr_calls += [
-            call("pr_curve_train", recall=0.66, precision=0.33, score_threshold=0.15),
-            call("pr_curve_train", recall=0.5, precision=0.5, score_threshold=0.4),
-            call("pr_curve_train", recall=0.33, precision=0.66, score_threshold=0.65)
-        ]
         per_label_calls += [
             call(
                 MetricsLiterals.PER_LABEL_METRICS + "_train",
                 class_name="dog", precision=0.2, recall=0.3, average_precision=0.4
             ),
-            call("pr_curve_train_dog", recall=0.5, precision=0.5, score_threshold=0.1),
-            call("pr_curve_train_dog", recall=0.5, precision=0.5, score_threshold=0.15),
             call(
                 MetricsLiterals.PER_LABEL_METRICS + "_train",
                 class_name="cat", precision=0.3, recall=0.4, average_precision=0.5
             ),
-            call("pr_curve_train_cat", recall=0.5, precision=0.5, score_threshold=0.2),
-            call("pr_curve_train_cat", recall=0.5, precision=0.5, score_threshold=0.25),
             call(
                 MetricsLiterals.PER_LABEL_METRICS + "_train",
                 class_name="axolotl", precision=0.4, recall=0.5, average_precision=0.6
-            ),
-            call("pr_curve_train_axolotl", recall=0.5, precision=0.5, score_threshold=0.3),
-            call("pr_curve_train_axolotl", recall=0.5, precision=0.5, score_threshold=0.35),
+            )
         ]
         image_level_calls += [
             call(
                 MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS + "_train",
                 precision=0.6, recall=0.7, average_precision=0.8
             )
         ]
@@ -1204,31 +1154,29 @@
                         "class_labels": ["dog", "cat", "axolotl", "Missed"],
                         "matrix": [[2, 3, 4, 5], [3, 4, 5, 6], [4, 5, 6, 7], ["N/A", "N/A", "N/A", "N/A"]]
                     }
                 }
             )
         ]
 
-    mock_log_row.assert_has_calls(global_pr_calls + per_label_calls + image_level_calls, any_order=True)
+    mock_log_row.assert_has_calls(per_label_calls + image_level_calls, any_order=True)
     mock_log_confusion_matrix.assert_has_calls(confusion_matrix_calls, any_order=True)
 
 
 @mock.patch("azureml.core.run.Run.log_confusion_matrix")
 @mock.patch("azureml.core.run.Run.log_row")
 @mock.patch("azureml.core.run.Run")
 def test_detailed_object_detection_metrics_zero_classes(mock_run, mock_log_row, mock_log_confusion_matrix):
     mock_run.return_value = None
     mock_log_row.return_value = None
 
     metrics = {
-        MetricsLiterals.PRECISION: -1.0,
-        MetricsLiterals.RECALL: -1.0,
-        MetricsLiterals.MEAN_AVERAGE_PRECISION: -1.0,
-        MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: {st / 100.0: -1.0 for st in range(100)},
-        MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: {st / 100.0: -1.0 for st in range(100)},
+        MetricsLiterals.PRECISION: 0.0,
+        MetricsLiterals.RECALL: 0.0,
+        MetricsLiterals.MEAN_AVERAGE_PRECISION: 0.0,
         MetricsLiterals.PER_LABEL_METRICS: {},
         MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS: {
             "precision": -1.0, "recall": -1.0, "average_precision": -1.0
         },
         MetricsLiterals.CONFUSION_MATRICES_PER_SCORE_THRESHOLD: {
             -1.0: []
         }
```

## tests/object_detection_tests/test_incremental_voc_evaluator.py

```diff
@@ -9,32 +9,29 @@
 
 
 PRECISION, RECALL = MetricsLiterals.PRECISION, MetricsLiterals.RECALL
 AVERAGE_PRECISION, MEAN_AVERAGE_PRECISION = MetricsLiterals.AVERAGE_PRECISION, MetricsLiterals.MEAN_AVERAGE_PRECISION
 PER_LABEL_METRICS = MetricsLiterals.PER_LABEL_METRICS
 IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS = MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS
 CONFUSION_MATRICES_PER_SCORE_THRESHOLD = MetricsLiterals.CONFUSION_MATRICES_PER_SCORE_THRESHOLD
-PRECISIONS_PER_SCORE_THRESHOLD = MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD
-RECALLS_PER_SCORE_THRESHOLD = MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD
 
 
 def _check_metrics_keys(metrics, task_is_detection=True):
     expected_metrics_keys = {
-        MEAN_AVERAGE_PRECISION, PRECISION, RECALL, PRECISIONS_PER_SCORE_THRESHOLD, RECALLS_PER_SCORE_THRESHOLD,
-        PER_LABEL_METRICS, IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS, CONFUSION_MATRICES_PER_SCORE_THRESHOLD
+        MEAN_AVERAGE_PRECISION, PRECISION, RECALL, PER_LABEL_METRICS, IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS,
+        CONFUSION_MATRICES_PER_SCORE_THRESHOLD
     } if task_is_detection else {
-        MEAN_AVERAGE_PRECISION, PRECISION, RECALL, PRECISIONS_PER_SCORE_THRESHOLD, RECALLS_PER_SCORE_THRESHOLD,
-        PER_LABEL_METRICS
+        MEAN_AVERAGE_PRECISION, PRECISION, RECALL, PER_LABEL_METRICS
     }
 
     assert set(metrics.keys()) == expected_metrics_keys
 
 
 def _check_valid_metric_value(metric_value):
-    assert (metric_value == -1.0) or ((metric_value >= 0.0) and (metric_value <= 1.0))
+    assert (metric_value is None) or ((metric_value >= 0.0) and (metric_value <= 1.0))
 
 
 def _make_random_objects(width, height, num_classes, num_boxes, is_ground_truth):
     xs = np.random.randint(0, width, size=(num_boxes, 2))
     ys = np.random.randint(0, height, size=(num_boxes, 2))
     boxes = np.concatenate(
         (
@@ -54,21 +51,14 @@
 
 def _xyxy2xywh(box):
     return [
         float(box[0]), float(box[1]), float(box[2]) - float(box[0]), float(box[3]) - float(box[1])
     ]
 
 
-def _image_level_base(metrics):
-    return {
-        k: v for k, v in metrics[IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS].items()
-        if k in {AVERAGE_PRECISION, PRECISION, RECALL}
-    }
-
-
 class TestIncrementalVocEvaluator:
     @staticmethod
     def _rle_mask_from_bbox(bbox, height, width):
         x1, y1, x2, y2 = bbox
         polygon = [[x1, y1, x2, y1, x2, y2, x1, y2, x1, y1]]
         rle_masks = masktools.convert_polygon_to_rle_masks(polygon, height, width)
         return rle_masks[0]
@@ -99,35 +89,23 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
         assert metrics[PER_LABEL_METRICS] == {
-            0: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
-            1: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
-            2: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
+            0: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+            1: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+            2: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
         }
 
-        assert metrics[MEAN_AVERAGE_PRECISION] == -1.0
-        assert metrics[PRECISION] == -1.0
-        assert metrics[RECALL] == -1.0
-        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == undefined_for_all_st
-        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == undefined_for_all_st
+        assert metrics[MEAN_AVERAGE_PRECISION] == approx(0.0)
+        assert metrics[PRECISION] == approx(0.0)
+        assert metrics[RECALL] == approx(0.0)
 
     def test_single_image_no_gt_one_pred(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [{"width": 640, "height": 480, "iscrowd": np.array([], dtype=bool)}]
         gt_objects_per_image = [
             {"boxes": np.zeros((0, 4)), "masks": None, "classes": np.zeros((0,)), "scores": None}
@@ -140,36 +118,23 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
-        zero_for_all_st = {st / 100.0: 0.0 for st in range(100)}
         assert metrics[PER_LABEL_METRICS] == {
-            0: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
-            1: {
-                AVERAGE_PRECISION: -1.0, PRECISION: 0.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: zero_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
-            2: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
+            0: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+            1: {AVERAGE_PRECISION: -1.0, PRECISION: 0.0, RECALL: -1.0},
+            2: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
         }
 
-        assert metrics[MEAN_AVERAGE_PRECISION] == -1.0
-        assert metrics[PRECISION] == 0.0
-        assert metrics[RECALL] == -1.0
-        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == zero_for_all_st
-        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == undefined_for_all_st
+        assert metrics[MEAN_AVERAGE_PRECISION] == approx(0.0)
+        assert metrics[PRECISION] == approx(0.0)
+        assert metrics[RECALL] == approx(0.0)
 
     def test_single_image_one_gt_no_pred1(self):
         # no predictions specified with empty prediction objects dictionary
 
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [{"width": 640, "height": 480, "iscrowd": np.array([False])}]
@@ -182,36 +147,23 @@
         predicted_objects_per_image = [{}]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
-        zero_for_all_st = {st / 100.0: 0.0 for st in range(100)}
         assert metrics[PER_LABEL_METRICS] == {
-            0: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
-            1: {
-                AVERAGE_PRECISION: 0.0, PRECISION: -1.0, RECALL: 0.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: zero_for_all_st
-            },
-            2: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
+            0: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+            1: {AVERAGE_PRECISION: approx(0.0), PRECISION: -1.0, RECALL: approx(0.0)},
+            2: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
         }
 
-        assert metrics[MEAN_AVERAGE_PRECISION] == 0.0
-        assert metrics[PRECISION] == -1.0
-        assert metrics[RECALL] == 0.0
-        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == undefined_for_all_st
-        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == zero_for_all_st
+        assert metrics[MEAN_AVERAGE_PRECISION] == approx(0.0)
+        assert metrics[PRECISION] == approx(0.0)
+        assert metrics[RECALL] == approx(0.0)
 
     def test_single_image_one_gt_no_pred2(self):
         # no predictions specified with prediction objects with empty boxes
 
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [{"width": 640, "height": 480, "iscrowd": np.array([False])}]
@@ -229,36 +181,23 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
-        zero_for_all_st = {st / 100.0: 0.0 for st in range(100)}
         assert metrics[PER_LABEL_METRICS] == {
-            0: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
-            1: {
-                AVERAGE_PRECISION: 0.0, PRECISION: -1.0, RECALL: 0.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: zero_for_all_st
-            },
-            2: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
+            0: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+            1: {AVERAGE_PRECISION: approx(0.0), PRECISION: -1.0, RECALL: approx(0.0)},
+            2: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
         }
 
-        assert metrics[MEAN_AVERAGE_PRECISION] == 0.0
-        assert metrics[PRECISION] == -1.0
-        assert metrics[RECALL] == 0.0
-        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == undefined_for_all_st
-        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == zero_for_all_st
+        assert metrics[MEAN_AVERAGE_PRECISION] == approx(0.0)
+        assert metrics[PRECISION] == approx(0.0)
+        assert metrics[RECALL] == approx(0.0)
 
     def test_single_image_one_gt_one_pred_perfect_overlap(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [{"width": 640, "height": 480, "iscrowd": np.array([False])}]
         gt_objects_per_image = [
             {
@@ -274,37 +213,23 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
-        precision_per_st = {st / 100.0: approx(1.0) if st <= 75 else -1.0 for st in range(100)}
-        recall_per_st = {st / 100.0: approx(1.0) if st <= 75 else 0.0 for st in range(100)}
-        assert metrics[PER_LABEL_METRICS] == {
-            0: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
-            1: {
-                AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0),
-                PRECISIONS_PER_SCORE_THRESHOLD: precision_per_st, RECALLS_PER_SCORE_THRESHOLD: recall_per_st
-            },
-            2: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
+        assert metrics[PER_LABEL_METRICS] == {
+            0: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+            1: {AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0)},
+            2: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
         }
 
         assert metrics[MEAN_AVERAGE_PRECISION] == approx(1.0)
         assert metrics[PRECISION] == approx(1.0)
         assert metrics[RECALL] == approx(1.0)
-        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == precision_per_st
-        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == recall_per_st
 
     def test_single_image_one_gt_one_pred_crowd(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [{"width": 640, "height": 480, "iscrowd": np.array([True])}]
         gt_objects_per_image = [
             {
@@ -320,36 +245,23 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
-        zero_for_all_st = {st / 100.0: 0.0 for st in range(100)}
         assert metrics[PER_LABEL_METRICS] == {
-            0: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
-            1: {
-                AVERAGE_PRECISION: -1.0, PRECISION: 0.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: zero_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
-            2: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
+            0: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+            1: {AVERAGE_PRECISION: -1.0, PRECISION: 0.0, RECALL: -1.0},
+            2: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
         }
 
-        assert metrics[MEAN_AVERAGE_PRECISION] == -1.0
-        assert metrics[PRECISION] == 0.0
-        assert metrics[RECALL] == -1.0
-        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == zero_for_all_st
-        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == undefined_for_all_st
+        assert metrics[MEAN_AVERAGE_PRECISION] == approx(0.0)
+        assert metrics[PRECISION] == approx(0.0)
+        assert metrics[RECALL] == approx(0.0)
 
     def test_single_image_one_gt_one_pred_different_class(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [{"width": 640, "height": 480, "iscrowd": np.array([False])}]
         gt_objects_per_image = [
             {
@@ -365,36 +277,23 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
-        zero_for_all_st = {st / 100.0: 0.0 for st in range(100)}
         assert metrics[PER_LABEL_METRICS] == {
-            0: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
-            1: {
-                AVERAGE_PRECISION: 0.0, PRECISION: -1.0, RECALL: 0.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: zero_for_all_st
-            },
-            2: {
-                AVERAGE_PRECISION: -1.0, PRECISION: 0.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: zero_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
+            0: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+            1: {AVERAGE_PRECISION: 0.0, PRECISION: -1.0, RECALL: 0.0},
+            2: {AVERAGE_PRECISION: -1.0, PRECISION: 0.0, RECALL: -1.0},
         }
 
-        assert metrics[MEAN_AVERAGE_PRECISION] == 0.0
-        assert metrics[PRECISION] == 0.0
-        assert metrics[RECALL] == 0.0
-        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == zero_for_all_st
-        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == zero_for_all_st
+        assert metrics[MEAN_AVERAGE_PRECISION] == approx(0.0)
+        assert metrics[PRECISION] == approx(0.0)
+        assert metrics[RECALL] == approx(0.0)
 
     def test_single_image_one_gt_one_pred_insufficient_overlap(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [{"width": 640, "height": 480, "iscrowd": np.array([False])}]
         gt_objects_per_image = [
             {
@@ -410,37 +309,23 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
-        zero_for_all_st = {st / 100.0: 0.0 for st in range(100)}
-        precision_per_st = {st / 100.0: 0.0 if st <= 75 else -1.0 for st in range(100)}
         assert metrics[PER_LABEL_METRICS] == {
-            0: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
-            1: {
-                AVERAGE_PRECISION: 0.0, PRECISION: 0.0, RECALL: 0.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: precision_per_st, RECALLS_PER_SCORE_THRESHOLD: zero_for_all_st
-            },
-            2: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
+            0: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+            1: {AVERAGE_PRECISION: 0.0, PRECISION: 0.0, RECALL: 0.0},
+            2: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
         }
 
-        assert metrics[MEAN_AVERAGE_PRECISION] == 0.0
-        assert metrics[PRECISION] == 0.0
-        assert metrics[RECALL] == 0.0
-        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == precision_per_st
-        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == zero_for_all_st
+        assert metrics[MEAN_AVERAGE_PRECISION] == approx(0.0)
+        assert metrics[PRECISION] == approx(0.0)
+        assert metrics[RECALL] == approx(0.0)
 
     def test_single_image_one_gt_one_pred_sufficient_overlap(self):
         # Like insufficient above, but with lower IOU threshold that makes the overlap sufficient.
         ive = IncrementalVocEvaluator(True, 3, 0.25)
 
         meta_info_per_image = [{"width": 640, "height": 480, "iscrowd": np.array([False])}]
         gt_objects_per_image = [
@@ -457,37 +342,23 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
-        precision_per_st = {st / 100.0: approx(1.0) if st <= 75 else -1.0 for st in range(100)}
-        recall_per_st = {st / 100.0: approx(1.0) if st <= 75 else 0.0 for st in range(100)}
-        assert metrics[PER_LABEL_METRICS] == {
-            0: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
-            1: {
-                AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0),
-                PRECISIONS_PER_SCORE_THRESHOLD: precision_per_st, RECALLS_PER_SCORE_THRESHOLD: recall_per_st
-            },
-            2: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
+        assert metrics[PER_LABEL_METRICS] == {
+            0: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+            1: {AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0)},
+            2: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
         }
 
         assert metrics[MEAN_AVERAGE_PRECISION] == approx(1.0)
         assert metrics[PRECISION] == approx(1.0)
         assert metrics[RECALL] == approx(1.0)
-        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == precision_per_st
-        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == recall_per_st
 
     def test_single_image_one_gt_one_pred_masks_small_overlap(self):
         ive = IncrementalVocEvaluator(False, 3, 0.1)
 
         # Two polygons roughly along the diagonals of a square.
         p1 = [0, 0, 35, 0, 200, 165, 200, 200, 165, 200, 0, 35, 0, 0]
         p2 = [200, 0, 165, 0, 0, 165, 0, 200, 35, 200, 200, 35, 200, 0]
@@ -507,37 +378,23 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics, task_is_detection=False)
 
-        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
-        precision_per_st = {st / 100.0: approx(1.0) if st <= 50 else -1.0 for st in range(100)}
-        recall_per_st = {st / 100.0: approx(1.0) if st <= 50 else 0.0 for st in range(100)}
-        assert metrics[PER_LABEL_METRICS] == {
-            0: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
-            1: {
-                AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0),
-                PRECISIONS_PER_SCORE_THRESHOLD: precision_per_st, RECALLS_PER_SCORE_THRESHOLD: recall_per_st
-            },
-            2: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
+        assert metrics[PER_LABEL_METRICS] == {
+            0: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+            1: {AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0)},
+            2: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
         }
 
         assert metrics[MEAN_AVERAGE_PRECISION] == approx(1.0)
         assert metrics[PRECISION] == approx(1.0)
         assert metrics[RECALL] == approx(1.0)
-        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == precision_per_st
-        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == recall_per_st
 
     def test_single_image_one_gt_one_pred_masks_zero_overlap(self):
         ive = IncrementalVocEvaluator(False, 3, 0.1)
 
         # Two completely disjoint polygons, each consisting of two squares placed on a diagonal.
         # p1 p2
         # p2 p1
@@ -565,37 +422,23 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics, task_is_detection=False)
 
-        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
-        zero_for_all_st = {st / 100.0: 0.0 for st in range(100)}
-        precision_per_st = {st / 100.0: 0.0 if st <= 50 else -1.0 for st in range(100)}
-        assert metrics[PER_LABEL_METRICS] == {
-            0: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
-            1: {
-                AVERAGE_PRECISION: 0.0, PRECISION: 0.0, RECALL: 0.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: precision_per_st, RECALLS_PER_SCORE_THRESHOLD: zero_for_all_st
-            },
-            2: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
+        assert metrics[PER_LABEL_METRICS] == {
+            0: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+            1: {AVERAGE_PRECISION: 0.0, PRECISION: 0.0, RECALL: 0.0},
+            2: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
         }
 
         assert metrics[MEAN_AVERAGE_PRECISION] == approx(0.0)
         assert metrics[PRECISION] == approx(0.0)
         assert metrics[RECALL] == approx(0.0)
-        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == precision_per_st
-        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == zero_for_all_st
 
     def test_single_image_one_gt_one_pred_not_clipped(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [{"width": 300, "height": 300, "iscrowd": np.array([False])}]
         gt_objects_per_image = [
             {
@@ -611,37 +454,23 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
-        zero_for_all_st = {st / 100.0: 0.0 for st in range(100)}
-        precision_per_st = {st / 100.0: 0.0 if st <= 75 else -1.0 for st in range(100)}
         assert metrics[PER_LABEL_METRICS] == {
-            0: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
-            1: {
-                AVERAGE_PRECISION: 0.0, PRECISION: 0.0, RECALL: 0.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: precision_per_st, RECALLS_PER_SCORE_THRESHOLD: zero_for_all_st
-            },
-            2: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
+            0: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+            1: {AVERAGE_PRECISION: 0.0, PRECISION: 0.0, RECALL: 0.0},
+            2: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
         }
 
-        assert metrics[MEAN_AVERAGE_PRECISION] == 0.0
-        assert metrics[PRECISION] == 0.0
-        assert metrics[RECALL] == 0.0
-        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == precision_per_st
-        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == zero_for_all_st
+        assert metrics[MEAN_AVERAGE_PRECISION] == approx(0.0)
+        assert metrics[PRECISION] == approx(0.0)
+        assert metrics[RECALL] == approx(0.0)
 
     def test_single_image_one_gt_one_pred_degenerate(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [{"width": 640, "height": 480, "iscrowd": np.array([False])}]
         gt_objects_per_image = [
             {
@@ -657,37 +486,23 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
-        zero_for_all_st = {st / 100.0: 0.0 for st in range(100)}
-        precision_per_st = {st / 100.0: 0.0 if st <= 50 else -1.0 for st in range(100)}
         assert metrics[PER_LABEL_METRICS] == {
-            0: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
-            1: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
-            2: {
-                AVERAGE_PRECISION: 0.0, PRECISION: 0.0, RECALL: 0.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: precision_per_st, RECALLS_PER_SCORE_THRESHOLD: zero_for_all_st
-            },
+            0: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+            1: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+            2: {AVERAGE_PRECISION: 0.0, PRECISION: 0.0, RECALL: 0.0},
         }
 
-        assert metrics[MEAN_AVERAGE_PRECISION] == 0.0
-        assert metrics[PRECISION] == 0.0
-        assert metrics[RECALL] == 0.0
-        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == precision_per_st
-        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == zero_for_all_st
+        assert metrics[MEAN_AVERAGE_PRECISION] == approx(0.0)
+        assert metrics[PRECISION] == approx(0.0)
+        assert metrics[RECALL] == approx(0.0)
 
     def test_single_image_two_gt_two_pred_good_overlap(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [{"width": 640, "height": 480, "iscrowd": np.array([False, False])}]
         gt_objects_per_image = [
             {
@@ -713,37 +528,23 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
-        precision_per_st = {st / 100.0: approx(1.0) for st in range(100)}
-        recall_per_st = {st / 100.0: approx(1.0) if st <= 88 else approx(0.5) for st in range(100)}
-        assert metrics[PER_LABEL_METRICS] == {
-            0: {
-                AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0),
-                PRECISIONS_PER_SCORE_THRESHOLD: precision_per_st, RECALLS_PER_SCORE_THRESHOLD: recall_per_st
-            },
-            1: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
-            2: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
+        assert metrics[PER_LABEL_METRICS] == {
+            0: {AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0)},
+            1: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+            2: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
         }
 
         assert metrics[MEAN_AVERAGE_PRECISION] == approx(1.0)
         assert metrics[PRECISION] == approx(1.0)
         assert metrics[RECALL] == approx(1.0)
-        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == precision_per_st
-        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == recall_per_st
 
     def test_single_image_two_gt_two_pred_good_overlap_different_class(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [{"width": 640, "height": 480, "iscrowd": np.array([False, False])}]
         gt_objects_per_image = [
             {
@@ -769,39 +570,23 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
-        zero_for_all_st = {st / 100.0: 0.0 for st in range(100)}
-        p1 = {st / 100.0: approx(1.0) for st in range(100)}
-        p2 = {st / 100.0: approx(0.5) if st <= 88 else approx(1.0) for st in range(100)}
-        recall_per_st = {st / 100.0: approx(0.5) for st in range(100)}
-        assert metrics[PER_LABEL_METRICS] == {
-            0: {
-                AVERAGE_PRECISION: approx(0.5), PRECISION: approx(1.0), RECALL: approx(0.5),
-                PRECISIONS_PER_SCORE_THRESHOLD: p1, RECALLS_PER_SCORE_THRESHOLD: recall_per_st
-            },
-            1: {
-                AVERAGE_PRECISION: -1.0, PRECISION: 0.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: zero_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
-            2: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
+        assert metrics[PER_LABEL_METRICS] == {
+            0: {AVERAGE_PRECISION: approx(0.5), PRECISION: approx(1.0), RECALL: approx(0.5)},
+            1: {AVERAGE_PRECISION: -1.0, PRECISION: 0.0, RECALL: -1.0},
+            2: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
         }
 
         assert metrics[MEAN_AVERAGE_PRECISION] == approx(0.5)
         assert metrics[PRECISION] == approx(0.5)
         assert metrics[RECALL] == approx(0.5)
-        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == p2
-        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == recall_per_st
 
     def test_single_image_two_gt_two_pred_one_match(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [{"width": 640, "height": 480, "iscrowd": np.array([False, False])}]
         gt_objects_per_image = [
             {
@@ -827,37 +612,23 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, pred_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
-        precision_per_st = {st / 100.0: approx(0.5) if st <= 75 else -1.0 for st in range(100)}
-        recall_per_st = {st / 100.0: approx(0.5) if st <= 75 else 0.0 for st in range(100)}
-        assert metrics[PER_LABEL_METRICS] == {
-            0: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
-            1: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
-            2: {
-                AVERAGE_PRECISION: approx(0.5), PRECISION: approx(0.5), RECALL: approx(0.5),
-                PRECISIONS_PER_SCORE_THRESHOLD: precision_per_st, RECALLS_PER_SCORE_THRESHOLD: recall_per_st
-            },
+        assert metrics[PER_LABEL_METRICS] == {
+            0: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+            1: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+            2: {AVERAGE_PRECISION: approx(0.5), PRECISION: approx(0.5), RECALL: approx(0.5)},
         }
 
         assert metrics[MEAN_AVERAGE_PRECISION] == approx(0.5)
         assert metrics[PRECISION] == approx(0.5)
         assert metrics[RECALL] == approx(0.5)
-        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == precision_per_st
-        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == recall_per_st
 
     def test_single_image_two_gt_two_pred_one_match_masks(self):
         ive = IncrementalVocEvaluator(False, 3, 0.5)
 
         meta_info_per_image = [{"width": 640, "height": 480, "iscrowd": np.array([False, False])}]
         gt_objects_per_image = [
             {
@@ -883,37 +654,23 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, pred_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics, task_is_detection=False)
 
-        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
-        precision_per_st = {st / 100.0: approx(0.5) if st <= 75 else -1.0 for st in range(100)}
-        recall_per_st = {st / 100.0: approx(0.5) if st <= 75 else 0.0 for st in range(100)}
-        assert metrics[PER_LABEL_METRICS] == {
-            0: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
-            1: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
-            2: {
-                AVERAGE_PRECISION: approx(0.5), PRECISION: approx(0.5), RECALL: approx(0.5),
-                PRECISIONS_PER_SCORE_THRESHOLD: precision_per_st, RECALLS_PER_SCORE_THRESHOLD: recall_per_st
-            },
+        assert metrics[PER_LABEL_METRICS] == {
+            0: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+            1: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+            2: {AVERAGE_PRECISION: approx(0.5), PRECISION: approx(0.5), RECALL: approx(0.5)},
         }
 
         assert metrics[MEAN_AVERAGE_PRECISION] == approx(0.5)
         assert metrics[PRECISION] == approx(0.5)
         assert metrics[RECALL] == approx(0.5)
-        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == precision_per_st
-        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == recall_per_st
 
     def test_single_image_1K_gt_1K_pred_random(self):
         ive = IncrementalVocEvaluator(True, 10, 0.5)
 
         meta_info_per_image = [{"width": 1600, "height": 1600, "iscrowd": np.array([False] * 1000)}]
 
         xs, ys = np.random.randint(0, 1600, size=(1000, 2)), np.random.randint(0, 1600, size=(1000, 2))
@@ -940,26 +697,18 @@
         for i in range(10):
             assert i in metrics[PER_LABEL_METRICS]
             m = metrics[PER_LABEL_METRICS][i]
 
             _check_valid_metric_value(m[AVERAGE_PRECISION])
             _check_valid_metric_value(m[PRECISION])
             _check_valid_metric_value(m[RECALL])
-            for precision_per_st in m[PRECISIONS_PER_SCORE_THRESHOLD].values():
-                _check_valid_metric_value(precision_per_st)
-            for recall_per_st in m[RECALLS_PER_SCORE_THRESHOLD].values():
-                _check_valid_metric_value(recall_per_st)
 
         _check_valid_metric_value(metrics[MEAN_AVERAGE_PRECISION])
         _check_valid_metric_value(metrics[PRECISION])
         _check_valid_metric_value(metrics[RECALL])
-        for precision_per_st in metrics[PRECISIONS_PER_SCORE_THRESHOLD].values():
-            _check_valid_metric_value(precision_per_st)
-        for recall_per_st in metrics[RECALLS_PER_SCORE_THRESHOLD].values():
-            _check_valid_metric_value(recall_per_st)
 
     def test_multi_image_one_gt_one_pred_good_overlap(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [
             {"width": 640, "height": 480, "iscrowd": np.array([False])},
             {"width": 1280, "height": 960, "iscrowd": np.array([False])}
@@ -998,36 +747,23 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
-        o = {st / 100.0: approx(1.0) for st in range(100)}
         assert metrics[PER_LABEL_METRICS] == {
-            0: {
-                AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0),
-                PRECISIONS_PER_SCORE_THRESHOLD: o, RECALLS_PER_SCORE_THRESHOLD: o
-            },
-            1: {
-                AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0),
-                PRECISIONS_PER_SCORE_THRESHOLD: o, RECALLS_PER_SCORE_THRESHOLD: o
-            },
-            2: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
+            0: {AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0)},
+            1: {AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0)},
+            2: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
         }
 
         assert metrics[MEAN_AVERAGE_PRECISION] == approx(1.0)
         assert metrics[PRECISION] == approx(1.0)
         assert metrics[RECALL] == approx(1.0)
-        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == o
-        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == o
 
     def test_multi_image_one_gt_one_pred_different_class(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [
             {"width": 640, "height": 480, "iscrowd": np.array([False])},
             {"width": 1280, "height": 960, "iscrowd": np.array([False])}
@@ -1074,36 +810,23 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
-        zero_for_all_st = {st / 100.0: 0.0 for st in range(100)}
         assert metrics[PER_LABEL_METRICS] == {
-            0: {
-                AVERAGE_PRECISION: 0.0, PRECISION: -1.0, RECALL: 0.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: zero_for_all_st
-            },
-            1: {
-                AVERAGE_PRECISION: 0.0, PRECISION: 0.0, RECALL: 0.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: zero_for_all_st, RECALLS_PER_SCORE_THRESHOLD: zero_for_all_st
-            },
-            2: {
-                AVERAGE_PRECISION: -1.0, PRECISION: 0.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: zero_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
+            0: {AVERAGE_PRECISION: 0.0, PRECISION: -1.0, RECALL: 0.0},
+            1: {AVERAGE_PRECISION: approx(0.0), PRECISION: approx(0.0), RECALL: approx(0.0)},
+            2: {AVERAGE_PRECISION: -1.0, PRECISION: 0.0, RECALL: -1.0},
         }
 
-        assert metrics[MEAN_AVERAGE_PRECISION] == 0.0
-        assert metrics[PRECISION] == 0.0
-        assert metrics[RECALL] == 0.0
-        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == zero_for_all_st
-        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == zero_for_all_st
+        assert metrics[MEAN_AVERAGE_PRECISION] == approx(0.0)
+        assert metrics[PRECISION] == approx(0.0)
+        assert metrics[RECALL] == approx(0.0)
 
     def test_multi_image_two_gt_two_pred_perfect_match(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [
             {"width": 400, "height": 500, "iscrowd": np.array([False, False])},
             {"width": 200, "height": 300, "iscrowd": np.array([False, False])}
@@ -1154,49 +877,23 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        p0 = {st / 100.0: approx(1.0) if st <= 80 else -1.0 for st in range(100)}
-        r0 = {st / 100.0: approx(1.0) if st <= 80 else 0.0 for st in range(100)}
-        p1 = {st / 100.0: approx(1.0) if st <= 90 else -1.0 for st in range(100)}
-        r1 = {st / 100.0: approx(1.0) if st <= 90 else 0.0 for st in range(100)}
-        p2 = {st / 100.0: approx(1.0) if st <= 70 else -1.0 for st in range(100)}
-        r2 = {st / 100.0: approx(1.0) if st <= 70 else 0.0 for st in range(100)}
-        precision_per_st = {st / 100.0: approx(1.0) if st <= 90 else -1.0 for st in range(100)}
-        recall_per_st = {
-            st / 100.0:
-                approx(1.0) if st <= 70
-                else approx(0.75) if st <= 80
-                else approx(0.5) if st <= 90
-                else 0.0
-            for st in range(100)
-        }
-        assert metrics[PER_LABEL_METRICS] == {
-            0: {
-                AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0),
-                PRECISIONS_PER_SCORE_THRESHOLD: p0, RECALLS_PER_SCORE_THRESHOLD: r0
-            },
-            1: {
-                AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0),
-                PRECISIONS_PER_SCORE_THRESHOLD: p1, RECALLS_PER_SCORE_THRESHOLD: r1
-            },
-            2: {
-                AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0),
-                PRECISIONS_PER_SCORE_THRESHOLD: p2, RECALLS_PER_SCORE_THRESHOLD: r2
-            },
+        assert metrics[PER_LABEL_METRICS] == {
+            0: {AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0)},
+            1: {AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0)},
+            2: {AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0)},
         }
 
         assert metrics[MEAN_AVERAGE_PRECISION] == approx(1.0)
         assert metrics[PRECISION] == approx(1.0)
         assert metrics[RECALL] == approx(1.0)
-        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == precision_per_st
-        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == recall_per_st
 
     def test_multi_image_three_gt_three_pred_single_match(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [
             {"width": 640, "height": 640, "iscrowd": np.array([False, False, False])},
             {"width": 6400, "height": 6400, "iscrowd": np.array([False, False, False])},
@@ -1269,36 +966,23 @@
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
         _13 = 1.0 / 3.0
-        precision_per_st = {st / 100.0: approx(1.0) if st <= 50 else -1.0 for st in range(100)}
-        recall_per_st = {st / 100.0: approx(_13) if st <= 50 else 0.0 for st in range(100)}
         assert metrics[PER_LABEL_METRICS] == {
-            0: {
-                AVERAGE_PRECISION: approx(_13), PRECISION: approx(1.0), RECALL: approx(_13),
-                PRECISIONS_PER_SCORE_THRESHOLD: precision_per_st, RECALLS_PER_SCORE_THRESHOLD: recall_per_st
-            },
-            1: {
-                AVERAGE_PRECISION: approx(_13), PRECISION: approx(1.0), RECALL: approx(_13),
-                PRECISIONS_PER_SCORE_THRESHOLD: precision_per_st, RECALLS_PER_SCORE_THRESHOLD: recall_per_st
-            },
-            2: {
-                AVERAGE_PRECISION: approx(_13), PRECISION: approx(1.0), RECALL: approx(_13),
-                PRECISIONS_PER_SCORE_THRESHOLD: precision_per_st, RECALLS_PER_SCORE_THRESHOLD: recall_per_st
-            },
+            0: {AVERAGE_PRECISION: approx(_13), PRECISION: approx(1.0), RECALL: approx(_13)},
+            1: {AVERAGE_PRECISION: approx(_13), PRECISION: approx(1.0), RECALL: approx(_13)},
+            2: {AVERAGE_PRECISION: approx(_13), PRECISION: approx(1.0), RECALL: approx(_13)},
         }
 
         assert metrics[MEAN_AVERAGE_PRECISION] == approx(_13)
         assert metrics[PRECISION] == approx(1.0)
         assert metrics[RECALL] == approx(_13)
-        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == precision_per_st
-        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == recall_per_st
 
     def test_multi_image_three_gt_three_pred_single_match_masks(self):
         ive = IncrementalVocEvaluator(False, 3, 0.5)
 
         meta_info_per_image = [
             {"width": 640, "height": 640, "iscrowd": np.array([False, False, False])},
             {"width": 6400, "height": 6400, "iscrowd": np.array([False, False, False])},
@@ -1371,36 +1055,23 @@
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics, task_is_detection=False)
 
         _13 = 1.0 / 3.0
-        precision_per_st = {st / 100.0: approx(1.0) if st <= 50 else -1.0 for st in range(100)}
-        recall_per_st = {st / 100.0: approx(_13) if st <= 50 else 0.0 for st in range(100)}
         assert metrics[PER_LABEL_METRICS] == {
-            0: {
-                AVERAGE_PRECISION: approx(_13), PRECISION: approx(1.0), RECALL: approx(_13),
-                PRECISIONS_PER_SCORE_THRESHOLD: precision_per_st, RECALLS_PER_SCORE_THRESHOLD: recall_per_st
-            },
-            1: {
-                AVERAGE_PRECISION: approx(_13), PRECISION: approx(1.0), RECALL: approx(_13),
-                PRECISIONS_PER_SCORE_THRESHOLD: precision_per_st, RECALLS_PER_SCORE_THRESHOLD: recall_per_st
-            },
-            2: {
-                AVERAGE_PRECISION: approx(_13), PRECISION: approx(1.0), RECALL: approx(_13),
-                PRECISIONS_PER_SCORE_THRESHOLD: precision_per_st, RECALLS_PER_SCORE_THRESHOLD: recall_per_st
-            },
+            0: {AVERAGE_PRECISION: approx(_13), PRECISION: approx(1.0), RECALL: approx(_13)},
+            1: {AVERAGE_PRECISION: approx(_13), PRECISION: approx(1.0), RECALL: approx(_13)},
+            2: {AVERAGE_PRECISION: approx(_13), PRECISION: approx(1.0), RECALL: approx(_13)},
         }
 
         assert metrics[MEAN_AVERAGE_PRECISION] == approx(_13)
         assert metrics[PRECISION] == approx(1.0)
         assert metrics[RECALL] == approx(_13)
-        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == precision_per_st
-        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == recall_per_st
 
     def test_set_from_one_other(self):
         # First and only evaluator.
         ive1 = IncrementalVocEvaluator(True, 3, 0.5)
         meta_info_per_image = [{"width": 640, "height": 480, "iscrowd": np.array([False])}]
         gt_objects_per_image = [
             {
@@ -1428,39 +1099,25 @@
         ive = IncrementalVocEvaluator(True, 3, 0.5)
         ive.set_from_others([ive1])
         metrics = ive.compute_metrics()
 
         # Check combined evaluator.
         _check_metrics_keys(metrics)
 
-        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
-        precision_per_st = {st / 100.0: approx(1.0) if st <= 50 else -1.0 for st in range(100)}
-        recall_per_st = {st / 100.0: approx(1.0) if st <= 50 else 0.0 for st in range(100)}
-        assert metrics[PER_LABEL_METRICS] == {
-            0: {
-                AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0),
-                PRECISIONS_PER_SCORE_THRESHOLD: precision_per_st, RECALLS_PER_SCORE_THRESHOLD: recall_per_st
-            },
-            1: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
-            2: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
+        assert metrics[PER_LABEL_METRICS] == {
+            0: {AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0)},
+            1: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+            2: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
         }
 
         assert metrics[MEAN_AVERAGE_PRECISION] == approx(1.0)
         assert metrics[PRECISION] == approx(1.0)
         assert metrics[RECALL] == approx(1.0)
-        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == precision_per_st
-        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == recall_per_st
 
-        assert _image_level_base(metrics) == {
+        assert metrics[IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS] == {
             AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0)
         }
 
         cm1 = [[1, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]
         cm2 = [[0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 0]]
         assert metrics[CONFUSION_MATRICES_PER_SCORE_THRESHOLD] == {
             st: cm1 if st <= 0.5 else cm2
@@ -1527,53 +1184,26 @@
         ive = IncrementalVocEvaluator(True, 3, 0.5)
         ive.set_from_others([ive1, ive2])
         metrics = ive.compute_metrics()
 
         # Check combined evaluator.
         _check_metrics_keys(metrics)
 
-        p0 = {st / 100.0: approx(1.0) if st <= 50 else -1.0 for st in range(100)}
-        r0 = {st / 100.0: approx(1.0) if st <= 50 else 0.0 for st in range(100)}
-        p1 = {st / 100.0: approx(0.5) if st <= 25 else approx(1.0) if st <= 75 else -1.0 for st in range(100)}
-        r1 = {st / 100.0: approx(1.0) if st <= 75 else 0.0 for st in range(100)}
-        p2 = {st / 100.0: 0.0 for st in range(100)}
-        r2 = {st / 100.0: 0.0 for st in range(100)}
-        assert metrics[PER_LABEL_METRICS] == {
-            0: {
-                AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0),
-                PRECISIONS_PER_SCORE_THRESHOLD: p0, RECALLS_PER_SCORE_THRESHOLD: r0
-            },
-            1: {
-                AVERAGE_PRECISION: approx(1.0), PRECISION: approx(0.5), RECALL: approx(1.0),
-                PRECISIONS_PER_SCORE_THRESHOLD: p1, RECALLS_PER_SCORE_THRESHOLD: r1
-            },
-            2: {
-                AVERAGE_PRECISION: approx(0.0), PRECISION: approx(0.0), RECALL: approx(0.0),
-                PRECISIONS_PER_SCORE_THRESHOLD: p2, RECALLS_PER_SCORE_THRESHOLD: r2
-            },
+        assert metrics[PER_LABEL_METRICS] == {
+            0: {AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0)},
+            1: {AVERAGE_PRECISION: approx(1.0), PRECISION: approx(0.5), RECALL: approx(1.0)},
+            2: {AVERAGE_PRECISION: approx(0.0), PRECISION: approx(0.0), RECALL: approx(0.0)},
         }
 
         _23 = 2.0 / 3.0
-        precision_per_st = {
-            st / 100.0:
-                approx(3.0 / 5.0) if st <= 25
-                else approx(3.0 / 4.0) if st <= 50
-                else approx(1.0 / 2.0) if st <= 75
-                else 0.0 for st in range(100)
-        }
-        recall_per_st = {
-            st / 100.0: approx(3.0 / 4.0) if st <= 50 else approx(1.0 / 4.0) if st <= 75 else 0.0 for st in range(100)
-        }
         assert metrics[MEAN_AVERAGE_PRECISION] == approx(_23)
         assert metrics[PRECISION] == approx(0.5)
         assert metrics[RECALL] == approx(_23)
-        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == precision_per_st
-        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == recall_per_st
 
-        assert _image_level_base(metrics) == {
+        assert metrics[IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS] == {
             AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0)
         }
 
         cm1 = [[2, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 1]]
         cm2 = [[0, 0, 0, 2], [0, 1, 0, 0], [0, 0, 0, 1]]
         cm3 = [[0, 0, 0, 2], [0, 0, 0, 1], [0, 0, 0, 1]]
         assert metrics[CONFUSION_MATRICES_PER_SCORE_THRESHOLD] == {
@@ -1658,43 +1288,25 @@
         ive = IncrementalVocEvaluator(True, 3, 0.5)
         ive.set_from_others(ives)
         metrics = ive.compute_metrics()
 
         # Check combined evaluator.
         _check_metrics_keys(metrics)
 
-        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
-        p0 = {st / 100.0: approx(1.0) if st <= 50 else -1.0 for st in range(100)}
-        r0 = {st / 100.0: approx(0.5) if st <= 50 else 0.0 for st in range(100)}
-        p1 = {st / 100.0: 0.0 if st <= 50 else -1.0 for st in range(100)}
-        r1 = {st / 100.0: 0.0 for st in range(100)}
         assert metrics[PER_LABEL_METRICS] == {
-            0: {
-                AVERAGE_PRECISION: approx(0.5), PRECISION: approx(1.0), RECALL: approx(0.5),
-                PRECISIONS_PER_SCORE_THRESHOLD: p0, RECALLS_PER_SCORE_THRESHOLD: r0
-            },
-            1: {
-                AVERAGE_PRECISION: approx(0.0), PRECISION: approx(0.0), RECALL: approx(0.0),
-                PRECISIONS_PER_SCORE_THRESHOLD: p1, RECALLS_PER_SCORE_THRESHOLD: r1
-            },
-            2: {
-                AVERAGE_PRECISION: approx(-1.0), PRECISION: approx(-1.0), RECALL: approx(-1.0),
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
-            },
+            0: {AVERAGE_PRECISION: approx(0.5), PRECISION: approx(1.0), RECALL: approx(0.5)},
+            1: {AVERAGE_PRECISION: approx(0.0), PRECISION: approx(0.0), RECALL: approx(0.0)},
+            2: {AVERAGE_PRECISION: approx(-1.0), PRECISION: approx(-1.0), RECALL: approx(-1.0)},
         }
 
-        precision_per_st = {st / 100.0: approx(1.0 / 3.0) if st <= 50 else -1.0 for st in range(100)}
-        recall_per_st = {st / 100.0: approx(1.0 / 3.0) if st <= 50 else 0.0 for st in range(100)}
         assert metrics[MEAN_AVERAGE_PRECISION] == approx(0.25)
         assert metrics[PRECISION] == approx(0.5)
         assert metrics[RECALL] == approx(0.25)
-        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == precision_per_st
-        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == recall_per_st
 
-        assert _image_level_base(metrics) == {
+        assert metrics[IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS] == {
             AVERAGE_PRECISION: approx(1.0 / 9.0), PRECISION: approx(1.0 / 3.0), RECALL: approx(1.0 / 3.0)
         }
 
         cm1 = [[5, 5, 0, 0], [0, 0, 0, 5], [0, 0, 0, 0]]
         cm2 = [[0, 0, 0, 10], [0, 0, 0, 5], [0, 0, 0, 0]]
         assert metrics[CONFUSION_MATRICES_PER_SCORE_THRESHOLD] == {
             st: cm1 if st <= 0.5 else cm2
@@ -1814,15 +1426,15 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        assert _image_level_base(metrics) == {
+        assert metrics[IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS] == {
             AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0)
         }
 
     def test_single_image_one_gt_two_pred_image_level(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [{"width": 640, "height": 480, "iscrowd": np.array([False])}]
@@ -1849,15 +1461,15 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        assert _image_level_base(metrics) == {
+        assert metrics[IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS] == {
             AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0)
         }
 
     def test_single_image_one_gt_one_pred_no_match_image_level(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [{"width": 640, "height": 480, "iscrowd": np.array([False])}]
@@ -1875,15 +1487,15 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        assert _image_level_base(metrics) == {
+        assert metrics[IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS] == {
             AVERAGE_PRECISION: approx(0.0), PRECISION: approx(0.0), RECALL: approx(0.0)
         }
 
     def test_two_images_multi_gt_multi_pred_image_level(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [
@@ -1921,15 +1533,15 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        assert _image_level_base(metrics) == {
+        assert metrics[IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS] == {
             AVERAGE_PRECISION: approx(0.25), PRECISION: approx(0.5), RECALL: approx(0.5)
         }
 
     def test_four_images_multi_gt_multi_pred_image_level(self):
         ive = IncrementalVocEvaluator(True, 3, 0.5)
 
         meta_info_per_image = [
@@ -1991,15 +1603,15 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        assert _image_level_base(metrics) == {
+        assert metrics[IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS] == {
             AVERAGE_PRECISION: approx(1.0 / 6.0), PRECISION: approx(0.5), RECALL: approx(1.0 / 3.0)
         }
 
     @pytest.mark.parametrize("num_classes", [0, 1])
     def test_three_images_no_gt_no_pred(self, num_classes):
         ive = IncrementalVocEvaluator(True, num_classes, 0.5)
 
@@ -2017,30 +1629,27 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        undefined_for_all_st = {st / 100.0: -1.0 for st in range(100)}
-        assert metrics[PER_LABEL_METRICS] == {
-            i: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: undefined_for_all_st, RECALLS_PER_SCORE_THRESHOLD: undefined_for_all_st
+        if num_classes == 0:
+            assert metrics[PER_LABEL_METRICS] == {}
+        else:
+            assert metrics[PER_LABEL_METRICS] == {
+                i: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0}
+                for i in range(num_classes)
             }
-            for i in range(num_classes)
-        }
 
-        assert metrics[MEAN_AVERAGE_PRECISION] == -1.0
-        assert metrics[PRECISION] == -1.0
-        assert metrics[RECALL] == -1.0
-        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == undefined_for_all_st
-        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == undefined_for_all_st
+        assert metrics[MEAN_AVERAGE_PRECISION] == approx(0.0)
+        assert metrics[PRECISION] == approx(0.0)
+        assert metrics[RECALL] == approx(0.0)
 
-        assert _image_level_base(metrics) == {
+        assert metrics[IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS] == {
             AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0
         }
 
         assert metrics[CONFUSION_MATRICES_PER_SCORE_THRESHOLD] == {
             -1.0: [] if num_classes == 0 else [[0, 0]]
         }
 
@@ -2091,15 +1700,15 @@
         ]
         ive.evaluate_batch(gt_objects_per_image, predicted_objects_per_image, meta_info_per_image)
 
         metrics = ive.compute_metrics()
 
         _check_metrics_keys(metrics)
 
-        assert _image_level_base(metrics) == {
+        assert metrics[IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS] == {
             AVERAGE_PRECISION: approx(0.5 / 9.0), PRECISION: approx(1.0 / 9.0), RECALL: approx(0.5)
         }
 
     def test_multi_image_random_gt_pred_image_level(self):
         np.random.seed(42)
 
         num_images, num_boxes_per_image = 40_000, 25
```

## tests/object_detection_tests/test_metric_computation_utils.py

```diff
@@ -5,69 +5,27 @@
 
 from pytest import approx
 
 from azureml.automl.dnn.vision.common.constants import MetricsLiterals
 from azureml.automl.dnn.vision.common.exceptions import AutoMLVisionSystemException
 from azureml.automl.dnn.vision.object_detection.common import masktools
 from azureml.automl.dnn.vision.object_detection.eval.metric_computation_utils import _map_score_voc_11_point_metric, \
-    _map_score_voc_auc, _SCORE_THRESHOLDS_FINE, _SCORE_THRESHOLDS_COARSE, calculate_pr_metrics, \
-    calculate_confusion_matrices, match_objects
+    _map_score_voc_auc, calculate_pr_metrics, calculate_confusion_matrices, match_objects
 
 
 PRECISION, RECALL = MetricsLiterals.PRECISION, MetricsLiterals.RECALL
 AVERAGE_PRECISION, MEAN_AVERAGE_PRECISION = MetricsLiterals.AVERAGE_PRECISION, MetricsLiterals.MEAN_AVERAGE_PRECISION
-PRECISIONS_PER_SCORE_THRESHOLD = MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD
-RECALLS_PER_SCORE_THRESHOLD = MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD
 
 
 def _xyxy2xywh(box):
     return [
         float(box[0]), float(box[1]), float(box[2]) - float(box[0]), float(box[3]) - float(box[1])
     ]
 
 
-def _fill_prpst_100_points(precisions_recalls_per_score_threshold):
-    new_precisions_per_score_threshold, new_recalls_per_score_threshold = {}, {}
-
-    precisions_per_score_threshold = {st: pr[0] for st, pr in precisions_recalls_per_score_threshold.items()}
-    recalls_per_score_threshold = {st: pr[1] for st, pr in precisions_recalls_per_score_threshold.items()}
-    for i in range(100):
-        score_threshold = i / 100.0
-
-        first_greater_equal_score_threshold = None
-        for st in precisions_recalls_per_score_threshold:
-            if st >= score_threshold:
-                if (first_greater_equal_score_threshold is None) or (st < first_greater_equal_score_threshold):
-                    first_greater_equal_score_threshold = st
-
-        new_precisions_per_score_threshold[score_threshold] = precisions_per_score_threshold.get(
-            first_greater_equal_score_threshold, -1.0
-        )
-        new_recalls_per_score_threshold[score_threshold] = recalls_per_score_threshold.get(
-            first_greater_equal_score_threshold, 0.0
-        )
-
-    return new_precisions_per_score_threshold, new_recalls_per_score_threshold
-
-
-def _check_prpst_equal(
-    precisions_per_score_threshold1, recalls_per_score_threshold1,
-    precisions_per_score_threshold2, recalls_per_score_threshold2
-):
-    score_thresholds1 = set(precisions_per_score_threshold1.keys()).union(recalls_per_score_threshold1.keys())
-    score_thresholds2 = set(precisions_per_score_threshold2.keys()).union(recalls_per_score_threshold2.keys())
-    assert score_thresholds1 == score_thresholds2
-
-    for st in score_thresholds1:
-        p1, r1 = precisions_per_score_threshold1[st], recalls_per_score_threshold1[st]
-        p2, r2 = precisions_per_score_threshold2[st], recalls_per_score_threshold2[st]
-        np.testing.assert_almost_equal(p1, p2, decimal=6)
-        np.testing.assert_almost_equal(r1, r2, decimal=6)
-
-
 def _fill_cmpst_10_points(num_gt_objects_per_class, confusion_matrices_per_score_threshold):
     num_classes = len(num_gt_objects_per_class)
 
     new_confusion_matrices_per_score_threshold = {}
     for i in range(10):
         score_threshold = i / 10.0
 
@@ -340,163 +298,100 @@
         tp_fp_labels = np.array([])
         scores = np.array([])
 
         metrics = calculate_pr_metrics(num_gt_boxes, tp_fp_labels, scores, None, False, -1.0)
         assert metrics[AVERAGE_PRECISION] == -1.0
         assert metrics[PRECISION] == -1.0
         assert metrics[RECALL] == -1.0
-        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == {st: -1.0 for st in _SCORE_THRESHOLDS_FINE}
-        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == {st: -1.0 for st in _SCORE_THRESHOLDS_FINE}
 
-    def test_pr_metrics_one_gt_no_pred(self):
-        num_gt_boxes = 1
-        tp_fp_labels = np.array([])
-        scores = np.array([])
-
-        metrics = calculate_pr_metrics(num_gt_boxes, tp_fp_labels, scores, None, False, -1.0)
-        assert metrics[AVERAGE_PRECISION] == 0.0
-        assert metrics[PRECISION] == -1.0
-        assert metrics[RECALL] == 0.0
-        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == {st: -1.0 for st in _SCORE_THRESHOLDS_FINE}
-        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == {st: 0.0 for st in _SCORE_THRESHOLDS_FINE}
-
-    def test_pr_metrics_no_gt_one_pred(self):
-        num_gt_boxes = 0
-        tp_fp_labels = np.array([0])
-        scores = np.array([0.3])
-
-        metrics = calculate_pr_metrics(num_gt_boxes, tp_fp_labels, scores, None, False, -1.0)
-        assert metrics[AVERAGE_PRECISION] == -1.0
-        assert metrics[PRECISION] == 0.0
-        assert metrics[RECALL] == -1.0
-        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == {st: 0.0 for st in _SCORE_THRESHOLDS_FINE}
-        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == {st: -1.0 for st in _SCORE_THRESHOLDS_FINE}
-
-    def test_pr_metrics_one_gt_two_pred1(self):
+    def test_pr_metrics_one_gt_two_pred(self):
         num_gt_boxes = 1
         tp_fp_labels = np.array([0, 1])
         scores = np.array([0.3, 0.9])
 
         metrics = calculate_pr_metrics(num_gt_boxes, tp_fp_labels, scores, None, False, -1.0)
         assert metrics[AVERAGE_PRECISION] == approx(1.0)
         assert metrics[PRECISION] == approx(0.5)
         assert metrics[RECALL] == approx(1.0)
-        _check_prpst_equal(
-            metrics[PRECISIONS_PER_SCORE_THRESHOLD], metrics[RECALLS_PER_SCORE_THRESHOLD],
-            *_fill_prpst_100_points({0.3: [0.5, 1.0], 0.9: [1.0, 1.0]})
-        )
 
     def test_pr_metrics_one_gt_two_pred_image_level1(self):
-        num_images_with_gt_boxes = 1
+        num_gt_boxes = 1
         tp_fp_labels = np.array([0, 1])
         scores = np.array([0.3, 0.9])
         image_indexes = np.array([0, 0])
 
-        metrics = calculate_pr_metrics(num_images_with_gt_boxes, tp_fp_labels, scores, image_indexes, False, -1.0)
+        metrics = calculate_pr_metrics(num_gt_boxes, tp_fp_labels, scores, image_indexes, False, -1.0)
         assert metrics[AVERAGE_PRECISION] == approx(1.0)
         assert metrics[PRECISION] == approx(1.0)
         assert metrics[RECALL] == approx(1.0)
-        _check_prpst_equal(
-            metrics[PRECISIONS_PER_SCORE_THRESHOLD], metrics[RECALLS_PER_SCORE_THRESHOLD],
-            *_fill_prpst_100_points({0.3: [1.0, 1.0], 0.9: [1.0, 1.0]})
-        )
 
     def test_pr_metrics_one_gt_two_pred_image_level2(self):
-        num_images_with_gt_boxes = 1
+        num_gt_boxes = 1
         tp_fp_labels = np.array([0, 1])
         scores = np.array([0.9, 0.3])
         image_indexes = np.array([0, 1])
 
-        metrics = calculate_pr_metrics(num_images_with_gt_boxes, tp_fp_labels, scores, image_indexes, False, -1.0)
+        metrics = calculate_pr_metrics(num_gt_boxes, tp_fp_labels, scores, image_indexes, False, -1.0)
         assert metrics[AVERAGE_PRECISION] == approx(0.5)
         assert metrics[PRECISION] == approx(0.5)
         assert metrics[RECALL] == approx(1.0)
-        _check_prpst_equal(
-            metrics[PRECISIONS_PER_SCORE_THRESHOLD], metrics[RECALLS_PER_SCORE_THRESHOLD],
-            *_fill_prpst_100_points({0.3: [0.5, 1.0], 0.9: [0.0, 0.0]})
-        )
 
     def test_pr_metrics_two_gt_two_pred(self):
         num_gt_boxes = 2
         tp_fp_labels = np.array([1, 1])
         scores = np.array([0.3, 0.9])
 
         metrics = calculate_pr_metrics(num_gt_boxes, tp_fp_labels, scores, None, False, -1.0)
         assert metrics[AVERAGE_PRECISION] == approx(1.0)
         assert metrics[PRECISION] == approx(1.0)
         assert metrics[RECALL] == approx(1.0)
-        _check_prpst_equal(
-            metrics[PRECISIONS_PER_SCORE_THRESHOLD], metrics[RECALLS_PER_SCORE_THRESHOLD],
-            *_fill_prpst_100_points({0.3: [1.0, 1.0], 0.9: [1.0, 0.5]})
-        )
 
     def test_pr_metrics_two_gt_two_pred_image_level(self):
-        num_images_with_gt_boxes = 2
+        num_gt_boxes = 2
         tp_fp_labels = np.array([1, 1])
         scores = np.array([0.3, 0.9])
         image_indexes = np.array([0, 1])
 
-        metrics = calculate_pr_metrics(num_images_with_gt_boxes, tp_fp_labels, scores, image_indexes, False, -1.0)
+        metrics = calculate_pr_metrics(num_gt_boxes, tp_fp_labels, scores, image_indexes, False, -1.0)
         assert metrics[AVERAGE_PRECISION] == approx(1.0)
         assert metrics[PRECISION] == approx(1.0)
         assert metrics[RECALL] == approx(1.0)
-        _check_prpst_equal(
-            metrics[PRECISIONS_PER_SCORE_THRESHOLD], metrics[RECALLS_PER_SCORE_THRESHOLD],
-            *_fill_prpst_100_points({0.3: [1.0, 1.0], 0.9: [1.0, 0.5]})
-        )
 
     def test_pr_metrics_four_gt_one_pred_other(self):
         num_gt_boxes = 4
         tp_fp_labels = np.array([0, 1, 1, 2])
         scores = np.array([0.4, 0.2, 0.8, 0.6])
 
         metrics = calculate_pr_metrics(num_gt_boxes, tp_fp_labels, scores, None, False, -1.0)
         assert metrics[AVERAGE_PRECISION] == approx(0.25 * 1.0 + 0.25 * (2.0 / 3.0))
         assert metrics[PRECISION] == approx(2.0 / 3.0)
         assert metrics[RECALL] == approx(0.5)
-        _check_prpst_equal(
-            metrics[PRECISIONS_PER_SCORE_THRESHOLD], metrics[RECALLS_PER_SCORE_THRESHOLD],
-            *_fill_prpst_100_points({0.2: [2.0 / 3.0, 0.5], 0.4: [0.5, 0.25], 0.6: [1.0, 0.25], 0.8: [1.0, 0.25]})
-        )
 
     def test_pr_metrics_two_gt_two_pred_other(self):
         num_gt_boxes = 2
         tp_fp_labels = np.array([2, 2])
         scores = np.array([0.25, 0.75])
 
         metrics = calculate_pr_metrics(num_gt_boxes, tp_fp_labels, scores, None, False, -1.0)
         assert metrics[AVERAGE_PRECISION] == approx(0.0)
         assert metrics[PRECISION] == approx(0.0)
         assert metrics[RECALL] == approx(0.0)
-        _check_prpst_equal(
-            metrics[PRECISIONS_PER_SCORE_THRESHOLD], metrics[RECALLS_PER_SCORE_THRESHOLD],
-            *_fill_prpst_100_points({0.25: [0.0, 0.0], 0.75: [0.0, 0.0]})
-        )
 
     def test_pr_metrics_11_point(self):
         num_gt_boxes = 3
 
         tp_fp_labels = np.array([1, 0, 1, 0, 1])
         scores = np.array([0.1, 0.2, 0.3, 0.4, 0.5])
 
         tp_fp_labels, scores = np.array(tp_fp_labels), np.array(scores)
 
         metrics = calculate_pr_metrics(num_gt_boxes, tp_fp_labels, scores, None, True, -1.0)
         assert metrics[AVERAGE_PRECISION] == approx((4 * 1.0 + 3 * (2.0 / 3.0) + 4 * 0.6) / 11.0)
         assert metrics[PRECISION] == approx(0.6)
         assert metrics[RECALL] == approx(1.0)
-        _check_prpst_equal(
-            metrics[PRECISIONS_PER_SCORE_THRESHOLD], metrics[RECALLS_PER_SCORE_THRESHOLD],
-            *_fill_prpst_100_points(
-                {
-                    0.1: [0.6, 1.0], 0.2: [0.5, 2.0 / 3.0], 0.3: [2.0 / 3.0, 2.0 / 3.0], 0.4: [0.5, 1.0 / 3.0],
-                    0.5: [1.0, 1.0 / 3.0]
-                }
-            )
-        )
 
     def test_confusion_matrices_diagonal(self):
         confusion_matrices_per_score_threshold = calculate_confusion_matrices(
             [1, 1, 1], np.array([[0, 0, 0.5], [1, 1, 0.6], [2, 2, 0.7]])
         )
 
         _check_cmpst_equal(confusion_matrices_per_score_threshold, _fill_cmpst_10_points(
@@ -580,10 +475,7 @@
             assert (cm.shape == cm_min.shape) and (cm.shape == cm_max.shape)
             assert (cm >= cm_min).all() and (cm <= cm_max).all()
             if cm_previous is not None:
                 assert cm.shape == cm_previous.shape
                 assert (cm[:, :-1] <= cm_previous[:, :-1]).all()
                 assert (cm[:, -1] >= cm_previous[:, -1]).all()
             cm_previous = cm
-
-    def test_score_thresholds_relation(self):
-        assert set(_SCORE_THRESHOLDS_FINE).issuperset(set(_SCORE_THRESHOLDS_COARSE))
```

## tests/object_detection_tests/test_object_detection_instance_segmentation_evaluator.py

```diff
@@ -17,16 +17,14 @@
 from azureml.automl.dnn.vision.object_detection_yolo.eval.yolo_evaluator import YoloEvaluator
 from azureml.automl.dnn.vision.object_detection_yolo.utils.utils import xyxy2xywh
 from tests.common.run_mock import ObjectDetectionDatasetMock
 
 
 PRECISION, RECALL = MetricsLiterals.PRECISION, MetricsLiterals.RECALL
 AVERAGE_PRECISION, MEAN_AVERAGE_PRECISION = MetricsLiterals.AVERAGE_PRECISION, MetricsLiterals.MEAN_AVERAGE_PRECISION
-PRECISIONS_PER_SCORE_THRESHOLD = MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD
-RECALLS_PER_SCORE_THRESHOLD = MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD
 PER_LABEL_METRICS = MetricsLiterals.PER_LABEL_METRICS
 IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS = MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS
 CONFUSION_MATRICES_PER_SCORE_THRESHOLD = MetricsLiterals.CONFUSION_MATRICES_PER_SCORE_THRESHOLD
 
 
 def convert_to_yolo_format(predicted_objects_per_image, gt_objects_per_image, meta_info_per_image):
     yolo_predtictions = []
@@ -70,14 +68,15 @@
                       "labels": torch.tensor([1])},
                      {"areas": [60000], "iscrowd": [0], "filename": "image_1.jpg",
                       "height": 640, "width": 480,
                       "original_width": 640, "original_height": 480
                       }
                       )]
     if eval_type == ValidationMetricType.COCO:
+
         dataset = ObjectDetectionDatasetMock(dataset_items, 3)
         dataset_wrapper = CommonObjectDetectionDatasetWrapper(dataset, DatasetProcessingType.IMAGES)
         val_index_map = dataset._classes
     else:
         dataset_wrapper = None
         val_index_map = ['1', '2', '3']
 
@@ -120,36 +119,23 @@
                              evaluator.val_metric_type,
                              evaluator.coco_index, ive,
                              metrics, {},
                              evaluator.coco_metric_time, evaluator.voc_metric_time,
                              evaluator.primary_metric, is_train=False)
 
     assert metrics[MEAN_AVERAGE_PRECISION] == approx(1.0)
+
     if eval_type == ValidationMetricType.VOC:
-        p = {st / 100.0: approx(1.0) if st <= 75 else -1.0 for st in range(100)}
-        r = {st / 100.0: approx(1.0) if st <= 75 else 0.0 for st in range(100)}
         assert metrics[PRECISION] == approx(1.0)
         assert metrics[RECALL] == approx(1.0)
-        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == p
-        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == r
 
-        u = {st / 100.0: -1.0 for st in range(100)}
         assert metrics[PER_LABEL_METRICS] == {
-            0: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: u, RECALLS_PER_SCORE_THRESHOLD: u,
-            },
-            1: {
-                AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0),
-                PRECISIONS_PER_SCORE_THRESHOLD: p, RECALLS_PER_SCORE_THRESHOLD: r,
-            },
-            2: {
-                AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0,
-                PRECISIONS_PER_SCORE_THRESHOLD: u, RECALLS_PER_SCORE_THRESHOLD: u,
-            },
+            0: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
+            1: {AVERAGE_PRECISION: approx(1.0), PRECISION: approx(1.0), RECALL: approx(1.0)},
+            2: {AVERAGE_PRECISION: -1.0, PRECISION: -1.0, RECALL: -1.0},
         }
 
         assert metrics[IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS] == {
             PRECISION: approx(1.0), RECALL: approx(1.0), AVERAGE_PRECISION: approx(1.0)
         }
 
         cm1 = [[0, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 0]]
@@ -271,36 +257,23 @@
                              evaluator.val_metric_type,
                              evaluator.coco_index, ive,
                              metrics, {},
                              evaluator.coco_metric_time, evaluator.voc_metric_time,
                              evaluator.primary_metric, is_train=False)
 
     _13 = 1.0 / 3.0
-    assert metrics[MEAN_AVERAGE_PRECISION] == approx(_13, rel=1e-2)
+    assert metrics[MEAN_AVERAGE_PRECISION] == approx(_13, rel=1e-1)
     if eval_type == ValidationMetricType.VOC:
-        p = {st / 100.0: approx(1.0) if st <= 50 else -1.0 for st in range(100)}
-        r = {st / 100.0: approx(_13, abs=1e-5) if st <= 50 else 0.0 for st in range(100)}
         assert metrics[PRECISION] == approx(1.0)
-        assert metrics[RECALL] == approx(_13, abs=1e-5)
-        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == p
-        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == r
+        assert metrics[RECALL] == approx(_13, rel=1e-3)
 
         assert metrics[PER_LABEL_METRICS] == {
-            0: {
-                AVERAGE_PRECISION: approx(_13, abs=1e-5), PRECISION: approx(1.0), RECALL: approx(_13, abs=1e-5),
-                PRECISIONS_PER_SCORE_THRESHOLD: p, RECALLS_PER_SCORE_THRESHOLD: r,
-            },
-            1: {
-                AVERAGE_PRECISION: approx(_13, abs=1e-5), PRECISION: approx(1.0), RECALL: approx(_13, abs=1e-5),
-                PRECISIONS_PER_SCORE_THRESHOLD: p, RECALLS_PER_SCORE_THRESHOLD: r,
-            },
-            2: {
-                AVERAGE_PRECISION: approx(_13, abs=1e-5), PRECISION: approx(1.0), RECALL: approx(_13, abs=1e-5),
-                PRECISIONS_PER_SCORE_THRESHOLD: p, RECALLS_PER_SCORE_THRESHOLD: r,
-            },
+            0: {AVERAGE_PRECISION: approx(_13, rel=1e-3), PRECISION: approx(1.0), RECALL: approx(_13, rel=1e-3)},
+            1: {AVERAGE_PRECISION: approx(_13, rel=1e-3), PRECISION: approx(1.0), RECALL: approx(_13, rel=1e-3)},
+            2: {AVERAGE_PRECISION: approx(_13, rel=1e-3), PRECISION: approx(1.0), RECALL: approx(_13, rel=1e-3)},
         }
 
         assert IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS not in metrics
         assert CONFUSION_MATRICES_PER_SCORE_THRESHOLD not in metrics
 
 
 @pytest.mark.parametrize("dataset_processing_type",
@@ -410,34 +383,21 @@
                              metrics, {},
                              evaluator.coco_metric_time, evaluator.voc_metric_time,
                              evaluator.primary_metric, is_train=False)
 
     _13 = 1.0 / 3.0
     assert metrics[MEAN_AVERAGE_PRECISION] == approx(_13, rel=1e-2)
     if eval_type == ValidationMetricType.VOC:
-        p = {st / 100.0: approx(1.0) if st <= 50 else -1.0 for st in range(100)}
-        r = {st / 100.0: approx(_13, abs=1e-5) if st <= 50 else 0.0 for st in range(100)}
         assert metrics[PRECISION] == approx(1.0)
-        assert metrics[RECALL] == approx(_13, abs=1e-5)
-        assert metrics[PRECISIONS_PER_SCORE_THRESHOLD] == p
-        assert metrics[RECALLS_PER_SCORE_THRESHOLD] == r
+        assert metrics[RECALL] == approx(_13, rel=1e-3)
 
         assert metrics[PER_LABEL_METRICS] == {
-            0: {
-                AVERAGE_PRECISION: approx(_13, abs=1e-5), PRECISION: approx(1.0), RECALL: approx(_13, abs=1e-5),
-                PRECISIONS_PER_SCORE_THRESHOLD: p, RECALLS_PER_SCORE_THRESHOLD: r,
-            },
-            1: {
-                AVERAGE_PRECISION: approx(_13, abs=1e-5), PRECISION: approx(1.0), RECALL: approx(_13, abs=1e-5),
-                PRECISIONS_PER_SCORE_THRESHOLD: p, RECALLS_PER_SCORE_THRESHOLD: r,
-            },
-            2: {
-                AVERAGE_PRECISION: approx(_13, abs=1e-5), PRECISION: approx(1.0), RECALL: approx(_13, abs=1e-5),
-                PRECISIONS_PER_SCORE_THRESHOLD: p, RECALLS_PER_SCORE_THRESHOLD: r,
-            },
+            0: {AVERAGE_PRECISION: approx(_13, rel=1e-3), PRECISION: approx(1.0), RECALL: approx(_13, rel=1e-3)},
+            1: {AVERAGE_PRECISION: approx(_13, rel=1e-3), PRECISION: approx(1.0), RECALL: approx(_13, rel=1e-3)},
+            2: {AVERAGE_PRECISION: approx(_13, rel=1e-3), PRECISION: approx(1.0), RECALL: approx(_13, rel=1e-3)},
         }
 
         assert metrics[IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS] == {
             PRECISION: approx(1.0), RECALL: approx(1.0), AVERAGE_PRECISION: approx(1.0)
         }
 
         cm1 = [[1, 0, 0, 2], [0, 1, 0, 2], [0, 0, 1, 2]]
```

## tests/object_detection_tests/test_object_detection_utils.py

```diff
@@ -54,30 +54,24 @@
         current_metrics = {}
         cumulative_per_label_metrics = {}
 
         voc_metrics = {
             MetricsLiterals.PRECISION: 0.35,
             MetricsLiterals.RECALL: 0.65,
             MetricsLiterals.MEAN_AVERAGE_PRECISION: 0.5,
-            MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: {0.2: 0.5, 0.4: 0.6, 0.6: 0.7, 0.8: 0.8},
-            MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: {0.2: 0.5, 0.4: 0.4, 0.6: 0.3, 0.8: 0.2},
             MetricsLiterals.PER_LABEL_METRICS: {
                 0: {
                     MetricsLiterals.PRECISION: 0.3,
                     MetricsLiterals.RECALL: 0.7,
-                    MetricsLiterals.AVERAGE_PRECISION: 0.25,
-                    MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: {0.1: 0.2, 0.3: 0.4},
-                    MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: {0.1: 0.5, 0.3: 0.6},
+                    MetricsLiterals.AVERAGE_PRECISION: 0.25
                 },
                 1: {
                     MetricsLiterals.PRECISION: 0.4,
                     MetricsLiterals.RECALL: 0.6,
-                    MetricsLiterals.AVERAGE_PRECISION: 0.75,
-                    MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: {0.1: 0.7, 0.3: 0.8},
-                    MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: {0.1: 0.9, 0.3: 1.0},
+                    MetricsLiterals.AVERAGE_PRECISION: 0.75
                 }
             },
             MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS: {
                 MetricsLiterals.PRECISION: 0.7,
                 MetricsLiterals.RECALL: 0.8,
                 MetricsLiterals.AVERAGE_PRECISION: 0.6
             },
@@ -90,30 +84,24 @@
             voc_metrics = _convert_numbers_to_tensors(voc_metrics)
 
         od_utils._update_with_voc_metrics(current_metrics, cumulative_per_label_metrics, voc_metrics)
 
         assert current_metrics == {
             MetricsLiterals.PRECISION: 0.35,
             MetricsLiterals.RECALL: 0.65,
-            MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: {0.2: 0.5, 0.4: 0.6, 0.6: 0.7, 0.8: 0.8},
-            MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: {0.2: 0.5, 0.4: 0.4, 0.6: 0.3, 0.8: 0.2},
             MetricsLiterals.PER_LABEL_METRICS: {
                 0: {
                     MetricsLiterals.PRECISION: 0.3,
                     MetricsLiterals.RECALL: 0.7,
-                    MetricsLiterals.AVERAGE_PRECISION: 0.25,
-                    MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: {0.1: 0.2, 0.3: 0.4},
-                    MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: {0.1: 0.5, 0.3: 0.6},
+                    MetricsLiterals.AVERAGE_PRECISION: 0.25
                 },
                 1: {
                     MetricsLiterals.PRECISION: 0.4,
                     MetricsLiterals.RECALL: 0.6,
-                    MetricsLiterals.AVERAGE_PRECISION: 0.75,
-                    MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: {0.1: 0.7, 0.3: 0.8},
-                    MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: {0.1: 0.9, 0.3: 1.0},
+                    MetricsLiterals.AVERAGE_PRECISION: 0.75
                 }
             },
             MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS: {
                 MetricsLiterals.PRECISION: 0.7,
                 MetricsLiterals.RECALL: 0.8,
                 MetricsLiterals.AVERAGE_PRECISION: 0.6
             },
@@ -154,49 +142,30 @@
         if data_type == "tensors":
             cumulative_per_label_metrics = _convert_numbers_to_tensors(cumulative_per_label_metrics)
 
         voc_metrics = {
             MetricsLiterals.PRECISION: 0.4287317322877335,
             MetricsLiterals.RECALL: 0.3727672265,
             MetricsLiterals.MEAN_AVERAGE_PRECISION: 0.609,
-            # precisions per threshold missing but no crash
-            MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: {
-                0.25349234: 0.89432452, 0.49456456: 0.41, 0.694356456: 0.393245, 0.80000001: 0.22
-            },
             MetricsLiterals.PER_LABEL_METRICS: {
                 1: {
                     MetricsLiterals.PRECISION: 0.12321,
                     MetricsLiterals.RECALL: 0.456,
-                    MetricsLiterals.AVERAGE_PRECISION: 0.55,
-                    MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: {
-                        0.0001: 0.12345, 0.001: 0.1234, 0.01: 0.123, 0.1: 0.12, 1.0: 0.1
-                    },
-                    MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: {
-                        0.0001: 0.745893534534534423, 0.001: 0.5, 0.01: 0.63, 0.1: 0.631, 1.0: 0.631001
-                    },
+                    MetricsLiterals.AVERAGE_PRECISION: 0.55
                 },
                 2: {
                     MetricsLiterals.PRECISION: 0.734253464575467,
                     MetricsLiterals.RECALL: 0.289534453,
-                    MetricsLiterals.AVERAGE_PRECISION: 0.668,
-                    MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: {
-                        0.1: 0.9534053, 0.2: 0.09453245, 0.5: 0.853451, 0.8: 0.84354, 0.9: 1.0
-                    },
-                    MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: {
-                        0.1: 0.13, 0.2: 0.1313, 0.5: 0.131313, 0.8: 0.13131313, 0.9: 0.131313131313
-                    },
+                    MetricsLiterals.AVERAGE_PRECISION: 0.668
                 }
             },
             MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS: {
                 MetricsLiterals.PRECISION: 0.7539893453945,
                 MetricsLiterals.RECALL: 0.8549328534,
-                MetricsLiterals.AVERAGE_PRECISION: 0.6235478395734,
-                # _update_with_voc_metrics() ignores precisions and recalls per score threshold at image level.
-                MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: {0.2: 0.75, 0.3: 0.25},
-                MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: {0.2: 0.25, 0.3: 0.75},
+                MetricsLiterals.AVERAGE_PRECISION: 0.6235478395734
             },
             MetricsLiterals.CONFUSION_MATRICES_PER_SCORE_THRESHOLD: {
                 0.85647: [[4, 0, 0, 0, 12], [0, 9, 0, 0, 7], [0, 0, 9, 0, 10], [0, 0, 0, 8, 8]],
                 0.85188: [[7, 0, 0, 0, 9], [0, 9, 0, 0, 7], [0, 0, 12, 0, 7], [0, 0, 0, 8, 8]],
                 0.84831: [[8, 0, 0, 0, 8], [0, 11, 0, 0, 5], [0, 0, 14, 0, 5], [0, 0, 0, 9, 7]],
                 0.84128: [[10, 0, 0, 0, 6], [0, 12, 0, 0, 4], [0, 0, 15, 0, 4], [0, 0, 0, 11, 5]],
                 0.83179: [[10, 0, 0, 0, 6], [0, 14, 0, 0, 2], [0, 0, 17, 0, 2], [0, 0, 0, 13, 3]]
@@ -206,39 +175,24 @@
             voc_metrics = _convert_numbers_to_tensors(voc_metrics)
 
         od_utils._update_with_voc_metrics(current_metrics, cumulative_per_label_metrics, voc_metrics)
 
         assert current_metrics == {
             MetricsLiterals.PRECISION: 0.42873,
             MetricsLiterals.RECALL: 0.37277,
-            MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: {
-                0.25349: 0.89432, 0.49456: 0.41, 0.69436: 0.39325, 0.8: 0.22
-            },
             MetricsLiterals.PER_LABEL_METRICS: {
                 1: {
                     MetricsLiterals.PRECISION: 0.12321,
                     MetricsLiterals.RECALL: 0.456,
-                    MetricsLiterals.AVERAGE_PRECISION: 0.55,
-                    MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: {
-                        0.0001: 0.12345, 0.001: 0.1234, 0.01: 0.123, 0.1: 0.12, 1.0: 0.1
-                    },
-                    MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: {
-                        0.0001: 0.74589, 0.001: 0.5, 0.01: 0.63, 0.1: 0.631, 1.0: 0.631
-                    }
+                    MetricsLiterals.AVERAGE_PRECISION: 0.55
                 },
                 2: {
                     MetricsLiterals.PRECISION: 0.73425,
                     MetricsLiterals.RECALL: 0.28953,
-                    MetricsLiterals.AVERAGE_PRECISION: 0.668,
-                    MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: {
-                        0.1: 0.95341, 0.2: 0.09453, 0.5: 0.85345, 0.8: 0.84354, 0.9: 1.0
-                    },
-                    MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: {
-                        0.1: 0.13, 0.2: 0.1313, 0.5: 0.13131, 0.8: 0.13131, 0.9: 0.13131
-                    }
+                    MetricsLiterals.AVERAGE_PRECISION: 0.668
                 }
             },
             MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS: {
                 MetricsLiterals.PRECISION: 0.75399,
                 MetricsLiterals.RECALL: 0.85493,
                 MetricsLiterals.AVERAGE_PRECISION: 0.62355
             },
@@ -381,32 +335,18 @@
     @mock.patch(od_utils.__name__ + '.IncrementalVocEvaluator.compute_metrics')
     def test_evaluate_and_log(self, mock_incremental_voc_evaluator_compute):
         # Set up mock objects
         metrics = {
             MetricsLiterals.PRECISION: 0.7,
             MetricsLiterals.RECALL: 0.8,
             MetricsLiterals.MEAN_AVERAGE_PRECISION: 0.9,
-            MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD: {0.1: 0.5, 0.9: 0.5},
-            MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD: {0.1: 0.9, 0.9: 0.1},
             MetricsLiterals.PER_LABEL_METRICS: {
-                1: {
-                    'precision': 0.1, 'recall': 0.2, 'average_precision': 0.3,
-                    'precisions_per_score_threshold': {0.2: 0.6, 1.0: 0.6},
-                    'recalls_per_score_threshold': {0.2: 1.0, 1.0: 0.2},
-                },
-                2: {
-                    'precision': 0.2, 'recall': 0.3, 'average_precision': 0.4,
-                    'precisions_per_score_threshold': {0.1: 0.5, 0.9: 0.5},
-                    'recalls_per_score_threshold': {0.1: 0.9, 0.9: 0.1},
-                },
-                3: {
-                    'precision': 0.3, 'recall': 0.4, 'average_precision': 0.5,
-                    'precisions_per_score_threshold': {0.01: 0.5, 0.09: 0.5},
-                    'recalls_per_score_threshold': {0.01: 0.9, 0.09: 0.1},
-                },
+                1: {'precision': 0.1, 'recall': 0.2, 'average_precision': 0.3},
+                2: {'precision': 0.2, 'recall': 0.3, 'average_precision': 0.4},
+                3: {'precision': 0.3, 'recall': 0.4, 'average_precision': 0.5},
             },
             MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS: {
                 MetricsLiterals.PRECISION: 0.2,
                 MetricsLiterals.RECALL: 0.4,
                 MetricsLiterals.AVERAGE_PRECISION: 0.6
             },
             MetricsLiterals.CONFUSION_MATRICES_PER_SCORE_THRESHOLD: {
@@ -429,28 +369,22 @@
         mock_incremental_voc_evaluator_compute.assert_called_once_with()
 
         # Validate properties contain only basic metric values
         properties = mock_run.properties
         assert properties[MetricsLiterals.PRECISION] == 0.7
         assert properties[MetricsLiterals.RECALL] == 0.8
         assert properties[MetricsLiterals.MEAN_AVERAGE_PRECISION] == 0.9
-        assert MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD not in properties
-        assert MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD not in properties
-        assert MetricsLiterals.PER_LABEL_METRICS not in properties
         assert MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS not in properties
         assert MetricsLiterals.CONFUSION_MATRICES_PER_SCORE_THRESHOLD not in properties
 
         # Validate metrics contain only basic metric values
         metrics = mock_run.metrics
         assert metrics[MetricsLiterals.PRECISION] == 0.7
         assert metrics[MetricsLiterals.RECALL] == 0.8
         assert metrics[MetricsLiterals.MEAN_AVERAGE_PRECISION] == 0.9
-        assert MetricsLiterals.PRECISIONS_PER_SCORE_THRESHOLD not in metrics
-        assert MetricsLiterals.RECALLS_PER_SCORE_THRESHOLD not in metrics
-        assert MetricsLiterals.PER_LABEL_METRICS not in metrics
         assert MetricsLiterals.IMAGE_LEVEL_BINARY_CLASSIFIER_METRICS not in metrics
         assert MetricsLiterals.CONFUSION_MATRICES_PER_SCORE_THRESHOLD not in metrics
 
 
 @pytest.mark.usefixtures('new_clean_dir')
 def test_score_validation_data(monkeypatch):
     def mock_fetch_model(run_id, device, model_settings):
```

## Comparing `azureml_automl_dnn_vision-1.52.0.dist-info/LICENSE.txt` & `azureml_automl_dnn_vision-1.52.0.post1.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `azureml_automl_dnn_vision-1.52.0.dist-info/METADATA` & `azureml_automl_dnn_vision-1.52.0.post1.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: azureml-automl-dnn-vision
-Version: 1.52.0
+Version: 1.52.0.post1
 Summary: AutoML DNN Vision Models
 Home-page: https://docs.microsoft.com/python/api/overview/azure/ml/?view=azure-ml-py
 Author: Microsoft Corp
 License: Proprietary https://aka.ms/azureml-preview-sdk-license 
 Platform: UNKNOWN
 Classifier: Intended Audience :: Developers
 Classifier: Intended Audience :: Science/Research
```

## Comparing `azureml_automl_dnn_vision-1.52.0.dist-info/RECORD` & `azureml_automl_dnn_vision-1.52.0.post1.dist-info/RECORD`

 * *Files 3% similar despite different names*

```diff
@@ -28,15 +28,15 @@
 azureml/automl/dnn/vision/common/NOTICE,sha256=HuhflQutvMLtIxOQzjD8EUgjSySzU1hmGw6GxvxFWJQ,85645
 azureml/automl/dnn/vision/common/__init__.py,sha256=hJxcB5ZxLIaRdoof-QBNH9rbcj-WGh3Azx7QzBGLYZs,263
 azureml/automl/dnn/vision/common/aml_dataset_base_wrapper.py,sha256=V7UYkdN1d5tIUij4DWyJnG9RS-jDX6fd7YWku99X8N4,1903
 azureml/automl/dnn/vision/common/artifacts_utils.py,sha256=CkNcL9VK7Ks73fEgFNX6YTCuTYO5J3cjV0wfzEfEe0E,19557
 azureml/automl/dnn/vision/common/average_meter.py,sha256=jgLAM2T4sqzcYwkqyZPonA-PzjTo22u_FkgjSXELJz4,2025
 azureml/automl/dnn/vision/common/base_model_factory.py,sha256=jqIwxJ4Uc2Yew0a6B3Ju6y1JA7LcqyZeZbVHMCj4rvo,2162
 azureml/automl/dnn/vision/common/base_model_settings.py,sha256=vjFc2VjnItGtNFI6GO_5RyY8TTh3nFMRciIM-darIws,1231
-azureml/automl/dnn/vision/common/constants.py,sha256=h8GkrNtXav9dX19zJDzJly8ONUB8ZZP1OAzC_WnJmeE,22734
+azureml/automl/dnn/vision/common/constants.py,sha256=cy6zUhiA1vKQ9HzM7oKL3ITwMSDOk-cA2iv8-ybbgJQ,22598
 azureml/automl/dnn/vision/common/data_utils.py,sha256=qdg2SmmN-0BJkaut0XtjhNGbNm7zvhabolJcnxJbDxM,2826
 azureml/automl/dnn/vision/common/dataloaders.py,sha256=k3lTVbW_1r_9zJAamCN1as8N60yS-IuwEXY4e5v8GDc,3184
 azureml/automl/dnn/vision/common/dataset_helper.py,sha256=cwZxMPnH31tL-LpmCGFX5SbX07449x3W_UXabliIDMw,18976
 azureml/automl/dnn/vision/common/distributed_utils.py,sha256=Ut6zamPMIxCcDsS3SwT4lHyulCouy9vrshRcziGdQX4,21108
 azureml/automl/dnn/vision/common/errors.py,sha256=9knV7WI1paVWHLXGTSIa-AxYMq0s2RtrrsyEXxVbqH0,672
 azureml/automl/dnn/vision/common/exceptions.py,sha256=eDxs4p3tI7JZG4Fv60rIehayqAO751xELLPGozsWuig,2379
 azureml/automl/dnn/vision/common/logging_utils.py,sha256=LldRV06kWtaI16snaGVgyH6rpMhWAnTy2SHBex0_R6A,2528
@@ -46,15 +46,15 @@
 azureml/automl/dnn/vision/common/pretrained_model_utilities.py,sha256=64IFy7zlMVfE68VVuzJXodG99pvcL5fgwnCLShoFj_s,37122
 azureml/automl/dnn/vision/common/sku_validation.py,sha256=RW_lZFsZFqhi3U8pkP57Vr1rnl7OQquwLy4hbjTkb88,3200
 azureml/automl/dnn/vision/common/system_meter.py,sha256=yOC8fQFD7Fu7-8GI3mXUnmwnTBHaAq-uHWRD1h9Gki0,9887
 azureml/automl/dnn/vision/common/tiling_dataset_element.py,sha256=Nrwmqy5qq5mkOljjEOnlMLMzGtVP91YXbZiWIIf8OmA,5477
 azureml/automl/dnn/vision/common/tiling_utils.py,sha256=JxnOSZkv2yS47rBLzFXszzUwm-CPIrZX3T2o26nrQOg,6399
 azureml/automl/dnn/vision/common/torch_utils.py,sha256=LgCIEUaJLj4vvGRPLFzW71G1kj5BdDW4hh2yYZpzhGk,1039
 azureml/automl/dnn/vision/common/training_state.py,sha256=u_hcK_Gkqqr0bO8EzHkiwJ39gEs8hT3QPxL1ySkUpuU,2994
-azureml/automl/dnn/vision/common/utils.py,sha256=XwZQbWRam-8yTjd14OfgS1HuN2rcsxBnmk48UkxFvc4,73199
+azureml/automl/dnn/vision/common/utils.py,sha256=9bi_pHUSSbgTHtv_zQ1JOukJw2GiyBoXd0I7R16fzSw,70662
 azureml/automl/dnn/vision/common/mlflow/__init__.py,sha256=TkLaDSO2l8DaHcSUqn9IeOq1sxv9QB7l-OlT_7b_-ao,237
 azureml/automl/dnn/vision/common/mlflow/mlflow_model_wrapper.py,sha256=HqfiJs0SH8TRLgh4bsjhGWOKObHBQewoTYIqyBjY4Q0,8165
 azureml/automl/dnn/vision/common/trainer/__init__.py,sha256=pDiItIKTd0YnUIgh2SBUpb-biWbwdEIEg_GctE0c3MA,272
 azureml/automl/dnn/vision/common/trainer/lrschedule.py,sha256=6RKR9lJHBW4MSbLJ78MT7tvlKsU7K0I62vMSQZzaWFg,7720
 azureml/automl/dnn/vision/common/trainer/lrschedule_parameters.py,sha256=KUAaOi2O8RYulDBfW52947R_oFN0Hb0x1EEpc_5a81Q,3336
 azureml/automl/dnn/vision/common/trainer/optimize.py,sha256=I5-u1O0gIPsq-GCaHqpXTz1xdVve7-25ERL7dZbxW10,9241
 azureml/automl/dnn/vision/common/trainer/optimize_parameters.py,sha256=NPKOxN6pBqFvflPS6fc0qW_AlGasd90kuzPzsjpwqAI,4337
@@ -70,29 +70,29 @@
 azureml/automl/dnn/vision/object_detection/runner.py,sha256=K9NiDpZ-zS9-VDLlKHB9STLyk4XFW5hxkCVuQilD_N8,20822
 azureml/automl/dnn/vision/object_detection/common/__init__.py,sha256=i6TcRNfQxVjsDyzq2SQCeYdOvAvIQ2LX2Y0WpSpN12w,243
 azureml/automl/dnn/vision/object_detection/common/augmentations.py,sha256=umZSLiOSsOZv17EQ9uTI1X_EpmtA4uIosHfJS_VyIzQ,9636
 azureml/automl/dnn/vision/object_detection/common/boundingbox.py,sha256=WpftxN0Nvej0_BJxHWQdPkRKUx-dCBHhUUE7vD-caGo,7356
 azureml/automl/dnn/vision/object_detection/common/coco_eval_box_converter.py,sha256=d4_qrM0zaV-gqxBlHF55QXHyYv4572a-pCs_0MACUl0,3009
 azureml/automl/dnn/vision/object_detection/common/constants.py,sha256=02XBI1JWG0jt7siuj0wk8zwHo2KuF5bcUQ-ehaH_zi8,11089
 azureml/automl/dnn/vision/object_detection/common/masktools.py,sha256=bl9jPxVuNjpG7qdUd3jkhx_xABtctk1qUqHme6CJ1m0,11945
-azureml/automl/dnn/vision/object_detection/common/object_detection_utils.py,sha256=Hto8NudMbilgtTKpTuMzPjGhoP4RaP_wy513GR5MIFk,28085
+azureml/automl/dnn/vision/object_detection/common/object_detection_utils.py,sha256=MXfinruUNhUghnEHiUqzQMXzGTfwKhQkRhGnS5AcwT0,27385
 azureml/automl/dnn/vision/object_detection/common/od_training_state.py,sha256=E_SMar741zRj6JCzJUW_5zL3N9lUBJHbhOFtg-chX5o,1632
 azureml/automl/dnn/vision/object_detection/common/parameters.py,sha256=ylYngnLI8Q_3FEcL2xKM3ktgwAXvhkFGvqCXqOdxmh0,4891
 azureml/automl/dnn/vision/object_detection/common/tiling_helper.py,sha256=DtAPW_H0u8zLckTj7sSLAMEUHc77UgQ67iPijgJI6pQ,26057
 azureml/automl/dnn/vision/object_detection/data/__init__.py,sha256=XEFSLiDC0lAhxmYq4MkWsVLzAO95Onn0fXbvXR0JTJ8,238
 azureml/automl/dnn/vision/object_detection/data/dataset_wrappers.py,sha256=ItAowpQ1vWrvFc6xFv-dETaaX2XvnQzZtdcQXI3m99o,4605
 azureml/automl/dnn/vision/object_detection/data/datasets.py,sha256=fhM2dkiknUa3s9TjxiI2ZPWrBoaRbdINbW1h6N5JEII,40467
 azureml/automl/dnn/vision/object_detection/data/loaders.py,sha256=QtV3k6QrJw82n2pMv_vH-D968ZOG9MYlM4S6_zLucDg,6110
 azureml/automl/dnn/vision/object_detection/data/object_annotation.py,sha256=hYYGIg3cms8J_3RFZSj0ifRt0vlk-B3Mqeo4U3Og0js,12574
 azureml/automl/dnn/vision/object_detection/data/tiling_distributed_sampler.py,sha256=SGlKInQQUeFt5mRlaHYXiVLvi_rBumRKUZ7Rzk8m46w,4985
 azureml/automl/dnn/vision/object_detection/data/utils.py,sha256=bj13WOkTUz2zn3LpkPlxY0w1Fa65cgHay698hfbIXwM,11231
 azureml/automl/dnn/vision/object_detection/eval/__init__.py,sha256=z1te76w-UN8eUJa5Fn0NRQne-cdDNU1jEs0_ARQBH9c,240
 azureml/automl/dnn/vision/object_detection/eval/cocotools.py,sha256=onTV6-iVpOsox9RSoPpMzMBIlqUBsatAKyrXJcaucQI,8380
-azureml/automl/dnn/vision/object_detection/eval/incremental_voc_evaluator.py,sha256=5GORFlvWX1oLKLrZT-NKIXRQe73tEMYGZMhOF3lwIBY,22337
-azureml/automl/dnn/vision/object_detection/eval/metric_computation_utils.py,sha256=b-J-dg9PdBv2Qi-IJ8OX-flX2yP0ns0T8HDrzmYhZKY,19785
+azureml/automl/dnn/vision/object_detection/eval/incremental_voc_evaluator.py,sha256=xTTy1bnZpPcL6nA4zDtDRJPWrmwYYFfaLl-9HD5Ftjk,21506
+azureml/automl/dnn/vision/object_detection/eval/metric_computation_utils.py,sha256=ASKw0j115uGAiMDXBdCtw8utllopAHry80PFCyOOagI,17354
 azureml/automl/dnn/vision/object_detection/eval/object_detection_instance_segmentation_evaluator.py,sha256=zxdt7ITVTxTl6rFUqss8cTMiHoM0v0tB4tDITD2r7hA,14609
 azureml/automl/dnn/vision/object_detection/eval/utils.py,sha256=WnlKSB8zSJFPGS5TPQJ3CwkAWZ1mW4_RZbrUJ-28Dxk,5220
 azureml/automl/dnn/vision/object_detection/models/__init__.py,sha256=NhNRu71Vk3XkfvXsrb0rJ7UeyJsi7O3iyqEoyOVVzpk,239
 azureml/automl/dnn/vision/object_detection/models/base_model_wrapper.py,sha256=S8mrdf5ePgwK4fo8FvLXNbk9dgYNCwNCBD1AwmTgnxc,14422
 azureml/automl/dnn/vision/object_detection/models/detection.py,sha256=8hDkCsCiL_aG0fYf9bRuQo0JhIz2xiu7K9RIrmfv7G4,5102
 azureml/automl/dnn/vision/object_detection/models/instance_segmentation_model_wrappers.py,sha256=EZyDm19W90CX3EtJOEWvH8HTAb_s5K3aXnmG9limWMo,14240
 azureml/automl/dnn/vision/object_detection/models/object_detection_model_wrappers.py,sha256=4oiMFOiWKpop06ZBQrXrFwCeNsXq8lEnAv1eipmgEPQ,28844
@@ -146,15 +146,15 @@
 tests/classification_tests/test_prediction_dataset.py,sha256=u0Qax67k7DxM5Yrtvgd60xXF2ytsx_UUQzXwiAUyCw0,11694
 tests/classification_tests/test_pretrained_model_factory.py,sha256=vOwVPR1gNbVZV9aMajwTGdTPW9gccn0Hq9OjMWpICwE,3067
 tests/common/__init__.py,sha256=JlCCObuOLZyNhIZlsoL51KtFxiY4xKIIzIRDBcdeu6Y,183
 tests/common/aml_dataset_mock.py,sha256=dCJETAPmt1VLTU3z8aBWz_rdMwYqc58jQS_YjQPrVkc,1237
 tests/common/run_mock.py,sha256=fKgIrdGAmZuwFRH2NyUl7L8lFEAhzqmqkCCcUztJ0Rk,5177
 tests/common/test_aml_dataset_helper.py,sha256=3g1vnARqj4RURkdO_J-AlM7TWh1HOkXtjaCOcOmKGN4,10438
 tests/common/test_artifacts_utils.py,sha256=l1wda9R3VgbBlZv6wJgQgVBm3V8a-l4INvPbvKH-MP0,16892
-tests/common/test_common_methods.py,sha256=LB1weptqv2YfPzL4PTA7CP50oTvaFnJwomZWAI-J250,61650
+tests/common/test_common_methods.py,sha256=wYLMQXeK6Ft4BMhA92mp8NPEcIX_OOC-BGEj2MFt_fY,58368
 tests/common/test_distributed_utils.py,sha256=cs08Um-czHe52GTLjhgHlJIbx0it6Sag83Bz30ck0Gs,16927
 tests/common/test_mlflow_model_wrapper.py,sha256=w54CRk49-v5COAmf4u85PW_oBYrKoPDKfrSA3TzhARo,4295
 tests/common/test_model_export_utils.py,sha256=KNo1Gc4xsZOQ30O1sg-0xDqM9pxeoD_E8nyx-Ejna2M,10586
 tests/common/test_pretrained_model_utilities.py,sha256=9yxWrj7IGLVkxOUSCyTKeu8XEmNgIBSxxT5cS_mibCs,6720
 tests/common/test_runner_default_args.py,sha256=K9Rhnfdaz69hl_CZP27HhfOgF_IRKIzTKNV0Yo3IcaE,6606
 tests/common/test_training_state.py,sha256=efxSLfV72e0M6bZwpbIIs4F-1dzmmoGtJqXqul82bRc,5624
 tests/common/utils.py,sha256=ZZoXTNgem_jI2G8r2wTmoLjh1nVqw5F9XIoEjR6sY7Y,6315
@@ -162,32 +162,32 @@
 tests/object_detection_tests/aml_dataset_mock.py,sha256=YzZ57ivEwZGO-ziZRFBDPsGy5fr1gX3nuItOFm70ffQ,430
 tests/object_detection_tests/conftest.py,sha256=1xubLF0mMfJ72UeDWyk4ShG903gHqq0lV0_zuIflALc,711
 tests/object_detection_tests/test_augmentations.py,sha256=-VAEzx5CMsDG23KXN3n3EPQcloZac-W7TbePTKoVA1I,5755
 tests/object_detection_tests/test_coco_eval_box_converter.py,sha256=fvekV1MxfGCTiJwa1LPFtFbxMtZ1EUwoNZZjsvwa4YU,4323
 tests/object_detection_tests/test_cocotools.py,sha256=3MP6QNli7XcMwTz2f2zJC9nCA_efr0mrIgS8M0xA1IE,1604
 tests/object_detection_tests/test_dataset_wrappers.py,sha256=ulH_CqDLssrbnplqC4gSQq_U2Bp8CSooaFr4LWcGJZ8,9815
 tests/object_detection_tests/test_datasets.py,sha256=Wlu6Ui0C8K_Ypw_oc-0gLdGlp6l2bYMcM30zWjjVZrA,73835
-tests/object_detection_tests/test_incremental_voc_evaluator.py,sha256=WP8r0tTuauYcf0jxRgdDG6tjYTAbijCmTQ8BwShawno,100617
+tests/object_detection_tests/test_incremental_voc_evaluator.py,sha256=9YjRZeGdHePLeNwWsz23fTRWEQDi-Cmy-LIbqLSyL-c,78443
 tests/object_detection_tests/test_maskutils.py,sha256=Nheu3leLh9DA4JRqjXAXsjimXAbc0DIvc_z0yclUDuA,6193
-tests/object_detection_tests/test_metric_computation_utils.py,sha256=Nb4msRrVoA3W_Yq8UUGfIYA2FDXlZWs5Vhk0cchv5vQ,26704
+tests/object_detection_tests/test_metric_computation_utils.py,sha256=xAEx-25IDbuNW0aNiVhJD9REg6sFa6Ys-aTDLbkpL7E,21122
 tests/object_detection_tests/test_model_wrappers.py,sha256=Qj8Lalw4Y1rcy6jfpNCrlX6Y8InVbiVREQuVl5huCtQ,15744
 tests/object_detection_tests/test_object_annotation.py,sha256=sLAmwHX48qVJR-aOqVtncGE7o_gD-sk3KmXAYTl3Dgs,14068
-tests/object_detection_tests/test_object_detection_instance_segmentation_evaluator.py,sha256=5VuoGzM3yTVM3S04fLfbBom-49GZqsivOqvBiqfmkAg,20800
+tests/object_detection_tests/test_object_detection_instance_segmentation_evaluator.py,sha256=ySWdOn7-JJwgs53caYxqYvv7cVM9YjsoGRmgOYZAMDw,18676
 tests/object_detection_tests/test_object_detection_local_run.py,sha256=Z1KR2YIYrs578bvdCW8e59vvcn__-Jxx4JnuJTD5_mA,4296
-tests/object_detection_tests/test_object_detection_utils.py,sha256=ZaVgEipD2reJiQnI6pBXcLEXO2LmWs5PE9ZpjCXpEIw,25852
+tests/object_detection_tests/test_object_detection_utils.py,sha256=o2X8gd80VZopMSdRs0eR-bHi8OeWrg435-anCWCZnO4,21358
 tests/object_detection_tests/test_pretrained_model_factory.py,sha256=HJn3YXbiYIQvVjTf88t8WwGFQO20-RGbSIPygjgEQaY,2728
 tests/object_detection_tests/test_scoring.py,sha256=Wa_MGonEjrw5NhaaEiZB0SMhNqXf8xHm0koOoLLPCiI,5550
 tests/object_detection_tests/test_secondary_model_wrappers.py,sha256=Yuj5Zq1r3hAfxL_eb8M-nEUtVQGuSBY90AnUwrpgnmw,5677
 tests/object_detection_tests/test_tiling_distributed_sampler.py,sha256=t8GN8eenYvDjQ6dXDUgC3HR4xsXfTPzOJzsJTQQ3thM,5050
 tests/object_detection_tests/test_tiling_helper.py,sha256=BTkNsHimEWquGlsm-CvGX196vaGp0ViR8RwICVWIluU,49608
 tests/object_detection_tests/test_tiling_utils.py,sha256=wLE0JZLO6y08JB4rpp4gH0A4MMCGXfURFgGpXtgHEvs,9531
 tests/object_detection_tests/test_trainer_criterion.py,sha256=KqsUiG5OhcJcLOQSfpXmMcnWsu3aqJZ5bx72d_fxhbA,1280
 tests/object_detection_tests/test_yolo_trainer_train.py,sha256=D4R3C--NmdYLWBO-KFVBErl5VAmM2FreIcFrt3Quet8,4133
 tests/object_detection_tests/test_yolo_utils.py,sha256=2bi7M2Ty7300-pYX6KjbcrfFEqmWhnPoVtNtTRAsez0,3710
 tests/object_detection_tests/test_yolo_wrapper.py,sha256=AnUUAoT6julWm0AT0gM2GTuktDGB16kGwv4fcCER39I,773
 tests/object_detection_tests/utils.py,sha256=k4b7GmBViQFEi_LP3JFMa9kCmfeqEtD3y8oLHa9tH6o,1088
-azureml_automl_dnn_vision-1.52.0.dist-info/LICENSE.txt,sha256=FOfkEEz4uS7g278F9Rq12rqKF3lLyBUZhVpLetuZrTg,1021
-azureml_automl_dnn_vision-1.52.0.dist-info/METADATA,sha256=cXr2VSZytgythPDc--Dge-4rxhEFQ2d1RH9QeFalrgc,1932
-azureml_automl_dnn_vision-1.52.0.dist-info/WHEEL,sha256=YUYzQ6UQdoqxXjimOitTqynltBCkwY6qlTfTh2IzqQU,97
-azureml_automl_dnn_vision-1.52.0.dist-info/namespace_packages.txt,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
-azureml_automl_dnn_vision-1.52.0.dist-info/top_level.txt,sha256=YGbVVonfOvHBwpsDBTpLWbK7KEdGg0m7LlGFY7j0SCw,14
-azureml_automl_dnn_vision-1.52.0.dist-info/RECORD,,
+azureml_automl_dnn_vision-1.52.0.post1.dist-info/LICENSE.txt,sha256=FOfkEEz4uS7g278F9Rq12rqKF3lLyBUZhVpLetuZrTg,1021
+azureml_automl_dnn_vision-1.52.0.post1.dist-info/METADATA,sha256=5GQA6TC5bw9pYz3IVcjY1vaysqnzr_PulE5tvK5R2Yo,1938
+azureml_automl_dnn_vision-1.52.0.post1.dist-info/WHEEL,sha256=YUYzQ6UQdoqxXjimOitTqynltBCkwY6qlTfTh2IzqQU,97
+azureml_automl_dnn_vision-1.52.0.post1.dist-info/namespace_packages.txt,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
+azureml_automl_dnn_vision-1.52.0.post1.dist-info/top_level.txt,sha256=YGbVVonfOvHBwpsDBTpLWbK7KEdGg0m7LlGFY7j0SCw,14
+azureml_automl_dnn_vision-1.52.0.post1.dist-info/RECORD,,
```

