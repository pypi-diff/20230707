# Comparing `tmp/gitlab_emulator-1.5.6-py3-none-any.whl.zip` & `tmp/gitlab_emulator-1.5.7-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,60 +1,60 @@
-Zip file size: 76075 bytes, number of entries: 58
--rwxr-xr-x  2.0 unx      125 b- defN 23-Jun-24 07:58 gitlab_emulator-1.5.6.data/scripts/locallab.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-04 09:47 gitlabemu/__init__.py
--rw-r--r--  2.0 unx       76 b- defN 23-Mar-04 09:47 gitlabemu/__main__.py
--rw-r--r--  2.0 unx      285 b- defN 23-Mar-04 09:47 gitlabemu/ansi.py
--rw-r--r--  2.0 unx     1043 b- defN 23-Mar-04 09:47 gitlabemu/artifacts.py
--rw-r--r--  2.0 unx      786 b- defN 23-Mar-04 09:47 gitlabemu/chown.py
--rw-r--r--  2.0 unx    28476 b- defN 23-Jun-24 07:57 gitlabemu/configloader.py
--rw-r--r--  2.0 unx     1863 b- defN 23-May-19 06:46 gitlabemu/configtool.py
--rw-r--r--  2.0 unx     1428 b- defN 23-May-19 06:46 gitlabemu/configtool_ctx.py
--rw-r--r--  2.0 unx     1614 b- defN 23-May-19 06:46 gitlabemu/configtool_gitlab.py
--rw-r--r--  2.0 unx     6073 b- defN 23-May-19 06:46 gitlabemu/configtool_runner.py
--rw-r--r--  2.0 unx     2024 b- defN 23-May-19 06:46 gitlabemu/configtool_vars.py
--rw-r--r--  2.0 unx      942 b- defN 23-May-19 06:46 gitlabemu/configtool_volumes.py
--rw-r--r--  2.0 unx    29459 b- defN 23-Jun-24 07:57 gitlabemu/docker.py
--rw-r--r--  2.0 unx      855 b- defN 23-Mar-04 09:47 gitlabemu/errors.py
--rw-r--r--  2.0 unx     5029 b- defN 23-Mar-04 09:47 gitlabemu/generator.py
--rw-r--r--  2.0 unx    19321 b- defN 23-Mar-04 09:47 gitlabemu/gitlab_client_api.py
--rw-r--r--  2.0 unx    14697 b- defN 23-Jun-24 07:57 gitlabemu/helpers.py
--rw-r--r--  2.0 unx    20940 b- defN 23-Jun-24 07:57 gitlabemu/jobs.py
--rw-r--r--  2.0 unx     1010 b- defN 23-May-21 21:45 gitlabemu/localfiles.py
--rw-r--r--  2.0 unx     1150 b- defN 23-May-19 06:46 gitlabemu/logmsg.py
--rw-r--r--  2.0 unx     9033 b- defN 23-Mar-04 09:47 gitlabemu/pipelines.py
--rw-r--r--  2.0 unx     1711 b- defN 23-Mar-04 09:47 gitlabemu/pstab.py
--rw-r--r--  2.0 unx     2302 b- defN 23-Mar-04 09:47 gitlabemu/references.py
--rw-r--r--  2.0 unx      901 b- defN 23-Mar-04 09:47 gitlabemu/resnamer.py
--rw-r--r--  2.0 unx     3264 b- defN 23-Mar-04 09:47 gitlabemu/ruleparser.py
--rw-r--r--  2.0 unx    22001 b- defN 23-Jun-24 07:57 gitlabemu/runner.py
--rw-r--r--  2.0 unx     1149 b- defN 23-Mar-04 09:47 gitlabemu/userconfig.py
--rw-r--r--  2.0 unx    13883 b- defN 23-May-19 06:46 gitlabemu/userconfigdata.py
--rw-r--r--  2.0 unx      906 b- defN 23-Mar-04 09:47 gitlabemu/variables.py
--rw-r--r--  2.0 unx     2447 b- defN 23-Mar-04 09:47 gitlabemu/yamlloader.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-04 09:47 gitlabemu/genericci/__init__.py
--rw-r--r--  2.0 unx     4938 b- defN 23-Mar-04 09:47 gitlabemu/genericci/types.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-04 09:47 gitlabemu/gitlab/__init__.py
--rw-r--r--  2.0 unx      340 b- defN 23-Mar-04 09:47 gitlabemu/gitlab/constraints.py
--rw-r--r--  2.0 unx      720 b- defN 23-Mar-04 09:47 gitlabemu/gitlab/types.py
--rw-r--r--  2.0 unx      109 b- defN 23-Mar-04 09:47 gitlabemu/gitlab/urls.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-04 09:47 gitlabemu/glp/__init__.py
--rw-r--r--  2.0 unx       28 b- defN 23-Mar-04 09:47 gitlabemu/glp/__main__.py
--rw-r--r--  2.0 unx      880 b- defN 23-Mar-04 09:47 gitlabemu/glp/buildtool.py
--rw-r--r--  2.0 unx      573 b- defN 23-Mar-04 09:47 gitlabemu/glp/canceltool.py
--rw-r--r--  2.0 unx     1160 b- defN 23-Mar-04 09:47 gitlabemu/glp/dumptool.py
--rw-r--r--  2.0 unx     1479 b- defN 23-Mar-04 09:47 gitlabemu/glp/exporttool.py
--rw-r--r--  2.0 unx     1146 b- defN 23-Mar-04 09:47 gitlabemu/glp/jobstool.py
--rw-r--r--  2.0 unx      555 b- defN 23-Mar-04 09:47 gitlabemu/glp/listtool.py
--rw-r--r--  2.0 unx     2276 b- defN 23-Mar-04 09:47 gitlabemu/glp/subcommand.py
--rw-r--r--  2.0 unx     5074 b- defN 23-Mar-04 09:47 gitlabemu/glp/subsettool.py
--rw-r--r--  2.0 unx     1906 b- defN 23-Mar-04 09:47 gitlabemu/glp/tool.py
--rw-r--r--  2.0 unx      780 b- defN 23-Mar-04 09:47 gitlabemu/glp/types.py
--rw-r--r--  2.0 unx     3350 b- defN 23-Mar-04 09:47 gitlabemu/rules/GitlabRuleLexer.py
--rw-r--r--  2.0 unx    15223 b- defN 23-Mar-04 09:47 gitlabemu/rules/GitlabRuleParser.py
--rw-r--r--  2.0 unx     1550 b- defN 23-Mar-04 09:47 gitlabemu/rules/GitlabRuleVisitor.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-04 09:47 gitlabemu/rules/__init__.py
--rw-r--r--  2.0 unx      670 b- defN 23-Jun-24 07:58 gitlab_emulator-1.5.6.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jun-24 07:58 gitlab_emulator-1.5.6.dist-info/WHEEL
--rw-r--r--  2.0 unx      113 b- defN 23-Jun-24 07:58 gitlab_emulator-1.5.6.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       10 b- defN 23-Jun-24 07:58 gitlab_emulator-1.5.6.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     4759 b- defN 23-Jun-24 07:58 gitlab_emulator-1.5.6.dist-info/RECORD
-58 files, 242524 bytes uncompressed, 68569 bytes compressed:  71.7%
+Zip file size: 76724 bytes, number of entries: 58
+-rw-rw-rw-  2.0 fat      133 b- defN 23-Jul-07 18:25 gitlab_emulator-1.5.7.data/scripts/locallab.py
+-rw-rw-rw-  2.0 fat        0 b- defN 21-Mar-21 09:37 gitlabemu/__init__.py
+-rw-rw-rw-  2.0 fat       82 b- defN 21-Mar-21 09:37 gitlabemu/__main__.py
+-rw-rw-rw-  2.0 fat      298 b- defN 22-Mar-22 18:57 gitlabemu/ansi.py
+-rw-rw-rw-  2.0 fat     1073 b- defN 23-Mar-05 14:37 gitlabemu/artifacts.py
+-rw-rw-rw-  2.0 fat      810 b- defN 23-Mar-05 14:37 gitlabemu/chown.py
+-rw-rw-rw-  2.0 fat    29445 b- defN 23-Jul-07 18:24 gitlabemu/configloader.py
+-rw-rw-rw-  2.0 fat     1921 b- defN 23-Mar-29 12:14 gitlabemu/configtool.py
+-rw-rw-rw-  2.0 fat     1471 b- defN 23-Mar-29 12:14 gitlabemu/configtool_ctx.py
+-rw-rw-rw-  2.0 fat     1653 b- defN 23-Mar-29 12:14 gitlabemu/configtool_gitlab.py
+-rw-rw-rw-  2.0 fat     6202 b- defN 23-Mar-29 12:14 gitlabemu/configtool_runner.py
+-rw-rw-rw-  2.0 fat     2077 b- defN 23-Mar-29 12:14 gitlabemu/configtool_vars.py
+-rw-rw-rw-  2.0 fat      970 b- defN 23-Mar-29 12:14 gitlabemu/configtool_volumes.py
+-rw-rw-rw-  2.0 fat    30213 b- defN 23-Jul-07 06:31 gitlabemu/docker.py
+-rw-rw-rw-  2.0 fat      901 b- defN 23-Mar-05 14:37 gitlabemu/errors.py
+-rw-rw-rw-  2.0 fat     5172 b- defN 23-Mar-05 14:37 gitlabemu/generator.py
+-rw-rw-rw-  2.0 fat    19858 b- defN 23-Mar-05 14:37 gitlabemu/gitlab_client_api.py
+-rw-rw-rw-  2.0 fat    15164 b- defN 23-Jul-07 06:31 gitlabemu/helpers.py
+-rw-rw-rw-  2.0 fat    21533 b- defN 23-Jul-07 06:31 gitlabemu/jobs.py
+-rw-rw-rw-  2.0 fat     1038 b- defN 23-Jul-07 06:31 gitlabemu/localfiles.py
+-rw-rw-rw-  2.0 fat     1210 b- defN 23-Jul-07 06:31 gitlabemu/logmsg.py
+-rw-rw-rw-  2.0 fat     9259 b- defN 23-Mar-05 14:37 gitlabemu/pipelines.py
+-rw-rw-rw-  2.0 fat     1775 b- defN 23-Mar-05 14:37 gitlabemu/pstab.py
+-rw-rw-rw-  2.0 fat     2359 b- defN 23-Mar-05 14:37 gitlabemu/references.py
+-rw-rw-rw-  2.0 fat      935 b- defN 22-Jul-03 09:38 gitlabemu/resnamer.py
+-rw-rw-rw-  2.0 fat     3353 b- defN 23-Mar-05 14:37 gitlabemu/ruleparser.py
+-rw-rw-rw-  2.0 fat    22547 b- defN 23-Jul-07 06:31 gitlabemu/runner.py
+-rw-rw-rw-  2.0 fat     1187 b- defN 22-Jul-03 09:38 gitlabemu/userconfig.py
+-rw-rw-rw-  2.0 fat    14303 b- defN 23-Jul-07 06:31 gitlabemu/userconfigdata.py
+-rw-rw-rw-  2.0 fat      934 b- defN 23-Mar-05 14:37 gitlabemu/variables.py
+-rw-rw-rw-  2.0 fat     2538 b- defN 23-Mar-05 14:37 gitlabemu/yamlloader.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Mar-05 14:37 gitlabemu/genericci/__init__.py
+-rw-rw-rw-  2.0 fat     5085 b- defN 23-Mar-05 14:37 gitlabemu/genericci/types.py
+-rw-rw-rw-  2.0 fat        0 b- defN 22-Mar-22 18:57 gitlabemu/gitlab/__init__.py
+-rw-rw-rw-  2.0 fat      358 b- defN 23-Mar-05 14:37 gitlabemu/gitlab/constraints.py
+-rw-rw-rw-  2.0 fat      751 b- defN 23-Mar-05 14:37 gitlabemu/gitlab/types.py
+-rw-rw-rw-  2.0 fat      110 b- defN 23-Mar-05 14:37 gitlabemu/gitlab/urls.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Mar-05 14:37 gitlabemu/glp/__init__.py
+-rw-rw-rw-  2.0 fat       30 b- defN 23-Mar-05 14:37 gitlabemu/glp/__main__.py
+-rw-rw-rw-  2.0 fat      906 b- defN 23-Mar-05 14:37 gitlabemu/glp/buildtool.py
+-rw-rw-rw-  2.0 fat      594 b- defN 23-Mar-05 14:37 gitlabemu/glp/canceltool.py
+-rw-rw-rw-  2.0 fat     1197 b- defN 23-Mar-05 14:37 gitlabemu/glp/dumptool.py
+-rw-rw-rw-  2.0 fat     1520 b- defN 23-Mar-05 14:37 gitlabemu/glp/exporttool.py
+-rw-rw-rw-  2.0 fat     1176 b- defN 23-Mar-05 14:37 gitlabemu/glp/jobstool.py
+-rw-rw-rw-  2.0 fat      576 b- defN 23-Mar-05 14:37 gitlabemu/glp/listtool.py
+-rw-rw-rw-  2.0 fat     2349 b- defN 23-Mar-05 14:37 gitlabemu/glp/subcommand.py
+-rw-rw-rw-  2.0 fat     5183 b- defN 23-Mar-05 14:37 gitlabemu/glp/subsettool.py
+-rw-rw-rw-  2.0 fat     1966 b- defN 23-Mar-05 14:37 gitlabemu/glp/tool.py
+-rw-rw-rw-  2.0 fat      811 b- defN 23-Mar-05 14:37 gitlabemu/glp/types.py
+-rw-rw-rw-  2.0 fat     3433 b- defN 23-Mar-05 14:37 gitlabemu/rules/GitlabRuleLexer.py
+-rw-rw-rw-  2.0 fat    15647 b- defN 23-Mar-05 14:37 gitlabemu/rules/GitlabRuleParser.py
+-rw-rw-rw-  2.0 fat     1597 b- defN 23-Mar-05 14:37 gitlabemu/rules/GitlabRuleVisitor.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Mar-05 14:37 gitlabemu/rules/__init__.py
+-rw-rw-rw-  2.0 fat      688 b- defN 23-Jul-07 18:25 gitlab_emulator-1.5.7.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-Jul-07 18:25 gitlab_emulator-1.5.7.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat      113 b- defN 23-Jul-07 18:25 gitlab_emulator-1.5.7.dist-info/entry_points.txt
+-rw-rw-rw-  2.0 fat       10 b- defN 23-Jul-07 18:25 gitlab_emulator-1.5.7.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     4759 b- defN 23-Jul-07 18:25 gitlab_emulator-1.5.7.dist-info/RECORD
+58 files, 249365 bytes uncompressed, 69218 bytes compressed:  72.2%
```

## zipnote {}

```diff
@@ -1,8 +1,8 @@
-Filename: gitlab_emulator-1.5.6.data/scripts/locallab.py
+Filename: gitlab_emulator-1.5.7.data/scripts/locallab.py
 Comment: 
 
 Filename: gitlabemu/__init__.py
 Comment: 
 
 Filename: gitlabemu/__main__.py
 Comment: 
@@ -153,23 +153,23 @@
 
 Filename: gitlabemu/rules/GitlabRuleVisitor.py
 Comment: 
 
 Filename: gitlabemu/rules/__init__.py
 Comment: 
 
-Filename: gitlab_emulator-1.5.6.dist-info/METADATA
+Filename: gitlab_emulator-1.5.7.dist-info/METADATA
 Comment: 
 
-Filename: gitlab_emulator-1.5.6.dist-info/WHEEL
+Filename: gitlab_emulator-1.5.7.dist-info/WHEEL
 Comment: 
 
-Filename: gitlab_emulator-1.5.6.dist-info/entry_points.txt
+Filename: gitlab_emulator-1.5.7.dist-info/entry_points.txt
 Comment: 
 
-Filename: gitlab_emulator-1.5.6.dist-info/top_level.txt
+Filename: gitlab_emulator-1.5.7.dist-info/top_level.txt
 Comment: 
 
-Filename: gitlab_emulator-1.5.6.dist-info/RECORD
+Filename: gitlab_emulator-1.5.7.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## gitlabemu/__main__.py

 * *Ordering differences only*

```diff
@@ -1,6 +1,6 @@
-"""
-Main module entrypoint for gitlabemu
-"""
-
-from .runner import run
-run()
+"""
+Main module entrypoint for gitlabemu
+"""
+
+from .runner import run
+run()
```

## gitlabemu/ansi.py

 * *Ordering differences only*

```diff
@@ -1,13 +1,13 @@
-"""
-ANSI control codes understood by gitlab
-"""
-
-ANSI_CLEAR_LINE = "\u001b[0K"
-ANSI_RED = "\u001b[31m"
-ANSI_GREEN = "\u001b[32m"
-ANSI_YELLOW = "\u001b[33m"
-ANSI_BLUE = "\u001b[34m"
-ANSI_MAGENTA = "\u001b[35m"
-ANSI_CYAN = "\u001b[36m"
-ANSI_WHITE = "\u001b[37m"
-ANSI_RESET = "\u001b[0m"
+"""
+ANSI control codes understood by gitlab
+"""
+
+ANSI_CLEAR_LINE = "\u001b[0K"
+ANSI_RED = "\u001b[31m"
+ANSI_GREEN = "\u001b[32m"
+ANSI_YELLOW = "\u001b[33m"
+ANSI_BLUE = "\u001b[34m"
+ANSI_MAGENTA = "\u001b[35m"
+ANSI_CYAN = "\u001b[36m"
+ANSI_WHITE = "\u001b[37m"
+ANSI_RESET = "\u001b[0m"
```

## gitlabemu/artifacts.py

 * *Ordering differences only*

```diff
@@ -1,30 +1,30 @@
-"""Represent job artifacts and reports"""
-from .errors import BadSyntaxError
-
-
-class GitlabArtifacts:
-    def __init__(self):
-        self.name = None
-        self.public = False
-        self.untracked = False
-        self.when = "on_success"
-        self.paths = []
-        self.exclude = []
-        self.reports = {}
-
-    def load(self, data: dict):
-        self.paths = data.get("paths", [])
-        self.exclude = data.get("exclude", [])
-        self.name = data.get("name", self.name)
-        self.public = data.get("public", self.public)
-        self.untracked = data.get("untracked", self.untracked)
-        self.reports = data.get("reports", {})
-        self.when = data.get("when", self.when)
-
-        if isinstance(self.reports.get("junit", []), str):
-            # is a string, canonicalize as a list
-            self.reports["junit"] = [self.reports["junit"]]
-
-        if not isinstance(self.paths, list):
-            raise BadSyntaxError(f"artifacts:paths must be a list of paths, got {self.paths} ({type(self.paths)} instead)")
-
+"""Represent job artifacts and reports"""
+from .errors import BadSyntaxError
+
+
+class GitlabArtifacts:
+    def __init__(self):
+        self.name = None
+        self.public = False
+        self.untracked = False
+        self.when = "on_success"
+        self.paths = []
+        self.exclude = []
+        self.reports = {}
+
+    def load(self, data: dict):
+        self.paths = data.get("paths", [])
+        self.exclude = data.get("exclude", [])
+        self.name = data.get("name", self.name)
+        self.public = data.get("public", self.public)
+        self.untracked = data.get("untracked", self.untracked)
+        self.reports = data.get("reports", {})
+        self.when = data.get("when", self.when)
+
+        if isinstance(self.reports.get("junit", []), str):
+            # is a string, canonicalize as a list
+            self.reports["junit"] = [self.reports["junit"]]
+
+        if not isinstance(self.paths, list):
+            raise BadSyntaxError(f"artifacts:paths must be a list of paths, got {self.paths} ({type(self.paths)} instead)")
+
```

## gitlabemu/chown.py

 * *Ordering differences only*

```diff
@@ -1,24 +1,24 @@
-#!/usr/bin/python3
-import os
-import time
-import argparse
-import subprocess
-
-parser = argparse.ArgumentParser(
-    description="Change all files in the current working directory to be owned by the given UID/GID")
-parser.add_argument("UID", type=int, help="UID to set")
-parser.add_argument("GID", type=int, help="GID to set")
-
-
-def run(args=None):  # pragma: cover if posix
-    opts = parser.parse_args(args=args)
-    started = time.time()
-    cmdline = ["chown", "-R", "-h", f"{str(opts.UID)}.{str(opts.GID)}", os.getcwd()]
-    print(f"Restoring ownership of {os.getcwd()} to {str(opts.UID)}.{str(opts.GID)}..")
-    subprocess.check_call(cmdline, shell=False)
-    ended = time.time()
-    print(f"Ownerships restored ({int(ended - started)} sec).")
-
-
-if __name__ == "__main__":
-    run()
+#!/usr/bin/python3
+import os
+import time
+import argparse
+import subprocess
+
+parser = argparse.ArgumentParser(
+    description="Change all files in the current working directory to be owned by the given UID/GID")
+parser.add_argument("UID", type=int, help="UID to set")
+parser.add_argument("GID", type=int, help="GID to set")
+
+
+def run(args=None):  # pragma: cover if posix
+    opts = parser.parse_args(args=args)
+    started = time.time()
+    cmdline = ["chown", "-R", "-h", f"{str(opts.UID)}.{str(opts.GID)}", os.getcwd()]
+    print(f"Restoring ownership of {os.getcwd()} to {str(opts.UID)}.{str(opts.GID)}..")
+    subprocess.check_call(cmdline, shell=False)
+    ended = time.time()
+    print(f"Ownerships restored ({int(ended - started)} sec).")
+
+
+if __name__ == "__main__":
+    run()
```

## gitlabemu/configloader.py

```diff
@@ -1,820 +1,824 @@
-"""
-Load a .gitlab-ci.yml file
-"""
-import os
-import copy
-import sys
-import tempfile
-from abc import ABC, abstractmethod
-from typing import Dict, Any, Union, Optional, List
-import requests
-
-from .errors import ConfigLoaderError, BadSyntaxError, FeatureNotSupportedError
-from .gitlab.types import RESERVED_TOP_KEYS, DEFAULT_JOB_KEYS
-from .gitlab.urls import GITLAB_ORG_TEMPLATE_BASEURL
-from .helpers import stringlist_if_string
-from .jobs import NoSuchJob, Job
-from .docker import DockerJob
-from . import yamlloader
-from .references import process_references
-from .userconfig import get_user_config_context
-from gitlabemu.ruleparser import evaluate_rule
-from .logmsg import warning, debugrule, fatal
-
-DEFAULT_CI_FILE = ".gitlab-ci.yml"
-
-
-def do_single_include(baseobj: Dict[str, Any],
-                      yamldir: str,
-                      inc: Union[str, Dict[str, Any]],
-                      handle_read=None,
-                      variables: Optional[Dict[str, str]] = None,
-                      filename: Optional[str] = None,
-                      handle_fetch_project=None) -> Dict[str, Any]:
-    """
-    Load a single included file and return it's object graph
-    :param handle_fetch_project: called to fetch an included file from another project
-    :param filename: the name of the parent file wanting to include this one
-    :param handle_read:
-    :param baseobj: previously loaded and included objects
-    :param yamldir: folder to search
-    :param inc: file to read
-    :param variables:
-    :return:
-    """
-    supported_includes = ["local", "remote", "template", "project"]
-
-    if variables is None:
-        variables = {}
-    if handle_read is None:
-        handle_read = read
-    location = None
-    inc_type = None
-    temp_content = None
-
-    if isinstance(inc, str):
-        location = inc.lstrip("/\\")
-        inc_type = "local"
-    elif isinstance(inc, dict):
-        supported = False
-        for inc_type in supported_includes:
-            if inc_type in inc:
-                supported = True
-                break
-        if not supported:
-            raise FeatureNotSupportedError(f"Do not understand how to include {inc}")
-
-        if "local" in inc:
-            location = inc["local"]
-            inc_type = "local"
-        elif "remote" in inc:
-            location = inc["remote"]
-            inc_type = "remote"
-        elif "template" in inc:  # pragma: no cover
-            location = inc["template"]
-            inc_type = "template"
-        elif "project" in inc:  # pragma: no cover
-            ref = inc.get("ref", "HEAD")
-            location = f"{inc['project']}/{inc['file']}#{ref}".replace("//", "/")
-            inc_type = "project"
-            fatal(f"Not Yet Implemented, {inc}")
-
-        rules = inc.get("rules", [])
-        if not rules:
-            debugrule(f"{filename} include: {inc}")
-        else:
-            matched_rules = False
-            for rule in rules:
-                # execute the rules, skip inclusion if none pass
-                if_rule = rule.get("if", None)
-                if if_rule:
-                    matched = evaluate_rule(if_rule, dict(variables))
-                    if matched:
-                        debugrule(f"{filename} include '{inc}' matched {if_rule}")
-                        matched_rules = True
-                        break
-            if not matched_rules:
-                debugrule(f"{filename} not including: {inc}")
-                return {}
-
-    assert location, f"cannot work out include source location: {inc}"
-    included = baseobj.get("include", [])
-    if location in included:
-        raise BadSyntaxError(f"{filename}: {location} has already been included")
-    baseobj["include"].append(location)
-
-    if inc_type == "local":
-        if os.sep != "/":  # pragma: cover if windows
-            location = location.replace("/", os.sep)
-        return handle_read(location, variables=False, validate_jobs=False, topdir=yamldir, baseobj=baseobj)
-    elif inc_type == "template":  # pragma: no cover
-        # get the template from gitlab.com
-        warning(f"Including gitlab.com template: {location}")
-        inc_type = "remote"
-        location = f"{GITLAB_ORG_TEMPLATE_BASEURL}/{location}"
-    elif inc_type == "project":  # pragma: no cover
-        warning(f"Including CI yaml from another project: {location}")
-        if not handle_fetch_project:
-            warning("no project handler added")
-        else:
-            temp_content = handle_fetch_project(inc)
-
-    if inc_type == "remote":
-        # fetch the file, no authentication needed
-        warning(f"Including remote CI yaml file: {location}")
-        resp = requests.get(location, allow_redirects=True)
-        if not resp.status_code == 200:  # pragma: no cover
-            raise ConfigLoaderError(f"HTTP error getting {location}: status {resp.status_code}")
-        temp_content = resp.text
-
-    if temp_content is not None:
-        with tempfile.TemporaryDirectory() as temp_folder:
-            path = os.path.join(str(temp_folder), "remote.yml")
-            with open(path, "w") as fd:
-                fd.write(temp_content)
-            return handle_read(path, variables=False, validate_jobs=False, topdir=str(temp_folder), baseobj=baseobj)
-
-
-def do_includes(baseobj: Dict[str, Any],
-                yamldir: str,
-                incs: Union[str, List],
-                handle_include=do_single_include,
-                filename: Optional[str] = None) -> None:
-    """
-    Deep process include directives
-    :param filename:
-    :param handle_include:
-    :param baseobj:
-    :param yamldir: load include files relative to here
-    :param incs: files to load
-    :return:
-    """
-    # include can be an array or a map.
-    #
-    # include: "/templates/scripts.yaml"
-    #
-    # include:
-    #   - "/templates/scripts.yaml"
-    #   - "/templates/windows-jobs.yaml"
-    #
-    # include:
-    #   local: "/templates/scripts.yaml"
-    #
-    # include:
-    #    - local: "/templates/scripts.yaml"
-    #      rules:
-    #        - if: $USE_SCRIPTS
-    #    - local: "/templates/after.yaml"
-    #    "/templates/windows-jobs.yaml"
-    if incs:
-        if isinstance(incs, list):
-            includes = incs
-        else:
-            includes = [incs]
-        for inc in includes:
-            obj = handle_include(baseobj, yamldir, inc, filename=filename)
-            for item in obj:
-                if item != "include":
-                    merge_dicts(baseobj, obj, item)
-
-
-def strict_needs_stages() -> bool:
-    """
-    Return True if gitlab needs requires stage (gitlab 14.1 or earlier)
-    :return:
-    """
-    ctx = get_user_config_context()
-    version = ctx.gitlab.version
-    if "." in version:
-        major, minor = version.split(".", 1)
-        if int(major) < 15:
-            return int(minor) < 2
-    return False
-
-
-class ExtendsMixin:
-
-    @staticmethod
-    def do_single_extend_recursive(alljobs: dict, default_job: Dict[str, Any], name: str) -> Dict[str, Any]:
-        """Do all the extends and !reference expansion for a single job"""
-        assert name in alljobs
-        current_un_extended = alljobs.get(name)
-        current_extended = {}
-        default_job = copy.deepcopy(default_job)
-        pipeline_variables = alljobs.get("variables", {})
-
-        base_jobs = stringlist_if_string(current_un_extended.get("extends", []))
-        if base_jobs is None:
-            base_jobs = []
-        if name in base_jobs:
-            raise BadSyntaxError(f"Job '{name}' cannot extend itself")
-
-        for base in base_jobs:
-            if base not in alljobs:
-                raise BadSyntaxError(f"Job '{name}' extends '{base}' which does not exist")
-            supp = alljobs[base]
-            if "extends" in supp:
-                supp = do_single_extend_recursive(alljobs, default_job, base)
-            recursive_merge_dicts(current_extended, supp)
-
-        # now do overrides
-        recursive_merge_dicts(current_extended, current_un_extended)
-
-        # implement inherit:
-        inherit_control = current_extended.get("inherit", {})
-        inherit_variables = inherit_control.get("variables", list(pipeline_variables.keys()))
-        inherit_default = inherit_control.get("default", list(default_job.keys()))
-
-        if "variables" not in current_extended:
-            current_extended["variables"] = {}
-
-        if inherit_variables:  # can be False or a list
-            inheritable_variables = {}
-            for varname in inherit_variables:
-                if varname in pipeline_variables:
-                    inheritable_variables[varname] = pipeline_variables[varname]
-
-            for varname, varvalue in inheritable_variables.items():
-                if varname not in current_extended["variables"]:
-                    current_extended["variables"][varname] = varvalue
-
-        if inherit_default: # can be False or a list
-            for valuekey in inherit_default:
-                if valuekey not in current_extended:
-                    if valuekey in default_job:
-                        current_extended[valuekey] = copy.deepcopy(default_job[valuekey])
-
-        if "extends" in current_extended:
-            del current_extended["extends"]
-
-        return current_extended
-
-    def do_extends(self, alljobs: Dict[str, Any]) -> None:
-        """
-        Process all the extends and !reference directives recursively
-        :return:
-        """
-        default_image = alljobs.get("image", None)
-        default_job = alljobs.get("default", None)
-        default_services = alljobs.get("services", None)
-
-        if not default_job:
-            alljobs["default"] = {}
-            if default_image:
-                alljobs["default"]["image"] = default_image
-                del alljobs["image"]
-            if default_services:
-                alljobs["default"]["services"] = default_services
-                del alljobs["services"]
-            default_job = alljobs["default"]
-
-        jobnames = [x for x in alljobs.keys() if x not in RESERVED_TOP_KEYS] + ["default"]
-
-        unextended = copy.deepcopy(alljobs)
-        for name in jobnames:
-            if name == "default":
-                unexpected_keys = [x for x in alljobs["default"].keys() if x not in DEFAULT_JOB_KEYS]
-                if unexpected_keys:
-                    raise BadSyntaxError(f"default config contains unknown keys: {unexpected_keys}")
-                continue
-            new_job = self.do_single_extend_recursive(unextended, default_job, name)
-            alljobs[name] = new_job
-
-        # flatten lists and ensure default variables are populated
-        for name in alljobs:
-            if name not in RESERVED_TOP_KEYS:
-                variables = alljobs[name].get("variables", {})
-                if name != "default":
-                    alljobs[name]["variables"] = dict(variables)
-                for scriptpart in ["before_script", "script", "after_script"]:
-                    if scriptpart in alljobs[name]:
-                        scriptlines = alljobs[name][scriptpart]
-                        newlines = []
-                        if scriptlines is not None:                            
-                            for line in scriptlines:
-                                if isinstance(line, bool):
-                                    print(f"warning, line: {line} in job {name} evaluates to a yaml boolean, you probably want to quote \"true\" or \"false\"", file=sys.stderr)
-                                    line = str(line).lower()
-                                newlines.append(line)
-                        alljobs[name][scriptpart] = list(newlines)
-
-
-def do_extends(alljobs: Dict[str, Any]) -> None:
-    em = ExtendsMixin()
-    return em.do_extends(alljobs)
-
-def do_single_extend_recursive(alljobs: dict, default_job: Dict[str, Any], name: str) -> Dict[str, Any]:
-    em = ExtendsMixin()
-    return em.do_single_extend_recursive(alljobs, default_job, name)
-
-
-def get_stages(config: Dict[str, Any]) -> List[str]:
-    """
-    Return a list of stages
-    :param config:
-    :return:
-    """
-    return config.get("stages", [".pre", "build", "test", "deploy", ".post"])
-
-
-def get_jobs(config: Dict[str, Any]) -> List[str]:
-    """
-    Return a list of job names from the given configuration
-    :param config:
-    :return:
-    """
-    jobs = []
-    for name in config:
-        if name in RESERVED_TOP_KEYS:
-            continue
-        child = config[name]
-        if isinstance(child, (dict,)):
-            jobs.append(name)
-    return jobs
-
-
-def get_job(config: Dict[str, Any], name: str) -> Optional[Dict[str, Any]]:
-    """
-    Get the job dictionary
-    :param config:
-    :param name:
-    :return:
-    """
-    assert name in get_jobs(config)
-
-    job = config.get(name)
-
-    # set some implied defaults
-    if "stage" not in job:
-        job["stage"] = "test"
-
-    return job
-
-
-def job_docker_image(config: Dict[str, Any], name: str) -> Optional[Union[str, Dict[str, Any]]]:
-    """
-    Return a docker image if a job is configured for it
-    :param config:
-    :param name:
-    :return:
-    """
-    if config.get("hide_docker"):
-        return None
-    return config[name].get("image")
-
-
-class JobLoaderMixin:
-
-    @staticmethod
-    def load_job_ex(
-            config: Dict[str, Any],
-            name: str,
-            allow_add_variables: Optional[bool] = True,
-            configloader: Optional["BaseLoader"] = None,
-            overrides: Optional[Dict[str, Any]] = None,
-    ) -> Union["Job", "DockerJob"]:
-        """Load a job from a parsed pipeline configuration dictionary"""
-        jobs = get_jobs(config)
-        if name not in jobs:
-            raise NoSuchJob(name)
-        image = job_docker_image(config, name)
-        if image:
-            job = DockerJob()
-        else:
-            job = Job()
-        job.configloader = configloader
-        job.allow_add_variables = allow_add_variables
-        job.load(name, config, overrides=overrides)
-
-        return job
-
-
-def load_job(config: Dict[str, Any],
-             name: str,
-             allow_add_variables: Optional[bool] = True,
-             configloader: Optional["BaseLoader"] = None,
-             overrides: Optional[Dict[str, Any]] = None,
-             ) -> Union["Job", "DockerJob"]:
-    """
-    Load a job from the configuration
-    :param allow_add_variables:
-    :param configloader:
-    :param config:
-    :param name:
-    :return:
-    """
-    jl = JobLoaderMixin()
-    return jl.load_job_ex(config, name,
-                          overrides=overrides,
-                          allow_add_variables=allow_add_variables,
-                          configloader=configloader)
-
-
-def compute_emulated_ci_vars(baseobj: Dict[str, Any]) -> Dict[str, Any]:
-    if "variables" not in baseobj:
-        baseobj["variables"] = {}
-
-    workspace = baseobj.get(".gitlab-emulator-workspace", None)
-    if workspace:
-        folder = os.path.basename(workspace)
-        baseobj["variables"]["CI_PROJECT_PATH"] = os.getenv(
-            "CI_PROJECT_PATH", f"local/{folder}")
-    baseobj["variables"]["CI_PIPELINE_ID"] = os.getenv(
-        "CI_PIPELINE_ID", "0")
-    baseobj["variables"]["CI_COMMIT_REF_SLUG"] = os.getenv(
-        "CI_COMMIT_REF_SLUG", "offline-build")
-    baseobj["variables"]["CI_COMMIT_SHA"] = os.getenv(
-        "CI_COMMIT_SHA", "unknown")
-    return baseobj
-
-
-class VariablesMixin:
-
-    @staticmethod
-    def handle_variables(baseobj: Optional[Dict[str, Any]], yamlfile: str) -> Dict[str, Any]:
-        baseobj = compute_emulated_ci_vars(baseobj)
-
-        for name in os.environ:
-            if name.startswith("CI_"):
-                baseobj["variables"][name] = os.getenv(name, "")
-        return compute_emulated_ci_vars(baseobj)
-
-def do_variables(baseobj: Optional[Dict[str, Any]], yamlfile: str) -> Dict[str, Any]:
-    # set CI_ values
-    baseobj = compute_emulated_ci_vars(baseobj)
-    vm = VariablesMixin()
-    return vm.handle_variables(baseobj, yamlfile)
-
-
-def merge_dicts(baseobj: dict, updated: dict, key: Any) -> None:
-    if key in updated:
-        newvalue = updated[key]
-        if key in baseobj and isinstance(baseobj[key], dict) and isinstance(newvalue, dict):
-            baseobj[key].update(newvalue)
-        else:
-            baseobj[key] = newvalue
-
-
-def recursive_merge_dicts(target: Dict[str, Any], supp: Dict[str, Any]) -> None:
-    # deep recursive merge supp into target
-    for keyname, value in supp.items():
-        current_value = target.get(keyname, None)
-        # recursive update if both are dicts
-        if isinstance(current_value, dict) and isinstance(value, dict):
-            recursive_merge_dicts(current_value, value)
-        else:
-            target[keyname] = copy.deepcopy(value)
-
-
-class ValidatorMixin:
-
-    @staticmethod
-    def validate(config: Dict[str, Any]) -> None:
-        """
-        Validate the jobs in the loaded config map, raise a GitlabEmulatorError on error
-        """
-        jobs = get_jobs(config)
-        stages = get_stages(config)
-
-        for name in jobs:
-            if name.startswith("."):
-                continue
-
-            job = get_job(config, name)
-
-            # check that script is set
-            if "trigger" not in job:
-                if "script" not in job:
-                    raise BadSyntaxError(f"Job '{name}' does not have a 'script' element.")
-
-            # check that the stage exists
-            if job["stage"] not in stages:
-                raise ConfigLoaderError("job {} has stage {} which does not exist".format(name, job["stage"]))
-
-            # check needs
-            needs = job.get("needs", [])
-            for need in needs:
-                # check the needed job exists
-                if isinstance(need, dict):
-                    need = need["job"]
-                if need not in jobs:
-                    raise ConfigLoaderError("job {} needs job {} which does not exist".format(name, need))
-
-                # check the needed job in an earlier stage if running in <14.2 mode
-                if strict_needs_stages():
-                    needed = get_job(config, need)
-                    stage_order = stages.index(job["stage"])
-                    need_stage_order = stages.index(needed["stage"])
-                    if not need_stage_order < stage_order:
-                        raise ConfigLoaderError("job {} needs {} that is not in an earlier stage".format(name, need))
-
-            if "artifacts" in job:
-                if "paths" in job["artifacts"]:
-                    if not isinstance(job["artifacts"]["paths"], list):
-                        raise ConfigLoaderError("artifacts->paths must be a list")
-                if "reports" in job["artifacts"]:
-                    if not isinstance(job["artifacts"]["reports"], dict):
-                        raise ConfigLoaderError("artifacts->reports must be a map")
-
-
-def validate(config: Dict[str, Any]) -> None:
-    vm = ValidatorMixin()
-    vm.validate(config)
-
-
-def read(
-        yamlfile: str, *,
-        variables=True,
-        validate_jobs=True,
-        topdir=None,
-        baseobj=None,
-        handle_include=do_includes,
-        handle_extends=do_extends,
-        handle_validate=validate,
-        handle_variables=do_variables
-         ) -> Dict[str, Any]:
-    """
-    Read a .gitlab-ci.yml file into python types
-    :param handle_variables:
-    :param handle_validate:
-    :param handle_extends:
-    :param handle_include:
-    :param yamlfile:
-    :param validate_jobs: if True, reject jobs with bad configuration (yet valid yaml)
-    :param variables: if True, inject a variables map (valid for top level only)
-    :param topdir: the root directory to search for include files
-    :param baseobj: the document tree loaded so far.
-    :return:
-    """
-    parent = False
-    if topdir is None:
-        topdir = os.path.dirname(yamlfile)
-    else:
-        yamlfile = os.path.join(topdir, yamlfile)
-    with open(yamlfile, "r") as yamlobj:
-        preloaded = yamlloader.ordered_load(yamlobj)
-    with open(yamlfile, "r") as yamlobj:
-        loaded = yamlloader.ordered_load(yamlobj, preloaded)
-
-    if loaded is None:
-        # file was empty?
-        loaded = {}
-
-    if not baseobj:
-        parent = True
-        baseobj = {"include": []}
-
-    for item in loaded:
-        if item != "include":
-            merge_dicts(baseobj, loaded, item)
-
-    handle_include(baseobj, topdir, loaded.get("include", []))
-    baseobj["include"].append(yamlfile)
-
-    # now process references
-    baseobj = process_references(baseobj)
-
-    if parent:
-        # now do extends
-        handle_extends(baseobj)
-
-    if validate_jobs:
-        if strict_needs_stages():
-            if "stages" not in baseobj:
-                baseobj["stages"] = ["test"]
-        handle_validate(baseobj)
-
-    if variables:
-        handle_variables(baseobj, yamlfile)
-
-    return baseobj
-
-
-class BaseLoader(ABC):
-    def __init__(self):
-        self.filename: Optional[str] = None
-        self.rootdir: Optional[str] = None
-        self.config: Dict[str, Any] = {
-            ".gle-extra_variables": {}
-        }
-        self.included_files = []
-
-        self._begun = False
-        self._done = False
-        self._current_file = None
-        self._job_sources = {}
-        self._job_classes = {}
-
-    @property
-    def variables(self) -> Dict[str, str]:
-        found = {}
-        found.update(self.config.get(".gle-extra_variables", {}))
-        found.update(self.config.get("variables", {}))
-        return found
-
-    def add_variable(self, name: str, value: Optional[str] = None) -> None:
-        """Add a pipeline variable"""
-        if value is not None:
-            self.config[".gle-extra_variables"][name] = value
-        else:
-            del self.config[".gle-extra_variables"][name]
-
-    def get_docker_image(self, jobname: str) -> Optional[str]:
-        """Get the docker image used by a job (if any)"""
-        return job_docker_image(self.config, jobname)
-
-    def get_jobs(self) -> List[str]:
-        """
-        Get the names of all jobs in the pipeline
-        :return:
-        """
-        return get_jobs(self.config)
-
-    def get_job(self, name: str) -> Dict[str, Any]:
-        """
-        Get a named job from the pipeline
-        :param name:
-        :return:
-        """
-        return get_job(self.config, name)
-
-    def get_stages(self) -> List[str]:
-        """
-        Get the list of stages
-        :return:
-        """
-        return get_stages(self.config)
-
-    @abstractmethod
-    def load(self, filename: str) -> None:
-        pass
-
-    @abstractmethod
-    def load_job(self, name: str) -> Union["Job", "DockerJob"]:
-        pass
-
-    def get_job_filename(self, jobname: str) -> Optional[str]:
-        """
-        Get the filename of for where the job is defined
-        :param jobname:
-        :return: job filename in unix format
-        """
-        jobfile = None
-        for filename in self._job_sources:
-            jobs = self._job_sources.get(filename)
-            if jobname in jobs:
-                jobfile = filename.replace("\\", "/")
-                break
-        return jobfile
-
-
-class Loader(BaseLoader, JobLoaderMixin, ValidatorMixin, ExtendsMixin):
-    """
-    A configuration loader for gitlab pipelines
-    """
-
-    def __init__(self, emulator_variables: Optional[bool] = True):
-        super().__init__()
-        self.create_emulator_variables = emulator_variables
-
-    def load_job(self, name: str, overrides: Optional[Dict[str, Any]] = None) -> Union["Job", "DockerJob"]:
-        """Return a loaded job object"""
-        job = self.load_job_ex(self.config, name,
-                               overrides=overrides,
-                               allow_add_variables=self.create_emulator_variables,
-                               configloader=self)
-        return job
-
-    def do_includes(self, baseobj: Dict[str, Any], yamldir: str, incs: Union[List, str]) -> None:
-        """
-        Process the list of include files
-        :param baseobj:
-        :param yamldir:
-        :param incs:
-        :return:
-        """
-        return do_includes(baseobj, yamldir, incs, handle_include=self.do_single_include, filename=self.filename)
-
-    def do_single_include(self,
-                          baseobj: Dict[str, Any],
-                          yamldir: str,
-                          inc: Union[str, Dict[str, Any]],
-                          filename: str
-                          ) -> Dict[str, Any]:
-        """
-        Include a single file and process it
-        """
-        return do_single_include(baseobj, yamldir, inc, handle_read=self._read, variables=self.variables, filename=filename)
-
-    def do_validate(self, baseobj: Dict[str, Any]) -> None:
-        """
-        Validate the pipeline is defined legally
-        :param baseobj:
-        :return:
-        """
-        return self.validate(baseobj)
-
-    def do_variables(self, baseobj: Dict[str, Any], yamlfile: Optional[str]) -> Dict[str, Any]:
-        """
-        Process the variables top level section
-        :param baseobj:
-        :param yamlfile:
-        :return:
-        """
-        if "variables" not in baseobj:
-            baseobj["variables"] = {}
-        baseobj[".gitlab-emulator-workspace"] = os.path.abspath(os.path.dirname(yamlfile))
-        if self.create_emulator_variables:
-            return do_variables(baseobj, yamlfile)
-
-    def _read(self,
-              filename: Optional[str],
-              baseobj: Optional[Dict[str, Any]] = None,
-              **kwargs
-              ) -> Dict[str, Any]:
-        relative_filename = "unknown"
-        if filename:
-            self._current_file = filename
-            # child triggered pipelines don't really have a file, so we should be parsing the real files here
-            if not self.included_files:
-                # first file
-                filename = os.path.abspath(filename)
-                self.rootdir = os.path.dirname(filename)
-                self.filename = os.path.basename(filename)
-                self._current_file = self.filename
-
-            relative_filename = self._current_file
-            self.included_files.append(relative_filename)
-
-        if baseobj is None:
-            before = {}
-        else:
-            before = dict(baseobj)
-
-        objdata = read(filename, **kwargs,
-                       baseobj=baseobj,
-                       handle_include=self.do_includes,
-                       handle_extends=self.do_extends,
-                       handle_validate=self.do_validate,
-                       handle_variables=self.do_variables,
-                       )
-
-        new_keys = (x for x in objdata if x not in before)
-        new_keys = [x for x in new_keys if x not in RESERVED_TOP_KEYS]
-        self._job_sources[relative_filename] = new_keys
-
-        # collapse down list-of-lists in scripts
-        for jobname in objdata:
-            if not isinstance(objdata[jobname], dict):
-                continue
-            objdata[jobname]: Dict[str, Any]
-            for script_name in ["before_script", "script", "after_script"]:
-                if script_name not in objdata[jobname]:
-                    continue
-                objdata[jobname][script_name] = normalise_script(objdata[jobname][script_name])
-
-        return objdata
-
-    def load(self, filename: str) -> None:
-        """
-        Load a pipeline configuration from disk
-        :param filename:
-        :return:
-        """
-        assert not self._done, "load() called more than once"
-        extra_vars = dict(self.config.get(".gle-extra_variables", {}))
-        self.config = self._read(filename)
-        self.config[".gle-extra_variables"] = dict(extra_vars)
-        self._done = True
-
-
-def normalise_script(script_item: Optional[Union[List[str], List[List[str]], str]]) -> List[str]:
-    """Convert scalar or 2d script lists into 1d lists"""
-    if isinstance(script_item, str):
-        return [script_item]
-    if script_item is None:
-        return []
-    # script is a list
-    result = []
-    for item in script_item:
-        if isinstance(item, list):
-            result.extend(item)
-        else:
-            result.append(item)
-    return result
-
-
-def find_ci_config(path: str) -> Optional[str]:
-    """
-    Starting in path go upwards looking for a .gitlab-ci.yml file
-    :param path:
-    :return:
-    """
-    initdir = path
-    path = os.path.abspath(path)
-    while os.path.dirname(path) != path:
-        filename = os.path.join(path, DEFAULT_CI_FILE)
-        if os.path.exists(filename):
-            return os.path.relpath(filename, initdir)
-        path = os.path.dirname(path)
-    return None
+"""
+Load a .gitlab-ci.yml file
+"""
+import os
+import copy
+import sys
+import tempfile
+from abc import ABC, abstractmethod
+from typing import Dict, Any, Union, Optional, List
+import requests
+
+from .errors import ConfigLoaderError, BadSyntaxError, FeatureNotSupportedError
+from .gitlab.types import RESERVED_TOP_KEYS, DEFAULT_JOB_KEYS
+from .gitlab.urls import GITLAB_ORG_TEMPLATE_BASEURL
+from .helpers import stringlist_if_string
+from .jobs import NoSuchJob, Job
+from .docker import DockerJob
+from . import yamlloader
+from .references import process_references
+from .userconfig import get_user_config_context
+from gitlabemu.ruleparser import evaluate_rule
+from .logmsg import warning, debugrule, fatal
+
+DEFAULT_CI_FILE = ".gitlab-ci.yml"
+
+
+def do_single_include(baseobj: Dict[str, Any],
+                      yamldir: str,
+                      inc: Union[str, Dict[str, Any]],
+                      handle_read=None,
+                      variables: Optional[Dict[str, str]] = None,
+                      filename: Optional[str] = None,
+                      handle_fetch_project=None) -> Dict[str, Any]:
+    """
+    Load a single included file and return it's object graph
+    :param handle_fetch_project: called to fetch an included file from another project
+    :param filename: the name of the parent file wanting to include this one
+    :param handle_read:
+    :param baseobj: previously loaded and included objects
+    :param yamldir: folder to search
+    :param inc: file to read
+    :param variables:
+    :return:
+    """
+    supported_includes = ["local", "remote", "template", "project"]
+
+    if variables is None:
+        variables = {}
+    if handle_read is None:
+        handle_read = read
+    location = None
+    inc_type = None
+    temp_content = None
+
+    if isinstance(inc, str):
+        location = inc.lstrip("/\\")
+        inc_type = "local"
+    elif isinstance(inc, dict):
+        supported = False
+        for inc_type in supported_includes:
+            if inc_type in inc:
+                supported = True
+                break
+        if not supported:
+            raise FeatureNotSupportedError(f"Do not understand how to include {inc}")
+
+        if "local" in inc:
+            location = inc["local"]
+            inc_type = "local"
+        elif "remote" in inc:
+            location = inc["remote"]
+            inc_type = "remote"
+        elif "template" in inc:  # pragma: no cover
+            location = inc["template"]
+            inc_type = "template"
+        elif "project" in inc:  # pragma: no cover
+            ref = inc.get("ref", "HEAD")
+            location = f"{inc['project']}/{inc['file']}#{ref}".replace("//", "/")
+            inc_type = "project"
+            fatal(f"Not Yet Implemented, {inc}")
+
+        rules = inc.get("rules", [])
+        if not rules:
+            debugrule(f"{filename} include: {inc}")
+        else:
+            matched_rules = False
+            for rule in rules:
+                # execute the rules, skip inclusion if none pass
+                if_rule = rule.get("if", None)
+                if if_rule:
+                    matched = evaluate_rule(if_rule, dict(variables))
+                    if matched:
+                        debugrule(f"{filename} include '{inc}' matched {if_rule}")
+                        matched_rules = True
+                        break
+            if not matched_rules:
+                debugrule(f"{filename} not including: {inc}")
+                return {}
+
+    assert location, f"cannot work out include source location: {inc}"
+    included = baseobj.get("include", [])
+    if location in included:
+        raise BadSyntaxError(f"{filename}: {location} has already been included")
+    baseobj["include"].append(location)
+
+    if inc_type == "local":
+        if os.sep != "/":  # pragma: cover if windows
+            location = location.replace("/", os.sep)
+        return handle_read(location, variables=False, validate_jobs=False, topdir=yamldir, baseobj=baseobj)
+    elif inc_type == "template":  # pragma: no cover
+        # get the template from gitlab.com
+        warning(f"Including gitlab.com template: {location}")
+        inc_type = "remote"
+        location = f"{GITLAB_ORG_TEMPLATE_BASEURL}/{location}"
+    elif inc_type == "project":  # pragma: no cover
+        warning(f"Including CI yaml from another project: {location}")
+        if not handle_fetch_project:
+            warning("no project handler added")
+        else:
+            temp_content = handle_fetch_project(inc)
+
+    if inc_type == "remote":
+        # fetch the file, no authentication needed
+        warning(f"Including remote CI yaml file: {location}")
+        resp = requests.get(location, allow_redirects=True)
+        if not resp.status_code == 200:  # pragma: no cover
+            raise ConfigLoaderError(f"HTTP error getting {location}: status {resp.status_code}")
+        temp_content = resp.text
+
+    if temp_content is not None:
+        with tempfile.TemporaryDirectory() as temp_folder:
+            path = os.path.join(str(temp_folder), "remote.yml")
+            with open(path, "w") as fd:
+                fd.write(temp_content)
+            return handle_read(path, variables=False, validate_jobs=False, topdir=str(temp_folder), baseobj=baseobj)
+
+
+def do_includes(baseobj: Dict[str, Any],
+                yamldir: str,
+                incs: Union[str, List],
+                handle_include=do_single_include,
+                filename: Optional[str] = None) -> None:
+    """
+    Deep process include directives
+    :param filename:
+    :param handle_include:
+    :param baseobj:
+    :param yamldir: load include files relative to here
+    :param incs: files to load
+    :return:
+    """
+    # include can be an array or a map.
+    #
+    # include: "/templates/scripts.yaml"
+    #
+    # include:
+    #   - "/templates/scripts.yaml"
+    #   - "/templates/windows-jobs.yaml"
+    #
+    # include:
+    #   local: "/templates/scripts.yaml"
+    #
+    # include:
+    #    - local: "/templates/scripts.yaml"
+    #      rules:
+    #        - if: $USE_SCRIPTS
+    #    - local: "/templates/after.yaml"
+    #    "/templates/windows-jobs.yaml"
+    if incs:
+        if isinstance(incs, list):
+            includes = incs
+        else:
+            includes = [incs]
+        for inc in includes:
+            obj = handle_include(baseobj, yamldir, inc, filename=filename)
+            for item in obj:
+                if item != "include":
+                    merge_dicts(baseobj, obj, item)
+
+
+def strict_needs_stages() -> bool:
+    """
+    Return True if gitlab needs requires stage (gitlab 14.1 or earlier)
+    :return:
+    """
+    ctx = get_user_config_context()
+    version = ctx.gitlab.version
+    if "." in version:
+        major, minor = version.split(".", 1)
+        if int(major) < 15:
+            return int(minor) < 2
+    return False
+
+
+class ExtendsMixin:
+
+    @staticmethod
+    def do_single_extend_recursive(alljobs: dict, default_job: Dict[str, Any], name: str) -> Dict[str, Any]:
+        """Do all the extends and !reference expansion for a single job"""
+        assert name in alljobs
+        current_un_extended = alljobs.get(name)
+        current_extended = {}
+        default_job = copy.deepcopy(default_job)
+        pipeline_variables = alljobs.get("variables", {})
+
+        base_jobs = stringlist_if_string(current_un_extended.get("extends", []))
+        if base_jobs is None:
+            base_jobs = []
+        if name in base_jobs:
+            raise BadSyntaxError(f"Job '{name}' cannot extend itself")
+
+        for base in base_jobs:
+            if base not in alljobs:
+                raise BadSyntaxError(f"Job '{name}' extends '{base}' which does not exist")
+            supp = alljobs[base]
+            if "extends" in supp:
+                supp = do_single_extend_recursive(alljobs, default_job, base)
+            recursive_merge_dicts(current_extended, supp)
+
+        # now do overrides
+        recursive_merge_dicts(current_extended, current_un_extended)
+
+        # implement inherit:
+        inherit_control = current_extended.get("inherit", {})
+        inherit_variables = inherit_control.get("variables", list(pipeline_variables.keys()))
+        inherit_default = inherit_control.get("default", list(default_job.keys()))
+
+        if "variables" not in current_extended:
+            current_extended["variables"] = {}
+
+        if inherit_variables:  # can be False or a list
+            inheritable_variables = {}
+            for varname in inherit_variables:
+                if varname in pipeline_variables:
+                    inheritable_variables[varname] = pipeline_variables[varname]
+
+            for varname, varvalue in inheritable_variables.items():
+                if varname not in current_extended["variables"]:
+                    current_extended["variables"][varname] = varvalue
+
+        if inherit_default: # can be False or a list
+            for valuekey in inherit_default:
+                if valuekey not in current_extended:
+                    if valuekey in default_job:
+                        current_extended[valuekey] = copy.deepcopy(default_job[valuekey])
+
+        if "extends" in current_extended:
+            del current_extended["extends"]
+
+        return current_extended
+
+    def do_extends(self, alljobs: Dict[str, Any]) -> None:
+        """
+        Process all the extends and !reference directives recursively
+        :return:
+        """
+        default_image = alljobs.get("image", None)
+        default_job = alljobs.get("default", None)
+        default_services = alljobs.get("services", None)
+
+        if not default_job:
+            alljobs["default"] = {}
+            if default_image:
+                alljobs["default"]["image"] = default_image
+                del alljobs["image"]
+            if default_services:
+                alljobs["default"]["services"] = default_services
+                del alljobs["services"]
+            default_job = alljobs["default"]
+
+        jobnames = [x for x in alljobs.keys() if x not in RESERVED_TOP_KEYS] + ["default"]
+
+        unextended = copy.deepcopy(alljobs)
+        for name in jobnames:
+            if name == "default":
+                unexpected_keys = [x for x in alljobs["default"].keys() if x not in DEFAULT_JOB_KEYS]
+                if unexpected_keys:
+                    raise BadSyntaxError(f"default config contains unknown keys: {unexpected_keys}")
+                continue
+            new_job = self.do_single_extend_recursive(unextended, default_job, name)
+            alljobs[name] = new_job
+
+        # flatten lists and ensure default variables are populated
+        for name in alljobs:
+            if name not in RESERVED_TOP_KEYS:
+                variables = alljobs[name].get("variables", {})
+                if name != "default":
+                    alljobs[name]["variables"] = dict(variables)
+                for scriptpart in ["before_script", "script", "after_script"]:
+                    if scriptpart in alljobs[name]:
+                        scriptlines = alljobs[name][scriptpart]
+                        newlines = []
+                        if scriptlines is not None:                            
+                            for line in scriptlines:
+                                if isinstance(line, bool):
+                                    print(f"warning, line: {line} in job {name} evaluates to a yaml boolean, you probably want to quote \"true\" or \"false\"", file=sys.stderr)
+                                    line = str(line).lower()
+                                newlines.append(line)
+                        alljobs[name][scriptpart] = list(newlines)
+
+
+def do_extends(alljobs: Dict[str, Any]) -> None:
+    em = ExtendsMixin()
+    return em.do_extends(alljobs)
+
+def do_single_extend_recursive(alljobs: dict, default_job: Dict[str, Any], name: str) -> Dict[str, Any]:
+    em = ExtendsMixin()
+    return em.do_single_extend_recursive(alljobs, default_job, name)
+
+
+def get_stages(config: Dict[str, Any]) -> List[str]:
+    """
+    Return a list of stages
+    :param config:
+    :return:
+    """
+    return config.get("stages", [".pre", "build", "test", "deploy", ".post"])
+
+
+def get_jobs(config: Dict[str, Any]) -> List[str]:
+    """
+    Return a list of job names from the given configuration
+    :param config:
+    :return:
+    """
+    jobs = []
+    for name in config:
+        if name in RESERVED_TOP_KEYS:
+            continue
+        child = config[name]
+        if isinstance(child, (dict,)):
+            jobs.append(name)
+    return jobs
+
+
+def get_job(config: Dict[str, Any], name: str) -> Optional[Dict[str, Any]]:
+    """
+    Get the job dictionary
+    :param config:
+    :param name:
+    :return:
+    """
+    assert name in get_jobs(config)
+
+    job = config.get(name)
+
+    # set some implied defaults
+    if "stage" not in job:
+        job["stage"] = "test"
+
+    return job
+
+
+def job_docker_image(config: Dict[str, Any], name: str) -> Optional[Union[str, Dict[str, Any]]]:
+    """
+    Return a docker image if a job is configured for it
+    :param config:
+    :param name:
+    :return:
+    """
+    if config.get("hide_docker"):
+        return None
+    return config[name].get("image")
+
+
+class JobLoaderMixin:
+
+    @staticmethod
+    def load_job_ex(
+            config: Dict[str, Any],
+            name: str,
+            allow_add_variables: Optional[bool] = True,
+            configloader: Optional["BaseLoader"] = None,
+            overrides: Optional[Dict[str, Any]] = None,
+    ) -> Union["Job", "DockerJob"]:
+        """Load a job from a parsed pipeline configuration dictionary"""
+        jobs = get_jobs(config)
+        if name not in jobs:
+            raise NoSuchJob(name)
+        image = job_docker_image(config, name)
+        if image:
+            job = DockerJob()
+        else:
+            job = Job()
+        job.configloader = configloader
+        job.allow_add_variables = allow_add_variables
+        job.load(name, config, overrides=overrides)
+
+        return job
+
+
+def load_job(config: Dict[str, Any],
+             name: str,
+             allow_add_variables: Optional[bool] = True,
+             configloader: Optional["BaseLoader"] = None,
+             overrides: Optional[Dict[str, Any]] = None,
+             ) -> Union["Job", "DockerJob"]:
+    """
+    Load a job from the configuration
+    :param allow_add_variables:
+    :param configloader:
+    :param config:
+    :param name:
+    :return:
+    """
+    jl = JobLoaderMixin()
+    return jl.load_job_ex(config, name,
+                          overrides=overrides,
+                          allow_add_variables=allow_add_variables,
+                          configloader=configloader)
+
+
+def compute_emulated_ci_vars(baseobj: Dict[str, Any]) -> Dict[str, Any]:
+    if "variables" not in baseobj:
+        baseobj["variables"] = {}
+
+    workspace = baseobj.get(".gitlab-emulator-workspace", None)
+    if workspace:
+        folder = os.path.basename(workspace)
+        baseobj["variables"]["CI_PROJECT_PATH"] = os.getenv(
+            "CI_PROJECT_PATH", f"local/{folder}")
+    baseobj["variables"]["CI_PIPELINE_ID"] = os.getenv(
+        "CI_PIPELINE_ID", "0")
+    baseobj["variables"]["CI_COMMIT_REF_SLUG"] = os.getenv(
+        "CI_COMMIT_REF_SLUG", "offline-build")
+    baseobj["variables"]["CI_COMMIT_SHA"] = os.getenv(
+        "CI_COMMIT_SHA", "unknown")
+    return baseobj
+
+
+class VariablesMixin:
+
+    @staticmethod
+    def handle_variables(baseobj: Optional[Dict[str, Any]], yamlfile: str) -> Dict[str, Any]:
+        baseobj = compute_emulated_ci_vars(baseobj)
+
+        for name in os.environ:
+            if name.startswith("CI_"):
+                baseobj["variables"][name] = os.getenv(name, "")
+        return compute_emulated_ci_vars(baseobj)
+
+def do_variables(baseobj: Optional[Dict[str, Any]], yamlfile: str) -> Dict[str, Any]:
+    # set CI_ values
+    baseobj = compute_emulated_ci_vars(baseobj)
+    vm = VariablesMixin()
+    return vm.handle_variables(baseobj, yamlfile)
+
+
+def merge_dicts(baseobj: dict, updated: dict, key: Any) -> None:
+    if key in updated:
+        newvalue = updated[key]
+        if key in baseobj and isinstance(baseobj[key], dict) and isinstance(newvalue, dict):
+            baseobj[key].update(newvalue)
+        else:
+            baseobj[key] = newvalue
+
+
+def recursive_merge_dicts(target: Dict[str, Any], supp: Dict[str, Any]) -> None:
+    # deep recursive merge supp into target
+    for keyname, value in supp.items():
+        current_value = target.get(keyname, None)
+        # recursive update if both are dicts
+        if isinstance(current_value, dict) and isinstance(value, dict):
+            recursive_merge_dicts(current_value, value)
+        else:
+            target[keyname] = copy.deepcopy(value)
+
+
+class ValidatorMixin:
+
+    @staticmethod
+    def validate(config: Dict[str, Any]) -> None:
+        """
+        Validate the jobs in the loaded config map, raise a GitlabEmulatorError on error
+        """
+        jobs = get_jobs(config)
+        stages = list(get_stages(config))
+        if ".pre" not in stages:
+            stages.insert(0, ".pre")
+        if ".post" not in stages:
+            stages.append(".post")
+
+        for name in jobs:
+            if name.startswith("."):
+                continue
+
+            job = get_job(config, name)
+
+            # check that script is set
+            if "trigger" not in job:
+                if "script" not in job:
+                    raise BadSyntaxError(f"Job '{name}' does not have a 'script' element.")
+
+            # check that the stage exists
+            if job["stage"] not in stages:
+                raise ConfigLoaderError("job {} has stage {} which does not exist".format(name, job["stage"]))
+
+            # check needs
+            needs = job.get("needs", [])
+            for need in needs:
+                # check the needed job exists
+                if isinstance(need, dict):
+                    need = need["job"]
+                if need not in jobs:
+                    raise ConfigLoaderError("job {} needs job {} which does not exist".format(name, need))
+
+                # check the needed job in an earlier stage if running in <14.2 mode
+                if strict_needs_stages():
+                    needed = get_job(config, need)
+                    stage_order = stages.index(job["stage"])
+                    need_stage_order = stages.index(needed["stage"])
+                    if not need_stage_order < stage_order:
+                        raise ConfigLoaderError("job {} needs {} that is not in an earlier stage".format(name, need))
+
+            if "artifacts" in job:
+                if "paths" in job["artifacts"]:
+                    if not isinstance(job["artifacts"]["paths"], list):
+                        raise ConfigLoaderError("artifacts->paths must be a list")
+                if "reports" in job["artifacts"]:
+                    if not isinstance(job["artifacts"]["reports"], dict):
+                        raise ConfigLoaderError("artifacts->reports must be a map")
+
+
+def validate(config: Dict[str, Any]) -> None:
+    vm = ValidatorMixin()
+    vm.validate(config)
+
+
+def read(
+        yamlfile: str, *,
+        variables=True,
+        validate_jobs=True,
+        topdir=None,
+        baseobj=None,
+        handle_include=do_includes,
+        handle_extends=do_extends,
+        handle_validate=validate,
+        handle_variables=do_variables
+         ) -> Dict[str, Any]:
+    """
+    Read a .gitlab-ci.yml file into python types
+    :param handle_variables:
+    :param handle_validate:
+    :param handle_extends:
+    :param handle_include:
+    :param yamlfile:
+    :param validate_jobs: if True, reject jobs with bad configuration (yet valid yaml)
+    :param variables: if True, inject a variables map (valid for top level only)
+    :param topdir: the root directory to search for include files
+    :param baseobj: the document tree loaded so far.
+    :return:
+    """
+    parent = False
+    if topdir is None:
+        topdir = os.path.dirname(yamlfile)
+    else:
+        yamlfile = os.path.join(topdir, yamlfile)
+    with open(yamlfile, "r") as yamlobj:
+        preloaded = yamlloader.ordered_load(yamlobj)
+    with open(yamlfile, "r") as yamlobj:
+        loaded = yamlloader.ordered_load(yamlobj, preloaded)
+
+    if loaded is None:
+        # file was empty?
+        loaded = {}
+
+    if not baseobj:
+        parent = True
+        baseobj = {"include": []}
+
+    for item in loaded:
+        if item != "include":
+            merge_dicts(baseobj, loaded, item)
+
+    handle_include(baseobj, topdir, loaded.get("include", []))
+    baseobj["include"].append(yamlfile)
+
+    # now process references
+    baseobj = process_references(baseobj)
+
+    if parent:
+        # now do extends
+        handle_extends(baseobj)
+
+    if validate_jobs:
+        if strict_needs_stages():
+            if "stages" not in baseobj:
+                baseobj["stages"] = ["test"]
+        handle_validate(baseobj)
+
+    if variables:
+        handle_variables(baseobj, yamlfile)
+
+    return baseobj
+
+
+class BaseLoader(ABC):
+    def __init__(self):
+        self.filename: Optional[str] = None
+        self.rootdir: Optional[str] = None
+        self.config: Dict[str, Any] = {
+            ".gle-extra_variables": {}
+        }
+        self.included_files = []
+
+        self._begun = False
+        self._done = False
+        self._current_file = None
+        self._job_sources = {}
+        self._job_classes = {}
+
+    @property
+    def variables(self) -> Dict[str, str]:
+        found = {}
+        found.update(self.config.get(".gle-extra_variables", {}))
+        found.update(self.config.get("variables", {}))
+        return found
+
+    def add_variable(self, name: str, value: Optional[str] = None) -> None:
+        """Add a pipeline variable"""
+        if value is not None:
+            self.config[".gle-extra_variables"][name] = value
+        else:
+            del self.config[".gle-extra_variables"][name]
+
+    def get_docker_image(self, jobname: str) -> Optional[str]:
+        """Get the docker image used by a job (if any)"""
+        return job_docker_image(self.config, jobname)
+
+    def get_jobs(self) -> List[str]:
+        """
+        Get the names of all jobs in the pipeline
+        :return:
+        """
+        return get_jobs(self.config)
+
+    def get_job(self, name: str) -> Dict[str, Any]:
+        """
+        Get a named job from the pipeline
+        :param name:
+        :return:
+        """
+        return get_job(self.config, name)
+
+    def get_stages(self) -> List[str]:
+        """
+        Get the list of stages
+        :return:
+        """
+        return get_stages(self.config)
+
+    @abstractmethod
+    def load(self, filename: str) -> None:
+        pass
+
+    @abstractmethod
+    def load_job(self, name: str) -> Union["Job", "DockerJob"]:
+        pass
+
+    def get_job_filename(self, jobname: str) -> Optional[str]:
+        """
+        Get the filename of for where the job is defined
+        :param jobname:
+        :return: job filename in unix format
+        """
+        jobfile = None
+        for filename in self._job_sources:
+            jobs = self._job_sources.get(filename)
+            if jobname in jobs:
+                jobfile = filename.replace("\\", "/")
+                break
+        return jobfile
+
+
+class Loader(BaseLoader, JobLoaderMixin, ValidatorMixin, ExtendsMixin):
+    """
+    A configuration loader for gitlab pipelines
+    """
+
+    def __init__(self, emulator_variables: Optional[bool] = True):
+        super().__init__()
+        self.create_emulator_variables = emulator_variables
+
+    def load_job(self, name: str, overrides: Optional[Dict[str, Any]] = None) -> Union["Job", "DockerJob"]:
+        """Return a loaded job object"""
+        job = self.load_job_ex(self.config, name,
+                               overrides=overrides,
+                               allow_add_variables=self.create_emulator_variables,
+                               configloader=self)
+        return job
+
+    def do_includes(self, baseobj: Dict[str, Any], yamldir: str, incs: Union[List, str]) -> None:
+        """
+        Process the list of include files
+        :param baseobj:
+        :param yamldir:
+        :param incs:
+        :return:
+        """
+        return do_includes(baseobj, yamldir, incs, handle_include=self.do_single_include, filename=self.filename)
+
+    def do_single_include(self,
+                          baseobj: Dict[str, Any],
+                          yamldir: str,
+                          inc: Union[str, Dict[str, Any]],
+                          filename: str
+                          ) -> Dict[str, Any]:
+        """
+        Include a single file and process it
+        """
+        return do_single_include(baseobj, yamldir, inc, handle_read=self._read, variables=self.variables, filename=filename)
+
+    def do_validate(self, baseobj: Dict[str, Any]) -> None:
+        """
+        Validate the pipeline is defined legally
+        :param baseobj:
+        :return:
+        """
+        return self.validate(baseobj)
+
+    def do_variables(self, baseobj: Dict[str, Any], yamlfile: Optional[str]) -> Dict[str, Any]:
+        """
+        Process the variables top level section
+        :param baseobj:
+        :param yamlfile:
+        :return:
+        """
+        if "variables" not in baseobj:
+            baseobj["variables"] = {}
+        baseobj[".gitlab-emulator-workspace"] = os.path.abspath(os.path.dirname(yamlfile))
+        if self.create_emulator_variables:
+            return do_variables(baseobj, yamlfile)
+
+    def _read(self,
+              filename: Optional[str],
+              baseobj: Optional[Dict[str, Any]] = None,
+              **kwargs
+              ) -> Dict[str, Any]:
+        relative_filename = "unknown"
+        if filename:
+            self._current_file = filename
+            # child triggered pipelines don't really have a file, so we should be parsing the real files here
+            if not self.included_files:
+                # first file
+                filename = os.path.abspath(filename)
+                self.rootdir = os.path.dirname(filename)
+                self.filename = os.path.basename(filename)
+                self._current_file = self.filename
+
+            relative_filename = self._current_file
+            self.included_files.append(relative_filename)
+
+        if baseobj is None:
+            before = {}
+        else:
+            before = dict(baseobj)
+
+        objdata = read(filename, **kwargs,
+                       baseobj=baseobj,
+                       handle_include=self.do_includes,
+                       handle_extends=self.do_extends,
+                       handle_validate=self.do_validate,
+                       handle_variables=self.do_variables,
+                       )
+
+        new_keys = (x for x in objdata if x not in before)
+        new_keys = [x for x in new_keys if x not in RESERVED_TOP_KEYS]
+        self._job_sources[relative_filename] = new_keys
+
+        # collapse down list-of-lists in scripts
+        for jobname in objdata:
+            if not isinstance(objdata[jobname], dict):
+                continue
+            objdata[jobname]: Dict[str, Any]
+            for script_name in ["before_script", "script", "after_script"]:
+                if script_name not in objdata[jobname]:
+                    continue
+                objdata[jobname][script_name] = normalise_script(objdata[jobname][script_name])
+
+        return objdata
+
+    def load(self, filename: str) -> None:
+        """
+        Load a pipeline configuration from disk
+        :param filename:
+        :return:
+        """
+        assert not self._done, "load() called more than once"
+        extra_vars = dict(self.config.get(".gle-extra_variables", {}))
+        self.config = self._read(filename)
+        self.config[".gle-extra_variables"] = dict(extra_vars)
+        self._done = True
+
+
+def normalise_script(script_item: Optional[Union[List[str], List[List[str]], str]]) -> List[str]:
+    """Convert scalar or 2d script lists into 1d lists"""
+    if isinstance(script_item, str):
+        return [script_item]
+    if script_item is None:
+        return []
+    # script is a list
+    result = []
+    for item in script_item:
+        if isinstance(item, list):
+            result.extend(item)
+        else:
+            result.append(item)
+    return result
+
+
+def find_ci_config(path: str) -> Optional[str]:
+    """
+    Starting in path go upwards looking for a .gitlab-ci.yml file
+    :param path:
+    :return:
+    """
+    initdir = path
+    path = os.path.abspath(path)
+    while os.path.dirname(path) != path:
+        filename = os.path.join(path, DEFAULT_CI_FILE)
+        if os.path.exists(filename):
+            return os.path.relpath(filename, initdir)
+        path = os.path.dirname(path)
+    return None
```

## gitlabemu/configtool.py

 * *Ordering differences only*

```diff
@@ -1,58 +1,58 @@
-"""
-Configure gitlab emulator context, servers, local variables and docker bind mounts
-"""
-from argparse import ArgumentParser, Namespace
-
-from . import configtool_ctx
-from . import configtool_gitlab
-from . import configtool_runner
-from . import configtool_vars
-from . import configtool_volumes
-from .userconfig import get_user_config
-
-GLOBAL_DESC = __doc__
-
-
-def win_shell_cmd(opts: Namespace):
-    cfg = get_user_config()
-    current = cfg.current_context
-    if opts.cmd or opts.powershell:
-        if opts.cmd:
-            cfg.contexts[current].windows.cmd = True
-        elif opts.powershell:
-            cfg.contexts[current].windows.cmd = False
-        cfg.save()
-
-    if cfg.contexts[current].windows.cmd:
-        print("Windows shell is cmd")
-    else:
-        print("Windows shell is powershell")
-
-
-def main(args=None):
-    parser = ArgumentParser(description=GLOBAL_DESC)
-    subparsers = parser.add_subparsers()
-    configtool_ctx.setup_cmd(subparsers)
-    configtool_gitlab.setup_cmd(subparsers)
-    configtool_vars.setup_cmd(subparsers)
-    configtool_volumes.setup_cmd(subparsers)
-    configtool_runner.setup_cmd(subparsers)
-
-    # might remove this feature
-    win_shell = subparsers.add_parser("windows-shell", help="Set the global shell for windows jobs (default is powershell)")
-    win_shell_grp = win_shell.add_mutually_exclusive_group()
-    win_shell_grp.add_argument("--cmd", default=False, action="store_true",
-                               help="Use cmd for jobs")
-    win_shell_grp.add_argument("--powershell", default=False, action="store_true",
-                               help="Use powershell for jobs (default)")
-    win_shell.set_defaults(func=win_shell_cmd)
-
-    opts = parser.parse_args(args)
-    if hasattr(opts, "func"):
-        opts.func(opts)
-    else:
-        parser.print_usage()
-
-
-if __name__ == "__main__":
-    main()
+"""
+Configure gitlab emulator context, servers, local variables and docker bind mounts
+"""
+from argparse import ArgumentParser, Namespace
+
+from . import configtool_ctx
+from . import configtool_gitlab
+from . import configtool_runner
+from . import configtool_vars
+from . import configtool_volumes
+from .userconfig import get_user_config
+
+GLOBAL_DESC = __doc__
+
+
+def win_shell_cmd(opts: Namespace):
+    cfg = get_user_config()
+    current = cfg.current_context
+    if opts.cmd or opts.powershell:
+        if opts.cmd:
+            cfg.contexts[current].windows.cmd = True
+        elif opts.powershell:
+            cfg.contexts[current].windows.cmd = False
+        cfg.save()
+
+    if cfg.contexts[current].windows.cmd:
+        print("Windows shell is cmd")
+    else:
+        print("Windows shell is powershell")
+
+
+def main(args=None):
+    parser = ArgumentParser(description=GLOBAL_DESC)
+    subparsers = parser.add_subparsers()
+    configtool_ctx.setup_cmd(subparsers)
+    configtool_gitlab.setup_cmd(subparsers)
+    configtool_vars.setup_cmd(subparsers)
+    configtool_volumes.setup_cmd(subparsers)
+    configtool_runner.setup_cmd(subparsers)
+
+    # might remove this feature
+    win_shell = subparsers.add_parser("windows-shell", help="Set the global shell for windows jobs (default is powershell)")
+    win_shell_grp = win_shell.add_mutually_exclusive_group()
+    win_shell_grp.add_argument("--cmd", default=False, action="store_true",
+                               help="Use cmd for jobs")
+    win_shell_grp.add_argument("--powershell", default=False, action="store_true",
+                               help="Use powershell for jobs (default)")
+    win_shell.set_defaults(func=win_shell_cmd)
+
+    opts = parser.parse_args(args)
+    if hasattr(opts, "func"):
+        opts.func(opts)
+    else:
+        parser.print_usage()
+
+
+if __name__ == "__main__":
+    main()
```

## gitlabemu/configtool_ctx.py

 * *Ordering differences only*

```diff
@@ -1,43 +1,43 @@
-from argparse import Namespace, ArgumentParser
-
-from .helpers import notice
-from .userconfig import get_user_config
-from .userconfigdata import DEFAULT_CONTEXT, UserContext
-
-
-def setup_cmd(subparsers):
-    set_ctx = subparsers.add_parser("context", help="Show/select the current and available gle contexts")
-    set_ctx.add_argument("NAME", type=str, help="Name of the context to use (or create)", nargs="?")
-    set_ctx.add_argument("--remove", default=False, action="store_true",
-                         help="Remove the context")
-    set_ctx.set_defaults(func=set_context_cmd)
-
-
-def set_context_cmd(opts: Namespace):
-    if opts.NAME is None:
-        print_contexts()
-    else:
-        cfg = get_user_config()
-        name = opts.NAME
-        if opts.remove:
-            if name in cfg.contexts:
-                notice(f"delete context {name}")
-                del cfg.contexts[name]
-            if name == cfg.current_context:
-                cfg.current_context = DEFAULT_CONTEXT
-        else:
-            cfg.current_context = name
-            if name not in cfg.contexts:
-                cfg.contexts[name] = UserContext()
-        notice(f"gle context set to {cfg.current_context}")
-        cfg.save()
-
-
-def print_contexts():
-    cfg = get_user_config()
-    current = cfg.current_context
-    for item in cfg.contexts:
-        mark = " "
-        if item == current:
-            mark = "*"
-        print(f"{mark} {item}")
+from argparse import Namespace, ArgumentParser
+
+from .helpers import notice
+from .userconfig import get_user_config
+from .userconfigdata import DEFAULT_CONTEXT, UserContext
+
+
+def setup_cmd(subparsers):
+    set_ctx = subparsers.add_parser("context", help="Show/select the current and available gle contexts")
+    set_ctx.add_argument("NAME", type=str, help="Name of the context to use (or create)", nargs="?")
+    set_ctx.add_argument("--remove", default=False, action="store_true",
+                         help="Remove the context")
+    set_ctx.set_defaults(func=set_context_cmd)
+
+
+def set_context_cmd(opts: Namespace):
+    if opts.NAME is None:
+        print_contexts()
+    else:
+        cfg = get_user_config()
+        name = opts.NAME
+        if opts.remove:
+            if name in cfg.contexts:
+                notice(f"delete context {name}")
+                del cfg.contexts[name]
+            if name == cfg.current_context:
+                cfg.current_context = DEFAULT_CONTEXT
+        else:
+            cfg.current_context = name
+            if name not in cfg.contexts:
+                cfg.contexts[name] = UserContext()
+        notice(f"gle context set to {cfg.current_context}")
+        cfg.save()
+
+
+def print_contexts():
+    cfg = get_user_config()
+    current = cfg.current_context
+    for item in cfg.contexts:
+        mark = " "
+        if item == current:
+            mark = "*"
+        print(f"{mark} {item}")
```

## gitlabemu/configtool_gitlab.py

 * *Ordering differences only*

```diff
@@ -1,39 +1,39 @@
-from argparse import Namespace
-
-from .helpers import die
-from .userconfig import get_user_config
-
-
-def setup_cmd(subparsers):
-    gl_ctx = subparsers.add_parser("gitlab", help="Update remote gitlab configurations")
-    gl_ctx.add_argument("NAME", type=str, help="Set the name", default=None, nargs="?")
-    gl_ctx.add_argument("--server", type=str, help="Set the URL for a gitlab server",
-                        default=None)
-    gl_ctx.add_argument("--insecure", default=True, action="store_false", dest="tls_verify",
-                        help="Disable TLS certificate verification for this server (default is to verify)")
-    gl_ctx.add_argument("--token", type=str,
-                        help="Set the gitlab API token (should have git and api write access for best use)")
-    gl_ctx.set_defaults(func=gitlab_cmd)
-
-
-def gitlab_cmd(opts: Namespace):
-    cfg = get_user_config()
-    ctx = cfg.contexts[cfg.current_context]
-    if not opts.NAME:
-        # list
-        for item in ctx.gitlab.servers:
-            print(f"{item.name:32} {item.server}")
-    else:
-        matched = [x for x in ctx.gitlab.servers if x.name == opts.NAME]
-        if len(matched):
-            first = matched[0]
-            if opts.token:
-                first.token = opts.token
-            first.tls_verify = opts.tls_verify
-        else:
-            # add a new one
-            if opts.server and opts.token:
-                ctx.gitlab.add(opts.NAME, opts.server, opts.token, opts.tls_verify)
-            else:
-                die("Adding a new gitlab server entry requires --server URL and --token TOKEN")
-        cfg.save()
+from argparse import Namespace
+
+from .helpers import die
+from .userconfig import get_user_config
+
+
+def setup_cmd(subparsers):
+    gl_ctx = subparsers.add_parser("gitlab", help="Update remote gitlab configurations")
+    gl_ctx.add_argument("NAME", type=str, help="Set the name", default=None, nargs="?")
+    gl_ctx.add_argument("--server", type=str, help="Set the URL for a gitlab server",
+                        default=None)
+    gl_ctx.add_argument("--insecure", default=True, action="store_false", dest="tls_verify",
+                        help="Disable TLS certificate verification for this server (default is to verify)")
+    gl_ctx.add_argument("--token", type=str,
+                        help="Set the gitlab API token (should have git and api write access for best use)")
+    gl_ctx.set_defaults(func=gitlab_cmd)
+
+
+def gitlab_cmd(opts: Namespace):
+    cfg = get_user_config()
+    ctx = cfg.contexts[cfg.current_context]
+    if not opts.NAME:
+        # list
+        for item in ctx.gitlab.servers:
+            print(f"{item.name:32} {item.server}")
+    else:
+        matched = [x for x in ctx.gitlab.servers if x.name == opts.NAME]
+        if len(matched):
+            first = matched[0]
+            if opts.token:
+                first.token = opts.token
+            first.tls_verify = opts.tls_verify
+        else:
+            # add a new one
+            if opts.server and opts.token:
+                ctx.gitlab.add(opts.NAME, opts.server, opts.token, opts.tls_verify)
+            else:
+                die("Adding a new gitlab server entry requires --server URL and --token TOKEN")
+        cfg.save()
```

## gitlabemu/configtool_runner.py

 * *Ordering differences only*

```diff
@@ -1,129 +1,129 @@
-import sys
-from argparse import Namespace
-
-import yaml
-
-from .helpers import note, die, truth_string, setenv_string
-from .userconfig import get_user_config
-from .userconfigdata import GleRunnerConfig, DockerExecutorConfig, RUNNER_SHELL_SHELLS, RUNNER_EXECUTOR_TYPES, \
-    EXECUTOR_DOCKER, DEFAULT_DOCKER_CLI
-
-
-def setup_cmd(subparsers):
-    runner = subparsers.add_parser("runner", help="Manage docker and shell executor settings and job tag handling")
-    runner.add_argument("action", metavar="ACTION", type=str, choices=["add", "rm", "edit", "ls"],
-                        nargs="?",
-                        default="ls",
-                        help="one of: add, rm, edit, ls")
-    runner.add_argument("name", nargs="?", type=str,
-                        help="Runner name to add/remove/edit")
-    runner.add_argument("--tags", type=str, default=None,
-                        help="Set the runner tags")
-    runner.add_argument("--executor", type=str, choices=RUNNER_EXECUTOR_TYPES, default="docker",
-                        help="Runner type")
-    runner.add_argument("--untagged", default=None, type=truth_string, metavar="BOOL",
-                        help="Make the runner run untagged jobs")
-
-    runner.add_argument("--set", dest="setenv", type=setenv_string, metavar="NAME=VALUE", default=None,
-                        help="Set an environment variable")
-    runner.add_argument("--unset", dest="unsetenv", type=str, metavar="NAME", default=None,
-                        help="Remove an environment variable")
-
-    docker_group = runner.add_argument_group("docker runners")
-    shell_group = runner.add_argument_group("shell options")
-
-    docker_group.add_argument("--privileged", default=None, type=truth_string, metavar="BOOL",
-                              help="Set a docker executor to use --privileged or not")
-    docker_group.add_argument("--add-volume", type=str, metavar="VOLUME",
-                              help="Add a docker volume")
-    docker_group.add_argument("--remove-volume", type=str, metavar="VOLUME",
-                              help="Remove a docker volume")
-    docker_group.add_argument("--docker-cli", type=str, metavar="TOOL", default=None,
-                              help="Use an alternate docker cli program (eg podman, nerdctl etc)")
-
-    shell_group.add_argument("--shell", default=None, type=str, choices=RUNNER_SHELL_SHELLS,
-                             help="Set the shell executor shell")
-
-    runner.set_defaults(func=runner_cmd)
-
-
-def runner_cmd(opts: Namespace):
-    cfg = get_user_config()
-    note(f"using {cfg.filename}")
-    ctx = cfg.contexts[cfg.current_context]
-
-    if opts.action == "ls":
-        print(f"{'name':<18} {'executor':<0} {' ':<7} {'tags':<32} {'untagged':<8}", file=sys.stderr)
-        print("-" * 78, file=sys.stderr, flush=True)
-        runners = ctx.runners + ctx.builtin_runners()
-        for runner in runners:
-            opt = ""
-            if runner.executor == "docker":
-                if runner.docker.privileged:
-                    opt = "priv"
-            taglist = list(runner.tags)
-            tags = ','.join(taglist)
-            untagged = " "
-            if runner.run_untagged:
-                untagged = "*"
-            print(f"{runner.name:18} {runner.executor:8} {opt:7} {tags:32} {untagged:8}")
-    elif opts.action in ["add", "edit"]:
-        if opts.action == "add":
-            runner = GleRunnerConfig()
-            if not opts.name:
-                die("missing required runner name")
-            runner.name = opts.name
-            runner.executor = opts.executor
-            if not ctx.can_add_name(runner.name):
-                if ctx.get_runner(runner.name) is not None:
-                    die(f"A runner named {runner.name} already exists")
-                die(f"Cannot add a new runner named {runner.name}")
-            if runner.executor == "docker":
-                runner.docker = DockerExecutorConfig()
-        else:
-            runner = ctx.get_runner(opts.name, builtins=True)
-            if runner is None:
-                die(f"No such runner {opts.name}")
-        before = str(runner.to_dict())
-        if opts.untagged is not None:
-            runner.run_untagged = opts.untagged
-        if opts.tags is not None:
-            runner.tags = opts.tags.split(",")
-        if runner.executor == "docker":
-            if opts.privileged is not None:
-                runner.docker.privileged = opts.privileged
-            if opts.add_volume:
-                runner.docker.add_volume(opts.add_volume)
-            if opts.remove_volume:
-                runner.docker.remove_volume(opts.remove_volume)
-            if opts.docker_cli is not None:
-                runner.docker.docker_cli = opts.docker_cli
-        if opts.setenv is not None:
-            set_envname, set_envval = opts.setenv
-            runner.environment[set_envname] = set_envval
-        if opts.unsetenv is not None:
-            if opts.unsetenv in runner.environment:
-                del runner.environment[opts.unsetenv]
-        if opts.shell is not None:
-            runner.shell = opts.shell
-        if not runner.is_builtin:
-            ctx.save_runner(runner)
-        after = str(runner.to_dict())
-        if after != before:
-            if runner.is_builtin:
-                # some things we can alter in the global settings
-                if runner.executor == EXECUTOR_DOCKER:
-                    ctx.docker.privileged = runner.docker.privileged
-                    ctx.docker.volumes = runner.docker.volumes
-                    ctx.docker.variables = runner.environment
-                    ctx.docker.docker_cli = runner.docker.docker_cli
-            note(f"saved runner {runner.name} :-")
-        else:
-            note(f"current settings for runner {runner.name} :-")
-        print(yaml.safe_dump(runner.to_dict(), indent=4))
-        cfg.save()
-    elif opts.action == "rm":
-        if ctx.get_runner(opts.name) is not None:
-            ctx.runners = [x for x in ctx.runners if x.name != opts.name]
-            cfg.save()
-            note(f"removed runner {opts.name}")
+import sys
+from argparse import Namespace
+
+import yaml
+
+from .helpers import note, die, truth_string, setenv_string
+from .userconfig import get_user_config
+from .userconfigdata import GleRunnerConfig, DockerExecutorConfig, RUNNER_SHELL_SHELLS, RUNNER_EXECUTOR_TYPES, \
+    EXECUTOR_DOCKER, DEFAULT_DOCKER_CLI
+
+
+def setup_cmd(subparsers):
+    runner = subparsers.add_parser("runner", help="Manage docker and shell executor settings and job tag handling")
+    runner.add_argument("action", metavar="ACTION", type=str, choices=["add", "rm", "edit", "ls"],
+                        nargs="?",
+                        default="ls",
+                        help="one of: add, rm, edit, ls")
+    runner.add_argument("name", nargs="?", type=str,
+                        help="Runner name to add/remove/edit")
+    runner.add_argument("--tags", type=str, default=None,
+                        help="Set the runner tags")
+    runner.add_argument("--executor", type=str, choices=RUNNER_EXECUTOR_TYPES, default="docker",
+                        help="Runner type")
+    runner.add_argument("--untagged", default=None, type=truth_string, metavar="BOOL",
+                        help="Make the runner run untagged jobs")
+
+    runner.add_argument("--set", dest="setenv", type=setenv_string, metavar="NAME=VALUE", default=None,
+                        help="Set an environment variable")
+    runner.add_argument("--unset", dest="unsetenv", type=str, metavar="NAME", default=None,
+                        help="Remove an environment variable")
+
+    docker_group = runner.add_argument_group("docker runners")
+    shell_group = runner.add_argument_group("shell options")
+
+    docker_group.add_argument("--privileged", default=None, type=truth_string, metavar="BOOL",
+                              help="Set a docker executor to use --privileged or not")
+    docker_group.add_argument("--add-volume", type=str, metavar="VOLUME",
+                              help="Add a docker volume")
+    docker_group.add_argument("--remove-volume", type=str, metavar="VOLUME",
+                              help="Remove a docker volume")
+    docker_group.add_argument("--docker-cli", type=str, metavar="TOOL", default=None,
+                              help="Use an alternate docker cli program (eg podman, nerdctl etc)")
+
+    shell_group.add_argument("--shell", default=None, type=str, choices=RUNNER_SHELL_SHELLS,
+                             help="Set the shell executor shell")
+
+    runner.set_defaults(func=runner_cmd)
+
+
+def runner_cmd(opts: Namespace):
+    cfg = get_user_config()
+    note(f"using {cfg.filename}")
+    ctx = cfg.contexts[cfg.current_context]
+
+    if opts.action == "ls":
+        print(f"{'name':<18} {'executor':<0} {' ':<7} {'tags':<32} {'untagged':<8}", file=sys.stderr)
+        print("-" * 78, file=sys.stderr, flush=True)
+        runners = ctx.runners + ctx.builtin_runners()
+        for runner in runners:
+            opt = ""
+            if runner.executor == "docker":
+                if runner.docker.privileged:
+                    opt = "priv"
+            taglist = list(runner.tags)
+            tags = ','.join(taglist)
+            untagged = " "
+            if runner.run_untagged:
+                untagged = "*"
+            print(f"{runner.name:18} {runner.executor:8} {opt:7} {tags:32} {untagged:8}")
+    elif opts.action in ["add", "edit"]:
+        if opts.action == "add":
+            runner = GleRunnerConfig()
+            if not opts.name:
+                die("missing required runner name")
+            runner.name = opts.name
+            runner.executor = opts.executor
+            if not ctx.can_add_name(runner.name):
+                if ctx.get_runner(runner.name) is not None:
+                    die(f"A runner named {runner.name} already exists")
+                die(f"Cannot add a new runner named {runner.name}")
+            if runner.executor == "docker":
+                runner.docker = DockerExecutorConfig()
+        else:
+            runner = ctx.get_runner(opts.name, builtins=True)
+            if runner is None:
+                die(f"No such runner {opts.name}")
+        before = str(runner.to_dict())
+        if opts.untagged is not None:
+            runner.run_untagged = opts.untagged
+        if opts.tags is not None:
+            runner.tags = opts.tags.split(",")
+        if runner.executor == "docker":
+            if opts.privileged is not None:
+                runner.docker.privileged = opts.privileged
+            if opts.add_volume:
+                runner.docker.add_volume(opts.add_volume)
+            if opts.remove_volume:
+                runner.docker.remove_volume(opts.remove_volume)
+            if opts.docker_cli is not None:
+                runner.docker.docker_cli = opts.docker_cli
+        if opts.setenv is not None:
+            set_envname, set_envval = opts.setenv
+            runner.environment[set_envname] = set_envval
+        if opts.unsetenv is not None:
+            if opts.unsetenv in runner.environment:
+                del runner.environment[opts.unsetenv]
+        if opts.shell is not None:
+            runner.shell = opts.shell
+        if not runner.is_builtin:
+            ctx.save_runner(runner)
+        after = str(runner.to_dict())
+        if after != before:
+            if runner.is_builtin:
+                # some things we can alter in the global settings
+                if runner.executor == EXECUTOR_DOCKER:
+                    ctx.docker.privileged = runner.docker.privileged
+                    ctx.docker.volumes = runner.docker.volumes
+                    ctx.docker.variables = runner.environment
+                    ctx.docker.docker_cli = runner.docker.docker_cli
+            note(f"saved runner {runner.name} :-")
+        else:
+            note(f"current settings for runner {runner.name} :-")
+        print(yaml.safe_dump(runner.to_dict(), indent=4))
+        cfg.save()
+    elif opts.action == "rm":
+        if ctx.get_runner(opts.name) is not None:
+            ctx.runners = [x for x in ctx.runners if x.name != opts.name]
+            cfg.save()
+            note(f"removed runner {opts.name}")
```

## gitlabemu/configtool_vars.py

 * *Ordering differences only*

```diff
@@ -1,53 +1,53 @@
-from argparse import Namespace
-
-from .helpers import trim_quotes, sensitive_varname, warning, notice
-from .userconfig import get_user_config
-
-def setup_cmd(subparsers):
-    set_var = subparsers.add_parser("vars", help="Show/set environment variables injected into jobs")
-    set_var.add_argument("--local", default=False, action="store_true",
-                         help="Set/Show variables for local shell jobs only")
-    set_var.add_argument("--docker", default=False, action="store_true",
-                         help="Set/Show variables for local docker jobs only")
-    set_var.add_argument("VAR", type=str, help="Set or unset an environment variable", nargs="?")
-    set_var.set_defaults(func=vars_cmd)
-
-def vars_cmd(opts: Namespace):
-    cfg = get_user_config()
-    current = cfg.current_context
-    if opts.local:
-        vars_container = cfg.contexts[current].local
-    elif opts.docker:
-        vars_container = cfg.contexts[current].docker
-    else:
-        vars_container = cfg.contexts[current]
-    variables = vars_container.variables
-    if opts.VAR is None:
-        print_sensitive_vars(variables)
-    elif "=" in opts.VAR:
-        name, value = opts.VAR.split("=", 1)
-        if not value:
-            # unset variable if set
-            if name in variables:
-                notice(f"Unsetting {name}")
-                del vars_container.variables[name]
-            else:
-                warning(f"{name} is not set. If you want an empty string, use {name}='\"\"'")
-        else:
-            notice(f"Setting {name}")
-            vars_container.variables[name] = trim_quotes(value)
-
-        cfg.save()
-    else:
-        if opts.VAR in variables:
-            print_sensitive_vars({opts.VAR: variables[opts.VAR]})
-        else:
-            print(f"{opts.VAR} is not set")
-
-
-def print_sensitive_vars(variables: dict) -> None:
-    for name in sorted(variables.keys()):
-        if sensitive_varname(name):
-            print(f"{name}=************")
-        else:
-            print(f"{name}={variables[name]}")
+from argparse import Namespace
+
+from .helpers import trim_quotes, sensitive_varname, warning, notice
+from .userconfig import get_user_config
+
+def setup_cmd(subparsers):
+    set_var = subparsers.add_parser("vars", help="Show/set environment variables injected into jobs")
+    set_var.add_argument("--local", default=False, action="store_true",
+                         help="Set/Show variables for local shell jobs only")
+    set_var.add_argument("--docker", default=False, action="store_true",
+                         help="Set/Show variables for local docker jobs only")
+    set_var.add_argument("VAR", type=str, help="Set or unset an environment variable", nargs="?")
+    set_var.set_defaults(func=vars_cmd)
+
+def vars_cmd(opts: Namespace):
+    cfg = get_user_config()
+    current = cfg.current_context
+    if opts.local:
+        vars_container = cfg.contexts[current].local
+    elif opts.docker:
+        vars_container = cfg.contexts[current].docker
+    else:
+        vars_container = cfg.contexts[current]
+    variables = vars_container.variables
+    if opts.VAR is None:
+        print_sensitive_vars(variables)
+    elif "=" in opts.VAR:
+        name, value = opts.VAR.split("=", 1)
+        if not value:
+            # unset variable if set
+            if name in variables:
+                notice(f"Unsetting {name}")
+                del vars_container.variables[name]
+            else:
+                warning(f"{name} is not set. If you want an empty string, use {name}='\"\"'")
+        else:
+            notice(f"Setting {name}")
+            vars_container.variables[name] = trim_quotes(value)
+
+        cfg.save()
+    else:
+        if opts.VAR in variables:
+            print_sensitive_vars({opts.VAR: variables[opts.VAR]})
+        else:
+            print(f"{opts.VAR} is not set")
+
+
+def print_sensitive_vars(variables: dict) -> None:
+    for name in sorted(variables.keys()):
+        if sensitive_varname(name):
+            print(f"{name}=************")
+        else:
+            print(f"{name}={variables[name]}")
```

## gitlabemu/configtool_volumes.py

 * *Ordering differences only*

```diff
@@ -1,28 +1,28 @@
-from argparse import Namespace
-
-from .userconfig import get_user_config
-
-
-def setup_cmd(subparsers):
-    set_vols = subparsers.add_parser("volumes", help="Show/set the global docker volumes")
-    vol_grp = set_vols.add_mutually_exclusive_group()
-    vol_grp.add_argument("--add", type=str, metavar="VOLUME",
-                         help="Volume to add (eg /path/to/folder:/mount/path:rw)")
-    vol_grp.add_argument("--remove", type=str, metavar="PATH",
-                         help="Volume to remove (eg /mount/path)")
-    set_vols.set_defaults(func=volumes_cmd)
-
-
-def volumes_cmd(opts: Namespace):
-    cfg = get_user_config()
-    current = cfg.current_context
-
-    if opts.add:
-        cfg.contexts[current].docker.add_volume(opts.add)
-        cfg.save()
-    elif opts.remove:
-        cfg.contexts[current].docker.remove_volume(opts.remove)
-        cfg.save()
-
-    for volume in cfg.contexts[current].docker.volumes:
-        print(volume)
+from argparse import Namespace
+
+from .userconfig import get_user_config
+
+
+def setup_cmd(subparsers):
+    set_vols = subparsers.add_parser("volumes", help="Show/set the global docker volumes")
+    vol_grp = set_vols.add_mutually_exclusive_group()
+    vol_grp.add_argument("--add", type=str, metavar="VOLUME",
+                         help="Volume to add (eg /path/to/folder:/mount/path:rw)")
+    vol_grp.add_argument("--remove", type=str, metavar="PATH",
+                         help="Volume to remove (eg /mount/path)")
+    set_vols.set_defaults(func=volumes_cmd)
+
+
+def volumes_cmd(opts: Namespace):
+    cfg = get_user_config()
+    current = cfg.current_context
+
+    if opts.add:
+        cfg.contexts[current].docker.add_volume(opts.add)
+        cfg.save()
+    elif opts.remove:
+        cfg.contexts[current].docker.remove_volume(opts.remove)
+        cfg.save()
+
+    for volume in cfg.contexts[current].docker.volumes:
+        print(volume)
```

## gitlabemu/docker.py

 * *Ordering differences only*

```diff
@@ -1,754 +1,754 @@
-import os
-import subprocess
-import sys
-import tempfile
-import threading
-import time
-import json
-from contextlib import contextmanager
-from pathlib import Path
-from typing import Dict, Optional, List, Any
-from .logmsg import warning, info, fatal
-from .jobs import Job, make_script
-from .helpers import communicate as comm, is_windows
-from .userconfig import get_user_config_context
-from .errors import DockerExecError, GitlabEmulatorError
-from .userconfigdata import GleRunnerConfig
-from .variables import expand_variable
-
-PULL_POLICY_ALWAYS = "always"
-PULL_POLICY_IF_NOT_PRESENT = "if-not-present"
-PULL_POLICY_NEVER = "never"
-
-
-class DockerToolError(GitlabEmulatorError):
-    """An error using docker"""
-    def __init__(self, msg: str):
-        self.message = msg
-
-
-class DockerToolFailed(DockerToolError):
-    """Running docker returned an error"""
-    def __init__(self, msg: str, stdout: str, stderr: str):
-        super().__init__(msg)
-        self.stdout = stdout
-        self.stderr = stderr
-
-    def __str__(self):
-        return f"docker error: {self.message}\nstderr: {self.stderr}\nstdout:{self.stdout}"
-
-
-class DockerTool(object):
-    """
-    Control docker containers
-    """
-    def __init__(self, retries: Optional[int] = 5):
-        self.retries = retries
-        self.container: Optional[Any] = None
-        self.image = None
-        self.env = {}
-        self.volumes = []
-        self.name = None
-        self.privileged = False
-        self.entrypoint = None
-        self.pulled = None
-        self._pull_policy = PULL_POLICY_ALWAYS
-        self.network = None
-        self._client = None
-        self._is_hyerv = None
-        self.tool = "docker"
-
-    @property
-    def containers(self) -> List[str]:
-        try:
-            output = self.docker_call("container", "ps", "-q").stdout.strip()
-            return output.splitlines(keepends=False)
-        except DockerToolError:
-            return []
-
-    def container_name(self, containerid: str) -> str:
-        output = self.docker_call("container", "inspect", containerid).stdout.strip()
-        data = json.loads(output)[0]
-        return data["Name"][1:]
-
-    def docker_call(self, *args, cwd: Optional[Path] = None) -> subprocess.CompletedProcess:
-        """Run the docker command line tool"""
-        basic_args = [self.tool] + list(args[:1])
-        cmdline = [self.tool] + [str(x) for x in args]
-        if cwd is None:
-            cwd = Path.cwd()
-        try:
-            return subprocess.run(cmdline,
-                                  cwd=str(cwd.absolute()),
-                                  encoding="utf-8",
-                                  capture_output=True, check=True)
-        except subprocess.CalledProcessError as cpe:
-            raise DockerToolFailed(f"{basic_args} failed", cpe.stdout, cpe.stderr)
-        except Exception as err:
-            raise DockerToolError(f"could not run {basic_args}, {err}")
-
-    def is_windows_hyperv(self) -> bool:
-        if self._is_hyerv is None:
-            self._is_hyerv = False
-            if is_windows():  # pramga: cover if windows
-                output = self.docker_call("info", "-f", "{{.Isolation}}").stdout.strip()
-                if output == "hyperv":  # pragma: no cover
-                    self._is_hyerv = True
-        return self._is_hyerv
-
-    @property
-    def pull_policy(self) -> str:
-        return self._pull_policy
-    
-    @pull_policy.setter
-    def pull_policy(self, value: str):
-        assert value in [PULL_POLICY_ALWAYS, PULL_POLICY_IF_NOT_PRESENT, PULL_POLICY_NEVER]
-        self._pull_policy = value
-
-    @property
-    def can_pull(self) -> bool:
-        return self.pull_policy in [PULL_POLICY_ALWAYS, PULL_POLICY_IF_NOT_PRESENT]
-
-    @property
-    def pull_always(self) -> bool:
-        return self.pull_policy == PULL_POLICY_ALWAYS
-
-    @property
-    def pull_if_not_present(self) -> bool:
-        return self.pull_policy == PULL_POLICY_IF_NOT_PRESENT
-
-    def add_volume(self, outside, inside):
-        self.volumes.append("{}:{}".format(outside, inside))
-
-    def add_env(self, name, value):
-        self.env[name] = value
-
-    @property
-    def image_present(self) -> bool:
-        try:
-            self.docker_call("image", "inspect", self.image)
-            return True
-        except DockerToolFailed:
-            return False
-
-    def inspect(self):
-        """
-        Inspect the image and return the Config dict
-        :return:
-        """
-        if self.image:
-            if not self.image_present:
-                if self.can_pull:
-                    self.pull()
-            try:
-                output = self.docker_call("image", "inspect", self.image).stdout
-                return json.loads(output)[0]
-            except DockerToolError:
-                pass
-        return None
-
-    def add_file(self, src, dest):
-        """
-        Copy a file to the container
-        :param src:
-        :param dest:
-        :return:
-        """
-        assert self.container
-        need_start = False
-        if self.is_windows_hyperv():  # pragma:  cover if windows
-            info(f"Pause hyperv container {self.name} for file copy..")
-            self.docker_call("stop", self.name)
-            info(f"Paused {self.name}")
-            need_start = True
-
-        self.docker_call("cp", src, f"{self.name}:{dest}")
-
-        if need_start:  # pragma:  cover if windows
-            info(f"Resume hyperv container {self.name} after file copy..")
-            self.docker_call("start", self.name)
-
-    def get_user(self):
-        image = self.inspect()
-        if image and len(image) > 0:
-            return image.get("Config", {}).get("User", None)
-        return None
-
-    def pull(self):
-        if self.can_pull:
-            info("pulling docker image {}".format(self.image))
-            sys.stdout.write("Pulling {}...\n".format(self.image))
-            sys.stdout.flush()
-            try:
-                self.docker_call("pull", self.image)
-                self.pulled = True
-            except DockerToolFailed as err:
-                info(f"error pulling image: {err}")
-                fatal(f"cannot pull image: {self.image} - image not found")
-
-    def get_envs(self):
-        cmdline = []
-        for name in self.env:
-            value = self.env.get(name)
-            if value is not None:
-                cmdline.extend(["-e", "{}={}".format(name, value)])
-            else:
-                cmdline.extend(["-e", name])
-        return cmdline
-
-    def run(self):
-        priv = self.privileged and not is_windows()
-        if self.is_windows_hyperv():  # pragma: cover if windows
-            warning("windows hyperv container support is very experimental, YMMV")
-        volumes = []
-        for volume in self.volumes:
-            entry = volume
-            if not entry.endswith(":ro") and not entry.endswith(":rw"):
-                entry += ":rw"
-            volumes.append(entry)
-        try:
-            image = self.inspect()
-            if self.entrypoint == ['']:
-                if image.get("Os") == "linux":  # pragma: cover if not windows
-                    self.entrypoint = "/bin/sh"
-                else:
-                    self.entrypoint = None
-            info(f"launching image {self.image} as container {self.name} ..")
-            cmdline = [
-                "run",
-                "-d",
-                "--name", self.name,
-            ]
-            if self.entrypoint is not None:
-                cmdline.extend(["--entrypoint", str(self.entrypoint)])
-            if self.network is not None:
-                cmdline.extend(["--network", self.network])
-            if priv:
-                cmdline.append("--privileged")
-            if not self.is_windows_hyperv():
-                cmdline.append("--rm")
-            for volume in volumes:
-                cmdline.extend(["-v", volume])
-            for name, value in self.env.items():
-                cmdline.extend(["-e", f"{name}={value}"])
-            cmdline.extend(["-i", self.image])
-
-            proc = self.docker_call(*cmdline)
-            self.container = proc.stdout.strip()
-        except DockerToolFailed:  # pragma: no cover
-            if not self.image_present:
-                fatal(f"Docker image {self.image} does not exist, (pull_policy={self.pull_policy})")
-            warning(f"problem running {self.image}")
-            raise
-
-    def kill(self):
-        if self.container:
-            self.docker_call("kill", "-s", "9", self.container)
-
-    def check_call(self, cwd: str, cmd: List[str], stdout=None, stderr=None, capture=False):
-        cmdline = [self.tool, "exec", "-w", cwd, self.container] + cmd
-        if capture:
-            return subprocess.check_output(cmdline, stderr=stderr)
-        else:
-            return subprocess.check_call(cmdline, stdout=stdout, stderr=stderr)
-
-    def exec(self, cwd: str, shell: List[str], tty=False, user=None, pipe=True):
-        cmdline = [self.tool, "exec", "-w", cwd]
-        cmdline.extend(self.get_envs())
-        if user is not None:
-            cmdline.extend(["-u", str(user)])
-        if tty:  # pragma: no cover
-            cmdline.append("-t")
-            pipe = False
-        cmdline.extend(["-i", self.container])
-        cmdline.extend(shell)
-
-        if pipe:
-            proc = subprocess.Popen(cmdline,
-                                    shell=False,
-                                    stdin=subprocess.PIPE,
-                                    stdout=subprocess.PIPE,
-                                    stderr=subprocess.STDOUT)
-            return proc
-        else:  # pragma: no cover
-            return subprocess.Popen(cmdline,
-                                    shell=False)
-
-
-class DockerJob(Job):
-    """
-    Run a job inside a docker container
-    """
-    def __init__(self):
-        super(DockerJob, self).__init__()
-        self._image = None
-        self.services = []
-        self.container = None
-        self.docker = DockerTool()
-        self._force_pull_policy = None
-        self._container_lock = threading.Lock()
-        self._has_bash = None
-        self._shell_uid = 0
-        self._shell_gid = 0
-
-    @property
-    def shell_is_user(self):
-        return super().shell_is_user
-
-    @shell_is_user.setter
-    def shell_is_user(self, value: bool):
-        self._shell_is_user = value
-        if value:
-            self._shell_uid = os.getuid()
-            self._shell_gid = os.getgid()
-
-    @property
-    def docker_image(self) -> str:
-        if isinstance(self._image, dict):
-            image = self._image.get("name", None)
-        else:
-            image = self._image
-        return expand_variable(self.get_envs(), image)
-
-    @property
-    def docker_entrypoint(self) -> Optional[List[str]]:
-        custom_entryppoint = None
-        if isinstance(self._image, dict):
-            custom_entryppoint = self._image.get("entrypoint", None)
-        return custom_entryppoint
-
-    @property
-    def docker_pull_policy(self) -> Optional[str]:
-        policy = self._force_pull_policy
-        if policy is None:
-            if isinstance(self._image, dict):
-                policy = self._image.get("pull_policy", None)
-        return policy
-
-    @docker_pull_policy.setter
-    def docker_pull_policy(self, value: Optional[str]):
-        self._force_pull_policy = value
-        self.docker.pull_policy = value
-
-    @property
-    def inside_workspace(self) -> str:
-        if is_windows():
-            import ntpath
-            # if the workspace is not on c:, map it to a c: location in the container
-            # or if the path is quite long
-            if not self.workspace.lower().startswith("c:") or len(self.workspace) > 32:
-                basedir = ntpath.basename(self.workspace)
-                return f"c:\\b\\{basedir}"[:14]
-        else:
-            if len(self.workspace) > 80:
-                # truncate really long paths even on linux
-                return f"/b/{os.path.basename(self.workspace)[:64]}"
-
-        return self.workspace
-
-    def allocate_runner(self):
-        if self.runner is None:  # pragma: no cover
-            raise GitlabEmulatorError(f"could not find a local docker runner for this job@ {self.name}")
-        super().allocate_runner()
-        if self.runner and self.runner.docker:
-            if self._image is None:
-                self._image = self.runner.docker.image
-            self.docker.privileged = self.runner.docker.privileged
-            if self.runner.docker.docker_cli is not None:
-                self.docker.tool = self.runner.docker.docker_cli
-
-    def load(self, name, config, overrides: Optional[Dict[str, Any]] = None):
-        super(DockerJob, self).load(name, config, overrides=overrides)
-        self.services = get_services(config, name)
-        pull_policy = self.docker_pull_policy
-        if pull_policy is not None:
-            self.docker.pull_policy = pull_policy
-        self.set_job_variables()
-
-    def get_emulator_runner(self) -> Optional[GleRunnerConfig]:
-        ctx = get_user_config_context()
-        return ctx.find_runner(image=True, tags=self.tags)
-
-    def set_job_variables(self):
-        super(DockerJob, self).set_job_variables()
-        all_images = self._config.get("image", None)
-        self._image = self._config[self.name].get("image", all_images)
-        if self.docker_image is not None:
-            self.configure_job_variable("CI_JOB_IMAGE", self.docker_image, force=True)
-        self.configure_job_variable("CI_DISPOSABLE_ENVIRONMENT", "true", force=True)
-        self.configure_job_variable("CI_PROJECT_DIR", self.inside_workspace)
-        self.configure_job_variable("CI_BUILDS_DIR", os.path.dirname(self.inside_workspace))
-
-    def abort(self):
-        """
-        Abort the build by killing our container
-        :return:
-        """
-        info("abort docker job {}".format(self.name))
-        # we need to wait for the container to start
-        if self.docker.container is None:
-            time.sleep(1)
-
-        if self.container and self.docker.container:
-            info("kill container {}".format(self.name))
-            self.docker.kill()
-        if self.build_process is not None:
-            try:  # pragma: no cover
-                if self.build_process.poll() is None:
-                    self.build_process.terminate()
-            except Exception as err:  # pragma: no cover
-                assert err is not None
-
-    def get_envs(self, expand_only_ci=True):
-        """
-        Get env vars for a docker job
-        :return:
-        """
-        envs = self.base_variables()
-        return self.get_defined_envs(envs, expand_only_ci=expand_only_ci)
-
-    def run_script(self, lines):
-        return self._run_script(lines)
-
-    def _run_script(self, lines, attempts=2, user=None):
-        task = None
-        if user is None:
-            if self.shell_is_user:  # pragma: cover if posix
-                user = self._shell_uid
-
-        filename = "generated-gitlab-script" + self.get_script_fileext()
-        temp = os.path.join(tempfile.gettempdir(), filename)
-        try:
-            with open(temp, "w") as fd:
-                print(lines, file=fd)
-            # copy it to the container
-            dest = "/tmp"
-            if is_windows():  # pragma: cover if windows
-                dest = "c:\\windows\\temp"
-            target_script = os.path.join(dest, filename)
-            info("Copying {} to container as {} ..".format(temp, target_script))
-            self.docker.add_file(temp, dest)
-
-            while attempts > 0:
-                try:
-                    interactive = bool(self.enter_shell or self.error_shell)
-                    if interactive:  # pragma: no cover
-                        try:
-                            if not os.isatty(sys.stdin.fileno()):
-                                interactive = False
-                        except OSError:
-                            # probably under pycharm pytest
-                            interactive = False
-                    cmdline = self.shell_command(target_script)
-                    task = self.docker.exec(self.inside_workspace,
-                                            cmdline,
-                                            tty=interactive,
-                                            user=user)
-                    self.communicate(task, script=None)
-                    break
-                except DockerExecError:  # pragma: no cover
-                    self.stdout.write(
-                        "Warning: docker exec error - https://gitlab.com/cunity/gitlab-emulator/-/issues/10")
-                    attempts -= 1
-                    if attempts == 0:
-                        raise
-                    else:
-                        time.sleep(2)
-            return task
-        finally:
-            if os.path.exists(temp):
-                os.unlink(temp)
-
-    def check_docker_exec_failed(self, line):
-        """
-        Raise an error if the build script has returned "No such exec instance"
-        :param line:
-        :return:
-        """
-        if line:
-            try:
-                decoded = line.decode()
-            except Exception:
-                return
-            if decoded:
-                if "No such exec instance" in decoded:
-                    raise DockerExecError()
-
-    def communicate(self, process, script=None):
-        comm(process, self.stdout, script=script, linehandler=self.check_docker_exec_failed)
-
-    def has_bash(self):
-        """
-        Return True of the container has bash
-        :return:
-        """
-        if self._has_bash is None:
-            self._has_bash = False
-            if not is_windows():
-                info("checking container for bash")
-                try:
-                    self.docker.check_call(
-                        self.inside_workspace, ["sh", "-c", "command -v bash"],
-                        capture=True,
-                        stderr=subprocess.STDOUT)
-                    self._has_bash = True
-                except subprocess.CalledProcessError as cpe:
-                    assert cpe
-        return self._has_bash
-
-    def shell_on_error(self):
-        """
-        Execute a shell command on job errors
-        :return:
-        """
-        print("Job {} script error..".format(self.name), flush=True)
-        lines = "\n".join(self.error_shell)
-        self.run_script(lines)
-
-    def git_safe_dir(self):
-        """Configure git safe.directory if possible"""
-        info("attempting to set git safe.directory..")
-        folder = self.inside_workspace
-        cmdline = f"command -v git 2>&1 >/dev/null && git config --global --add safe.directory '{folder}'"
-        if is_windows():  # pragma: cover if windows
-            folder = folder.replace("\\", "/")  # windows git won't understand \ chars for this
-            cmdline = f"git config --global --add safe.directory {folder}"
-        info(f"running {cmdline}")
-        self.run_script(cmdline)
-
-    def run_impl(self):
-        info(f"running docker job {self.name}")
-        info(f"runner = {self.runner}")
-        from .resnamer import generate_resource_name
-        if is_windows():  # pragma: cover if windows
-            warning("warning windows docker is experimental")
-        if self.runner.docker is None:
-            raise GitlabEmulatorError("docker not detected")
-
-        with self._container_lock:
-            self.docker.image = self.docker_image
-            self.container = generate_resource_name()
-            self.docker.name = self.container
-            if not is_windows():  # pragma: cover if not windows
-                if self.runner.docker:
-                    self.docker.privileged = self.runner.docker.privileged
-
-            if not is_windows():  # pragma: cover if not windows
-                image_name = self.docker.image
-                image_name = image_name.split("/")[-1].split("@")[0].split(":")[0]
-                if self.error_shell or self.enter_shell:
-                    self.docker.add_env("PS1", f"[{self.name}] \\u@{image_name}:$PWD $ ")
-
-            if self.docker.pull_always or (self.docker.pull_if_not_present and not self.docker.image_present):
-                self.docker.pull()
-
-            environ = self.get_envs(expand_only_ci=False)
-            with docker_services(self, environ) as network:
-                if network:
-                    self.docker.network = network
-                for envname in environ:
-                    self.docker.add_env(envname, environ[envname])
-
-                if self.docker_entrypoint is not None:
-                    if is_windows():  # pragma: cover if windows
-                        # windows can't have multiple args
-                        args = self.docker_entrypoint
-                        if len(args) > 0:
-                            if len(args) > 1:
-                                warning("windows docker entrypoint override may fail with several args")
-                            self.docker.entrypoint = args[0]
-                    else:
-                        self.docker.entrypoint = self.docker_entrypoint
-                volumes = self.runner.docker.runtime_volumes()
-                if volumes:
-                    info("Extra docker volumes registered:")
-                    for item in volumes:
-                        info("- {}".format(item))
-
-                self.docker.volumes = volumes + [f"{self.workspace}:{self.inside_workspace}:rw"]
-
-                self.docker.run()
-
-                if not is_windows():  # pragma: cover if not windows
-                    # work out default USER from the image
-                    docker_user_cfg = self.docker.get_user()
-                    if docker_user_cfg and ":" in docker_user_cfg:
-                        info(f"Container image defines USER: {docker_user_cfg}")
-                        docker_user, docker_grp = docker_user_cfg.split(":", 1)
-                        info(f"Setting ownership to {docker_user}:{docker_grp}")
-                        self._run_script(f"chown -R {docker_user}.{docker_grp} .", attempts=1, user="0")
-
-                if self.shell_is_user:
-                    if not is_windows():  # pragma: cover if not windows
-                        # try to make a more functional user account inside the container, this may not always
-                        # work due to missing tools, but it's worth a try
-                        _homedir = "/gle-tmp-home"
-                        _passwd = f"gle:x:{self._shell_uid}:{self._shell_gid}:gitlab-emulator:{_homedir}:/bin/sh"
-                        _shadow = f"gle:!:{self._shell_uid}::::::"
-                        try:
-                            info(f"setting up interactive user with uid={self._shell_uid}..")
-                            self.docker.check_call("/",
-                                                   ["sh", "-c", f"echo {_passwd} >> /etc/passwd"], capture=True)
-                            self.docker.check_call("/",
-                                                   ["sh", "-c", f"echo {_shadow} >> /etc/shadow"], capture=True)
-                            self.docker.check_call("/",
-                                                   ["mkdir", _homedir], capture=True)
-                            self.docker.check_call("/",
-                                                   ["chown", str(self._shell_uid), _homedir], capture=True)
-                            self.docker.check_call("/",
-                                                   ["chgrp", str(self._shell_gid), _homedir], capture=True)
-                            # append this user to the sudoers file
-                            info(f"granting sudo inside container..")
-                            self.docker.check_call("/",
-                                                   ["sh", "-c", f"echo 'gle ALL=(ALL) NOPASSWD: ALL' >> /etc/sudoers"])
-                            info("interactive user setup completed.")
-                        except subprocess.CalledProcessError:
-                            warning("interactive user setup failed, some features may not work fully.")
-
-                self.git_safe_dir()
-
-                try:
-                    lines = self.before_script + self.script
-                    if self.enter_shell:
-                        lines.extend(self.get_interactive_shell_command())
-
-                    self.build_process = self.run_script(make_script(lines, powershell=self.is_powershell()))
-                finally:
-                    try:
-                        if self.error_shell:  # pragma: no cover
-                            if not self.build_process or self.build_process.returncode:
-                                self.shell_on_error()
-                        if self.after_script:
-                            info("Running after_script..")
-                            self.run_script(make_script(self.after_script, powershell=self.is_powershell()))
-                    except subprocess.CalledProcessError:  # pragma: no cover
-                        pass
-                    finally:
-                        subprocess.call([self.docker.tool, "kill", self.container], stderr=subprocess.STDOUT)
-                        if self.docker.is_windows_hyperv():  # pragma: no cover
-                            subprocess.call([self.docker.tool, "rm", self.container])
-
-        result = self.build_process.returncode
-        if result:
-            fatal("Docker job {} failed".format(self.name))
-
-
-def get_services(config, jobname):
-    """
-    Get the service containers that should be started for a particular job
-    :param config:
-    :param jobname:
-    :return:
-    """
-    job = config.get(jobname)
-
-    services = []
-    service_defs = []
-
-    if "image" in config or "image" in job:
-        # yes we are using docker, so we can offer services for this job
-        all_services = config.get("services", [])
-        job_services = job.get("services", [])
-        services = all_services + job_services
-
-    for service in services:
-        item = {}
-        # if this is a dict use the extended version
-        # else make extended versions out of the single strings
-        if isinstance(service, str):
-            item["name"] = service
-
-        # if this is a dict, it needs to at least have name but could have
-        # alias and others
-        if isinstance(service, dict):
-            assert "name" in service
-            item = service
-
-        if item:
-            service_defs.append(item)
-
-    return service_defs
-
-
-@contextmanager
-def docker_services(job: DockerJob, variables: Dict[str, str]):
-    """
-    Setup docker services required by the given job
-    :param job:
-    :param variables: dict of env vars to set in the service container
-    :return:
-    """
-    services = job.services
-    service_network = None
-    containers = []
-    try:
-        if services:
-            net_name = "gle-service-network"
-            try:
-                job.docker.docker_call("network", "inspect", net_name)
-            except DockerToolFailed:
-                job.docker.docker_call(
-                    "network", "create",
-                    "--driver", "bridge",
-                    "--subnet", "192.168.94.0/24",
-                    net_name
-                )
-
-            for service in services:
-                aliases = []
-                if isinstance(service, str):
-                    image = service
-                    service = {
-                        "name": image
-                    }
-                else:
-                    image = service["name"]
-                name = image
-                if ":" in name:
-                    name = image.split(":", 1)[0]
-                aliases.append(name.replace("/", "-"))
-                if "alias" in service:
-                    aliases.append(service["alias"])
-
-                job.stdout.write(f"create docker service : {name} ({aliases})\n")
-                if job.docker.can_pull:
-                    try:
-                        job.stdout.write(f"pulling {image} ..\n")
-                        job.docker.docker_call("pull", image)
-                    except DockerToolFailed:  # pragma: no cover
-                        fatal(f"No such image {image}")
-                priv = not is_windows()
-                service_cmdline = [
-                    "run",
-                    "--rm",
-                    "-d",
-                ]
-                if priv:  # pragma: cover if not windows
-                    service_cmdline.append("--privileged")
-                for name, value in variables.items():
-                    service_cmdline.extend(["-e", f"{name}={value}"])
-                service_cmdline.append(image)
-
-                container = job.docker.docker_call(
-                    *service_cmdline
-                ).stdout.strip()
-
-                info(f"creating docker service {name} ({aliases})")
-                info(f"service {name} is container {container}")
-                containers.append(container)
-                info(f"connect {name} to service network")
-                connect_cmdline = [
-                    "network", "connect"
-                ]
-                for alias in aliases:
-                    connect_cmdline.extend(["--alias", alias])
-                connect_cmdline.extend([net_name, container])
-                job.docker.docker_call(*connect_cmdline)
-                service_network = net_name
-
-        yield service_network
-    finally:
-        for container in containers:
-            info(f"clean up docker service {container}")
-            job.docker.docker_call("kill", "-s", "9", container)
-        time.sleep(1)
+import os
+import subprocess
+import sys
+import tempfile
+import threading
+import time
+import json
+from contextlib import contextmanager
+from pathlib import Path
+from typing import Dict, Optional, List, Any
+from .logmsg import warning, info, fatal
+from .jobs import Job, make_script
+from .helpers import communicate as comm, is_windows
+from .userconfig import get_user_config_context
+from .errors import DockerExecError, GitlabEmulatorError
+from .userconfigdata import GleRunnerConfig
+from .variables import expand_variable
+
+PULL_POLICY_ALWAYS = "always"
+PULL_POLICY_IF_NOT_PRESENT = "if-not-present"
+PULL_POLICY_NEVER = "never"
+
+
+class DockerToolError(GitlabEmulatorError):
+    """An error using docker"""
+    def __init__(self, msg: str):
+        self.message = msg
+
+
+class DockerToolFailed(DockerToolError):
+    """Running docker returned an error"""
+    def __init__(self, msg: str, stdout: str, stderr: str):
+        super().__init__(msg)
+        self.stdout = stdout
+        self.stderr = stderr
+
+    def __str__(self):
+        return f"docker error: {self.message}\nstderr: {self.stderr}\nstdout:{self.stdout}"
+
+
+class DockerTool(object):
+    """
+    Control docker containers
+    """
+    def __init__(self, retries: Optional[int] = 5):
+        self.retries = retries
+        self.container: Optional[Any] = None
+        self.image = None
+        self.env = {}
+        self.volumes = []
+        self.name = None
+        self.privileged = False
+        self.entrypoint = None
+        self.pulled = None
+        self._pull_policy = PULL_POLICY_ALWAYS
+        self.network = None
+        self._client = None
+        self._is_hyerv = None
+        self.tool = "docker"
+
+    @property
+    def containers(self) -> List[str]:
+        try:
+            output = self.docker_call("container", "ps", "-q").stdout.strip()
+            return output.splitlines(keepends=False)
+        except DockerToolError:
+            return []
+
+    def container_name(self, containerid: str) -> str:
+        output = self.docker_call("container", "inspect", containerid).stdout.strip()
+        data = json.loads(output)[0]
+        return data["Name"][1:]
+
+    def docker_call(self, *args, cwd: Optional[Path] = None) -> subprocess.CompletedProcess:
+        """Run the docker command line tool"""
+        basic_args = [self.tool] + list(args[:1])
+        cmdline = [self.tool] + [str(x) for x in args]
+        if cwd is None:
+            cwd = Path.cwd()
+        try:
+            return subprocess.run(cmdline,
+                                  cwd=str(cwd.absolute()),
+                                  encoding="utf-8",
+                                  capture_output=True, check=True)
+        except subprocess.CalledProcessError as cpe:
+            raise DockerToolFailed(f"{basic_args} failed", cpe.stdout, cpe.stderr)
+        except Exception as err:
+            raise DockerToolError(f"could not run {basic_args}, {err}")
+
+    def is_windows_hyperv(self) -> bool:
+        if self._is_hyerv is None:
+            self._is_hyerv = False
+            if is_windows():  # pramga: cover if windows
+                output = self.docker_call("info", "-f", "{{.Isolation}}").stdout.strip()
+                if output == "hyperv":  # pragma: no cover
+                    self._is_hyerv = True
+        return self._is_hyerv
+
+    @property
+    def pull_policy(self) -> str:
+        return self._pull_policy
+    
+    @pull_policy.setter
+    def pull_policy(self, value: str):
+        assert value in [PULL_POLICY_ALWAYS, PULL_POLICY_IF_NOT_PRESENT, PULL_POLICY_NEVER]
+        self._pull_policy = value
+
+    @property
+    def can_pull(self) -> bool:
+        return self.pull_policy in [PULL_POLICY_ALWAYS, PULL_POLICY_IF_NOT_PRESENT]
+
+    @property
+    def pull_always(self) -> bool:
+        return self.pull_policy == PULL_POLICY_ALWAYS
+
+    @property
+    def pull_if_not_present(self) -> bool:
+        return self.pull_policy == PULL_POLICY_IF_NOT_PRESENT
+
+    def add_volume(self, outside, inside):
+        self.volumes.append("{}:{}".format(outside, inside))
+
+    def add_env(self, name, value):
+        self.env[name] = value
+
+    @property
+    def image_present(self) -> bool:
+        try:
+            self.docker_call("image", "inspect", self.image)
+            return True
+        except DockerToolFailed:
+            return False
+
+    def inspect(self):
+        """
+        Inspect the image and return the Config dict
+        :return:
+        """
+        if self.image:
+            if not self.image_present:
+                if self.can_pull:
+                    self.pull()
+            try:
+                output = self.docker_call("image", "inspect", self.image).stdout
+                return json.loads(output)[0]
+            except DockerToolError:
+                pass
+        return None
+
+    def add_file(self, src, dest):
+        """
+        Copy a file to the container
+        :param src:
+        :param dest:
+        :return:
+        """
+        assert self.container
+        need_start = False
+        if self.is_windows_hyperv():  # pragma:  cover if windows
+            info(f"Pause hyperv container {self.name} for file copy..")
+            self.docker_call("stop", self.name)
+            info(f"Paused {self.name}")
+            need_start = True
+
+        self.docker_call("cp", src, f"{self.name}:{dest}")
+
+        if need_start:  # pragma:  cover if windows
+            info(f"Resume hyperv container {self.name} after file copy..")
+            self.docker_call("start", self.name)
+
+    def get_user(self):
+        image = self.inspect()
+        if image and len(image) > 0:
+            return image.get("Config", {}).get("User", None)
+        return None
+
+    def pull(self):
+        if self.can_pull:
+            info("pulling docker image {}".format(self.image))
+            sys.stdout.write("Pulling {}...\n".format(self.image))
+            sys.stdout.flush()
+            try:
+                self.docker_call("pull", self.image)
+                self.pulled = True
+            except DockerToolFailed as err:
+                info(f"error pulling image: {err}")
+                fatal(f"cannot pull image: {self.image} - image not found")
+
+    def get_envs(self):
+        cmdline = []
+        for name in self.env:
+            value = self.env.get(name)
+            if value is not None:
+                cmdline.extend(["-e", "{}={}".format(name, value)])
+            else:
+                cmdline.extend(["-e", name])
+        return cmdline
+
+    def run(self):
+        priv = self.privileged and not is_windows()
+        if self.is_windows_hyperv():  # pragma: cover if windows
+            warning("windows hyperv container support is very experimental, YMMV")
+        volumes = []
+        for volume in self.volumes:
+            entry = volume
+            if not entry.endswith(":ro") and not entry.endswith(":rw"):
+                entry += ":rw"
+            volumes.append(entry)
+        try:
+            image = self.inspect()
+            if self.entrypoint == ['']:
+                if image.get("Os") == "linux":  # pragma: cover if not windows
+                    self.entrypoint = "/bin/sh"
+                else:
+                    self.entrypoint = None
+            info(f"launching image {self.image} as container {self.name} ..")
+            cmdline = [
+                "run",
+                "-d",
+                "--name", self.name,
+            ]
+            if self.entrypoint is not None:
+                cmdline.extend(["--entrypoint", str(self.entrypoint)])
+            if self.network is not None:
+                cmdline.extend(["--network", self.network])
+            if priv:
+                cmdline.append("--privileged")
+            if not self.is_windows_hyperv():
+                cmdline.append("--rm")
+            for volume in volumes:
+                cmdline.extend(["-v", volume])
+            for name, value in self.env.items():
+                cmdline.extend(["-e", f"{name}={value}"])
+            cmdline.extend(["-i", self.image])
+
+            proc = self.docker_call(*cmdline)
+            self.container = proc.stdout.strip()
+        except DockerToolFailed:  # pragma: no cover
+            if not self.image_present:
+                fatal(f"Docker image {self.image} does not exist, (pull_policy={self.pull_policy})")
+            warning(f"problem running {self.image}")
+            raise
+
+    def kill(self):
+        if self.container:
+            self.docker_call("kill", "-s", "9", self.container)
+
+    def check_call(self, cwd: str, cmd: List[str], stdout=None, stderr=None, capture=False):
+        cmdline = [self.tool, "exec", "-w", cwd, self.container] + cmd
+        if capture:
+            return subprocess.check_output(cmdline, stderr=stderr)
+        else:
+            return subprocess.check_call(cmdline, stdout=stdout, stderr=stderr)
+
+    def exec(self, cwd: str, shell: List[str], tty=False, user=None, pipe=True):
+        cmdline = [self.tool, "exec", "-w", cwd]
+        cmdline.extend(self.get_envs())
+        if user is not None:
+            cmdline.extend(["-u", str(user)])
+        if tty:  # pragma: no cover
+            cmdline.append("-t")
+            pipe = False
+        cmdline.extend(["-i", self.container])
+        cmdline.extend(shell)
+
+        if pipe:
+            proc = subprocess.Popen(cmdline,
+                                    shell=False,
+                                    stdin=subprocess.PIPE,
+                                    stdout=subprocess.PIPE,
+                                    stderr=subprocess.STDOUT)
+            return proc
+        else:  # pragma: no cover
+            return subprocess.Popen(cmdline,
+                                    shell=False)
+
+
+class DockerJob(Job):
+    """
+    Run a job inside a docker container
+    """
+    def __init__(self):
+        super(DockerJob, self).__init__()
+        self._image = None
+        self.services = []
+        self.container = None
+        self.docker = DockerTool()
+        self._force_pull_policy = None
+        self._container_lock = threading.Lock()
+        self._has_bash = None
+        self._shell_uid = 0
+        self._shell_gid = 0
+
+    @property
+    def shell_is_user(self):
+        return super().shell_is_user
+
+    @shell_is_user.setter
+    def shell_is_user(self, value: bool):
+        self._shell_is_user = value
+        if value:
+            self._shell_uid = os.getuid()
+            self._shell_gid = os.getgid()
+
+    @property
+    def docker_image(self) -> str:
+        if isinstance(self._image, dict):
+            image = self._image.get("name", None)
+        else:
+            image = self._image
+        return expand_variable(self.get_envs(), image)
+
+    @property
+    def docker_entrypoint(self) -> Optional[List[str]]:
+        custom_entryppoint = None
+        if isinstance(self._image, dict):
+            custom_entryppoint = self._image.get("entrypoint", None)
+        return custom_entryppoint
+
+    @property
+    def docker_pull_policy(self) -> Optional[str]:
+        policy = self._force_pull_policy
+        if policy is None:
+            if isinstance(self._image, dict):
+                policy = self._image.get("pull_policy", None)
+        return policy
+
+    @docker_pull_policy.setter
+    def docker_pull_policy(self, value: Optional[str]):
+        self._force_pull_policy = value
+        self.docker.pull_policy = value
+
+    @property
+    def inside_workspace(self) -> str:
+        if is_windows():
+            import ntpath
+            # if the workspace is not on c:, map it to a c: location in the container
+            # or if the path is quite long
+            if not self.workspace.lower().startswith("c:") or len(self.workspace) > 32:
+                basedir = ntpath.basename(self.workspace)
+                return f"c:\\b\\{basedir}"[:14]
+        else:
+            if len(self.workspace) > 80:
+                # truncate really long paths even on linux
+                return f"/b/{os.path.basename(self.workspace)[:64]}"
+
+        return self.workspace
+
+    def allocate_runner(self):
+        if self.runner is None:  # pragma: no cover
+            raise GitlabEmulatorError(f"could not find a local docker runner for this job@ {self.name}")
+        super().allocate_runner()
+        if self.runner and self.runner.docker:
+            if self._image is None:
+                self._image = self.runner.docker.image
+            self.docker.privileged = self.runner.docker.privileged
+            if self.runner.docker.docker_cli is not None:
+                self.docker.tool = self.runner.docker.docker_cli
+
+    def load(self, name, config, overrides: Optional[Dict[str, Any]] = None):
+        super(DockerJob, self).load(name, config, overrides=overrides)
+        self.services = get_services(config, name)
+        pull_policy = self.docker_pull_policy
+        if pull_policy is not None:
+            self.docker.pull_policy = pull_policy
+        self.set_job_variables()
+
+    def get_emulator_runner(self) -> Optional[GleRunnerConfig]:
+        ctx = get_user_config_context()
+        return ctx.find_runner(image=True, tags=self.tags)
+
+    def set_job_variables(self):
+        super(DockerJob, self).set_job_variables()
+        all_images = self._config.get("image", None)
+        self._image = self._config[self.name].get("image", all_images)
+        if self.docker_image is not None:
+            self.configure_job_variable("CI_JOB_IMAGE", self.docker_image, force=True)
+        self.configure_job_variable("CI_DISPOSABLE_ENVIRONMENT", "true", force=True)
+        self.configure_job_variable("CI_PROJECT_DIR", self.inside_workspace)
+        self.configure_job_variable("CI_BUILDS_DIR", os.path.dirname(self.inside_workspace))
+
+    def abort(self):
+        """
+        Abort the build by killing our container
+        :return:
+        """
+        info("abort docker job {}".format(self.name))
+        # we need to wait for the container to start
+        if self.docker.container is None:
+            time.sleep(1)
+
+        if self.container and self.docker.container:
+            info("kill container {}".format(self.name))
+            self.docker.kill()
+        if self.build_process is not None:
+            try:  # pragma: no cover
+                if self.build_process.poll() is None:
+                    self.build_process.terminate()
+            except Exception as err:  # pragma: no cover
+                assert err is not None
+
+    def get_envs(self, expand_only_ci=True):
+        """
+        Get env vars for a docker job
+        :return:
+        """
+        envs = self.base_variables()
+        return self.get_defined_envs(envs, expand_only_ci=expand_only_ci)
+
+    def run_script(self, lines):
+        return self._run_script(lines)
+
+    def _run_script(self, lines, attempts=2, user=None):
+        task = None
+        if user is None:
+            if self.shell_is_user:  # pragma: cover if posix
+                user = self._shell_uid
+
+        filename = "generated-gitlab-script" + self.get_script_fileext()
+        temp = os.path.join(tempfile.gettempdir(), filename)
+        try:
+            with open(temp, "w") as fd:
+                print(lines, file=fd)
+            # copy it to the container
+            dest = "/tmp"
+            if is_windows():  # pragma: cover if windows
+                dest = "c:\\windows\\temp"
+            target_script = os.path.join(dest, filename)
+            info("Copying {} to container as {} ..".format(temp, target_script))
+            self.docker.add_file(temp, dest)
+
+            while attempts > 0:
+                try:
+                    interactive = bool(self.enter_shell or self.error_shell)
+                    if interactive:  # pragma: no cover
+                        try:
+                            if not os.isatty(sys.stdin.fileno()):
+                                interactive = False
+                        except OSError:
+                            # probably under pycharm pytest
+                            interactive = False
+                    cmdline = self.shell_command(target_script)
+                    task = self.docker.exec(self.inside_workspace,
+                                            cmdline,
+                                            tty=interactive,
+                                            user=user)
+                    self.communicate(task, script=None)
+                    break
+                except DockerExecError:  # pragma: no cover
+                    self.stdout.write(
+                        "Warning: docker exec error - https://gitlab.com/cunity/gitlab-emulator/-/issues/10")
+                    attempts -= 1
+                    if attempts == 0:
+                        raise
+                    else:
+                        time.sleep(2)
+            return task
+        finally:
+            if os.path.exists(temp):
+                os.unlink(temp)
+
+    def check_docker_exec_failed(self, line):
+        """
+        Raise an error if the build script has returned "No such exec instance"
+        :param line:
+        :return:
+        """
+        if line:
+            try:
+                decoded = line.decode()
+            except Exception:
+                return
+            if decoded:
+                if "No such exec instance" in decoded:
+                    raise DockerExecError()
+
+    def communicate(self, process, script=None):
+        comm(process, self.stdout, script=script, linehandler=self.check_docker_exec_failed)
+
+    def has_bash(self):
+        """
+        Return True of the container has bash
+        :return:
+        """
+        if self._has_bash is None:
+            self._has_bash = False
+            if not is_windows():
+                info("checking container for bash")
+                try:
+                    self.docker.check_call(
+                        self.inside_workspace, ["sh", "-c", "command -v bash"],
+                        capture=True,
+                        stderr=subprocess.STDOUT)
+                    self._has_bash = True
+                except subprocess.CalledProcessError as cpe:
+                    assert cpe
+        return self._has_bash
+
+    def shell_on_error(self):
+        """
+        Execute a shell command on job errors
+        :return:
+        """
+        print("Job {} script error..".format(self.name), flush=True)
+        lines = "\n".join(self.error_shell)
+        self.run_script(lines)
+
+    def git_safe_dir(self):
+        """Configure git safe.directory if possible"""
+        info("attempting to set git safe.directory..")
+        folder = self.inside_workspace
+        cmdline = f"command -v git 2>&1 >/dev/null && git config --global --add safe.directory '{folder}'"
+        if is_windows():  # pragma: cover if windows
+            folder = folder.replace("\\", "/")  # windows git won't understand \ chars for this
+            cmdline = f"git config --global --add safe.directory {folder}"
+        info(f"running {cmdline}")
+        self.run_script(cmdline)
+
+    def run_impl(self):
+        info(f"running docker job {self.name}")
+        info(f"runner = {self.runner}")
+        from .resnamer import generate_resource_name
+        if is_windows():  # pragma: cover if windows
+            warning("warning windows docker is experimental")
+        if self.runner.docker is None:
+            raise GitlabEmulatorError("docker not detected")
+
+        with self._container_lock:
+            self.docker.image = self.docker_image
+            self.container = generate_resource_name()
+            self.docker.name = self.container
+            if not is_windows():  # pragma: cover if not windows
+                if self.runner.docker:
+                    self.docker.privileged = self.runner.docker.privileged
+
+            if not is_windows():  # pragma: cover if not windows
+                image_name = self.docker.image
+                image_name = image_name.split("/")[-1].split("@")[0].split(":")[0]
+                if self.error_shell or self.enter_shell:
+                    self.docker.add_env("PS1", f"[{self.name}] \\u@{image_name}:$PWD $ ")
+
+            if self.docker.pull_always or (self.docker.pull_if_not_present and not self.docker.image_present):
+                self.docker.pull()
+
+            environ = self.get_envs(expand_only_ci=False)
+            with docker_services(self, environ) as network:
+                if network:
+                    self.docker.network = network
+                for envname in environ:
+                    self.docker.add_env(envname, environ[envname])
+
+                if self.docker_entrypoint is not None:
+                    if is_windows():  # pragma: cover if windows
+                        # windows can't have multiple args
+                        args = self.docker_entrypoint
+                        if len(args) > 0:
+                            if len(args) > 1:
+                                warning("windows docker entrypoint override may fail with several args")
+                            self.docker.entrypoint = args[0]
+                    else:
+                        self.docker.entrypoint = self.docker_entrypoint
+                volumes = self.runner.docker.runtime_volumes()
+                if volumes:
+                    info("Extra docker volumes registered:")
+                    for item in volumes:
+                        info("- {}".format(item))
+
+                self.docker.volumes = volumes + [f"{self.workspace}:{self.inside_workspace}:rw"]
+
+                self.docker.run()
+
+                if not is_windows():  # pragma: cover if not windows
+                    # work out default USER from the image
+                    docker_user_cfg = self.docker.get_user()
+                    if docker_user_cfg and ":" in docker_user_cfg:
+                        info(f"Container image defines USER: {docker_user_cfg}")
+                        docker_user, docker_grp = docker_user_cfg.split(":", 1)
+                        info(f"Setting ownership to {docker_user}:{docker_grp}")
+                        self._run_script(f"chown -R {docker_user}.{docker_grp} .", attempts=1, user="0")
+
+                if self.shell_is_user:
+                    if not is_windows():  # pragma: cover if not windows
+                        # try to make a more functional user account inside the container, this may not always
+                        # work due to missing tools, but it's worth a try
+                        _homedir = "/gle-tmp-home"
+                        _passwd = f"gle:x:{self._shell_uid}:{self._shell_gid}:gitlab-emulator:{_homedir}:/bin/sh"
+                        _shadow = f"gle:!:{self._shell_uid}::::::"
+                        try:
+                            info(f"setting up interactive user with uid={self._shell_uid}..")
+                            self.docker.check_call("/",
+                                                   ["sh", "-c", f"echo {_passwd} >> /etc/passwd"], capture=True)
+                            self.docker.check_call("/",
+                                                   ["sh", "-c", f"echo {_shadow} >> /etc/shadow"], capture=True)
+                            self.docker.check_call("/",
+                                                   ["mkdir", _homedir], capture=True)
+                            self.docker.check_call("/",
+                                                   ["chown", str(self._shell_uid), _homedir], capture=True)
+                            self.docker.check_call("/",
+                                                   ["chgrp", str(self._shell_gid), _homedir], capture=True)
+                            # append this user to the sudoers file
+                            info(f"granting sudo inside container..")
+                            self.docker.check_call("/",
+                                                   ["sh", "-c", f"echo 'gle ALL=(ALL) NOPASSWD: ALL' >> /etc/sudoers"])
+                            info("interactive user setup completed.")
+                        except subprocess.CalledProcessError:
+                            warning("interactive user setup failed, some features may not work fully.")
+
+                self.git_safe_dir()
+
+                try:
+                    lines = self.before_script + self.script
+                    if self.enter_shell:
+                        lines.extend(self.get_interactive_shell_command())
+
+                    self.build_process = self.run_script(make_script(lines, powershell=self.is_powershell()))
+                finally:
+                    try:
+                        if self.error_shell:  # pragma: no cover
+                            if not self.build_process or self.build_process.returncode:
+                                self.shell_on_error()
+                        if self.after_script:
+                            info("Running after_script..")
+                            self.run_script(make_script(self.after_script, powershell=self.is_powershell()))
+                    except subprocess.CalledProcessError:  # pragma: no cover
+                        pass
+                    finally:
+                        subprocess.call([self.docker.tool, "kill", self.container], stderr=subprocess.STDOUT)
+                        if self.docker.is_windows_hyperv():  # pragma: no cover
+                            subprocess.call([self.docker.tool, "rm", self.container])
+
+        result = self.build_process.returncode
+        if result:
+            fatal("Docker job {} failed".format(self.name))
+
+
+def get_services(config, jobname):
+    """
+    Get the service containers that should be started for a particular job
+    :param config:
+    :param jobname:
+    :return:
+    """
+    job = config.get(jobname)
+
+    services = []
+    service_defs = []
+
+    if "image" in config or "image" in job:
+        # yes we are using docker, so we can offer services for this job
+        all_services = config.get("services", [])
+        job_services = job.get("services", [])
+        services = all_services + job_services
+
+    for service in services:
+        item = {}
+        # if this is a dict use the extended version
+        # else make extended versions out of the single strings
+        if isinstance(service, str):
+            item["name"] = service
+
+        # if this is a dict, it needs to at least have name but could have
+        # alias and others
+        if isinstance(service, dict):
+            assert "name" in service
+            item = service
+
+        if item:
+            service_defs.append(item)
+
+    return service_defs
+
+
+@contextmanager
+def docker_services(job: DockerJob, variables: Dict[str, str]):
+    """
+    Setup docker services required by the given job
+    :param job:
+    :param variables: dict of env vars to set in the service container
+    :return:
+    """
+    services = job.services
+    service_network = None
+    containers = []
+    try:
+        if services:
+            net_name = "gle-service-network"
+            try:
+                job.docker.docker_call("network", "inspect", net_name)
+            except DockerToolFailed:
+                job.docker.docker_call(
+                    "network", "create",
+                    "--driver", "bridge",
+                    "--subnet", "192.168.94.0/24",
+                    net_name
+                )
+
+            for service in services:
+                aliases = []
+                if isinstance(service, str):
+                    image = service
+                    service = {
+                        "name": image
+                    }
+                else:
+                    image = service["name"]
+                name = image
+                if ":" in name:
+                    name = image.split(":", 1)[0]
+                aliases.append(name.replace("/", "-"))
+                if "alias" in service:
+                    aliases.append(service["alias"])
+
+                job.stdout.write(f"create docker service : {name} ({aliases})\n")
+                if job.docker.can_pull:
+                    try:
+                        job.stdout.write(f"pulling {image} ..\n")
+                        job.docker.docker_call("pull", image)
+                    except DockerToolFailed:  # pragma: no cover
+                        fatal(f"No such image {image}")
+                priv = not is_windows()
+                service_cmdline = [
+                    "run",
+                    "--rm",
+                    "-d",
+                ]
+                if priv:  # pragma: cover if not windows
+                    service_cmdline.append("--privileged")
+                for name, value in variables.items():
+                    service_cmdline.extend(["-e", f"{name}={value}"])
+                service_cmdline.append(image)
+
+                container = job.docker.docker_call(
+                    *service_cmdline
+                ).stdout.strip()
+
+                info(f"creating docker service {name} ({aliases})")
+                info(f"service {name} is container {container}")
+                containers.append(container)
+                info(f"connect {name} to service network")
+                connect_cmdline = [
+                    "network", "connect"
+                ]
+                for alias in aliases:
+                    connect_cmdline.extend(["--alias", alias])
+                connect_cmdline.extend([net_name, container])
+                job.docker.docker_call(*connect_cmdline)
+                service_network = net_name
+
+        yield service_network
+    finally:
+        for container in containers:
+            info(f"clean up docker service {container}")
+            job.docker.docker_call("kill", "-s", "9", container)
+        time.sleep(1)
```

## gitlabemu/errors.py

 * *Ordering differences only*

```diff
@@ -1,46 +1,46 @@
-"""
-Base error types
-"""
-
-
-class GitlabEmulatorError(Exception):
-    """
-    Common base for all errors we raise
-    """
-    pass
-
-
-class DockerExecError(GitlabEmulatorError):
-    """
-    Docker exec failed to start
-    """
-    pass
-
-
-class ConfigLoaderError(GitlabEmulatorError):
-    """
-    There was an error loading a gitlab configuration
-    """
-    pass
-
-
-class BadSyntaxError(ConfigLoaderError):
-    """
-    The yaml was somehow invalid
-    """
-
-    def __init__(self, message):
-        super(BadSyntaxError, self).__init__(message)
-
-
-class FeatureNotSupportedError(ConfigLoaderError):
-    """
-    The loaded configuration contained gitlab features locallab does not
-    yet support
-    """
-
-    def __init__(self, feature):
-        self.feature = feature
-
-    def __str__(self):
-        return "FeatureNotSupportedError ({})".format(self.feature)
+"""
+Base error types
+"""
+
+
+class GitlabEmulatorError(Exception):
+    """
+    Common base for all errors we raise
+    """
+    pass
+
+
+class DockerExecError(GitlabEmulatorError):
+    """
+    Docker exec failed to start
+    """
+    pass
+
+
+class ConfigLoaderError(GitlabEmulatorError):
+    """
+    There was an error loading a gitlab configuration
+    """
+    pass
+
+
+class BadSyntaxError(ConfigLoaderError):
+    """
+    The yaml was somehow invalid
+    """
+
+    def __init__(self, message):
+        super(BadSyntaxError, self).__init__(message)
+
+
+class FeatureNotSupportedError(ConfigLoaderError):
+    """
+    The loaded configuration contained gitlab features locallab does not
+    yet support
+    """
+
+    def __init__(self, feature):
+        self.feature = feature
+
+    def __str__(self):
+        return "FeatureNotSupportedError ({})".format(self.feature)
```

## gitlabemu/generator.py

 * *Ordering differences only*

```diff
@@ -1,143 +1,143 @@
-"""Generate sub-pipelines"""
-import os.path
-import subprocess
-import time
-from typing import List, Dict, Optional
-
-from gitlab.v4.objects import ProjectPipeline
-from .yamlloader import StringableOrderedDict
-from .configloader import Loader
-from .helpers import git_top_level, git_commit_sha, git_uncommitted_changes, git_current_branch, git_push_force_upstream
-
-
-def generate_artifact_fetch_job(
-        loader: Loader,
-        stage: str,
-        needed: Dict[str, str],
-        tls_verify: Optional[bool] = True
-) -> dict:
-    """Generate a job to fetch artifacts of needed jobs from a completed pipeline"""
-    # use CI_JOB_TOKEN to fetch the artifacts
-    script = []
-    paths = []
-    generated = {}
-    verify = "--cacert $CI_SERVER_TLS_CA_FILE"
-    if not tls_verify:
-        verify = "--insecure"
-    for name in needed:
-        job = loader.get_job(name)
-        # does it define any artifacts?
-        artifacts = job.get("artifacts", {})
-        artifact_paths = artifacts.get("paths", [])
-        if artifact_paths:
-            url = needed[name]
-            script.extend(
-                [
-                    'apk add curl',
-                    f'curl {verify} --location --output {name}-artifacts.zip --header "JOB-TOKEN: $CI_JOB_TOKEN" {url}',
-                    f'unzip -o {name}-artifacts.zip',
-                    f'rm -f {name}-artifacts.zip',
-                ]
-            )
-
-            paths.extend(artifact_paths)
-    if paths:
-        generated = {
-            "stage": stage,
-            "image": "alpine:3.14",
-            "script": script,
-            "artifacts": {
-                "paths": list(set(paths)),
-                "expire_in": '1 day'
-            },
-            "variables": {
-                "KUBERNETES_CPU_REQUEST": "1",
-                "KUBERNETES_MEMORY_REQUEST": "2G",
-            }
-        }
-
-    return generated
-
-
-def generate_pipeline_yaml(loader: Loader,
-                           *goals: str,
-                           recurse: Optional[bool] = True) -> dict:
-    """Generate a subset pipeline to build the given goals"""
-    generated = StringableOrderedDict()
-    stages = loader.config.get("stages", [])
-    needed = set(goals)
-
-    while len(needed):
-        for name in list(needed):
-            needed.remove(name)
-            job = loader.get_job(name)
-            # strip out extends and rules
-            for remove in ["extends", "when", "only", "rules", "except"]:
-                if remove in job:
-                    del job[remove]
-            stage = job.get("stage", None)
-            if stage:
-                if stage not in stages:
-                    stages.append(stage)
-            generated[name] = job
-            loaded = loader.load_job(name)
-            if recurse:
-                # build the needed jobs in the pipeline
-                for item in loaded.dependencies:
-                    if isinstance(item, str):
-                        needed.add(item)
-
-    if stages:
-        generated["stages"] = list(stages)
-
-    # get the variables and defaults sections etc
-    variables = dict(loader.config.get("variables", {}))
-    generated["variables"] = variables
-    for item in ["image", "default", "before_script", "after_script", "services"]:
-        if item in loader.config:
-            generated[item] = loader.config.get(item)
-
-    return generated
-
-
-def create_pipeline_branch(repo: str,
-                           remote: str,
-                           new_branch: str,
-                           commit_message: str,
-                           files: Dict[str, str],
-                           ) -> Optional[str]:
-    """"""
-    commit = None
-    topdir = git_top_level(repo)
-    original = git_current_branch(topdir)
-    changes = git_uncommitted_changes(topdir)
-    if not changes:
-        try:
-            subprocess.check_call(["git", "-C", topdir, "checkout", "-B", new_branch])
-            for filename in files:
-                filepath = os.path.join(topdir, filename)
-                folder = os.path.dirname(filepath)
-                if not os.path.exists(folder):
-                    os.makedirs(folder)
-                with open(filepath, "w") as fd:
-                    fd.write(files[filename])
-                subprocess.check_call(["git", "-C", topdir, "add", filepath])
-
-            subprocess.check_call(["git", "-C", topdir, "commit", "-am", commit_message])
-            git_push_force_upstream(topdir, remote, new_branch)
-            commit = git_commit_sha(topdir)
-        finally:
-            subprocess.check_call(["git", "-C", topdir, "checkout", "-qf", original])
-    return commit
-
-
-def wait_for_project_commit_pipeline(project, commit, timeout=30) -> Optional[ProjectPipeline]:
-    # pragma: no cover
-    started = time.time()
-    while time.time() - started < timeout:
-        time.sleep(2)
-        pipes = project.pipelines.list(sort="desc", order_by="updated_at", page=1, per_page=16)
-        for pipeline in pipes:
-            if pipeline.sha == commit:
-                return pipeline
-    return None
+"""Generate sub-pipelines"""
+import os.path
+import subprocess
+import time
+from typing import List, Dict, Optional
+
+from gitlab.v4.objects import ProjectPipeline
+from .yamlloader import StringableOrderedDict
+from .configloader import Loader
+from .helpers import git_top_level, git_commit_sha, git_uncommitted_changes, git_current_branch, git_push_force_upstream
+
+
+def generate_artifact_fetch_job(
+        loader: Loader,
+        stage: str,
+        needed: Dict[str, str],
+        tls_verify: Optional[bool] = True
+) -> dict:
+    """Generate a job to fetch artifacts of needed jobs from a completed pipeline"""
+    # use CI_JOB_TOKEN to fetch the artifacts
+    script = []
+    paths = []
+    generated = {}
+    verify = "--cacert $CI_SERVER_TLS_CA_FILE"
+    if not tls_verify:
+        verify = "--insecure"
+    for name in needed:
+        job = loader.get_job(name)
+        # does it define any artifacts?
+        artifacts = job.get("artifacts", {})
+        artifact_paths = artifacts.get("paths", [])
+        if artifact_paths:
+            url = needed[name]
+            script.extend(
+                [
+                    'apk add curl',
+                    f'curl {verify} --location --output {name}-artifacts.zip --header "JOB-TOKEN: $CI_JOB_TOKEN" {url}',
+                    f'unzip -o {name}-artifacts.zip',
+                    f'rm -f {name}-artifacts.zip',
+                ]
+            )
+
+            paths.extend(artifact_paths)
+    if paths:
+        generated = {
+            "stage": stage,
+            "image": "alpine:3.14",
+            "script": script,
+            "artifacts": {
+                "paths": list(set(paths)),
+                "expire_in": '1 day'
+            },
+            "variables": {
+                "KUBERNETES_CPU_REQUEST": "1",
+                "KUBERNETES_MEMORY_REQUEST": "2G",
+            }
+        }
+
+    return generated
+
+
+def generate_pipeline_yaml(loader: Loader,
+                           *goals: str,
+                           recurse: Optional[bool] = True) -> dict:
+    """Generate a subset pipeline to build the given goals"""
+    generated = StringableOrderedDict()
+    stages = loader.config.get("stages", [])
+    needed = set(goals)
+
+    while len(needed):
+        for name in list(needed):
+            needed.remove(name)
+            job = loader.get_job(name)
+            # strip out extends and rules
+            for remove in ["extends", "when", "only", "rules", "except"]:
+                if remove in job:
+                    del job[remove]
+            stage = job.get("stage", None)
+            if stage:
+                if stage not in stages:
+                    stages.append(stage)
+            generated[name] = job
+            loaded = loader.load_job(name)
+            if recurse:
+                # build the needed jobs in the pipeline
+                for item in loaded.dependencies:
+                    if isinstance(item, str):
+                        needed.add(item)
+
+    if stages:
+        generated["stages"] = list(stages)
+
+    # get the variables and defaults sections etc
+    variables = dict(loader.config.get("variables", {}))
+    generated["variables"] = variables
+    for item in ["image", "default", "before_script", "after_script", "services"]:
+        if item in loader.config:
+            generated[item] = loader.config.get(item)
+
+    return generated
+
+
+def create_pipeline_branch(repo: str,
+                           remote: str,
+                           new_branch: str,
+                           commit_message: str,
+                           files: Dict[str, str],
+                           ) -> Optional[str]:
+    """"""
+    commit = None
+    topdir = git_top_level(repo)
+    original = git_current_branch(topdir)
+    changes = git_uncommitted_changes(topdir)
+    if not changes:
+        try:
+            subprocess.check_call(["git", "-C", topdir, "checkout", "-B", new_branch])
+            for filename in files:
+                filepath = os.path.join(topdir, filename)
+                folder = os.path.dirname(filepath)
+                if not os.path.exists(folder):
+                    os.makedirs(folder)
+                with open(filepath, "w") as fd:
+                    fd.write(files[filename])
+                subprocess.check_call(["git", "-C", topdir, "add", filepath])
+
+            subprocess.check_call(["git", "-C", topdir, "commit", "-am", commit_message])
+            git_push_force_upstream(topdir, remote, new_branch)
+            commit = git_commit_sha(topdir)
+        finally:
+            subprocess.check_call(["git", "-C", topdir, "checkout", "-qf", original])
+    return commit
+
+
+def wait_for_project_commit_pipeline(project, commit, timeout=30) -> Optional[ProjectPipeline]:
+    # pragma: no cover
+    started = time.time()
+    while time.time() - started < timeout:
+        time.sleep(2)
+        pipes = project.pipelines.list(sort="desc", order_by="updated_at", page=1, per_page=16)
+        for pipeline in pipes:
+            if pipeline.sha == commit:
+                return pipeline
+    return None
```

## gitlabemu/gitlab_client_api.py

 * *Ordering differences only*

```diff
@@ -1,537 +1,537 @@
-import contextlib
-import os
-import random
-import re
-import shutil
-import subprocess
-import time
-import zipfile
-import requests
-import tempfile
-import certifi
-import multiprocessing
-from functools import lru_cache
-from threading import RLock
-from typing import Optional, Tuple, Iterable, List, Any, Dict
-from urllib.parse import urlparse
-from gitlab import Gitlab, GitlabGetError
-from gitlab.v4.objects import Project
-from urllib3.exceptions import InsecureRequestWarning
-
-from .helpers import die, note, get_git_remote_urls, remote_servers
-from .userconfig import get_user_config_context
-
-GITLAB_SERVER_ENV = "GLE_GITLAB_SERVER"
-GITLAB_PROJECT_ENV = "GLE_GITLAB_PROJECT"
-SYSTEM_CA_CERTS = "/etc/ssl/certs/ca-certificates.crt"
-
-
-class GitlabIdent:
-    def __init__(self, server=None, project=None, pipeline=None, gitref=None, secure=True):
-        self.server: Optional[str] = server
-        self.project: Optional[str] = project
-        self.pipeline: Optional[int] = pipeline
-        self.gitref: Optional[str] = gitref
-        self.secure: Optional[bool] = secure
-
-    def __str__(self):  # pragma: no cover
-        attribs = []
-        if self.server:
-            attribs.append(f"server={self.server}")
-        if self.project:
-            attribs.append(f"project={self.project}")
-        if self.gitref:
-            attribs.append(f"git_ref={self.gitref}")
-        elif self.pipeline:
-            attribs.append(f"id={self.pipeline}")
-
-        return f"Pipeline {', '.join(attribs)}"
-
-
-class TaskError(Exception):
-    def __init__(self, task, inner):
-        self.task = task
-        self.inner = inner
-
-
-class PipelineError(Exception):
-    def __init__(self, pipeline: str):
-        super(PipelineError, self).__init__()
-        self.pipeline = pipeline
-
-
-class PipelineInvalid(PipelineError):
-    def __init__(self, pipeline: str):
-        super(PipelineInvalid, self).__init__(pipeline)
-
-    def __str__(self):
-        return f"'{self.pipeline}' is not a valid pipeline specification"
-
-
-class PipelineNotFound(PipelineError):
-    def __init__(self, pipeline):
-        super(PipelineNotFound, self).__init__(pipeline)
-
-    def __str__(self):
-        return f"Cannot find pipeline '{self.pipeline}'"
-
-
-def gitlab_api(alias: str, secure=True) -> Gitlab:
-    """Create a Gitlab API client"""
-    ctx = get_user_config_context()
-    server = None
-    token = None
-    for item in ctx.gitlab.servers:
-        if item.name == alias:
-            server = item.server
-            token = item.token
-            break
-
-        parsed = urlparse(item.server)
-        if parsed.hostname == alias:
-            server = item.server
-            token = item.token
-            break
-
-    if not server:
-        note(f"using {alias} as server hostname")
-        server = alias
-        if "://" not in server:
-            server = f"https://{server}"
-
-    environment_token = os.getenv("GITLAB_PRIVATE_TOKEN", None)
-    if environment_token:
-        token = environment_token
-        note("Using GITLAB_PRIVATE_TOKEN for authentication")
-
-    if not token:
-        die(f"Could not find a configured token for {alias} or GITLAB_PRIVATE_TOKEN not set")
-
-    client = Gitlab(url=server, private_token=token, ssl_verify=secure)
-    if secure:
-        gitlab_session_head(client.session, server)
-    return client
-
-
-def parse_gitlab_from_arg(arg: str, prefer_gitref: Optional[bool] = False) -> GitlabIdent:
-    """Decode an identifier into a project and optionally pipeline ID or git reference"""
-    # server/group/project/1234    = pipeline 1234 from server/group/project
-    # 1234                         = pipeline 1234 from current project
-    # server/group/project=gitref  = last successful pipeline for group/project at gitref commit/tag/branch
-    # =gitref                      = last successful pipeline at the gitref of the current project
-    gitref = None
-    project = None
-    server = None
-    pipeline = None
-    if arg.isnumeric():
-        pipeline = int(arg)
-    elif prefer_gitref:
-        gitref = arg
-        arg = ""
-    elif "=" in arg:
-        arg, gitref = arg.rsplit("=", 1)
-
-    if "/" in arg:
-        parts = arg.split("/")
-        if len(parts) > 2:
-            server = parts[0]
-            if parts[-1].isnumeric():
-                pipeline = int(parts[-1])
-                project = "/".join(parts[1:-1])
-            else:
-                project = "/".join(parts[1:])
-
-    return GitlabIdent(project=project,
-                       server=server,
-                       pipeline=pipeline,
-                       gitref=gitref)
-
-
-def find_project_pipeline(project,
-                          pipeline: Optional[int] = 0,
-                          ref: Optional[str] = None):
-    """Get a pipeline from the current project"""
-    try:
-        if pipeline:
-            return project.pipelines.get(pipeline)
-        match = {}
-        if ref:
-            match["ref"] = ref
-
-        found = project.pipelines.list(sort="desc", order_by="updated_at", page=1, pagesize=1, **match)
-        if not found:
-            raise PipelineNotFound(str(match))
-        return found[0]
-
-    except GitlabGetError as err:
-        if err.response_code == 404:
-            raise PipelineNotFound(str(pipeline))
-
-
-def get_pipeline(fromline, secure: Optional[bool] = True):
-    """Get a pipeline"""
-    pipeline = None
-    ident = parse_gitlab_from_arg(fromline)
-    if not secure:
-        note("TLS server validation disabled by --insecure")
-        requests.packages.urllib3.disable_warnings(InsecureRequestWarning)
-
-    if not ident.pipeline:
-        if not ident.gitref:
-            raise PipelineInvalid(fromline)
-
-    if not ident.server:
-        cwd = os.getcwd()
-        gitlab, project, remotename = get_gitlab_project_client(cwd, secure)
-    else:
-        gitlab = gitlab_api(ident.server, secure=secure)
-        # get project
-        project = gitlab.projects.get(ident.project)
-
-    if not project:
-        raise PipelineInvalid(fromline)
-
-    # get pipeline
-    if ident.pipeline:
-        try:
-            pipeline = find_project_pipeline(project, pipeline=ident.pipeline)
-        except GitlabGetError as err:
-            if err.response_code == 404:
-                raise PipelineNotFound(fromline)
-
-    return gitlab, project, pipeline
-
-_CA_FIXUP_LOCK = RLock()
-
-
-@lru_cache(1)
-def get_ca_bundle() -> str:
-    bundles = []
-    for env in ["REQUESTS_CA_BUNDLE", "CI_SERVER_TLS_CA_FILE"]:
-        bundle = os.getenv(env, None)
-        if bundle and os.path.isfile(bundle):
-            note(f"Using extra CAs from {env} in {bundle}")
-            bundles.append(bundle)
-    certs = certifi.contents()
-    if os.path.exists(SYSTEM_CA_CERTS) and not os.path.samefile(SYSTEM_CA_CERTS, certifi.where()):
-        with open(SYSTEM_CA_CERTS, "r") as etc_certs:
-            certs += etc_certs.read()
-    for bundle in bundles:
-        with open(bundle, "r") as data:
-            certs += data.read()
-    return certs
-
-
-@contextlib.contextmanager
-def posix_cert_fixup():
-    with _CA_FIXUP_LOCK:
-        if "GLE_CA_BUNDLE" not in os.environ:
-            with tempfile.TemporaryDirectory() as tempdir:
-                os.environ["GLE_CA_BUNDLE"] = "1"
-                certs = get_ca_bundle()
-                new_bundle = os.path.join(tempdir, "ca-certs.pem")
-                with open(new_bundle, "w") as data:
-                    data.write(certs)
-                os.environ["REQUESTS_CA_BUNDLE"] = new_bundle
-                try:
-                    yield
-                finally:
-                    del os.environ["GLE_CA_BUNDLE"]
-                    del os.environ["REQUESTS_CA_BUNDLE"]
-        else:
-            yield
-
-
-def gitlab_session_head(session, geturl, **kwargs):
-    """HEAD using requests to try different CA options"""
-    with posix_cert_fixup():
-        return session.head(geturl, **kwargs)
-
-
-def get_one_file(session: requests.Session,
-                 url: str,
-                 outfile: str,
-                 headers: Optional[dict] = None) -> str:
-    """Download one file from a gitlab url and save it"""
-    outdir = os.path.dirname(outfile)
-    partfile = outfile + ".part"
-    if os.path.exists(outfile):
-        os.unlink(outfile)
-    os.makedirs(outdir, exist_ok=True)
-    with posix_cert_fixup():
-        resp = session.get(url, headers=headers, stream=True)
-        resp.raise_for_status()
-        with open(partfile, "wb") as data:
-            shutil.copyfileobj(resp.raw, data, length=2 * 1024 * 1024)
-        shutil.move(partfile, outfile)
-    return outfile
-
-
-def unpack_one_artifact(temp_zip_file: str, outdir: str, name: str,
-                        progress: Optional[bool] = True):
-    """Extract a downloaded artifact zip"""
-    if progress:
-        note(f" Extracting artifacts from job '{name}' ..")
-    with open(temp_zip_file, "rb") as compressed:
-        with zipfile.ZipFile(compressed) as zf:
-            for item in zf.infolist():
-                savefile = os.path.join(outdir, item.filename)
-                if os.path.exists(savefile):
-                    note(f"  Warning: File {savefile} already exists, overwriting..")
-                if progress:
-                    note(f"  Saving {savefile} ..")
-                zf.extract(item, path=outdir)
-
-
-def multi_download_unpack_jobs(gitlab: Gitlab,
-                               project,
-                               outdir: str,
-                               jobs,
-                               callback: Optional[List[str]] = None,
-                               headers: Optional[dict] = None,
-                               export_mode: Optional[bool] = False):
-    """Download and unpack multiple jobs, use parallelization if in export mode"""
-    workers = (multiprocessing.cpu_count() - 1)
-    if workers < 1:
-        workers = 1
-    if workers > 4:  # pragma: no cover
-        workers = 4
-    if not export_mode:
-        workers = 1
-    downloads = []
-
-    for fetch_job in jobs:
-        artifact_url = None
-        if [x for x in fetch_job.artifacts if x["file_type"] == "archive"]:
-            artifact_url = f"{gitlab.api_url}/projects/{project.id}/jobs/{fetch_job.id}/artifacts"
-        jobdir = outdir
-        trace_url = None
-
-        if export_mode:
-            jobdir = os.path.join(outdir, sanitize_pathname(fetch_job.name))
-            trace_url = f"{gitlab.api_url}/projects/{project.id}/jobs/{fetch_job.id}/trace"
-
-        downloads.append([fetch_job.name,
-                          gitlab.session.auth,
-                          callback,
-                          jobdir,
-                          artifact_url,
-                          trace_url,
-                          headers])
-    if export_mode:
-        random.shuffle(downloads)
-    progress = 0
-    sizes = []
-    times = []
-    started = time.monotonic()
-    with multiprocessing.Pool(processes=workers) as pool:
-        manager = multiprocessing.Manager()
-        printmutex = manager.Lock()
-        execmutex = manager.Lock()
-        tasks = []
-        for args in downloads:
-            args.append(printmutex)
-            args.append(execmutex)
-            task = pool.apply_async(downloader, args)
-            tasks.append(task)
-
-        for task in tasks:
-            try:
-                size, download_time = task.get()
-            except TaskError as err:  # pragma: no cover
-                note(f"error! failed downloading {err.task}!")
-                raise err.inner
-            sizes.append(size)
-            times.append(download_time)
-            progress += 1
-            sofar = time.monotonic() - started
-            note(f"{int(sofar):-4}s {progress:-3}/{len(downloads):-3} completed.")
-    finished = time.monotonic()
-    duration = finished - started
-    total_downloaded = sum(sizes)
-    total_times = sum(times)
-    combined_rate = total_downloaded / total_times
-    note(f"All downloads complete: {int(total_downloaded/1024/1024)} mb.")
-    note(f"Combined transfer rate: {int(combined_rate/1024/1024)} mb/s.")
-    note(f"Total time {int(duration)} sec.")
-
-
-def sanitize_pathname(path: str) -> str:
-    path = path.lower()
-    path = re.sub(r"[^a-z\d\-.]", "_", path)
-    return path
-
-
-def downloader(jobname: str,
-               auth: Optional[Any],
-               cbscript: Optional[List[str]],
-               outdir: str,
-               archive_url: Optional[str],
-               trace_url: Optional[str],
-               hdrs: Optional[dict],
-               printlock: multiprocessing.Lock,
-               execlock: multiprocessing.Lock):
-    try:
-        session = requests.Session()
-        session.auth = auth
-        started = time.monotonic()
-        size = 0
-        archive_file = None
-        if archive_url:
-            archive_file = os.path.join(outdir, "archive.zip")
-            get_one_file(session, archive_url, archive_file, hdrs)
-            size += os.path.getsize(archive_file)
-        if trace_url:
-            trace_file = os.path.join(outdir, "trace.log")
-            get_one_file(session, trace_url, trace_file, hdrs)
-            size += os.path.getsize(trace_file)
-        ended = time.monotonic()
-        duration = ended - started
-        rate = size / duration
-
-        with printlock:
-            note(f"Fetching job {jobname} .. done {int(size / 1024)} kb, {int(rate / 1024)} kb/s")
-        if archive_file:
-            with printlock:
-                note(f"Unpack job {jobname} archive into {outdir} ..")
-            unpack_one_artifact(archive_file, outdir, jobname, progress=False)
-            os.unlink(archive_file)
-            with printlock:
-                note(f"Unpack job {jobname} archive into {outdir} .. done")
-        if cbscript:
-            args = []
-            for x in cbscript:
-                args.append(x.replace("%p", outdir))
-            with execlock:
-                with printlock:
-                    note(f"Executing '{' '.join(args)}'..")
-                subprocess.check_call(args, shell=False)
-        return size, duration
-    except Exception as err:
-        raise TaskError(jobname, err)
-
-
-def do_gitlab_fetch(from_pipeline: str,
-                    get_jobs: Iterable[str],
-                    download_to: Optional[str] = None,
-                    export_to: Optional[str] = False,
-                    callback: Optional[List[str]] = None,
-                    tls_verify: Optional[bool] = True):
-    """Fetch builds and logs from gitlab"""
-    gitlab, project, pipeline = get_pipeline(from_pipeline, secure=tls_verify)
-    gitlab.session.verify = tls_verify  # hmm ?
-    pipeline_jobs = pipeline.jobs.list(all=True)
-    known_jobs = [x.name for x in pipeline_jobs]
-    want_jobs = []
-    if get_jobs:
-        want_jobs = list(get_jobs)
-        for item in list(want_jobs):
-            if item not in known_jobs:  # pragma: no cover
-                # most of the time this is not reached, but if a pipeline has job
-                # rules then the server may not have made a job for this even if it
-                # existed in the yaml. This is quite hard to test without mocking more of the server
-                errmsg = f"Pipeline {pipeline.id} does not contain a job named '{item}'"
-                similar = [name for name in known_jobs if "/" in name and name.startswith(item)]
-                if not similar:
-                    die(errmsg)
-                want_jobs.extend(similar)
-            else:
-                want_jobs.append(item)
-
-    fetch_jobs = [x for x in pipeline_jobs if not want_jobs or x.name in want_jobs]
-
-    assert export_to or download_to
-    outdir = download_to
-    if export_to:
-        mode = "Exporting"
-        outdir = export_to
-    else:
-        mode = "Fetching"
-    note(f"{mode} {len(fetch_jobs)} jobs from {pipeline.web_url}..")
-    headers = {}
-    if gitlab.private_token:
-        headers = {"PRIVATE-TOKEN": gitlab.private_token}
-    multi_download_unpack_jobs(gitlab, project, outdir, fetch_jobs,
-                               callback=callback,
-                               headers=headers,
-                               export_mode=export_to)
-
-
-def find_gitlab_project_config(servers: Dict[str, str]) -> Optional[GitlabIdent]:
-    """From a list of git remotes and project servers addresses find the named gitlab config entry if any"""
-    ctx = get_user_config_context()
-    ident: Optional[GitlabIdent] = None
-    for remote in servers:
-        remote_host, remote_path = servers[remote].split("/", 1)
-        https_remote = f"https://{remote_host}"
-        for cfg in ctx.gitlab.servers:
-            if cfg.server == https_remote:
-                ident = GitlabIdent(server=remote_host, project=remote_path, secure=cfg.tls_verify)
-                break
-    return ident
-
-
-def get_gitlab_project_client(repo: str, secure=True) -> Tuple[Optional[Gitlab], Optional[Project], Optional[str]]:
-    """Get the gitlab client, project and git remote name for the given git repo"""
-    override_server = os.getenv(GITLAB_SERVER_ENV)
-    override_project = os.getenv(GITLAB_PROJECT_ENV)
-    client = None
-    project = None
-    git_remote = None
-
-    if override_server and override_project:
-        ident = GitlabIdent(server=override_server, project=override_project)
-        remotes = []
-    else:
-        # in here, we need to figure out the gitlab server by looking
-        # at the available git remotes
-        remotes = get_git_remote_urls(repo)
-        if not remotes:
-            die(f"Folder {repo} has no remotes, is it a git repo?")
-
-        possible_servers = remote_servers(remotes)
-        ident = find_gitlab_project_config(possible_servers)
-        if ident and ident.secure:
-            secure = ident.secure
-
-        if not ident:
-            note(f"Could not find a gitlab config for {repo}")
-            if len(possible_servers) == 1:
-                gitlab_host, gitlab_path = list(possible_servers.values())[0].split("/", 1)
-                git_remote = list(possible_servers.keys())[0]
-                ident = GitlabIdent(server=gitlab_host, project=gitlab_path)
-
-    if ident:
-        # we have an ident, try to find the gitlab config
-        api = gitlab_api(ident.server, secure=secure)
-        api.auth()
-        for proj in api.projects.list(membership=True, all=True):
-            if ident.project:
-                if proj.path_with_namespace == ident.project:
-                    project = proj
-                    client = api
-                    break
-
-        if remotes and project:
-            project_remotes = [project.ssh_url_to_repo, project.http_url_to_repo]
-            for remote in remotes:
-                if remotes[remote] in project_remotes:
-                    git_remote = remote
-                    break
-
-    return client, project, git_remote
-
-
-def get_current_project_client(tls_verify: Optional[bool] = True,
-                               need_remote: Optional[bool] = True) -> Tuple[Gitlab, Project, str]:
-    """Get the requested/current gitlab client, gitlab project and optional git remote"""
-    cwd = os.getcwd()
-    client, project, remotename = get_gitlab_project_client(cwd, tls_verify)
-
-    if not (client and project):
-        die("Could not find a gitlab server configuration, please add one with 'gle-config gitlab'")
-
-    if need_remote:
-        if not remotename:  # pragma: no cover
-            die("Could not find a gitlab configuration that matches any of our git remotes")
-    return client, project, remotename
+import contextlib
+import os
+import random
+import re
+import shutil
+import subprocess
+import time
+import zipfile
+import requests
+import tempfile
+import certifi
+import multiprocessing
+from functools import lru_cache
+from threading import RLock
+from typing import Optional, Tuple, Iterable, List, Any, Dict
+from urllib.parse import urlparse
+from gitlab import Gitlab, GitlabGetError
+from gitlab.v4.objects import Project
+from urllib3.exceptions import InsecureRequestWarning
+
+from .helpers import die, note, get_git_remote_urls, remote_servers
+from .userconfig import get_user_config_context
+
+GITLAB_SERVER_ENV = "GLE_GITLAB_SERVER"
+GITLAB_PROJECT_ENV = "GLE_GITLAB_PROJECT"
+SYSTEM_CA_CERTS = "/etc/ssl/certs/ca-certificates.crt"
+
+
+class GitlabIdent:
+    def __init__(self, server=None, project=None, pipeline=None, gitref=None, secure=True):
+        self.server: Optional[str] = server
+        self.project: Optional[str] = project
+        self.pipeline: Optional[int] = pipeline
+        self.gitref: Optional[str] = gitref
+        self.secure: Optional[bool] = secure
+
+    def __str__(self):  # pragma: no cover
+        attribs = []
+        if self.server:
+            attribs.append(f"server={self.server}")
+        if self.project:
+            attribs.append(f"project={self.project}")
+        if self.gitref:
+            attribs.append(f"git_ref={self.gitref}")
+        elif self.pipeline:
+            attribs.append(f"id={self.pipeline}")
+
+        return f"Pipeline {', '.join(attribs)}"
+
+
+class TaskError(Exception):
+    def __init__(self, task, inner):
+        self.task = task
+        self.inner = inner
+
+
+class PipelineError(Exception):
+    def __init__(self, pipeline: str):
+        super(PipelineError, self).__init__()
+        self.pipeline = pipeline
+
+
+class PipelineInvalid(PipelineError):
+    def __init__(self, pipeline: str):
+        super(PipelineInvalid, self).__init__(pipeline)
+
+    def __str__(self):
+        return f"'{self.pipeline}' is not a valid pipeline specification"
+
+
+class PipelineNotFound(PipelineError):
+    def __init__(self, pipeline):
+        super(PipelineNotFound, self).__init__(pipeline)
+
+    def __str__(self):
+        return f"Cannot find pipeline '{self.pipeline}'"
+
+
+def gitlab_api(alias: str, secure=True) -> Gitlab:
+    """Create a Gitlab API client"""
+    ctx = get_user_config_context()
+    server = None
+    token = None
+    for item in ctx.gitlab.servers:
+        if item.name == alias:
+            server = item.server
+            token = item.token
+            break
+
+        parsed = urlparse(item.server)
+        if parsed.hostname == alias:
+            server = item.server
+            token = item.token
+            break
+
+    if not server:
+        note(f"using {alias} as server hostname")
+        server = alias
+        if "://" not in server:
+            server = f"https://{server}"
+
+    environment_token = os.getenv("GITLAB_PRIVATE_TOKEN", None)
+    if environment_token:
+        token = environment_token
+        note("Using GITLAB_PRIVATE_TOKEN for authentication")
+
+    if not token:
+        die(f"Could not find a configured token for {alias} or GITLAB_PRIVATE_TOKEN not set")
+
+    client = Gitlab(url=server, private_token=token, ssl_verify=secure)
+    if secure:
+        gitlab_session_head(client.session, server)
+    return client
+
+
+def parse_gitlab_from_arg(arg: str, prefer_gitref: Optional[bool] = False) -> GitlabIdent:
+    """Decode an identifier into a project and optionally pipeline ID or git reference"""
+    # server/group/project/1234    = pipeline 1234 from server/group/project
+    # 1234                         = pipeline 1234 from current project
+    # server/group/project=gitref  = last successful pipeline for group/project at gitref commit/tag/branch
+    # =gitref                      = last successful pipeline at the gitref of the current project
+    gitref = None
+    project = None
+    server = None
+    pipeline = None
+    if arg.isnumeric():
+        pipeline = int(arg)
+    elif prefer_gitref:
+        gitref = arg
+        arg = ""
+    elif "=" in arg:
+        arg, gitref = arg.rsplit("=", 1)
+
+    if "/" in arg:
+        parts = arg.split("/")
+        if len(parts) > 2:
+            server = parts[0]
+            if parts[-1].isnumeric():
+                pipeline = int(parts[-1])
+                project = "/".join(parts[1:-1])
+            else:
+                project = "/".join(parts[1:])
+
+    return GitlabIdent(project=project,
+                       server=server,
+                       pipeline=pipeline,
+                       gitref=gitref)
+
+
+def find_project_pipeline(project,
+                          pipeline: Optional[int] = 0,
+                          ref: Optional[str] = None):
+    """Get a pipeline from the current project"""
+    try:
+        if pipeline:
+            return project.pipelines.get(pipeline)
+        match = {}
+        if ref:
+            match["ref"] = ref
+
+        found = project.pipelines.list(sort="desc", order_by="updated_at", page=1, pagesize=1, **match)
+        if not found:
+            raise PipelineNotFound(str(match))
+        return found[0]
+
+    except GitlabGetError as err:
+        if err.response_code == 404:
+            raise PipelineNotFound(str(pipeline))
+
+
+def get_pipeline(fromline, secure: Optional[bool] = True):
+    """Get a pipeline"""
+    pipeline = None
+    ident = parse_gitlab_from_arg(fromline)
+    if not secure:
+        note("TLS server validation disabled by --insecure")
+        requests.packages.urllib3.disable_warnings(InsecureRequestWarning)
+
+    if not ident.pipeline:
+        if not ident.gitref:
+            raise PipelineInvalid(fromline)
+
+    if not ident.server:
+        cwd = os.getcwd()
+        gitlab, project, remotename = get_gitlab_project_client(cwd, secure)
+    else:
+        gitlab = gitlab_api(ident.server, secure=secure)
+        # get project
+        project = gitlab.projects.get(ident.project)
+
+    if not project:
+        raise PipelineInvalid(fromline)
+
+    # get pipeline
+    if ident.pipeline:
+        try:
+            pipeline = find_project_pipeline(project, pipeline=ident.pipeline)
+        except GitlabGetError as err:
+            if err.response_code == 404:
+                raise PipelineNotFound(fromline)
+
+    return gitlab, project, pipeline
+
+_CA_FIXUP_LOCK = RLock()
+
+
+@lru_cache(1)
+def get_ca_bundle() -> str:
+    bundles = []
+    for env in ["REQUESTS_CA_BUNDLE", "CI_SERVER_TLS_CA_FILE"]:
+        bundle = os.getenv(env, None)
+        if bundle and os.path.isfile(bundle):
+            note(f"Using extra CAs from {env} in {bundle}")
+            bundles.append(bundle)
+    certs = certifi.contents()
+    if os.path.exists(SYSTEM_CA_CERTS) and not os.path.samefile(SYSTEM_CA_CERTS, certifi.where()):
+        with open(SYSTEM_CA_CERTS, "r") as etc_certs:
+            certs += etc_certs.read()
+    for bundle in bundles:
+        with open(bundle, "r") as data:
+            certs += data.read()
+    return certs
+
+
+@contextlib.contextmanager
+def posix_cert_fixup():
+    with _CA_FIXUP_LOCK:
+        if "GLE_CA_BUNDLE" not in os.environ:
+            with tempfile.TemporaryDirectory() as tempdir:
+                os.environ["GLE_CA_BUNDLE"] = "1"
+                certs = get_ca_bundle()
+                new_bundle = os.path.join(tempdir, "ca-certs.pem")
+                with open(new_bundle, "w") as data:
+                    data.write(certs)
+                os.environ["REQUESTS_CA_BUNDLE"] = new_bundle
+                try:
+                    yield
+                finally:
+                    del os.environ["GLE_CA_BUNDLE"]
+                    del os.environ["REQUESTS_CA_BUNDLE"]
+        else:
+            yield
+
+
+def gitlab_session_head(session, geturl, **kwargs):
+    """HEAD using requests to try different CA options"""
+    with posix_cert_fixup():
+        return session.head(geturl, **kwargs)
+
+
+def get_one_file(session: requests.Session,
+                 url: str,
+                 outfile: str,
+                 headers: Optional[dict] = None) -> str:
+    """Download one file from a gitlab url and save it"""
+    outdir = os.path.dirname(outfile)
+    partfile = outfile + ".part"
+    if os.path.exists(outfile):
+        os.unlink(outfile)
+    os.makedirs(outdir, exist_ok=True)
+    with posix_cert_fixup():
+        resp = session.get(url, headers=headers, stream=True)
+        resp.raise_for_status()
+        with open(partfile, "wb") as data:
+            shutil.copyfileobj(resp.raw, data, length=2 * 1024 * 1024)
+        shutil.move(partfile, outfile)
+    return outfile
+
+
+def unpack_one_artifact(temp_zip_file: str, outdir: str, name: str,
+                        progress: Optional[bool] = True):
+    """Extract a downloaded artifact zip"""
+    if progress:
+        note(f" Extracting artifacts from job '{name}' ..")
+    with open(temp_zip_file, "rb") as compressed:
+        with zipfile.ZipFile(compressed) as zf:
+            for item in zf.infolist():
+                savefile = os.path.join(outdir, item.filename)
+                if os.path.exists(savefile):
+                    note(f"  Warning: File {savefile} already exists, overwriting..")
+                if progress:
+                    note(f"  Saving {savefile} ..")
+                zf.extract(item, path=outdir)
+
+
+def multi_download_unpack_jobs(gitlab: Gitlab,
+                               project,
+                               outdir: str,
+                               jobs,
+                               callback: Optional[List[str]] = None,
+                               headers: Optional[dict] = None,
+                               export_mode: Optional[bool] = False):
+    """Download and unpack multiple jobs, use parallelization if in export mode"""
+    workers = (multiprocessing.cpu_count() - 1)
+    if workers < 1:
+        workers = 1
+    if workers > 4:  # pragma: no cover
+        workers = 4
+    if not export_mode:
+        workers = 1
+    downloads = []
+
+    for fetch_job in jobs:
+        artifact_url = None
+        if [x for x in fetch_job.artifacts if x["file_type"] == "archive"]:
+            artifact_url = f"{gitlab.api_url}/projects/{project.id}/jobs/{fetch_job.id}/artifacts"
+        jobdir = outdir
+        trace_url = None
+
+        if export_mode:
+            jobdir = os.path.join(outdir, sanitize_pathname(fetch_job.name))
+            trace_url = f"{gitlab.api_url}/projects/{project.id}/jobs/{fetch_job.id}/trace"
+
+        downloads.append([fetch_job.name,
+                          gitlab.session.auth,
+                          callback,
+                          jobdir,
+                          artifact_url,
+                          trace_url,
+                          headers])
+    if export_mode:
+        random.shuffle(downloads)
+    progress = 0
+    sizes = []
+    times = []
+    started = time.monotonic()
+    with multiprocessing.Pool(processes=workers) as pool:
+        manager = multiprocessing.Manager()
+        printmutex = manager.Lock()
+        execmutex = manager.Lock()
+        tasks = []
+        for args in downloads:
+            args.append(printmutex)
+            args.append(execmutex)
+            task = pool.apply_async(downloader, args)
+            tasks.append(task)
+
+        for task in tasks:
+            try:
+                size, download_time = task.get()
+            except TaskError as err:  # pragma: no cover
+                note(f"error! failed downloading {err.task}!")
+                raise err.inner
+            sizes.append(size)
+            times.append(download_time)
+            progress += 1
+            sofar = time.monotonic() - started
+            note(f"{int(sofar):-4}s {progress:-3}/{len(downloads):-3} completed.")
+    finished = time.monotonic()
+    duration = finished - started
+    total_downloaded = sum(sizes)
+    total_times = sum(times)
+    combined_rate = total_downloaded / total_times
+    note(f"All downloads complete: {int(total_downloaded/1024/1024)} mb.")
+    note(f"Combined transfer rate: {int(combined_rate/1024/1024)} mb/s.")
+    note(f"Total time {int(duration)} sec.")
+
+
+def sanitize_pathname(path: str) -> str:
+    path = path.lower()
+    path = re.sub(r"[^a-z\d\-.]", "_", path)
+    return path
+
+
+def downloader(jobname: str,
+               auth: Optional[Any],
+               cbscript: Optional[List[str]],
+               outdir: str,
+               archive_url: Optional[str],
+               trace_url: Optional[str],
+               hdrs: Optional[dict],
+               printlock: multiprocessing.Lock,
+               execlock: multiprocessing.Lock):
+    try:
+        session = requests.Session()
+        session.auth = auth
+        started = time.monotonic()
+        size = 0
+        archive_file = None
+        if archive_url:
+            archive_file = os.path.join(outdir, "archive.zip")
+            get_one_file(session, archive_url, archive_file, hdrs)
+            size += os.path.getsize(archive_file)
+        if trace_url:
+            trace_file = os.path.join(outdir, "trace.log")
+            get_one_file(session, trace_url, trace_file, hdrs)
+            size += os.path.getsize(trace_file)
+        ended = time.monotonic()
+        duration = ended - started
+        rate = size / duration
+
+        with printlock:
+            note(f"Fetching job {jobname} .. done {int(size / 1024)} kb, {int(rate / 1024)} kb/s")
+        if archive_file:
+            with printlock:
+                note(f"Unpack job {jobname} archive into {outdir} ..")
+            unpack_one_artifact(archive_file, outdir, jobname, progress=False)
+            os.unlink(archive_file)
+            with printlock:
+                note(f"Unpack job {jobname} archive into {outdir} .. done")
+        if cbscript:
+            args = []
+            for x in cbscript:
+                args.append(x.replace("%p", outdir))
+            with execlock:
+                with printlock:
+                    note(f"Executing '{' '.join(args)}'..")
+                subprocess.check_call(args, shell=False)
+        return size, duration
+    except Exception as err:
+        raise TaskError(jobname, err)
+
+
+def do_gitlab_fetch(from_pipeline: str,
+                    get_jobs: Iterable[str],
+                    download_to: Optional[str] = None,
+                    export_to: Optional[str] = False,
+                    callback: Optional[List[str]] = None,
+                    tls_verify: Optional[bool] = True):
+    """Fetch builds and logs from gitlab"""
+    gitlab, project, pipeline = get_pipeline(from_pipeline, secure=tls_verify)
+    gitlab.session.verify = tls_verify  # hmm ?
+    pipeline_jobs = pipeline.jobs.list(all=True)
+    known_jobs = [x.name for x in pipeline_jobs]
+    want_jobs = []
+    if get_jobs:
+        want_jobs = list(get_jobs)
+        for item in list(want_jobs):
+            if item not in known_jobs:  # pragma: no cover
+                # most of the time this is not reached, but if a pipeline has job
+                # rules then the server may not have made a job for this even if it
+                # existed in the yaml. This is quite hard to test without mocking more of the server
+                errmsg = f"Pipeline {pipeline.id} does not contain a job named '{item}'"
+                similar = [name for name in known_jobs if "/" in name and name.startswith(item)]
+                if not similar:
+                    die(errmsg)
+                want_jobs.extend(similar)
+            else:
+                want_jobs.append(item)
+
+    fetch_jobs = [x for x in pipeline_jobs if not want_jobs or x.name in want_jobs]
+
+    assert export_to or download_to
+    outdir = download_to
+    if export_to:
+        mode = "Exporting"
+        outdir = export_to
+    else:
+        mode = "Fetching"
+    note(f"{mode} {len(fetch_jobs)} jobs from {pipeline.web_url}..")
+    headers = {}
+    if gitlab.private_token:
+        headers = {"PRIVATE-TOKEN": gitlab.private_token}
+    multi_download_unpack_jobs(gitlab, project, outdir, fetch_jobs,
+                               callback=callback,
+                               headers=headers,
+                               export_mode=export_to)
+
+
+def find_gitlab_project_config(servers: Dict[str, str]) -> Optional[GitlabIdent]:
+    """From a list of git remotes and project servers addresses find the named gitlab config entry if any"""
+    ctx = get_user_config_context()
+    ident: Optional[GitlabIdent] = None
+    for remote in servers:
+        remote_host, remote_path = servers[remote].split("/", 1)
+        https_remote = f"https://{remote_host}"
+        for cfg in ctx.gitlab.servers:
+            if cfg.server == https_remote:
+                ident = GitlabIdent(server=remote_host, project=remote_path, secure=cfg.tls_verify)
+                break
+    return ident
+
+
+def get_gitlab_project_client(repo: str, secure=True) -> Tuple[Optional[Gitlab], Optional[Project], Optional[str]]:
+    """Get the gitlab client, project and git remote name for the given git repo"""
+    override_server = os.getenv(GITLAB_SERVER_ENV)
+    override_project = os.getenv(GITLAB_PROJECT_ENV)
+    client = None
+    project = None
+    git_remote = None
+
+    if override_server and override_project:
+        ident = GitlabIdent(server=override_server, project=override_project)
+        remotes = []
+    else:
+        # in here, we need to figure out the gitlab server by looking
+        # at the available git remotes
+        remotes = get_git_remote_urls(repo)
+        if not remotes:
+            die(f"Folder {repo} has no remotes, is it a git repo?")
+
+        possible_servers = remote_servers(remotes)
+        ident = find_gitlab_project_config(possible_servers)
+        if ident and ident.secure:
+            secure = ident.secure
+
+        if not ident:
+            note(f"Could not find a gitlab config for {repo}")
+            if len(possible_servers) == 1:
+                gitlab_host, gitlab_path = list(possible_servers.values())[0].split("/", 1)
+                git_remote = list(possible_servers.keys())[0]
+                ident = GitlabIdent(server=gitlab_host, project=gitlab_path)
+
+    if ident:
+        # we have an ident, try to find the gitlab config
+        api = gitlab_api(ident.server, secure=secure)
+        api.auth()
+        for proj in api.projects.list(membership=True, all=True):
+            if ident.project:
+                if proj.path_with_namespace == ident.project:
+                    project = proj
+                    client = api
+                    break
+
+        if remotes and project:
+            project_remotes = [project.ssh_url_to_repo, project.http_url_to_repo]
+            for remote in remotes:
+                if remotes[remote] in project_remotes:
+                    git_remote = remote
+                    break
+
+    return client, project, git_remote
+
+
+def get_current_project_client(tls_verify: Optional[bool] = True,
+                               need_remote: Optional[bool] = True) -> Tuple[Gitlab, Project, str]:
+    """Get the requested/current gitlab client, gitlab project and optional git remote"""
+    cwd = os.getcwd()
+    client, project, remotename = get_gitlab_project_client(cwd, tls_verify)
+
+    if not (client and project):
+        die("Could not find a gitlab server configuration, please add one with 'gle-config gitlab'")
+
+    if need_remote:
+        if not remotename:  # pragma: no cover
+            die("Could not find a gitlab configuration that matches any of our git remotes")
+    return client, project, remotename
```

## gitlabemu/helpers.py

 * *Ordering differences only*

```diff
@@ -1,467 +1,467 @@
-"""
-Various useful common funcs
-"""
-import os.path
-from select import select
-from threading import Thread
-import sys
-import re
-import platform
-import subprocess
-from typing import Optional, List, Dict, Union, Tuple
-from urllib.parse import urlparse
-
-from .errors import DockerExecError
-from .resnamer import resource_owner_alive, is_gle_resource
-from .logmsg import info, debug
-
-
-class ProcessLineProxyThread(Thread):
-    def __init__(self, process, stdout, linehandler=None):
-        super(ProcessLineProxyThread, self).__init__()
-        self.errors = []
-        self.process = process
-        self.stdout = stdout
-        self.linehandler = linehandler
-        self.daemon = True
-
-    def writeout(self, data):
-        if self.stdout and data:
-            encoding = "ascii"
-            if hasattr(self.stdout, "encoding"):
-                encoding = self.stdout.encoding
-            try:
-                decoded = data.decode(encoding, "namereplace")
-                self.stdout.write(decoded)
-            except (TypeError, UnicodeError):
-                # codec cant handle namereplace or the codec cant represent this.
-                # decode it to utf-8 and replace non-printable ascii with
-                # chars with '?'
-                decoded = data.decode("utf-8", "replace")
-                text = re.sub(r'[^\x00-\x7F]', "?", decoded)
-                self.stdout.write(text)
-
-            if self.linehandler:
-                try:
-                    self.linehandler(data)
-                except DockerExecError as err:
-                    self.errors.append(err)
-
-    def run(self):
-        """Pump stdout Wait for a job to end """
-        # do nothing for interactive jobs
-        while self.process.stdout is not None:
-            data = None
-            if not is_windows():
-                select([self.process.stdout], [], [], 1)
-            try:
-                data = self.process.stdout.readline()
-            except ValueError:  # pragma: no cover
-                pass
-            except Exception as err:  # pragma: no cover
-                self.errors.append(err)
-                raise
-            finally:
-                if data:
-                    self.writeout(data)
-            if self.process.poll() is not None:
-                break
-
-        if hasattr(self.stdout, "flush"):
-            self.stdout.flush()
-
-
-def communicate(process, stdout=sys.stdout, script=None, throw=False, linehandler=None):
-    """
-    Write output incrementally to stdout, waits for process to end
-    :param process: a Popened child process
-    :param stdout: a file-like object to write to
-    :param script: a script (ie, bytes) to stream to stdin
-    :param throw: raise an exception if the process exits non-zero
-    :param linehandler: if set, pass the line to this callable
-    :return:
-    """
-    if process.stdout is None:  # pragma: no cover
-        # interactive job, just wait
-        process.wait()
-        return
-
-    data = None
-    if script is not None:
-        process.stdin.write(script)
-        process.stdin.flush()
-        process.stdin.close()
-
-    comm_thread = ProcessLineProxyThread(process, stdout, linehandler=linehandler)
-    thread_started = False
-    try:
-        comm_thread.start()
-        thread_started = True
-    except RuntimeError:  # pragma: no cover
-        # could not create the thread, so use a loop
-        pass
-
-    # use a thread to stream build output if we can (hpux can't)
-    if comm_thread and thread_started:
-        while process.poll() is None:
-            if comm_thread.is_alive():
-                comm_thread.join(timeout=5)
-
-        if comm_thread.is_alive():
-            comm_thread.join()
-
-    # either the task has ended or we could not create a thread, either way,
-    # stream the remaining stdout data
-    while True:
-        try:
-            if process.stdout is not None:
-                data = process.stdout.readline()
-        except ValueError:  # pragma: no cover
-            pass
-        if data:
-            # we can still use our proxy object to decode and write the data
-            comm_thread.writeout(data)
-
-        if process.poll() is not None:
-            break
-
-    # process has definitely already ended, read all the lines, this wont deadlock
-    while True:
-        line = None
-        if process.stdout is not None:
-            line = process.stdout.readline()
-        if line:
-            comm_thread.writeout(line)
-        else:
-            break
-
-    if throw:
-        if process.returncode != 0:
-            args = []
-            if hasattr(process, "args"):
-                args = process.args
-            raise subprocess.CalledProcessError(process.returncode, cmd=args)
-
-    if comm_thread:
-        for err in comm_thread.errors:  # pragma: cover if windows
-            if isinstance(err, DockerExecError) or throw:
-                raise err
-
-
-def is_windows():
-    return platform.system() == "Windows"
-
-
-def is_linux():
-    return platform.system() == "Linux"
-
-
-def is_apple():
-    return platform.system() == "Darwin"
-
-
-def parse_timeout(text):
-    """
-    Decode a human-readable time to seconds.
-    eg, 1h 30m
-
-    default is minutes without any suffix
-    """
-    if isinstance(text, int):
-        text = str(text)
-    # collapse the long form
-    text = text.replace(" hours", "h")
-    text = text.replace(" minutes", "m")
-
-    words = text.split()
-    seconds = 0
-
-    if len(words) == 1:
-        # plain single time
-        word = words[0]
-        try:
-            mins = float(word)
-            # plain bare number, use it as minutes
-            return int(60.0 * mins)
-        except ValueError:
-            pass
-
-    pattern = re.compile(r"([\d\.]+)\s*([hm])")
-
-    for word in words:
-        m = pattern.search(word)
-        if m and m.groups():
-            num, suffix = m.groups()
-            num = float(num)
-            if suffix == "h":
-                if seconds > 0:
-                    raise ValueError("Unexpected h value {}".format(text))
-                seconds += num * 60 * 60
-            elif suffix == "m":
-                seconds += num * 60
-
-    if seconds == 0:
-        raise ValueError("Cannot decode timeout {}".format(text))
-    return seconds
-
-
-def git_worktree(path: str) -> Optional[str]:
-    """
-    If the given path contains a git worktree, return the path to it
-    :param path:
-    :return:
-    """
-    gitpath = os.path.join(path, ".git")
-
-    if os.path.isfile(gitpath):  # pragma: no cover
-        # this is an odd case where you have .git files instead of folders
-        with open(gitpath, "r") as fd:
-            full = fd.read()
-            for line in full.splitlines():
-                name, value = line.split(":", 1)
-                if name == "gitdir":
-                    value = value.strip()
-                    realpath = value
-                    # keep going upwards until we find a .git folder
-                    for _ in value.split(os.sep):
-                        realpath = os.path.dirname(realpath)
-                        gitdir = os.path.join(realpath, ".git")
-                        if os.path.isdir(gitdir):
-                            return gitdir
-    return None
-
-
-def make_path_slug(text: str) -> str:
-    """Convert a string into one suitable for a folder basename"""
-    return re.sub(r"[^a-zA-Z0-9\-.]", "_", text)
-
-
-def clean_leftovers():
-    """Clean up any unused leftover docker containers or networks"""
-    from .docker import DockerTool, DockerToolFailed
-    tool = DockerTool()
-    for container in tool.containers:  # pragma: no cover
-        name = tool.container_name(container)
-        pid = is_gle_resource(name)
-        if pid is not None:
-            if not resource_owner_alive(name):
-                # kill this container
-                info(f"Killing leftover docker container: {name}")
-                tool.docker_call("kill", container)
-    try:
-        tool.docker_call("network", "rm", "gle-service-network")
-    except DockerToolFailed:
-        pass
-
-
-class DockerVolume:
-    def __init__(self, host, mount, mode):
-        if host != "/":
-            host = host.rstrip(os.sep)
-        self.host = host
-        self.mount = mount.rstrip(os.sep)
-        assert mode in ["rw", "ro"]
-        self.mode = mode
-
-    def __str__(self):
-        return f"{self.host}:{self.mount}:{self.mode}"
-
-
-def plausible_docker_volume(text: str) -> Optional[DockerVolume]:
-    """Decode a docker volume string or return None"""
-    mode = "rw"
-    parts = text.split(":")
-    src = None
-    mount = None
-    if len(parts) >= 4:
-        import ntpath
-        # c:\thing:c:\container
-        # c:\thing:c:\container[:mode]
-        if len(parts) == 5:
-            # has mode
-            mode = parts[-1]
-        src = ntpath.abspath(f"{parts[0]}:{parts[1]}")
-        mount = ntpath.abspath(f"{parts[2]}:{parts[3]}")
-    else:
-        if len(parts) >= 2:
-            import posixpath
-            # /host/path:/mount/path[:mode]
-            src = posixpath.abspath(parts[0])
-            mount = posixpath.abspath(parts[1])
-            if len(parts) == 3:
-                mode = parts[2]
-    if not src:
-        return None
-    return DockerVolume(src, mount, mode)
-
-
-def sensitive_varname(name) -> bool:
-    """Return True if the variable might be a sensitive/secret one"""
-    for check in ["PASSWORD", "TOKEN", "PRIVATE"]:
-        if check in name:
-            return True
-    return False
-
-
-def trim_quotes(text: str) -> str:
-    """If the string is wrapped in quotes, strip them off"""
-    if text:
-        if text[0] in ["'", "\""]:
-            if text[0] == text[-1]:
-                text = text[1:-1]
-    return text
-
-
-def powershell_escape(text: str, variables=False) -> str:  # pragma: cover if windows
-    # taken from: http://www.robvanderwoude.com/escapechars.php
-    text = text.replace("`", "``")
-    text = text.replace("\a", "`a")
-    text = text.replace("\b", "`b")
-    text = text.replace("\f", "^f")
-    text = text.replace("\r", "`r")
-    text = text.replace("\n", "`n")
-    text = text.replace("\t", "^t")
-    text = text.replace("\v", "^v")
-    text = text.replace("#", "`#")
-    text = text.replace("'", "`'")
-    text = text.replace("\"", "`\"")
-    text = f"\"{text}\""
-    if variables:
-        text = text.replace("$", "`$")
-        text = text.replace("``e", "`e")
-    return text
-
-
-def die(msg):
-    """print an error and exit"""
-    print("error: " + str(msg), file=sys.stderr)
-    sys.exit(1)
-
-
-def note(msg):
-    """Print to stderr"""
-    print(msg, file=sys.stderr, flush=True)
-
-
-def git_uncommitted_changes(path: str) -> bool:
-    """Return True if the given repo has uncommitted changes to tracked files"""
-    topdir = git_top_level(path)
-    output = subprocess.check_output(
-        ["git", "-C", topdir, "status", "--porcelain", "--untracked=no"],
-        encoding="utf-8", stderr=subprocess.DEVNULL).strip()
-    for _ in output.splitlines(keepends=False):
-        return True
-    return False
-
-
-def git_current_branch(path: str) -> str:
-    """Get the current branch"""
-    return subprocess.check_output(
-        ["git", "-C", path, "rev-parse", "--abbrev-ref", "HEAD"], encoding="utf-8", stderr=subprocess.DEVNULL
-    ).strip()
-
-
-def git_commit_sha(path: str) -> str:
-    """Get the current commit hash"""
-    return subprocess.check_output(
-        ["git", "-C", path, "rev-parse", "HEAD"], encoding="utf-8", stderr=subprocess.DEVNULL
-    ).strip()
-
-
-def git_remotes(path: str) -> List[str]:
-    """Get the remote names of the given git repo"""
-    try:
-        output = subprocess.check_output(
-            ["git", "-C", path, "remote"], encoding="utf-8", stderr=subprocess.DEVNULL)
-        return list(output.splitlines(keepends=False))
-    except subprocess.CalledProcessError:
-        return []
-
-
-def git_remote_url(path: str, remote: str) -> str:
-    """Get the URL of the given git remote"""
-    return subprocess.check_output(
-        ["git", "-C", path, "remote", "get-url", remote], encoding="utf-8", stderr=subprocess.DEVNULL).strip()
-
-
-def get_git_remote_urls(repo: str) -> Dict[str, str]:
-    """Return all the git remotes defined in the given git repo"""
-    remotes = git_remotes(repo)
-    urls = {}
-    for item in remotes:
-        urls[item] = git_remote_url(repo, item)
-    return urls
-
-
-def git_top_level(repo: str) -> str:
-    """Get the top folder of the git repo"""
-    return subprocess.check_output(
-        ["git", "-C", repo, "rev-parse", "--show-toplevel"], encoding="utf-8", stderr=subprocess.DEVNULL).strip()
-
-
-def git_push_force_upstream(repo: str, remote: str, branch: str):  # pragma: no cover
-    subprocess.check_call(["git", "-C", repo, "push", "--force", "-q", "--set-upstream", remote, branch])
-
-
-def stringlist_if_string(value: Union[str, list]) -> list:
-    """If value is a string, return a one element list, else return value"""
-    if isinstance(value, str):
-        return [value]
-    return value
-
-
-def remote_servers(remotes: Dict[str, str]) -> Dict[str, str]:
-    """From a map of git remotes, Get a map of git remotes to server addresses"""
-    servers: Dict[str, str] = {}
-    for remote_name in remotes:
-        remote_url = remotes[remote_name]
-        if remote_url.startswith("git@") and remote_url.endswith(".git"):
-            if ":" in remote_url:
-                lhs, rhs = remote_url.split(":", 1)
-                host = lhs.split("@", 1)[1]
-                project_path = rhs.rsplit(".", 1)[0].lstrip("/")
-                servers[remote_name] = f"{host}/{project_path}"
-        elif "://" in remote_url and remote_url.startswith("http"):
-            parsed = urlparse(remote_url)
-            host = parsed.hostname
-            project_path = parsed.path.rsplit(".", 1)[0].lstrip("/")
-            servers[remote_name] = f"{host}/{project_path}"
-    return servers
-
-
-def has_docker() -> bool:
-    """
-    Return True if this system can run docker containers
-    :return:
-    """
-    # noinspection PyBroadException
-    try:
-        subprocess.check_output(["docker", "info"], stderr=subprocess.STDOUT)
-        debug("docker detected")
-        return True
-    except Exception as err:  # pragma: no cover
-        pass
-    return False  # pragma: no cover
-
-
-def warning(text: str) -> None:
-    print(f"warning: {text}", file=sys.stderr, flush=True)
-
-
-def notice(text: str) -> None:
-    print(f"notice: {text}", file=sys.stderr, flush=True)
-
-
-def truth_string(text: str) -> bool:
-    if text:
-        text = text.lower()
-        if text in ["y", "yes", "true", "on", "1"]:
-            return True
-    return False
-
-
-def setenv_string(text: str) -> Tuple[str, str]:
-    parts = text.split("=", 1)
-    if len(parts) == 2:
-        return parts[0], parts[1]
-    raise ValueError(f"{text} is not in the form NAME=VALUE")
+"""
+Various useful common funcs
+"""
+import os.path
+from select import select
+from threading import Thread
+import sys
+import re
+import platform
+import subprocess
+from typing import Optional, List, Dict, Union, Tuple
+from urllib.parse import urlparse
+
+from .errors import DockerExecError
+from .resnamer import resource_owner_alive, is_gle_resource
+from .logmsg import info, debug
+
+
+class ProcessLineProxyThread(Thread):
+    def __init__(self, process, stdout, linehandler=None):
+        super(ProcessLineProxyThread, self).__init__()
+        self.errors = []
+        self.process = process
+        self.stdout = stdout
+        self.linehandler = linehandler
+        self.daemon = True
+
+    def writeout(self, data):
+        if self.stdout and data:
+            encoding = "ascii"
+            if hasattr(self.stdout, "encoding"):
+                encoding = self.stdout.encoding
+            try:
+                decoded = data.decode(encoding, "namereplace")
+                self.stdout.write(decoded)
+            except (TypeError, UnicodeError):
+                # codec cant handle namereplace or the codec cant represent this.
+                # decode it to utf-8 and replace non-printable ascii with
+                # chars with '?'
+                decoded = data.decode("utf-8", "replace")
+                text = re.sub(r'[^\x00-\x7F]', "?", decoded)
+                self.stdout.write(text)
+
+            if self.linehandler:
+                try:
+                    self.linehandler(data)
+                except DockerExecError as err:
+                    self.errors.append(err)
+
+    def run(self):
+        """Pump stdout Wait for a job to end """
+        # do nothing for interactive jobs
+        while self.process.stdout is not None:
+            data = None
+            if not is_windows():
+                select([self.process.stdout], [], [], 1)
+            try:
+                data = self.process.stdout.readline()
+            except ValueError:  # pragma: no cover
+                pass
+            except Exception as err:  # pragma: no cover
+                self.errors.append(err)
+                raise
+            finally:
+                if data:
+                    self.writeout(data)
+            if self.process.poll() is not None:
+                break
+
+        if hasattr(self.stdout, "flush"):
+            self.stdout.flush()
+
+
+def communicate(process, stdout=sys.stdout, script=None, throw=False, linehandler=None):
+    """
+    Write output incrementally to stdout, waits for process to end
+    :param process: a Popened child process
+    :param stdout: a file-like object to write to
+    :param script: a script (ie, bytes) to stream to stdin
+    :param throw: raise an exception if the process exits non-zero
+    :param linehandler: if set, pass the line to this callable
+    :return:
+    """
+    if process.stdout is None:  # pragma: no cover
+        # interactive job, just wait
+        process.wait()
+        return
+
+    data = None
+    if script is not None:
+        process.stdin.write(script)
+        process.stdin.flush()
+        process.stdin.close()
+
+    comm_thread = ProcessLineProxyThread(process, stdout, linehandler=linehandler)
+    thread_started = False
+    try:
+        comm_thread.start()
+        thread_started = True
+    except RuntimeError:  # pragma: no cover
+        # could not create the thread, so use a loop
+        pass
+
+    # use a thread to stream build output if we can (hpux can't)
+    if comm_thread and thread_started:
+        while process.poll() is None:
+            if comm_thread.is_alive():
+                comm_thread.join(timeout=5)
+
+        if comm_thread.is_alive():
+            comm_thread.join()
+
+    # either the task has ended or we could not create a thread, either way,
+    # stream the remaining stdout data
+    while True:
+        try:
+            if process.stdout is not None:
+                data = process.stdout.readline()
+        except ValueError:  # pragma: no cover
+            pass
+        if data:
+            # we can still use our proxy object to decode and write the data
+            comm_thread.writeout(data)
+
+        if process.poll() is not None:
+            break
+
+    # process has definitely already ended, read all the lines, this wont deadlock
+    while True:
+        line = None
+        if process.stdout is not None:
+            line = process.stdout.readline()
+        if line:
+            comm_thread.writeout(line)
+        else:
+            break
+
+    if throw:
+        if process.returncode != 0:
+            args = []
+            if hasattr(process, "args"):
+                args = process.args
+            raise subprocess.CalledProcessError(process.returncode, cmd=args)
+
+    if comm_thread:
+        for err in comm_thread.errors:  # pragma: cover if windows
+            if isinstance(err, DockerExecError) or throw:
+                raise err
+
+
+def is_windows():
+    return platform.system() == "Windows"
+
+
+def is_linux():
+    return platform.system() == "Linux"
+
+
+def is_apple():
+    return platform.system() == "Darwin"
+
+
+def parse_timeout(text):
+    """
+    Decode a human-readable time to seconds.
+    eg, 1h 30m
+
+    default is minutes without any suffix
+    """
+    if isinstance(text, int):
+        text = str(text)
+    # collapse the long form
+    text = text.replace(" hours", "h")
+    text = text.replace(" minutes", "m")
+
+    words = text.split()
+    seconds = 0
+
+    if len(words) == 1:
+        # plain single time
+        word = words[0]
+        try:
+            mins = float(word)
+            # plain bare number, use it as minutes
+            return int(60.0 * mins)
+        except ValueError:
+            pass
+
+    pattern = re.compile(r"([\d\.]+)\s*([hm])")
+
+    for word in words:
+        m = pattern.search(word)
+        if m and m.groups():
+            num, suffix = m.groups()
+            num = float(num)
+            if suffix == "h":
+                if seconds > 0:
+                    raise ValueError("Unexpected h value {}".format(text))
+                seconds += num * 60 * 60
+            elif suffix == "m":
+                seconds += num * 60
+
+    if seconds == 0:
+        raise ValueError("Cannot decode timeout {}".format(text))
+    return seconds
+
+
+def git_worktree(path: str) -> Optional[str]:
+    """
+    If the given path contains a git worktree, return the path to it
+    :param path:
+    :return:
+    """
+    gitpath = os.path.join(path, ".git")
+
+    if os.path.isfile(gitpath):  # pragma: no cover
+        # this is an odd case where you have .git files instead of folders
+        with open(gitpath, "r") as fd:
+            full = fd.read()
+            for line in full.splitlines():
+                name, value = line.split(":", 1)
+                if name == "gitdir":
+                    value = value.strip()
+                    realpath = value
+                    # keep going upwards until we find a .git folder
+                    for _ in value.split(os.sep):
+                        realpath = os.path.dirname(realpath)
+                        gitdir = os.path.join(realpath, ".git")
+                        if os.path.isdir(gitdir):
+                            return gitdir
+    return None
+
+
+def make_path_slug(text: str) -> str:
+    """Convert a string into one suitable for a folder basename"""
+    return re.sub(r"[^a-zA-Z0-9\-.]", "_", text)
+
+
+def clean_leftovers():
+    """Clean up any unused leftover docker containers or networks"""
+    from .docker import DockerTool, DockerToolFailed
+    tool = DockerTool()
+    for container in tool.containers:  # pragma: no cover
+        name = tool.container_name(container)
+        pid = is_gle_resource(name)
+        if pid is not None:
+            if not resource_owner_alive(name):
+                # kill this container
+                info(f"Killing leftover docker container: {name}")
+                tool.docker_call("kill", container)
+    try:
+        tool.docker_call("network", "rm", "gle-service-network")
+    except DockerToolFailed:
+        pass
+
+
+class DockerVolume:
+    def __init__(self, host, mount, mode):
+        if host != "/":
+            host = host.rstrip(os.sep)
+        self.host = host
+        self.mount = mount.rstrip(os.sep)
+        assert mode in ["rw", "ro"]
+        self.mode = mode
+
+    def __str__(self):
+        return f"{self.host}:{self.mount}:{self.mode}"
+
+
+def plausible_docker_volume(text: str) -> Optional[DockerVolume]:
+    """Decode a docker volume string or return None"""
+    mode = "rw"
+    parts = text.split(":")
+    src = None
+    mount = None
+    if len(parts) >= 4:
+        import ntpath
+        # c:\thing:c:\container
+        # c:\thing:c:\container[:mode]
+        if len(parts) == 5:
+            # has mode
+            mode = parts[-1]
+        src = ntpath.abspath(f"{parts[0]}:{parts[1]}")
+        mount = ntpath.abspath(f"{parts[2]}:{parts[3]}")
+    else:
+        if len(parts) >= 2:
+            import posixpath
+            # /host/path:/mount/path[:mode]
+            src = posixpath.abspath(parts[0])
+            mount = posixpath.abspath(parts[1])
+            if len(parts) == 3:
+                mode = parts[2]
+    if not src:
+        return None
+    return DockerVolume(src, mount, mode)
+
+
+def sensitive_varname(name) -> bool:
+    """Return True if the variable might be a sensitive/secret one"""
+    for check in ["PASSWORD", "TOKEN", "PRIVATE"]:
+        if check in name:
+            return True
+    return False
+
+
+def trim_quotes(text: str) -> str:
+    """If the string is wrapped in quotes, strip them off"""
+    if text:
+        if text[0] in ["'", "\""]:
+            if text[0] == text[-1]:
+                text = text[1:-1]
+    return text
+
+
+def powershell_escape(text: str, variables=False) -> str:  # pragma: cover if windows
+    # taken from: http://www.robvanderwoude.com/escapechars.php
+    text = text.replace("`", "``")
+    text = text.replace("\a", "`a")
+    text = text.replace("\b", "`b")
+    text = text.replace("\f", "^f")
+    text = text.replace("\r", "`r")
+    text = text.replace("\n", "`n")
+    text = text.replace("\t", "^t")
+    text = text.replace("\v", "^v")
+    text = text.replace("#", "`#")
+    text = text.replace("'", "`'")
+    text = text.replace("\"", "`\"")
+    text = f"\"{text}\""
+    if variables:
+        text = text.replace("$", "`$")
+        text = text.replace("``e", "`e")
+    return text
+
+
+def die(msg):
+    """print an error and exit"""
+    print("error: " + str(msg), file=sys.stderr)
+    sys.exit(1)
+
+
+def note(msg):
+    """Print to stderr"""
+    print(msg, file=sys.stderr, flush=True)
+
+
+def git_uncommitted_changes(path: str) -> bool:
+    """Return True if the given repo has uncommitted changes to tracked files"""
+    topdir = git_top_level(path)
+    output = subprocess.check_output(
+        ["git", "-C", topdir, "status", "--porcelain", "--untracked=no"],
+        encoding="utf-8", stderr=subprocess.DEVNULL).strip()
+    for _ in output.splitlines(keepends=False):
+        return True
+    return False
+
+
+def git_current_branch(path: str) -> str:
+    """Get the current branch"""
+    return subprocess.check_output(
+        ["git", "-C", path, "rev-parse", "--abbrev-ref", "HEAD"], encoding="utf-8", stderr=subprocess.DEVNULL
+    ).strip()
+
+
+def git_commit_sha(path: str) -> str:
+    """Get the current commit hash"""
+    return subprocess.check_output(
+        ["git", "-C", path, "rev-parse", "HEAD"], encoding="utf-8", stderr=subprocess.DEVNULL
+    ).strip()
+
+
+def git_remotes(path: str) -> List[str]:
+    """Get the remote names of the given git repo"""
+    try:
+        output = subprocess.check_output(
+            ["git", "-C", path, "remote"], encoding="utf-8", stderr=subprocess.DEVNULL)
+        return list(output.splitlines(keepends=False))
+    except subprocess.CalledProcessError:
+        return []
+
+
+def git_remote_url(path: str, remote: str) -> str:
+    """Get the URL of the given git remote"""
+    return subprocess.check_output(
+        ["git", "-C", path, "remote", "get-url", remote], encoding="utf-8", stderr=subprocess.DEVNULL).strip()
+
+
+def get_git_remote_urls(repo: str) -> Dict[str, str]:
+    """Return all the git remotes defined in the given git repo"""
+    remotes = git_remotes(repo)
+    urls = {}
+    for item in remotes:
+        urls[item] = git_remote_url(repo, item)
+    return urls
+
+
+def git_top_level(repo: str) -> str:
+    """Get the top folder of the git repo"""
+    return subprocess.check_output(
+        ["git", "-C", repo, "rev-parse", "--show-toplevel"], encoding="utf-8", stderr=subprocess.DEVNULL).strip()
+
+
+def git_push_force_upstream(repo: str, remote: str, branch: str):  # pragma: no cover
+    subprocess.check_call(["git", "-C", repo, "push", "--force", "-q", "--set-upstream", remote, branch])
+
+
+def stringlist_if_string(value: Union[str, list]) -> list:
+    """If value is a string, return a one element list, else return value"""
+    if isinstance(value, str):
+        return [value]
+    return value
+
+
+def remote_servers(remotes: Dict[str, str]) -> Dict[str, str]:
+    """From a map of git remotes, Get a map of git remotes to server addresses"""
+    servers: Dict[str, str] = {}
+    for remote_name in remotes:
+        remote_url = remotes[remote_name]
+        if remote_url.startswith("git@") and remote_url.endswith(".git"):
+            if ":" in remote_url:
+                lhs, rhs = remote_url.split(":", 1)
+                host = lhs.split("@", 1)[1]
+                project_path = rhs.rsplit(".", 1)[0].lstrip("/")
+                servers[remote_name] = f"{host}/{project_path}"
+        elif "://" in remote_url and remote_url.startswith("http"):
+            parsed = urlparse(remote_url)
+            host = parsed.hostname
+            project_path = parsed.path.rsplit(".", 1)[0].lstrip("/")
+            servers[remote_name] = f"{host}/{project_path}"
+    return servers
+
+
+def has_docker() -> bool:
+    """
+    Return True if this system can run docker containers
+    :return:
+    """
+    # noinspection PyBroadException
+    try:
+        subprocess.check_output(["docker", "info"], stderr=subprocess.STDOUT)
+        debug("docker detected")
+        return True
+    except Exception as err:  # pragma: no cover
+        pass
+    return False  # pragma: no cover
+
+
+def warning(text: str) -> None:
+    print(f"warning: {text}", file=sys.stderr, flush=True)
+
+
+def notice(text: str) -> None:
+    print(f"notice: {text}", file=sys.stderr, flush=True)
+
+
+def truth_string(text: str) -> bool:
+    if text:
+        text = text.lower()
+        if text in ["y", "yes", "true", "on", "1"]:
+            return True
+    return False
+
+
+def setenv_string(text: str) -> Tuple[str, str]:
+    parts = text.split("=", 1)
+    if len(parts) == 2:
+        return parts[0], parts[1]
+    raise ValueError(f"{text} is not in the form NAME=VALUE")
```

## gitlabemu/jobs.py

 * *Ordering differences only*

```diff
@@ -1,593 +1,593 @@
-"""
-Represent a gitlab job
-"""
-import os
-import shutil
-import signal
-
-import sys
-import subprocess
-import tempfile
-import threading
-import time
-from typing import Optional, Dict, List, Any
-
-from .artifacts import GitlabArtifacts
-from .logmsg import info, fatal, debugrule, warning, debug
-from .errors import GitlabEmulatorError
-from .helpers import communicate as comm, is_windows, is_apple, is_linux, parse_timeout, powershell_escape
-from .ansi import ANSI_GREEN, ANSI_RESET
-from .ruleparser import evaluate_rule
-from .userconfig import get_user_config_context
-from .userconfigdata import GleRunnerConfig
-from .variables import expand_variable
-from .gitlab.constraints import JOB_PERSISTED_VARIABLES, PIPELINE_PERSISTED_VARIABLES
-
-
-class NoSuchJob(GitlabEmulatorError):
-    """
-    Could not find a job with the given name
-    """
-    def __init__(self, name):
-        self.name = name
-
-    def __str__(self):
-        return "NoSuchJob {}".format(self.name)
-
-
-class Job(object):
-    """
-    A Gitlab Job
-    """
-    def __init__(self):
-        self.name = None
-        self.build_process = None
-        self.before_script = []
-        self.script = []
-        self.after_script = []
-        self.error_shell = None
-        self.enter_shell = False
-        self.before_script_enter_shell = False
-        self.tags = []
-        self.stage = "test"
-        self.variables = {}
-        self.extra_variables = {}
-        self.allow_add_variables = True
-        self.dependencies = []
-        self.needed_artifacts = []
-        self.artifacts = GitlabArtifacts()
-        self._shell = None
-        self._runner: Optional[GleRunnerConfig] = None
-        self._parallel = None
-        self._config = {}
-        if is_windows():  # pragma: cover if windows
-            self._shell = "powershell"
-        else:  # pragma: cover if not windows
-            self._shell = "sh"
-
-        self.workspace = None
-        self.stderr = sys.stderr
-        self.stdout = sys.stdout
-        self.started_time = 0
-        self.ended_time = 0
-        self.timeout_seconds = 0
-        self.timed_out = False
-        self.monitor_thread = None
-        self.exit_monitor = False
-        self.skipped_reason = None
-        self.rules = None
-        self.configloader = None
-        self._shell_is_user = False
-
-    @property
-    def shell_is_user(self) -> bool:
-        return self._shell_is_user
-
-    @shell_is_user.setter
-    def shell_is_user(self, value: bool):
-        self._shell_is_user = value
-
-    def get_emulator_runner(self) -> GleRunnerConfig:
-        ctx = get_user_config_context()
-        return ctx.find_runner(image=False, tags=self.tags)
-
-    def copy_config(self) -> dict:
-        return dict(self._config)
-
-    def check_skipped(self) -> bool:
-        """Return True if this job is skipped by rules"""
-        return self.skipped_reason is not None
-
-    def interactive_mode(self):
-        """Return True if in interactive mode"""
-        return self.enter_shell or self.before_script_enter_shell
-
-    def __str__(self):
-        return "job {}".format(self.name)
-
-    def duration(self):
-        if self.started_time:
-            ended = self.ended_time
-            if not ended:
-                ended = time.monotonic()
-            return ended - self.started_time
-        return 0
-
-    def monitor_thread_loop_once(self):
-        """
-        Execute each time around the monitor loop
-        """
-        # check for timeout
-        if self.timeout_seconds:
-            duration = self.duration()
-            if duration > self.timeout_seconds:
-                info(f"Job exceeded {int(self.timeout_seconds)} sec timeout")
-                self.timed_out = True
-                self.abort()
-                self.exit_monitor = True
-
-    def monitor_thread_loop(self):
-        """
-        Executed by the monitor thread when a job is started
-        and exits when it finishes
-        """
-        while not self.exit_monitor:
-            try:
-                self.monitor_thread_loop_once()
-            except Exception as err:  # pragma: no cover
-                info(f"timeout monitor thread error: {err}")
-                break
-            time.sleep(2)
-
-    def is_powershell(self) -> bool:
-        return "powershell" == self.shell
-
-    @property
-    def shell(self):
-        return self._shell
-
-    @shell.setter
-    def shell(self, value):
-        if value not in ["cmd", "powershell", "sh", "bash"]:
-            raise NotImplementedError("Unsupported shell type " + value)
-        self._shell = value
-
-    def shell_command(self, scriptfile):
-        if is_windows():  # pragma: cover if windows
-            if self.shell == "powershell":
-                return ["powershell.exe",
-                        "-NoProfile",
-                        "-NonInteractive",
-                        "-ExecutionPolicy", "Bypass",
-                        "-Command", scriptfile]
-            return ["powershell", "-Command", "& cmd /Q /C " + scriptfile]
-        # else unix/linux
-        interp = f"/bin/{self.shell}"
-        if self.shell == "bash":
-            if not self.has_bash():
-                warning("settings said to use bash but it is not installed, using /bin/sh")
-                interp = "/bin/sh"
-                self.shell = "sh"
-        return [interp, scriptfile]
-
-    @property
-    def parallel(self) -> Optional[int]:
-        return self._parallel
-
-    def allocate_runner(self):
-        """Finish loading low level details before we run the job"""
-        info("allocating runner")
-        if self.shell == "cmd":
-            warning("the windows cmd shell is obsolete and does not work on real gitlab any more")
-        else:
-            self.shell = self.runner.shell
-
-        for name, value in self.runner.environment.items():
-            if name not in self.variables:
-                self.variables[name] = value
-
-    def load(self, name: str, config: dict, overrides: Optional[Dict[str, Any]] = None):
-        """
-        Load a job from a dictionary
-        :param name:
-        :param config:
-        :param overrides: set/unset any item in the job config.
-        :return:
-        """
-        self.workspace = config[".gitlab-emulator-workspace"]
-        self.name = name
-        job = config[name]
-        if overrides is not None:
-            # set/unset things in the job
-            for ov_name, ov_value in overrides.items():
-                if ov_value is None:
-                    if ov_name in job:
-                        del job[ov_name]
-                else:
-                    job[ov_name] = ov_value
-
-        self.shell = config.get(".gitlabemu-windows-shell", self.shell)
-
-        self.error_shell = None
-        self.enter_shell = None
-
-        all_before = config.get("before_script", [])
-        self.before_script = job.get("before_script", all_before)
-        self.script = job.get("script", [])
-
-        all_after = config.get("after_script", [])
-        self.after_script = job.get("after_script", all_after)
-        self.variables = dict(job.get("variables", {}))
-        self.extra_variables = dict(config.get(".gle-extra_variables", {}))
-        self.tags = job.get("tags", [])
-        # prefer needs over dependencies
-        needed = job.get("needs", job.get("dependencies", []))
-        self.dependencies = []
-        for item in needed:
-            if isinstance(item, dict):
-                self.dependencies.append(item.get("job"))
-                if item.get("artifacts", False):
-                    self.needed_artifacts.append(item.get("job"))
-            else:
-                self.dependencies.append(item)
-                self.needed_artifacts.append(item)
-        self.dependencies = list(set(self.dependencies))
-
-        if "timeout" in config[self.name]:
-            self.timeout_seconds = parse_timeout(config[self.name].get("timeout"))
-
-        parallel = config[self.name].get("parallel", None)
-        self._parallel = parallel
-        self._config = dict(config)
-
-        self.set_job_variables()
-        self.artifacts.load(dict(job.get("artifacts", {})))
-
-        # load and match the rules
-        if self.configloader:
-            rules = config[self.name].get("rules", [])
-            if rules:
-                for rule_item in rules:
-                    rule_item: dict
-                    # each should be a dict,
-                    debugrule(f"job={self.name}: checking rule: {rule_item}")
-                    if "if" in rule_item:
-                        # match it now
-                        if_matched = evaluate_rule(rule_item["if"], self.configloader.variables)
-                    else:
-                        # is a bare rule with no "if", usually this is the last rule
-                        if_matched = True
-
-                    if if_matched:
-                        debugrule(f"job={self.name}: rule matched")
-                        when = rule_item.get("when", "on_success")
-                        if when:
-                            if when == "never":
-                                self.skipped_reason = f"matched {rule_item}"
-                        # take the first hit
-                        break
-
-    @property
-    def runner(self) -> GleRunnerConfig:
-        if self._runner is None:
-            runner = self.get_emulator_runner()
-            if runner is not None:
-                self._runner = runner
-        return self._runner
-
-    def get_config(self, name: str):
-        return self._config.get(name)
-
-    def set_job_variables(self):
-        self.configure_job_variable("CI_JOB_ID", str(int(time.time())), force=True)
-        self.configure_job_variable("CI_CONFIG_PATH", self.get_config("ci_config_file"))
-        self.configure_job_variable("CI_PROJECT_DIR", self.workspace)
-        self.configure_job_variable("CI_BUILDS_DIR", os.path.dirname(self.workspace))
-        jobname = self.name
-        if self._parallel is not None:
-            pindex = self._config.get(".gitlabemu-parallel-index", 1)
-            ptotal = self._config.get(".gitlabemu-parallel-total", 1)
-            # set 1 parallel job
-            jobname += " {}/{}".format(pindex, ptotal)
-            self.configure_job_variable("CI_NODE_INDEX", str(pindex), force=True)
-            self.configure_job_variable("CI_NODE_TOTAL", str(ptotal), force=True)
-
-        self.configure_job_variable("CI_JOB_NAME", jobname, force=True)
-        self.configure_job_variable("CI_JOB_STAGE", self.stage, force=True)
-        self.configure_job_variable("CI_JOB_TOKEN", "00" * 32)
-        self.configure_job_variable("CI_JOB_URL", "file://gitlab-emulator/none")
-
-    def configure_job_variable(self, name, value, force=False):
-        """
-        Set job variable defaults. If the variable is not present in self.extra_variables, set it to the given value. If the variable is present in os.environ, use that value instead
-        :return:
-        """
-        if not self.allow_add_variables:
-            return
-
-        if value is None:
-            value = ""
-        value = str(value)
-
-        if force:
-            self.extra_variables[name] = value
-        else:
-            # set job related env vars
-            if name not in self.extra_variables:
-                if name in os.environ:
-                    value = os.environ[name]  # prefer env variables if set
-                self.extra_variables[name] = value
-
-    def abort(self):
-        """
-        Abort the build and attempt cleanup
-        :return:
-        """
-        info("aborting job {}".format(self.name))
-        if self.build_process and self.build_process.poll() is None:
-            info("killing child build process..")
-            os.kill(self.build_process.pid, signal.SIGTERM)
-            killing = time.monotonic()
-            while self.build_process.poll() is None: # pragma: no cover
-                time.sleep(1)
-                if time.monotonic() - killing > 10:
-                    os.kill(self.build_process.pid, signal.SIGKILL)
-
-    def communicate(self, process, script=None):
-        """
-        Process STDIO for a build process
-        :param process: child started by POpen
-        :param script: script (eg bytes) to pipe into stdin
-        :return:
-        """
-        comm(process, stdout=self.stdout, script=script)
-
-    def has_bash(self):
-        """
-        Return True if this system has bash and isn't windows
-        """
-        if not is_windows():  # pragma: cover if not windows
-            return os.path.exists("/bin/bash")
-        return False  # pragma: cover if windows
-
-    def base_variables(self) -> Dict[str, str]:
-        return dict(self._config.get("variables", {}))
-
-    @staticmethod
-    def ci_expandable_variables(variables: Dict[str, str]) -> Dict[str, str]:
-        expandable = {}
-        for name in variables:
-            if name in PIPELINE_PERSISTED_VARIABLES + JOB_PERSISTED_VARIABLES:
-                expandable[name] = variables[name]
-        return expandable
-
-    def expand_variables(self, variables: Dict[str, str], only_ci=True) -> Dict[str, str]:
-        expanded = {}
-        for name in variables:
-            value = variables[name]
-            if only_ci:
-                value = expand_variable(self.ci_expandable_variables(variables), value)
-            else:
-                value = expand_variable(variables, value)
-            expanded[name] = value
-        return expanded
-
-    def get_envs(self, expand_only_ci=True):
-        """
-        Get environment variable dict for the job
-        :return:
-        """
-        envs = self.base_variables()
-        envs.update(os.environ)
-        return self.get_defined_envs(envs, expand_only_ci=expand_only_ci)
-
-    def get_defined_envs(self, envs: dict, expand_only_ci=True):
-        for name in self.variables:
-            value = self.variables[name]
-            if value is None:
-                value = ""
-            if not isinstance(value, dict):
-                value = str(value)
-            envs[name] = value
-
-        for name in self.extra_variables:
-            envs[name] = self.extra_variables[name]
-        # expand any predefeined variables
-        return self.expand_variables(envs, only_ci=expand_only_ci)
-
-    def get_script_fileext(self):
-        ext = ".sh"
-        if is_windows():  # pragma: cover if windows
-            if self.is_powershell():
-                ext = ".ps1"
-            else:
-                ext = ".bat"
-        return ext
-
-    def run_script(self, lines):
-        """
-        Execute a script
-        :param lines:
-        :return:
-        """
-        envs = self.get_envs()
-        envs["PWD"] = os.path.abspath(self.workspace)
-        script = make_script(lines, powershell=self.is_powershell())
-        temp = tempfile.mkdtemp()
-        try:
-            ext = self.get_script_fileext()
-            generated = os.path.join(temp, "generated-gitlab-script" + ext)
-            with open(generated, "w") as fd:
-                print(script, file=fd)
-            cmdline = self.shell_command(generated)
-            debug("cmdline: {}".format(cmdline))
-            if self.enter_shell or self.error_shell:  # pragma: no cover
-                # TODO figure out how to cover tty stuff
-                opened = subprocess.Popen(cmdline,
-                                          env=envs,
-                                          shell=False,
-                                          cwd=self.workspace)
-            else:
-                opened = subprocess.Popen(cmdline,
-                                          env=envs,
-                                          shell=False,
-                                          cwd=self.workspace,
-                                          stdin=subprocess.DEVNULL,
-                                          stdout=subprocess.PIPE,
-                                          stderr=subprocess.STDOUT)
-            self.build_process = opened
-            self.communicate(opened, script=None)
-        finally:
-            shutil.rmtree(temp)
-
-        return opened.returncode
-
-    def get_interactive_shell_command(self) -> List[str]:
-        prog = ["/bin/sh"]
-        if is_windows():  # pragma:  cover if windows
-            if "powershell.exe" in self.shell:
-                prog = ["powershell"]
-            else:
-                prog = ["cmd.exe"]
-        return prog
-
-    def shell_on_error(self):
-        """
-        Execute a shell command on job errors
-        :return:
-        """
-        # this is interactive only and cant really be easily tested
-        try:  # pragma: no cover
-            print("Job {} script error..".format(self.name), flush=True)
-            print("Running error-shell..", flush=True)
-            subprocess.check_call("\n".join(self.error_shell))
-        except subprocess.CalledProcessError:
-            pass
-
-    def run(self):
-        """
-        Run the job on the local machine
-        :return:
-        """
-        self.allocate_runner()
-        self.started_time = time.monotonic()
-        self.monitor_thread = None
-
-        if self.timeout_seconds and not self.interactive_mode():
-            self.monitor_thread = threading.Thread(target=self.monitor_thread_loop, daemon=True)
-            try:
-                self.monitor_thread.start()
-            except RuntimeError as err:
-                # funky hpux special case
-                # pragma: no cover
-                info("could not create a monitor thread, job timeouts may not work: {}".format(err))
-                self.monitor_thread = None
-
-            info("job {} timeout set to {} mins".format(self.name, int(self.timeout_seconds/60)))
-            if not self.monitor_thread:  # pragma: no cover
-                # funky hpux special case
-                def alarm_handler(x, y):
-                    info("Got SIGALRM, aborting build..")
-                    self.abort()
-
-                signal.signal(signal.SIGALRM, alarm_handler)
-                signal.alarm(self.timeout_seconds)
-
-        try:
-            self.run_impl()
-        finally:
-            self.ended_time = time.monotonic()
-            self.exit_monitor = True
-            if self.monitor_thread and self.timeout_seconds:
-                self.monitor_thread.join(timeout=5)
-
-    def run_impl(self):
-        info(f"running shell job {self.name}")
-        info(f"runner = {self.runner}")
-        lines = self.before_script + self.script
-        if self.enter_shell:  # pragma: no cover
-            # TODO cover TTY tests
-            lines.extend(self.get_interactive_shell_command())
-        result = self.run_script(lines)
-        if result and self.error_shell:  # pragma: no cover
-            self.shell_on_error()
-        if self.after_script:
-            if not self.timed_out:
-                self.run_script(self.after_script)
-
-        if result:
-            fatal("Shell job {} failed".format(self.name))
-
-
-def make_script(lines, powershell=False):
-    """
-    Join lines together to make a script
-    :param lines:
-    :return:
-    """
-    extra = []
-    tail = []
-
-    line_wrap_before = []
-    line_wrap_tail = []
-
-    if is_linux() or is_apple():
-        extra = ["set -e"]
-
-    if is_windows():  # pragma: cover if windows
-        if powershell:
-            extra = [
-                '$ErrorActionPreference = "Stop"',
-                'echo ...',
-                'echo "Running on $([Environment]::MachineName)..."',
-            ]
-            line_wrap_before = [
-                '& {' + os.linesep,
-            ]
-            line_wrap_tail = [
-                '}' + os.linesep,
-                'if(!$?) { Exit $LASTEXITCODE }' + os.linesep,
-            ]
-        else:
-            extra = [
-                '@echo off',
-                'setlocal enableextensions',
-                'setlocal enableDelayedExpansion',
-                'set nl=^',
-                'echo ...',
-                'echo Running on %COMPUTERNAME%...',
-                'echo Warning: cmd shells on windows are no longer supported by gitlab',
-                'call :buildscript',
-                'if !errorlevel! NEQ 0 exit /b !errorlevel!',
-                'goto :EOF',
-                ':buildscript',
-            ]
-            line_wrap_tail = [
-            ]
-
-            tail = [
-                'goto :EOF',
-            ]
-    else:  # pragma: not-windows
-        powershell = False
-
-    content = os.linesep.join(extra) + os.linesep
-    for line in lines:
-        if "\n" in line:
-            content += line
-        else:
-            content += os.linesep.join(line_wrap_before)
-            if powershell:  # pragma: cover if windows
-                content += f"echo {powershell_escape(ANSI_GREEN + line + ANSI_RESET, variables=True)}" + os.linesep
-                content += "& " + line + os.linesep
-                content += "if(!$?) { Exit $LASTEXITCODE }" + os.linesep
-            else:
-                content += line + os.linesep
-            content += os.linesep.join(line_wrap_tail)
-    for line in tail:
-        content += line
-
-    if is_windows():  # pragma: cover if windows
-        content += os.linesep
-
-    return content
+"""
+Represent a gitlab job
+"""
+import os
+import shutil
+import signal
+
+import sys
+import subprocess
+import tempfile
+import threading
+import time
+from typing import Optional, Dict, List, Any
+
+from .artifacts import GitlabArtifacts
+from .logmsg import info, fatal, debugrule, warning, debug
+from .errors import GitlabEmulatorError
+from .helpers import communicate as comm, is_windows, is_apple, is_linux, parse_timeout, powershell_escape
+from .ansi import ANSI_GREEN, ANSI_RESET
+from .ruleparser import evaluate_rule
+from .userconfig import get_user_config_context
+from .userconfigdata import GleRunnerConfig
+from .variables import expand_variable
+from .gitlab.constraints import JOB_PERSISTED_VARIABLES, PIPELINE_PERSISTED_VARIABLES
+
+
+class NoSuchJob(GitlabEmulatorError):
+    """
+    Could not find a job with the given name
+    """
+    def __init__(self, name):
+        self.name = name
+
+    def __str__(self):
+        return "NoSuchJob {}".format(self.name)
+
+
+class Job(object):
+    """
+    A Gitlab Job
+    """
+    def __init__(self):
+        self.name = None
+        self.build_process = None
+        self.before_script = []
+        self.script = []
+        self.after_script = []
+        self.error_shell = None
+        self.enter_shell = False
+        self.before_script_enter_shell = False
+        self.tags = []
+        self.stage = "test"
+        self.variables = {}
+        self.extra_variables = {}
+        self.allow_add_variables = True
+        self.dependencies = []
+        self.needed_artifacts = []
+        self.artifacts = GitlabArtifacts()
+        self._shell = None
+        self._runner: Optional[GleRunnerConfig] = None
+        self._parallel = None
+        self._config = {}
+        if is_windows():  # pragma: cover if windows
+            self._shell = "powershell"
+        else:  # pragma: cover if not windows
+            self._shell = "sh"
+
+        self.workspace = None
+        self.stderr = sys.stderr
+        self.stdout = sys.stdout
+        self.started_time = 0
+        self.ended_time = 0
+        self.timeout_seconds = 0
+        self.timed_out = False
+        self.monitor_thread = None
+        self.exit_monitor = False
+        self.skipped_reason = None
+        self.rules = None
+        self.configloader = None
+        self._shell_is_user = False
+
+    @property
+    def shell_is_user(self) -> bool:
+        return self._shell_is_user
+
+    @shell_is_user.setter
+    def shell_is_user(self, value: bool):
+        self._shell_is_user = value
+
+    def get_emulator_runner(self) -> GleRunnerConfig:
+        ctx = get_user_config_context()
+        return ctx.find_runner(image=False, tags=self.tags)
+
+    def copy_config(self) -> dict:
+        return dict(self._config)
+
+    def check_skipped(self) -> bool:
+        """Return True if this job is skipped by rules"""
+        return self.skipped_reason is not None
+
+    def interactive_mode(self):
+        """Return True if in interactive mode"""
+        return self.enter_shell or self.before_script_enter_shell
+
+    def __str__(self):
+        return "job {}".format(self.name)
+
+    def duration(self):
+        if self.started_time:
+            ended = self.ended_time
+            if not ended:
+                ended = time.monotonic()
+            return ended - self.started_time
+        return 0
+
+    def monitor_thread_loop_once(self):
+        """
+        Execute each time around the monitor loop
+        """
+        # check for timeout
+        if self.timeout_seconds:
+            duration = self.duration()
+            if duration > self.timeout_seconds:
+                info(f"Job exceeded {int(self.timeout_seconds)} sec timeout")
+                self.timed_out = True
+                self.abort()
+                self.exit_monitor = True
+
+    def monitor_thread_loop(self):
+        """
+        Executed by the monitor thread when a job is started
+        and exits when it finishes
+        """
+        while not self.exit_monitor:
+            try:
+                self.monitor_thread_loop_once()
+            except Exception as err:  # pragma: no cover
+                info(f"timeout monitor thread error: {err}")
+                break
+            time.sleep(2)
+
+    def is_powershell(self) -> bool:
+        return "powershell" == self.shell
+
+    @property
+    def shell(self):
+        return self._shell
+
+    @shell.setter
+    def shell(self, value):
+        if value not in ["cmd", "powershell", "sh", "bash"]:
+            raise NotImplementedError("Unsupported shell type " + value)
+        self._shell = value
+
+    def shell_command(self, scriptfile):
+        if is_windows():  # pragma: cover if windows
+            if self.shell == "powershell":
+                return ["powershell.exe",
+                        "-NoProfile",
+                        "-NonInteractive",
+                        "-ExecutionPolicy", "Bypass",
+                        "-Command", scriptfile]
+            return ["powershell", "-Command", "& cmd /Q /C " + scriptfile]
+        # else unix/linux
+        interp = f"/bin/{self.shell}"
+        if self.shell == "bash":
+            if not self.has_bash():
+                warning("settings said to use bash but it is not installed, using /bin/sh")
+                interp = "/bin/sh"
+                self.shell = "sh"
+        return [interp, scriptfile]
+
+    @property
+    def parallel(self) -> Optional[int]:
+        return self._parallel
+
+    def allocate_runner(self):
+        """Finish loading low level details before we run the job"""
+        info("allocating runner")
+        if self.shell == "cmd":
+            warning("the windows cmd shell is obsolete and does not work on real gitlab any more")
+        else:
+            self.shell = self.runner.shell
+
+        for name, value in self.runner.environment.items():
+            if name not in self.variables:
+                self.variables[name] = value
+
+    def load(self, name: str, config: dict, overrides: Optional[Dict[str, Any]] = None):
+        """
+        Load a job from a dictionary
+        :param name:
+        :param config:
+        :param overrides: set/unset any item in the job config.
+        :return:
+        """
+        self.workspace = config[".gitlab-emulator-workspace"]
+        self.name = name
+        job = config[name]
+        if overrides is not None:
+            # set/unset things in the job
+            for ov_name, ov_value in overrides.items():
+                if ov_value is None:
+                    if ov_name in job:
+                        del job[ov_name]
+                else:
+                    job[ov_name] = ov_value
+
+        self.shell = config.get(".gitlabemu-windows-shell", self.shell)
+
+        self.error_shell = None
+        self.enter_shell = None
+
+        all_before = config.get("before_script", [])
+        self.before_script = job.get("before_script", all_before)
+        self.script = job.get("script", [])
+
+        all_after = config.get("after_script", [])
+        self.after_script = job.get("after_script", all_after)
+        self.variables = dict(job.get("variables", {}))
+        self.extra_variables = dict(config.get(".gle-extra_variables", {}))
+        self.tags = job.get("tags", [])
+        # prefer needs over dependencies
+        needed = job.get("needs", job.get("dependencies", []))
+        self.dependencies = []
+        for item in needed:
+            if isinstance(item, dict):
+                self.dependencies.append(item.get("job"))
+                if item.get("artifacts", False):
+                    self.needed_artifacts.append(item.get("job"))
+            else:
+                self.dependencies.append(item)
+                self.needed_artifacts.append(item)
+        self.dependencies = list(set(self.dependencies))
+
+        if "timeout" in config[self.name]:
+            self.timeout_seconds = parse_timeout(config[self.name].get("timeout"))
+
+        parallel = config[self.name].get("parallel", None)
+        self._parallel = parallel
+        self._config = dict(config)
+
+        self.set_job_variables()
+        self.artifacts.load(dict(job.get("artifacts", {})))
+
+        # load and match the rules
+        if self.configloader:
+            rules = config[self.name].get("rules", [])
+            if rules:
+                for rule_item in rules:
+                    rule_item: dict
+                    # each should be a dict,
+                    debugrule(f"job={self.name}: checking rule: {rule_item}")
+                    if "if" in rule_item:
+                        # match it now
+                        if_matched = evaluate_rule(rule_item["if"], self.configloader.variables)
+                    else:
+                        # is a bare rule with no "if", usually this is the last rule
+                        if_matched = True
+
+                    if if_matched:
+                        debugrule(f"job={self.name}: rule matched")
+                        when = rule_item.get("when", "on_success")
+                        if when:
+                            if when == "never":
+                                self.skipped_reason = f"matched {rule_item}"
+                        # take the first hit
+                        break
+
+    @property
+    def runner(self) -> GleRunnerConfig:
+        if self._runner is None:
+            runner = self.get_emulator_runner()
+            if runner is not None:
+                self._runner = runner
+        return self._runner
+
+    def get_config(self, name: str):
+        return self._config.get(name)
+
+    def set_job_variables(self):
+        self.configure_job_variable("CI_JOB_ID", str(int(time.time())), force=True)
+        self.configure_job_variable("CI_CONFIG_PATH", self.get_config("ci_config_file"))
+        self.configure_job_variable("CI_PROJECT_DIR", self.workspace)
+        self.configure_job_variable("CI_BUILDS_DIR", os.path.dirname(self.workspace))
+        jobname = self.name
+        if self._parallel is not None:
+            pindex = self._config.get(".gitlabemu-parallel-index", 1)
+            ptotal = self._config.get(".gitlabemu-parallel-total", 1)
+            # set 1 parallel job
+            jobname += " {}/{}".format(pindex, ptotal)
+            self.configure_job_variable("CI_NODE_INDEX", str(pindex), force=True)
+            self.configure_job_variable("CI_NODE_TOTAL", str(ptotal), force=True)
+
+        self.configure_job_variable("CI_JOB_NAME", jobname, force=True)
+        self.configure_job_variable("CI_JOB_STAGE", self.stage, force=True)
+        self.configure_job_variable("CI_JOB_TOKEN", "00" * 32)
+        self.configure_job_variable("CI_JOB_URL", "file://gitlab-emulator/none")
+
+    def configure_job_variable(self, name, value, force=False):
+        """
+        Set job variable defaults. If the variable is not present in self.extra_variables, set it to the given value. If the variable is present in os.environ, use that value instead
+        :return:
+        """
+        if not self.allow_add_variables:
+            return
+
+        if value is None:
+            value = ""
+        value = str(value)
+
+        if force:
+            self.extra_variables[name] = value
+        else:
+            # set job related env vars
+            if name not in self.extra_variables:
+                if name in os.environ:
+                    value = os.environ[name]  # prefer env variables if set
+                self.extra_variables[name] = value
+
+    def abort(self):
+        """
+        Abort the build and attempt cleanup
+        :return:
+        """
+        info("aborting job {}".format(self.name))
+        if self.build_process and self.build_process.poll() is None:
+            info("killing child build process..")
+            os.kill(self.build_process.pid, signal.SIGTERM)
+            killing = time.monotonic()
+            while self.build_process.poll() is None: # pragma: no cover
+                time.sleep(1)
+                if time.monotonic() - killing > 10:
+                    os.kill(self.build_process.pid, signal.SIGKILL)
+
+    def communicate(self, process, script=None):
+        """
+        Process STDIO for a build process
+        :param process: child started by POpen
+        :param script: script (eg bytes) to pipe into stdin
+        :return:
+        """
+        comm(process, stdout=self.stdout, script=script)
+
+    def has_bash(self):
+        """
+        Return True if this system has bash and isn't windows
+        """
+        if not is_windows():  # pragma: cover if not windows
+            return os.path.exists("/bin/bash")
+        return False  # pragma: cover if windows
+
+    def base_variables(self) -> Dict[str, str]:
+        return dict(self._config.get("variables", {}))
+
+    @staticmethod
+    def ci_expandable_variables(variables: Dict[str, str]) -> Dict[str, str]:
+        expandable = {}
+        for name in variables:
+            if name in PIPELINE_PERSISTED_VARIABLES + JOB_PERSISTED_VARIABLES:
+                expandable[name] = variables[name]
+        return expandable
+
+    def expand_variables(self, variables: Dict[str, str], only_ci=True) -> Dict[str, str]:
+        expanded = {}
+        for name in variables:
+            value = variables[name]
+            if only_ci:
+                value = expand_variable(self.ci_expandable_variables(variables), value)
+            else:
+                value = expand_variable(variables, value)
+            expanded[name] = value
+        return expanded
+
+    def get_envs(self, expand_only_ci=True):
+        """
+        Get environment variable dict for the job
+        :return:
+        """
+        envs = self.base_variables()
+        envs.update(os.environ)
+        return self.get_defined_envs(envs, expand_only_ci=expand_only_ci)
+
+    def get_defined_envs(self, envs: dict, expand_only_ci=True):
+        for name in self.variables:
+            value = self.variables[name]
+            if value is None:
+                value = ""
+            if not isinstance(value, dict):
+                value = str(value)
+            envs[name] = value
+
+        for name in self.extra_variables:
+            envs[name] = self.extra_variables[name]
+        # expand any predefeined variables
+        return self.expand_variables(envs, only_ci=expand_only_ci)
+
+    def get_script_fileext(self):
+        ext = ".sh"
+        if is_windows():  # pragma: cover if windows
+            if self.is_powershell():
+                ext = ".ps1"
+            else:
+                ext = ".bat"
+        return ext
+
+    def run_script(self, lines):
+        """
+        Execute a script
+        :param lines:
+        :return:
+        """
+        envs = self.get_envs()
+        envs["PWD"] = os.path.abspath(self.workspace)
+        script = make_script(lines, powershell=self.is_powershell())
+        temp = tempfile.mkdtemp()
+        try:
+            ext = self.get_script_fileext()
+            generated = os.path.join(temp, "generated-gitlab-script" + ext)
+            with open(generated, "w") as fd:
+                print(script, file=fd)
+            cmdline = self.shell_command(generated)
+            debug("cmdline: {}".format(cmdline))
+            if self.enter_shell or self.error_shell:  # pragma: no cover
+                # TODO figure out how to cover tty stuff
+                opened = subprocess.Popen(cmdline,
+                                          env=envs,
+                                          shell=False,
+                                          cwd=self.workspace)
+            else:
+                opened = subprocess.Popen(cmdline,
+                                          env=envs,
+                                          shell=False,
+                                          cwd=self.workspace,
+                                          stdin=subprocess.DEVNULL,
+                                          stdout=subprocess.PIPE,
+                                          stderr=subprocess.STDOUT)
+            self.build_process = opened
+            self.communicate(opened, script=None)
+        finally:
+            shutil.rmtree(temp)
+
+        return opened.returncode
+
+    def get_interactive_shell_command(self) -> List[str]:
+        prog = ["/bin/sh"]
+        if is_windows():  # pragma:  cover if windows
+            if "powershell.exe" in self.shell:
+                prog = ["powershell"]
+            else:
+                prog = ["cmd.exe"]
+        return prog
+
+    def shell_on_error(self):
+        """
+        Execute a shell command on job errors
+        :return:
+        """
+        # this is interactive only and cant really be easily tested
+        try:  # pragma: no cover
+            print("Job {} script error..".format(self.name), flush=True)
+            print("Running error-shell..", flush=True)
+            subprocess.check_call("\n".join(self.error_shell))
+        except subprocess.CalledProcessError:
+            pass
+
+    def run(self):
+        """
+        Run the job on the local machine
+        :return:
+        """
+        self.allocate_runner()
+        self.started_time = time.monotonic()
+        self.monitor_thread = None
+
+        if self.timeout_seconds and not self.interactive_mode():
+            self.monitor_thread = threading.Thread(target=self.monitor_thread_loop, daemon=True)
+            try:
+                self.monitor_thread.start()
+            except RuntimeError as err:
+                # funky hpux special case
+                # pragma: no cover
+                info("could not create a monitor thread, job timeouts may not work: {}".format(err))
+                self.monitor_thread = None
+
+            info("job {} timeout set to {} mins".format(self.name, int(self.timeout_seconds/60)))
+            if not self.monitor_thread:  # pragma: no cover
+                # funky hpux special case
+                def alarm_handler(x, y):
+                    info("Got SIGALRM, aborting build..")
+                    self.abort()
+
+                signal.signal(signal.SIGALRM, alarm_handler)
+                signal.alarm(self.timeout_seconds)
+
+        try:
+            self.run_impl()
+        finally:
+            self.ended_time = time.monotonic()
+            self.exit_monitor = True
+            if self.monitor_thread and self.timeout_seconds:
+                self.monitor_thread.join(timeout=5)
+
+    def run_impl(self):
+        info(f"running shell job {self.name}")
+        info(f"runner = {self.runner}")
+        lines = self.before_script + self.script
+        if self.enter_shell:  # pragma: no cover
+            # TODO cover TTY tests
+            lines.extend(self.get_interactive_shell_command())
+        result = self.run_script(lines)
+        if result and self.error_shell:  # pragma: no cover
+            self.shell_on_error()
+        if self.after_script:
+            if not self.timed_out:
+                self.run_script(self.after_script)
+
+        if result:
+            fatal("Shell job {} failed".format(self.name))
+
+
+def make_script(lines, powershell=False):
+    """
+    Join lines together to make a script
+    :param lines:
+    :return:
+    """
+    extra = []
+    tail = []
+
+    line_wrap_before = []
+    line_wrap_tail = []
+
+    if is_linux() or is_apple():
+        extra = ["set -e"]
+
+    if is_windows():  # pragma: cover if windows
+        if powershell:
+            extra = [
+                '$ErrorActionPreference = "Stop"',
+                'echo ...',
+                'echo "Running on $([Environment]::MachineName)..."',
+            ]
+            line_wrap_before = [
+                '& {' + os.linesep,
+            ]
+            line_wrap_tail = [
+                '}' + os.linesep,
+                'if(!$?) { Exit $LASTEXITCODE }' + os.linesep,
+            ]
+        else:
+            extra = [
+                '@echo off',
+                'setlocal enableextensions',
+                'setlocal enableDelayedExpansion',
+                'set nl=^',
+                'echo ...',
+                'echo Running on %COMPUTERNAME%...',
+                'echo Warning: cmd shells on windows are no longer supported by gitlab',
+                'call :buildscript',
+                'if !errorlevel! NEQ 0 exit /b !errorlevel!',
+                'goto :EOF',
+                ':buildscript',
+            ]
+            line_wrap_tail = [
+            ]
+
+            tail = [
+                'goto :EOF',
+            ]
+    else:  # pragma: not-windows
+        powershell = False
+
+    content = os.linesep.join(extra) + os.linesep
+    for line in lines:
+        if "\n" in line:
+            content += line
+        else:
+            content += os.linesep.join(line_wrap_before)
+            if powershell:  # pragma: cover if windows
+                content += f"echo {powershell_escape(ANSI_GREEN + line + ANSI_RESET, variables=True)}" + os.linesep
+                content += "& " + line + os.linesep
+                content += "if(!$?) { Exit $LASTEXITCODE }" + os.linesep
+            else:
+                content += line + os.linesep
+            content += os.linesep.join(line_wrap_tail)
+    for line in tail:
+        content += line
+
+    if is_windows():  # pragma: cover if windows
+        content += os.linesep
+
+    return content
```

## gitlabemu/localfiles.py

 * *Ordering differences only*

```diff
@@ -1,28 +1,28 @@
-"""
-Utils for local file housekeeping
-"""
-import os
-from .helpers import is_windows, has_docker
-from .docker import DockerTool
-
-
-def restore_path_ownership(path):
-    path = os.path.abspath(path)
-    chowner = os.path.abspath(os.path.join(os.path.dirname(__file__), "chown.py"))
-    if not is_windows():  # pragma: cover if not windows
-        if os.getuid() != 0:
-            if has_docker():
-                from .resnamer import generate_resource_name
-                dt = DockerTool()
-                dt.name = generate_resource_name("chowner")
-                dt.image = "python:3.9-alpine3.14"
-                dt.add_volume(path, path)
-                dt.entrypoint = "/bin/sh"
-                if not dt.image_present:
-                    dt.pull()
-                dt.run()
-                try:
-                    dt.add_file(chowner, "/tmp")
-                    dt.check_call(path, ["python", "/tmp/chown.py", str(os.getuid()), str(os.getgid())])
-                finally:
-                    dt.kill()
+"""
+Utils for local file housekeeping
+"""
+import os
+from .helpers import is_windows, has_docker
+from .docker import DockerTool
+
+
+def restore_path_ownership(path):
+    path = os.path.abspath(path)
+    chowner = os.path.abspath(os.path.join(os.path.dirname(__file__), "chown.py"))
+    if not is_windows():  # pragma: cover if not windows
+        if os.getuid() != 0:
+            if has_docker():
+                from .resnamer import generate_resource_name
+                dt = DockerTool()
+                dt.name = generate_resource_name("chowner")
+                dt.image = "python:3.9-alpine3.14"
+                dt.add_volume(path, path)
+                dt.entrypoint = "/bin/sh"
+                if not dt.image_present:
+                    dt.pull()
+                dt.run()
+                try:
+                    dt.add_file(chowner, "/tmp")
+                    dt.check_call(path, ["python", "/tmp/chown.py", str(os.getuid()), str(os.getgid())])
+                finally:
+                    dt.kill()
```

## gitlabemu/logmsg.py

 * *Ordering differences only*

```diff
@@ -1,60 +1,60 @@
-"""
-Logging functions
-"""
-from __future__ import print_function
-import sys
-import os
-import logging
-from .errors import GitlabEmulatorError
-
-FORMAT = '%(asctime)-15s %(name)s  %(message)s'
-logging.basicConfig(format=FORMAT)
-
-LOGGER = logging.getLogger('gitlab-emulator')
-LOGGER.setLevel(logging.INFO)
-
-FATAL_EXIT = True
-
-AFIRMATIVE = ["y", "yes", "1", "enabled", "on"]
-
-def is_afirmative(txt: str) -> bool:
-    return str(txt).lower() in AFIRMATIVE
-
-
-def enable_rule_debug():
-    os.environ["GLE_DEBUG_RULES"] = "y"
-
-def enable_debug():
-    os.environ["GLE_DEBUG"] = "y"
-
-
-def info(msg):
-    LOGGER.info(msg)
-
-def debug_enabled() -> bool:
-    return is_afirmative(os.environ.get("GLE_DEBUG", "n"))
-
-
-def debugrule_enabled() -> bool:
-    return is_afirmative(os.environ.get("GLE_DEBUG_RULES", "n"))
-
-
-def debugrule(msg):
-    if debugrule_enabled() or debug_enabled():
-        LOGGER.info(f"D: {msg}")
-
-
-def debug(msg):
-    if debug_enabled():
-        LOGGER.info(f"D: {msg}")
-
-
-def warning(msg):
-    LOGGER.warning(f"W! {msg}")
-
-
-def fatal(msg):
-    LOGGER.critical(f"E!: {msg}")
-    if FATAL_EXIT:
-        sys.exit(1)
-    raise GitlabEmulatorError()
+"""
+Logging functions
+"""
+from __future__ import print_function
+import sys
+import os
+import logging
+from .errors import GitlabEmulatorError
+
+FORMAT = '%(asctime)-15s %(name)s  %(message)s'
+logging.basicConfig(format=FORMAT)
+
+LOGGER = logging.getLogger('gitlab-emulator')
+LOGGER.setLevel(logging.INFO)
+
+FATAL_EXIT = True
+
+AFIRMATIVE = ["y", "yes", "1", "enabled", "on"]
+
+def is_afirmative(txt: str) -> bool:
+    return str(txt).lower() in AFIRMATIVE
+
+
+def enable_rule_debug():
+    os.environ["GLE_DEBUG_RULES"] = "y"
+
+def enable_debug():
+    os.environ["GLE_DEBUG"] = "y"
+
+
+def info(msg):
+    LOGGER.info(msg)
+
+def debug_enabled() -> bool:
+    return is_afirmative(os.environ.get("GLE_DEBUG", "n"))
+
+
+def debugrule_enabled() -> bool:
+    return is_afirmative(os.environ.get("GLE_DEBUG_RULES", "n"))
+
+
+def debugrule(msg):
+    if debugrule_enabled() or debug_enabled():
+        LOGGER.info(f"D: {msg}")
+
+
+def debug(msg):
+    if debug_enabled():
+        LOGGER.info(f"D: {msg}")
+
+
+def warning(msg):
+    LOGGER.warning(f"W! {msg}")
+
+
+def fatal(msg):
+    LOGGER.critical(f"E!: {msg}")
+    if FATAL_EXIT:
+        sys.exit(1)
+    raise GitlabEmulatorError()
```

## gitlabemu/pipelines.py

 * *Ordering differences only*

```diff
@@ -1,226 +1,226 @@
-import os
-from typing import Optional, Dict, List
-from gitlab import GitlabGetError
-
-from .generator import (generate_pipeline_yaml,
-                        generate_artifact_fetch_job,
-                        create_pipeline_branch,
-                        wait_for_project_commit_pipeline)
-from .gitlab.types import RESERVED_TOP_KEYS
-from .gitlab_client_api import parse_gitlab_from_arg, get_current_project_client, do_gitlab_fetch
-from .helpers import note, die, git_current_branch
-from .logmsg import info
-from .yamlloader import ordered_dump
-
-
-def print_pipeline_jobs(pipeline,
-                        status: Optional[bool] = False,
-                        completed: Optional[bool] = False):
-    """print the jobs in the pipeline"""
-    jobs = list(pipeline.jobs.list(all=True))
-    if completed:
-        jobs = [x for x in jobs if x.status == "success"]
-        note(f"Listing completed jobs in {pipeline.web_url}")
-    else:
-        note(f"Listing jobs in {pipeline.web_url}")
-    jobdict = {}
-    for x in jobs:
-        jobdict[x.name] = x
-    names = sorted([x.name for x in jobs])
-
-    if status:
-        note(f"{'Name':48} Status")
-
-    for name in names:
-        if status:
-            print(f"{name:48} {jobdict[name].status}", flush=True)
-        else:
-            print(name, flush=True)
-
-
-def pipelines_cmd(tls_verify: Optional[bool] = True,
-                  matchers: Optional[Dict[str, str]] = None,
-                  do_list: Optional[bool] = False,
-                  do_cancel: Optional[bool] = False,
-                  limit: Optional[int] = 10,
-                  ):
-    """List/Cancel/Generate pipelines"""
-    client, project, _ = get_current_project_client(tls_verify=tls_verify, need_remote=False)
-    if not matchers:
-        matchers = {}
-
-    if do_list or do_cancel:
-        if do_list:
-            matching = ""
-            if matchers:
-                matching = f"matching {matchers}"
-            note(f"Recent pipelines from project '{project.path_with_namespace}' on {client.api_url} {matching}")
-        elif do_cancel:
-            note(f"Cancel pipelines in project '{project.path_with_namespace}' on {client.api_url} matching: {matchers}")
-        page = 1
-        seen = 0
-        pagesize = 10
-        if do_list:
-            print(f"{'# ID':<12} {'Status':<8} {'Commit':<40} Git Ref", flush=True)
-        while seen < limit:
-            pipes = project.pipelines.list(sort="desc", order_by="updated_at", page=page, per_page=pagesize, **matchers)
-            if not pipes:
-                break
-            for pipe in pipes:
-                if seen >= limit:
-                    break
-                if do_list:
-                    print(f"{pipe.id:>12} {pipe.status:<8} {pipe.sha} {pipe.ref}")
-                elif do_cancel:
-                    print(f"Cancelling {pipe.id:>12} {pipe.status:<8} {pipe.sha} {pipe.ref}")
-                    pipe.cancel()
-                seen += 1
-            if len(pipes) == pagesize:
-                page += 1
-
-
-def create_pipeline(vars: Optional[Dict[str, str]] = None,
-                    tls_verify: Optional[bool] = True):
-    """Trigger a pipeline on the current branch if possible"""
-    cwd = os.getcwd()
-    client, project, remotename = get_current_project_client(tls_verify=tls_verify)
-    branch = git_current_branch(cwd)
-    note(f"Creating pipeline for {branch} ..")
-    started = project.pipelines.create(data={"ref": branch, "variables": vars})
-    if started:
-        note(f"Created pipeline {started.id} with ref={started.ref} at commit {started.sha}")
-        note(started.web_url)
-
-
-def generate_partial_pipeline(loader, *goals, variables: Optional[Dict[str, str]] = None):
-    if not goals:
-        die("Cannot generate a pipeline with zero jobs")
-
-
-
-def generate_pipeline(loader, *goals,
-                      variables: Optional[Dict[str, str]] = None,
-                      use_from: Optional[str] = None,
-                      dump_only: Optional[bool] = False,
-                      tls_verify: Optional[bool] = True) -> Optional[dict]:
-    """Generate and push a subset pipeline"""
-    pipeline = None
-    download_jobs = {}
-    deps = {}
-    client, project, remotename = get_current_project_client(tls_verify=tls_verify)
-    cwd = os.getcwd()
-    if not goals:
-        die("Cannot generate a pipeline with zero jobs")
-
-    note(f"Generate subset pipeline to build '{goals}'..")
-    recurse = True
-    if use_from:
-        recurse = False
-        ident = parse_gitlab_from_arg(use_from, prefer_gitref=True)
-        if ident.pipeline:
-            note(f"Checking source pipeline {ident.pipeline} ..")
-            try:
-                pipeline = project.pipelines.get(ident.pipeline)
-            except GitlabGetError as err:
-                die(f"Failed to read pipeline {ident.pipeline}, {err.error_message}")
-        elif ident.gitref:
-            note(f"Searching for latest pipeline on {ident.gitref} ..")
-            # find the newest pipeline for this git reference
-            found = project.pipelines.list(
-                sort="desc",
-                ref=ident.gitref,
-                order_by="updated_at",
-                page=1, per_page=5,
-                status='success')
-            if not found:
-                die(f"Could not find a completed pipeline for git reference {ident.gitref}")
-            pipeline = found[0]
-        else:
-            die(f"Cannot work out pipeline --from {use_from}")
-
-        # now make sure the pipeline contains the jobs we need
-        pipeline_jobs = {}
-
-        for item in pipeline.jobs.list(all=True):
-            if item.status == "success":
-                pipeline_jobs[item.name] = item
-
-        for goal in goals:
-            loaded = loader.load_job(goal)
-            if loaded.check_skipped():
-                info(f"{goal} skipped by rules")
-                continue
-
-            for dep in loaded.dependencies:
-                if dep in goals:
-                    continue
-                if dep not in pipeline_jobs:
-                    die(f"Pipeline did not contain a successful '{dep}' job needed by {goal}")
-                else:
-                    from_job = pipeline_jobs[dep]
-                    if hasattr(from_job, "artifacts"):  # missing if it created no artifacts
-                        archives = [x for x in from_job.artifacts if x["file_type"] == "archive"]
-                        if archives:
-                            artifact_url = f"{client.api_url}/projects/{project.id}/jobs/{from_job.id}/artifacts"
-                            download_jobs[dep] = artifact_url
-                            if goal not in deps:
-                                deps[goal] = []
-                            deps[goal].append(dep)
-
-    generated = generate_pipeline_yaml(loader, *goals, recurse=recurse)
-    jobs = [name for name in generated.keys() if name not in RESERVED_TOP_KEYS]
-    note(f"Will build jobs: {jobs} ..")
-    stages = generated.get("stages", ["test"])
-
-    for from_name in download_jobs:
-        fetch_job = generate_artifact_fetch_job(loader,
-                                                stages[0],
-                                                {from_name: download_jobs[from_name]},
-                                                tls_verify=client.ssl_verify)
-        generated[from_name] = fetch_job
-
-    if variables is not None:
-        for varname in variables:
-            generated["variables"][varname] = variables[varname]
-
-    if dump_only:
-        return generated
-
-    branch_name = generate_subset_branch_name(client, cwd)
-    note(f"Creating temporary pipeline branch '{branch_name}'..")
-    commit = create_pipeline_branch(cwd,
-                                    remotename,
-                                    branch_name,
-                                    f"subset pipeline for {goals}",
-                                    {
-                                        ".gitlab-ci.yml": ordered_dump(generated)
-                                    })
-    if commit:
-        note(f"Custom build commit is {commit}")
-        note(f"Waiting for new pipeline to start..")
-        pipeline = wait_for_project_commit_pipeline(project, commit)
-        if not pipeline:
-            die("Could not find the pipeline for our change")
-        else:
-            note(f"Building: {pipeline.web_url}")
-
-    else:
-        die("Could not make a custom pipeline branch, "
-            "please make sure your local changes are committed first")
-
-
-def get_subset_prefix() -> str:
-    return "temp/"
-
-def generate_subset_branch_name(client, cwd):
-    """Get the subset name of the current branch"""
-    branch_name = f"{get_subset_prefix()}{client.user.username}/{git_current_branch(cwd)}"
-    return branch_name
-
-
-def export_cmd(pipeline: str, outdir: str, *jobs, tls_verify: Optional[bool] = True, exec_export: Optional[List[str]] = None):
-    do_gitlab_fetch(pipeline,
-                    [x for x in jobs],
-                    tls_verify=tls_verify,
-                    callback=exec_export,
-                    export_to=outdir)
+import os
+from typing import Optional, Dict, List
+from gitlab import GitlabGetError
+
+from .generator import (generate_pipeline_yaml,
+                        generate_artifact_fetch_job,
+                        create_pipeline_branch,
+                        wait_for_project_commit_pipeline)
+from .gitlab.types import RESERVED_TOP_KEYS
+from .gitlab_client_api import parse_gitlab_from_arg, get_current_project_client, do_gitlab_fetch
+from .helpers import note, die, git_current_branch
+from .logmsg import info
+from .yamlloader import ordered_dump
+
+
+def print_pipeline_jobs(pipeline,
+                        status: Optional[bool] = False,
+                        completed: Optional[bool] = False):
+    """print the jobs in the pipeline"""
+    jobs = list(pipeline.jobs.list(all=True))
+    if completed:
+        jobs = [x for x in jobs if x.status == "success"]
+        note(f"Listing completed jobs in {pipeline.web_url}")
+    else:
+        note(f"Listing jobs in {pipeline.web_url}")
+    jobdict = {}
+    for x in jobs:
+        jobdict[x.name] = x
+    names = sorted([x.name for x in jobs])
+
+    if status:
+        note(f"{'Name':48} Status")
+
+    for name in names:
+        if status:
+            print(f"{name:48} {jobdict[name].status}", flush=True)
+        else:
+            print(name, flush=True)
+
+
+def pipelines_cmd(tls_verify: Optional[bool] = True,
+                  matchers: Optional[Dict[str, str]] = None,
+                  do_list: Optional[bool] = False,
+                  do_cancel: Optional[bool] = False,
+                  limit: Optional[int] = 10,
+                  ):
+    """List/Cancel/Generate pipelines"""
+    client, project, _ = get_current_project_client(tls_verify=tls_verify, need_remote=False)
+    if not matchers:
+        matchers = {}
+
+    if do_list or do_cancel:
+        if do_list:
+            matching = ""
+            if matchers:
+                matching = f"matching {matchers}"
+            note(f"Recent pipelines from project '{project.path_with_namespace}' on {client.api_url} {matching}")
+        elif do_cancel:
+            note(f"Cancel pipelines in project '{project.path_with_namespace}' on {client.api_url} matching: {matchers}")
+        page = 1
+        seen = 0
+        pagesize = 10
+        if do_list:
+            print(f"{'# ID':<12} {'Status':<8} {'Commit':<40} Git Ref", flush=True)
+        while seen < limit:
+            pipes = project.pipelines.list(sort="desc", order_by="updated_at", page=page, per_page=pagesize, **matchers)
+            if not pipes:
+                break
+            for pipe in pipes:
+                if seen >= limit:
+                    break
+                if do_list:
+                    print(f"{pipe.id:>12} {pipe.status:<8} {pipe.sha} {pipe.ref}")
+                elif do_cancel:
+                    print(f"Cancelling {pipe.id:>12} {pipe.status:<8} {pipe.sha} {pipe.ref}")
+                    pipe.cancel()
+                seen += 1
+            if len(pipes) == pagesize:
+                page += 1
+
+
+def create_pipeline(vars: Optional[Dict[str, str]] = None,
+                    tls_verify: Optional[bool] = True):
+    """Trigger a pipeline on the current branch if possible"""
+    cwd = os.getcwd()
+    client, project, remotename = get_current_project_client(tls_verify=tls_verify)
+    branch = git_current_branch(cwd)
+    note(f"Creating pipeline for {branch} ..")
+    started = project.pipelines.create(data={"ref": branch, "variables": vars})
+    if started:
+        note(f"Created pipeline {started.id} with ref={started.ref} at commit {started.sha}")
+        note(started.web_url)
+
+
+def generate_partial_pipeline(loader, *goals, variables: Optional[Dict[str, str]] = None):
+    if not goals:
+        die("Cannot generate a pipeline with zero jobs")
+
+
+
+def generate_pipeline(loader, *goals,
+                      variables: Optional[Dict[str, str]] = None,
+                      use_from: Optional[str] = None,
+                      dump_only: Optional[bool] = False,
+                      tls_verify: Optional[bool] = True) -> Optional[dict]:
+    """Generate and push a subset pipeline"""
+    pipeline = None
+    download_jobs = {}
+    deps = {}
+    client, project, remotename = get_current_project_client(tls_verify=tls_verify)
+    cwd = os.getcwd()
+    if not goals:
+        die("Cannot generate a pipeline with zero jobs")
+
+    note(f"Generate subset pipeline to build '{goals}'..")
+    recurse = True
+    if use_from:
+        recurse = False
+        ident = parse_gitlab_from_arg(use_from, prefer_gitref=True)
+        if ident.pipeline:
+            note(f"Checking source pipeline {ident.pipeline} ..")
+            try:
+                pipeline = project.pipelines.get(ident.pipeline)
+            except GitlabGetError as err:
+                die(f"Failed to read pipeline {ident.pipeline}, {err.error_message}")
+        elif ident.gitref:
+            note(f"Searching for latest pipeline on {ident.gitref} ..")
+            # find the newest pipeline for this git reference
+            found = project.pipelines.list(
+                sort="desc",
+                ref=ident.gitref,
+                order_by="updated_at",
+                page=1, per_page=5,
+                status='success')
+            if not found:
+                die(f"Could not find a completed pipeline for git reference {ident.gitref}")
+            pipeline = found[0]
+        else:
+            die(f"Cannot work out pipeline --from {use_from}")
+
+        # now make sure the pipeline contains the jobs we need
+        pipeline_jobs = {}
+
+        for item in pipeline.jobs.list(all=True):
+            if item.status == "success":
+                pipeline_jobs[item.name] = item
+
+        for goal in goals:
+            loaded = loader.load_job(goal)
+            if loaded.check_skipped():
+                info(f"{goal} skipped by rules")
+                continue
+
+            for dep in loaded.dependencies:
+                if dep in goals:
+                    continue
+                if dep not in pipeline_jobs:
+                    die(f"Pipeline did not contain a successful '{dep}' job needed by {goal}")
+                else:
+                    from_job = pipeline_jobs[dep]
+                    if hasattr(from_job, "artifacts"):  # missing if it created no artifacts
+                        archives = [x for x in from_job.artifacts if x["file_type"] == "archive"]
+                        if archives:
+                            artifact_url = f"{client.api_url}/projects/{project.id}/jobs/{from_job.id}/artifacts"
+                            download_jobs[dep] = artifact_url
+                            if goal not in deps:
+                                deps[goal] = []
+                            deps[goal].append(dep)
+
+    generated = generate_pipeline_yaml(loader, *goals, recurse=recurse)
+    jobs = [name for name in generated.keys() if name not in RESERVED_TOP_KEYS]
+    note(f"Will build jobs: {jobs} ..")
+    stages = generated.get("stages", ["test"])
+
+    for from_name in download_jobs:
+        fetch_job = generate_artifact_fetch_job(loader,
+                                                stages[0],
+                                                {from_name: download_jobs[from_name]},
+                                                tls_verify=client.ssl_verify)
+        generated[from_name] = fetch_job
+
+    if variables is not None:
+        for varname in variables:
+            generated["variables"][varname] = variables[varname]
+
+    if dump_only:
+        return generated
+
+    branch_name = generate_subset_branch_name(client, cwd)
+    note(f"Creating temporary pipeline branch '{branch_name}'..")
+    commit = create_pipeline_branch(cwd,
+                                    remotename,
+                                    branch_name,
+                                    f"subset pipeline for {goals}",
+                                    {
+                                        ".gitlab-ci.yml": ordered_dump(generated)
+                                    })
+    if commit:
+        note(f"Custom build commit is {commit}")
+        note(f"Waiting for new pipeline to start..")
+        pipeline = wait_for_project_commit_pipeline(project, commit)
+        if not pipeline:
+            die("Could not find the pipeline for our change")
+        else:
+            note(f"Building: {pipeline.web_url}")
+
+    else:
+        die("Could not make a custom pipeline branch, "
+            "please make sure your local changes are committed first")
+
+
+def get_subset_prefix() -> str:
+    return "temp/"
+
+def generate_subset_branch_name(client, cwd):
+    """Get the subset name of the current branch"""
+    branch_name = f"{get_subset_prefix()}{client.user.username}/{git_current_branch(cwd)}"
+    return branch_name
+
+
+def export_cmd(pipeline: str, outdir: str, *jobs, tls_verify: Optional[bool] = True, exec_export: Optional[List[str]] = None):
+    do_gitlab_fetch(pipeline,
+                    [x for x in jobs],
+                    tls_verify=tls_verify,
+                    callback=exec_export,
+                    export_to=outdir)
```

## gitlabemu/pstab.py

 * *Ordering differences only*

```diff
@@ -1,64 +1,64 @@
-"""Cross platform pure python process inspection"""
-import platform
-from typing import List
-import os
-import subprocess
-
-
-class Base:
-
-    cmdline = "/bin/ps ax"
-    shell = True
-
-    def get_process_list(self) -> List[str]:
-        stdout = subprocess.check_output(self.cmdline,
-                                         shell=self.shell,
-                                         encoding="utf-8",
-                                         stderr=subprocess.DEVNULL)
-        lines = [line for line in stdout.splitlines(keepends=False)]
-        return lines
-
-    def get_pids(self) -> List[int]:
-        pids = []
-        for line in self.get_process_list():
-            content = line.strip()
-            if content:
-                words = content.split()
-                if words:
-                    try:
-                        pids.append(int(words[0]))
-                    except ValueError:
-                        pass
-        return pids
-
-
-class Proc(Base):
-    """Inspect processes using /proc"""
-
-    def get_pids(self) -> List[int]:
-        files = os.listdir("/proc")
-        pids = []
-        for item in files:
-            try:
-                pids.append(int(item))
-            except ValueError:
-                pass
-        return pids
-
-
-class Powershell(Base):
-    """Windows powershell"""
-    # pragma:  cover if windows
-    shell = False
-    cmdline = "powershell -Command \"Get-Process|Format-Table -Property Id\""
-
-
-def get_pids() -> List[int]:
-    if platform.system() == "Windows":  # pragma: cover if windows
-        p = Powershell()
-    elif os.path.isdir("/proc"):  # pragma: not-windows
-        p = Proc()
-    else:  # pragma: not-windows
-        p = Base()
-
-    return p.get_pids()
+"""Cross platform pure python process inspection"""
+import platform
+from typing import List
+import os
+import subprocess
+
+
+class Base:
+
+    cmdline = "/bin/ps ax"
+    shell = True
+
+    def get_process_list(self) -> List[str]:
+        stdout = subprocess.check_output(self.cmdline,
+                                         shell=self.shell,
+                                         encoding="utf-8",
+                                         stderr=subprocess.DEVNULL)
+        lines = [line for line in stdout.splitlines(keepends=False)]
+        return lines
+
+    def get_pids(self) -> List[int]:
+        pids = []
+        for line in self.get_process_list():
+            content = line.strip()
+            if content:
+                words = content.split()
+                if words:
+                    try:
+                        pids.append(int(words[0]))
+                    except ValueError:
+                        pass
+        return pids
+
+
+class Proc(Base):
+    """Inspect processes using /proc"""
+
+    def get_pids(self) -> List[int]:
+        files = os.listdir("/proc")
+        pids = []
+        for item in files:
+            try:
+                pids.append(int(item))
+            except ValueError:
+                pass
+        return pids
+
+
+class Powershell(Base):
+    """Windows powershell"""
+    # pragma:  cover if windows
+    shell = False
+    cmdline = "powershell -Command \"Get-Process|Format-Table -Property Id\""
+
+
+def get_pids() -> List[int]:
+    if platform.system() == "Windows":  # pragma: cover if windows
+        p = Powershell()
+    elif os.path.isdir("/proc"):  # pragma: not-windows
+        p = Proc()
+    else:  # pragma: not-windows
+        p = Base()
+
+    return p.get_pids()
```

## gitlabemu/references.py

 * *Ordering differences only*

```diff
@@ -1,57 +1,57 @@
-"""Expand !reference tags to values"""
-from typing import Dict, Any, Union
-from .gitlab.types import RESERVED_TOP_KEYS, DEFAULT_JOB_KEYS
-
-from .yamlloader import GitlabReference, GitlabReferenceError
-
-def process_references(baseobj: dict) -> dict:
-    """Expand all jobs with references"""
-    for jobname in baseobj.keys():
-        if jobname not in RESERVED_TOP_KEYS:
-            value = process_block_references(baseobj, baseobj[jobname])
-            baseobj[jobname] = value
-    return baseobj
-
-
-def process_reference_value(baseobj: dict, item: Union[GitlabReference, int, str], depth: int = 0) -> Union[str, int, list]:
-    """Process a reference"""
-    if isinstance(item, GitlabReference):
-        src = baseobj.get(item.job, None)
-        if src is None:
-            raise GitlabReferenceError(f"cannot find referent job for {item}")
-        src = src.get(item.element, None)
-        if src is None:
-            raise GitlabReferenceError(f"cannot find referent key for {item}")
-        if item.value:
-            if not isinstance(src, dict):
-                raise GitlabReferenceError(f"can only reference values from maps: {item}")
-            src = src.get(item.value, None)
-            if src is None:
-                raise GitlabReferenceError(f"cannot find referent value for {item}")
-        return src
-    return item
-
-def process_block_references(baseobj: dict, block: Dict[str, Any], depth: int = 0) -> dict:
-    """In a job, expand all references"""
-    if depth > 9:
-        raise GitlabReferenceError("!references cannot be used more than 10 levels deep")
-
-    for name, value in block.items():
-        if isinstance(value, GitlabReference):
-            block[name] = process_reference_value(baseobj, value, depth + 1)
-        elif isinstance(value, list):
-            value = [process_reference_value(baseobj, x, depth + 1) for x in value]
-            # flatten the list
-            flat_value = []
-            for item in value:
-                if isinstance(item, list):
-                    flat_value.extend(item)
-                else:
-                    flat_value.append(item)
-            block[name] = flat_value
-        if isinstance(value, dict):
-            value = process_block_references(baseobj, value, depth + 1)
-            block[name] = value
-
-    return block
-
+"""Expand !reference tags to values"""
+from typing import Dict, Any, Union
+from .gitlab.types import RESERVED_TOP_KEYS, DEFAULT_JOB_KEYS
+
+from .yamlloader import GitlabReference, GitlabReferenceError
+
+def process_references(baseobj: dict) -> dict:
+    """Expand all jobs with references"""
+    for jobname in baseobj.keys():
+        if jobname not in RESERVED_TOP_KEYS:
+            value = process_block_references(baseobj, baseobj[jobname])
+            baseobj[jobname] = value
+    return baseobj
+
+
+def process_reference_value(baseobj: dict, item: Union[GitlabReference, int, str], depth: int = 0) -> Union[str, int, list]:
+    """Process a reference"""
+    if isinstance(item, GitlabReference):
+        src = baseobj.get(item.job, None)
+        if src is None:
+            raise GitlabReferenceError(f"cannot find referent job for {item}")
+        src = src.get(item.element, None)
+        if src is None:
+            raise GitlabReferenceError(f"cannot find referent key for {item}")
+        if item.value:
+            if not isinstance(src, dict):
+                raise GitlabReferenceError(f"can only reference values from maps: {item}")
+            src = src.get(item.value, None)
+            if src is None:
+                raise GitlabReferenceError(f"cannot find referent value for {item}")
+        return src
+    return item
+
+def process_block_references(baseobj: dict, block: Dict[str, Any], depth: int = 0) -> dict:
+    """In a job, expand all references"""
+    if depth > 9:
+        raise GitlabReferenceError("!references cannot be used more than 10 levels deep")
+
+    for name, value in block.items():
+        if isinstance(value, GitlabReference):
+            block[name] = process_reference_value(baseobj, value, depth + 1)
+        elif isinstance(value, list):
+            value = [process_reference_value(baseobj, x, depth + 1) for x in value]
+            # flatten the list
+            flat_value = []
+            for item in value:
+                if isinstance(item, list):
+                    flat_value.extend(item)
+                else:
+                    flat_value.append(item)
+            block[name] = flat_value
+        if isinstance(value, dict):
+            value = process_block_references(baseobj, value, depth + 1)
+            block[name] = value
+
+    return block
+
```

## gitlabemu/resnamer.py

 * *Ordering differences only*

```diff
@@ -1,34 +1,34 @@
-import os
-from typing import Optional
-import uuid
-from .pstab import get_pids
-
-PID = os.getpid()
-
-
-def generate_resource_name(kind: Optional[str] = None) -> str:
-    """Generate a name related to this instance of gitlab-emulator"""
-    if not kind:
-        kind = "docker"
-    rnd = str(uuid.uuid4())[0:7]
-    return f"gle-{kind}-{PID}-{rnd}"
-
-
-def is_gle_resource(name: str) -> Optional[int]:
-    """Return the PID if this resource was created by gitlab emulator"""
-    if name:
-        parts = name.split("-")
-        if len(parts) == 4:
-            prefix = parts[0]
-            if prefix == "gle":
-                pid = int(parts[2])
-                return pid
-    return None
-
-
-def resource_owner_alive(name: str) -> bool:
-    """Return True if the owner of this resource is still alive"""
-    pid = is_gle_resource(name)
-    if pid is not None:
-        return pid in get_pids()
-    return False
+import os
+from typing import Optional
+import uuid
+from .pstab import get_pids
+
+PID = os.getpid()
+
+
+def generate_resource_name(kind: Optional[str] = None) -> str:
+    """Generate a name related to this instance of gitlab-emulator"""
+    if not kind:
+        kind = "docker"
+    rnd = str(uuid.uuid4())[0:7]
+    return f"gle-{kind}-{PID}-{rnd}"
+
+
+def is_gle_resource(name: str) -> Optional[int]:
+    """Return the PID if this resource was created by gitlab emulator"""
+    if name:
+        parts = name.split("-")
+        if len(parts) == 4:
+            prefix = parts[0]
+            if prefix == "gle":
+                pid = int(parts[2])
+                return pid
+    return None
+
+
+def resource_owner_alive(name: str) -> bool:
+    """Return True if the owner of this resource is still alive"""
+    pid = is_gle_resource(name)
+    if pid is not None:
+        return pid in get_pids()
+    return False
```

## gitlabemu/ruleparser.py

 * *Ordering differences only*

```diff
@@ -1,89 +1,89 @@
-import re
-from typing import Dict, Optional
-
-from antlr4 import InputStream, CommonTokenStream
-from antlr4.Token import CommonToken
-
-from gitlabemu.rules.GitlabRuleParser import GitlabRuleParser
-from gitlabemu.rules.GitlabRuleLexer import GitlabRuleLexer
-from gitlabemu.rules.GitlabRuleVisitor import GitlabRuleVisitor
-
-
-class RuleVisitor(GitlabRuleVisitor):
-    def __init__(self, variables: Optional[Dict[str, str]] = None):
-        self.variables: Dict[str, str] = {}
-        if variables:
-            self.variables.update(variables)
-
-    def get_variable_name(self, symbol: CommonToken):
-        if symbol.type == GitlabRuleLexer.VARIABLE:
-            name = symbol.text[1:]
-            return name
-        return ""
-
-    def resolve_variable(self, symbol: CommonToken):
-        text = symbol.text
-        name = self.get_variable_name(symbol)
-        if name:
-            return self.variables.get(name, "")
-        # strip quotes
-        if text[0] == '"':
-            return text[1:-1]
-        return text
-
-    def visitRegex(self, ctx: GitlabRuleParser.RegexContext):
-        assert len(ctx.children) == 1
-        return ctx.children[0].symbol.text
-
-    def visitVariable(self, ctx: GitlabRuleParser.VariableContext):
-        """Return True if VARNAME is set to anything except the empty string"""
-        assert len(ctx.children) == 1
-        name = self.get_variable_name(ctx.children[0].symbol)
-        return self.variables.get(name, "") != ""
-
-    def visitCompare(self, ctx: GitlabRuleParser.CompareContext):
-        """Compare strings/variables for equality"""
-        assert len(ctx.children) == 3
-        assert ctx.op.type in [GitlabRuleLexer.EQ, GitlabRuleLexer.NE]
-        left = self.resolve_variable(ctx.children[0].symbol)
-        right = self.resolve_variable(ctx.children[2].symbol)
-
-        if ctx.op.type == GitlabRuleLexer.EQ:
-            return left == right
-        return left != right
-
-    def visitMatch(self, ctx: GitlabRuleParser.MatchContext):
-        assert len(ctx.children) == 3
-        assert ctx.op.type in [GitlabRuleLexer.MATCH, GitlabRuleLexer.NMATCH]
-        left = self.resolve_variable(ctx.children[0].symbol)
-        right = self.resolve_variable(ctx.children[2].symbol)
-        if right.startswith("/") and right.endswith("/"):
-            # is a regex
-            patt = re.compile(right[1:-1])
-            matched = patt.search(left)
-            if ctx.op.type == GitlabRuleLexer.MATCH:
-                return matched is not None
-            return matched is None
-        return False
-
-    def visitBoolAnd(self, ctx: GitlabRuleParser.BoolAndContext):
-        left = self.visit(ctx.expr(0))
-        right = self.visit(ctx.expr(1))
-        return left and right
-
-    def visitBoolOr(self, ctx:GitlabRuleParser.BoolOrContext):
-        left = self.visit(ctx.expr(0))
-        right = self.visit(ctx.expr(1))
-        return left or right
-
-    def visitParens(self, ctx: GitlabRuleParser.ParensContext):
-        return self.visit(ctx.expr())
-
-
-def evaluate_rule(rule: str, variables: Dict[str, str]):
-    lexer = GitlabRuleLexer(InputStream(rule))
-    stream = CommonTokenStream(lexer)
-    parser = GitlabRuleParser(stream)
-    tree = parser.expr()
-    visitor = RuleVisitor(variables)
-    return visitor.visit(tree)
+import re
+from typing import Dict, Optional
+
+from antlr4 import InputStream, CommonTokenStream
+from antlr4.Token import CommonToken
+
+from gitlabemu.rules.GitlabRuleParser import GitlabRuleParser
+from gitlabemu.rules.GitlabRuleLexer import GitlabRuleLexer
+from gitlabemu.rules.GitlabRuleVisitor import GitlabRuleVisitor
+
+
+class RuleVisitor(GitlabRuleVisitor):
+    def __init__(self, variables: Optional[Dict[str, str]] = None):
+        self.variables: Dict[str, str] = {}
+        if variables:
+            self.variables.update(variables)
+
+    def get_variable_name(self, symbol: CommonToken):
+        if symbol.type == GitlabRuleLexer.VARIABLE:
+            name = symbol.text[1:]
+            return name
+        return ""
+
+    def resolve_variable(self, symbol: CommonToken):
+        text = symbol.text
+        name = self.get_variable_name(symbol)
+        if name:
+            return self.variables.get(name, "")
+        # strip quotes
+        if text[0] == '"':
+            return text[1:-1]
+        return text
+
+    def visitRegex(self, ctx: GitlabRuleParser.RegexContext):
+        assert len(ctx.children) == 1
+        return ctx.children[0].symbol.text
+
+    def visitVariable(self, ctx: GitlabRuleParser.VariableContext):
+        """Return True if VARNAME is set to anything except the empty string"""
+        assert len(ctx.children) == 1
+        name = self.get_variable_name(ctx.children[0].symbol)
+        return self.variables.get(name, "") != ""
+
+    def visitCompare(self, ctx: GitlabRuleParser.CompareContext):
+        """Compare strings/variables for equality"""
+        assert len(ctx.children) == 3
+        assert ctx.op.type in [GitlabRuleLexer.EQ, GitlabRuleLexer.NE]
+        left = self.resolve_variable(ctx.children[0].symbol)
+        right = self.resolve_variable(ctx.children[2].symbol)
+
+        if ctx.op.type == GitlabRuleLexer.EQ:
+            return left == right
+        return left != right
+
+    def visitMatch(self, ctx: GitlabRuleParser.MatchContext):
+        assert len(ctx.children) == 3
+        assert ctx.op.type in [GitlabRuleLexer.MATCH, GitlabRuleLexer.NMATCH]
+        left = self.resolve_variable(ctx.children[0].symbol)
+        right = self.resolve_variable(ctx.children[2].symbol)
+        if right.startswith("/") and right.endswith("/"):
+            # is a regex
+            patt = re.compile(right[1:-1])
+            matched = patt.search(left)
+            if ctx.op.type == GitlabRuleLexer.MATCH:
+                return matched is not None
+            return matched is None
+        return False
+
+    def visitBoolAnd(self, ctx: GitlabRuleParser.BoolAndContext):
+        left = self.visit(ctx.expr(0))
+        right = self.visit(ctx.expr(1))
+        return left and right
+
+    def visitBoolOr(self, ctx:GitlabRuleParser.BoolOrContext):
+        left = self.visit(ctx.expr(0))
+        right = self.visit(ctx.expr(1))
+        return left or right
+
+    def visitParens(self, ctx: GitlabRuleParser.ParensContext):
+        return self.visit(ctx.expr())
+
+
+def evaluate_rule(rule: str, variables: Dict[str, str]):
+    lexer = GitlabRuleLexer(InputStream(rule))
+    stream = CommonTokenStream(lexer)
+    parser = GitlabRuleParser(stream)
+    tree = parser.expr()
+    visitor = RuleVisitor(variables)
+    return visitor.visit(tree)
```

## gitlabemu/runner.py

 * *Ordering differences only*

```diff
@@ -1,546 +1,546 @@
-import re
-import subprocess
-import sys
-import os
-import argparse
-from typing import Dict, Any, Optional
-
-import gitlabemu.errors
-from . import configloader
-from .docker import DockerJob
-from .gitlab_client_api import PipelineError, PipelineInvalid, PipelineNotFound, posix_cert_fixup
-from .jobs import Job
-from .localfiles import restore_path_ownership
-from .helpers import is_apple, is_linux, is_windows, git_worktree, clean_leftovers, die, note, has_docker
-from .logmsg import debugrule, enable_rule_debug
-from .pipelines import pipelines_cmd, generate_pipeline, print_pipeline_jobs, export_cmd
-from .userconfig import USER_CFG_ENV, get_user_config_context
-from .userconfigdata import UserContext
-from .glp.types import Match
-from .yamlloader import ordered_dump
-
-CONFIG_DEFAULT = ".gitlab-ci.yml"
-
-parser = argparse.ArgumentParser(prog="{} -m gitlabemu".format(os.path.basename(sys.executable)))
-list_mutex = parser.add_mutually_exclusive_group()
-list_mutex.add_argument("--list", "-l", dest="LIST", default=False,
-                        action="store_true",
-                        help="List runnable jobs")
-parser.add_argument("--version", default=False, action="store_true")
-parser.add_argument("--hidden", default=False, action="store_true",
-                    help="Show hidden jobs in --list(those that start with '.')")
-parser.add_argument("--noop", "-n", default=False, action="store_true",
-                    help="Execute a pipeline but print each command instead of running")
-list_mutex.add_argument("--full", "-r", dest="FULL", default=False,
-                        action="store_true",
-                        help="Run any jobs that are dependencies")
-parser.add_argument("--config", "-c", dest="CONFIG", default=CONFIG_DEFAULT,
-                    type=str,
-                    help="Use an alternative gitlab yaml file")
-parser.add_argument("--settings", "-s", dest="USER_SETTINGS", type=str, default=None,
-                    help="Load gitlab emulator settings from a file")
-parser.add_argument("--chdir", "-C", dest="chdir", default=None, type=str, metavar="DIR",
-                    help="Change to this directory before running")
-parser.add_argument("--enter", "-i", dest="enter_shell", default=False, action="store_true",
-                    help="Run an interactive shell but do not run the build"
-                    )
-parser.add_argument("--exec", default=False, action="store_true",
-                    help="Execute the job using 'gitlab-runner exec' if possible. Note, this "
-                         "will not test uncommitted changes and will not produce any output files")
-parser.add_argument("--before-script", "-b", dest="only_before_script", default=False,
-                    action="store_true",
-                    help="Run only the 'before_script' commands"
-                    )
-parser.add_argument("--image", type=str, default=None,
-                    help="Replace the 'image'. Can be used to force running a shell job in a container or to change "
-                         "the container a job uses")
-parser.add_argument("--timeout", type=int, default=None,
-                    help="Set/unset a timeout (minutes). set to 0 to disable timeouts")
-parser.add_argument("--user", "-u", dest="shell_is_user", default=False, action="store_true",
-                    help="Run the interactive shell as the current user instead of root")
-
-parser.add_argument("--shell-on-error", "-e", dest="error_shell", type=str,
-                    help="If a docker job fails, execute this process (can be a shell)")
-
-parser.add_argument("--ignore-docker", dest="no_docker", action="store_true", default=False,
-                    help="If set, run jobs using the local system as a shell job instead of docker"
-                    )
-
-parser.add_argument("--docker-pull", dest="docker_pull", type=str,
-                    choices=["always", "if-not-present", "never"],
-                    default=None,
-                    help="Force set the docker pull policy")
-
-parser.add_argument("--debug-rules", default=False, action="store_true",
-                    help="Print log messages relating to include and job rule processing")
-parser.add_argument("--var", dest="var", type=str, default=[], action="append",
-                    help="Set a pipeline variable, eg DEBUG or DEBUG=1")
-
-parser.add_argument("--revar", dest="revars", metavar="REGEX", type=str, default=[], action="append",
-                    help="Set pipeline variables that match the given regex")
-
-parser.add_argument("--parallel", type=str,
-                    help="Run JOB as one part of a parallel axis (eg 2/4 runs job 2 in a 4 parallel matrix)")
-
-parser.add_argument("--pipeline", default=False, action="store_true",
-                    help="Run JOB on or list pipelines from a gitlab server")
-
-parser.add_argument("--from", type=str, dest="FROM",
-                    metavar="SERVER/PROJECT/PIPELINE",
-                    help="Fetch needed artifacts for the current job from "
-                         "the given pipeline, eg server/grp/project/41881, "
-                         "=master, 23156")
-
-list_mutex.add_argument("--download", default=False, action="store_true",
-                        help="Instead of building JOB, download the artifacts of JOB from gitlab (requires --from)")
-
-list_mutex.add_argument("--export", type=str, dest="export", metavar="EXPORT",
-                        help="Download JOB logs and artifacts to EXPORT/JOBNAME (requires --from)")
-
-parser.add_argument("--completed", default=False, action="store_true",
-                    help="Show all currently completed jobs in the --from pipeline or all "
-                         "completed pipelines with --pipeline --list")
-
-parser.add_argument("--match", default=None, type=Match,
-                    metavar="X=Y",
-                    help="when using --pipeline with --list or --cancel, filter the results with this expression")
-
-parser.add_argument("--insecure", "-k", dest="insecure", default=False, action="store_true",
-                    help="Ignore TLS certificate errors when fetching from remote servers")
-
-list_mutex.add_argument("--clean", dest="clean", default=False, action="store_true",
-                        help="Clean up any leftover docker containers or networks")
-list_mutex.add_argument("--cancel", default=False, action="store_true",
-                        help="Cancel pipelines that match --match x=y, (requires --pipeline)")
-
-
-if is_windows():  # pragma: cover if windows
-    shellgrp = parser.add_mutually_exclusive_group()
-    shellgrp.add_argument("--powershell",
-                          dest="windows_shell",
-                          action="store_const",
-                          const="powershell",
-                          help="Force use of powershell for windows jobs (default)")
-    shellgrp.add_argument("--cmd", default=None,
-                          dest="windows_shell",
-                          action="store_const",
-                          const="cmd",
-                          help="Force use of cmd for windows jobs")
-
-parser.add_argument("JOB", type=str, default=None,
-                    nargs="?",
-                    help="Run this named job")
-
-parser.add_argument("EXTRA_JOBS", type=str,
-                    nargs="*",
-                    help=argparse.SUPPRESS)
-
-
-def apply_user_config(loader: configloader.Loader, is_docker: bool):
-    """
-    Add the user config values to the loader
-    :param loader:
-    :param is_docker:
-    :return:
-    """
-    ctx: UserContext = get_user_config_context()
-    if ".gle-extra_variables" not in loader.config:
-        loader.config[".gle-extra_variables"] = {}
-
-    for name in ctx.variables:
-        loader.config[".gle-extra_variables"][name] = ctx.variables[name]
-
-
-def gitlab_runner_exec(jobobj: Job):
-    """Execute a job locally using 'gitlab-runner exec"""
-    loader = configloader.Loader()
-    loader.config.update(jobobj.copy_config())
-
-    result = generate_pipeline(loader, jobobj.name,
-                               dump_only=True,
-                               use_from=None,
-                               tls_verify=True)
-
-    # ensure that all variables are strings as gitlab-runner exec doesnt like the ints used for KUBERNETES settings
-    def ensure_strings(dictionary: dict):
-        copied = dict(dictionary)
-        for varname in copied:
-            dictionary[varname] = str(copied[varname])
-    if "variables" in result:
-        ensure_strings(result["variables"])
-
-    for name in result:
-        if isinstance(result[name], dict):
-            if "variables" in result[name]:
-                ensure_strings(result[name]["variables"])
-
-    # save the config
-    temp_pipeline_file = os.path.join(os.getcwd(), ".temp-pipeline.yml")
-    try:
-        with open(temp_pipeline_file, "w") as tempcfg:
-            tempcfg.write(ordered_dump(result))
-        cmdline = ["gitlab-runner", "exec"]
-        if isinstance(jobobj, DockerJob):
-            cmdline.append("docker")
-        else:
-            cmdline.append("shell")
-        cmdline.extend(["--cicd-config-file", temp_pipeline_file, jobobj.name])
-
-        subprocess.check_call(cmdline)
-
-    finally:
-        os.unlink(temp_pipeline_file)
-
-
-def execute_job(config: Dict[str, Any],
-                jobname: str,
-                seen=None,
-                recurse=False,
-                use_runner=False,
-                noop=False,
-                options: Optional[Dict[str, Any]] = None,
-                overrides: Optional[Dict[str, Any]] = None,
-                ):
-    """
-    Run a job, optionally run required dependencies
-    :param config: the config dictionary
-    :param jobname: the job to start
-    :param seen: completed jobs are added to this set
-    :param recurse: if True, execute in dependency order
-    :param use_runner: if True, execute using "gitlab-runner exec"
-    :param noop: if True, print instead of execute commands
-    :param options: If given, set attributes on the job before use.
-    :param overrides: If given, replace properties in the top level of a job dictionary.
-    :return:
-    """
-    if seen is None:
-        seen = set()
-    if jobname not in seen:
-        jobobj = configloader.load_job(config, jobname, overrides=overrides)
-        if options:
-            for name in options:
-                setattr(jobobj, name, options[name])
-
-        if recurse:
-            for need in jobobj.dependencies:
-                execute_job(config, need, seen=seen, recurse=True, noop=noop)
-        print(f">>> execute {jobobj.name}:")
-        if noop:
-            if isinstance(jobobj, DockerJob):
-                print(f"image: {jobobj.docker_image}")
-            for envname, envvalue in jobobj.get_envs().items():
-                print(f"setenv {envname}={envvalue}")
-
-            for line in jobobj.before_script + jobobj.script:
-                print(f"script {line}")
-        else:
-            if use_runner:
-                gitlab_runner_exec(jobobj)
-            else:
-                jobobj.run()
-        seen.add(jobname)
-
-
-def do_pipeline(options: argparse.Namespace, loader):
-    """Run/List/Cancel gitlab pipelines in the current project"""
-    matchers = {}
-    if options.completed:
-        matchers["status"] = "success"
-
-    if options.match:
-        matchers[options.match.name] = options.match.value
-
-    elif options.cancel:
-        die("--pipeline --cancel requires --match x=y")
-
-    jobs = []
-    if options.JOB:
-        jobs.append(options.JOB)
-    jobs.extend(options.EXTRA_JOBS)
-
-    note("notice! `gle --pipeline' is deprecated in favor of `glp'")
-    if not jobs:
-        return pipelines_cmd(tls_verify=not options.insecure,
-                             matchers=matchers,
-                             do_cancel=options.cancel,
-                             do_list=options.LIST)
-
-    return generate_pipeline(loader, *jobs,
-                             use_from=options.FROM,
-                             tls_verify=not options.insecure)
-
-
-def do_gitlab_from(options: argparse.Namespace, loader):
-    """Perform actions using a gitlab server artifacts"""
-    from .gitlab_client_api import get_pipeline
-    from .gitlab_client_api import do_gitlab_fetch
-
-    if options.download and not options.FROM:
-        die("--download requires --from PIPELINE")
-    if options.FROM:
-        try:
-            if options.LIST:
-                # print the jobs in the pipeline
-                gitlab, project, pipeline = get_pipeline(options.FROM, secure=not options.insecure)
-                if not pipeline:
-                    raise PipelineInvalid(options.FROM)
-                print_pipeline_jobs(pipeline, completed=options.completed)
-            elif options.export:
-                # export a pipeline
-                note(f"Export full '{options.FROM}' pipeline")
-                export_cmd(options.FROM,
-                           options.export,
-                           tls_verify=not options.insecure,
-                           )
-            elif options.JOB:
-                # download a job, or artifacts needed by a job
-                jobobj: Job = configloader.load_job(loader.config, options.JOB)
-                if options.download:
-                    # download a job's artifacts
-                    if options.parallel:
-                        download_jobs = [f"{options.JOB} {options.parallel}/{jobobj.parallel}"]
-                    else:
-                        download_jobs = [options.JOB]
-                    note(f"Download '{download_jobs[0]}' artifacts")
-                else:
-                    # download jobs needed by a job
-                    note(f"Download artifacts required by '{options.JOB}'")
-                    download_jobs = jobobj.dependencies
-
-                # download what we need
-                outdir = os.getcwd()
-                do_gitlab_fetch(options.FROM,
-                                download_jobs,
-                                tls_verify=not options.insecure,
-                                download_to=outdir)
-            else:
-                die("--from PIPELINE requires JOB or --export")
-        except PipelineNotFound:
-            die(str(PipelineNotFound(options.FROM)))
-        except PipelineError as error:
-            die(str(error))
-
-
-def do_version():
-    """Print the current package version"""
-    try:  # pragma: no cover
-        ver = subprocess.check_output([sys.executable, "-m", "pip", "show", "gitlab-emulator"],
-                                      encoding="utf-8",
-                                      stderr=subprocess.STDOUT)
-        for line in ver.splitlines(keepends=False):
-            if "Version:" in line:
-                words = line.split(":", 1)
-                ver = words[1]
-                break
-    except subprocess.CalledProcessError:
-        ver = "unknown"
-    print(ver.strip())
-    sys.exit(0)
-
-
-def get_loader(variables: Dict[str, str], **kwargs) -> configloader.Loader:
-    loader = configloader.Loader(**kwargs)
-    apply_user_config(loader, is_docker=False)
-    for name in variables:
-        loader.config[".gle-extra_variables"][name] = str(variables[name])
-    return loader
-
-
-def run(args=None):
-    options = parser.parse_args(args)
-    yamlfile = options.CONFIG
-    jobname = options.JOB
-
-    variables = {}
-
-    if options.debug_rules:
-        enable_rule_debug()
-
-    if options.version:
-        do_version()
-
-    if options.clean:
-        clean_leftovers()
-        sys.exit()
-
-    if options.chdir:
-        if not os.path.exists(options.chdir):
-            die(f"Cannot change to {options.chdir}, no such directory")
-        os.chdir(options.chdir)
-
-    if not os.path.exists(yamlfile):
-        note(f"{configloader.DEFAULT_CI_FILE} not found.")
-        find = configloader.find_ci_config(os.getcwd())
-        if find:
-            topdir = os.path.abspath(os.path.dirname(find))
-            note(f"Found config: {find}")
-            die(f"Please re-run from {topdir}")
-        sys.exit(1)
-
-    if options.USER_SETTINGS:
-        os.environ[USER_CFG_ENV] = options.USER_SETTINGS
-
-    for item in options.revars:
-        patt = re.compile(item)
-        for name in os.environ:
-            if patt.search(name):
-                variables[name] = os.environ.get(name)
-
-    for item in options.var:
-        var = item.split("=", 1)
-        if len(var) == 2:
-            name, value = var[0], var[1]
-        else:
-            name = var[0]
-            value = os.environ.get(name, None)
-
-        if value is not None:
-            variables[name] = value
-
-    ctx = get_user_config_context()
-    fullpath = os.path.abspath(yamlfile)
-    rootdir = os.path.dirname(fullpath)
-    os.chdir(rootdir)
-    loader = get_loader(variables)
-    hide_dot_jobs = not options.hidden
-    try:
-        if options.pipeline or options.FROM:
-            loader = get_loader(variables, emulator_variables=False)
-            loader.load(fullpath)
-            with posix_cert_fixup():
-                if options.pipeline:
-                    do_pipeline(options, loader)
-                    return
-
-                if options.FULL and options.parallel:
-                    die("--full and --parallel cannot be used together")
-
-                if options.FROM:
-                    do_gitlab_from(options, loader)
-                    return
-        else:
-            loader.load(fullpath)
-    except gitlabemu.jobs.NoSuchJob as err:
-        die(f"Job error: {err}")
-    except gitlabemu.errors.ConfigLoaderError as err:
-        die(f"Config error: {err}")
-
-    if is_windows():  # pragma: cover if windows
-        windows_shell = "powershell"
-        if ctx.windows.cmd:
-            windows_shell = "cmd"
-        if options.windows_shell:
-            # command line option given, use that
-            windows_shell = options.windows_shell
-        loader.config[".gitlabemu-windows-shell"] = windows_shell
-
-    if options.LIST:
-        for jobname in sorted(loader.get_jobs()):
-            if jobname.startswith(".") and hide_dot_jobs:
-                continue
-            job = loader.load_job(jobname)
-            if job.check_skipped():
-                debugrule(f"{jobname} skipped by rules: {job.skipped_reason}")
-            print(jobname)
-    elif not jobname:
-        parser.print_usage()
-        sys.exit(1)
-    else:
-        jobs = sorted(loader.get_jobs())
-        if jobname not in jobs:
-            die(f"No such job {jobname}")
-        job_options = {}
-
-        if options.parallel:
-            if loader.config[jobname].get("parallel", None) is None:
-                die(f"Job {jobname} is not a parallel enabled job")
-
-            pindex, ptotal = options.parallel.split("/", 1)
-            pindex = int(pindex)
-            ptotal = int(ptotal)
-            if pindex < 1:
-                die("CI_NODE_INDEX must be > 0")
-            if ptotal < 1:
-                die("CI_NODE_TOTAL must be > 1")
-            if pindex > ptotal:
-                die("CI_NODE_INDEX must be <= CI_NODE_TOTAL, (got {}/{})".format(pindex, ptotal))
-
-            loader.config[".gitlabemu-parallel-index"] = pindex
-            loader.config[".gitlabemu-parallel-total"] = ptotal
-
-        fix_ownership = has_docker()
-        if options.no_docker:
-            loader.config["hide_docker"] = True
-            fix_ownership = False
-
-        docker_job = loader.get_docker_image(jobname)
-        apply_docker_config = False
-        if docker_job:
-            apply_docker_config = True
-            if options.docker_pull is not None:
-                job_options["docker_pull_policy"] = options.docker_pull
-            gwt = git_worktree(rootdir)
-            if gwt:  # pragma: no cover
-                note(f"f{rootdir} is a git worktree, adding {gwt} as a docker volume.")
-                # add the real git repo as a docker volume
-                volumes = ctx.docker.runtime_volumes()
-                volumes.append(f"{gwt}:{gwt}:ro")
-                ctx.docker.volumes = volumes
-        else:
-            fix_ownership = False
-
-        overrides = {}
-        if options.image:
-            overrides["image"] = options.image
-        if options.timeout:
-            if options.timeout == 0:
-                overrides["timeout"] = None
-            else:
-                overrides["timeout"] = options.timeout
-
-        apply_user_config(loader, is_docker=apply_docker_config)
-
-        if not is_linux():
-            fix_ownership = False
-
-        if options.enter_shell:
-            if options.FULL:
-                die("-i is not compatible with --full")
-
-        if options.only_before_script:
-            job_options["script"] = []
-            job_options["after_script"] = []
-
-        if options.enter_shell:  # pragma: no cover
-            overrides["timeout"] = None
-            job_options["enter_shell"] = True
-            if not options.only_before_script:
-                job_options["before_script"] = []
-                job_options["script"] = []
-
-        if options.shell_is_user:
-            job_options["shell_is_user"] = True
-        loader.config["ci_config_file"] = os.path.relpath(fullpath, rootdir)
-
-        if options.error_shell:  # pragma: no cover
-            job_options["error_shell"] = [options.error_shell]
-            overrides["timeout"] = None
-        try:
-            executed_jobs = set()
-            execute_job(loader.config, jobname,
-                        seen=executed_jobs,
-                        use_runner=options.exec,
-                        recurse=options.FULL,
-                        noop=options.noop,
-                        options=job_options,
-                        overrides=overrides,
-                        )
-        finally:
-            if not options.noop:
-                if has_docker() and fix_ownership:
-                    if is_linux() or is_apple():  # pragma: cover if posix
-                        if os.getuid() > 0:
-                            note("Fixing up local file ownerships..")
-                            restore_path_ownership(os.getcwd())
-                            note("finished")
-        print("Build complete!")
+import re
+import subprocess
+import sys
+import os
+import argparse
+from typing import Dict, Any, Optional
+
+import gitlabemu.errors
+from . import configloader
+from .docker import DockerJob
+from .gitlab_client_api import PipelineError, PipelineInvalid, PipelineNotFound, posix_cert_fixup
+from .jobs import Job
+from .localfiles import restore_path_ownership
+from .helpers import is_apple, is_linux, is_windows, git_worktree, clean_leftovers, die, note, has_docker
+from .logmsg import debugrule, enable_rule_debug
+from .pipelines import pipelines_cmd, generate_pipeline, print_pipeline_jobs, export_cmd
+from .userconfig import USER_CFG_ENV, get_user_config_context
+from .userconfigdata import UserContext
+from .glp.types import Match
+from .yamlloader import ordered_dump
+
+CONFIG_DEFAULT = ".gitlab-ci.yml"
+
+parser = argparse.ArgumentParser(prog="{} -m gitlabemu".format(os.path.basename(sys.executable)))
+list_mutex = parser.add_mutually_exclusive_group()
+list_mutex.add_argument("--list", "-l", dest="LIST", default=False,
+                        action="store_true",
+                        help="List runnable jobs")
+parser.add_argument("--version", default=False, action="store_true")
+parser.add_argument("--hidden", default=False, action="store_true",
+                    help="Show hidden jobs in --list(those that start with '.')")
+parser.add_argument("--noop", "-n", default=False, action="store_true",
+                    help="Execute a pipeline but print each command instead of running")
+list_mutex.add_argument("--full", "-r", dest="FULL", default=False,
+                        action="store_true",
+                        help="Run any jobs that are dependencies")
+parser.add_argument("--config", "-c", dest="CONFIG", default=CONFIG_DEFAULT,
+                    type=str,
+                    help="Use an alternative gitlab yaml file")
+parser.add_argument("--settings", "-s", dest="USER_SETTINGS", type=str, default=None,
+                    help="Load gitlab emulator settings from a file")
+parser.add_argument("--chdir", "-C", dest="chdir", default=None, type=str, metavar="DIR",
+                    help="Change to this directory before running")
+parser.add_argument("--enter", "-i", dest="enter_shell", default=False, action="store_true",
+                    help="Run an interactive shell but do not run the build"
+                    )
+parser.add_argument("--exec", default=False, action="store_true",
+                    help="Execute the job using 'gitlab-runner exec' if possible. Note, this "
+                         "will not test uncommitted changes and will not produce any output files")
+parser.add_argument("--before-script", "-b", dest="only_before_script", default=False,
+                    action="store_true",
+                    help="Run only the 'before_script' commands"
+                    )
+parser.add_argument("--image", type=str, default=None,
+                    help="Replace the 'image'. Can be used to force running a shell job in a container or to change "
+                         "the container a job uses")
+parser.add_argument("--timeout", type=int, default=None,
+                    help="Set/unset a timeout (minutes). set to 0 to disable timeouts")
+parser.add_argument("--user", "-u", dest="shell_is_user", default=False, action="store_true",
+                    help="Run the interactive shell as the current user instead of root")
+
+parser.add_argument("--shell-on-error", "-e", dest="error_shell", type=str,
+                    help="If a docker job fails, execute this process (can be a shell)")
+
+parser.add_argument("--ignore-docker", dest="no_docker", action="store_true", default=False,
+                    help="If set, run jobs using the local system as a shell job instead of docker"
+                    )
+
+parser.add_argument("--docker-pull", dest="docker_pull", type=str,
+                    choices=["always", "if-not-present", "never"],
+                    default=None,
+                    help="Force set the docker pull policy")
+
+parser.add_argument("--debug-rules", default=False, action="store_true",
+                    help="Print log messages relating to include and job rule processing")
+parser.add_argument("--var", dest="var", type=str, default=[], action="append",
+                    help="Set a pipeline variable, eg DEBUG or DEBUG=1")
+
+parser.add_argument("--revar", dest="revars", metavar="REGEX", type=str, default=[], action="append",
+                    help="Set pipeline variables that match the given regex")
+
+parser.add_argument("--parallel", type=str,
+                    help="Run JOB as one part of a parallel axis (eg 2/4 runs job 2 in a 4 parallel matrix)")
+
+parser.add_argument("--pipeline", default=False, action="store_true",
+                    help="Run JOB on or list pipelines from a gitlab server")
+
+parser.add_argument("--from", type=str, dest="FROM",
+                    metavar="SERVER/PROJECT/PIPELINE",
+                    help="Fetch needed artifacts for the current job from "
+                         "the given pipeline, eg server/grp/project/41881, "
+                         "=master, 23156")
+
+list_mutex.add_argument("--download", default=False, action="store_true",
+                        help="Instead of building JOB, download the artifacts of JOB from gitlab (requires --from)")
+
+list_mutex.add_argument("--export", type=str, dest="export", metavar="EXPORT",
+                        help="Download JOB logs and artifacts to EXPORT/JOBNAME (requires --from)")
+
+parser.add_argument("--completed", default=False, action="store_true",
+                    help="Show all currently completed jobs in the --from pipeline or all "
+                         "completed pipelines with --pipeline --list")
+
+parser.add_argument("--match", default=None, type=Match,
+                    metavar="X=Y",
+                    help="when using --pipeline with --list or --cancel, filter the results with this expression")
+
+parser.add_argument("--insecure", "-k", dest="insecure", default=False, action="store_true",
+                    help="Ignore TLS certificate errors when fetching from remote servers")
+
+list_mutex.add_argument("--clean", dest="clean", default=False, action="store_true",
+                        help="Clean up any leftover docker containers or networks")
+list_mutex.add_argument("--cancel", default=False, action="store_true",
+                        help="Cancel pipelines that match --match x=y, (requires --pipeline)")
+
+
+if is_windows():  # pragma: cover if windows
+    shellgrp = parser.add_mutually_exclusive_group()
+    shellgrp.add_argument("--powershell",
+                          dest="windows_shell",
+                          action="store_const",
+                          const="powershell",
+                          help="Force use of powershell for windows jobs (default)")
+    shellgrp.add_argument("--cmd", default=None,
+                          dest="windows_shell",
+                          action="store_const",
+                          const="cmd",
+                          help="Force use of cmd for windows jobs")
+
+parser.add_argument("JOB", type=str, default=None,
+                    nargs="?",
+                    help="Run this named job")
+
+parser.add_argument("EXTRA_JOBS", type=str,
+                    nargs="*",
+                    help=argparse.SUPPRESS)
+
+
+def apply_user_config(loader: configloader.Loader, is_docker: bool):
+    """
+    Add the user config values to the loader
+    :param loader:
+    :param is_docker:
+    :return:
+    """
+    ctx: UserContext = get_user_config_context()
+    if ".gle-extra_variables" not in loader.config:
+        loader.config[".gle-extra_variables"] = {}
+
+    for name in ctx.variables:
+        loader.config[".gle-extra_variables"][name] = ctx.variables[name]
+
+
+def gitlab_runner_exec(jobobj: Job):
+    """Execute a job locally using 'gitlab-runner exec"""
+    loader = configloader.Loader()
+    loader.config.update(jobobj.copy_config())
+
+    result = generate_pipeline(loader, jobobj.name,
+                               dump_only=True,
+                               use_from=None,
+                               tls_verify=True)
+
+    # ensure that all variables are strings as gitlab-runner exec doesnt like the ints used for KUBERNETES settings
+    def ensure_strings(dictionary: dict):
+        copied = dict(dictionary)
+        for varname in copied:
+            dictionary[varname] = str(copied[varname])
+    if "variables" in result:
+        ensure_strings(result["variables"])
+
+    for name in result:
+        if isinstance(result[name], dict):
+            if "variables" in result[name]:
+                ensure_strings(result[name]["variables"])
+
+    # save the config
+    temp_pipeline_file = os.path.join(os.getcwd(), ".temp-pipeline.yml")
+    try:
+        with open(temp_pipeline_file, "w") as tempcfg:
+            tempcfg.write(ordered_dump(result))
+        cmdline = ["gitlab-runner", "exec"]
+        if isinstance(jobobj, DockerJob):
+            cmdline.append("docker")
+        else:
+            cmdline.append("shell")
+        cmdline.extend(["--cicd-config-file", temp_pipeline_file, jobobj.name])
+
+        subprocess.check_call(cmdline)
+
+    finally:
+        os.unlink(temp_pipeline_file)
+
+
+def execute_job(config: Dict[str, Any],
+                jobname: str,
+                seen=None,
+                recurse=False,
+                use_runner=False,
+                noop=False,
+                options: Optional[Dict[str, Any]] = None,
+                overrides: Optional[Dict[str, Any]] = None,
+                ):
+    """
+    Run a job, optionally run required dependencies
+    :param config: the config dictionary
+    :param jobname: the job to start
+    :param seen: completed jobs are added to this set
+    :param recurse: if True, execute in dependency order
+    :param use_runner: if True, execute using "gitlab-runner exec"
+    :param noop: if True, print instead of execute commands
+    :param options: If given, set attributes on the job before use.
+    :param overrides: If given, replace properties in the top level of a job dictionary.
+    :return:
+    """
+    if seen is None:
+        seen = set()
+    if jobname not in seen:
+        jobobj = configloader.load_job(config, jobname, overrides=overrides)
+        if options:
+            for name in options:
+                setattr(jobobj, name, options[name])
+
+        if recurse:
+            for need in jobobj.dependencies:
+                execute_job(config, need, seen=seen, recurse=True, noop=noop)
+        print(f">>> execute {jobobj.name}:")
+        if noop:
+            if isinstance(jobobj, DockerJob):
+                print(f"image: {jobobj.docker_image}")
+            for envname, envvalue in jobobj.get_envs().items():
+                print(f"setenv {envname}={envvalue}")
+
+            for line in jobobj.before_script + jobobj.script:
+                print(f"script {line}")
+        else:
+            if use_runner:
+                gitlab_runner_exec(jobobj)
+            else:
+                jobobj.run()
+        seen.add(jobname)
+
+
+def do_pipeline(options: argparse.Namespace, loader):
+    """Run/List/Cancel gitlab pipelines in the current project"""
+    matchers = {}
+    if options.completed:
+        matchers["status"] = "success"
+
+    if options.match:
+        matchers[options.match.name] = options.match.value
+
+    elif options.cancel:
+        die("--pipeline --cancel requires --match x=y")
+
+    jobs = []
+    if options.JOB:
+        jobs.append(options.JOB)
+    jobs.extend(options.EXTRA_JOBS)
+
+    note("notice! `gle --pipeline' is deprecated in favor of `glp'")
+    if not jobs:
+        return pipelines_cmd(tls_verify=not options.insecure,
+                             matchers=matchers,
+                             do_cancel=options.cancel,
+                             do_list=options.LIST)
+
+    return generate_pipeline(loader, *jobs,
+                             use_from=options.FROM,
+                             tls_verify=not options.insecure)
+
+
+def do_gitlab_from(options: argparse.Namespace, loader):
+    """Perform actions using a gitlab server artifacts"""
+    from .gitlab_client_api import get_pipeline
+    from .gitlab_client_api import do_gitlab_fetch
+
+    if options.download and not options.FROM:
+        die("--download requires --from PIPELINE")
+    if options.FROM:
+        try:
+            if options.LIST:
+                # print the jobs in the pipeline
+                gitlab, project, pipeline = get_pipeline(options.FROM, secure=not options.insecure)
+                if not pipeline:
+                    raise PipelineInvalid(options.FROM)
+                print_pipeline_jobs(pipeline, completed=options.completed)
+            elif options.export:
+                # export a pipeline
+                note(f"Export full '{options.FROM}' pipeline")
+                export_cmd(options.FROM,
+                           options.export,
+                           tls_verify=not options.insecure,
+                           )
+            elif options.JOB:
+                # download a job, or artifacts needed by a job
+                jobobj: Job = configloader.load_job(loader.config, options.JOB)
+                if options.download:
+                    # download a job's artifacts
+                    if options.parallel:
+                        download_jobs = [f"{options.JOB} {options.parallel}/{jobobj.parallel}"]
+                    else:
+                        download_jobs = [options.JOB]
+                    note(f"Download '{download_jobs[0]}' artifacts")
+                else:
+                    # download jobs needed by a job
+                    note(f"Download artifacts required by '{options.JOB}'")
+                    download_jobs = jobobj.dependencies
+
+                # download what we need
+                outdir = os.getcwd()
+                do_gitlab_fetch(options.FROM,
+                                download_jobs,
+                                tls_verify=not options.insecure,
+                                download_to=outdir)
+            else:
+                die("--from PIPELINE requires JOB or --export")
+        except PipelineNotFound:
+            die(str(PipelineNotFound(options.FROM)))
+        except PipelineError as error:
+            die(str(error))
+
+
+def do_version():
+    """Print the current package version"""
+    try:  # pragma: no cover
+        ver = subprocess.check_output([sys.executable, "-m", "pip", "show", "gitlab-emulator"],
+                                      encoding="utf-8",
+                                      stderr=subprocess.STDOUT)
+        for line in ver.splitlines(keepends=False):
+            if "Version:" in line:
+                words = line.split(":", 1)
+                ver = words[1]
+                break
+    except subprocess.CalledProcessError:
+        ver = "unknown"
+    print(ver.strip())
+    sys.exit(0)
+
+
+def get_loader(variables: Dict[str, str], **kwargs) -> configloader.Loader:
+    loader = configloader.Loader(**kwargs)
+    apply_user_config(loader, is_docker=False)
+    for name in variables:
+        loader.config[".gle-extra_variables"][name] = str(variables[name])
+    return loader
+
+
+def run(args=None):
+    options = parser.parse_args(args)
+    yamlfile = options.CONFIG
+    jobname = options.JOB
+
+    variables = {}
+
+    if options.debug_rules:
+        enable_rule_debug()
+
+    if options.version:
+        do_version()
+
+    if options.clean:
+        clean_leftovers()
+        sys.exit()
+
+    if options.chdir:
+        if not os.path.exists(options.chdir):
+            die(f"Cannot change to {options.chdir}, no such directory")
+        os.chdir(options.chdir)
+
+    if not os.path.exists(yamlfile):
+        note(f"{configloader.DEFAULT_CI_FILE} not found.")
+        find = configloader.find_ci_config(os.getcwd())
+        if find:
+            topdir = os.path.abspath(os.path.dirname(find))
+            note(f"Found config: {find}")
+            die(f"Please re-run from {topdir}")
+        sys.exit(1)
+
+    if options.USER_SETTINGS:
+        os.environ[USER_CFG_ENV] = options.USER_SETTINGS
+
+    for item in options.revars:
+        patt = re.compile(item)
+        for name in os.environ:
+            if patt.search(name):
+                variables[name] = os.environ.get(name)
+
+    for item in options.var:
+        var = item.split("=", 1)
+        if len(var) == 2:
+            name, value = var[0], var[1]
+        else:
+            name = var[0]
+            value = os.environ.get(name, None)
+
+        if value is not None:
+            variables[name] = value
+
+    ctx = get_user_config_context()
+    fullpath = os.path.abspath(yamlfile)
+    rootdir = os.path.dirname(fullpath)
+    os.chdir(rootdir)
+    loader = get_loader(variables)
+    hide_dot_jobs = not options.hidden
+    try:
+        if options.pipeline or options.FROM:
+            loader = get_loader(variables, emulator_variables=False)
+            loader.load(fullpath)
+            with posix_cert_fixup():
+                if options.pipeline:
+                    do_pipeline(options, loader)
+                    return
+
+                if options.FULL and options.parallel:
+                    die("--full and --parallel cannot be used together")
+
+                if options.FROM:
+                    do_gitlab_from(options, loader)
+                    return
+        else:
+            loader.load(fullpath)
+    except gitlabemu.jobs.NoSuchJob as err:
+        die(f"Job error: {err}")
+    except gitlabemu.errors.ConfigLoaderError as err:
+        die(f"Config error: {err}")
+
+    if is_windows():  # pragma: cover if windows
+        windows_shell = "powershell"
+        if ctx.windows.cmd:
+            windows_shell = "cmd"
+        if options.windows_shell:
+            # command line option given, use that
+            windows_shell = options.windows_shell
+        loader.config[".gitlabemu-windows-shell"] = windows_shell
+
+    if options.LIST:
+        for jobname in sorted(loader.get_jobs()):
+            if jobname.startswith(".") and hide_dot_jobs:
+                continue
+            job = loader.load_job(jobname)
+            if job.check_skipped():
+                debugrule(f"{jobname} skipped by rules: {job.skipped_reason}")
+            print(jobname)
+    elif not jobname:
+        parser.print_usage()
+        sys.exit(1)
+    else:
+        jobs = sorted(loader.get_jobs())
+        if jobname not in jobs:
+            die(f"No such job {jobname}")
+        job_options = {}
+
+        if options.parallel:
+            if loader.config[jobname].get("parallel", None) is None:
+                die(f"Job {jobname} is not a parallel enabled job")
+
+            pindex, ptotal = options.parallel.split("/", 1)
+            pindex = int(pindex)
+            ptotal = int(ptotal)
+            if pindex < 1:
+                die("CI_NODE_INDEX must be > 0")
+            if ptotal < 1:
+                die("CI_NODE_TOTAL must be > 1")
+            if pindex > ptotal:
+                die("CI_NODE_INDEX must be <= CI_NODE_TOTAL, (got {}/{})".format(pindex, ptotal))
+
+            loader.config[".gitlabemu-parallel-index"] = pindex
+            loader.config[".gitlabemu-parallel-total"] = ptotal
+
+        fix_ownership = has_docker()
+        if options.no_docker:
+            loader.config["hide_docker"] = True
+            fix_ownership = False
+
+        docker_job = loader.get_docker_image(jobname)
+        apply_docker_config = False
+        if docker_job:
+            apply_docker_config = True
+            if options.docker_pull is not None:
+                job_options["docker_pull_policy"] = options.docker_pull
+            gwt = git_worktree(rootdir)
+            if gwt:  # pragma: no cover
+                note(f"f{rootdir} is a git worktree, adding {gwt} as a docker volume.")
+                # add the real git repo as a docker volume
+                volumes = ctx.docker.runtime_volumes()
+                volumes.append(f"{gwt}:{gwt}:ro")
+                ctx.docker.volumes = volumes
+        else:
+            fix_ownership = False
+
+        overrides = {}
+        if options.image:
+            overrides["image"] = options.image
+        if options.timeout:
+            if options.timeout == 0:
+                overrides["timeout"] = None
+            else:
+                overrides["timeout"] = options.timeout
+
+        apply_user_config(loader, is_docker=apply_docker_config)
+
+        if not is_linux():
+            fix_ownership = False
+
+        if options.enter_shell:
+            if options.FULL:
+                die("-i is not compatible with --full")
+
+        if options.only_before_script:
+            job_options["script"] = []
+            job_options["after_script"] = []
+
+        if options.enter_shell:  # pragma: no cover
+            overrides["timeout"] = None
+            job_options["enter_shell"] = True
+            if not options.only_before_script:
+                job_options["before_script"] = []
+                job_options["script"] = []
+
+        if options.shell_is_user:
+            job_options["shell_is_user"] = True
+        loader.config["ci_config_file"] = os.path.relpath(fullpath, rootdir)
+
+        if options.error_shell:  # pragma: no cover
+            job_options["error_shell"] = [options.error_shell]
+            overrides["timeout"] = None
+        try:
+            executed_jobs = set()
+            execute_job(loader.config, jobname,
+                        seen=executed_jobs,
+                        use_runner=options.exec,
+                        recurse=options.FULL,
+                        noop=options.noop,
+                        options=job_options,
+                        overrides=overrides,
+                        )
+        finally:
+            if not options.noop:
+                if has_docker() and fix_ownership:
+                    if is_linux() or is_apple():  # pragma: cover if posix
+                        if os.getuid() > 0:
+                            note("Fixing up local file ownerships..")
+                            restore_path_ownership(os.getcwd())
+                            note("finished")
+        print("Build complete!")
```

## gitlabemu/userconfig.py

 * *Ordering differences only*

```diff
@@ -1,38 +1,38 @@
-import os
-from typing import Optional
-from .userconfigdata import UserConfigFile, UserContext
-from .logmsg import fatal
-
-USER_CFG_ENV = "GLE_CONFIG"
-USER_CFG_DIR = os.environ.get("LOCALAPPDATA", os.environ.get("HOME", os.getcwd()))
-USER_CFG_DEFAULT = os.path.join(USER_CFG_DIR, ".gle", "emulator.yml")
-
-
-def get_user_config_path() -> str:
-    cfg = os.environ.get(USER_CFG_ENV, None)
-    if not cfg:
-        cfg = USER_CFG_DEFAULT
-    return cfg
-
-
-def get_user_config(filename: Optional[str] = None) -> UserConfigFile:
-    config = UserConfigFile()
-    if filename is None:
-        filename = get_user_config_path()
-    config.load(filename)
-    return config
-
-
-def get_user_config_context() -> UserContext:
-    cfg = get_user_config()
-    return cfg.contexts[cfg.current_context]
-
-
-def get_current_user_context() -> str:
-    """Get the currently set context name"""
-    current_context = os.getenv("GLE_CONTEXT", None)
-    if current_context == "current_context":
-        fatal("'current_context' is not allowed for GLE_CONFIG")
-    if current_context is None:
-        current_context = get_user_config().current_context
-    return current_context
+import os
+from typing import Optional
+from .userconfigdata import UserConfigFile, UserContext
+from .logmsg import fatal
+
+USER_CFG_ENV = "GLE_CONFIG"
+USER_CFG_DIR = os.environ.get("LOCALAPPDATA", os.environ.get("HOME", os.getcwd()))
+USER_CFG_DEFAULT = os.path.join(USER_CFG_DIR, ".gle", "emulator.yml")
+
+
+def get_user_config_path() -> str:
+    cfg = os.environ.get(USER_CFG_ENV, None)
+    if not cfg:
+        cfg = USER_CFG_DEFAULT
+    return cfg
+
+
+def get_user_config(filename: Optional[str] = None) -> UserConfigFile:
+    config = UserConfigFile()
+    if filename is None:
+        filename = get_user_config_path()
+    config.load(filename)
+    return config
+
+
+def get_user_config_context() -> UserContext:
+    cfg = get_user_config()
+    return cfg.contexts[cfg.current_context]
+
+
+def get_current_user_context() -> str:
+    """Get the currently set context name"""
+    current_context = os.getenv("GLE_CONTEXT", None)
+    if current_context == "current_context":
+        fatal("'current_context' is not allowed for GLE_CONFIG")
+    if current_context is None:
+        current_context = get_user_config().current_context
+    return current_context
```

## gitlabemu/userconfigdata.py

 * *Ordering differences only*

```diff
@@ -1,420 +1,420 @@
-import abc
-import os.path
-from typing import Optional, Dict, List, cast
-from yaml import safe_dump, safe_load
-
-from .helpers import plausible_docker_volume, DockerVolume, is_windows, has_docker, notice
-from .logmsg import fatal, debug
-
-DEFAULT_CONTEXT = "emulator"
-FORBIDDEN_CONTEXT_NAMES = [
-    "current_context",
-    # reserved
-    "version",
-]
-DEFAULT_GITLAB_VERSION = "15.7"
-DEFAULT_DOCKER_CLI = "docker"
-
-EXECUTOR_SHELL = "shell"
-EXECUTOR_DOCKER = "docker"
-EXECUTOR_DOCKER_WINDOWS = "docker-windows"
-RUNNER_EXECUTOR_TYPES = [EXECUTOR_SHELL, EXECUTOR_DOCKER, EXECUTOR_DOCKER_WINDOWS]
-
-SHELL_SH = "sh"
-SHELL_BASH = "bash"
-SHELL_POWERSHELL = "powershell"
-RUNNER_SHELL_SHELLS = [SHELL_SH, SHELL_BASH, SHELL_POWERSHELL]
-
-
-class ToYaml(abc.ABC):
-
-    @property
-    def serialize_empty(self) -> bool:
-        return False
-
-    def to_dict(self) -> dict:
-        res = {}
-        for name in self.yaml_keys:
-            assert hasattr(self, name), name
-            value = getattr(self, name)
-            # omit empty lists, dicts and None values
-            if isinstance(value, ToYaml):
-                dict_value = value.to_dict()
-                if dict_value or value.serialize_empty:
-                    res[name] = dict_value
-            elif isinstance(value, dict):
-                if len(value):
-                    dictvalue = {}
-                    for item_key, item_value in value.items():
-                        if isinstance(item_value, ToYaml):
-                            dictvalue[item_key] = item_value.to_dict()
-                        else:
-                            dictvalue[item_key] = item_value
-                    res[name] = dictvalue
-            elif isinstance(value, list):
-                if len(value):
-                    listvalue = []
-                    for item_value in value:
-                        if isinstance(item_value, ToYaml):
-                            listvalue.append(item_value.to_dict())
-                        else:
-                            listvalue.append(item_value)
-                    res[name] = listvalue
-            elif value is not None:
-                res[name] = value
-        return res
-
-    @property
-    def yaml_keys(self) -> List[str]:
-        return []
-
-    def populate(self, data: dict):
-        for name, value in data.items():
-            if name in self.yaml_keys:
-                if hasattr(self, name):
-                    setattr(self, name, value)
-        return self
-
-    def setattrs_from_dict(self, data: dict, *props) -> None:
-        for prop in props:
-            if prop in data:
-                if hasattr(self, prop):
-                    setattr(self, prop, data.get(prop))
-
-
-class GitlabServer(ToYaml):
-    def __init__(self):
-        self.name = None
-        self.server = None
-        self.token = None
-        self.tls_verify = True
-
-    @property
-    def yaml_keys(self) -> List[str]:
-        return ["name", "server", "token", "tls_verify"]
-
-    def populate(self, data):
-        self.setattrs_from_dict(data, *self.yaml_keys)
-        return self
-
-
-class GitlabConfiguration(ToYaml):
-    def __init__(self):
-        self.version = DEFAULT_GITLAB_VERSION
-        self.servers = []
-
-    def add(self, name: str, url: str, token: str, tls_verify: bool):
-        server = GitlabServer()
-        server.tls_verify = tls_verify
-        server.token = token
-        server.server = url
-        server.name = name
-        self.servers.append(server)
-        return server
-
-    @property
-    def yaml_keys(self) -> List[str]:
-        return ["version", "servers"]
-
-    def populate(self, data):
-        self.version = str(data.get("version", DEFAULT_GITLAB_VERSION))
-        for item in data.get("servers", []):
-            server = GitlabServer()
-            server.populate(item)
-            self.servers.append(server)
-        return self
-
-
-class VariablesConfiguration(ToYaml):
-    def __init__(self):
-        self.variables = dict()
-
-    @property
-    def yaml_keys(self) -> List[str]:
-        return ["variables"]
-
-    def populate(self, data: dict):
-        self.setattrs_from_dict(data, "variables")
-        return self
-
-
-class VolumesMixin:
-    def runtime_volumes(self) -> List[str]:
-        volumes = os.getenv("GLE_DOCKER_VOLUMES", None)
-        if volumes is not None:
-            volumes = volumes.split(",")
-        else:
-            volumes = getattr(self, "volumes")
-        return list(volumes)
-
-    def add_volume(self, text: str) -> DockerVolume:
-        """Add a docker volume mount point"""
-        volume = plausible_docker_volume(text)
-        if volume:
-            # remove this mount point if it is already used
-            self.remove_volume(volume.mount)
-            getattr(self, "volumes").append(str(volume))
-        return volume
-
-    def remove_volume(self, mount: str):
-        """Remove a docker volume mount point"""
-        possible = plausible_docker_volume(mount)
-        if possible:
-            mount = possible.mount
-        keep_volumes = []
-        volumes = getattr(self, "volumes")
-        for item in volumes:
-            volume = plausible_docker_volume(item)
-            if volume.mount == mount:
-                continue
-            keep_volumes.append(str(volume))
-        setattr(self, "volumes", list(set(keep_volumes)))
-
-
-class DockerConfiguration(VariablesConfiguration, VolumesMixin):
-    def __init__(self):
-        self.privileged = True
-        self.docker_cli = DEFAULT_DOCKER_CLI
-        super(DockerConfiguration, self).__init__()
-        self.volumes = []
-
-    @property
-    def yaml_keys(self) -> List[str]:
-        return super().yaml_keys + ["volumes", "privileged", "docker_cli"]
-
-    def populate(self, data: dict):
-        super(DockerConfiguration, self).populate(data)
-        self.setattrs_from_dict(data, "volumes", "privileged", "docker_cli")
-        # validate the volumes
-        self.validate()
-        return self
-
-    def validate(self):
-        for volume in self.volumes:
-            if plausible_docker_volume(volume) is None:
-                fatal(f"Unable to parse docker volume string '{volume}'")
-
-
-class WindowsConfiguration(ToYaml):
-    def __init__(self):
-        self.cmd = False
-
-    @property
-    def yaml_keys(self) -> List[str]:
-        return ["cmd"]
-
-    def populate(self, data: dict):
-        self.setattrs_from_dict(data, "cmd")
-        return self
-
-
-class UserContext(VariablesConfiguration):
-    def __init__(self):
-        super(UserContext, self).__init__()
-        self.windows = WindowsConfiguration()
-        self.docker = DockerConfiguration()
-        self.local = VariablesConfiguration()
-        self.gitlab = GitlabConfiguration()
-        self.filename: Optional[str] = None
-        self.runners: List[GleRunnerConfig] = []
-
-    def remove_runner(self, name: str):
-        self.runners = [x for x in self.runners if x.name != name]
-
-    def save_runner(self, runner: "GleRunnerConfig"):
-        exists = self.get_runner(runner.name)
-        if exists:
-            self.remove_runner(runner.name)
-        self.runners.append(runner)
-
-    def find_runner(self,
-                    image: bool = False,
-                    tags: Optional[List[str]] = None,
-                    ) -> Optional["GleRunnerConfig"]:
-        """Find a runner configuration for the given image+tag combo"""
-        if tags is None:
-            tags = []
-
-        tagset = set(tags)
-        debug(f"find runner for image={image}, tags={tags}")
-        for runner in self.runners + self.builtin_runners():
-            if image and runner.executor not in [EXECUTOR_DOCKER, EXECUTOR_DOCKER_WINDOWS]:
-                continue
-            if not image:
-                if runner.executor in [EXECUTOR_DOCKER, EXECUTOR_DOCKER_WINDOWS]:
-                    continue
-            debug(f"considering runner {runner}")
-            if len(tagset) == 0:
-                if runner.run_untagged:
-                    debug(f"matching {runner} - allow untagged")
-                    return runner
-            else:
-                runner_tagset = set(runner.tags)
-                if tagset.issubset(runner_tagset):
-                    debug(f"matching {runner} - tags {tagset}")
-                    return runner
-
-        # nothing matched, return the defaults but warn
-        if image:
-            runner = self.get_runner("default-docker", builtins=True)
-        else:
-            runner = self.get_runner("default-shell", builtins=True)
-        debug(f"matched default {runner}")
-        return runner
-
-    def get_runner(self, name: str, builtins=False) -> Optional["GleRunnerConfig"]:
-        runners = list(self.runners)
-        if builtins:
-            runners.extend(self.builtin_runners())
-        for runner in runners:
-            if runner.name == name:
-                return runner
-        return None
-
-    def can_add_name(self, name: str) -> bool:
-        return self.get_runner(name, builtins=True) is None
-
-    def builtin_runners(self) -> List["GleRunnerConfig"]:
-        ret = []
-        if has_docker():
-            builtin_docker = GleRunnerConfig()
-            builtin_docker.name = "default-docker"
-            builtin_docker.is_builtin = True
-            builtin_docker.executor = "docker"
-            builtin_docker.docker = DockerExecutorConfig()
-            builtin_docker.docker.privileged = self.docker.privileged
-            builtin_docker.run_untagged = True
-            builtin_docker.docker.volumes = list(self.docker.runtime_volumes())
-            builtin_docker.environment = dict(self.docker.variables)
-            ret.append(builtin_docker)
-        builtin_shell = GleRunnerConfig()
-        builtin_shell.executor = "shell"
-        builtin_shell.name = "default-shell"
-        builtin_shell.shell = SHELL_BASH
-        if is_windows():  # pragma: cover if windows
-            builtin_shell.shell = SHELL_POWERSHELL
-        ret.append(builtin_shell)
-        return ret
-
-    @property
-    def yaml_keys(self) -> List[str]:
-        return super().yaml_keys + ["gitlab", "docker", "local", "windows", "runners"]
-                
-    def populate(self, data: dict) -> None:
-        super(UserContext, self).populate(data)
-        for name in ["windows", "docker", "gitlab", "local"]:
-            element = getattr(self, name)
-            element.populate(data.get(name, {}))
-        runners = data.get("runners", [])
-        self.runners = [GleRunnerConfig().populate(x) for x in runners]
-
-
-class UserConfigFile(ToYaml):
-    def __init__(self):
-        self.current_context: Optional[str] = None
-        self.contexts: Dict[str, UserContext] = {}
-        self.filename: Optional[str] = None
-
-    def to_dict(self) -> dict:
-        res = {
-            "current_context": self.current_context
-        }
-        for name, ctx in self.contexts.items():
-            res[name] = ctx.to_dict()
-        return res
-
-    def load(self, filename: str) -> None:
-        self.filename = os.path.abspath(filename)
-        data = {}
-        if os.path.exists(filename):
-            with open(self.filename, "r") as yfile:
-                data: dict = safe_load(yfile)
-        if data is None:
-            # the file was empty?
-            data = {}
-        self.populate(data)
-
-    def populate(self, data: dict):
-        self.current_context = data.get("current_context", DEFAULT_CONTEXT)
-        for name in data.keys():
-            if name not in FORBIDDEN_CONTEXT_NAMES:
-                self.contexts[name] = UserContext()
-                self.contexts[name].populate(data.get(name, {}))
-
-        if self.current_context not in self.contexts:
-            self.contexts[self.current_context] = UserContext()
-        return self
-
-    def save(self, filename: Optional[str] = None):
-        if filename is None:
-            filename = self.filename
-
-        if filename and os.path.basename(filename):
-            self.filename = os.path.abspath(filename)
-            filename = self.filename
-        else:
-            filename = None
-
-        if filename:
-            folder = os.path.dirname(os.path.abspath(filename))
-            if not os.path.exists(folder):
-                os.makedirs(folder)
-
-            with open(filename, "w") as yfile:
-                data = self.to_dict()
-                safe_dump(data, yfile,
-                          width=120,
-                          indent=2,
-                          default_flow_style=False)
-        return filename
-
-
-class DockerExecutorConfig(ToYaml, VolumesMixin):
-
-    def __init__(self):
-        self.privileged = False
-        self.image: str = "alpine:latest"
-        self.volumes: List[str] = []
-        self.cap_add: str = ""
-        self.mac_address: str = ""
-        self.docker_cli = DEFAULT_DOCKER_CLI
-
-    @property
-    def yaml_keys(self) -> List[str]:
-        return ["privileged",
-                "image",
-                "volumes",
-                "cap_add",
-                "mac_address",
-                "docker_cli",
-                ]
-
-
-class GleRunnerConfig(ToYaml):
-
-    def __init__(self):
-        self.name: str = "local"
-        self.tags: List[str] = []
-        self.run_untagged = False
-        self.executor: str = "docker"
-        self.shell: str = SHELL_POWERSHELL if is_windows() else SHELL_BASH
-        self.docker: Optional[DockerExecutorConfig] = None
-        self.pre_build_script: str = ""
-        self.environment: Dict[str, str] = {}
-        self.is_builtin: bool = False
-
-    def __str__(self):
-        return f"{self.executor}-runner {self.name}"
-
-    @property
-    def yaml_keys(self) -> List[str]:
-        keys = ["name", "tags", "run_untagged", "executor", "environment", "shell", "pre_build_script"]
-        if self.executor == "docker":
-            keys.append("docker")
-        return keys
-
-    def populate(self, data: dict):
-        super().populate(data)
-        if self.docker is not None:
-            self.docker = DockerExecutorConfig().populate(cast(dict, self.docker))
-        return self
+import abc
+import os.path
+from typing import Optional, Dict, List, cast
+from yaml import safe_dump, safe_load
+
+from .helpers import plausible_docker_volume, DockerVolume, is_windows, has_docker, notice
+from .logmsg import fatal, debug
+
+DEFAULT_CONTEXT = "emulator"
+FORBIDDEN_CONTEXT_NAMES = [
+    "current_context",
+    # reserved
+    "version",
+]
+DEFAULT_GITLAB_VERSION = "15.7"
+DEFAULT_DOCKER_CLI = "docker"
+
+EXECUTOR_SHELL = "shell"
+EXECUTOR_DOCKER = "docker"
+EXECUTOR_DOCKER_WINDOWS = "docker-windows"
+RUNNER_EXECUTOR_TYPES = [EXECUTOR_SHELL, EXECUTOR_DOCKER, EXECUTOR_DOCKER_WINDOWS]
+
+SHELL_SH = "sh"
+SHELL_BASH = "bash"
+SHELL_POWERSHELL = "powershell"
+RUNNER_SHELL_SHELLS = [SHELL_SH, SHELL_BASH, SHELL_POWERSHELL]
+
+
+class ToYaml(abc.ABC):
+
+    @property
+    def serialize_empty(self) -> bool:
+        return False
+
+    def to_dict(self) -> dict:
+        res = {}
+        for name in self.yaml_keys:
+            assert hasattr(self, name), name
+            value = getattr(self, name)
+            # omit empty lists, dicts and None values
+            if isinstance(value, ToYaml):
+                dict_value = value.to_dict()
+                if dict_value or value.serialize_empty:
+                    res[name] = dict_value
+            elif isinstance(value, dict):
+                if len(value):
+                    dictvalue = {}
+                    for item_key, item_value in value.items():
+                        if isinstance(item_value, ToYaml):
+                            dictvalue[item_key] = item_value.to_dict()
+                        else:
+                            dictvalue[item_key] = item_value
+                    res[name] = dictvalue
+            elif isinstance(value, list):
+                if len(value):
+                    listvalue = []
+                    for item_value in value:
+                        if isinstance(item_value, ToYaml):
+                            listvalue.append(item_value.to_dict())
+                        else:
+                            listvalue.append(item_value)
+                    res[name] = listvalue
+            elif value is not None:
+                res[name] = value
+        return res
+
+    @property
+    def yaml_keys(self) -> List[str]:
+        return []
+
+    def populate(self, data: dict):
+        for name, value in data.items():
+            if name in self.yaml_keys:
+                if hasattr(self, name):
+                    setattr(self, name, value)
+        return self
+
+    def setattrs_from_dict(self, data: dict, *props) -> None:
+        for prop in props:
+            if prop in data:
+                if hasattr(self, prop):
+                    setattr(self, prop, data.get(prop))
+
+
+class GitlabServer(ToYaml):
+    def __init__(self):
+        self.name = None
+        self.server = None
+        self.token = None
+        self.tls_verify = True
+
+    @property
+    def yaml_keys(self) -> List[str]:
+        return ["name", "server", "token", "tls_verify"]
+
+    def populate(self, data):
+        self.setattrs_from_dict(data, *self.yaml_keys)
+        return self
+
+
+class GitlabConfiguration(ToYaml):
+    def __init__(self):
+        self.version = DEFAULT_GITLAB_VERSION
+        self.servers = []
+
+    def add(self, name: str, url: str, token: str, tls_verify: bool):
+        server = GitlabServer()
+        server.tls_verify = tls_verify
+        server.token = token
+        server.server = url
+        server.name = name
+        self.servers.append(server)
+        return server
+
+    @property
+    def yaml_keys(self) -> List[str]:
+        return ["version", "servers"]
+
+    def populate(self, data):
+        self.version = str(data.get("version", DEFAULT_GITLAB_VERSION))
+        for item in data.get("servers", []):
+            server = GitlabServer()
+            server.populate(item)
+            self.servers.append(server)
+        return self
+
+
+class VariablesConfiguration(ToYaml):
+    def __init__(self):
+        self.variables = dict()
+
+    @property
+    def yaml_keys(self) -> List[str]:
+        return ["variables"]
+
+    def populate(self, data: dict):
+        self.setattrs_from_dict(data, "variables")
+        return self
+
+
+class VolumesMixin:
+    def runtime_volumes(self) -> List[str]:
+        volumes = os.getenv("GLE_DOCKER_VOLUMES", None)
+        if volumes is not None:
+            volumes = volumes.split(",")
+        else:
+            volumes = getattr(self, "volumes")
+        return list(volumes)
+
+    def add_volume(self, text: str) -> DockerVolume:
+        """Add a docker volume mount point"""
+        volume = plausible_docker_volume(text)
+        if volume:
+            # remove this mount point if it is already used
+            self.remove_volume(volume.mount)
+            getattr(self, "volumes").append(str(volume))
+        return volume
+
+    def remove_volume(self, mount: str):
+        """Remove a docker volume mount point"""
+        possible = plausible_docker_volume(mount)
+        if possible:
+            mount = possible.mount
+        keep_volumes = []
+        volumes = getattr(self, "volumes")
+        for item in volumes:
+            volume = plausible_docker_volume(item)
+            if volume.mount == mount:
+                continue
+            keep_volumes.append(str(volume))
+        setattr(self, "volumes", list(set(keep_volumes)))
+
+
+class DockerConfiguration(VariablesConfiguration, VolumesMixin):
+    def __init__(self):
+        self.privileged = True
+        self.docker_cli = DEFAULT_DOCKER_CLI
+        super(DockerConfiguration, self).__init__()
+        self.volumes = []
+
+    @property
+    def yaml_keys(self) -> List[str]:
+        return super().yaml_keys + ["volumes", "privileged", "docker_cli"]
+
+    def populate(self, data: dict):
+        super(DockerConfiguration, self).populate(data)
+        self.setattrs_from_dict(data, "volumes", "privileged", "docker_cli")
+        # validate the volumes
+        self.validate()
+        return self
+
+    def validate(self):
+        for volume in self.volumes:
+            if plausible_docker_volume(volume) is None:
+                fatal(f"Unable to parse docker volume string '{volume}'")
+
+
+class WindowsConfiguration(ToYaml):
+    def __init__(self):
+        self.cmd = False
+
+    @property
+    def yaml_keys(self) -> List[str]:
+        return ["cmd"]
+
+    def populate(self, data: dict):
+        self.setattrs_from_dict(data, "cmd")
+        return self
+
+
+class UserContext(VariablesConfiguration):
+    def __init__(self):
+        super(UserContext, self).__init__()
+        self.windows = WindowsConfiguration()
+        self.docker = DockerConfiguration()
+        self.local = VariablesConfiguration()
+        self.gitlab = GitlabConfiguration()
+        self.filename: Optional[str] = None
+        self.runners: List[GleRunnerConfig] = []
+
+    def remove_runner(self, name: str):
+        self.runners = [x for x in self.runners if x.name != name]
+
+    def save_runner(self, runner: "GleRunnerConfig"):
+        exists = self.get_runner(runner.name)
+        if exists:
+            self.remove_runner(runner.name)
+        self.runners.append(runner)
+
+    def find_runner(self,
+                    image: bool = False,
+                    tags: Optional[List[str]] = None,
+                    ) -> Optional["GleRunnerConfig"]:
+        """Find a runner configuration for the given image+tag combo"""
+        if tags is None:
+            tags = []
+
+        tagset = set(tags)
+        debug(f"find runner for image={image}, tags={tags}")
+        for runner in self.runners + self.builtin_runners():
+            if image and runner.executor not in [EXECUTOR_DOCKER, EXECUTOR_DOCKER_WINDOWS]:
+                continue
+            if not image:
+                if runner.executor in [EXECUTOR_DOCKER, EXECUTOR_DOCKER_WINDOWS]:
+                    continue
+            debug(f"considering runner {runner}")
+            if len(tagset) == 0:
+                if runner.run_untagged:
+                    debug(f"matching {runner} - allow untagged")
+                    return runner
+            else:
+                runner_tagset = set(runner.tags)
+                if tagset.issubset(runner_tagset):
+                    debug(f"matching {runner} - tags {tagset}")
+                    return runner
+
+        # nothing matched, return the defaults but warn
+        if image:
+            runner = self.get_runner("default-docker", builtins=True)
+        else:
+            runner = self.get_runner("default-shell", builtins=True)
+        debug(f"matched default {runner}")
+        return runner
+
+    def get_runner(self, name: str, builtins=False) -> Optional["GleRunnerConfig"]:
+        runners = list(self.runners)
+        if builtins:
+            runners.extend(self.builtin_runners())
+        for runner in runners:
+            if runner.name == name:
+                return runner
+        return None
+
+    def can_add_name(self, name: str) -> bool:
+        return self.get_runner(name, builtins=True) is None
+
+    def builtin_runners(self) -> List["GleRunnerConfig"]:
+        ret = []
+        if has_docker():
+            builtin_docker = GleRunnerConfig()
+            builtin_docker.name = "default-docker"
+            builtin_docker.is_builtin = True
+            builtin_docker.executor = "docker"
+            builtin_docker.docker = DockerExecutorConfig()
+            builtin_docker.docker.privileged = self.docker.privileged
+            builtin_docker.run_untagged = True
+            builtin_docker.docker.volumes = list(self.docker.runtime_volumes())
+            builtin_docker.environment = dict(self.docker.variables)
+            ret.append(builtin_docker)
+        builtin_shell = GleRunnerConfig()
+        builtin_shell.executor = "shell"
+        builtin_shell.name = "default-shell"
+        builtin_shell.shell = SHELL_BASH
+        if is_windows():  # pragma: cover if windows
+            builtin_shell.shell = SHELL_POWERSHELL
+        ret.append(builtin_shell)
+        return ret
+
+    @property
+    def yaml_keys(self) -> List[str]:
+        return super().yaml_keys + ["gitlab", "docker", "local", "windows", "runners"]
+                
+    def populate(self, data: dict) -> None:
+        super(UserContext, self).populate(data)
+        for name in ["windows", "docker", "gitlab", "local"]:
+            element = getattr(self, name)
+            element.populate(data.get(name, {}))
+        runners = data.get("runners", [])
+        self.runners = [GleRunnerConfig().populate(x) for x in runners]
+
+
+class UserConfigFile(ToYaml):
+    def __init__(self):
+        self.current_context: Optional[str] = None
+        self.contexts: Dict[str, UserContext] = {}
+        self.filename: Optional[str] = None
+
+    def to_dict(self) -> dict:
+        res = {
+            "current_context": self.current_context
+        }
+        for name, ctx in self.contexts.items():
+            res[name] = ctx.to_dict()
+        return res
+
+    def load(self, filename: str) -> None:
+        self.filename = os.path.abspath(filename)
+        data = {}
+        if os.path.exists(filename):
+            with open(self.filename, "r") as yfile:
+                data: dict = safe_load(yfile)
+        if data is None:
+            # the file was empty?
+            data = {}
+        self.populate(data)
+
+    def populate(self, data: dict):
+        self.current_context = data.get("current_context", DEFAULT_CONTEXT)
+        for name in data.keys():
+            if name not in FORBIDDEN_CONTEXT_NAMES:
+                self.contexts[name] = UserContext()
+                self.contexts[name].populate(data.get(name, {}))
+
+        if self.current_context not in self.contexts:
+            self.contexts[self.current_context] = UserContext()
+        return self
+
+    def save(self, filename: Optional[str] = None):
+        if filename is None:
+            filename = self.filename
+
+        if filename and os.path.basename(filename):
+            self.filename = os.path.abspath(filename)
+            filename = self.filename
+        else:
+            filename = None
+
+        if filename:
+            folder = os.path.dirname(os.path.abspath(filename))
+            if not os.path.exists(folder):
+                os.makedirs(folder)
+
+            with open(filename, "w") as yfile:
+                data = self.to_dict()
+                safe_dump(data, yfile,
+                          width=120,
+                          indent=2,
+                          default_flow_style=False)
+        return filename
+
+
+class DockerExecutorConfig(ToYaml, VolumesMixin):
+
+    def __init__(self):
+        self.privileged = False
+        self.image: str = "alpine:latest"
+        self.volumes: List[str] = []
+        self.cap_add: str = ""
+        self.mac_address: str = ""
+        self.docker_cli = DEFAULT_DOCKER_CLI
+
+    @property
+    def yaml_keys(self) -> List[str]:
+        return ["privileged",
+                "image",
+                "volumes",
+                "cap_add",
+                "mac_address",
+                "docker_cli",
+                ]
+
+
+class GleRunnerConfig(ToYaml):
+
+    def __init__(self):
+        self.name: str = "local"
+        self.tags: List[str] = []
+        self.run_untagged = False
+        self.executor: str = "docker"
+        self.shell: str = SHELL_POWERSHELL if is_windows() else SHELL_BASH
+        self.docker: Optional[DockerExecutorConfig] = None
+        self.pre_build_script: str = ""
+        self.environment: Dict[str, str] = {}
+        self.is_builtin: bool = False
+
+    def __str__(self):
+        return f"{self.executor}-runner {self.name}"
+
+    @property
+    def yaml_keys(self) -> List[str]:
+        keys = ["name", "tags", "run_untagged", "executor", "environment", "shell", "pre_build_script"]
+        if self.executor == "docker":
+            keys.append("docker")
+        return keys
+
+    def populate(self, data: dict):
+        super().populate(data)
+        if self.docker is not None:
+            self.docker = DockerExecutorConfig().populate(cast(dict, self.docker))
+        return self
```

## gitlabemu/variables.py

 * *Ordering differences only*

```diff
@@ -1,28 +1,28 @@
-"""Helper functions for handling pipeline variable substitution"""
-import re
-from typing import Dict, Union, Any
-
-VARIABLE_PATTERN = re.compile(r"(\$\w+)")
-
-
-def expand_variable(variables: Dict[str, str], haystack: Union[str, Dict[str, Any]]) -> str:
-    """Expand a $NAME style variable"""
-    if isinstance(haystack, dict):
-        if haystack.get("expand") is False:
-            return haystack.get("value", "")
-        # expandable variable
-        haystack = haystack.get("value", "")
-    else:
-        haystack = str(haystack)
-
-    while True:
-        match = VARIABLE_PATTERN.search(haystack)
-        if match:
-            variable = match.group(0)
-            if variable:
-                name = variable[1:]
-                value = expand_variable(variables, variables.get(name, ""))
-                haystack = haystack.replace(variable, value)
-        else:
-            break
-    return haystack
+"""Helper functions for handling pipeline variable substitution"""
+import re
+from typing import Dict, Union, Any
+
+VARIABLE_PATTERN = re.compile(r"(\$\w+)")
+
+
+def expand_variable(variables: Dict[str, str], haystack: Union[str, Dict[str, Any]]) -> str:
+    """Expand a $NAME style variable"""
+    if isinstance(haystack, dict):
+        if haystack.get("expand") is False:
+            return haystack.get("value", "")
+        # expandable variable
+        haystack = haystack.get("value", "")
+    else:
+        haystack = str(haystack)
+
+    while True:
+        match = VARIABLE_PATTERN.search(haystack)
+        if match:
+            variable = match.group(0)
+            if variable:
+                name = variable[1:]
+                value = expand_variable(variables, variables.get(name, ""))
+                haystack = haystack.replace(variable, value)
+        else:
+            break
+    return haystack
```

## gitlabemu/yamlloader.py

 * *Ordering differences only*

```diff
@@ -1,91 +1,91 @@
-"""
-Preserve order of keys
-"""
-import json
-from typing import List
-
-import yaml
-from collections import OrderedDict
-from yaml.resolver import BaseResolver
-
-
-class StringableOrderedDict(OrderedDict):
-    def __str__(self):
-        return str(dict(self))
-
-    def __repr__(self):
-        return json.dumps(self)
-
-
-class GitlabReferenceError(Exception):
-    def __init__(self, message):
-        self.message = message
-
-
-class GitlabReference:
-    def __init__(self, job, element, value):
-        self.job = job
-        self.element = element
-        self.value = value
-        self.location = ""
-
-    def __repr__(self):
-        if self.value:
-            return "!reference [{}, {}, {}]".format(self.job, self.element, self.value)
-        return "!reference [{}, {}]".format(self.job, self.element)
-
-    def __str__(self):
-        return repr(self) + f" at {self.location}"
-
-
-class OrderedLoader(yaml.FullLoader):
-    def __init__(self, stream, firstpass=None):
-        super(OrderedLoader, self).__init__(stream)
-        if firstpass is None:
-            firstpass = StringableOrderedDict()
-        self.first_pass = firstpass
-
-
-def reference_constructor(loader: OrderedLoader, node):
-    address = []
-    for item in node.value:
-        address.append(item.value)
-
-    jobname = address[0]
-    jobelement = address[1]
-    elementvalue = None
-    if len(address) > 2:
-        elementvalue = address[2]
-
-    reference = GitlabReference(jobname, jobelement, elementvalue)
-    reference.location = node.start_mark
-    return reference
-
-
-yaml.add_constructor(u"!reference", reference_constructor)
-
-
-def ordered_load(stream, preloaded=None):
-
-    def construct_mapping(loader, node):
-        loader.flatten_mapping(node)
-        return StringableOrderedDict(loader.construct_pairs(node))
-
-    OrderedLoader.add_constructor(BaseResolver.DEFAULT_MAPPING_TAG, construct_mapping)
-
-    def context_loader(*kwargs):
-        ret = OrderedLoader(*kwargs, preloaded)
-        return ret
-
-    return yaml.load(stream, context_loader)
-
-
-def ordered_dump(data, **kwargs):
-    """Dump data with OrderedDict content"""
-    def get_dumper(*args, **x):
-        dumper = yaml.Dumper(*args, **x)
-        dict_repr = dumper.yaml_representers.get(dict)
-        dumper.yaml_representers[OrderedDict] = dict_repr
-        dumper.yaml_representers[StringableOrderedDict] = dict_repr
-        return dumper
-    return yaml.dump(data, Dumper=get_dumper, **kwargs, sort_keys=False)
+"""
+Preserve order of keys
+"""
+import json
+from typing import List
+
+import yaml
+from collections import OrderedDict
+from yaml.resolver import BaseResolver
+
+
+class StringableOrderedDict(OrderedDict):
+    def __str__(self):
+        return str(dict(self))
+
+    def __repr__(self):
+        return json.dumps(self)
+
+
+class GitlabReferenceError(Exception):
+    def __init__(self, message):
+        self.message = message
+
+
+class GitlabReference:
+    def __init__(self, job, element, value):
+        self.job = job
+        self.element = element
+        self.value = value
+        self.location = ""
+
+    def __repr__(self):
+        if self.value:
+            return "!reference [{}, {}, {}]".format(self.job, self.element, self.value)
+        return "!reference [{}, {}]".format(self.job, self.element)
+
+    def __str__(self):
+        return repr(self) + f" at {self.location}"
+
+
+class OrderedLoader(yaml.FullLoader):
+    def __init__(self, stream, firstpass=None):
+        super(OrderedLoader, self).__init__(stream)
+        if firstpass is None:
+            firstpass = StringableOrderedDict()
+        self.first_pass = firstpass
+
+
+def reference_constructor(loader: OrderedLoader, node):
+    address = []
+    for item in node.value:
+        address.append(item.value)
+
+    jobname = address[0]
+    jobelement = address[1]
+    elementvalue = None
+    if len(address) > 2:
+        elementvalue = address[2]
+
+    reference = GitlabReference(jobname, jobelement, elementvalue)
+    reference.location = node.start_mark
+    return reference
+
+
+yaml.add_constructor(u"!reference", reference_constructor)
+
+
+def ordered_load(stream, preloaded=None):
+
+    def construct_mapping(loader, node):
+        loader.flatten_mapping(node)
+        return StringableOrderedDict(loader.construct_pairs(node))
+
+    OrderedLoader.add_constructor(BaseResolver.DEFAULT_MAPPING_TAG, construct_mapping)
+
+    def context_loader(*kwargs):
+        ret = OrderedLoader(*kwargs, preloaded)
+        return ret
+
+    return yaml.load(stream, context_loader)
+
+
+def ordered_dump(data, **kwargs):
+    """Dump data with OrderedDict content"""
+    def get_dumper(*args, **x):
+        dumper = yaml.Dumper(*args, **x)
+        dict_repr = dumper.yaml_representers.get(dict)
+        dumper.yaml_representers[OrderedDict] = dict_repr
+        dumper.yaml_representers[StringableOrderedDict] = dict_repr
+        return dumper
+    return yaml.dump(data, Dumper=get_dumper, **kwargs, sort_keys=False)
```

## gitlabemu/genericci/types.py

 * *Ordering differences only*

```diff
@@ -1,147 +1,147 @@
-"""Classes that represent generic CI jobs"""
-from typing import Dict, List, Any, Optional
-from abc import ABC, abstractmethod
-
-from ..docker import DockerJob
-from ..jobs import Job
-
-
-class ToDict(ABC):
-
-    @abstractmethod
-    def to_dict(self) -> Dict[str, Any]:
-        pass
-
-
-class GenericContainerSpec(ToDict):
-    def __init__(self):
-        self.image: str = ""
-        self.entrypoint: Optional[List[str]] = []
-
-    def to_dict(self) -> Dict[str, Any]:
-        return {
-            "image": self.image,
-            "entrypoint": self.entrypoint
-        }
-
-
-class GenericContainerServiceSpec(GenericContainerSpec):
-    def __init__(self):
-        super(GenericContainerServiceSpec, self).__init__()
-        self.alias: Optional[str] = None
-        self.command: List[str] = []
-        self.variables: Dict[str, str] = {}
-
-    def to_dict(self) -> Dict[str, Any]:
-        basedata = super(GenericContainerServiceSpec, self).to_dict()
-        basedata.update({
-            "alias": self.alias,
-            "command": self.command,
-            "variables": self.variables
-        })
-        return basedata
-
-
-class GenericDependency(ToDict):
-    def __init__(self):
-        self.job: str = ""
-        self.need_artifacts: bool = True
-
-    def to_dict(self) -> Dict[str, Any]:
-        return {
-            "job": self.job,
-            "artifacts": self.need_artifacts
-        }
-
-
-class GenericArtifacts(ToDict):
-    def to_dict(self) -> Dict[str, Any]:
-        return {
-            "if_failed": self.if_failed,
-            "if_success": self.if_success,
-            "paths": self.paths
-        }
-
-    def __init__(self):
-        self.if_failed: bool = False
-        self.if_success: bool = True
-        self.paths: List[str] = []
-
-
-class GenericJob(ToDict):
-    def to_dict(self) -> Dict[str, Any]:
-        return {
-            "name": self.name,
-            "stage_name": self.stage_name,
-            "depends": [x.to_dict() for x in self.needed_jobs],
-            "set_variables": self.set_variables,
-            "machine_tags": self.machine_tags,
-            "container": self.container.to_dict(),
-            "services": [x.to_dict() for x in self.services],
-            "job_script": {
-                "lines": self.job_script_lines,
-                "exit_on_nonzero": True,
-                "fail_on_nonzero": True
-            },
-            "finally_script": {
-                "lines": self.finally_script_lines,
-                "exit_on_nonzero": True,
-                "fail_on_nonzero": False
-            },
-            "artifacts": self.artifacts.to_dict()
-        }
-
-    def __init__(self):
-        self.name: str = ""
-        self.stage_name: str = ""
-        self.needed_jobs: List[GenericDependency] = []
-        self.machine_tags: List[str] = []
-        self.set_variables: Dict[str, str] = {}
-        self.container: GenericContainerSpec = GenericContainerSpec()
-        self.services: List[GenericContainerServiceSpec] = []
-        self.job_script_lines: List[str] = []
-        self.finally_script_lines: List[str] = []
-        self.artifacts: GenericArtifacts = GenericArtifacts()
-
-    def from_job(self, loader, job: Job):
-        self.name = job.name
-        self.stage_name = job.stage
-        self.job_script_lines = job.before_script + job.script
-        self.finally_script_lines = job.after_script
-        self.machine_tags = job.tags
-        self.set_variables = job.variables
-        self.set_variables["CI_JOB_NAME"] = job.name
-        if job.stage:
-            self.set_variables["CI_JOB_STAGE"] = job.stage
-        if job.parallel:
-            self.set_variables["CI_NODE_TOTAL"] = "1"
-            self.set_variables["CI_NODE_INDEX"] = "1"
-        for dep in job.dependencies:
-            depend = GenericDependency()
-            depend.job = dep
-            depend.need_artifacts = dep in job.needed_artifacts
-            self.needed_jobs.append(depend)
-
-        if job.artifacts:
-            if job.artifacts.when == "on_success":
-                self.artifacts.if_failed = False
-            elif job.artifacts.when == "always":
-                self.artifacts.if_failed = True
-            elif job.artifacts.when == "on_failure":
-                self.artifacts.if_success = False
-            self.artifacts.paths = job.artifacts.paths
-
-        if isinstance(job, DockerJob):
-            image = loader.get_docker_image(self.name)
-            if isinstance(image, str):
-                self.container.image = image
-            else:
-                self.container.image = image["name"]
-                self.container.entrypoint = image.get("entrypoint", [])
-            if job.services:
-                for service in job.services:
-                    svc = GenericContainerServiceSpec()
-                    svc.variables.update(self.set_variables)
-                    svc.image = service.get("name")
-                    svc.alias = service.get("alias", None)
-                    self.services.append(svc)
+"""Classes that represent generic CI jobs"""
+from typing import Dict, List, Any, Optional
+from abc import ABC, abstractmethod
+
+from ..docker import DockerJob
+from ..jobs import Job
+
+
+class ToDict(ABC):
+
+    @abstractmethod
+    def to_dict(self) -> Dict[str, Any]:
+        pass
+
+
+class GenericContainerSpec(ToDict):
+    def __init__(self):
+        self.image: str = ""
+        self.entrypoint: Optional[List[str]] = []
+
+    def to_dict(self) -> Dict[str, Any]:
+        return {
+            "image": self.image,
+            "entrypoint": self.entrypoint
+        }
+
+
+class GenericContainerServiceSpec(GenericContainerSpec):
+    def __init__(self):
+        super(GenericContainerServiceSpec, self).__init__()
+        self.alias: Optional[str] = None
+        self.command: List[str] = []
+        self.variables: Dict[str, str] = {}
+
+    def to_dict(self) -> Dict[str, Any]:
+        basedata = super(GenericContainerServiceSpec, self).to_dict()
+        basedata.update({
+            "alias": self.alias,
+            "command": self.command,
+            "variables": self.variables
+        })
+        return basedata
+
+
+class GenericDependency(ToDict):
+    def __init__(self):
+        self.job: str = ""
+        self.need_artifacts: bool = True
+
+    def to_dict(self) -> Dict[str, Any]:
+        return {
+            "job": self.job,
+            "artifacts": self.need_artifacts
+        }
+
+
+class GenericArtifacts(ToDict):
+    def to_dict(self) -> Dict[str, Any]:
+        return {
+            "if_failed": self.if_failed,
+            "if_success": self.if_success,
+            "paths": self.paths
+        }
+
+    def __init__(self):
+        self.if_failed: bool = False
+        self.if_success: bool = True
+        self.paths: List[str] = []
+
+
+class GenericJob(ToDict):
+    def to_dict(self) -> Dict[str, Any]:
+        return {
+            "name": self.name,
+            "stage_name": self.stage_name,
+            "depends": [x.to_dict() for x in self.needed_jobs],
+            "set_variables": self.set_variables,
+            "machine_tags": self.machine_tags,
+            "container": self.container.to_dict(),
+            "services": [x.to_dict() for x in self.services],
+            "job_script": {
+                "lines": self.job_script_lines,
+                "exit_on_nonzero": True,
+                "fail_on_nonzero": True
+            },
+            "finally_script": {
+                "lines": self.finally_script_lines,
+                "exit_on_nonzero": True,
+                "fail_on_nonzero": False
+            },
+            "artifacts": self.artifacts.to_dict()
+        }
+
+    def __init__(self):
+        self.name: str = ""
+        self.stage_name: str = ""
+        self.needed_jobs: List[GenericDependency] = []
+        self.machine_tags: List[str] = []
+        self.set_variables: Dict[str, str] = {}
+        self.container: GenericContainerSpec = GenericContainerSpec()
+        self.services: List[GenericContainerServiceSpec] = []
+        self.job_script_lines: List[str] = []
+        self.finally_script_lines: List[str] = []
+        self.artifacts: GenericArtifacts = GenericArtifacts()
+
+    def from_job(self, loader, job: Job):
+        self.name = job.name
+        self.stage_name = job.stage
+        self.job_script_lines = job.before_script + job.script
+        self.finally_script_lines = job.after_script
+        self.machine_tags = job.tags
+        self.set_variables = job.variables
+        self.set_variables["CI_JOB_NAME"] = job.name
+        if job.stage:
+            self.set_variables["CI_JOB_STAGE"] = job.stage
+        if job.parallel:
+            self.set_variables["CI_NODE_TOTAL"] = "1"
+            self.set_variables["CI_NODE_INDEX"] = "1"
+        for dep in job.dependencies:
+            depend = GenericDependency()
+            depend.job = dep
+            depend.need_artifacts = dep in job.needed_artifacts
+            self.needed_jobs.append(depend)
+
+        if job.artifacts:
+            if job.artifacts.when == "on_success":
+                self.artifacts.if_failed = False
+            elif job.artifacts.when == "always":
+                self.artifacts.if_failed = True
+            elif job.artifacts.when == "on_failure":
+                self.artifacts.if_success = False
+            self.artifacts.paths = job.artifacts.paths
+
+        if isinstance(job, DockerJob):
+            image = loader.get_docker_image(self.name)
+            if isinstance(image, str):
+                self.container.image = image
+            else:
+                self.container.image = image["name"]
+                self.container.entrypoint = image.get("entrypoint", [])
+            if job.services:
+                for service in job.services:
+                    svc = GenericContainerServiceSpec()
+                    svc.variables.update(self.set_variables)
+                    svc.image = service.get("name")
+                    svc.alias = service.get("alias", None)
+                    self.services.append(svc)
```

## gitlabemu/gitlab/constraints.py

 * *Ordering differences only*

```diff
@@ -1,18 +1,18 @@
-PIPELINE_PERSISTED_VARIABLES = [
-    "CI_PROJECT_PATH",
-    "CI_PIPELINE_ID",
-    "CI_PIPELINE_URL",
-]
-
-JOB_PERSISTED_VARIABLES = [
-    "CI_JOB_ID",
-    "CI_JOB_URL",
-    "CI_JOB_TOKEN",
-    "CI_JOB_STARTED_AT",
-    "CI_REGISTRY_USER",
-    "CI_REGISTRY_PASSWORD",
-    "CI_REPOSITORY_URL",
-    "CI_DEPLOY_USER",
-    "CI_DEPLOY_PASSWORD",
-]
-
+PIPELINE_PERSISTED_VARIABLES = [
+    "CI_PROJECT_PATH",
+    "CI_PIPELINE_ID",
+    "CI_PIPELINE_URL",
+]
+
+JOB_PERSISTED_VARIABLES = [
+    "CI_JOB_ID",
+    "CI_JOB_URL",
+    "CI_JOB_TOKEN",
+    "CI_JOB_STARTED_AT",
+    "CI_REGISTRY_USER",
+    "CI_REGISTRY_PASSWORD",
+    "CI_REPOSITORY_URL",
+    "CI_DEPLOY_USER",
+    "CI_DEPLOY_PASSWORD",
+]
+
```

## gitlabemu/gitlab/types.py

 * *Ordering differences only*

```diff
@@ -1,32 +1,32 @@
-"""
-See https://gitlab.com/gitlab-org/gitlab-runner/-/blob/main/common/network.go for reference
-"""
-
-
-
-RESERVED_TOP_KEYS = ["stages",
-                     "services",
-                     "image",
-                     "cache",
-                     "before_script",
-                     "after_script",
-                     "pages",
-                     "variables",
-                     "include",
-                     "workflow",
-                     "default",
-                     ".gitlab-emulator-workspace"
-                     ]
-
-DEFAULT_JOB_KEYS = [
-    "after_script",
-    "artifacts",
-    "before_script",
-    "cache",
-    "image",
-    "interruptible",
-    "retry",
-    "services",
-    "tags",
-    "timeout",
+"""
+See https://gitlab.com/gitlab-org/gitlab-runner/-/blob/main/common/network.go for reference
+"""
+
+
+
+RESERVED_TOP_KEYS = ["stages",
+                     "services",
+                     "image",
+                     "cache",
+                     "before_script",
+                     "after_script",
+                     "pages",
+                     "variables",
+                     "include",
+                     "workflow",
+                     "default",
+                     ".gitlab-emulator-workspace"
+                     ]
+
+DEFAULT_JOB_KEYS = [
+    "after_script",
+    "artifacts",
+    "before_script",
+    "cache",
+    "image",
+    "interruptible",
+    "retry",
+    "services",
+    "tags",
+    "timeout",
 ]
```

## gitlabemu/gitlab/urls.py

 * *Ordering differences only*

```diff
@@ -1 +1 @@
-GITLAB_ORG_TEMPLATE_BASEURL = "https://gitlab.com/gitlab-org/gitlab-foss/-/raw/HEAD/lib/gitlab/ci/templates"
+GITLAB_ORG_TEMPLATE_BASEURL = "https://gitlab.com/gitlab-org/gitlab-foss/-/raw/HEAD/lib/gitlab/ci/templates"
```

## gitlabemu/glp/__main__.py

 * *Ordering differences only*

```diff
@@ -1,2 +1,2 @@
-from .tool import run
-run()
+from .tool import run
+run()
```

## gitlabemu/glp/buildtool.py

 * *Ordering differences only*

```diff
@@ -1,26 +1,26 @@
-"""Start gitlab pipelines"""
-from argparse import ArgumentParser, Namespace
-from .subcommand import Command
-from .types import NameValuePair
-from ..pipelines import create_pipeline
-
-
-class BuildCommand(Command):
-    name = "build"
-    description = __doc__
-
-    def setup(self, parser: ArgumentParser) -> None:
-        parser.add_argument("-e",
-                            dest="variables",
-                            default=[],
-                            action="append",
-                            metavar="NAME=VALUE",
-                            type=NameValuePair,
-                            help="Add a pipeline variable")
-
-    def run(self, opts: Namespace):
-        vars = {}
-        for item in opts.variables:
-            vars[item.name] = item.value
-        # create a pipeline for the current branch
-        create_pipeline(vars=vars, tls_verify=opts.tls_verify)
+"""Start gitlab pipelines"""
+from argparse import ArgumentParser, Namespace
+from .subcommand import Command
+from .types import NameValuePair
+from ..pipelines import create_pipeline
+
+
+class BuildCommand(Command):
+    name = "build"
+    description = __doc__
+
+    def setup(self, parser: ArgumentParser) -> None:
+        parser.add_argument("-e",
+                            dest="variables",
+                            default=[],
+                            action="append",
+                            metavar="NAME=VALUE",
+                            type=NameValuePair,
+                            help="Add a pipeline variable")
+
+    def run(self, opts: Namespace):
+        vars = {}
+        for item in opts.variables:
+            vars[item.name] = item.value
+        # create a pipeline for the current branch
+        create_pipeline(vars=vars, tls_verify=opts.tls_verify)
```

## gitlabemu/glp/canceltool.py

 * *Ordering differences only*

```diff
@@ -1,21 +1,21 @@
-"""Cancel running gitlab project pipelines"""
-from argparse import Namespace
-
-from .subcommand import MatcherCommand
-from ..pipelines import pipelines_cmd
-
-
-class CancelCommand(MatcherCommand):
-    """Cancel pipelines"""
-    name = "cancel"
-    description = __doc__
-
-    def run(self, opts: Namespace):
-        matchers = {}
-        for item in opts.match:
-            matchers[item.name] = item.value
-
-        pipelines_cmd(matchers=matchers,
-                      limit=opts.limit,
-                      tls_verify=opts.tls_verify,
-                      do_cancel=True)
+"""Cancel running gitlab project pipelines"""
+from argparse import Namespace
+
+from .subcommand import MatcherCommand
+from ..pipelines import pipelines_cmd
+
+
+class CancelCommand(MatcherCommand):
+    """Cancel pipelines"""
+    name = "cancel"
+    description = __doc__
+
+    def run(self, opts: Namespace):
+        matchers = {}
+        for item in opts.match:
+            matchers[item.name] = item.value
+
+        pipelines_cmd(matchers=matchers,
+                      limit=opts.limit,
+                      tls_verify=opts.tls_verify,
+                      do_cancel=True)
```

## gitlabemu/glp/dumptool.py

 * *Ordering differences only*

```diff
@@ -1,37 +1,37 @@
-"""Dump gitlab pipelines as a generic document"""
-import sys
-
-import yaml
-from argparse import ArgumentParser, Namespace
-from .subcommand import Command
-from ..configloader import Loader, DEFAULT_CI_FILE
-from ..genericci.types import GenericJob
-from ..logmsg import info
-
-
-class DumpCommand(Command):
-    name = "dump"
-    description = __doc__
-
-    def setup(self, parser: ArgumentParser) -> None:
-        parser.add_argument("-c", dest="cifile", type=str, default=DEFAULT_CI_FILE,
-                            help="Load a specific CI file")
-        parser.add_argument("JOB", type=str,
-                            nargs="*",
-                            help="Dump only one job")
-
-    def run(self, opts: Namespace):
-        loader = Loader()
-        loader.load(opts.cifile)
-        dump_jobs = loader.get_jobs()
-        if opts.JOB:
-            dump_jobs = [x for x in dump_jobs if x in opts.JOB]
-
-        for jobname in dump_jobs:
-            job = loader.load_job(jobname)
-            generic = GenericJob()
-            generic.from_job(loader, job)
-            print(f"# job {jobname}")
-            yaml.safe_dump(generic.to_dict(), stream=sys.stdout)
-
-
+"""Dump gitlab pipelines as a generic document"""
+import sys
+
+import yaml
+from argparse import ArgumentParser, Namespace
+from .subcommand import Command
+from ..configloader import Loader, DEFAULT_CI_FILE
+from ..genericci.types import GenericJob
+from ..logmsg import info
+
+
+class DumpCommand(Command):
+    name = "dump"
+    description = __doc__
+
+    def setup(self, parser: ArgumentParser) -> None:
+        parser.add_argument("-c", dest="cifile", type=str, default=DEFAULT_CI_FILE,
+                            help="Load a specific CI file")
+        parser.add_argument("JOB", type=str,
+                            nargs="*",
+                            help="Dump only one job")
+
+    def run(self, opts: Namespace):
+        loader = Loader()
+        loader.load(opts.cifile)
+        dump_jobs = loader.get_jobs()
+        if opts.JOB:
+            dump_jobs = [x for x in dump_jobs if x in opts.JOB]
+
+        for jobname in dump_jobs:
+            job = loader.load_job(jobname)
+            generic = GenericJob()
+            generic.from_job(loader, job)
+            print(f"# job {jobname}")
+            yaml.safe_dump(generic.to_dict(), stream=sys.stdout)
+
+
```

## gitlabemu/glp/exporttool.py

 * *Ordering differences only*

```diff
@@ -1,41 +1,41 @@
-"""Export pipelines"""
-from argparse import ArgumentParser, Namespace
-from .subcommand import Command
-from ..pipelines import export_cmd
-
-
-class ExportCommand(Command):
-    name = "export"
-    description = __doc__
-
-    def setup(self, parser: ArgumentParser) -> None:
-        parser.add_argument("PIPELINE", type=str,
-                            help="Pipeline to export")
-        parser.add_argument("SAVEDIR", type=str,
-                            help="Save exported files to this folder")
-        parser.add_argument("JOB", type=str, default=None,
-                            nargs="*",
-                            help="Limit export to only these named jobs")
-        parser.add_argument("--exec", type=str, nargs="+",
-                            default=None,
-                            help="For each job exported, execute this process and substitute %%p for the job output folder")
-
-    def run(self, opts: Namespace):
-        jobs = []
-        # allow use of --
-        seen_exec = False
-        exec_args = []
-        for item in opts.JOB:
-            if not seen_exec:
-                if item in ["--exec"]:
-                    seen_exec = True
-                    continue
-                jobs.append(item)
-            else:
-                exec_args.append(item)
-        if seen_exec:
-            opts.exec = exec_args
-
-        export_cmd(opts.PIPELINE, opts.SAVEDIR, *jobs,
-                   exec_export=opts.exec,
-                   tls_verify=opts.tls_verify)
+"""Export pipelines"""
+from argparse import ArgumentParser, Namespace
+from .subcommand import Command
+from ..pipelines import export_cmd
+
+
+class ExportCommand(Command):
+    name = "export"
+    description = __doc__
+
+    def setup(self, parser: ArgumentParser) -> None:
+        parser.add_argument("PIPELINE", type=str,
+                            help="Pipeline to export")
+        parser.add_argument("SAVEDIR", type=str,
+                            help="Save exported files to this folder")
+        parser.add_argument("JOB", type=str, default=None,
+                            nargs="*",
+                            help="Limit export to only these named jobs")
+        parser.add_argument("--exec", type=str, nargs="+",
+                            default=None,
+                            help="For each job exported, execute this process and substitute %%p for the job output folder")
+
+    def run(self, opts: Namespace):
+        jobs = []
+        # allow use of --
+        seen_exec = False
+        exec_args = []
+        for item in opts.JOB:
+            if not seen_exec:
+                if item in ["--exec"]:
+                    seen_exec = True
+                    continue
+                jobs.append(item)
+            else:
+                exec_args.append(item)
+        if seen_exec:
+            opts.exec = exec_args
+
+        export_cmd(opts.PIPELINE, opts.SAVEDIR, *jobs,
+                   exec_export=opts.exec,
+                   tls_verify=opts.tls_verify)
```

## gitlabemu/glp/jobstool.py

 * *Ordering differences only*

```diff
@@ -1,30 +1,30 @@
-"""List job status in a pipeline"""
-import os
-from argparse import Namespace, ArgumentParser
-
-from ..helpers import note, die
-from .subcommand import Command
-from ..gitlab_client_api import get_current_project_client, find_project_pipeline
-from ..pipelines import print_pipeline_jobs
-from ..helpers import git_current_branch
-
-
-class JobListCommand(Command):
-    """List pipeline jobs"""
-    name = "jobs"
-    description = __doc__
-
-    def setup(self, parser: ArgumentParser) -> None:
-        parser.add_argument("PIPELINE", type=int, default=None, nargs="?",
-                            help="The pipeline number to fetch, defaults to the last on this branch")
-
-    def run(self, opts: Namespace):
-        cwd = os.getcwd()
-        client, project, _ = get_current_project_client(tls_verify=opts.tls_verify, need_remote=False)
-        branch = None
-        if opts.PIPELINE is None:
-            branch = git_current_branch(cwd)
-            note(f"Searching for most recent pipeline on branch: {branch} ..")
-
-        pipeline = find_project_pipeline(project, pipeline=opts.PIPELINE, ref=branch)
-        print_pipeline_jobs(pipeline, status=True)
+"""List job status in a pipeline"""
+import os
+from argparse import Namespace, ArgumentParser
+
+from ..helpers import note, die
+from .subcommand import Command
+from ..gitlab_client_api import get_current_project_client, find_project_pipeline
+from ..pipelines import print_pipeline_jobs
+from ..helpers import git_current_branch
+
+
+class JobListCommand(Command):
+    """List pipeline jobs"""
+    name = "jobs"
+    description = __doc__
+
+    def setup(self, parser: ArgumentParser) -> None:
+        parser.add_argument("PIPELINE", type=int, default=None, nargs="?",
+                            help="The pipeline number to fetch, defaults to the last on this branch")
+
+    def run(self, opts: Namespace):
+        cwd = os.getcwd()
+        client, project, _ = get_current_project_client(tls_verify=opts.tls_verify, need_remote=False)
+        branch = None
+        if opts.PIPELINE is None:
+            branch = git_current_branch(cwd)
+            note(f"Searching for most recent pipeline on branch: {branch} ..")
+
+        pipeline = find_project_pipeline(project, pipeline=opts.PIPELINE, ref=branch)
+        print_pipeline_jobs(pipeline, status=True)
```

## gitlabemu/glp/listtool.py

 * *Ordering differences only*

```diff
@@ -1,21 +1,21 @@
-"""List gitlab project pipelines"""
-from argparse import Namespace
-
-from .subcommand import MatcherCommand
-from ..pipelines import pipelines_cmd
-
-
-class ListCommand(MatcherCommand):
-    """List pipelines"""
-    name = "list"
-    description = __doc__
-
-    def run(self, opts: Namespace):
-        matchers = {}
-        for item in opts.match:
-            matchers[item.name] = item.value
-
-        pipelines_cmd(matchers=matchers,
-                      limit=opts.limit,
-                      tls_verify=opts.tls_verify,
-                      do_list=True)
+"""List gitlab project pipelines"""
+from argparse import Namespace
+
+from .subcommand import MatcherCommand
+from ..pipelines import pipelines_cmd
+
+
+class ListCommand(MatcherCommand):
+    """List pipelines"""
+    name = "list"
+    description = __doc__
+
+    def run(self, opts: Namespace):
+        matchers = {}
+        for item in opts.match:
+            matchers[item.name] = item.value
+
+        pipelines_cmd(matchers=matchers,
+                      limit=opts.limit,
+                      tls_verify=opts.tls_verify,
+                      do_list=True)
```

## gitlabemu/glp/subcommand.py

 * *Ordering differences only*

```diff
@@ -1,73 +1,73 @@
-"""Base classes for subcommands"""
-
-from abc import ABC, abstractmethod
-from argparse import ArgumentParser, Namespace
-from typing import Optional
-from .types import Match, RefMatch
-
-
-class ArgumentParserEx(ArgumentParser):
-    """An ArgumentParser where subparsers is accessible"""
-
-    def __init__(self, **kwargs):
-        super(ArgumentParserEx, self).__init__(**kwargs)
-        self._commands = None
-
-    def add_subcommand(self, command: "Command"):
-        if self._commands is None:
-            self._commands = self.add_subparsers(title="subcommands", help="additional help")
-        command.register(self)
-
-    @property
-    def commands(self):
-        return self._commands
-
-
-class Command(ABC):
-    def __init__(self):
-        self.parser: Optional[ArgumentParser] = None
-        self.parent: Optional[ArgumentParserEx] = None
-
-    def register(self, parser: ArgumentParserEx):
-        self.parser = parser.commands.add_parser(self.name, description=self.description)
-        self.setup(self.parser)
-        self.parser.set_defaults(func=self.run)
-
-    @property
-    @abstractmethod
-    def description(self) -> str:
-        pass
-
-    @property
-    @abstractmethod
-    def name(self) -> str:
-        pass
-
-    @abstractmethod
-    def setup(self, parser: ArgumentParser) -> None:
-        pass
-
-    @abstractmethod
-    def run(self, opts: Namespace):
-        pass
-
-
-class MatcherCommand(Command, ABC):
-    def setup(self, parser: ArgumentParser) -> None:
-        parser.add_argument("--match",
-                            dest="match",
-                            type=Match,
-                            action="append",
-                            default=[],
-                            help="Filter pipelines by status/ref")
-        parser.add_argument("--ref",
-                            metavar="REFERENCE",
-                            dest="match",
-                            type=RefMatch,
-                            action="append",
-                            help="Filter pipelines by ref (shortcut for --match ref=REFERENCE")
-        parser.add_argument("--limit",
-                            type=int,
-                            default=10,
-                            metavar="COUNT",
-                            help="Include up to COUNT results")
+"""Base classes for subcommands"""
+
+from abc import ABC, abstractmethod
+from argparse import ArgumentParser, Namespace
+from typing import Optional
+from .types import Match, RefMatch
+
+
+class ArgumentParserEx(ArgumentParser):
+    """An ArgumentParser where subparsers is accessible"""
+
+    def __init__(self, **kwargs):
+        super(ArgumentParserEx, self).__init__(**kwargs)
+        self._commands = None
+
+    def add_subcommand(self, command: "Command"):
+        if self._commands is None:
+            self._commands = self.add_subparsers(title="subcommands", help="additional help")
+        command.register(self)
+
+    @property
+    def commands(self):
+        return self._commands
+
+
+class Command(ABC):
+    def __init__(self):
+        self.parser: Optional[ArgumentParser] = None
+        self.parent: Optional[ArgumentParserEx] = None
+
+    def register(self, parser: ArgumentParserEx):
+        self.parser = parser.commands.add_parser(self.name, description=self.description)
+        self.setup(self.parser)
+        self.parser.set_defaults(func=self.run)
+
+    @property
+    @abstractmethod
+    def description(self) -> str:
+        pass
+
+    @property
+    @abstractmethod
+    def name(self) -> str:
+        pass
+
+    @abstractmethod
+    def setup(self, parser: ArgumentParser) -> None:
+        pass
+
+    @abstractmethod
+    def run(self, opts: Namespace):
+        pass
+
+
+class MatcherCommand(Command, ABC):
+    def setup(self, parser: ArgumentParser) -> None:
+        parser.add_argument("--match",
+                            dest="match",
+                            type=Match,
+                            action="append",
+                            default=[],
+                            help="Filter pipelines by status/ref")
+        parser.add_argument("--ref",
+                            metavar="REFERENCE",
+                            dest="match",
+                            type=RefMatch,
+                            action="append",
+                            help="Filter pipelines by ref (shortcut for --match ref=REFERENCE")
+        parser.add_argument("--limit",
+                            type=int,
+                            default=10,
+                            metavar="COUNT",
+                            help="Include up to COUNT results")
```

## gitlabemu/glp/subsettool.py

 * *Ordering differences only*

```diff
@@ -1,109 +1,109 @@
-"""Generate partial gitlab pipelines using temporary branches"""
-import os
-import sys
-from argparse import ArgumentParser, Namespace
-
-from .subcommand import MatcherCommand
-from .types import NameValuePair
-from .. import configloader
-from ..gitlab_client_api import get_current_project_client
-from ..helpers import die
-from ..pipelines import generate_pipeline, generate_subset_branch_name, get_subset_prefix, pipelines_cmd
-from ..yamlloader import ordered_dump
-
-
-class SubsetCommand(MatcherCommand):
-    name = "subset"
-    description = __doc__
-
-    def setup(self, parser: ArgumentParser) -> None:
-        super(SubsetCommand, self).setup(parser)
-        parser.add_argument("JOB", nargs="*",
-                            type=str,
-                            default=[],
-                            help="Generate a temporary pipeline including JOB (may be repeated)")
-        parser.add_argument("-e",
-                            dest="variables",
-                            default=[],
-                            action="append",
-                            metavar="NAME=VALUE",
-                            type=NameValuePair,
-                            help="Add a pipeline variable")
-        parser.add_argument("--from",
-                            dest="FROM",
-                            type=str,
-                            metavar="PIPELINE",
-                            help="Re-use artifacts from a pipeline")
-        parser.add_argument("--branches",
-                            default=False, action="store_true",
-                            help="List subset branch names (see also --match)")
-        parser.add_argument("--dump", type=str,
-                            help="Dump the generated pipeline to a file instead of running")
-        parser.add_argument("--clean",
-                            default=False, action="store_true",
-                            help="Delete leftover subset branches from the gitlab repo")
-
-    def run(self, opts: Namespace):
-        variables = {}
-        for item in opts.variables:
-            variables[item.name] = item.value
-        jobs = opts.JOB
-        # only allow clean if there are no jobs
-        if jobs:
-            if opts.clean or opts.match:
-                die("Cannot --clean or --match and run a build at the same time")
-        else:
-            if opts.FROM:
-                die("--from requires one or more jobs")
-
-        if len(jobs):
-            fullpath = os.path.abspath(configloader.find_ci_config(os.getcwd()))
-            loader = configloader.Loader(emulator_variables=False)
-            loader.load(fullpath)
-            # generate a subset pipeline
-            result = generate_pipeline(loader, *jobs, variables=variables,
-                                       dump_only=opts.dump is not None,
-                                       use_from=opts.FROM,
-                                       tls_verify=opts.tls_verify)
-            if opts.dump:
-                print(f"Saving generated pipeline as: {opts.dump}", file=sys.stderr)
-                with open(opts.dump, "w") as dumpcfg:
-                    dumpcfg.write(ordered_dump(result))
-        else:  # pragma: no cover
-            # TODO figure out how to test this in gitlab
-            client, project, remotename = get_current_project_client(tls_verify=opts.tls_verify)
-
-            reference = generate_subset_branch_name(client, os.getcwd())
-            match = {}
-            if opts.match:
-                if opts.match[0].name == "ref":
-                    reference = f"{get_subset_prefix()}{opts.match[0].value}"
-            match["ref"] = reference
-            if opts.clean or opts.branches:
-                search = f"^{reference}"
-                if opts.branches:
-                    # search for all subsets
-                    search = f"^{get_subset_prefix()}"
-                branches = project.branches.list(search=search, all=True)
-                if search:
-                    if not opts.branches:
-                        branches = [x for x in branches if x.name == reference]
-                branches = [x for x in branches if x.commit.get("title", "").startswith("subset pipeline for ")]
-                if opts.clean:
-                    for branch in branches:
-                        # if the branch isn't running
-                        pipelines = project.pipelines.list(ref=branch.commit.get("id"), all=True)
-                        if not pipelines:
-                            print(f"Deleting branch: {branch.name} ", flush=True)
-                            branch.delete()
-                        else:
-                            print(f"Not cleaning, branch '{branch.name}' is currently running a pipeline")
-                else:
-                    for branch in branches:
-                        print(branch.name)
-            else:
-                # print pipelines for this branch:
-                pipelines_cmd(tls_verify=opts.tls_verify,
-                              matchers=match,
-                              limit=opts.limit,
-                              do_list=True)
+"""Generate partial gitlab pipelines using temporary branches"""
+import os
+import sys
+from argparse import ArgumentParser, Namespace
+
+from .subcommand import MatcherCommand
+from .types import NameValuePair
+from .. import configloader
+from ..gitlab_client_api import get_current_project_client
+from ..helpers import die
+from ..pipelines import generate_pipeline, generate_subset_branch_name, get_subset_prefix, pipelines_cmd
+from ..yamlloader import ordered_dump
+
+
+class SubsetCommand(MatcherCommand):
+    name = "subset"
+    description = __doc__
+
+    def setup(self, parser: ArgumentParser) -> None:
+        super(SubsetCommand, self).setup(parser)
+        parser.add_argument("JOB", nargs="*",
+                            type=str,
+                            default=[],
+                            help="Generate a temporary pipeline including JOB (may be repeated)")
+        parser.add_argument("-e",
+                            dest="variables",
+                            default=[],
+                            action="append",
+                            metavar="NAME=VALUE",
+                            type=NameValuePair,
+                            help="Add a pipeline variable")
+        parser.add_argument("--from",
+                            dest="FROM",
+                            type=str,
+                            metavar="PIPELINE",
+                            help="Re-use artifacts from a pipeline")
+        parser.add_argument("--branches",
+                            default=False, action="store_true",
+                            help="List subset branch names (see also --match)")
+        parser.add_argument("--dump", type=str,
+                            help="Dump the generated pipeline to a file instead of running")
+        parser.add_argument("--clean",
+                            default=False, action="store_true",
+                            help="Delete leftover subset branches from the gitlab repo")
+
+    def run(self, opts: Namespace):
+        variables = {}
+        for item in opts.variables:
+            variables[item.name] = item.value
+        jobs = opts.JOB
+        # only allow clean if there are no jobs
+        if jobs:
+            if opts.clean or opts.match:
+                die("Cannot --clean or --match and run a build at the same time")
+        else:
+            if opts.FROM:
+                die("--from requires one or more jobs")
+
+        if len(jobs):
+            fullpath = os.path.abspath(configloader.find_ci_config(os.getcwd()))
+            loader = configloader.Loader(emulator_variables=False)
+            loader.load(fullpath)
+            # generate a subset pipeline
+            result = generate_pipeline(loader, *jobs, variables=variables,
+                                       dump_only=opts.dump is not None,
+                                       use_from=opts.FROM,
+                                       tls_verify=opts.tls_verify)
+            if opts.dump:
+                print(f"Saving generated pipeline as: {opts.dump}", file=sys.stderr)
+                with open(opts.dump, "w") as dumpcfg:
+                    dumpcfg.write(ordered_dump(result))
+        else:  # pragma: no cover
+            # TODO figure out how to test this in gitlab
+            client, project, remotename = get_current_project_client(tls_verify=opts.tls_verify)
+
+            reference = generate_subset_branch_name(client, os.getcwd())
+            match = {}
+            if opts.match:
+                if opts.match[0].name == "ref":
+                    reference = f"{get_subset_prefix()}{opts.match[0].value}"
+            match["ref"] = reference
+            if opts.clean or opts.branches:
+                search = f"^{reference}"
+                if opts.branches:
+                    # search for all subsets
+                    search = f"^{get_subset_prefix()}"
+                branches = project.branches.list(search=search, all=True)
+                if search:
+                    if not opts.branches:
+                        branches = [x for x in branches if x.name == reference]
+                branches = [x for x in branches if x.commit.get("title", "").startswith("subset pipeline for ")]
+                if opts.clean:
+                    for branch in branches:
+                        # if the branch isn't running
+                        pipelines = project.pipelines.list(ref=branch.commit.get("id"), all=True)
+                        if not pipelines:
+                            print(f"Deleting branch: {branch.name} ", flush=True)
+                            branch.delete()
+                        else:
+                            print(f"Not cleaning, branch '{branch.name}' is currently running a pipeline")
+                else:
+                    for branch in branches:
+                        print(branch.name)
+            else:
+                # print pipelines for this branch:
+                pipelines_cmd(tls_verify=opts.tls_verify,
+                              matchers=match,
+                              limit=opts.limit,
+                              do_list=True)
```

## gitlabemu/glp/tool.py

 * *Ordering differences only*

```diff
@@ -1,60 +1,60 @@
-"""Gitlab Pipeline Tool"""
-import os
-import sys
-from argparse import ArgumentError
-from typing import Optional, List
-
-from .subcommand import ArgumentParserEx
-from .buildtool import BuildCommand
-from .canceltool import CancelCommand
-from .dumptool import DumpCommand
-from .exporttool import ExportCommand
-from .jobstool import JobListCommand
-from .listtool import ListCommand
-from .subsettool import SubsetCommand
-from ..gitlab_client_api import GITLAB_SERVER_ENV, GITLAB_PROJECT_ENV, posix_cert_fixup
-from ..errors import ConfigLoaderError
-from ..helpers import die
-
-
-def override_server(server: str) -> str:
-    # pragma: no cover
-    if server:
-        if "/" not in server:
-            raise ArgumentError("--project should be HOST/GROUP/PROJECT")
-        host, project = server.split("/", 1)
-        os.environ[GITLAB_SERVER_ENV] = host
-        os.environ[GITLAB_PROJECT_ENV] = project
-        return server
-
-
-parser = ArgumentParserEx(description=__doc__)
-parser.add_argument("--project", type=override_server,
-                    help="Use this gitlab project instead of the the current git repo, eg SERVER/GROUP/PROJECT")
-parser.add_argument("--insecure", "-k", dest="tls_verify",
-                    default=True, action="store_false",
-                    help="Turn off SSL/TLS cert validation")
-parser.add_subcommand(BuildCommand())
-parser.add_subcommand(CancelCommand())
-parser.add_subcommand(DumpCommand())
-parser.add_subcommand(ExportCommand())
-parser.add_subcommand(JobListCommand())
-parser.add_subcommand(ListCommand())
-parser.add_subcommand(SubsetCommand())
-
-
-def top_level_usage(opts):
-    parser.print_usage()
-    sys.exit(1)
-
-
-parser.set_defaults(func=top_level_usage)
-
-
-def run(args: Optional[List[str]] = None) -> None:
-    opts = parser.parse_args(args)
-    try:
-        with posix_cert_fixup():
-            opts.func(opts)
-    except ConfigLoaderError as err:
-        die(str(err))
+"""Gitlab Pipeline Tool"""
+import os
+import sys
+from argparse import ArgumentError
+from typing import Optional, List
+
+from .subcommand import ArgumentParserEx
+from .buildtool import BuildCommand
+from .canceltool import CancelCommand
+from .dumptool import DumpCommand
+from .exporttool import ExportCommand
+from .jobstool import JobListCommand
+from .listtool import ListCommand
+from .subsettool import SubsetCommand
+from ..gitlab_client_api import GITLAB_SERVER_ENV, GITLAB_PROJECT_ENV, posix_cert_fixup
+from ..errors import ConfigLoaderError
+from ..helpers import die
+
+
+def override_server(server: str) -> str:
+    # pragma: no cover
+    if server:
+        if "/" not in server:
+            raise ArgumentError("--project should be HOST/GROUP/PROJECT")
+        host, project = server.split("/", 1)
+        os.environ[GITLAB_SERVER_ENV] = host
+        os.environ[GITLAB_PROJECT_ENV] = project
+        return server
+
+
+parser = ArgumentParserEx(description=__doc__)
+parser.add_argument("--project", type=override_server,
+                    help="Use this gitlab project instead of the the current git repo, eg SERVER/GROUP/PROJECT")
+parser.add_argument("--insecure", "-k", dest="tls_verify",
+                    default=True, action="store_false",
+                    help="Turn off SSL/TLS cert validation")
+parser.add_subcommand(BuildCommand())
+parser.add_subcommand(CancelCommand())
+parser.add_subcommand(DumpCommand())
+parser.add_subcommand(ExportCommand())
+parser.add_subcommand(JobListCommand())
+parser.add_subcommand(ListCommand())
+parser.add_subcommand(SubsetCommand())
+
+
+def top_level_usage(opts):
+    parser.print_usage()
+    sys.exit(1)
+
+
+parser.set_defaults(func=top_level_usage)
+
+
+def run(args: Optional[List[str]] = None) -> None:
+    opts = parser.parse_args(args)
+    try:
+        with posix_cert_fixup():
+            opts.func(opts)
+    except ConfigLoaderError as err:
+        die(str(err))
```

## gitlabemu/glp/types.py

 * *Ordering differences only*

```diff
@@ -1,31 +1,31 @@
-"""Type validators"""
-
-from argparse import ArgumentTypeError
-
-
-class NameValuePair:
-    def __init__(self, text: str):
-        self.name = None
-        self.value = None
-        params = text.split("=")
-        if len(params) == 2:
-            self.name = params[0]
-            self.value = params[1]
-
-        if self.name and self.value:
-            return
-        raise ArgumentTypeError("expected X=Y")
-
-
-class Match(NameValuePair):
-    NAMES = ["status", "ref"]
-
-    def __init__(self, text: str):
-        super(Match, self).__init__(text)
-        if self.name not in self.NAMES:
-            raise ArgumentTypeError(f"'{self.name}' is not one of {self.NAMES}")
-
-
-class RefMatch(Match):
-    def __init__(self, text: str):
-        super(RefMatch, self).__init__(f"ref={text}")
+"""Type validators"""
+
+from argparse import ArgumentTypeError
+
+
+class NameValuePair:
+    def __init__(self, text: str):
+        self.name = None
+        self.value = None
+        params = text.split("=")
+        if len(params) == 2:
+            self.name = params[0]
+            self.value = params[1]
+
+        if self.name and self.value:
+            return
+        raise ArgumentTypeError("expected X=Y")
+
+
+class Match(NameValuePair):
+    NAMES = ["status", "ref"]
+
+    def __init__(self, text: str):
+        super(Match, self).__init__(text)
+        if self.name not in self.NAMES:
+            raise ArgumentTypeError(f"'{self.name}' is not one of {self.NAMES}")
+
+
+class RefMatch(Match):
+    def __init__(self, text: str):
+        super(RefMatch, self).__init__(f"ref={text}")
```

## gitlabemu/rules/GitlabRuleLexer.py

 * *Ordering differences only*

```diff
@@ -1,83 +1,83 @@
-# Generated from GitlabRule.g4 by ANTLR 4.11.1
-from antlr4 import *
-from io import StringIO
-import sys
-if sys.version_info[1] > 5:
-    from typing import TextIO
-else:
-    from typing.io import TextIO
-
-
-def serializedATN():
-    return [
-        4,0,12,78,6,-1,2,0,7,0,2,1,7,1,2,2,7,2,2,3,7,3,2,4,7,4,2,5,7,5,2,
-        6,7,6,2,7,7,7,2,8,7,8,2,9,7,9,2,10,7,10,2,11,7,11,1,0,1,0,1,0,1,
-        1,1,1,1,1,1,2,1,2,1,2,1,3,1,3,1,3,1,4,1,4,1,4,1,5,1,5,1,5,1,6,1,
-        6,1,7,1,7,1,8,1,8,5,8,50,8,8,10,8,12,8,53,9,8,1,8,1,8,1,9,1,9,5,
-        9,59,8,9,10,9,12,9,62,9,9,1,9,1,9,1,10,1,10,4,10,68,8,10,11,10,12,
-        10,69,1,11,4,11,73,8,11,11,11,12,11,74,1,11,1,11,0,0,12,1,1,3,2,
-        5,3,7,4,9,5,11,6,13,7,15,8,17,9,19,10,21,11,23,12,1,0,4,1,0,34,34,
-        1,0,47,47,4,0,48,57,65,90,95,95,97,122,3,0,9,10,13,13,32,32,81,0,
-        1,1,0,0,0,0,3,1,0,0,0,0,5,1,0,0,0,0,7,1,0,0,0,0,9,1,0,0,0,0,11,1,
-        0,0,0,0,13,1,0,0,0,0,15,1,0,0,0,0,17,1,0,0,0,0,19,1,0,0,0,0,21,1,
-        0,0,0,0,23,1,0,0,0,1,25,1,0,0,0,3,28,1,0,0,0,5,31,1,0,0,0,7,34,1,
-        0,0,0,9,37,1,0,0,0,11,40,1,0,0,0,13,43,1,0,0,0,15,45,1,0,0,0,17,
-        47,1,0,0,0,19,56,1,0,0,0,21,65,1,0,0,0,23,72,1,0,0,0,25,26,5,124,
-        0,0,26,27,5,124,0,0,27,2,1,0,0,0,28,29,5,38,0,0,29,30,5,38,0,0,30,
-        4,1,0,0,0,31,32,5,61,0,0,32,33,5,61,0,0,33,6,1,0,0,0,34,35,5,33,
-        0,0,35,36,5,61,0,0,36,8,1,0,0,0,37,38,5,61,0,0,38,39,5,126,0,0,39,
-        10,1,0,0,0,40,41,5,33,0,0,41,42,5,126,0,0,42,12,1,0,0,0,43,44,5,
-        40,0,0,44,14,1,0,0,0,45,46,5,41,0,0,46,16,1,0,0,0,47,51,5,34,0,0,
-        48,50,8,0,0,0,49,48,1,0,0,0,50,53,1,0,0,0,51,49,1,0,0,0,51,52,1,
-        0,0,0,52,54,1,0,0,0,53,51,1,0,0,0,54,55,5,34,0,0,55,18,1,0,0,0,56,
-        60,5,47,0,0,57,59,8,1,0,0,58,57,1,0,0,0,59,62,1,0,0,0,60,58,1,0,
-        0,0,60,61,1,0,0,0,61,63,1,0,0,0,62,60,1,0,0,0,63,64,5,47,0,0,64,
-        20,1,0,0,0,65,67,5,36,0,0,66,68,7,2,0,0,67,66,1,0,0,0,68,69,1,0,
-        0,0,69,67,1,0,0,0,69,70,1,0,0,0,70,22,1,0,0,0,71,73,7,3,0,0,72,71,
-        1,0,0,0,73,74,1,0,0,0,74,72,1,0,0,0,74,75,1,0,0,0,75,76,1,0,0,0,
-        76,77,6,11,0,0,77,24,1,0,0,0,5,0,51,60,69,74,1,6,0,0
-    ]
-
-class GitlabRuleLexer(Lexer):
-
-    atn = ATNDeserializer().deserialize(serializedATN())
-
-    decisionsToDFA = [ DFA(ds, i) for i, ds in enumerate(atn.decisionToState) ]
-
-    OR = 1
-    AND = 2
-    EQ = 3
-    NE = 4
-    MATCH = 5
-    NMATCH = 6
-    OPAR = 7
-    CPAR = 8
-    STRING = 9
-    REGEX = 10
-    VARIABLE = 11
-    WHITESPACE = 12
-
-    channelNames = [ u"DEFAULT_TOKEN_CHANNEL", u"HIDDEN" ]
-
-    modeNames = [ "DEFAULT_MODE" ]
-
-    literalNames = [ "<INVALID>",
-            "'||'", "'&&'", "'=='", "'!='", "'=~'", "'!~'", "'('", "')'" ]
-
-    symbolicNames = [ "<INVALID>",
-            "OR", "AND", "EQ", "NE", "MATCH", "NMATCH", "OPAR", "CPAR", 
-            "STRING", "REGEX", "VARIABLE", "WHITESPACE" ]
-
-    ruleNames = [ "OR", "AND", "EQ", "NE", "MATCH", "NMATCH", "OPAR", "CPAR", 
-                  "STRING", "REGEX", "VARIABLE", "WHITESPACE" ]
-
-    grammarFileName = "GitlabRule.g4"
-
-    def __init__(self, input=None, output:TextIO = sys.stdout):
-        super().__init__(input, output)
-        self.checkVersion("4.11.1")
-        self._interp = LexerATNSimulator(self, self.atn, self.decisionsToDFA, PredictionContextCache())
-        self._actions = None
-        self._predicates = None
-
-
+# Generated from GitlabRule.g4 by ANTLR 4.11.1
+from antlr4 import *
+from io import StringIO
+import sys
+if sys.version_info[1] > 5:
+    from typing import TextIO
+else:
+    from typing.io import TextIO
+
+
+def serializedATN():
+    return [
+        4,0,12,78,6,-1,2,0,7,0,2,1,7,1,2,2,7,2,2,3,7,3,2,4,7,4,2,5,7,5,2,
+        6,7,6,2,7,7,7,2,8,7,8,2,9,7,9,2,10,7,10,2,11,7,11,1,0,1,0,1,0,1,
+        1,1,1,1,1,1,2,1,2,1,2,1,3,1,3,1,3,1,4,1,4,1,4,1,5,1,5,1,5,1,6,1,
+        6,1,7,1,7,1,8,1,8,5,8,50,8,8,10,8,12,8,53,9,8,1,8,1,8,1,9,1,9,5,
+        9,59,8,9,10,9,12,9,62,9,9,1,9,1,9,1,10,1,10,4,10,68,8,10,11,10,12,
+        10,69,1,11,4,11,73,8,11,11,11,12,11,74,1,11,1,11,0,0,12,1,1,3,2,
+        5,3,7,4,9,5,11,6,13,7,15,8,17,9,19,10,21,11,23,12,1,0,4,1,0,34,34,
+        1,0,47,47,4,0,48,57,65,90,95,95,97,122,3,0,9,10,13,13,32,32,81,0,
+        1,1,0,0,0,0,3,1,0,0,0,0,5,1,0,0,0,0,7,1,0,0,0,0,9,1,0,0,0,0,11,1,
+        0,0,0,0,13,1,0,0,0,0,15,1,0,0,0,0,17,1,0,0,0,0,19,1,0,0,0,0,21,1,
+        0,0,0,0,23,1,0,0,0,1,25,1,0,0,0,3,28,1,0,0,0,5,31,1,0,0,0,7,34,1,
+        0,0,0,9,37,1,0,0,0,11,40,1,0,0,0,13,43,1,0,0,0,15,45,1,0,0,0,17,
+        47,1,0,0,0,19,56,1,0,0,0,21,65,1,0,0,0,23,72,1,0,0,0,25,26,5,124,
+        0,0,26,27,5,124,0,0,27,2,1,0,0,0,28,29,5,38,0,0,29,30,5,38,0,0,30,
+        4,1,0,0,0,31,32,5,61,0,0,32,33,5,61,0,0,33,6,1,0,0,0,34,35,5,33,
+        0,0,35,36,5,61,0,0,36,8,1,0,0,0,37,38,5,61,0,0,38,39,5,126,0,0,39,
+        10,1,0,0,0,40,41,5,33,0,0,41,42,5,126,0,0,42,12,1,0,0,0,43,44,5,
+        40,0,0,44,14,1,0,0,0,45,46,5,41,0,0,46,16,1,0,0,0,47,51,5,34,0,0,
+        48,50,8,0,0,0,49,48,1,0,0,0,50,53,1,0,0,0,51,49,1,0,0,0,51,52,1,
+        0,0,0,52,54,1,0,0,0,53,51,1,0,0,0,54,55,5,34,0,0,55,18,1,0,0,0,56,
+        60,5,47,0,0,57,59,8,1,0,0,58,57,1,0,0,0,59,62,1,0,0,0,60,58,1,0,
+        0,0,60,61,1,0,0,0,61,63,1,0,0,0,62,60,1,0,0,0,63,64,5,47,0,0,64,
+        20,1,0,0,0,65,67,5,36,0,0,66,68,7,2,0,0,67,66,1,0,0,0,68,69,1,0,
+        0,0,69,67,1,0,0,0,69,70,1,0,0,0,70,22,1,0,0,0,71,73,7,3,0,0,72,71,
+        1,0,0,0,73,74,1,0,0,0,74,72,1,0,0,0,74,75,1,0,0,0,75,76,1,0,0,0,
+        76,77,6,11,0,0,77,24,1,0,0,0,5,0,51,60,69,74,1,6,0,0
+    ]
+
+class GitlabRuleLexer(Lexer):
+
+    atn = ATNDeserializer().deserialize(serializedATN())
+
+    decisionsToDFA = [ DFA(ds, i) for i, ds in enumerate(atn.decisionToState) ]
+
+    OR = 1
+    AND = 2
+    EQ = 3
+    NE = 4
+    MATCH = 5
+    NMATCH = 6
+    OPAR = 7
+    CPAR = 8
+    STRING = 9
+    REGEX = 10
+    VARIABLE = 11
+    WHITESPACE = 12
+
+    channelNames = [ u"DEFAULT_TOKEN_CHANNEL", u"HIDDEN" ]
+
+    modeNames = [ "DEFAULT_MODE" ]
+
+    literalNames = [ "<INVALID>",
+            "'||'", "'&&'", "'=='", "'!='", "'=~'", "'!~'", "'('", "')'" ]
+
+    symbolicNames = [ "<INVALID>",
+            "OR", "AND", "EQ", "NE", "MATCH", "NMATCH", "OPAR", "CPAR", 
+            "STRING", "REGEX", "VARIABLE", "WHITESPACE" ]
+
+    ruleNames = [ "OR", "AND", "EQ", "NE", "MATCH", "NMATCH", "OPAR", "CPAR", 
+                  "STRING", "REGEX", "VARIABLE", "WHITESPACE" ]
+
+    grammarFileName = "GitlabRule.g4"
+
+    def __init__(self, input=None, output:TextIO = sys.stdout):
+        super().__init__(input, output)
+        self.checkVersion("4.11.1")
+        self._interp = LexerATNSimulator(self, self.atn, self.decisionsToDFA, PredictionContextCache())
+        self._actions = None
+        self._predicates = None
+
+
```

## gitlabemu/rules/GitlabRuleParser.py

 * *Ordering differences only*

```diff
@@ -1,424 +1,424 @@
-# Generated from GitlabRule.g4 by ANTLR 4.11.1
-# encoding: utf-8
-from antlr4 import *
-from io import StringIO
-import sys
-if sys.version_info[1] > 5:
-	from typing import TextIO
-else:
-	from typing.io import TextIO
-
-def serializedATN():
-    return [
-        4,1,12,35,2,0,7,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,
-        0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,3,0,22,8,0,1,0,1,0,1,0,1,0,1,0,1,0,
-        5,0,30,8,0,10,0,12,0,33,9,0,1,0,0,1,0,1,0,0,2,1,0,3,4,1,0,5,6,41,
-        0,21,1,0,0,0,2,3,6,0,-1,0,3,4,5,7,0,0,4,5,3,0,0,0,5,6,5,8,0,0,6,
-        22,1,0,0,0,7,22,5,11,0,0,8,22,5,10,0,0,9,10,5,11,0,0,10,11,7,0,0,
-        0,11,22,5,9,0,0,12,13,5,9,0,0,13,14,7,0,0,0,14,22,5,11,0,0,15,16,
-        5,11,0,0,16,17,7,1,0,0,17,22,5,10,0,0,18,19,5,11,0,0,19,20,7,1,0,
-        0,20,22,5,11,0,0,21,2,1,0,0,0,21,7,1,0,0,0,21,8,1,0,0,0,21,9,1,0,
-        0,0,21,12,1,0,0,0,21,15,1,0,0,0,21,18,1,0,0,0,22,31,1,0,0,0,23,24,
-        10,2,0,0,24,25,5,2,0,0,25,30,3,0,0,3,26,27,10,1,0,0,27,28,5,1,0,
-        0,28,30,3,0,0,2,29,23,1,0,0,0,29,26,1,0,0,0,30,33,1,0,0,0,31,29,
-        1,0,0,0,31,32,1,0,0,0,32,1,1,0,0,0,33,31,1,0,0,0,3,21,29,31
-    ]
-
-class GitlabRuleParser ( Parser ):
-
-    grammarFileName = "GitlabRule.g4"
-
-    atn = ATNDeserializer().deserialize(serializedATN())
-
-    decisionsToDFA = [ DFA(ds, i) for i, ds in enumerate(atn.decisionToState) ]
-
-    sharedContextCache = PredictionContextCache()
-
-    literalNames = [ "<INVALID>", "'||'", "'&&'", "'=='", "'!='", "'=~'", 
-                     "'!~'", "'('", "')'" ]
-
-    symbolicNames = [ "<INVALID>", "OR", "AND", "EQ", "NE", "MATCH", "NMATCH", 
-                      "OPAR", "CPAR", "STRING", "REGEX", "VARIABLE", "WHITESPACE" ]
-
-    RULE_expr = 0
-
-    ruleNames =  [ "expr" ]
-
-    EOF = Token.EOF
-    OR=1
-    AND=2
-    EQ=3
-    NE=4
-    MATCH=5
-    NMATCH=6
-    OPAR=7
-    CPAR=8
-    STRING=9
-    REGEX=10
-    VARIABLE=11
-    WHITESPACE=12
-
-    def __init__(self, input:TokenStream, output:TextIO = sys.stdout):
-        super().__init__(input, output)
-        self.checkVersion("4.11.1")
-        self._interp = ParserATNSimulator(self, self.atn, self.decisionsToDFA, self.sharedContextCache)
-        self._predicates = None
-
-
-
-
-    class ExprContext(ParserRuleContext):
-        __slots__ = 'parser'
-
-        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
-            super().__init__(parent, invokingState)
-            self.parser = parser
-
-
-        def getRuleIndex(self):
-            return GitlabRuleParser.RULE_expr
-
-     
-        def copyFrom(self, ctx:ParserRuleContext):
-            super().copyFrom(ctx)
-
-
-    class ParensContext(ExprContext):
-
-        def __init__(self, parser, ctx:ParserRuleContext): # actually a GitlabRuleParser.ExprContext
-            super().__init__(parser)
-            self.copyFrom(ctx)
-
-        def OPAR(self):
-            return self.getToken(GitlabRuleParser.OPAR, 0)
-        def expr(self):
-            return self.getTypedRuleContext(GitlabRuleParser.ExprContext,0)
-
-        def CPAR(self):
-            return self.getToken(GitlabRuleParser.CPAR, 0)
-
-        def accept(self, visitor:ParseTreeVisitor):
-            if hasattr( visitor, "visitParens" ):
-                return visitor.visitParens(self)
-            else:
-                return visitor.visitChildren(self)
-
-
-    class RegexContext(ExprContext):
-
-        def __init__(self, parser, ctx:ParserRuleContext): # actually a GitlabRuleParser.ExprContext
-            super().__init__(parser)
-            self.copyFrom(ctx)
-
-        def REGEX(self):
-            return self.getToken(GitlabRuleParser.REGEX, 0)
-
-        def accept(self, visitor:ParseTreeVisitor):
-            if hasattr( visitor, "visitRegex" ):
-                return visitor.visitRegex(self)
-            else:
-                return visitor.visitChildren(self)
-
-
-    class CompareContext(ExprContext):
-
-        def __init__(self, parser, ctx:ParserRuleContext): # actually a GitlabRuleParser.ExprContext
-            super().__init__(parser)
-            self.op = None # Token
-            self.copyFrom(ctx)
-
-        def VARIABLE(self):
-            return self.getToken(GitlabRuleParser.VARIABLE, 0)
-        def STRING(self):
-            return self.getToken(GitlabRuleParser.STRING, 0)
-        def EQ(self):
-            return self.getToken(GitlabRuleParser.EQ, 0)
-        def NE(self):
-            return self.getToken(GitlabRuleParser.NE, 0)
-
-        def accept(self, visitor:ParseTreeVisitor):
-            if hasattr( visitor, "visitCompare" ):
-                return visitor.visitCompare(self)
-            else:
-                return visitor.visitChildren(self)
-
-
-    class BoolOrContext(ExprContext):
-
-        def __init__(self, parser, ctx:ParserRuleContext): # actually a GitlabRuleParser.ExprContext
-            super().__init__(parser)
-            self.op = None # Token
-            self.copyFrom(ctx)
-
-        def expr(self, i:int=None):
-            if i is None:
-                return self.getTypedRuleContexts(GitlabRuleParser.ExprContext)
-            else:
-                return self.getTypedRuleContext(GitlabRuleParser.ExprContext,i)
-
-        def OR(self):
-            return self.getToken(GitlabRuleParser.OR, 0)
-
-        def accept(self, visitor:ParseTreeVisitor):
-            if hasattr( visitor, "visitBoolOr" ):
-                return visitor.visitBoolOr(self)
-            else:
-                return visitor.visitChildren(self)
-
-
-    class BoolAndContext(ExprContext):
-
-        def __init__(self, parser, ctx:ParserRuleContext): # actually a GitlabRuleParser.ExprContext
-            super().__init__(parser)
-            self.op = None # Token
-            self.copyFrom(ctx)
-
-        def expr(self, i:int=None):
-            if i is None:
-                return self.getTypedRuleContexts(GitlabRuleParser.ExprContext)
-            else:
-                return self.getTypedRuleContext(GitlabRuleParser.ExprContext,i)
-
-        def AND(self):
-            return self.getToken(GitlabRuleParser.AND, 0)
-
-        def accept(self, visitor:ParseTreeVisitor):
-            if hasattr( visitor, "visitBoolAnd" ):
-                return visitor.visitBoolAnd(self)
-            else:
-                return visitor.visitChildren(self)
-
-
-    class VariableContext(ExprContext):
-
-        def __init__(self, parser, ctx:ParserRuleContext): # actually a GitlabRuleParser.ExprContext
-            super().__init__(parser)
-            self.copyFrom(ctx)
-
-        def VARIABLE(self):
-            return self.getToken(GitlabRuleParser.VARIABLE, 0)
-
-        def accept(self, visitor:ParseTreeVisitor):
-            if hasattr( visitor, "visitVariable" ):
-                return visitor.visitVariable(self)
-            else:
-                return visitor.visitChildren(self)
-
-
-    class MatchContext(ExprContext):
-
-        def __init__(self, parser, ctx:ParserRuleContext): # actually a GitlabRuleParser.ExprContext
-            super().__init__(parser)
-            self.op = None # Token
-            self.copyFrom(ctx)
-
-        def VARIABLE(self, i:int=None):
-            if i is None:
-                return self.getTokens(GitlabRuleParser.VARIABLE)
-            else:
-                return self.getToken(GitlabRuleParser.VARIABLE, i)
-        def REGEX(self):
-            return self.getToken(GitlabRuleParser.REGEX, 0)
-        def MATCH(self):
-            return self.getToken(GitlabRuleParser.MATCH, 0)
-        def NMATCH(self):
-            return self.getToken(GitlabRuleParser.NMATCH, 0)
-
-        def accept(self, visitor:ParseTreeVisitor):
-            if hasattr( visitor, "visitMatch" ):
-                return visitor.visitMatch(self)
-            else:
-                return visitor.visitChildren(self)
-
-
-
-    def expr(self, _p:int=0):
-        _parentctx = self._ctx
-        _parentState = self.state
-        localctx = GitlabRuleParser.ExprContext(self, self._ctx, _parentState)
-        _prevctx = localctx
-        _startState = 0
-        self.enterRecursionRule(localctx, 0, self.RULE_expr, _p)
-        self._la = 0 # Token type
-        try:
-            self.enterOuterAlt(localctx, 1)
-            self.state = 21
-            self._errHandler.sync(self)
-            la_ = self._interp.adaptivePredict(self._input,0,self._ctx)
-            if la_ == 1:
-                localctx = GitlabRuleParser.ParensContext(self, localctx)
-                self._ctx = localctx
-                _prevctx = localctx
-
-                self.state = 3
-                self.match(GitlabRuleParser.OPAR)
-                self.state = 4
-                self.expr(0)
-                self.state = 5
-                self.match(GitlabRuleParser.CPAR)
-                pass
-
-            elif la_ == 2:
-                localctx = GitlabRuleParser.VariableContext(self, localctx)
-                self._ctx = localctx
-                _prevctx = localctx
-                self.state = 7
-                self.match(GitlabRuleParser.VARIABLE)
-                pass
-
-            elif la_ == 3:
-                localctx = GitlabRuleParser.RegexContext(self, localctx)
-                self._ctx = localctx
-                _prevctx = localctx
-                self.state = 8
-                self.match(GitlabRuleParser.REGEX)
-                pass
-
-            elif la_ == 4:
-                localctx = GitlabRuleParser.CompareContext(self, localctx)
-                self._ctx = localctx
-                _prevctx = localctx
-                self.state = 9
-                self.match(GitlabRuleParser.VARIABLE)
-                self.state = 10
-                localctx.op = self._input.LT(1)
-                _la = self._input.LA(1)
-                if not(_la==3 or _la==4):
-                    localctx.op = self._errHandler.recoverInline(self)
-                else:
-                    self._errHandler.reportMatch(self)
-                    self.consume()
-                self.state = 11
-                self.match(GitlabRuleParser.STRING)
-                pass
-
-            elif la_ == 5:
-                localctx = GitlabRuleParser.CompareContext(self, localctx)
-                self._ctx = localctx
-                _prevctx = localctx
-                self.state = 12
-                self.match(GitlabRuleParser.STRING)
-                self.state = 13
-                localctx.op = self._input.LT(1)
-                _la = self._input.LA(1)
-                if not(_la==3 or _la==4):
-                    localctx.op = self._errHandler.recoverInline(self)
-                else:
-                    self._errHandler.reportMatch(self)
-                    self.consume()
-                self.state = 14
-                self.match(GitlabRuleParser.VARIABLE)
-                pass
-
-            elif la_ == 6:
-                localctx = GitlabRuleParser.MatchContext(self, localctx)
-                self._ctx = localctx
-                _prevctx = localctx
-                self.state = 15
-                self.match(GitlabRuleParser.VARIABLE)
-                self.state = 16
-                localctx.op = self._input.LT(1)
-                _la = self._input.LA(1)
-                if not(_la==5 or _la==6):
-                    localctx.op = self._errHandler.recoverInline(self)
-                else:
-                    self._errHandler.reportMatch(self)
-                    self.consume()
-                self.state = 17
-                self.match(GitlabRuleParser.REGEX)
-                pass
-
-            elif la_ == 7:
-                localctx = GitlabRuleParser.MatchContext(self, localctx)
-                self._ctx = localctx
-                _prevctx = localctx
-                self.state = 18
-                self.match(GitlabRuleParser.VARIABLE)
-                self.state = 19
-                localctx.op = self._input.LT(1)
-                _la = self._input.LA(1)
-                if not(_la==5 or _la==6):
-                    localctx.op = self._errHandler.recoverInline(self)
-                else:
-                    self._errHandler.reportMatch(self)
-                    self.consume()
-                self.state = 20
-                self.match(GitlabRuleParser.VARIABLE)
-                pass
-
-
-            self._ctx.stop = self._input.LT(-1)
-            self.state = 31
-            self._errHandler.sync(self)
-            _alt = self._interp.adaptivePredict(self._input,2,self._ctx)
-            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
-                if _alt==1:
-                    if self._parseListeners is not None:
-                        self.triggerExitRuleEvent()
-                    _prevctx = localctx
-                    self.state = 29
-                    self._errHandler.sync(self)
-                    la_ = self._interp.adaptivePredict(self._input,1,self._ctx)
-                    if la_ == 1:
-                        localctx = GitlabRuleParser.BoolAndContext(self, GitlabRuleParser.ExprContext(self, _parentctx, _parentState))
-                        self.pushNewRecursionContext(localctx, _startState, self.RULE_expr)
-                        self.state = 23
-                        if not self.precpred(self._ctx, 2):
-                            from antlr4.error.Errors import FailedPredicateException
-                            raise FailedPredicateException(self, "self.precpred(self._ctx, 2)")
-                        self.state = 24
-                        localctx.op = self.match(GitlabRuleParser.AND)
-                        self.state = 25
-                        self.expr(3)
-                        pass
-
-                    elif la_ == 2:
-                        localctx = GitlabRuleParser.BoolOrContext(self, GitlabRuleParser.ExprContext(self, _parentctx, _parentState))
-                        self.pushNewRecursionContext(localctx, _startState, self.RULE_expr)
-                        self.state = 26
-                        if not self.precpred(self._ctx, 1):
-                            from antlr4.error.Errors import FailedPredicateException
-                            raise FailedPredicateException(self, "self.precpred(self._ctx, 1)")
-                        self.state = 27
-                        localctx.op = self.match(GitlabRuleParser.OR)
-                        self.state = 28
-                        self.expr(2)
-                        pass
-
-             
-                self.state = 33
-                self._errHandler.sync(self)
-                _alt = self._interp.adaptivePredict(self._input,2,self._ctx)
-
-        except RecognitionException as re:
-            localctx.exception = re
-            self._errHandler.reportError(self, re)
-            self._errHandler.recover(self, re)
-        finally:
-            self.unrollRecursionContexts(_parentctx)
-        return localctx
-
-
-
-    def sempred(self, localctx:RuleContext, ruleIndex:int, predIndex:int):
-        if self._predicates == None:
-            self._predicates = dict()
-        self._predicates[0] = self.expr_sempred
-        pred = self._predicates.get(ruleIndex, None)
-        if pred is None:
-            raise Exception("No predicate with index:" + str(ruleIndex))
-        else:
-            return pred(localctx, predIndex)
-
-    def expr_sempred(self, localctx:ExprContext, predIndex:int):
-            if predIndex == 0:
-                return self.precpred(self._ctx, 2)
-         
-
-            if predIndex == 1:
-                return self.precpred(self._ctx, 1)
-         
-
-
-
-
+# Generated from GitlabRule.g4 by ANTLR 4.11.1
+# encoding: utf-8
+from antlr4 import *
+from io import StringIO
+import sys
+if sys.version_info[1] > 5:
+	from typing import TextIO
+else:
+	from typing.io import TextIO
+
+def serializedATN():
+    return [
+        4,1,12,35,2,0,7,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,
+        0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,3,0,22,8,0,1,0,1,0,1,0,1,0,1,0,1,0,
+        5,0,30,8,0,10,0,12,0,33,9,0,1,0,0,1,0,1,0,0,2,1,0,3,4,1,0,5,6,41,
+        0,21,1,0,0,0,2,3,6,0,-1,0,3,4,5,7,0,0,4,5,3,0,0,0,5,6,5,8,0,0,6,
+        22,1,0,0,0,7,22,5,11,0,0,8,22,5,10,0,0,9,10,5,11,0,0,10,11,7,0,0,
+        0,11,22,5,9,0,0,12,13,5,9,0,0,13,14,7,0,0,0,14,22,5,11,0,0,15,16,
+        5,11,0,0,16,17,7,1,0,0,17,22,5,10,0,0,18,19,5,11,0,0,19,20,7,1,0,
+        0,20,22,5,11,0,0,21,2,1,0,0,0,21,7,1,0,0,0,21,8,1,0,0,0,21,9,1,0,
+        0,0,21,12,1,0,0,0,21,15,1,0,0,0,21,18,1,0,0,0,22,31,1,0,0,0,23,24,
+        10,2,0,0,24,25,5,2,0,0,25,30,3,0,0,3,26,27,10,1,0,0,27,28,5,1,0,
+        0,28,30,3,0,0,2,29,23,1,0,0,0,29,26,1,0,0,0,30,33,1,0,0,0,31,29,
+        1,0,0,0,31,32,1,0,0,0,32,1,1,0,0,0,33,31,1,0,0,0,3,21,29,31
+    ]
+
+class GitlabRuleParser ( Parser ):
+
+    grammarFileName = "GitlabRule.g4"
+
+    atn = ATNDeserializer().deserialize(serializedATN())
+
+    decisionsToDFA = [ DFA(ds, i) for i, ds in enumerate(atn.decisionToState) ]
+
+    sharedContextCache = PredictionContextCache()
+
+    literalNames = [ "<INVALID>", "'||'", "'&&'", "'=='", "'!='", "'=~'", 
+                     "'!~'", "'('", "')'" ]
+
+    symbolicNames = [ "<INVALID>", "OR", "AND", "EQ", "NE", "MATCH", "NMATCH", 
+                      "OPAR", "CPAR", "STRING", "REGEX", "VARIABLE", "WHITESPACE" ]
+
+    RULE_expr = 0
+
+    ruleNames =  [ "expr" ]
+
+    EOF = Token.EOF
+    OR=1
+    AND=2
+    EQ=3
+    NE=4
+    MATCH=5
+    NMATCH=6
+    OPAR=7
+    CPAR=8
+    STRING=9
+    REGEX=10
+    VARIABLE=11
+    WHITESPACE=12
+
+    def __init__(self, input:TokenStream, output:TextIO = sys.stdout):
+        super().__init__(input, output)
+        self.checkVersion("4.11.1")
+        self._interp = ParserATNSimulator(self, self.atn, self.decisionsToDFA, self.sharedContextCache)
+        self._predicates = None
+
+
+
+
+    class ExprContext(ParserRuleContext):
+        __slots__ = 'parser'
+
+        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
+            super().__init__(parent, invokingState)
+            self.parser = parser
+
+
+        def getRuleIndex(self):
+            return GitlabRuleParser.RULE_expr
+
+     
+        def copyFrom(self, ctx:ParserRuleContext):
+            super().copyFrom(ctx)
+
+
+    class ParensContext(ExprContext):
+
+        def __init__(self, parser, ctx:ParserRuleContext): # actually a GitlabRuleParser.ExprContext
+            super().__init__(parser)
+            self.copyFrom(ctx)
+
+        def OPAR(self):
+            return self.getToken(GitlabRuleParser.OPAR, 0)
+        def expr(self):
+            return self.getTypedRuleContext(GitlabRuleParser.ExprContext,0)
+
+        def CPAR(self):
+            return self.getToken(GitlabRuleParser.CPAR, 0)
+
+        def accept(self, visitor:ParseTreeVisitor):
+            if hasattr( visitor, "visitParens" ):
+                return visitor.visitParens(self)
+            else:
+                return visitor.visitChildren(self)
+
+
+    class RegexContext(ExprContext):
+
+        def __init__(self, parser, ctx:ParserRuleContext): # actually a GitlabRuleParser.ExprContext
+            super().__init__(parser)
+            self.copyFrom(ctx)
+
+        def REGEX(self):
+            return self.getToken(GitlabRuleParser.REGEX, 0)
+
+        def accept(self, visitor:ParseTreeVisitor):
+            if hasattr( visitor, "visitRegex" ):
+                return visitor.visitRegex(self)
+            else:
+                return visitor.visitChildren(self)
+
+
+    class CompareContext(ExprContext):
+
+        def __init__(self, parser, ctx:ParserRuleContext): # actually a GitlabRuleParser.ExprContext
+            super().__init__(parser)
+            self.op = None # Token
+            self.copyFrom(ctx)
+
+        def VARIABLE(self):
+            return self.getToken(GitlabRuleParser.VARIABLE, 0)
+        def STRING(self):
+            return self.getToken(GitlabRuleParser.STRING, 0)
+        def EQ(self):
+            return self.getToken(GitlabRuleParser.EQ, 0)
+        def NE(self):
+            return self.getToken(GitlabRuleParser.NE, 0)
+
+        def accept(self, visitor:ParseTreeVisitor):
+            if hasattr( visitor, "visitCompare" ):
+                return visitor.visitCompare(self)
+            else:
+                return visitor.visitChildren(self)
+
+
+    class BoolOrContext(ExprContext):
+
+        def __init__(self, parser, ctx:ParserRuleContext): # actually a GitlabRuleParser.ExprContext
+            super().__init__(parser)
+            self.op = None # Token
+            self.copyFrom(ctx)
+
+        def expr(self, i:int=None):
+            if i is None:
+                return self.getTypedRuleContexts(GitlabRuleParser.ExprContext)
+            else:
+                return self.getTypedRuleContext(GitlabRuleParser.ExprContext,i)
+
+        def OR(self):
+            return self.getToken(GitlabRuleParser.OR, 0)
+
+        def accept(self, visitor:ParseTreeVisitor):
+            if hasattr( visitor, "visitBoolOr" ):
+                return visitor.visitBoolOr(self)
+            else:
+                return visitor.visitChildren(self)
+
+
+    class BoolAndContext(ExprContext):
+
+        def __init__(self, parser, ctx:ParserRuleContext): # actually a GitlabRuleParser.ExprContext
+            super().__init__(parser)
+            self.op = None # Token
+            self.copyFrom(ctx)
+
+        def expr(self, i:int=None):
+            if i is None:
+                return self.getTypedRuleContexts(GitlabRuleParser.ExprContext)
+            else:
+                return self.getTypedRuleContext(GitlabRuleParser.ExprContext,i)
+
+        def AND(self):
+            return self.getToken(GitlabRuleParser.AND, 0)
+
+        def accept(self, visitor:ParseTreeVisitor):
+            if hasattr( visitor, "visitBoolAnd" ):
+                return visitor.visitBoolAnd(self)
+            else:
+                return visitor.visitChildren(self)
+
+
+    class VariableContext(ExprContext):
+
+        def __init__(self, parser, ctx:ParserRuleContext): # actually a GitlabRuleParser.ExprContext
+            super().__init__(parser)
+            self.copyFrom(ctx)
+
+        def VARIABLE(self):
+            return self.getToken(GitlabRuleParser.VARIABLE, 0)
+
+        def accept(self, visitor:ParseTreeVisitor):
+            if hasattr( visitor, "visitVariable" ):
+                return visitor.visitVariable(self)
+            else:
+                return visitor.visitChildren(self)
+
+
+    class MatchContext(ExprContext):
+
+        def __init__(self, parser, ctx:ParserRuleContext): # actually a GitlabRuleParser.ExprContext
+            super().__init__(parser)
+            self.op = None # Token
+            self.copyFrom(ctx)
+
+        def VARIABLE(self, i:int=None):
+            if i is None:
+                return self.getTokens(GitlabRuleParser.VARIABLE)
+            else:
+                return self.getToken(GitlabRuleParser.VARIABLE, i)
+        def REGEX(self):
+            return self.getToken(GitlabRuleParser.REGEX, 0)
+        def MATCH(self):
+            return self.getToken(GitlabRuleParser.MATCH, 0)
+        def NMATCH(self):
+            return self.getToken(GitlabRuleParser.NMATCH, 0)
+
+        def accept(self, visitor:ParseTreeVisitor):
+            if hasattr( visitor, "visitMatch" ):
+                return visitor.visitMatch(self)
+            else:
+                return visitor.visitChildren(self)
+
+
+
+    def expr(self, _p:int=0):
+        _parentctx = self._ctx
+        _parentState = self.state
+        localctx = GitlabRuleParser.ExprContext(self, self._ctx, _parentState)
+        _prevctx = localctx
+        _startState = 0
+        self.enterRecursionRule(localctx, 0, self.RULE_expr, _p)
+        self._la = 0 # Token type
+        try:
+            self.enterOuterAlt(localctx, 1)
+            self.state = 21
+            self._errHandler.sync(self)
+            la_ = self._interp.adaptivePredict(self._input,0,self._ctx)
+            if la_ == 1:
+                localctx = GitlabRuleParser.ParensContext(self, localctx)
+                self._ctx = localctx
+                _prevctx = localctx
+
+                self.state = 3
+                self.match(GitlabRuleParser.OPAR)
+                self.state = 4
+                self.expr(0)
+                self.state = 5
+                self.match(GitlabRuleParser.CPAR)
+                pass
+
+            elif la_ == 2:
+                localctx = GitlabRuleParser.VariableContext(self, localctx)
+                self._ctx = localctx
+                _prevctx = localctx
+                self.state = 7
+                self.match(GitlabRuleParser.VARIABLE)
+                pass
+
+            elif la_ == 3:
+                localctx = GitlabRuleParser.RegexContext(self, localctx)
+                self._ctx = localctx
+                _prevctx = localctx
+                self.state = 8
+                self.match(GitlabRuleParser.REGEX)
+                pass
+
+            elif la_ == 4:
+                localctx = GitlabRuleParser.CompareContext(self, localctx)
+                self._ctx = localctx
+                _prevctx = localctx
+                self.state = 9
+                self.match(GitlabRuleParser.VARIABLE)
+                self.state = 10
+                localctx.op = self._input.LT(1)
+                _la = self._input.LA(1)
+                if not(_la==3 or _la==4):
+                    localctx.op = self._errHandler.recoverInline(self)
+                else:
+                    self._errHandler.reportMatch(self)
+                    self.consume()
+                self.state = 11
+                self.match(GitlabRuleParser.STRING)
+                pass
+
+            elif la_ == 5:
+                localctx = GitlabRuleParser.CompareContext(self, localctx)
+                self._ctx = localctx
+                _prevctx = localctx
+                self.state = 12
+                self.match(GitlabRuleParser.STRING)
+                self.state = 13
+                localctx.op = self._input.LT(1)
+                _la = self._input.LA(1)
+                if not(_la==3 or _la==4):
+                    localctx.op = self._errHandler.recoverInline(self)
+                else:
+                    self._errHandler.reportMatch(self)
+                    self.consume()
+                self.state = 14
+                self.match(GitlabRuleParser.VARIABLE)
+                pass
+
+            elif la_ == 6:
+                localctx = GitlabRuleParser.MatchContext(self, localctx)
+                self._ctx = localctx
+                _prevctx = localctx
+                self.state = 15
+                self.match(GitlabRuleParser.VARIABLE)
+                self.state = 16
+                localctx.op = self._input.LT(1)
+                _la = self._input.LA(1)
+                if not(_la==5 or _la==6):
+                    localctx.op = self._errHandler.recoverInline(self)
+                else:
+                    self._errHandler.reportMatch(self)
+                    self.consume()
+                self.state = 17
+                self.match(GitlabRuleParser.REGEX)
+                pass
+
+            elif la_ == 7:
+                localctx = GitlabRuleParser.MatchContext(self, localctx)
+                self._ctx = localctx
+                _prevctx = localctx
+                self.state = 18
+                self.match(GitlabRuleParser.VARIABLE)
+                self.state = 19
+                localctx.op = self._input.LT(1)
+                _la = self._input.LA(1)
+                if not(_la==5 or _la==6):
+                    localctx.op = self._errHandler.recoverInline(self)
+                else:
+                    self._errHandler.reportMatch(self)
+                    self.consume()
+                self.state = 20
+                self.match(GitlabRuleParser.VARIABLE)
+                pass
+
+
+            self._ctx.stop = self._input.LT(-1)
+            self.state = 31
+            self._errHandler.sync(self)
+            _alt = self._interp.adaptivePredict(self._input,2,self._ctx)
+            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
+                if _alt==1:
+                    if self._parseListeners is not None:
+                        self.triggerExitRuleEvent()
+                    _prevctx = localctx
+                    self.state = 29
+                    self._errHandler.sync(self)
+                    la_ = self._interp.adaptivePredict(self._input,1,self._ctx)
+                    if la_ == 1:
+                        localctx = GitlabRuleParser.BoolAndContext(self, GitlabRuleParser.ExprContext(self, _parentctx, _parentState))
+                        self.pushNewRecursionContext(localctx, _startState, self.RULE_expr)
+                        self.state = 23
+                        if not self.precpred(self._ctx, 2):
+                            from antlr4.error.Errors import FailedPredicateException
+                            raise FailedPredicateException(self, "self.precpred(self._ctx, 2)")
+                        self.state = 24
+                        localctx.op = self.match(GitlabRuleParser.AND)
+                        self.state = 25
+                        self.expr(3)
+                        pass
+
+                    elif la_ == 2:
+                        localctx = GitlabRuleParser.BoolOrContext(self, GitlabRuleParser.ExprContext(self, _parentctx, _parentState))
+                        self.pushNewRecursionContext(localctx, _startState, self.RULE_expr)
+                        self.state = 26
+                        if not self.precpred(self._ctx, 1):
+                            from antlr4.error.Errors import FailedPredicateException
+                            raise FailedPredicateException(self, "self.precpred(self._ctx, 1)")
+                        self.state = 27
+                        localctx.op = self.match(GitlabRuleParser.OR)
+                        self.state = 28
+                        self.expr(2)
+                        pass
+
+             
+                self.state = 33
+                self._errHandler.sync(self)
+                _alt = self._interp.adaptivePredict(self._input,2,self._ctx)
+
+        except RecognitionException as re:
+            localctx.exception = re
+            self._errHandler.reportError(self, re)
+            self._errHandler.recover(self, re)
+        finally:
+            self.unrollRecursionContexts(_parentctx)
+        return localctx
+
+
+
+    def sempred(self, localctx:RuleContext, ruleIndex:int, predIndex:int):
+        if self._predicates == None:
+            self._predicates = dict()
+        self._predicates[0] = self.expr_sempred
+        pred = self._predicates.get(ruleIndex, None)
+        if pred is None:
+            raise Exception("No predicate with index:" + str(ruleIndex))
+        else:
+            return pred(localctx, predIndex)
+
+    def expr_sempred(self, localctx:ExprContext, predIndex:int):
+            if predIndex == 0:
+                return self.precpred(self._ctx, 2)
+         
+
+            if predIndex == 1:
+                return self.precpred(self._ctx, 1)
+         
+
+
+
+
```

## gitlabemu/rules/GitlabRuleVisitor.py

 * *Ordering differences only*

```diff
@@ -1,48 +1,48 @@
-# Generated from GitlabRule.g4 by ANTLR 4.11.1
-from antlr4 import *
-if __name__ is not None and "." in __name__:
-    from .GitlabRuleParser import GitlabRuleParser
-else:
-    from GitlabRuleParser import GitlabRuleParser
-
-# This class defines a complete generic visitor for a parse tree produced by GitlabRuleParser.
-
-class GitlabRuleVisitor(ParseTreeVisitor):
-
-    # Visit a parse tree produced by GitlabRuleParser#parens.
-    def visitParens(self, ctx:GitlabRuleParser.ParensContext):
-        return self.visitChildren(ctx)
-
-
-    # Visit a parse tree produced by GitlabRuleParser#regex.
-    def visitRegex(self, ctx:GitlabRuleParser.RegexContext):
-        return self.visitChildren(ctx)
-
-
-    # Visit a parse tree produced by GitlabRuleParser#compare.
-    def visitCompare(self, ctx:GitlabRuleParser.CompareContext):
-        return self.visitChildren(ctx)
-
-
-    # Visit a parse tree produced by GitlabRuleParser#boolOr.
-    def visitBoolOr(self, ctx:GitlabRuleParser.BoolOrContext):
-        return self.visitChildren(ctx)
-
-
-    # Visit a parse tree produced by GitlabRuleParser#boolAnd.
-    def visitBoolAnd(self, ctx:GitlabRuleParser.BoolAndContext):
-        return self.visitChildren(ctx)
-
-
-    # Visit a parse tree produced by GitlabRuleParser#variable.
-    def visitVariable(self, ctx:GitlabRuleParser.VariableContext):
-        return self.visitChildren(ctx)
-
-
-    # Visit a parse tree produced by GitlabRuleParser#match.
-    def visitMatch(self, ctx:GitlabRuleParser.MatchContext):
-        return self.visitChildren(ctx)
-
-
-
+# Generated from GitlabRule.g4 by ANTLR 4.11.1
+from antlr4 import *
+if __name__ is not None and "." in __name__:
+    from .GitlabRuleParser import GitlabRuleParser
+else:
+    from GitlabRuleParser import GitlabRuleParser
+
+# This class defines a complete generic visitor for a parse tree produced by GitlabRuleParser.
+
+class GitlabRuleVisitor(ParseTreeVisitor):
+
+    # Visit a parse tree produced by GitlabRuleParser#parens.
+    def visitParens(self, ctx:GitlabRuleParser.ParensContext):
+        return self.visitChildren(ctx)
+
+
+    # Visit a parse tree produced by GitlabRuleParser#regex.
+    def visitRegex(self, ctx:GitlabRuleParser.RegexContext):
+        return self.visitChildren(ctx)
+
+
+    # Visit a parse tree produced by GitlabRuleParser#compare.
+    def visitCompare(self, ctx:GitlabRuleParser.CompareContext):
+        return self.visitChildren(ctx)
+
+
+    # Visit a parse tree produced by GitlabRuleParser#boolOr.
+    def visitBoolOr(self, ctx:GitlabRuleParser.BoolOrContext):
+        return self.visitChildren(ctx)
+
+
+    # Visit a parse tree produced by GitlabRuleParser#boolAnd.
+    def visitBoolAnd(self, ctx:GitlabRuleParser.BoolAndContext):
+        return self.visitChildren(ctx)
+
+
+    # Visit a parse tree produced by GitlabRuleParser#variable.
+    def visitVariable(self, ctx:GitlabRuleParser.VariableContext):
+        return self.visitChildren(ctx)
+
+
+    # Visit a parse tree produced by GitlabRuleParser#match.
+    def visitMatch(self, ctx:GitlabRuleParser.MatchContext):
+        return self.visitChildren(ctx)
+
+
+
 del GitlabRuleParser
```

## Comparing `gitlab_emulator-1.5.6.dist-info/METADATA` & `gitlab_emulator-1.5.7.dist-info/METADATA`

 * *Files 10% similar despite different names*

```diff
@@ -1,18 +1,18 @@
-Metadata-Version: 2.1
-Name: gitlab-emulator
-Version: 1.5.6
-Summary: Run/Inspect a .gitlab-ci.yml jobs and pipelines locally
-Home-page: https://gitlab.com/cunity/gitlab-emulator
-Author: Ian Norton
-Author-email: inorton@gmail.com
-License: License :: OSI Approved :: MIT License
-Platform: any
-Requires-Dist: pyyaml (>=5.1)
-Requires-Dist: requests (>=2.25.0)
-Requires-Dist: requests-toolbelt (>=0.9.1)
-Requires-Dist: certifi (>=2022.6)
-Requires-Dist: antlr4-python3-runtime (==4.11.1)
-Requires-Dist: python-gitlab (==2.10.1) ; python_version <= "3.6"
-Requires-Dist: python-gitlab (>=3.2.0) ; python_version >= "3.7"
-
-Run a subset of .gitlab-ci.yml jobs locally using docker
+Metadata-Version: 2.1
+Name: gitlab-emulator
+Version: 1.5.7
+Summary: Run/Inspect a .gitlab-ci.yml jobs and pipelines locally
+Home-page: https://gitlab.com/cunity/gitlab-emulator
+Author: Ian Norton
+Author-email: inorton@gmail.com
+License: License :: OSI Approved :: MIT License
+Platform: any
+Requires-Dist: pyyaml (>=5.1)
+Requires-Dist: requests (>=2.25.0)
+Requires-Dist: requests-toolbelt (>=0.9.1)
+Requires-Dist: certifi (>=2022.6)
+Requires-Dist: antlr4-python3-runtime (==4.11.1)
+Requires-Dist: python-gitlab (==2.10.1) ; python_version <= "3.6"
+Requires-Dist: python-gitlab (>=3.2.0) ; python_version >= "3.7"
+
+Run a subset of .gitlab-ci.yml jobs locally using docker
```

## Comparing `gitlab_emulator-1.5.6.dist-info/RECORD` & `gitlab_emulator-1.5.7.dist-info/RECORD`

 * *Files 14% similar despite different names*

```diff
@@ -1,58 +1,58 @@
-gitlab_emulator-1.5.6.data/scripts/locallab.py,sha256=NMziiHAT5Iwx9cAekIfJ8-6pYXr0OWA6m6UeVO4_PMM,125
+gitlab_emulator-1.5.7.data/scripts/locallab.py,sha256=hbpEFIIZIz6Zvc12gg0WkvLAbsT8BesQmW2LIU-NQ38,133
 gitlabemu/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-gitlabemu/__main__.py,sha256=dIhMLC-IhR1Oj9OxFyS2KfqTOFx3F9o9Nf9IJMrpZeQ,76
-gitlabemu/ansi.py,sha256=zFmlWuE8a8llZ1sBzq0yON4YlpqCcbN3ZDjQ5K0n0VM,285
-gitlabemu/artifacts.py,sha256=O4mvdP6qGAUpGTrXtaOBRjpgnHU9DTisyMyCe9b1TRk,1043
-gitlabemu/chown.py,sha256=Ynf6_evsokd0P9TqhiIxJ9eGJbXoE2HkFgDYRVqm7QU,786
-gitlabemu/configloader.py,sha256=COb1rBHmSiSD5oYOuUxQXCx19mdcaZ0RhdYHNWHAXr8,28476
-gitlabemu/configtool.py,sha256=_i77n1RyBtW-KJ2cPDlKkD7nhcvwNjuQoPFPjnlybQQ,1863
-gitlabemu/configtool_ctx.py,sha256=rbYPMg4F650H1RqioRAhZtANLnyUVTqf5kE3I6wNNCE,1428
-gitlabemu/configtool_gitlab.py,sha256=ix1zm3eM6wTkMKLJIY-OXusZa9A2_kM1rfbmYuNQjtE,1614
-gitlabemu/configtool_runner.py,sha256=hIhNvux6aDfNcgYcG1kN8g9BOTUPzSv0A5ExPJ59_EE,6073
-gitlabemu/configtool_vars.py,sha256=XqhZkfi20pRGI6o99p3WCFA_kU4iZhc5rqd-e7WNmCI,2024
-gitlabemu/configtool_volumes.py,sha256=hqKA3TlhiRrqHmQVAgt0Efkl8m9MxUYSt47CPLK3_hI,942
-gitlabemu/docker.py,sha256=nbbLU9qkfIgZHh2aZvKLWxFiJTY9phOy2wCCe_LMTPk,29459
-gitlabemu/errors.py,sha256=lgxulpeijXP4r-1v6EFyNV9hvsvcONujgI0IWU63MHU,855
-gitlabemu/generator.py,sha256=haP33slIMEA-RQ9QMuNDDzj2TcUVMuMaD9mfwNUKs5Y,5029
-gitlabemu/gitlab_client_api.py,sha256=-bmx21w9559OZpwVSVJ94KfaeFIqNYEhvjkMzNp4qk0,19321
-gitlabemu/helpers.py,sha256=v9K5UKB9VooZGlzYjeebdswXE0ZX5x1vWwzVmnBuFfs,14697
-gitlabemu/jobs.py,sha256=Jx7_MPo5oiE7E4TQ7mTE8AuL52ioJAi2gURWG3vdidk,20940
-gitlabemu/localfiles.py,sha256=Tx6TrjNoP-Xl1uyhf-3GDwPdTyEyiqRmpaELPAT0K2o,1010
-gitlabemu/logmsg.py,sha256=M6LQ2kv1CAIaS7sOtNtZbj3zWtHa9QqCn0kDpC0mIFI,1150
-gitlabemu/pipelines.py,sha256=UltQT7xWlh6AAbsoPp_7v1GHhpx35IetI75NtzoeriU,9033
-gitlabemu/pstab.py,sha256=Ov9ErIZzQkcZDaBmQ2lMf8Em0YDP0XTjuyKArZu-hJE,1711
-gitlabemu/references.py,sha256=-yWbrVWepmHaZBHrRi8DDDGAyWFkhu864e-LVRpie8E,2302
-gitlabemu/resnamer.py,sha256=JGE33OvzzSFab9vUist78C4MVCM0vVVCrcgpT3x5pl4,901
-gitlabemu/ruleparser.py,sha256=i-9AqRHe3pZhSNwA-Z1YRcFUEPPSEr4nBfzW1WEKuTM,3264
-gitlabemu/runner.py,sha256=1-0esNqyNxCnH8eSUmvhxujTnNwf2ghpGshDCuIe9a8,22001
-gitlabemu/userconfig.py,sha256=rf5QnAabJwM3R1QJZtxEiEEuSbrjobf0WabhpGPcgf0,1149
-gitlabemu/userconfigdata.py,sha256=8R-E_EDNFBCgbqsiiBpP5treniGl3D9VQmHr4aXiRQI,13883
-gitlabemu/variables.py,sha256=VG2Vkhwjdi1wdWu7mPjpVLAEsVzuKoH3_FHjq1JqwOU,906
-gitlabemu/yamlloader.py,sha256=AeMkCJKUS_IuwcYvLZT8J6rWr_ayjfwp5ublV0TDf9A,2447
+gitlabemu/__main__.py,sha256=R5q6qAHGMXw2SjavcsP-R4spDyyf8Z2f8Ez3cj9rMCQ,82
+gitlabemu/ansi.py,sha256=JzMLefJzFUEQ3ZHIFZ9NQbZGp8BNmWS9Z9-KaoGxE_Q,298
+gitlabemu/artifacts.py,sha256=cOMPPEF3HNZ3HhRgrfoBbC1XmqC6EiKXujpHXj4S-Mc,1073
+gitlabemu/chown.py,sha256=mC3RSHRrjRYhgLKotXKfIQq0kCzBauxTlUl4CCQTOIU,810
+gitlabemu/configloader.py,sha256=STA50AgztleNkh1IMbAcwxYr_thCS52-Wns-XYbP1kQ,29445
+gitlabemu/configtool.py,sha256=S5BHeK-5QBLBXKRKrNiOBd7a51st_flxcc5Or5Zdwrk,1921
+gitlabemu/configtool_ctx.py,sha256=XdXarcU2IeMGynaLdKYxityBSXnzS_dza--jVEi4cPU,1471
+gitlabemu/configtool_gitlab.py,sha256=RlT4LtE1qyf-Bi-V0godusO4UqqEsSnngZdxBOWnnU0,1653
+gitlabemu/configtool_runner.py,sha256=nKopfZUYmaFUWJRuZs4-Rdws3seUMlwZ1jArYqHc-2o,6202
+gitlabemu/configtool_vars.py,sha256=kTSbXcmtE2n6Bi-KgT8TVJHUk2dIvsq50jXrUzji4Uk,2077
+gitlabemu/configtool_volumes.py,sha256=neZHWjXBP3jVj6W9KS9exql-DZV1h2_ntcIJSiwNxMo,970
+gitlabemu/docker.py,sha256=tLmlGbFvSvVJvawkmR3h2PwduZefTaS3Ux-GrKCZyjY,30213
+gitlabemu/errors.py,sha256=sjBHZ0pFqC8qvZqpgm3jttNrjgmG-IJFslmYnPmyBZs,901
+gitlabemu/generator.py,sha256=4Cxf7M7KnU3mUUBdEWNejWYECjv4IBuMn7R4pjGbFaA,5172
+gitlabemu/gitlab_client_api.py,sha256=q7TIBUmnX_RsbPZs8yVqy6rjz8mJBGKk1F1zuxwouk8,19858
+gitlabemu/helpers.py,sha256=0StrJ7Qu5ejY7I-c9_K1Uy-wUfnW9ioMyT3CqWLjYCc,15164
+gitlabemu/jobs.py,sha256=qU2Y4s7cOc8s2L4-CiNdBJdyhFV-NffBMD8ewe-LROs,21533
+gitlabemu/localfiles.py,sha256=tRDLkNE4PoLOa6mBp5gx_hCsDNaj1zk_ve-WxXmqdkA,1038
+gitlabemu/logmsg.py,sha256=woHap-c24TL0BtId4QJv-WIOFbvhgE_rDF8-jPoD_mY,1210
+gitlabemu/pipelines.py,sha256=ydVs_ByIM_YUiopysQioBfzv1l_qWz1sJlSVH7bO1Jg,9259
+gitlabemu/pstab.py,sha256=_D4_7hoDzdxuqcXo4DgjvAcuUn6RBRPv7jdUoP6ticw,1775
+gitlabemu/references.py,sha256=ddjteXKLwhGa6kMADfo0GQopea4r8Iq6Q-B9C_EAvug,2359
+gitlabemu/resnamer.py,sha256=wtZZM8vpGsQKr-Bz-fYBC6jKvKKIutb3wBnNvoC4Dk4,935
+gitlabemu/ruleparser.py,sha256=HVbQ5-G6Vcumhfy7PfKVTgZ6QeVMjfClBcCH15p-ZsY,3353
+gitlabemu/runner.py,sha256=n2M1nrYGpZg9H3qxkPeLhAMdL9bWnmMM8stfoRSPXho,22547
+gitlabemu/userconfig.py,sha256=KXnyocSsHln8i0GZQUKi-PVm9yzRSOjztXhSboTJTeQ,1187
+gitlabemu/userconfigdata.py,sha256=T3rYlHmFknzzckTc3672tgkMwY6GI-1W2ucNj29l7Qw,14303
+gitlabemu/variables.py,sha256=P6YIeb5cjpjn01seK4zVjkqQJWkV4EpZlRq4uvawj20,934
+gitlabemu/yamlloader.py,sha256=q2VngKepbMp8JIhNCd-o4r-lbz87d4GKypQaMqfloNw,2538
 gitlabemu/genericci/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-gitlabemu/genericci/types.py,sha256=Q7B-TjEUjO6Ln_Rkwkad07dfm03tjqdjIaRBAR0VY1Q,4938
+gitlabemu/genericci/types.py,sha256=tiYAGJo9HIclLXqAEU5QAa0kw2yxQKhMl-6QQ4AB-xE,5085
 gitlabemu/gitlab/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-gitlabemu/gitlab/constraints.py,sha256=Obq7B4HBVLV46t2bznFyN8nbBAW8pQigi7AquuFoO58,340
-gitlabemu/gitlab/types.py,sha256=NzdUif98EmkJ1RXXDorBVTmb2a3TChvaxF6JXtRehYU,720
-gitlabemu/gitlab/urls.py,sha256=Y2kOeLAfK1JNEnLQzDZE-uFhBL-zzjb8DzmvGMr5pFs,109
+gitlabemu/gitlab/constraints.py,sha256=MofLzqL13No_5lkUiNrctWRl5gwv2uWOCyrOYaut2ig,358
+gitlabemu/gitlab/types.py,sha256=qKWbKmb4WBYUgcxGUWww80ICjk7XL5Jl-ZJw_BIk3xc,751
+gitlabemu/gitlab/urls.py,sha256=cbouUJjEooqL6vfHUCpE94K26H2ADnAPBaXj0aqGr3k,110
 gitlabemu/glp/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-gitlabemu/glp/__main__.py,sha256=9Q-us8nIu1nTanVSNnKxcgXpzl9LGmlSmFN9nA5qLDc,28
-gitlabemu/glp/buildtool.py,sha256=5vRlB5iLN1QERe6y9_unduUEyzZVhO4Onz8am6imUA0,880
-gitlabemu/glp/canceltool.py,sha256=oqfSFmXOtcVMUVu8qDm4T6sd9k8hF5c-0AcVPR78Gtk,573
-gitlabemu/glp/dumptool.py,sha256=M71KdENiSF7Z5bagOj5KZpoMGZDMgrMpQXXFjWWafg4,1160
-gitlabemu/glp/exporttool.py,sha256=eokK1KqKxWZyKx2DlV3PxuPFfbhMfohhNjypz7n5PuI,1479
-gitlabemu/glp/jobstool.py,sha256=48xBrhBnatkGIUA5k75FtWv08jqecEPSP-v3mBtKnM0,1146
-gitlabemu/glp/listtool.py,sha256=JG18BtTMNBaikAUutirm-W1NJNaZrwJt5kZUSh_qrUs,555
-gitlabemu/glp/subcommand.py,sha256=bUWlBz5TCreyzERGYBhkqUjQqWjNF0MQvg6V5TnVoFY,2276
-gitlabemu/glp/subsettool.py,sha256=_Ws1TingrCBbNGFdKseTLluYdIZWa5brQVFD6PCuhic,5074
-gitlabemu/glp/tool.py,sha256=jW0AS0Scrf70snB1TgFvSuKreHi1g5qEXXI9qUmRO_Q,1906
-gitlabemu/glp/types.py,sha256=0oyZwZhkWFHOUJtcK4h0hj3S96OqkMXXKtKl5qhgIuU,780
-gitlabemu/rules/GitlabRuleLexer.py,sha256=MCjp46og0bIZgEYBOI0K9x-M_sk5t3AjNK4KepghpCQ,3350
-gitlabemu/rules/GitlabRuleParser.py,sha256=tOtqGxQaeJLkoQfXnG4xpWf009DC_QBGa_C0EYdXTX4,15223
-gitlabemu/rules/GitlabRuleVisitor.py,sha256=FTbdHtgdLWPWBvsSwMNAH8nlYyYsblAZbDoPQAOI2cE,1550
+gitlabemu/glp/__main__.py,sha256=cPieEUovizYiCTzpKXILcdBZDJVESEt4fL3oBqIzh58,30
+gitlabemu/glp/buildtool.py,sha256=Nm2fkXatYQear3MWabelL7TCj0mYDwgbYbgNmHKEdFI,906
+gitlabemu/glp/canceltool.py,sha256=_ROSz4K9p78MNpEgozgaojbponQTQWWxJLyfi8mxXKI,594
+gitlabemu/glp/dumptool.py,sha256=DN5A0Nga79QQ8MQAFQDfjVcVZ4q71jSl7d0lE44oOow,1197
+gitlabemu/glp/exporttool.py,sha256=mzhzvXJKLsLWf5pbooaMrP6GKOx7xSPReZPBJkZuKF8,1520
+gitlabemu/glp/jobstool.py,sha256=ZoThImY4Oal-0bHdQuTyCVnfWrr8T975wNayetuCPrg,1176
+gitlabemu/glp/listtool.py,sha256=aYBlhSDO_NpiOiwvYeIbvA6fsYXwcRplbyrDb_IoZvE,576
+gitlabemu/glp/subcommand.py,sha256=vm3cUKpHXuT3IF5TVjcMmQIMhFwnDtfP3kj6miVeigQ,2349
+gitlabemu/glp/subsettool.py,sha256=WWVX_lOFCIQDaj3y0_en_3y3kOW69dejk0tev0tks_I,5183
+gitlabemu/glp/tool.py,sha256=hUfATWJm9dgoQiAVQGOeFAtDK_LqgYZ14lUMVDlTmlY,1966
+gitlabemu/glp/types.py,sha256=Fhcig6UzSt8aNVKtfwtr-PT6gMxR9RZ7NoEFJiOJT3M,811
+gitlabemu/rules/GitlabRuleLexer.py,sha256=4W3t9pDf7UfwBLoRoQtD6Q5X56_xeEcVR6M7nfF6U7o,3433
+gitlabemu/rules/GitlabRuleParser.py,sha256=Dyw0Guy4f2CzmRsy21dPnx0Ar7yHcm4-mQDbFaPUHGc,15647
+gitlabemu/rules/GitlabRuleVisitor.py,sha256=7JQUr3jmXF6hqrp45TFgZ-iBXreUwoQBCcDfvqPybNY,1597
 gitlabemu/rules/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-gitlab_emulator-1.5.6.dist-info/METADATA,sha256=_mSRO6l3_Kpo2mJgE83jYV3qb3-E8hDnHEDrMvBaorI,670
-gitlab_emulator-1.5.6.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-gitlab_emulator-1.5.6.dist-info/entry_points.txt,sha256=PK5X6B57JD9w1f8NDmt3REzkZm6aF0XoHvOoCh48ayw,113
-gitlab_emulator-1.5.6.dist-info/top_level.txt,sha256=iHfyzwynY0_rzNMxNSGez2uET21lTIm8BSnAIjzZB3o,10
-gitlab_emulator-1.5.6.dist-info/RECORD,,
+gitlab_emulator-1.5.7.dist-info/METADATA,sha256=it_29_8217nBDfvAkJNp_E_0b6RUlVhy9leUK-EA5D4,688
+gitlab_emulator-1.5.7.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+gitlab_emulator-1.5.7.dist-info/entry_points.txt,sha256=PK5X6B57JD9w1f8NDmt3REzkZm6aF0XoHvOoCh48ayw,113
+gitlab_emulator-1.5.7.dist-info/top_level.txt,sha256=iHfyzwynY0_rzNMxNSGez2uET21lTIm8BSnAIjzZB3o,10
+gitlab_emulator-1.5.7.dist-info/RECORD,,
```

