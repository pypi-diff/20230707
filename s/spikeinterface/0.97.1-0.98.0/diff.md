# Comparing `tmp/spikeinterface-0.97.1.tar.gz` & `tmp/spikeinterface-0.98.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "spikeinterface-0.97.1.tar", last modified: Fri Mar 10 15:52:08 2023, max compression
+gzip compressed data, was "spikeinterface-0.98.0.tar", last modified: Fri Jul  7 12:50:21 2023, max compression
```

## Comparing `spikeinterface-0.97.1.tar` & `spikeinterface-0.98.0.tar`

### file list

```diff
@@ -1,367 +1,383 @@
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-10 15:52:08.057875 spikeinterface-0.97.1/
--rw-r--r--   0 runner    (1001) docker     (123)     1071 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/LICENSE
--rw-r--r--   0 runner    (1001) docker     (123)     5192 2023-03-10 15:52:08.057875 spikeinterface-0.97.1/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)     4114 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/README.md
--rw-r--r--   0 runner    (1001) docker     (123)     3391 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/pyproject.toml
--rw-r--r--   0 runner    (1001) docker     (123)       38 2023-03-10 15:52:08.057875 spikeinterface-0.97.1/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (123)       70 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/setup.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-10 15:52:07.997873 spikeinterface-0.97.1/spikeinterface/
--rw-r--r--   0 runner    (1001) docker     (123)      444 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-10 15:52:08.001873 spikeinterface-0.97.1/spikeinterface/comparison/
--rw-r--r--   0 runner    (1001) docker     (123)     1323 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/comparison/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    11792 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/comparison/basecomparison.py
--rw-r--r--   0 runner    (1001) docker     (123)     5611 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/comparison/collisioncomparison.py
--rw-r--r--   0 runner    (1001) docker     (123)     3849 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/comparison/collisionstudy.py
--rw-r--r--   0 runner    (1001) docker     (123)    24649 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/comparison/comparisontools.py
--rw-r--r--   0 runner    (1001) docker     (123)     4191 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/comparison/correlogramcomparison.py
--rw-r--r--   0 runner    (1001) docker     (123)     3053 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/comparison/correlogramstudy.py
--rw-r--r--   0 runner    (1001) docker     (123)    11975 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/comparison/groundtruthstudy.py
--rw-r--r--   0 runner    (1001) docker     (123)    10539 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/comparison/hybrid.py
--rw-r--r--   0 runner    (1001) docker     (123)    14117 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/comparison/multicomparisons.py
--rw-r--r--   0 runner    (1001) docker     (123)    26032 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/comparison/paircomparisons.py
--rw-r--r--   0 runner    (1001) docker     (123)    10602 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/comparison/studytools.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-10 15:52:08.009874 spikeinterface-0.97.1/spikeinterface/core/
--rw-r--r--   0 runner    (1001) docker     (123)     4072 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    34611 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     5053 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/baseevent.py
--rw-r--r--   0 runner    (1001) docker     (123)    23004 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/baserecording.py
--rw-r--r--   0 runner    (1001) docker     (123)    18122 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/baserecordingsnippets.py
--rw-r--r--   0 runner    (1001) docker     (123)    10034 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/basesnippets.py
--rw-r--r--   0 runner    (1001) docker     (123)    12408 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/basesorting.py
--rw-r--r--   0 runner    (1001) docker     (123)     2023 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/binaryfolder.py
--rw-r--r--   0 runner    (1001) docker     (123)     5885 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/binaryrecordingextractor.py
--rw-r--r--   0 runner    (1001) docker     (123)     7959 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/channelsaggregationrecording.py
--rw-r--r--   0 runner    (1001) docker     (123)     7930 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/channelslice.py
--rw-r--r--   0 runner    (1001) docker     (123)    28864 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/core_tools.py
--rw-r--r--   0 runner    (1001) docker     (123)     2482 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/datasets.py
--rw-r--r--   0 runner    (1001) docker     (123)     2991 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/frameslicerecording.py
--rw-r--r--   0 runner    (1001) docker     (123)     3045 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/frameslicesorting.py
--rw-r--r--   0 runner    (1001) docker     (123)    12819 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/generate.py
--rw-r--r--   0 runner    (1001) docker     (123)     3352 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/globals.py
--rw-r--r--   0 runner    (1001) docker     (123)     9347 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/injecttemplates.py
--rw-r--r--   0 runner    (1001) docker     (123)    15034 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/job_tools.py
--rw-r--r--   0 runner    (1001) docker     (123)     1524 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/npyfoldersnippets.py
--rw-r--r--   0 runner    (1001) docker     (123)     5538 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/npysnippetsextractor.py
--rw-r--r--   0 runner    (1001) docker     (123)     1507 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/npzfolder.py
--rw-r--r--   0 runner    (1001) docker     (123)     3440 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/npzsortingextractor.py
--rw-r--r--   0 runner    (1001) docker     (123)    15809 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/numpyextractors.py
--rw-r--r--   0 runner    (1001) docker     (123)    13871 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/old_api_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     9945 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/recording_tools.py
--rw-r--r--   0 runner    (1001) docker     (123)    16586 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/segmentutils.py
--rw-r--r--   0 runner    (1001) docker     (123)     1976 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/snippets_tools.py
--rw-r--r--   0 runner    (1001) docker     (123)    12330 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/sparsity.py
--rw-r--r--   0 runner    (1001) docker     (123)     7650 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/template_tools.py
--rw-r--r--   0 runner    (1001) docker     (123)     4918 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/testing.py
--rw-r--r--   0 runner    (1001) docker     (123)      210 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/testing_tools.py
--rw-r--r--   0 runner    (1001) docker     (123)     5698 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/unitsaggregationsorting.py
--rw-r--r--   0 runner    (1001) docker     (123)     2467 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/unitsselectionsorting.py
--rw-r--r--   0 runner    (1001) docker     (123)    75926 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/waveform_extractor.py
--rw-r--r--   0 runner    (1001) docker     (123)    14386 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/waveform_tools.py
--rw-r--r--   0 runner    (1001) docker     (123)     6369 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/core/zarrrecordingextractor.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-10 15:52:08.009874 spikeinterface-0.97.1/spikeinterface/curation/
--rw-r--r--   0 runner    (1001) docker     (123)      518 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/curation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    18486 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/curation/auto_merge.py
--rw-r--r--   0 runner    (1001) docker     (123)     4735 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/curation/curation_tools.py
--rw-r--r--   0 runner    (1001) docker     (123)     7758 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/curation/curationsorting.py
--rw-r--r--   0 runner    (1001) docker     (123)     8699 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/curation/mergeunitssorting.py
--rw-r--r--   0 runner    (1001) docker     (123)     4070 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/curation/remove_duplicated_spikes.py
--rw-r--r--   0 runner    (1001) docker     (123)     2850 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/curation/remove_excess_spikes.py
--rw-r--r--   0 runner    (1001) docker     (123)     7384 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/curation/remove_redundant.py
--rw-r--r--   0 runner    (1001) docker     (123)     4535 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/curation/sortingview_curation.py
--rw-r--r--   0 runner    (1001) docker     (123)     6495 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/curation/splitunitsorting.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-10 15:52:08.009874 spikeinterface-0.97.1/spikeinterface/exporters/
--rw-r--r--   0 runner    (1001) docker     (123)       68 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/exporters/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5440 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/exporters/report.py
--rw-r--r--   0 runner    (1001) docker     (123)    10695 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/exporters/to_phy.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-10 15:52:08.013874 spikeinterface-0.97.1/spikeinterface/extractors/
--rw-r--r--   0 runner    (1001) docker     (123)      260 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6876 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/alfsortingextractor.py
--rw-r--r--   0 runner    (1001) docker     (123)     3832 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/bids.py
--rw-r--r--   0 runner    (1001) docker     (123)     4420 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/cbin_ibl.py
--rw-r--r--   0 runner    (1001) docker     (123)     5815 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/cellexplorersortingextractor.py
--rw-r--r--   0 runner    (1001) docker     (123)     4283 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/combinatoextractors.py
--rw-r--r--   0 runner    (1001) docker     (123)     5023 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/extractorlist.py
--rw-r--r--   0 runner    (1001) docker     (123)    10142 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/hdsortextractors.py
--rw-r--r--   0 runner    (1001) docker     (123)     5773 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/herdingspikesextractors.py
--rw-r--r--   0 runner    (1001) docker     (123)    10525 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/iblstreamingrecording.py
--rw-r--r--   0 runner    (1001) docker     (123)     6097 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/klustaextractors.py
--rw-r--r--   0 runner    (1001) docker     (123)     2982 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/matlabhelpers.py
--rw-r--r--   0 runner    (1001) docker     (123)     3397 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/mclustextractors.py
--rw-r--r--   0 runner    (1001) docker     (123)     5673 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/mcsh5extractors.py
--rw-r--r--   0 runner    (1001) docker     (123)    25538 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/mdaextractors.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-10 15:52:08.017874 spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/
--rw-r--r--   0 runner    (1001) docker     (123)     3005 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2538 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/alphaomega.py
--rw-r--r--   0 runner    (1001) docker     (123)     1142 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/axona.py
--rw-r--r--   0 runner    (1001) docker     (123)     2392 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/biocam.py
--rw-r--r--   0 runner    (1001) docker     (123)     2864 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/blackrock.py
--rw-r--r--   0 runner    (1001) docker     (123)     1699 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/ced.py
--rw-r--r--   0 runner    (1001) docker     (123)     1614 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/edf.py
--rw-r--r--   0 runner    (1001) docker     (123)     1552 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/intan.py
--rw-r--r--   0 runner    (1001) docker     (123)     5214 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/maxwell.py
--rw-r--r--   0 runner    (1001) docker     (123)     1888 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/mcsraw.py
--rw-r--r--   0 runner    (1001) docker     (123)     2540 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/mearec.py
--rw-r--r--   0 runner    (1001) docker     (123)     2218 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/neo_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    15633 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/neobaseextractor.py
--rw-r--r--   0 runner    (1001) docker     (123)     2874 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/neuralynx.py
--rw-r--r--   0 runner    (1001) docker     (123)    13653 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/neuroscope.py
--rw-r--r--   0 runner    (1001) docker     (123)     1738 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/nix.py
--rw-r--r--   0 runner    (1001) docker     (123)    11161 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/openephys.py
--rw-r--r--   0 runner    (1001) docker     (123)     2640 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/plexon.py
--rw-r--r--   0 runner    (1001) docker     (123)     1645 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/spike2.py
--rw-r--r--   0 runner    (1001) docker     (123)     1589 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/spikegadgets.py
--rw-r--r--   0 runner    (1001) docker     (123)     4361 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/spikeglx.py
--rw-r--r--   0 runner    (1001) docker     (123)     1562 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/tdt.py
--rw-r--r--   0 runner    (1001) docker     (123)     3930 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/neuropixels_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    21824 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/nwbextractors.py
--rw-r--r--   0 runner    (1001) docker     (123)     9375 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/phykilosortextractors.py
--rw-r--r--   0 runner    (1001) docker     (123)     8806 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/shybridextractors.py
--rw-r--r--   0 runner    (1001) docker     (123)     4040 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/spykingcircusextractors.py
--rw-r--r--   0 runner    (1001) docker     (123)    10935 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/toy_example.py
--rw-r--r--   0 runner    (1001) docker     (123)     2929 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/tridesclousextractors.py
--rw-r--r--   0 runner    (1001) docker     (123)     6054 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/waveclussnippetstextractors.py
--rw-r--r--   0 runner    (1001) docker     (123)     2257 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/waveclustextractors.py
--rw-r--r--   0 runner    (1001) docker     (123)     2402 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/extractors/yassextractors.py
--rw-r--r--   0 runner    (1001) docker     (123)      641 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/full.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-10 15:52:08.021874 spikeinterface-0.97.1/spikeinterface/postprocessing/
--rw-r--r--   0 runner    (1001) docker     (123)     1724 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/postprocessing/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2256 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/postprocessing/alignsorting.py
--rw-r--r--   0 runner    (1001) docker     (123)    14516 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/postprocessing/correlograms.py
--rw-r--r--   0 runner    (1001) docker     (123)     9174 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/postprocessing/isi.py
--rw-r--r--   0 runner    (1001) docker     (123)     2799 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/postprocessing/noise_level.py
--rw-r--r--   0 runner    (1001) docker     (123)    28125 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/postprocessing/principal_component.py
--rw-r--r--   0 runner    (1001) docker     (123)    10355 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/postprocessing/spike_amplitudes.py
--rw-r--r--   0 runner    (1001) docker     (123)     5925 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/postprocessing/spike_locations.py
--rw-r--r--   0 runner    (1001) docker     (123)    12054 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/postprocessing/template_metrics.py
--rw-r--r--   0 runner    (1001) docker     (123)     7220 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/postprocessing/template_similarity.py
--rw-r--r--   0 runner    (1001) docker     (123)      994 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/postprocessing/template_tools.py
--rw-r--r--   0 runner    (1001) docker     (123)    15212 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/postprocessing/unit_localization.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-10 15:52:08.025874 spikeinterface-0.97.1/spikeinterface/preprocessing/
--rw-r--r--   0 runner    (1001) docker     (123)      247 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/preprocessing/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4530 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/preprocessing/align_snippets.py
--rw-r--r--   0 runner    (1001) docker     (123)     1413 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/preprocessing/basepreprocessor.py
--rw-r--r--   0 runner    (1001) docker     (123)     6689 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/preprocessing/clip.py
--rw-r--r--   0 runner    (1001) docker     (123)     8143 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/preprocessing/common_reference.py
--rw-r--r--   0 runner    (1001) docker     (123)     3167 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/preprocessing/correct_lsb.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-10 15:52:08.025874 spikeinterface-0.97.1/spikeinterface/preprocessing/deepinterpolation/
--rw-r--r--   0 runner    (1001) docker     (123)       73 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/preprocessing/deepinterpolation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    17232 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/preprocessing/deepinterpolation/deepinterpolation.py
--rw-r--r--   0 runner    (1001) docker     (123)    13466 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/preprocessing/detect_bad_channels.py
--rw-r--r--   0 runner    (1001) docker     (123)     9489 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/preprocessing/filter.py
--rw-r--r--   0 runner    (1001) docker     (123)     9239 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/preprocessing/filter_opencl.py
--rw-r--r--   0 runner    (1001) docker     (123)    11443 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/preprocessing/highpass_spatial_filter.py
--rw-r--r--   0 runner    (1001) docker     (123)     5301 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/preprocessing/interpolate_bad_channels.py
--rw-r--r--   0 runner    (1001) docker     (123)     9393 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/preprocessing/normalize_scale.py
--rw-r--r--   0 runner    (1001) docker     (123)     6784 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/preprocessing/phase_shift.py
--rw-r--r--   0 runner    (1001) docker     (123)     6221 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/preprocessing/preprocessing_tools.py
--rw-r--r--   0 runner    (1001) docker     (123)     2113 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/preprocessing/preprocessinglist.py
--rw-r--r--   0 runner    (1001) docker     (123)     1036 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/preprocessing/rectify.py
--rw-r--r--   0 runner    (1001) docker     (123)    20901 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/preprocessing/remove_artifacts.py
--rw-r--r--   0 runner    (1001) docker     (123)     7446 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/preprocessing/resample.py
--rw-r--r--   0 runner    (1001) docker     (123)     3366 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/preprocessing/whiten.py
--rw-r--r--   0 runner    (1001) docker     (123)     4257 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/preprocessing/zero_channel_pad.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-10 15:52:08.025874 spikeinterface-0.97.1/spikeinterface/qualitymetrics/
--rw-r--r--   0 runner    (1001) docker     (123)      230 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/qualitymetrics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    38358 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/qualitymetrics/misc_metrics.py
--rw-r--r--   0 runner    (1001) docker     (123)    30272 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/qualitymetrics/pca_metrics.py
--rw-r--r--   0 runner    (1001) docker     (123)     8764 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/qualitymetrics/quality_metric_calculator.py
--rw-r--r--   0 runner    (1001) docker     (123)     1217 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/qualitymetrics/quality_metric_list.py
--rw-r--r--   0 runner    (1001) docker     (123)     1773 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/qualitymetrics/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-10 15:52:08.029874 spikeinterface-0.97.1/spikeinterface/sorters/
--rw-r--r--   0 runner    (1001) docker     (123)      237 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    14808 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/basesorter.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-10 15:52:08.033875 spikeinterface-0.97.1/spikeinterface/sorters/external/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/external/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7859 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/external/combinato.py
--rw-r--r--   0 runner    (1001) docker     (123)    12583 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/external/hdsort.py
--rw-r--r--   0 runner    (1001) docker     (123)     2782 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/external/hdsort_master.m
--rw-r--r--   0 runner    (1001) docker     (123)     9089 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/external/herdingspikes.py
--rw-r--r--   0 runner    (1001) docker     (123)    11907 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/external/ironclust.py
--rw-r--r--   0 runner    (1001) docker     (123)     8770 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/external/kilosort.py
--rw-r--r--   0 runner    (1001) docker     (123)     7585 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/external/kilosort2.py
--rw-r--r--   0 runner    (1001) docker     (123)     8964 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/external/kilosort2_5.py
--rw-r--r--   0 runner    (1001) docker     (123)     2097 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/external/kilosort2_5_master.m
--rw-r--r--   0 runner    (1001) docker     (123)     1364 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/external/kilosort2_master.m
--rw-r--r--   0 runner    (1001) docker     (123)     8686 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/external/kilosort3.py
--rw-r--r--   0 runner    (1001) docker     (123)     1524 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/external/kilosort3_master.m
--rw-r--r--   0 runner    (1001) docker     (123)     1313 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/external/kilosort_master.m
--rw-r--r--   0 runner    (1001) docker     (123)     8404 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/external/kilosortbase.py
--rw-r--r--   0 runner    (1001) docker     (123)     6343 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/external/klusta.py
--rwxr-xr-x   0 runner    (1001) docker     (123)      533 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/external/klusta_config_default.prm
--rw-r--r--   0 runner    (1001) docker     (123)     5923 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/external/mountainsort4.py
--rw-r--r--   0 runner    (1001) docker     (123)     8696 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/external/pykilosort.py
--rw-r--r--   0 runner    (1001) docker     (123)     7564 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/external/sc_config_default.params
--rw-r--r--   0 runner    (1001) docker     (123)     6635 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/external/spyking_circus.py
--rw-r--r--   0 runner    (1001) docker     (123)     6827 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/external/tridesclous.py
--rw-r--r--   0 runner    (1001) docker     (123)    12121 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/external/waveclus.py
--rw-r--r--   0 runner    (1001) docker     (123)     6559 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/external/waveclus_master.m
--rw-r--r--   0 runner    (1001) docker     (123)     8412 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/external/waveclus_snippets.py
--rw-r--r--   0 runner    (1001) docker     (123)     3706 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/external/waveclus_snippets_master.m
--rw-r--r--   0 runner    (1001) docker     (123)    14555 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/external/yass.py
--rw-r--r--   0 runner    (1001) docker     (123)     6034 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/external/yass_config_default.yaml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-10 15:52:08.033875 spikeinterface-0.97.1/spikeinterface/sorters/internal/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/internal/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      751 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/internal/si_based.py
--rw-r--r--   0 runner    (1001) docker     (123)     6697 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/internal/spyking_circus2.py
--rw-r--r--   0 runner    (1001) docker     (123)     6454 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/internal/tridesclous2.py
--rw-r--r--   0 runner    (1001) docker     (123)    17077 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/launcher.py
--rw-r--r--   0 runner    (1001) docker     (123)    25905 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/runsorter.py
--rw-r--r--   0 runner    (1001) docker     (123)     3968 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/sorterlist.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-10 15:52:08.033875 spikeinterface-0.97.1/spikeinterface/sorters/utils/
--rw-r--r--   0 runner    (1001) docker     (123)      141 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2850 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/utils/constructNPYheader.m
--rw-r--r--   0 runner    (1001) docker     (123)     2342 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/utils/misc.py
--rw-r--r--   0 runner    (1001) docker     (123)     7222 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/utils/shellscript.py
--rw-r--r--   0 runner    (1001) docker     (123)      549 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sorters/utils/writeNPY.m
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-10 15:52:08.033875 spikeinterface-0.97.1/spikeinterface/sortingcomponents/
--rw-r--r--   0 runner    (1001) docker     (123)      109 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-10 15:52:08.037875 spikeinterface-0.97.1/spikeinterface/sortingcomponents/benchmark/
--rw-r--r--   0 runner    (1001) docker     (123)      100 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/benchmark/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    20496 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/benchmark/benchmark_clustering.py
--rw-r--r--   0 runner    (1001) docker     (123)    10216 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/benchmark/benchmark_matching.py
--rw-r--r--   0 runner    (1001) docker     (123)    23594 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/benchmark/benchmark_motion_correction.py
--rw-r--r--   0 runner    (1001) docker     (123)    20506 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/benchmark/benchmark_motion_estimation.py
--rw-r--r--   0 runner    (1001) docker     (123)    24376 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/benchmark/benchmark_peak_selection.py
--rw-r--r--   0 runner    (1001) docker     (123)     5362 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/benchmark/benchmark_tools.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-10 15:52:08.037875 spikeinterface-0.97.1/spikeinterface/sortingcomponents/clustering/
--rw-r--r--   0 runner    (1001) docker     (123)       86 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/clustering/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    10054 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/clustering/circus.py
--rw-r--r--   0 runner    (1001) docker     (123)    27372 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/clustering/clustering_tools.py
--rw-r--r--   0 runner    (1001) docker     (123)      441 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/clustering/dummy.py
--rw-r--r--   0 runner    (1001) docker     (123)     7749 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/clustering/isocut5.py
--rw-r--r--   0 runner    (1001) docker     (123)     1275 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/clustering/main.py
--rw-r--r--   0 runner    (1001) docker     (123)      882 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/clustering/method_list.py
--rw-r--r--   0 runner    (1001) docker     (123)     2673 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/clustering/position.py
--rw-r--r--   0 runner    (1001) docker     (123)     6620 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/clustering/position_and_features.py
--rw-r--r--   0 runner    (1001) docker     (123)     8902 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/clustering/position_and_pca.py
--rw-r--r--   0 runner    (1001) docker     (123)     3293 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/clustering/position_ptp_scaled.py
--rw-r--r--   0 runner    (1001) docker     (123)     9047 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/clustering/random_projections.py
--rw-r--r--   0 runner    (1001) docker     (123)    21205 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/clustering/sliding_hdbscan.py
--rw-r--r--   0 runner    (1001) docker     (123)    28268 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/clustering/sliding_nn.py
--rw-r--r--   0 runner    (1001) docker     (123)      578 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/clustering/triage.py
--rw-r--r--   0 runner    (1001) docker     (123)    14343 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/features_from_peaks.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-10 15:52:08.041875 spikeinterface-0.97.1/spikeinterface/sortingcomponents/matching/
--rw-r--r--   0 runner    (1001) docker     (123)       88 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/matching/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    32936 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/matching/circus.py
--rw-r--r--   0 runner    (1001) docker     (123)     4850 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/matching/main.py
--rw-r--r--   0 runner    (1001) docker     (123)      275 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/matching/method_list.py
--rw-r--r--   0 runner    (1001) docker     (123)     4386 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/matching/naive.py
--rw-r--r--   0 runner    (1001) docker     (123)    14746 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/matching/tdc.py
--rw-r--r--   0 runner    (1001) docker     (123)    15671 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/motion_correction.py
--rw-r--r--   0 runner    (1001) docker     (123)    58921 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/motion_estimation.py
--rw-r--r--   0 runner    (1001) docker     (123)    33008 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/peak_detection.py
--rw-r--r--   0 runner    (1001) docker     (123)     8323 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/peak_localization.py
--rw-r--r--   0 runner    (1001) docker     (123)    12214 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/peak_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)    13403 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/peak_selection.py
--rw-r--r--   0 runner    (1001) docker     (123)      413 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/tools.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-10 15:52:08.041875 spikeinterface-0.97.1/spikeinterface/sortingcomponents/waveforms/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/waveforms/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5012 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/waveforms/neural_network_denoiser.py
--rw-r--r--   0 runner    (1001) docker     (123)    11581 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/waveforms/temporal_pca.py
--rw-r--r--   0 runner    (1001) docker     (123)      891 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/sortingcomponents/waveforms/waveform_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-10 15:52:08.045875 spikeinterface-0.97.1/spikeinterface/widgets/
--rw-r--r--   0 runner    (1001) docker     (123)      974 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-10 15:52:08.049875 spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/
--rw-r--r--   0 runner    (1001) docker     (123)     3447 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5249 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/activity.py
--rw-r--r--   0 runner    (1001) docker     (123)     2784 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/agreementmatrix.py
--rw-r--r--   0 runner    (1001) docker     (123)     4454 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/amplitudes.py
--rw-r--r--   0 runner    (1001) docker     (123)     2871 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/basewidget.py
--rw-r--r--   0 runner    (1001) docker     (123)    15476 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/collisioncomp.py
--rw-r--r--   0 runner    (1001) docker     (123)     2898 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/confusionmatrix.py
--rw-r--r--   0 runner    (1001) docker     (123)     5184 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/correlogramcomp.py
--rw-r--r--   0 runner    (1001) docker     (123)     3406 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/correlograms_.py
--rw-r--r--   0 runner    (1001) docker     (123)     2098 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/depthamplitude.py
--rw-r--r--   0 runner    (1001) docker     (123)     9997 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/drift.py
--rw-r--r--   0 runner    (1001) docker     (123)     5214 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/gtcomparison.py
--rw-r--r--   0 runner    (1001) docker     (123)    17278 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/gtstudy.py
--rw-r--r--   0 runner    (1001) docker     (123)     3570 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/isidistribution.py
--rw-r--r--   0 runner    (1001) docker     (123)     9248 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/multicompgraph.py
--rw-r--r--   0 runner    (1001) docker     (123)     5072 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/presence.py
--rw-r--r--   0 runner    (1001) docker     (123)     1172 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/principalcomponent.py
--rw-r--r--   0 runner    (1001) docker     (123)     1902 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/probemap.py
--rw-r--r--   0 runner    (1001) docker     (123)     4121 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/rasters.py
--rw-r--r--   0 runner    (1001) docker     (123)     2393 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/sortingperformance.py
--rw-r--r--   0 runner    (1001) docker     (123)     8072 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/timeseries_.py
--rw-r--r--   0 runner    (1001) docker     (123)     3484 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/unitlocalization_.py
--rw-r--r--   0 runner    (1001) docker     (123)     3963 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/unitprobemap.py
--rw-r--r--   0 runner    (1001) docker     (123)     3124 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/unitsummary.py
--rw-r--r--   0 runner    (1001) docker     (123)     6906 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/unitwaveformdensitymap_.py
--rw-r--r--   0 runner    (1001) docker     (123)     7360 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/unitwaveforms_.py
--rw-r--r--   0 runner    (1001) docker     (123)      994 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     1373 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/all_amplitudes_distributions.py
--rw-r--r--   0 runner    (1001) docker     (123)     3949 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/amplitudes.py
--rw-r--r--   0 runner    (1001) docker     (123)      316 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/autocorrelograms.py
--rw-r--r--   0 runner    (1001) docker     (123)     3987 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     2259 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/crosscorrelograms.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-10 15:52:08.049875 spikeinterface-0.97.1/spikeinterface/widgets/ipywidgets/
--rw-r--r--   0 runner    (1001) docker     (123)      436 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/ipywidgets/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3007 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/ipywidgets/amplitudes.py
--rw-r--r--   0 runner    (1001) docker     (123)      749 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/ipywidgets/base_ipywidgets.py
--rw-r--r--   0 runner    (1001) docker     (123)     3476 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/ipywidgets/metrics.py
--rw-r--r--   0 runner    (1001) docker     (123)      206 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/ipywidgets/quality_metrics.py
--rw-r--r--   0 runner    (1001) docker     (123)     2712 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/ipywidgets/spike_locations.py
--rw-r--r--   0 runner    (1001) docker     (123)     5137 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/ipywidgets/spikes_on_traces.py
--rw-r--r--   0 runner    (1001) docker     (123)      211 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/ipywidgets/template_metrics.py
--rw-r--r--   0 runner    (1001) docker     (123)     8574 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/ipywidgets/timeseries.py
--rw-r--r--   0 runner    (1001) docker     (123)     2755 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/ipywidgets/unit_locations.py
--rw-r--r--   0 runner    (1001) docker     (123)      381 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/ipywidgets/unit_templates.py
--rw-r--r--   0 runner    (1001) docker     (123)     5668 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/ipywidgets/unit_waveforms.py
--rw-r--r--   0 runner    (1001) docker     (123)     3477 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/ipywidgets/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-10 15:52:08.053875 spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/
--rw-r--r--   0 runner    (1001) docker     (123)      839 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1427 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/all_amplitudes_distributions.py
--rw-r--r--   0 runner    (1001) docker     (123)     2621 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/amplitudes.py
--rw-r--r--   0 runner    (1001) docker     (123)     1001 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/autocorrelograms.py
--rw-r--r--   0 runner    (1001) docker     (123)     4207 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/base_mpl.py
--rw-r--r--   0 runner    (1001) docker     (123)     1418 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/crosscorrelograms.py
--rw-r--r--   0 runner    (1001) docker     (123)     1921 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/metrics.py
--rw-r--r--   0 runner    (1001) docker     (123)      206 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/quality_metrics.py
--rw-r--r--   0 runner    (1001) docker     (123)     3460 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/spike_locations.py
--rw-r--r--   0 runner    (1001) docker     (123)     4242 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/spikes_on_traces.py
--rw-r--r--   0 runner    (1001) docker     (123)      211 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/template_metrics.py
--rw-r--r--   0 runner    (1001) docker     (123)      963 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/template_similarity.py
--rw-r--r--   0 runner    (1001) docker     (123)     2504 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/timeseries.py
--rw-r--r--   0 runner    (1001) docker     (123)      700 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/unit_depths.py
--rw-r--r--   0 runner    (1001) docker     (123)     3144 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/unit_locations.py
--rw-r--r--   0 runner    (1001) docker     (123)     2536 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/unit_summary.py
--rw-r--r--   0 runner    (1001) docker     (123)      209 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/unit_templates.py
--rw-r--r--   0 runner    (1001) docker     (123)     3646 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/unit_waveforms.py
--rw-r--r--   0 runner    (1001) docker     (123)     2647 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/unit_waveforms_density_map.py
--rw-r--r--   0 runner    (1001) docker     (123)     2435 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/metrics.py
--rw-r--r--   0 runner    (1001) docker     (123)     1674 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/quality_metrics.py
--rw-r--r--   0 runner    (1001) docker     (123)     3008 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/sorting_summary.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-10 15:52:08.057875 spikeinterface-0.97.1/spikeinterface/widgets/sortingview/
--rw-r--r--   0 runner    (1001) docker     (123)      554 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/sortingview/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1148 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/sortingview/amplitudes.py
--rw-r--r--   0 runner    (1001) docker     (123)     1236 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/sortingview/autocorrelograms.py
--rw-r--r--   0 runner    (1001) docker     (123)     4052 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/sortingview/base_sortingview.py
--rw-r--r--   0 runner    (1001) docker     (123)     1307 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/sortingview/crosscorrelograms.py
--rw-r--r--   0 runner    (1001) docker     (123)     2566 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/sortingview/metrics.py
--rw-r--r--   0 runner    (1001) docker     (123)      254 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/sortingview/quality_metrics.py
--rw-r--r--   0 runner    (1001) docker     (123)     3907 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/sortingview/sorting_summary.py
--rw-r--r--   0 runner    (1001) docker     (123)     2308 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/sortingview/spike_locations.py
--rw-r--r--   0 runner    (1001) docker     (123)      260 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/sortingview/template_metrics.py
--rw-r--r--   0 runner    (1001) docker     (123)     1163 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/sortingview/template_similarity.py
--rw-r--r--   0 runner    (1001) docker     (123)     1797 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/sortingview/timeseries.py
--rw-r--r--   0 runner    (1001) docker     (123)     1702 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/sortingview/unit_locations.py
--rw-r--r--   0 runner    (1001) docker     (123)     2335 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/sortingview/unit_templates.py
--rw-r--r--   0 runner    (1001) docker     (123)     3948 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/spike_locations.py
--rw-r--r--   0 runner    (1001) docker     (123)     4953 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/spikes_on_traces.py
--rw-r--r--   0 runner    (1001) docker     (123)     1753 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/template_metrics.py
--rw-r--r--   0 runner    (1001) docker     (123)     2008 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/template_similarity.py
--rw-r--r--   0 runner    (1001) docker     (123)     8647 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/timeseries.py
--rw-r--r--   0 runner    (1001) docker     (123)     1865 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/unit_depths.py
--rw-r--r--   0 runner    (1001) docker     (123)     2680 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/unit_locations.py
--rw-r--r--   0 runner    (1001) docker     (123)     3424 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/unit_summary.py
--rw-r--r--   0 runner    (1001) docker     (123)      331 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/unit_templates.py
--rw-r--r--   0 runner    (1001) docker     (123)     7105 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/unit_waveforms.py
--rw-r--r--   0 runner    (1001) docker     (123)     5845 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/unit_waveforms_density_map.py
--rw-r--r--   0 runner    (1001) docker     (123)     6913 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     4236 2023-03-10 15:51:07.000000 spikeinterface-0.97.1/spikeinterface/widgets/widget_list.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-10 15:52:07.997873 spikeinterface-0.97.1/spikeinterface.egg-info/
--rw-r--r--   0 runner    (1001) docker     (123)     5192 2023-03-10 15:52:07.000000 spikeinterface-0.97.1/spikeinterface.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)    16160 2023-03-10 15:52:07.000000 spikeinterface-0.97.1/spikeinterface.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-03-10 15:52:07.000000 spikeinterface-0.97.1/spikeinterface.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (123)      543 2023-03-10 15:52:07.000000 spikeinterface-0.97.1/spikeinterface.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (123)       15 2023-03-10 15:52:07.000000 spikeinterface-0.97.1/spikeinterface.egg-info/top_level.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-07 12:50:21.396705 spikeinterface-0.98.0/
+-rw-r--r--   0 runner    (1001) docker     (123)     1071 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (123)     5275 2023-07-07 12:50:21.396705 spikeinterface-0.98.0/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)     4134 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)     3847 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/pyproject.toml
+-rw-r--r--   0 runner    (1001) docker     (123)       38 2023-07-07 12:50:21.396705 spikeinterface-0.98.0/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (123)      225 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-07 12:50:21.340702 spikeinterface-0.98.0/src/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-07 12:50:21.344702 spikeinterface-0.98.0/src/spikeinterface/
+-rw-r--r--   0 runner    (1001) docker     (123)      553 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-07 12:50:21.348703 spikeinterface-0.98.0/src/spikeinterface/comparison/
+-rw-r--r--   0 runner    (1001) docker     (123)     1218 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/comparison/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11506 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/comparison/basecomparison.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5590 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/comparison/collisioncomparison.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3947 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/comparison/collisionstudy.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24739 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/comparison/comparisontools.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4206 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/comparison/correlogramcomparison.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3068 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/comparison/correlogramstudy.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12177 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/comparison/groundtruthstudy.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10563 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/comparison/hybrid.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14149 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/comparison/multicomparisons.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25783 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/comparison/paircomparisons.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10662 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/comparison/studytools.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-07 12:50:21.352703 spikeinterface-0.98.0/src/spikeinterface/core/
+-rw-r--r--   0 runner    (1001) docker     (123)     4109 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    40071 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4956 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/baseevent.py
+-rw-r--r--   0 runner    (1001) docker     (123)    29103 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/baserecording.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19487 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/baserecordingsnippets.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9560 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/basesnippets.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15053 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/basesorting.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2179 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/binaryfolder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7800 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/binaryrecordingextractor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7582 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/channelsaggregationrecording.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7858 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/channelslice.py
+-rw-r--r--   0 runner    (1001) docker     (123)    31367 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/core_tools.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2425 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/datasets.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3335 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/frameslicerecording.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4918 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/frameslicesorting.py
+-rw-r--r--   0 runner    (1001) docker     (123)    27128 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/generate.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3336 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/globals.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9776 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/injecttemplates.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15527 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/job_tools.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1523 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/npyfoldersnippets.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5473 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/npysnippetsextractor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1437 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/npzfolder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3416 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/npzsortingextractor.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16040 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/numpyextractors.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13457 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/old_api_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13093 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/recording_tools.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25339 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/segmentutils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1918 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/snippets_tools.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13492 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/sparsity.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7675 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/template_tools.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4308 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/testing.py
+-rw-r--r--   0 runner    (1001) docker     (123)      197 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/testing_tools.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5573 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/unitsaggregationsorting.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2350 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/unitsselectionsorting.py
+-rw-r--r--   0 runner    (1001) docker     (123)    77877 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/waveform_extractor.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14371 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/waveform_tools.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6274 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/core/zarrrecordingextractor.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-07 12:50:21.356703 spikeinterface-0.98.0/src/spikeinterface/curation/
+-rw-r--r--   0 runner    (1001) docker     (123)      517 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/curation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19016 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/curation/auto_merge.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4730 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/curation/curation_tools.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7970 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/curation/curationsorting.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8628 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/curation/mergeunitssorting.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4009 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/curation/remove_duplicated_spikes.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2861 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/curation/remove_excess_spikes.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7057 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/curation/remove_redundant.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4560 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/curation/sortingview_curation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6490 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/curation/splitunitsorting.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-07 12:50:21.356703 spikeinterface-0.98.0/src/spikeinterface/exporters/
+-rw-r--r--   0 runner    (1001) docker     (123)       68 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/exporters/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5455 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/exporters/report.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11012 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/exporters/to_phy.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-07 12:50:21.356703 spikeinterface-0.98.0/src/spikeinterface/extractors/
+-rw-r--r--   0 runner    (1001) docker     (123)      254 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6815 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/alfsortingextractor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3782 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/bids.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4510 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/cbin_ibl.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10595 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/cellexplorersortingextractor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4303 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/combinatoextractors.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4891 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/extractorlist.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10107 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/hdsortextractors.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5788 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/herdingspikesextractors.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10548 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/iblstreamingrecording.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6156 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/klustaextractors.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2866 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/matlabhelpers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3394 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/mclustextractors.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5639 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/mcsh5extractors.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25261 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/mdaextractors.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-07 12:50:21.360703 spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/
+-rw-r--r--   0 runner    (1001) docker     (123)     2546 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2414 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/alphaomega.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1058 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/axona.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2308 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/biocam.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4038 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/blackrock.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1593 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/ced.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1507 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/edf.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1564 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/intan.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5129 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/maxwell.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1844 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/mcsraw.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3276 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/mearec.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2157 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/neo_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    27769 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/neobaseextractor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3471 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/neuralynx.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13911 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/neuroscope.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1647 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/nix.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11766 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/openephys.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2498 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/plexon.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1537 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/spike2.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1481 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/spikegadgets.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4322 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/spikeglx.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1473 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/tdt.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4355 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/neuropixels_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25073 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/nwbextractors.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10192 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/phykilosortextractors.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8915 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/shybridextractors.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4040 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/spykingcircusextractors.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10769 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/toy_example.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2939 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/tridesclousextractors.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5896 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/waveclussnippetstextractors.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2254 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/waveclustextractors.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2417 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/extractors/yassextractors.py
+-rw-r--r--   0 runner    (1001) docker     (123)      642 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/full.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-07 12:50:21.364703 spikeinterface-0.98.0/src/spikeinterface/postprocessing/
+-rw-r--r--   0 runner    (1001) docker     (123)     1655 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/postprocessing/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2154 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/postprocessing/alignsorting.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12186 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/postprocessing/amplitude_scalings.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13723 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/postprocessing/correlograms.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8996 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/postprocessing/isi.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2725 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/postprocessing/noise_level.py
+-rw-r--r--   0 runner    (1001) docker     (123)    28500 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/postprocessing/principal_component.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9772 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/postprocessing/spike_amplitudes.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5656 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/postprocessing/spike_locations.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11796 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/postprocessing/template_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6769 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/postprocessing/template_similarity.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1000 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/postprocessing/template_tools.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22736 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/postprocessing/unit_localization.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-07 12:50:21.368703 spikeinterface-0.98.0/src/spikeinterface/preprocessing/
+-rw-r--r--   0 runner    (1001) docker     (123)      303 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/preprocessing/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4761 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/preprocessing/align_snippets.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1608 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/preprocessing/astype.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5497 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/preprocessing/average_across_direction.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1413 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/preprocessing/basepreprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6576 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/preprocessing/clip.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8601 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/preprocessing/common_reference.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3126 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/preprocessing/correct_lsb.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-07 12:50:21.368703 spikeinterface-0.98.0/src/spikeinterface/preprocessing/deepinterpolation/
+-rw-r--r--   0 runner    (1001) docker     (123)       74 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/preprocessing/deepinterpolation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16137 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/preprocessing/deepinterpolation/deepinterpolation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1394 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/preprocessing/depth_order.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16141 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/preprocessing/detect_bad_channels.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5483 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/preprocessing/directional_derivative.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9679 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/preprocessing/filter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4241 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/preprocessing/filter_gaussian.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9164 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/preprocessing/filter_opencl.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11118 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/preprocessing/highpass_spatial_filter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4692 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/preprocessing/interpolate_bad_channels.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14094 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/preprocessing/motion.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11035 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/preprocessing/normalize_scale.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6582 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/preprocessing/phase_shift.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7032 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/preprocessing/preprocessing_tools.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2789 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/preprocessing/preprocessinglist.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1026 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/preprocessing/rectify.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20814 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/preprocessing/remove_artifacts.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7354 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/preprocessing/resample.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5315 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/preprocessing/silence_periods.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2023 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/preprocessing/unsigned_to_signed.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6187 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/preprocessing/whiten.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9269 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/preprocessing/zero_channel_pad.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-07 12:50:21.368703 spikeinterface-0.98.0/src/spikeinterface/qualitymetrics/
+-rw-r--r--   0 runner    (1001) docker     (123)      245 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/qualitymetrics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    41381 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/qualitymetrics/misc_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (123)    38030 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/qualitymetrics/pca_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9255 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/qualitymetrics/quality_metric_calculator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1264 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/qualitymetrics/quality_metric_list.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1601 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/qualitymetrics/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-07 12:50:21.368703 spikeinterface-0.98.0/src/spikeinterface/sorters/
+-rw-r--r--   0 runner    (1001) docker     (123)      237 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14970 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/basesorter.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-07 12:50:21.372703 spikeinterface-0.98.0/src/spikeinterface/sorters/external/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/external/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7937 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/external/combinato.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12446 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/external/hdsort.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2782 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/external/hdsort_master.m
+-rw-r--r--   0 runner    (1001) docker     (123)     9115 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/external/herdingspikes.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12045 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/external/ironclust.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9152 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/external/kilosort.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8775 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/external/kilosort2.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10155 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/external/kilosort2_5.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5190 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/external/kilosort2_5_master.m
+-rw-r--r--   0 runner    (1001) docker     (123)     4261 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/external/kilosort2_master.m
+-rw-r--r--   0 runner    (1001) docker     (123)     9942 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/external/kilosort3.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4614 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/external/kilosort3_master.m
+-rw-r--r--   0 runner    (1001) docker     (123)     1313 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/external/kilosort_master.m
+-rw-r--r--   0 runner    (1001) docker     (123)    10097 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/external/kilosortbase.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6053 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/external/klusta.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)      533 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/external/klusta_config_default.prm
+-rw-r--r--   0 runner    (1001) docker     (123)     5873 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/external/mountainsort4.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8363 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/external/mountainsort5.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8783 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/external/pykilosort.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7564 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/external/sc_config_default.params
+-rw-r--r--   0 runner    (1001) docker     (123)     6625 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/external/spyking_circus.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6812 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/external/tridesclous.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12251 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/external/waveclus.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6560 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/external/waveclus_master.m
+-rw-r--r--   0 runner    (1001) docker     (123)     8403 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/external/waveclus_snippets.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3707 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/external/waveclus_snippets_master.m
+-rw-r--r--   0 runner    (1001) docker     (123)    14413 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/external/yass.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6034 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/external/yass_config_default.yaml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-07 12:50:21.372703 spikeinterface-0.98.0/src/spikeinterface/sorters/internal/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/internal/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      733 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/internal/si_based.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6704 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/internal/spyking_circus2.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6340 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/internal/tridesclous2.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16483 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/launcher.py
+-rw-r--r--   0 runner    (1001) docker     (123)    29273 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/runsorter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4050 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/sorterlist.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-07 12:50:21.372703 spikeinterface-0.98.0/src/spikeinterface/sorters/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)      139 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2850 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/utils/constructNPYheader.m
+-rw-r--r--   0 runner    (1001) docker     (123)     2565 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/utils/misc.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7279 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/utils/shellscript.py
+-rw-r--r--   0 runner    (1001) docker     (123)      550 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sorters/utils/writeNPY.m
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-07 12:50:21.376704 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/
+-rw-r--r--   0 runner    (1001) docker     (123)      109 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-07 12:50:21.376704 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/benchmark/
+-rw-r--r--   0 runner    (1001) docker     (123)      101 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/benchmark/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21585 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/benchmark/benchmark_clustering.py
+-rw-r--r--   0 runner    (1001) docker     (123)    29179 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/benchmark/benchmark_matching.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21903 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/benchmark/benchmark_motion_estimation.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25550 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/benchmark/benchmark_motion_interpolation.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18626 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/benchmark/benchmark_peak_localization.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25089 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/benchmark/benchmark_peak_selection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5336 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/benchmark/benchmark_tools.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-07 12:50:21.380704 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/clustering/
+-rw-r--r--   0 runner    (1001) docker     (123)       87 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/clustering/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10611 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/clustering/circus.py
+-rw-r--r--   0 runner    (1001) docker     (123)    27271 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/clustering/clustering_tools.py
+-rw-r--r--   0 runner    (1001) docker     (123)      438 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/clustering/dummy.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7307 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/clustering/isocut5.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1275 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/clustering/main.py
+-rw-r--r--   0 runner    (1001) docker     (123)      883 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/clustering/method_list.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2657 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/clustering/position.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7082 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/clustering/position_and_features.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9208 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/clustering/position_and_pca.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3266 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/clustering/position_ptp_scaled.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9443 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/clustering/random_projections.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21202 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/clustering/sliding_hdbscan.py
+-rw-r--r--   0 runner    (1001) docker     (123)    27670 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/clustering/sliding_nn.py
+-rw-r--r--   0 runner    (1001) docker     (123)      578 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/clustering/triage.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14589 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/features_from_peaks.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-07 12:50:21.380704 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/matching/
+-rw-r--r--   0 runner    (1001) docker     (123)       88 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/matching/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    33854 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/matching/circus.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4819 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/matching/main.py
+-rw-r--r--   0 runner    (1001) docker     (123)      330 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/matching/method_list.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4308 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/matching/naive.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13661 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/matching/tdc.py
+-rw-r--r--   0 runner    (1001) docker     (123)    44820 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/matching/wobble.py
+-rw-r--r--   0 runner    (1001) docker     (123)    59332 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/motion_estimation.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15705 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/motion_interpolation.py
+-rw-r--r--   0 runner    (1001) docker     (123)    40354 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/peak_detection.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17227 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/peak_localization.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21800 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/peak_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13320 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/peak_selection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1303 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/tools.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-07 12:50:21.380704 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/waveforms/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/waveforms/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5439 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/waveforms/neural_network_denoiser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2049 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/waveforms/savgol_denoiser.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11587 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/waveforms/temporal_pca.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3686 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/waveforms/waveform_thresholder.py
+-rw-r--r--   0 runner    (1001) docker     (123)      891 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/waveforms/waveform_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-07 12:50:21.384704 spikeinterface-0.98.0/src/spikeinterface/widgets/
+-rw-r--r--   0 runner    (1001) docker     (123)      974 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-07 12:50:21.388704 spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/
+-rw-r--r--   0 runner    (1001) docker     (123)     3330 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5230 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/activity.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2776 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/agreementmatrix.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4421 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/amplitudes.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2924 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/basewidget.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16496 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/collisioncomp.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2924 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/confusionmatrix.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5391 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/correlogramcomp.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3349 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/correlograms_.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2081 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/depthamplitude.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5395 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/gtcomparison.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17941 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/gtstudy.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3614 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/isidistribution.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10124 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/multicompgraph.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4792 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/presence.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1114 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/principalcomponent.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2478 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/probemap.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4100 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/rasters.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2463 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/sortingperformance.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8116 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/timeseries_.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3410 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/unitlocalization_.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4089 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/unitprobemap.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3104 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/unitsummary.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7107 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/unitwaveformdensitymap_.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7379 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/unitwaveforms_.py
+-rw-r--r--   0 runner    (1001) docker     (123)      974 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1381 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/all_amplitudes_distributions.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4042 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/amplitudes.py
+-rw-r--r--   0 runner    (1001) docker     (123)      310 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/autocorrelograms.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3926 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2251 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/crosscorrelograms.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-07 12:50:21.388704 spikeinterface-0.98.0/src/spikeinterface/widgets/ipywidgets/
+-rw-r--r--   0 runner    (1001) docker     (123)      437 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/ipywidgets/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2922 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/ipywidgets/amplitudes.py
+-rw-r--r--   0 runner    (1001) docker     (123)      718 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/ipywidgets/base_ipywidgets.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3403 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/ipywidgets/metrics.py
+-rw-r--r--   0 runner    (1001) docker     (123)      198 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/ipywidgets/quality_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2690 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/ipywidgets/spike_locations.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4864 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/ipywidgets/spikes_on_traces.py
+-rw-r--r--   0 runner    (1001) docker     (123)      203 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/ipywidgets/template_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8375 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/ipywidgets/timeseries.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2634 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/ipywidgets/unit_locations.py
+-rw-r--r--   0 runner    (1001) docker     (123)      377 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/ipywidgets/unit_templates.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5665 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/ipywidgets/unit_waveforms.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3439 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/ipywidgets/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-07 12:50:21.392704 spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/
+-rw-r--r--   0 runner    (1001) docker     (123)      873 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1402 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/all_amplitudes_distributions.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2414 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/amplitudes.py
+-rw-r--r--   0 runner    (1001) docker     (123)      991 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/autocorrelograms.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4095 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/base_mpl.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1377 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/crosscorrelograms.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1861 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/metrics.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3571 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/motion.py
+-rw-r--r--   0 runner    (1001) docker     (123)      198 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/quality_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3289 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/spike_locations.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4318 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/spikes_on_traces.py
+-rw-r--r--   0 runner    (1001) docker     (123)      203 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/template_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (123)      962 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/template_similarity.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2640 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/timeseries.py
+-rw-r--r--   0 runner    (1001) docker     (123)      699 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/unit_depths.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3094 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/unit_locations.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2485 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/unit_summary.py
+-rw-r--r--   0 runner    (1001) docker     (123)      210 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/unit_templates.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3687 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/unit_waveforms.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2794 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/unit_waveforms_density_map.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2589 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/metrics.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1650 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/motion.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1777 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/quality_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2963 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/sorting_summary.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-07 12:50:21.396705 spikeinterface-0.98.0/src/spikeinterface/widgets/sortingview/
+-rw-r--r--   0 runner    (1001) docker     (123)      554 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/sortingview/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1098 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/sortingview/amplitudes.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1210 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/sortingview/autocorrelograms.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3919 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/sortingview/base_sortingview.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1283 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/sortingview/crosscorrelograms.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2277 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/sortingview/metrics.py
+-rw-r--r--   0 runner    (1001) docker     (123)      254 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/sortingview/quality_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3415 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/sortingview/sorting_summary.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2274 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/sortingview/spike_locations.py
+-rw-r--r--   0 runner    (1001) docker     (123)      260 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/sortingview/template_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1089 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/sortingview/template_similarity.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1782 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/sortingview/timeseries.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1561 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/sortingview/unit_locations.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2123 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/sortingview/unit_templates.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4060 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/spike_locations.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5155 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/spikes_on_traces.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1731 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/template_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1969 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/template_similarity.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8863 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/timeseries.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1837 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/unit_depths.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2707 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/unit_locations.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3340 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/unit_summary.py
+-rw-r--r--   0 runner    (1001) docker     (123)      325 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/unit_templates.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7353 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/unit_waveforms.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5767 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/unit_waveforms_density_map.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7151 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4379 2023-07-07 12:48:26.000000 spikeinterface-0.98.0/src/spikeinterface/widgets/widget_list.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-07 12:50:21.344702 spikeinterface-0.98.0/src/spikeinterface.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (123)     5275 2023-07-07 12:50:21.000000 spikeinterface-0.98.0/src/spikeinterface.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    18324 2023-07-07 12:50:21.000000 spikeinterface-0.98.0/src/spikeinterface.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-07-07 12:50:21.000000 spikeinterface-0.98.0/src/spikeinterface.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      715 2023-07-07 12:50:21.000000 spikeinterface-0.98.0/src/spikeinterface.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       15 2023-07-07 12:50:21.000000 spikeinterface-0.98.0/src/spikeinterface.egg-info/top_level.txt
```

### Comparing `spikeinterface-0.97.1/LICENSE` & `spikeinterface-0.98.0/LICENSE`

 * *Files identical despite different names*

### Comparing `spikeinterface-0.97.1/PKG-INFO` & `spikeinterface-0.98.0/PKG-INFO`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: spikeinterface
-Version: 0.97.1
+Version: 0.98.0
 Summary: Python toolkit for analysis, visualization, and comparison of spike sorting output
 Author-email: Alessio Buccino <alessiop.buccino@gmail.com>, Samuel Garcia <sam.garcia.die@gmail.com>
 Project-URL: homepage, https://github.com/SpikeInterface/spikeinterface
 Project-URL: repository, https://github.com/SpikeInterface/spikeinterface
 Project-URL: documentation, https://spikeinterface.readthedocs.io/
 Project-URL: changelog, https://spikeinterface.readthedocs.io/en/latest/whatisnew.html
 Classifier: Programming Language :: Python :: 3 :: Only
@@ -13,16 +13,18 @@
 Classifier: Operating System :: POSIX :: Linux
 Classifier: Operating System :: Microsoft :: Windows
 Classifier: Operating System :: MacOS
 Classifier: Operating System :: OS Independent
 Requires-Python: <4.0,>=3.8
 Description-Content-Type: text/markdown
 Provides-Extra: extractors
+Provides-Extra: streaming_extractors
 Provides-Extra: full
 Provides-Extra: widgets
+Provides-Extra: test_core
 Provides-Extra: test
 License-File: LICENSE
 
 # SpikeInterface: a unified framework for spike sorting
 
 <table>
 <tr>
@@ -48,16 +50,16 @@
     <img src="https://img.shields.io/pypi/l/spikeinterface.svg" alt="license" />
     </a>
 </td>
 </tr>
 <tr>
   <td>Build Status</td>
   <td>
-    <a href="https://github.com/SpikeInterface/spikeinterface/actions/workflows/full-test.yml/badge.svg">
-    <img src="https://github.com/SpikeInterface/spikeinterface/actions/workflows/full-test.yml/badge.svg" alt="travis build status" />
+    <a href="https://github.com/SpikeInterface/spikeinterface/actions/workflows/full-test-with-codecov.yml/badge.svg">
+    <img src="https://github.com/SpikeInterface/spikeinterface/actions/workflows/full-test-with-codecov.yml/badge.svg" alt="CI build status" />
     </a>
   </td>
 </tr>
 <tr>
 	<td>Codecov</td>
 	<td>
 		<a href="https://codecov.io/github/spikeinterface/spikeinterface">
@@ -91,29 +93,29 @@
 
 ## Documentation
 
 Detailed documentation for spikeinterface can be found [here](https://spikeinterface.readthedocs.io/en/latest).
 
 Several tutorials to get started can be found in [spiketutorials](https://github.com/SpikeInterface/spiketutorials).
 
-There are also some useful notebooks [on our blog](https://spikeinterface.github.io) that cover advanced benchmarking 
+There are also some useful notebooks [on our blog](https://spikeinterface.github.io) that cover advanced benchmarking
 and sorting components.
 
 You can also have a look at the [spikeinterface-gui](https://github.com/SpikeInterface/spikeinterface-gui).
 
 
 ## How to install spikeinteface
 
 You can install the new `spikeinterface` version with pip:
 
 ```bash
 pip install spikeinterface[full]
 ```
 
-The `[full]` option installs all the extra dependencies for all the different sub-modules. 
+The `[full]` option installs all the extra dependencies for all the different sub-modules.
 
 To install all interactive widget backends, you can use:
 
 ```bash
  pip install spikeinterface[full,widgets]
 ```
```

#### html2text {}

```diff
@@ -1,28 +1,28 @@
-Metadata-Version: 2.1 Name: spikeinterface Version: 0.97.1 Summary: Python
+Metadata-Version: 2.1 Name: spikeinterface Version: 0.98.0 Summary: Python
 toolkit for analysis, visualization, and comparison of spike sorting output
 Author-email: Alessio Buccino
 buccino@gmail.com>, Samuel Garcia
 garcia.die@gmail.com> Project-URL: homepage, https://github.com/SpikeInterface/
 spikeinterface Project-URL: repository, https://github.com/SpikeInterface/
 spikeinterface Project-URL: documentation, https://
 spikeinterface.readthedocs.io/ Project-URL: changelog, https://
 spikeinterface.readthedocs.io/en/latest/whatisnew.html Classifier: Programming
 Language :: Python :: 3 :: Only Classifier: License :: OSI Approved :: MIT
 License Classifier: Intended Audience :: Science/Research Classifier: Operating
 System :: POSIX :: Linux Classifier: Operating System :: Microsoft :: Windows
 Classifier: Operating System :: MacOS Classifier: Operating System :: OS
 Independent Requires-Python: <4.0,>=3.8 Description-Content-Type: text/markdown
-Provides-Extra: extractors Provides-Extra: full Provides-Extra: widgets
-Provides-Extra: test License-File: LICENSE # SpikeInterface: a unified
-framework for spike sorting
+Provides-Extra: extractors Provides-Extra: streaming_extractors Provides-Extra:
+full Provides-Extra: widgets Provides-Extra: test_core Provides-Extra: test
+License-File: LICENSE # SpikeInterface: a unified framework for spike sorting
 Latest Release [latest_release]
 Documentation  [latest_documentation]
 License        [license]
-Build Status   [travis_build_status]
+Build Status   [CI_build_status]
 Codecov        [codecov]
 [![Twitter](https://img.shields.io/badge/@spikeinterface-
 %231DA1F2.svg?style=for-the-badge&logo=Twitter&logoColor=white)](https://
 twitter.com/spikeinterface) [![Mastodon](https://img.shields.io/badge/-
 @spikeinterface-%232B90D9?style=for-the-badge&logo=mastodon&logoColor=white)]
 (https://fosstodon.org/@spikeinterface) SpikeInterface is a Python framework
 designed to unify preexisting spike sorting technologies into a single code
```

### Comparing `spikeinterface-0.97.1/README.md` & `spikeinterface-0.98.0/README.md`

 * *Files 1% similar despite different names*

```diff
@@ -24,16 +24,16 @@
     <img src="https://img.shields.io/pypi/l/spikeinterface.svg" alt="license" />
     </a>
 </td>
 </tr>
 <tr>
   <td>Build Status</td>
   <td>
-    <a href="https://github.com/SpikeInterface/spikeinterface/actions/workflows/full-test.yml/badge.svg">
-    <img src="https://github.com/SpikeInterface/spikeinterface/actions/workflows/full-test.yml/badge.svg" alt="travis build status" />
+    <a href="https://github.com/SpikeInterface/spikeinterface/actions/workflows/full-test-with-codecov.yml/badge.svg">
+    <img src="https://github.com/SpikeInterface/spikeinterface/actions/workflows/full-test-with-codecov.yml/badge.svg" alt="CI build status" />
     </a>
   </td>
 </tr>
 <tr>
 	<td>Codecov</td>
 	<td>
 		<a href="https://codecov.io/github/spikeinterface/spikeinterface">
@@ -67,29 +67,29 @@
 
 ## Documentation
 
 Detailed documentation for spikeinterface can be found [here](https://spikeinterface.readthedocs.io/en/latest).
 
 Several tutorials to get started can be found in [spiketutorials](https://github.com/SpikeInterface/spiketutorials).
 
-There are also some useful notebooks [on our blog](https://spikeinterface.github.io) that cover advanced benchmarking 
+There are also some useful notebooks [on our blog](https://spikeinterface.github.io) that cover advanced benchmarking
 and sorting components.
 
 You can also have a look at the [spikeinterface-gui](https://github.com/SpikeInterface/spikeinterface-gui).
 
 
 ## How to install spikeinteface
 
 You can install the new `spikeinterface` version with pip:
 
 ```bash
 pip install spikeinterface[full]
 ```
 
-The `[full]` option installs all the extra dependencies for all the different sub-modules. 
+The `[full]` option installs all the extra dependencies for all the different sub-modules.
 
 To install all interactive widget backends, you can use:
 
 ```bash
  pip install spikeinterface[full,widgets]
 ```
```

#### html2text {}

```diff
@@ -1,12 +1,12 @@
 # SpikeInterface: a unified framework for spike sorting
 Latest Release [latest_release]
 Documentation  [latest_documentation]
 License        [license]
-Build Status   [travis_build_status]
+Build Status   [CI_build_status]
 Codecov        [codecov]
 [![Twitter](https://img.shields.io/badge/@spikeinterface-
 %231DA1F2.svg?style=for-the-badge&logo=Twitter&logoColor=white)](https://
 twitter.com/spikeinterface) [![Mastodon](https://img.shields.io/badge/-
 @spikeinterface-%232B90D9?style=for-the-badge&logo=mastodon&logoColor=white)]
 (https://fosstodon.org/@spikeinterface) SpikeInterface is a Python framework
 designed to unify preexisting spike sorting technologies into a single code
```

### Comparing `spikeinterface-0.97.1/pyproject.toml` & `spikeinterface-0.98.0/pyproject.toml`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 [project]
 name = "spikeinterface"
-version = "0.97.1"
+version = "0.98.0"
 authors = [
   { name="Alessio Buccino", email="alessiop.buccino@gmail.com" },
   { name="Samuel Garcia", email="sam.garcia.die@gmail.com" },
 ]
 description = "Python toolkit for analysis, visualization, and comparison of spike sorting output"
 readme = "README.md"
 requires-python = ">=3.8,<4.0"
@@ -35,63 +35,83 @@
 include-package-data = true
 package-data = {"spikeinterface.sorters" = ["**/*.m", "**/*.prm", "**/*.params", "**/*.yaml"]}
 
 [tool.setuptools.exclude-package-data]
 spikeinterface = ["**/tests/test_*"]
 
 [tool.setuptools.packages.find]
-where = ["."]
+where = ["src"]
 include = ["spikeinterface*"]
 namespaces = false
 exclude = ["spikeinterface.*.tests"]
 
+[tool.black]
+line-length = 120
 
 [project.urls]
 homepage = "https://github.com/SpikeInterface/spikeinterface"
 repository = "https://github.com/SpikeInterface/spikeinterface"
 documentation = "https://spikeinterface.readthedocs.io/"
 changelog = "https://spikeinterface.readthedocs.io/en/latest/whatisnew.html"
 
 
 [project.optional-dependencies]
 
 extractors = [
     "MEArec>=1.8",
-    "pynwb>=2.1.0",
+    "pynwb>=2.3.0",
     "pyedflib>=0.1.30",
     "sonpy;python_version<'3.10'",
     "lxml", # lxml for neuroscope
-    "hdf5storage", # hdf5storage and scipy for cellexplorer
     "scipy",
     # ONE-api and ibllib for streaming IBL
     "ONE-api>=1.19.1",
     "ibllib>=2.21.0",
+    "pymatreader>=0.0.32", # For cell explorer matlab files
+]
+
+streaming_extractors = [
+    # ONE-api and ibllib for streaming IBL
+    "ONE-api>=1.19.1",
+    "ibllib>=2.21.0",
+    # Following dependencies are for streaming with nwb files
+    "fsspec",
+    "aiohttp",
+    "requests",
+    "pynwb>=2.3.0",
 ]
 
 full = [
     "zarr",
     "h5py",
     "pandas",
     "xarray",
     "scipy",
     "scikit-learn",
     "networkx",
     "distinctipy",
     "matplotlib",
-    "cuda-python",
+    "cuda-python; sys_platform != 'darwin'",
+    "numba",
 ]
 
 widgets = [
     "matplotlib",
     "ipympl",
     "ipywidgets",
-    "sortingview>=0.11.0",
+    "sortingview>=0.11.15",
     "figurl-jupyter"
 ]
 
+test_core = [
+    "pytest",
+    "zarr",
+    "psutil",
+]
+
 test = [
     "pytest",
     "pytest-cov",
 
     # zarr is needed for testing
     "zarr",
     "xarray",
@@ -117,15 +137,14 @@
     "torch",
     "pynndescent",
 
     # for github test : probeinterface and neo from master
     # for release we need pypi, so this need to be commented
     # "probeinterface @ git+https://github.com/SpikeInterface/probeinterface.git",
     # "neo @ git+https://github.com/NeuralEnsemble/python-neo.git",
-    
 ]
 
 
 [tool.pytest.ini_options]
 markers = [
     "core",
     "extractors",
@@ -136,12 +155,13 @@
     "sorters_external",
     "sorters_internal",
     "comparison",
     "curation",
     "exporters",
     "widgets",
     "sortingcomponents",
+    "streaming_extractors: extractors that require streaming such as ross and fsspec",
 ]
 filterwarnings =[
     'ignore:.*distutils Version classes are deprecated.*:DeprecationWarning',
     'ignore:.*the imp module is deprecated in favour of importlib.*:DeprecationWarning',
-]
+]
```

### Comparing `spikeinterface-0.97.1/spikeinterface/comparison/basecomparison.py` & `spikeinterface-0.98.0/src/spikeinterface/comparison/basecomparison.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,45 +1,45 @@
-import numpy as np
-import networkx as nx
-
 from copy import deepcopy
 from typing import OrderedDict
+import numpy as np
 
-from .comparisontools import (make_possible_match, make_best_match, make_hungarian_match)
+from .comparisontools import make_possible_match, make_best_match, make_hungarian_match
 
 
 class BaseComparison:
     """
     Base class for all comparisons (SpikeTrain and Template)
     """
 
-    def __init__(self, object_list, name_list,
-                 match_score=0.5, chance_score=0.1,
-                 verbose=False):
+    def __init__(self, object_list, name_list, match_score=0.5, chance_score=0.1, verbose=False):
         self.object_list = object_list
         self.name_list = name_list
         self._verbose = verbose
         self.match_score = match_score
         self.chance_score = chance_score
 
 
 class BaseMultiComparison(BaseComparison):
     """
     Base class for graph-based multi comparison classes.
-    
+
     It handles graph operations, comparisons, and agreements.
     """
-    def __init__(self, object_list, name_list,
-                 match_score=0.5, chance_score=0.1,
-                 verbose=False):
-        BaseComparison.__init__(self, object_list=object_list,
-                                name_list=name_list,
-                                match_score=match_score,
-                                chance_score=chance_score,
-                                verbose=verbose)
+
+    def __init__(self, object_list, name_list, match_score=0.5, chance_score=0.1, verbose=False):
+        import networkx as nx
+
+        BaseComparison.__init__(
+            self,
+            object_list=object_list,
+            name_list=name_list,
+            match_score=match_score,
+            chance_score=chance_score,
+            verbose=verbose,
+        )
         self.graph = None
         self.subgraphs = None
         self.clean_graph = None
 
     def _compute_all(self):
         self._do_comparison()
         self._do_graph()
@@ -66,31 +66,36 @@
         sg_units: list
             List of unit ids for each node in the connected component subgraph
         """
         if self.clean_graph is not None:
             g = self.clean_graph
         else:
             g = self.graph
+
+        import networkx as nx
+
         subgraphs = (g.subgraph(c).copy() for c in nx.connected_components(g))
         sg_object_names = []
         sg_units = []
         for sg in subgraphs:
             object_names = []
             unit_names = []
             for node in sg.nodes:
                 object_names.append(node[0])
                 unit_names.append(node[1])
             sg_object_names.append(object_names)
             sg_units.append(unit_names)
         return sg_object_names, sg_units
 
-    def _do_comparison(self, ):
+    def _do_comparison(
+        self,
+    ):
         # do pairwise matching
         if self._verbose:
-            print('Multicomaprison step 1: pairwise comparison')
+            print("Multicomaprison step 1: pairwise comparison")
 
         self.comparisons = {}
         for i in range(len(self.object_list)):
             for j in range(i + 1, len(self.object_list)):
                 if self.name_list is not None:
                     name_i = self.name_list[i]
                     name_j = self.name_list[j]
@@ -100,15 +105,17 @@
                 if self._verbose:
                     print(f"  Comparing: {name_i} and {name_j}")
                 comp = self._compare_ij(i, j)
                 self.comparisons[(name_i, name_j)] = comp
 
     def _do_graph(self):
         if self._verbose:
-            print('Multicomparison step 2: make graph')
+            print("Multicomparison step 2: make graph")
+
+        import networkx as nx
 
         self.graph = nx.Graph()
         # nodes
         self._populate_nodes()
 
         # edges
         for comp_name, comp in self.comparisons.items():
@@ -122,18 +129,19 @@
                     self.graph.add_edge(node1, node2, weight=score)
 
         # the graph is symmetrical
         self.graph = self.graph.to_undirected()
 
     def _clean_graph(self):
         if self._verbose:
-            print('Multicomaprison step 3: clean graph')
+            print("Multicomaprison step 3: clean graph")
         clean_graph = self.graph.copy()
-        subgraphs = (clean_graph.subgraph(c).copy()
-                     for c in nx.connected_components(clean_graph))
+        import networkx as nx
+
+        subgraphs = (clean_graph.subgraph(c).copy() for c in nx.connected_components(clean_graph))
         removed_nodes = 0
         for sg in subgraphs:
             object_names = []
             for node in sg.nodes:
                 object_names.append(node[0])
             sorters, counts = np.unique(object_names, return_counts=True)
 
@@ -143,26 +151,24 @@
                     # get edges
                     edges_duplicates = []
                     weights_duplicates = []
                     for n in nodes_duplicate:
                         edges = sg.edges(n, data=True)
                         for e in edges:
                             edges_duplicates.append(e)
-                            weights_duplicates.append(e[2]['weight'])
+                            weights_duplicates.append(e[2]["weight"])
 
                     # remove extra edges
                     n_edges_to_remove = len(nodes_duplicate) - 1
                     remove_idxs = np.argsort(weights_duplicates)[:n_edges_to_remove]
                     edges_to_remove = np.array(edges_duplicates, dtype=object)[remove_idxs]
 
                     for edge_to_remove in edges_to_remove:
-                        clean_graph.remove_edge(
-                            edge_to_remove[0], edge_to_remove[1])
-                        sg.remove_edge(
-                            edge_to_remove[0], edge_to_remove[1])
+                        clean_graph.remove_edge(edge_to_remove[0], edge_to_remove[1])
+                        sg.remove_edge(edge_to_remove[0], edge_to_remove[1])
                         if self._verbose:
                             print(f"Removed edge: {edge_to_remove}")
 
                     # remove extra nodes (as a second step to not affect edge removal)
                     for edge_to_remove in edges_to_remove:
                         if edge_to_remove[0] in nodes_duplicate:
                             node_to_remove = edge_to_remove[0]
@@ -170,83 +176,86 @@
                             node_to_removed = edge_to_remove[1]
                         if node_to_remove in sg.nodes:
                             sg.remove_node(node_to_remove)
                             print(f"Removed node: {node_to_remove}")
                             removed_nodes += 1
 
         if self._verbose:
-            print(f'Removed {removed_nodes} duplicate nodes')
+            print(f"Removed {removed_nodes} duplicate nodes")
         self.clean_graph = clean_graph
 
     def _do_agreement(self):
         # extract agreement from graph
         if self._verbose:
-            print('Multicomparison step 4: extract agreement from graph')
+            print("Multicomparison step 4: extract agreement from graph")
 
         self._new_units = {}
 
         # save new units
-        self.subgraphs = [self.clean_graph.subgraph(c).copy()
-                          for c in nx.connected_components(self.clean_graph)]
+        import networkx as nx
+
+        self.subgraphs = [self.clean_graph.subgraph(c).copy() for c in nx.connected_components(self.clean_graph)]
         for new_unit, sg in enumerate(self.subgraphs):
             edges = list(sg.edges(data=True))
             if len(edges) > 0:
-                avg_agr = np.mean([d['weight'] for u, v, d in edges])
+                avg_agr = np.mean([d["weight"] for u, v, d in edges])
             else:
                 avg_agr = 0
             object_unit_ids = {}
             for node in sg.nodes:
                 object_name, unit_name = node
                 object_unit_ids[object_name] = unit_name
             # sort dict based on name list
             sorted_object_unit_ids = OrderedDict()
             for name in self.name_list:
                 if name in object_unit_ids:
                     sorted_object_unit_ids[name] = object_unit_ids[name]
-            self._new_units[new_unit] = {'avg_agreement': avg_agr, 'unit_ids': sorted_object_unit_ids,
-                                         'agreement_number': len(sg.nodes)}
+            self._new_units[new_unit] = {
+                "avg_agreement": avg_agr,
+                "unit_ids": sorted_object_unit_ids,
+                "agreement_number": len(sg.nodes),
+            }
 
 
 class BasePairComparison(BaseComparison):
     """
     Base class for pair comparisons.
-    
+
     It handles the matching procedurs.
-    
-    Agreement scores must be computed in inherited classes by overriding the 
+
+    Agreement scores must be computed in inherited classes by overriding the
     '_do_agreement(self)' function
     """
-    def __init__(self, object1, object2, name1, name2, 
-                 match_score=0.5, chance_score=0.1,
-                 verbose=False):
-        BaseComparison.__init__(self, object_list=[object1, object2],
-                                name_list=[name1, name2],
-                                match_score=match_score,
-                                chance_score=chance_score,
-                                verbose=verbose)
+
+    def __init__(self, object1, object2, name1, name2, match_score=0.5, chance_score=0.1, verbose=False):
+        BaseComparison.__init__(
+            self,
+            object_list=[object1, object2],
+            name_list=[name1, name2],
+            match_score=match_score,
+            chance_score=chance_score,
+            verbose=verbose,
+        )
         self.possible_match_12, self.possible_match_21 = None, None
         self.best_match_12, self.best_match_21 = None, None
         self.hungarian_match_12, self.hungarian_match_21 = None, None
         self.agreement_scores = None
-    
+
     def _do_agreement(self):
         # populate self.agreement_scores
         raise NotImplementedError
 
     def _do_matching(self):
         if self._verbose:
             print("Matching...")
 
-        self.possible_match_12, self.possible_match_21 = make_possible_match(
-            self.agreement_scores, self.chance_score)
-        self.best_match_12, self.best_match_21 = make_best_match(
-            self.agreement_scores, self.chance_score)
-        self.hungarian_match_12, self.hungarian_match_21 = make_hungarian_match(
-            self.agreement_scores, self.match_score)
-        
+        self.possible_match_12, self.possible_match_21 = make_possible_match(self.agreement_scores, self.chance_score)
+        self.best_match_12, self.best_match_21 = make_best_match(self.agreement_scores, self.chance_score)
+        self.hungarian_match_12, self.hungarian_match_21 = make_hungarian_match(self.agreement_scores, self.match_score)
+
     def get_ordered_agreement_scores(self):
         assert self.agreement_scores is not None, "'agreement_scores' have not been computed!"
         # order rows
         order0 = self.agreement_scores.max(axis=1).argsort()
         scores = self.agreement_scores.iloc[order0.values[::-1], :]
 
         # order columns
@@ -267,46 +276,44 @@
 class MixinSpikeTrainComparison:
     """
     Mixin for spike train comparisons to define:
        * delta_time / delta_frames
        * sampling frequency
        * n_jobs
     """
+
     def __init__(self, delta_time=0.4, n_jobs=-1):
         self.delta_time = delta_time
         self.n_jobs = n_jobs
         self.sampling_frequency = None
         self.delta_frames = None
-        
+
     def set_frames_and_frequency(self, sorting_list):
         sorting0 = sorting_list[0]
         # check num segments
-        if not np.all(sorting.get_num_segments() == sorting0.get_num_segments()
-                      for sorting in sorting_list):
-            raise Exception('Sorting objects must have the same number of segments.')
+        if not np.all(sorting.get_num_segments() == sorting0.get_num_segments() for sorting in sorting_list):
+            raise Exception("Sorting objects must have the same number of segments.")
 
         # take sampling frequency from sorting list and test that they are equivalent.
-        sampling_freqs = np.array([sorting.get_sampling_frequency()
-                                  for sorting in sorting_list], dtype='float64')
+        sampling_freqs = np.array([sorting.get_sampling_frequency() for sorting in sorting_list], dtype="float64")
 
         # Some sorter round the sampling freq lets emit a warning
         sf0 = sampling_freqs[0]
         if not np.all(sf0 == sampling_freqs):
             delta_freq_ratio = np.abs(sampling_freqs - sf0) / sf0
             # tolerance of 0.1%
-            assert np.all(
-                delta_freq_ratio < 0.001), "Inconsistent sampling frequency among sorting list"
+            assert np.all(delta_freq_ratio < 0.001), "Inconsistent sampling frequency among sorting list"
 
         self.sampling_frequency = sf0
-        self.delta_frames = int(
-            self.delta_time / 1000 * self.sampling_frequency)
+        self.delta_frames = int(self.delta_time / 1000 * self.sampling_frequency)
 
 
 class MixinTemplateComparison:
     """
     Mixin for template comparisons to define:
        * similarity method
        * sparsity
     """
+
     def __init__(self, similarity_method="cosine_similarity", sparsity_dict=None):
         self.similarity_method = similarity_method
         self.sparsity_dict = sparsity_dict
```

### Comparing `spikeinterface-0.97.1/spikeinterface/comparison/collisioncomparison.py` & `spikeinterface-0.98.0/src/spikeinterface/comparison/collisioncomparison.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,9 @@
-import pandas as pd
 import numpy as np
+
 from .paircomparisons import GroundTruthComparison
 from .comparisontools import make_collision_events
 
 
 class CollisionGTComparison(GroundTruthComparison):
     """
     This class is an extension of GroundTruthComparison by focusing
@@ -12,17 +12,16 @@
 
     collision_lag: float
         Collision lag in ms.
 
     """
 
     def __init__(self, gt_sorting, tested_sorting, collision_lag=2.0, nbins=11, **kwargs):
-
         # Force compute labels
-        kwargs['compute_labels'] = True
+        kwargs["compute_labels"] = True
 
         if gt_sorting.get_num_segments() > 1 or tested_sorting.get_num_segments() > 1:
             raise NotImplementedError("Collision comparison is only available for mono-segment sorting objects")
 
         GroundTruthComparison.__init__(self, gt_sorting, tested_sorting, **kwargs)
 
         self.collision_lag = collision_lag
@@ -41,71 +40,68 @@
         if gt_index1 > gt_index2:
             gt_unit_id1, gt_unit_id2 = gt_unit_id2, gt_unit_id1
             reversed = True
         else:
             reversed = False
 
         # events
-        mask = (self.collision_events['unit_id1'] == gt_unit_id1) & (self.collision_events['unit_id2'] == gt_unit_id2)
+        mask = (self.collision_events["unit_id1"] == gt_unit_id1) & (self.collision_events["unit_id2"] == gt_unit_id2)
         event = self.collision_events[mask]
 
-        score_label1 = self._labels_st1[gt_unit_id1][0][event['index1']]
-        score_label2 = self._labels_st1[gt_unit_id2][0][event['index2']]
-        delta = event['delta_frame']
+        score_label1 = self._labels_st1[gt_unit_id1][0][event["index1"]]
+        score_label2 = self._labels_st1[gt_unit_id2][0][event["index2"]]
+        delta = event["delta_frame"]
 
         if reversed:
             score_label1, score_label2 = score_label2, score_label1
             delta = -delta
 
         return score_label1, score_label2, delta
 
     def get_label_count_per_collision_bins(self, gt_unit_id1, gt_unit_id2, bins):
-
         score_label1, score_label2, delta = self.get_label_for_collision(gt_unit_id1, gt_unit_id2)
 
-
         tp_count1 = np.zeros(bins.size - 1)
         fn_count1 = np.zeros(bins.size - 1)
         tp_count2 = np.zeros(bins.size - 1)
         fn_count2 = np.zeros(bins.size - 1)
 
         for i in range(tp_count1.size):
             l0, l1 = bins[i], bins[i + 1]
             mask = (delta >= l0) & (delta < l1)
 
-            tp_count1[i] = np.sum(score_label1[mask] == 'TP')
-            fn_count1[i] = np.sum(score_label1[mask] == 'FN')
-            tp_count2[i] = np.sum(score_label2[mask] == 'TP')
-            fn_count2[i] = np.sum(score_label2[mask] == 'FN')
+            tp_count1[i] = np.sum(score_label1[mask] == "TP")
+            fn_count1[i] = np.sum(score_label1[mask] == "FN")
+            tp_count2[i] = np.sum(score_label2[mask] == "TP")
+            fn_count2[i] = np.sum(score_label2[mask] == "FN")
 
         # inverse for unit_id2
         tp_count2 = tp_count2[::-1]
         fn_count2 = fn_count2[::-1]
 
         return tp_count1, fn_count1, tp_count2, fn_count2
 
     def compute_all_pair_collision_bins(self):
-
         d = int(self.collision_lag / 1000 * self.sampling_frequency)
-        bins = np.linspace(-d, d, self.nbins+1)
+        bins = np.linspace(-d, d, self.nbins + 1)
         self.bins = bins
 
         unit_ids = self.sorting1.unit_ids
         n = len(unit_ids)
 
         all_tp_count1 = []
         all_fn_count1 = []
         all_tp_count2 = []
         all_fn_count2 = []
 
-        self.all_tp = np.zeros((n, n, self.nbins), dtype='int64')
-        self.all_fn = np.zeros((n, n, self.nbins), dtype='int64')
+        self.all_tp = np.zeros((n, n, self.nbins), dtype="int64")
+        self.all_fn = np.zeros((n, n, self.nbins), dtype="int64")
 
         for i in range(n):
-            for j in range(i+1, n):
+            for j in range(i + 1, n):
                 u1 = unit_ids[i]
                 u2 = unit_ids[j]
 
                 tp_count1, fn_count1, tp_count2, fn_count2 = self.get_label_count_per_collision_bins(u1, u2, bins)
 
                 self.all_tp[i, j, :] = tp_count1
                 self.all_tp[j, i, :] = tp_count2
@@ -118,43 +114,41 @@
 
         n = len(unit_ids)
 
         recall_scores = []
         similarities = []
         pair_names = []
 
-        performances = self.get_performance()['accuracy']
+        performances = self.get_performance()["accuracy"]
 
         for r in range(n):
             for c in range(r + 1, n):
-
                 u1 = unit_ids[r]
                 u2 = unit_ids[c]
 
                 if good_only:
                     if (performances[u1] < min_accuracy) or (performances[u2] < min_accuracy):
                         continue
 
-
                 ind1 = self.sorting1.id_to_index(u1)
                 ind2 = self.sorting1.id_to_index(u2)
 
                 tp1 = self.all_tp[ind1, ind2, :]
                 fn1 = self.all_fn[ind1, ind2, :]
                 recall1 = tp1 / (tp1 + fn1)
                 recall_scores.append(recall1)
                 similarities.append(similarity_matrix[r, c])
-                pair_names.append(f'{u1} {u2}')
+                pair_names.append(f"{u1} {u2}")
 
                 tp2 = self.all_tp[ind2, ind1, :]
                 fn2 = self.all_fn[ind2, ind1, :]
                 recall2 = tp2 / (tp2 + fn2)
                 recall_scores.append(recall2)
                 similarities.append(similarity_matrix[r, c])
-                pair_names.append(f'{u2} {u1}')
+                pair_names.append(f"{u2} {u1}")
 
         recall_scores = np.array(recall_scores)
         similarities = np.array(similarities)
         pair_names = np.array(pair_names)
 
         order = np.argsort(similarities)
         similarities = similarities[order]
```

### Comparing `spikeinterface-0.97.1/spikeinterface/comparison/collisionstudy.py` & `spikeinterface-0.98.0/src/spikeinterface/comparison/collisionstudy.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,81 +1,79 @@
-
 from .groundtruthstudy import GroundTruthStudy
 from .studytools import iter_computed_sorting
 from .collisioncomparison import CollisionGTComparison
 
 import numpy as np
 
-class CollisionGTStudy(GroundTruthStudy):
 
+class CollisionGTStudy(GroundTruthStudy):
     def run_comparisons(self, exhaustive_gt=True, collision_lag=2.0, nbins=11, **kwargs):
         self.comparisons = {}
         for rec_name, sorter_name, sorting in iter_computed_sorting(self.study_folder):
             gt_sorting = self.get_ground_truth(rec_name)
-            comp = CollisionGTComparison(gt_sorting, sorting, exhaustive_gt=exhaustive_gt, collision_lag=collision_lag, nbins=nbins)
+            comp = CollisionGTComparison(
+                gt_sorting, sorting, exhaustive_gt=exhaustive_gt, collision_lag=collision_lag, nbins=nbins
+            )
             self.comparisons[(rec_name, sorter_name)] = comp
         self.exhaustive_gt = exhaustive_gt
         self.collision_lag = collision_lag
 
     def get_lags(self):
         fs = self.comparisons[(self.rec_names[0], self.sorter_names[0])].sorting1.get_sampling_frequency()
         lags = self.comparisons[(self.rec_names[0], self.sorter_names[0])].bins / fs * 1000
         return lags
 
     def precompute_scores_by_similarities(self, good_only=True, min_accuracy=0.9):
-
-        if not hasattr(self, '_good_only') or self._good_only != good_only:
-
+        if not hasattr(self, "_good_only") or self._good_only != good_only:
             import sklearn
 
             similarity_matrix = {}
             for rec_name in self.rec_names:
                 templates = self.get_templates(rec_name)
                 flat_templates = templates.reshape(templates.shape[0], -1)
                 similarity_matrix[rec_name] = sklearn.metrics.pairwise.cosine_similarity(flat_templates)
 
             self.all_similarities = {}
             self.all_recall_scores = {}
             self.good_only = good_only
 
             for sorter_ind, sorter_name in enumerate(self.sorter_names):
-
                 # loop over recordings
                 all_similarities = []
                 all_recall_scores = []
 
                 for rec_name in self.rec_names:
-
                     if (rec_name, sorter_name) in self.comparisons.keys():
-
                         comp = self.comparisons[(rec_name, sorter_name)]
-                        similarities, recall_scores, pair_names = comp.compute_collision_by_similarity(similarity_matrix[rec_name], good_only=good_only, min_accuracy=min_accuracy)
+                        similarities, recall_scores, pair_names = comp.compute_collision_by_similarity(
+                            similarity_matrix[rec_name], good_only=good_only, min_accuracy=min_accuracy
+                        )
 
                     all_similarities.append(similarities)
                     all_recall_scores.append(recall_scores)
 
                 self.all_similarities[sorter_name] = np.concatenate(all_similarities, axis=0)
                 self.all_recall_scores[sorter_name] = np.concatenate(all_recall_scores, axis=0)
 
     def get_mean_over_similarity_range(self, similarity_range, sorter_name):
-
-        idx = (self.all_similarities[sorter_name] >= similarity_range[0]) & (self.all_similarities[sorter_name] <= similarity_range[1])
+        idx = (self.all_similarities[sorter_name] >= similarity_range[0]) & (
+            self.all_similarities[sorter_name] <= similarity_range[1]
+        )
         all_similarities = self.all_similarities[sorter_name][idx]
         all_recall_scores = self.all_recall_scores[sorter_name][idx]
 
         order = np.argsort(all_similarities)
         all_similarities = all_similarities[order]
         all_recall_scores = all_recall_scores[order, :]
 
         mean_recall_scores = np.nanmean(all_recall_scores, axis=0)
 
         return mean_recall_scores
 
     def get_lag_profile_over_similarity_bins(self, similarity_bins, sorter_name):
-
         all_similarities = self.all_similarities[sorter_name]
         all_recall_scores = self.all_recall_scores[sorter_name]
 
         order = np.argsort(all_similarities)
         all_similarities = all_similarities[order]
         all_recall_scores = all_recall_scores[order, :]
```

### Comparing `spikeinterface-0.97.1/spikeinterface/comparison/comparisontools.py` & `spikeinterface-0.98.0/src/spikeinterface/comparison/comparisontools.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,15 +1,13 @@
 """
 Some functions internally use by SortingComparison.
 """
 
 import numpy as np
-import pandas as pd
 from joblib import Parallel, delayed
-from scipy.optimize import linear_sum_assignment
 
 
 def count_matching_events(times1, times2, delta=10):
     """
     Counts matching events.
 
     Parameters
@@ -72,19 +70,22 @@
         A sorting extractor
 
     Returns
     -------
     event_count: pd.Series
         Nb of spike by units.
     """
+    import pandas as pd
+
     unit_ids = sorting.get_unit_ids()
     ev_counts = np.zeros(len(unit_ids), dtype="int64")
     for segment_index in range(sorting.get_num_segments()):
-        ev_counts += np.array([len(sorting.get_unit_spike_train(u, segment_index=segment_index)) 
-                               for u in unit_ids], dtype='int64')
+        ev_counts += np.array(
+            [len(sorting.get_unit_spike_train(u, segment_index=segment_index)) for u in unit_ids], dtype="int64"
+        )
     event_counts = pd.Series(ev_counts, index=unit_ids)
     return event_counts
 
 
 def count_match_spikes(times1, all_times2, delta_frames):  # , event_counts1, event_counts2  unit2_ids,
     """
     Computes matching spikes between one spike train and a list of others.
@@ -97,15 +98,15 @@
         List of spike trains from sorting 2
 
     Returns
     -------
     matching_events_count: list
         List of counts of matching events
     """
-    matching_event_counts = np.zeros(len(all_times2), dtype='int64')
+    matching_event_counts = np.zeros(len(all_times2), dtype="int64")
     for i2, times2 in enumerate(all_times2):
         num_matches = count_matching_events(times1, times2, delta=delta_frames)
         matching_event_counts[i2] = num_matches
     return matching_event_counts
 
 
 def make_match_count_matrix(sorting1, sorting2, delta_frames, n_jobs=1):
@@ -126,30 +127,34 @@
         Number of jobs to run in parallel
 
     Returns
     -------
     match_event_count: array (int64)
         Matrix of match count spike
     """
+    import pandas as pd
+
     unit1_ids = np.array(sorting1.get_unit_ids())
     unit2_ids = np.array(sorting2.get_unit_ids())
 
     match_event_counts = np.zeros((len(unit1_ids), len(unit2_ids)), dtype="int64")
 
     # preload all spiketrains 2 into a list
     for segment_index in range(sorting1.get_num_segments()):
         s2_spiketrains = [sorting2.get_unit_spike_train(u2, segment_index=segment_index) for u2 in unit2_ids]
 
         match_event_count_segment = Parallel(n_jobs=n_jobs)(
-            delayed(count_match_spikes)(sorting1.get_unit_spike_train(u1, segment_index=segment_index), 
-                                        s2_spiketrains, delta_frames) for i1, u1 in enumerate(unit1_ids))
+            delayed(count_match_spikes)(
+                sorting1.get_unit_spike_train(u1, segment_index=segment_index), s2_spiketrains, delta_frames
+            )
+            for i1, u1 in enumerate(unit1_ids)
+        )
         match_event_counts += np.array(match_event_count_segment)
 
-    match_event_counts_df = pd.DataFrame(np.array(match_event_counts),
-                                         index=unit1_ids, columns=unit2_ids)
+    match_event_counts_df = pd.DataFrame(np.array(match_event_counts), index=unit1_ids, columns=unit2_ids)
 
     return match_event_counts_df
 
 
 def make_agreement_scores(sorting1, sorting2, delta_frames, n_jobs=1):
     """
     Make the agreement matrix.
@@ -170,14 +175,16 @@
         Number of jobs to run in parallel
 
     Returns
     -------
     agreement_scores: array (float)
         The agreement score matrix.
     """
+    import pandas as pd
+
     unit1_ids = np.array(sorting1.get_unit_ids())
     unit2_ids = np.array(sorting2.get_unit_ids())
 
     ev_counts1 = np.array(list(sorting1.get_total_num_spikes().values()))
     ev_counts2 = np.array(list(sorting2.get_total_num_spikes().values()))
     event_counts1 = pd.Series(ev_counts1, index=unit1_ids)
     event_counts2 = pd.Series(ev_counts2, index=unit2_ids)
@@ -203,16 +210,17 @@
     # numpy broadcast style
     denom = event_counts1.values[:, None] + event_counts2.values[None, :] - match_event_count.values
     # little trick here when denom is 0 to avoid 0 division : lets put -1
     # it will 0 anyway
     denom[denom == 0] = -1
 
     agreement_scores = match_event_count.values / denom
-    agreement_scores = pd.DataFrame(agreement_scores,
-                                    index=match_event_count.index, columns=match_event_count.columns)
+    import pandas as pd
+
+    agreement_scores = pd.DataFrame(agreement_scores, index=match_event_count.index, columns=match_event_count.columns)
     return agreement_scores
 
 
 def make_possible_match(agreement_scores, min_score):
     """
     Given an agreement matrix and a min_score threshold.
     Return as a dict all possible match for each spiketrain in each side.
@@ -238,20 +246,20 @@
 
     # threshold the matrix
     scores = agreement_scores.values.copy()
     scores[scores < min_score] = 0
 
     possible_match_12 = {}
     for i1, u1 in enumerate(unit1_ids):
-        inds_match, = np.nonzero(scores[i1, :])
+        (inds_match,) = np.nonzero(scores[i1, :])
         possible_match_12[u1] = unit2_ids[inds_match]
 
     possible_match_21 = {}
     for i2, u2 in enumerate(unit2_ids):
-        inds_match, = np.nonzero(scores[:, i2])
+        (inds_match,) = np.nonzero(scores[:, i2])
         possible_match_21[u2] = unit1_ids[inds_match]
 
     return possible_match_12, possible_match_21
 
 
 def make_best_match(agreement_scores, min_score):
     """
@@ -270,14 +278,16 @@
     Returns
     -------
     best_match_12: pd.Series
 
     best_match_21: pd.Series
 
     """
+    import pandas as pd
+
     unit1_ids = np.array(agreement_scores.index)
     unit2_ids = np.array(agreement_scores.columns)
 
     scores = agreement_scores.values.copy()
 
     best_match_12 = pd.Series(index=unit1_ids, dtype=unit2_ids.dtype)
     best_match_12[:] = -1
@@ -314,21 +324,25 @@
     Returns
     -------
     hungarian_match_12: pd.Series
 
     hungarian_match_21: pd.Series
 
     """
+    import pandas as pd
+
     unit1_ids = np.array(agreement_scores.index)
     unit2_ids = np.array(agreement_scores.columns)
 
     # threshold the matrix
     scores = agreement_scores.values.copy()
     scores[scores < min_score] = 0
 
+    from scipy.optimize import linear_sum_assignment
+
     [inds1, inds2] = linear_sum_assignment(-scores)
 
     hungarian_match_12 = pd.Series(index=unit1_ids, dtype=unit2_ids.dtype)
     hungarian_match_12[:] = -1
     hungarian_match_21 = pd.Series(index=unit2_ids, dtype=unit1_ids.dtype)
     hungarian_match_21[:] = -1
 
@@ -376,24 +390,22 @@
     unit1_ids = sorting1.get_unit_ids()
     unit2_ids = sorting2.get_unit_ids()
     labels_st1 = dict()
     labels_st2 = dict()
 
     # copy spike trains for faster access from extractors with memmapped data
     num_segments = sorting1.get_num_segments()
-    sts1 = {u1: [sorting1.get_unit_spike_train(u1, seg_index) for seg_index in range(num_segments)] 
-            for u1 in unit1_ids}
-    sts2 = {u2: [sorting2.get_unit_spike_train(u2, seg_index) for seg_index in range(num_segments)] 
-            for u2 in unit2_ids}
+    sts1 = {u1: [sorting1.get_unit_spike_train(u1, seg_index) for seg_index in range(num_segments)] for u1 in unit1_ids}
+    sts2 = {u2: [sorting2.get_unit_spike_train(u2, seg_index) for seg_index in range(num_segments)] for u2 in unit2_ids}
 
     for u1 in unit1_ids:
-        lab_st1 = [np.array(['UNPAIRED'] * len(sts), dtype='<U8') for sts in sts1[u1]]
+        lab_st1 = [np.array(["UNPAIRED"] * len(sts), dtype="<U8") for sts in sts1[u1]]
         labels_st1[u1] = lab_st1
     for u2 in unit2_ids:
-        lab_st2 = [np.array(['UNPAIRED'] * len(sts), dtype='<U8') for sts in sts2[u2]]
+        lab_st2 = [np.array(["UNPAIRED"] * len(sts), dtype="<U8") for sts in sts2[u2]]
         labels_st2[u2] = lab_st2
 
     for seg_index in range(num_segments):
         for u1 in unit1_ids:
             u2 = unit_map12[u1]
             sts = sts1[u1][seg_index]
             if u2 != -1:
@@ -410,48 +422,48 @@
                 if len(inds) > 0:
                     inds2 = inds[np.where(inds[:-1] + 1 != inds[1:])[0]] + 1
                     inds2 = np.concatenate((inds2, [inds[-1]]))
                     times_matched = times_concat_sorted[inds2]
                     # find and label closest spikes
                     ind_st1 = np.array([np.abs(sts1[u1] - tm).argmin() for tm in times_matched])
                     ind_st2 = np.array([np.abs(mapped_st - tm).argmin() for tm in times_matched])
-                    assert (len(np.unique(ind_st1)) == len(ind_st1))
-                    assert (len(np.unique(ind_st2)) == len(ind_st2))
-                    lab_st1[ind_st1] = 'TP'
-                    lab_st2[ind_st2] = 'TP'
+                    assert len(np.unique(ind_st1)) == len(ind_st1)
+                    assert len(np.unique(ind_st2)) == len(ind_st2)
+                    lab_st1[ind_st1] = "TP"
+                    lab_st2[ind_st2] = "TP"
             else:
-                lab_st1 = np.array(['FN'] * len(sts))
+                lab_st1 = np.array(["FN"] * len(sts))
                 labels_st1[u1][seg_index] = lab_st1
 
     if label_misclassification:
         for seg_index in range(num_segments):
             for u1 in unit1_ids:
                 lab_st1 = labels_st1[u1][seg_index]
                 st1 = sts1[u1][seg_index]
                 for l_gt, lab in enumerate(lab_st1):
-                    if lab == 'UNPAIRED':
+                    if lab == "UNPAIRED":
                         for u2 in unit2_ids:
                             if u2 in unit_map12.values and unit_map12[u1] != -1:
                                 lab_st2 = labels_st2[u2][seg_index]
                                 n_sp = st1[l_gt]
                                 mapped_st = sts2[u2][seg_index]
-                                matches = (np.abs(mapped_st.astype(int) - n_sp) <= delta_frames)
+                                matches = np.abs(mapped_st.astype(int) - n_sp) <= delta_frames
                                 if np.sum(matches) > 0:
-                                    if 'CL' not in lab_st1[l_gt] and 'CL' not in lab_st2[np.where(matches)[0][0]]:
-                                        lab_st1[l_gt] = 'CL_' + str(u1) + '_' + str(u2)
-                                        lab_st2[np.where(matches)[0][0]] = 'CL_' + str(u2) + '_' + str(u1)
+                                    if "CL" not in lab_st1[l_gt] and "CL" not in lab_st2[np.where(matches)[0][0]]:
+                                        lab_st1[l_gt] = "CL_" + str(u1) + "_" + str(u2)
+                                        lab_st2[np.where(matches)[0][0]] = "CL_" + str(u2) + "_" + str(u1)
 
     for seg_index in range(num_segments):
         for u1 in unit1_ids:
             lab_st1 = labels_st1[u1][seg_index]
-            lab_st1[lab_st1 == 'UNPAIRED'] = 'FN'
+            lab_st1[lab_st1 == "UNPAIRED"] = "FN"
 
         for u2 in unit2_ids:
             lab_st2 = labels_st2[u2][seg_index]
-            lab_st2[lab_st2 == 'UNPAIRED'] = 'FP'
+            lab_st2[lab_st2 == "UNPAIRED"] = "FP"
 
     return labels_st1, labels_st2
 
 
 def compare_spike_trains(spiketrain1, spiketrain2, delta_frames=10):
     """
     Compares 2 spike trains.
@@ -467,32 +479,32 @@
         Times of spikes for the 2 spike trains.
 
     Returns
     -------
     lab_st1, lab_st2: numpy.array
         Label of score for each spike
     """
-    lab_st1 = np.array(['UNPAIRED'] * len(spiketrain1))
-    lab_st2 = np.array(['UNPAIRED'] * len(spiketrain2))
+    lab_st1 = np.array(["UNPAIRED"] * len(spiketrain1))
+    lab_st2 = np.array(["UNPAIRED"] * len(spiketrain2))
 
     # from gtst: TP, TPO, TPSO, FN, FNO, FNSO
     for sp_i, n_sp in enumerate(spiketrain1):
-        matches = (np.abs(spiketrain2.astype(int) - n_sp) <= delta_frames // 2)
+        matches = np.abs(spiketrain2.astype(int) - n_sp) <= delta_frames // 2
         if np.sum(matches) > 0:
-            if lab_st1[sp_i] != 'TP' and lab_st2[np.where(matches)[0][0]] != 'TP':
-                lab_st1[sp_i] = 'TP'
-                lab_st2[np.where(matches)[0][0]] = 'TP'
+            if lab_st1[sp_i] != "TP" and lab_st2[np.where(matches)[0][0]] != "TP":
+                lab_st1[sp_i] = "TP"
+                lab_st2[np.where(matches)[0][0]] = "TP"
 
     for l_gt, lab in enumerate(lab_st1):
-        if lab == 'UNPAIRED':
-            lab_st1[l_gt] = 'FN'
+        if lab == "UNPAIRED":
+            lab_st1[l_gt] = "FN"
 
     for l_gt, lab in enumerate(lab_st2):
-        if lab == 'UNPAIRED':
-            lab_st2[l_gt] = 'FP'
+        if lab == "UNPAIRED":
+            lab_st2[l_gt] = "FP"
 
     return lab_st1, lab_st2
 
 
 def do_confusion_matrix(event_counts1, event_counts2, match_12, match_event_count):
     """
     Computes the confusion matrix between one ground truth sorting
@@ -527,30 +539,34 @@
 
     unmatched_units1 = match_12[match_12 == -1].index
     unmatched_units2 = unit2_ids[~np.in1d(unit2_ids, matched_units2)]
 
     ordered_units1 = np.hstack([matched_units1, unmatched_units1])
     ordered_units2 = np.hstack([matched_units2, unmatched_units2])
 
-    conf_matrix = pd.DataFrame(np.zeros((N1 + 1, N2 + 1), dtype=int),
-                               index=list(ordered_units1) + ['FP'],
-                               columns=list(ordered_units2) + ['FN'])
+    import pandas as pd
+
+    conf_matrix = pd.DataFrame(
+        np.zeros((N1 + 1, N2 + 1), dtype=int),
+        index=list(ordered_units1) + ["FP"],
+        columns=list(ordered_units2) + ["FN"],
+    )
 
     for u1 in matched_units1:
         u2 = match_12[u1]
         num_match = match_event_count.at[u1, u2]
         conf_matrix.at[u1, u2] = num_match
-        conf_matrix.at[u1, 'FN'] = event_counts1.at[u1] - num_match
-        conf_matrix.at['FP', u2] = event_counts2.at[u2] - num_match
+        conf_matrix.at[u1, "FN"] = event_counts1.at[u1] - num_match
+        conf_matrix.at["FP", u2] = event_counts2.at[u2] - num_match
 
     for u1 in unmatched_units1:
-        conf_matrix.at[u1, 'FN'] = event_counts1.at[u1]
+        conf_matrix.at[u1, "FN"] = event_counts1.at[u1]
 
     for u2 in unmatched_units2:
-        conf_matrix.at['FP', u2] = event_counts2.at[u2]
+        conf_matrix.at["FP", u2] = event_counts2.at[u2]
 
     return conf_matrix
 
 
 def do_count_score(event_counts1, event_counts2, match_12, match_event_count):
     """
     For each ground truth units count how many:
@@ -573,40 +589,42 @@
     count_score: pd.DataFrame
         A table with one line per GT units and columns
         are tp/fn/fp/...
     """
 
     unit1_ids = event_counts1.index
 
-    columns = ['tp', 'fn', 'fp', 'num_gt', 'num_tested', 'tested_id']
+    columns = ["tp", "fn", "fp", "num_gt", "num_tested", "tested_id"]
+
+    import pandas as pd
 
     count_score = pd.DataFrame(index=unit1_ids, columns=columns)
-    count_score.index.name = 'gt_unit_id'
+    count_score.index.name = "gt_unit_id"
     for i1, u1 in enumerate(unit1_ids):
         u2 = match_12[u1]
-        count_score.at[u1, 'tested_id'] = u2
+        count_score.at[u1, "tested_id"] = u2
         if u2 == -1:
-            count_score.at[u1, 'num_tested'] = 0
-            count_score.at[u1, 'tp'] = 0
-            count_score.at[u1, 'fp'] = 0
-            count_score.at[u1, 'fn'] = event_counts1.at[u1]
-            count_score.at[u1, 'num_gt'] = event_counts1.at[u1]
+            count_score.at[u1, "num_tested"] = 0
+            count_score.at[u1, "tp"] = 0
+            count_score.at[u1, "fp"] = 0
+            count_score.at[u1, "fn"] = event_counts1.at[u1]
+            count_score.at[u1, "num_gt"] = event_counts1.at[u1]
         else:
             num_match = match_event_count.at[u1, u2]
-            count_score.at[u1, 'tp'] = num_match
-            count_score.at[u1, 'fn'] = event_counts1.at[u1] - num_match
-            count_score.at[u1, 'fp'] = event_counts2.at[u2] - num_match
+            count_score.at[u1, "tp"] = num_match
+            count_score.at[u1, "fn"] = event_counts1.at[u1] - num_match
+            count_score.at[u1, "fp"] = event_counts2.at[u2] - num_match
 
-            count_score.at[u1, 'num_gt'] = event_counts1.at[u1]
-            count_score.at[u1, 'num_tested'] = event_counts2.at[u2]
+            count_score.at[u1, "num_gt"] = event_counts1.at[u1]
+            count_score.at[u1, "num_tested"] = event_counts2.at[u2]
 
     return count_score
 
 
-_perf_keys = ['accuracy', 'recall', 'precision', 'false_discovery_rate', 'miss_rate']
+_perf_keys = ["accuracy", "recall", "precision", "false_discovery_rate", "miss_rate"]
 
 
 def compute_performance(count_score):
     """
     This compute perf formula.
     this trick here is that it works both on pd.Series and pd.Dataframe
     line by line.
@@ -615,30 +633,31 @@
     https://en.wikipedia.org/wiki/Sensitivity_and_specificity
 
     Note :
       * we don't have TN because it do not make sens here.
       * 'accuracy' = 'tp_rate' because TN=0
       * 'recall' = 'sensitivity'
     """
+    import pandas as pd
 
     perf = pd.DataFrame(index=count_score.index, columns=_perf_keys)
-    perf.index.name = 'gt_unit_id'
+    perf.index.name = "gt_unit_id"
     perf[:] = 0
 
     # make it robust when num_gt is 0
-    keep = (count_score['num_gt'] > 0) & (count_score['tp'] > 0)
+    keep = (count_score["num_gt"] > 0) & (count_score["tp"] > 0)
 
     c = count_score.loc[keep]
-    tp, fn, fp, num_gt = c['tp'], c['fn'], c['fp'], c['num_gt']
+    tp, fn, fp, num_gt = c["tp"], c["fn"], c["fp"], c["num_gt"]
 
-    perf.loc[keep, 'accuracy'] = tp / (tp + fn + fp)
-    perf.loc[keep, 'recall'] = tp / (tp + fn)
-    perf.loc[keep, 'precision'] = tp / (tp + fp)
-    perf.loc[keep, 'false_discovery_rate'] = fp / (tp + fp)
-    perf.loc[keep, 'miss_rate'] = fn / num_gt
+    perf.loc[keep, "accuracy"] = tp / (tp + fn + fp)
+    perf.loc[keep, "recall"] = tp / (tp + fn)
+    perf.loc[keep, "precision"] = tp / (tp + fp)
+    perf.loc[keep, "false_discovery_rate"] = fp / (tp + fp)
+    perf.loc[keep, "miss_rate"] = fn / num_gt
 
     return perf
 
 
 def make_matching_events(times1, times2, delta):
     """
     Similar to count_matching_events but get index instead of counting.
@@ -656,45 +675,45 @@
     Returns
     -------
     matching_event: numpy array dtype = ['index1', 'index2', 'delta']
         1d of collision
     """
     times_concat = np.concatenate((times1, times2))
     membership = np.concatenate((np.ones(times1.shape) * 1, np.ones(times2.shape) * 2))
-    spike_idx = np.concatenate((np.arange(times1.size, dtype='int64'), np.arange(times2.size, dtype='int64')))
+    spike_idx = np.concatenate((np.arange(times1.size, dtype="int64"), np.arange(times2.size, dtype="int64")))
     indices = times_concat.argsort()
 
     times_concat_sorted = times_concat[indices]
     membership_sorted = membership[indices]
     spike_index_sorted = spike_idx[indices]
 
-    inds, = np.nonzero((np.diff(times_concat_sorted) <= delta) & (np.diff(membership_sorted) != 0))
+    (inds,) = np.nonzero((np.diff(times_concat_sorted) <= delta) & (np.diff(membership_sorted) != 0))
 
-    dtype = [('index1', 'int64'), ('index2', 'int64'), ('delta_frame', 'int64')]
+    dtype = [("index1", "int64"), ("index2", "int64"), ("delta_frame", "int64")]
 
     if len(inds) == 0:
         return np.array([], dtype=dtype)
 
     matching_event = np.zeros(inds.size, dtype=dtype)
 
     mask1 = membership_sorted[inds] == 1
     inds1 = inds[mask1]
     n1 = np.sum(mask1)
-    matching_event[:n1]['index1'] = spike_index_sorted[inds1]
-    matching_event[:n1]['index2'] = spike_index_sorted[inds1 + 1]
-    matching_event[:n1]['delta_frame'] = times_concat_sorted[inds1 + 1] - times_concat_sorted[inds1]
+    matching_event[:n1]["index1"] = spike_index_sorted[inds1]
+    matching_event[:n1]["index2"] = spike_index_sorted[inds1 + 1]
+    matching_event[:n1]["delta_frame"] = times_concat_sorted[inds1 + 1] - times_concat_sorted[inds1]
 
     mask2 = membership_sorted[inds] == 2
     inds2 = inds[mask2]
     n2 = np.sum(mask2)
-    matching_event[n1:]['index1'] = spike_index_sorted[inds2 + 1]
-    matching_event[n1:]['index2'] = spike_index_sorted[inds2]
-    matching_event[n1:]['delta_frame'] = times_concat_sorted[inds2] - times_concat_sorted[inds2 + 1]
+    matching_event[n1:]["index1"] = spike_index_sorted[inds2 + 1]
+    matching_event[n1:]["index2"] = spike_index_sorted[inds2]
+    matching_event[n1:]["delta_frame"] = times_concat_sorted[inds2] - times_concat_sorted[inds2 + 1]
 
-    order = np.argsort(matching_event['index1'])
+    order = np.argsort(matching_event["index1"])
     matching_event = matching_event[order]
 
     return matching_event
 
 
 def make_collision_events(sorting, delta):
     """
@@ -714,36 +733,37 @@
             dtype =  [('index1', 'int64'), ('unit_id1', 'int64'),
                 ('index2', 'int64'), ('unit_id2', 'int64'),
                 ('delta', 'int64')]
         1d of all collision
     """
     unit_ids = np.array(sorting.get_unit_ids())
     dtype = [
-        ('index1', 'int64'), ('unit_id1', unit_ids.dtype),
-        ('index2', 'int64'), ('unit_id2', unit_ids.dtype),
-        ('delta_frame', 'int64')
+        ("index1", "int64"),
+        ("unit_id1", unit_ids.dtype),
+        ("index2", "int64"),
+        ("unit_id2", unit_ids.dtype),
+        ("delta_frame", "int64"),
     ]
 
     collision_events = []
     for i, u1 in enumerate(unit_ids):
         times1 = sorting.get_unit_spike_train(u1)
 
-        for u2 in unit_ids[i + 1:]:
+        for u2 in unit_ids[i + 1 :]:
             times2 = sorting.get_unit_spike_train(u2)
 
             matching_event = make_matching_events(times1, times2, delta)
             ce = np.zeros(matching_event.size, dtype=dtype)
-            ce['index1'] = matching_event['index1']
-            ce['unit_id1'] = u1
-            ce['index2'] = matching_event['index2']
-            ce['unit_id2'] = u2
-            ce['delta_frame'] = matching_event['delta_frame']
+            ce["index1"] = matching_event["index1"]
+            ce["unit_id1"] = u1
+            ce["index2"] = matching_event["index2"]
+            ce["unit_id2"] = u2
+            ce["delta_frame"] = matching_event["delta_frame"]
 
             collision_events.append(ce)
 
     if len(collision_events) > 0:
         collision_events = np.concatenate(collision_events)
     else:
         collision_events = np.zeros(0, dtype=dtype)
 
     return collision_events
-
```

### Comparing `spikeinterface-0.97.1/spikeinterface/comparison/correlogramcomparison.py` & `spikeinterface-0.98.0/src/spikeinterface/comparison/correlogramcomparison.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,8 +1,7 @@
-import pandas as pd
 import numpy as np
 from .paircomparisons import GroundTruthComparison
 from spikeinterface.postprocessing import compute_correlograms
 
 
 class CorrelogramGTComparison(GroundTruthComparison):
     """
@@ -12,17 +11,16 @@
 
     collision_lag: float
         Collision lag in ms.
 
     """
 
     def __init__(self, gt_sorting, tested_sorting, window_ms=100.0, bin_ms=1.0, well_detected_score=0.8, **kwargs):
-
         # Force compute labels
-        kwargs['compute_labels'] = True
+        kwargs["compute_labels"] = True
 
         if gt_sorting.get_num_segments() > 1 or tested_sorting.get_num_segments() > 1:
             raise NotImplementedError("Correlogram comparison is only available for mono-segment sorting objects")
 
         GroundTruthComparison.__init__(self, gt_sorting, tested_sorting, **kwargs)
 
         self.window_ms = window_ms
@@ -30,78 +28,76 @@
         self.well_detected_score = well_detected_score
         self.compute_kwargs = dict(window_ms=window_ms, bin_ms=bin_ms)
         self.correlograms = {}
         self.compute_correlograms()
 
     @property
     def time_bins(self):
-        return np.linspace(-self.window_ms/2, self.window_ms/2, self.nb_timesteps)
+        return np.linspace(-self.window_ms / 2, self.window_ms / 2, self.nb_timesteps)
 
     def compute_correlograms(self):
-
         correlograms_1, bins = compute_correlograms(self.sorting1, **self.compute_kwargs)
         correlograms_2, bins = compute_correlograms(self.sorting2, **self.compute_kwargs)
 
         self.good_sorting = self.get_well_detected_units(self.well_detected_score)
         self.good_gt = self.hungarian_match_21[self.good_sorting].values
         self.good_idx_gt = self.sorting1.ids_to_indices(self.good_gt)
         self.good_idx_sorting = self.sorting2.ids_to_indices(self.good_sorting)
 
-        #order = np.argsort(self.good_idx_gt)
-        #self.good_idx_gt = self.good_idx_gt[order]
+        # order = np.argsort(self.good_idx_gt)
+        # self.good_idx_gt = self.good_idx_gt[order]
 
         if len(self.good_idx_gt) > 0:
             correlograms_1 = correlograms_1[self.good_idx_gt, :, :]
-            self.correlograms['true'] = correlograms_1[:, self.good_idx_gt, :]
+            self.correlograms["true"] = correlograms_1[:, self.good_idx_gt, :]
 
         if len(self.good_idx_sorting) > 0:
             correlograms_2 = correlograms_2[self.good_idx_sorting, :, :]
-            self.correlograms['estimated'] = correlograms_2[:, self.good_idx_sorting, :]
+            self.correlograms["estimated"] = correlograms_2[:, self.good_idx_sorting, :]
 
         if len(self.good_idx_gt) > 0:
-            self.nb_cells = self.correlograms['true'].shape[0]
-            self.nb_timesteps = self.correlograms['true'].shape[2]
+            self.nb_cells = self.correlograms["true"].shape[0]
+            self.nb_timesteps = self.correlograms["true"].shape[2]
         else:
             self.nb_cells = 0
             self.nb_timesteps = 11
-            self.correlograms['true'] = np.zeros((0, 0, self.nb_timesteps))
-            self.correlograms['estimated'] = np.zeros((0, 0, self.nb_timesteps))
+            self.correlograms["true"] = np.zeros((0, 0, self.nb_timesteps))
+            self.correlograms["estimated"] = np.zeros((0, 0, self.nb_timesteps))
 
         self._center = self.nb_timesteps // 2
 
     def _get_slice(self, window_ms=None):
         if window_ms is None:
             amin = 0
             amax = self.nb_timesteps
         else:
-            amin = self._center - int(window_ms/self.bin_ms)
-            amax = self._center + int(window_ms/self.bin_ms) + 1
-
+            amin = self._center - int(window_ms / self.bin_ms)
+            amax = self._center + int(window_ms / self.bin_ms) + 1
 
         res = np.nan * np.ones((self.nb_cells, self.nb_cells, amax - amin))
 
-        indices = np.where(self.correlograms['true'][:,:,amin:amax] > 0)
-        res[indices] = np.abs(1 - self.correlograms['estimated'][:,:,amin:amax]/self.correlograms['true'][:,:,amin:amax])[indices]
+        indices = np.where(self.correlograms["true"][:, :, amin:amax] > 0)
+        res[indices] = np.abs(
+            1 - self.correlograms["estimated"][:, :, amin:amax] / self.correlograms["true"][:, :, amin:amax]
+        )[indices]
 
         return res
 
     def error(self, window_ms=None):
         data = self._get_slice(window_ms)
         res = data.reshape(self.nb_cells**2, data.shape[2])
         return np.mean(res, 0)
 
     def compute_correlogram_by_similarity(self, similarity_matrix, window_ms=None):
-
         errors = []
         similarities = []
         error = self._get_slice(window_ms)
 
         for r, u1 in enumerate(self.good_gt):
             for c, u2 in enumerate(self.good_gt):
-
                 ind1 = self.sorting1.id_to_index(u1)
                 ind2 = self.sorting1.id_to_index(u2)
 
                 similarities.append(similarity_matrix[ind1, ind2])
                 errors.append(error[r, c])
 
         errors = np.array(errors)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/comparison/correlogramstudy.py` & `spikeinterface-0.98.0/src/spikeinterface/comparison/correlogramstudy.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,75 +1,76 @@
-
 from .groundtruthstudy import GroundTruthStudy
 from .studytools import iter_computed_sorting
 from .correlogramcomparison import CorrelogramGTComparison
 
 import numpy as np
 
-class CorrelogramGTStudy(GroundTruthStudy):
 
+class CorrelogramGTStudy(GroundTruthStudy):
     def run_comparisons(self, exhaustive_gt=True, window_ms=100.0, bin_ms=1.0, well_detected_score=0.8, **kwargs):
         self.comparisons = {}
         for rec_name, sorter_name, sorting in iter_computed_sorting(self.study_folder):
             gt_sorting = self.get_ground_truth(rec_name)
-            comp = CorrelogramGTComparison(gt_sorting, sorting, exhaustive_gt=exhaustive_gt, window_ms=window_ms, bin_ms=bin_ms, well_detected_score=well_detected_score)
+            comp = CorrelogramGTComparison(
+                gt_sorting,
+                sorting,
+                exhaustive_gt=exhaustive_gt,
+                window_ms=window_ms,
+                bin_ms=bin_ms,
+                well_detected_score=well_detected_score,
+            )
             self.comparisons[(rec_name, sorter_name)] = comp
 
         self.exhaustive_gt = exhaustive_gt
 
     @property
     def time_bins(self):
         for key, value in self.comparisons.items():
             return value.time_bins
 
     def precompute_scores_by_similarities(self, good_only=True):
-
-        if not hasattr(self, '_computed'):
-
+        if not hasattr(self, "_computed"):
             import sklearn
-                    
+
             similarity_matrix = {}
             for rec_name in self.rec_names:
                 templates = self.get_templates(rec_name)
                 flat_templates = templates.reshape(templates.shape[0], -1)
                 similarity_matrix[rec_name] = sklearn.metrics.pairwise.cosine_similarity(flat_templates)
 
             self.all_similarities = {}
             self.all_errors = {}
             self._computed = True
 
             for sorter_ind, sorter_name in enumerate(self.sorter_names):
-            
                 # loop over recordings
                 all_errors = []
                 all_similarities = []
                 for rec_name in self.rec_names:
-                             
-                    try:  
+                    try:
                         comp = self.comparisons[(rec_name, sorter_name)]
                         similarities, errors = comp.compute_correlogram_by_similarity(similarity_matrix[rec_name])
                         all_similarities.append(similarities)
                         all_errors.append(errors)
                     except Exception:
                         pass
-                
+
                 self.all_similarities[sorter_name] = np.concatenate(all_similarities, axis=0)
                 self.all_errors[sorter_name] = np.concatenate(all_errors, axis=0)
 
     def get_error_profile_over_similarity_bins(self, similarity_bins, sorter_name):
-
         all_similarities = self.all_similarities[sorter_name]
         all_errors = self.all_errors[sorter_name]
-            
+
         order = np.argsort(all_similarities)
         all_similarities = all_similarities[order]
         all_errors = all_errors[order, :]
 
         result = {}
 
         for i in range(similarity_bins.size - 1):
             cmin, cmax = similarity_bins[i], similarity_bins[i + 1]
             amin, amax = np.searchsorted(all_similarities, [cmin, cmax])
             mean_errors = np.nanmean(all_errors[amin:amax], axis=0)
             result[(cmin, cmax)] = mean_errors
 
-        return result
+        return result
```

### Comparing `spikeinterface-0.97.1/spikeinterface/comparison/groundtruthstudy.py` & `spikeinterface-0.98.0/src/spikeinterface/comparison/groundtruthstudy.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,46 +1,53 @@
 from pathlib import Path
-import os
 import shutil
+
 import numpy as np
-import pandas as pd
 
 from spikeinterface.core import load_extractor
 from spikeinterface.extractors import NpzSortingExtractor
 from spikeinterface.sorters import sorter_dict, run_sorters
 
 from spikeinterface import WaveformExtractor
 from spikeinterface.qualitymetrics import compute_quality_metrics
 
-from .comparisontools import _perf_keys
 from .paircomparisons import compare_sorter_to_ground_truth
 
-from .studytools import (setup_comparison_study, get_rec_names, get_recordings,
-                         iter_working_folder, iter_computed_names, iter_computed_sorting, collect_run_times)
+from .studytools import (
+    setup_comparison_study,
+    get_rec_names,
+    get_recordings,
+    iter_working_folder,
+    iter_computed_names,
+    iter_computed_sorting,
+    collect_run_times,
+)
 
 
 class GroundTruthStudy:
     def __init__(self, study_folder=None):
+        import pandas as pd
+
         self.study_folder = Path(study_folder)
         self._is_scanned = False
         self.computed_names = None
         self.rec_names = None
         self.sorter_names = None
 
         self.scan_folder()
 
         self.comparisons = None
         self.exhaustive_gt = None
 
     def __repr__(self):
-        t = 'Ground truth study\n'
-        t += '  ' + str(self.study_folder) + '\n'
-        t += '  recordings: {} {}\n'.format(len(self.rec_names), self.rec_names)
+        t = "Ground truth study\n"
+        t += "  " + str(self.study_folder) + "\n"
+        t += "  recordings: {} {}\n".format(len(self.rec_names), self.rec_names)
         if len(self.sorter_names):
-            t += '  sorters: {} {}\n'.format(len(self.sorter_names), self.sorter_names)
+            t += "  sorters: {} {}\n".format(len(self.sorter_names), self.sorter_names)
 
         return t
 
     def scan_folder(self):
         self.rec_names = get_rec_names(self.study_folder)
         # scan computed names
         self.computed_names = list(iter_computed_names(self.study_folder))  # list of pair (rec_name, sorter_name)
@@ -48,81 +55,86 @@
         self._is_scanned = True
 
     @classmethod
     def create(cls, study_folder, gt_dict, **job_kwargs):
         setup_comparison_study(study_folder, gt_dict, **job_kwargs)
         return cls(study_folder)
 
-    def run_sorters(self, sorter_list, mode_if_folder_exists='keep', remove_sorter_folders=False, **kwargs):
-
-        sorter_folders = self.study_folder / 'sorter_folders'
+    def run_sorters(self, sorter_list, mode_if_folder_exists="keep", remove_sorter_folders=False, **kwargs):
+        sorter_folders = self.study_folder / "sorter_folders"
         recording_dict = get_recordings(self.study_folder)
 
-        run_sorters(sorter_list, recording_dict, sorter_folders,
-                    with_output=False, mode_if_folder_exists=mode_if_folder_exists, **kwargs)
+        run_sorters(
+            sorter_list,
+            recording_dict,
+            sorter_folders,
+            with_output=False,
+            mode_if_folder_exists=mode_if_folder_exists,
+            **kwargs,
+        )
 
         # results are copied so the heavy sorter_folders can be removed
         self.copy_sortings()
 
         if remove_sorter_folders:
-            shutil.rmtree(self.study_folder / 'sorter_folders')
+            shutil.rmtree(self.study_folder / "sorter_folders")
 
     def _check_rec_name(self, rec_name):
         if not self._is_scanned:
             self.scan_folder()
         if len(self.rec_names) > 1 and rec_name is None:
             raise Exception("Pass 'rec_name' parameter to select which recording to use.")
         elif len(self.rec_names) == 1:
             rec_name = self.rec_names[0]
         else:
             rec_name = self.rec_names[self.rec_names.index(rec_name)]
         return rec_name
 
     def get_ground_truth(self, rec_name=None):
         rec_name = self._check_rec_name(rec_name)
-        sorting = load_extractor(self.study_folder / 'ground_truth' / rec_name)
+        sorting = load_extractor(self.study_folder / "ground_truth" / rec_name)
         return sorting
 
     def get_recording(self, rec_name=None):
         rec_name = self._check_rec_name(rec_name)
-        rec = load_extractor(self.study_folder / 'raw_files' / rec_name)
+        rec = load_extractor(self.study_folder / "raw_files" / rec_name)
         return rec
 
     def get_sorting(self, sort_name, rec_name=None):
         rec_name = self._check_rec_name(rec_name)
 
         selected_sorting = None
         if sort_name in self.sorter_names:
             for r_name, sorter_name, sorting in iter_computed_sorting(self.study_folder):
                 if sort_name == sorter_name and r_name == rec_name:
                     selected_sorting = sorting
         return selected_sorting
 
     def copy_sortings(self):
-
-        sorter_folders = self.study_folder / 'sorter_folders'
-        sorting_folders = self.study_folder / 'sortings'
-        log_olders = self.study_folder / 'sortings' / 'run_log'
+        sorter_folders = self.study_folder / "sorter_folders"
+        sorting_folders = self.study_folder / "sortings"
+        log_olders = self.study_folder / "sortings" / "run_log"
 
         log_olders.mkdir(parents=True, exist_ok=True)
 
         for rec_name, sorter_name, output_folder in iter_working_folder(sorter_folders):
             SorterClass = sorter_dict[sorter_name]
-            fname = rec_name + '[#]' + sorter_name
-            npz_filename = sorting_folders / (fname + '.npz')
+            fname = rec_name + "[#]" + sorter_name
+            npz_filename = sorting_folders / (fname + ".npz")
 
             try:
                 sorting = SorterClass.get_result_from_folder(output_folder)
                 NpzSortingExtractor.write_sorting(sorting, npz_filename)
             except:
                 if npz_filename.is_file():
                     npz_filename.unlink()
-            if (output_folder / 'spikeinterface_log.json').is_file():
-                shutil.copyfile(output_folder / 'spikeinterface_log.json',
-                                sorting_folders / 'run_log' / (fname + '.json'))
+            if (output_folder / "spikeinterface_log.json").is_file():
+                shutil.copyfile(
+                    output_folder / "spikeinterface_log.json", sorting_folders / "run_log" / (fname + ".json")
+                )
 
         self.scan_folder()
 
     def run_comparisons(self, exhaustive_gt=False, **kwargs):
         self.comparisons = {}
         for rec_name, sorter_name, sorting in iter_computed_sorting(self.study_folder):
             gt_sorting = self.get_ground_truth(rec_name)
@@ -130,166 +142,186 @@
             self.comparisons[(rec_name, sorter_name)] = sc
         self.exhaustive_gt = exhaustive_gt
 
     def aggregate_run_times(self):
         return collect_run_times(self.study_folder)
 
     def aggregate_performance_by_unit(self):
-        assert self.comparisons is not None, 'run_comparisons first'
+        assert self.comparisons is not None, "run_comparisons first"
 
         perf_by_unit = []
         for rec_name, sorter_name, sorting in iter_computed_sorting(self.study_folder):
             comp = self.comparisons[(rec_name, sorter_name)]
 
-            perf = comp.get_performance(method='by_unit', output='pandas')
-            perf['rec_name'] = rec_name
-            perf['sorter_name'] = sorter_name
+            perf = comp.get_performance(method="by_unit", output="pandas")
+            perf["rec_name"] = rec_name
+            perf["sorter_name"] = sorter_name
             perf = perf.reset_index()
             perf_by_unit.append(perf)
 
+        import pandas as pd
+
         perf_by_unit = pd.concat(perf_by_unit)
-        perf_by_unit = perf_by_unit.set_index(['rec_name', 'sorter_name', 'gt_unit_id'])
+        perf_by_unit = perf_by_unit.set_index(["rec_name", "sorter_name", "gt_unit_id"])
 
         return perf_by_unit
 
     def aggregate_count_units(self, well_detected_score=None, redundant_score=None, overmerged_score=None):
-        assert self.comparisons is not None, 'run_comparisons first'
+        assert self.comparisons is not None, "run_comparisons first"
+
+        import pandas as pd
 
-        index = pd.MultiIndex.from_tuples(self.computed_names, names=['rec_name', 'sorter_name'])
+        index = pd.MultiIndex.from_tuples(self.computed_names, names=["rec_name", "sorter_name"])
 
-        count_units = pd.DataFrame(index=index, columns=['num_gt', 'num_sorter', 'num_well_detected', 'num_redundant',
-                                                         'num_overmerged'], dtype=int)
+        count_units = pd.DataFrame(
+            index=index,
+            columns=["num_gt", "num_sorter", "num_well_detected", "num_redundant", "num_overmerged"],
+            dtype=int,
+        )
 
         if self.exhaustive_gt:
-            count_units['num_false_positive'] = pd.Series(dtype=int)
-            count_units['num_bad'] = pd.Series(dtype=int)
+            count_units["num_false_positive"] = pd.Series(dtype=int)
+            count_units["num_bad"] = pd.Series(dtype=int)
 
         for rec_name, sorter_name, sorting in iter_computed_sorting(self.study_folder):
             gt_sorting = self.get_ground_truth(rec_name)
             comp = self.comparisons[(rec_name, sorter_name)]
 
-            count_units.loc[(rec_name, sorter_name), 'num_gt'] = len(gt_sorting.get_unit_ids())
-            count_units.loc[(rec_name, sorter_name), 'num_sorter'] = len(sorting.get_unit_ids())
-            count_units.loc[(rec_name, sorter_name), 'num_well_detected'] = \
-                comp.count_well_detected_units(well_detected_score)
+            count_units.loc[(rec_name, sorter_name), "num_gt"] = len(gt_sorting.get_unit_ids())
+            count_units.loc[(rec_name, sorter_name), "num_sorter"] = len(sorting.get_unit_ids())
+            count_units.loc[(rec_name, sorter_name), "num_well_detected"] = comp.count_well_detected_units(
+                well_detected_score
+            )
             if self.exhaustive_gt:
-                count_units.loc[(rec_name, sorter_name), 'num_overmerged'] = \
-                    comp.count_overmerged_units(overmerged_score)
-                count_units.loc[(rec_name, sorter_name), 'num_redundant'] = \
-                    comp.count_redundant_units(redundant_score)
-                count_units.loc[(rec_name, sorter_name), 'num_false_positive'] = \
-                    comp.count_false_positive_units(redundant_score)
-                count_units.loc[(rec_name, sorter_name), 'num_bad'] = comp.count_bad_units()
+                count_units.loc[(rec_name, sorter_name), "num_overmerged"] = comp.count_overmerged_units(
+                    overmerged_score
+                )
+                count_units.loc[(rec_name, sorter_name), "num_redundant"] = comp.count_redundant_units(redundant_score)
+                count_units.loc[(rec_name, sorter_name), "num_false_positive"] = comp.count_false_positive_units(
+                    redundant_score
+                )
+                count_units.loc[(rec_name, sorter_name), "num_bad"] = comp.count_bad_units()
 
         return count_units
 
     def aggregate_dataframes(self, copy_into_folder=True, **karg_thresh):
         dataframes = {}
-        dataframes['run_times'] = self.aggregate_run_times().reset_index()
+        dataframes["run_times"] = self.aggregate_run_times().reset_index()
         perfs = self.aggregate_performance_by_unit()
 
-        dataframes['perf_by_unit'] = perfs.reset_index()
-        dataframes['count_units'] = self.aggregate_count_units(**karg_thresh).reset_index()
+        dataframes["perf_by_unit"] = perfs.reset_index()
+        dataframes["count_units"] = self.aggregate_count_units(**karg_thresh).reset_index()
 
         if copy_into_folder:
-            tables_folder = self.study_folder / 'tables'
+            tables_folder = self.study_folder / "tables"
             tables_folder.mkdir(parents=True, exist_ok=True)
 
             for name, df in dataframes.items():
-                df.to_csv(str(tables_folder / (name + '.csv')), sep='\t', index=False)
+                df.to_csv(str(tables_folder / (name + ".csv")), sep="\t", index=False)
 
         return dataframes
 
     def get_waveform_extractor(self, rec_name, sorter_name=None):
         rec = self.get_recording(rec_name)
 
         if sorter_name is None:
-            name = 'GroundTruth'
+            name = "GroundTruth"
             sorting = self.get_ground_truth(rec_name)
         else:
             assert sorter_name in self.sorter_names
             name = sorter_name
             sorting = self.get_sorting(sorter_name, rec_name)
 
-        waveform_folder = self.study_folder / 'waveforms' / f'waveforms_{name}_{rec_name}'
+        waveform_folder = self.study_folder / "waveforms" / f"waveforms_{name}_{rec_name}"
 
         if waveform_folder.is_dir():
             we = WaveformExtractor.load(waveform_folder)
         else:
             we = WaveformExtractor.create(rec, sorting, waveform_folder)
         return we
 
-    def compute_waveforms(self, rec_name, sorter_name=None,
-                ms_before=3., ms_after=4., max_spikes_per_unit=500,
-                n_jobs=-1, total_memory='1G'):
-
+    def compute_waveforms(
+        self,
+        rec_name,
+        sorter_name=None,
+        ms_before=3.0,
+        ms_after=4.0,
+        max_spikes_per_unit=500,
+        n_jobs=-1,
+        total_memory="1G",
+    ):
         we = self.get_waveform_extractor(rec_name, sorter_name)
         we.set_params(ms_before=ms_before, ms_after=ms_after, max_spikes_per_unit=max_spikes_per_unit)
         we.run_extract_waveforms(n_jobs=n_jobs, total_memory=total_memory)
 
-    def get_templates(self, rec_name, sorter_name=None, mode='median'):
+    def get_templates(self, rec_name, sorter_name=None, mode="median"):
         """
         Get template for a given recording.
 
         If sorter_name=None then template are from the ground truth.
 
         """
         we = self.get_waveform_extractor(rec_name, sorter_name=sorter_name)
         templates = we.get_all_templates(mode=mode)
         return templates
 
-    def compute_metrics(self, rec_name, metric_names=['snr'],
-                ms_before=3., ms_after=4., max_spikes_per_unit=500,
-                n_jobs=-1, total_memory='1G'):
-
+    def compute_metrics(
+        self,
+        rec_name,
+        metric_names=["snr"],
+        ms_before=3.0,
+        ms_after=4.0,
+        max_spikes_per_unit=500,
+        n_jobs=-1,
+        total_memory="1G",
+    ):
         we = self.get_waveform_extractor(rec_name)
         we.set_params(ms_before=ms_before, ms_after=ms_after, max_spikes_per_unit=max_spikes_per_unit)
         we.run_extract_waveforms(n_jobs=n_jobs, total_memory=total_memory)
 
         # metrics
         metrics = compute_quality_metrics(we, metric_names=metric_names)
-        folder = self.study_folder / 'metrics'
+        folder = self.study_folder / "metrics"
         folder.mkdir(exist_ok=True)
-        filename = folder / f'metrics _{rec_name}.txt'
-        metrics.to_csv(filename, sep='\t', index=True)
+        filename = folder / f"metrics _{rec_name}.txt"
+        metrics.to_csv(filename, sep="\t", index=True)
 
         return metrics
 
-
     def get_metrics(self, rec_name=None, **metric_kwargs):
         """
         Load or compute units metrics  for a given recording.
         """
         rec_name = self._check_rec_name(rec_name)
-        metrics_folder = self.study_folder / 'metrics'
+        metrics_folder = self.study_folder / "metrics"
         metrics_folder.mkdir(parents=True, exist_ok=True)
 
-        filename = self.study_folder / 'metrics' / f'metrics _{rec_name}.txt'
+        filename = self.study_folder / "metrics" / f"metrics _{rec_name}.txt"
+        import pandas as pd
+
         if filename.is_file():
-            metrics = pd.read_csv(filename, sep='\t', index_col=0)
+            metrics = pd.read_csv(filename, sep="\t", index_col=0)
             gt_sorting = self.get_ground_truth(rec_name)
             metrics.index = gt_sorting.unit_ids
         else:
             metrics = self.compute_metrics(rec_name, **metric_kwargs)
 
-        metrics.index.name = 'unit_id'
+        metrics.index.name = "unit_id"
         #  add rec name columns
-        metrics['rec_name'] = rec_name
+        metrics["rec_name"] = rec_name
 
         return metrics
 
     def get_units_snr(self, rec_name=None, **metric_kwargs):
-        """
-
-        """
+        """ """
         metric = self.get_metrics(rec_name=rec_name, **metric_kwargs)
-        return metric['snr']
+        return metric["snr"]
 
     def concat_all_snr(self):
         metrics = []
         for rec_name in self.rec_names:
             df = self.get_metrics(rec_name)
             df = df.reset_index()
             metrics.append(df)
         metrics = pd.concat(metrics)
-        metrics = metrics.set_index(['rec_name', 'unit_id'])
-        return metrics['snr']
+        metrics = metrics.set_index(["rec_name", "unit_id"])
+        return metrics["snr"]
```

### Comparing `spikeinterface-0.97.1/spikeinterface/comparison/hybrid.py` & `spikeinterface-0.98.0/src/spikeinterface/comparison/hybrid.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,11 +1,18 @@
 from pathlib import Path
 from typing import List, Union
 import numpy as np
-from spikeinterface.core import BaseRecording, BaseSorting, WaveformExtractor, NumpySorting, NpzSortingExtractor, InjectTemplatesRecording
+from spikeinterface.core import (
+    BaseRecording,
+    BaseSorting,
+    WaveformExtractor,
+    NumpySorting,
+    NpzSortingExtractor,
+    InjectTemplatesRecording,
+)
 from spikeinterface.core.core_tools import define_function_from_class
 from spikeinterface.core import generate_sorting
 
 
 class HybridUnitsRecording(InjectTemplatesRecording):
     """
     Class for creating a hybrid recording where additional units are added
@@ -38,63 +45,81 @@
 
     Returns
     -------
     hybrid_units_recording: HybridUnitsRecording
         The recording containing real and hybrid units.
     """
 
-    def __init__(self, parent_recording: BaseRecording, templates: np.ndarray,
-                 injected_sorting: Union[BaseSorting, None] = None, nbefore: Union[List[int], int, None] = None,
-                 firing_rate: float = 10, amplitude_factor: Union[np.ndarray, None] = None,
-                 amplitude_std: float = 0.0, refractory_period_ms: float = 2.0,
-                 injected_sorting_folder: Union[str, Path, None] = None,
-                 ):
-        num_samples = [parent_recording.get_num_frames(seg_index)
-                       for seg_index in range(parent_recording.get_num_segments())]
+    def __init__(
+        self,
+        parent_recording: BaseRecording,
+        templates: np.ndarray,
+        injected_sorting: Union[BaseSorting, None] = None,
+        nbefore: Union[List[int], int, None] = None,
+        firing_rate: float = 10,
+        amplitude_factor: Union[np.ndarray, None] = None,
+        amplitude_std: float = 0.0,
+        refractory_period_ms: float = 2.0,
+        injected_sorting_folder: Union[str, Path, None] = None,
+    ):
+        num_samples = [
+            parent_recording.get_num_frames(seg_index) for seg_index in range(parent_recording.get_num_segments())
+        ]
         fs = parent_recording.sampling_frequency
         n_units = len(templates)
 
         if injected_sorting is not None:
             assert injected_sorting.get_num_units() == n_units
             assert parent_recording.get_num_segments() == injected_sorting.get_num_segments()
         else:
-            assert injected_sorting_folder is not None, \
-                "Provide sorting_folder to save generated sorting object"
-            durations = [parent_recording.get_num_frames(seg_index) / fs
-                         for seg_index in range(parent_recording.get_num_segments())]
-            injected_sorting = generate_sorting(num_units=len(templates), sampling_frequency=fs,
-                                                durations=durations, firing_rate=firing_rate,
-                                                refractory_period=refractory_period_ms)
+            assert injected_sorting_folder is not None, "Provide sorting_folder to save generated sorting object"
+            durations = [
+                parent_recording.get_num_frames(seg_index) / fs
+                for seg_index in range(parent_recording.get_num_segments())
+            ]
+            injected_sorting = generate_sorting(
+                num_units=len(templates),
+                sampling_frequency=fs,
+                durations=durations,
+                firing_rate=firing_rate,
+                refractory_period=refractory_period_ms,
+            )
         # save injected sorting if necessary
         self.injected_sorting = injected_sorting
-        if not self.injected_sorting.is_dumpable:
-            assert injected_sorting_folder is not None, \
-                "Provide injected_sorting_folder to injected sorting object"
+        if not self.injected_sorting.check_if_json_serializable():
+            assert injected_sorting_folder is not None, "Provide injected_sorting_folder to injected sorting object"
             self.injected_sorting = self.injected_sorting.save(folder=injected_sorting_folder)
 
         if amplitude_factor is None:
-            amplitude_factor = [[np.random.normal(loc=1.0, scale=amplitude_std,
-                                                  size=len(self.injected_sorting.get_unit_spike_train(unit_id,
-                                                                                                      segment_index=seg_index)))
-                                for unit_id in self.injected_sorting.unit_ids]
-                                for seg_index in range(parent_recording.get_num_segments())]
+            amplitude_factor = [
+                [
+                    np.random.normal(
+                        loc=1.0,
+                        scale=amplitude_std,
+                        size=len(self.injected_sorting.get_unit_spike_train(unit_id, segment_index=seg_index)),
+                    )
+                    for unit_id in self.injected_sorting.unit_ids
+                ]
+                for seg_index in range(parent_recording.get_num_segments())
+            ]
 
         InjectTemplatesRecording.__init__(
-            self, self.injected_sorting, templates, nbefore, amplitude_factor, parent_recording, num_samples)
+            self, self.injected_sorting, templates, nbefore, amplitude_factor, parent_recording, num_samples
+        )
 
         self._kwargs = dict(
-            parent_recording=parent_recording.to_dict(),
+            parent_recording=parent_recording,
             templates=templates,
-            injected_sorting=self.injected_sorting.to_dict(),
+            injected_sorting=self.injected_sorting,
             nbefore=nbefore,
             firing_rate=firing_rate,
             amplitude_factor=amplitude_factor,
             amplitude_std=amplitude_std,
             refractory_period_ms=refractory_period_ms,
-            injected_sorting_folder=None
+            injected_sorting_folder=None,
         )
 
 
 class HybridSpikesRecording(InjectTemplatesRecording):
     """
     Class for creating a hybrid recording where additional spikes are added
     to already existing units.
@@ -123,72 +148,87 @@
 
     Returns
     -------
     hybrid_spikes_recording: HybridSpikesRecording:
         The recording containing units with real and hybrid spikes.
     """
 
-    def __init__(self, wvf_extractor: Union[WaveformExtractor, Path], injected_sorting: Union[BaseSorting, None] = None,
-                 unit_ids: Union[List[int], None] = None, max_injected_per_unit: int = 1000,
-                 injected_rate: float = 0.05, refractory_period_ms: float = 1.5,
-                 injected_sorting_folder: Union[str, Path, None] = None) -> None:
+    def __init__(
+        self,
+        wvf_extractor: Union[WaveformExtractor, Path],
+        injected_sorting: Union[BaseSorting, None] = None,
+        unit_ids: Union[List[int], None] = None,
+        max_injected_per_unit: int = 1000,
+        injected_rate: float = 0.05,
+        refractory_period_ms: float = 1.5,
+        injected_sorting_folder: Union[str, Path, None] = None,
+    ) -> None:
         if isinstance(wvf_extractor, (Path, str)):
             wvf_extractor = WaveformExtractor.load(wvf_extractor)
 
         target_recording = wvf_extractor.recording
         target_sorting = wvf_extractor.sorting
         templates = wvf_extractor.get_all_templates()
 
         if unit_ids is not None:
             target_sorting = target_sorting.select_units(unit_ids)
             templates = templates[target_sorting.ids_to_indices(unit_ids)]
 
         if injected_sorting is None:
-            assert injected_sorting_folder is not None, \
-                "Provide injected_sorting_folder to save generated injected sorting object"
-            num_samples = [target_recording.get_num_frames(seg_index)
-                           for seg_index in range(target_recording.get_num_segments())]
-            self.injected_sorting = generate_injected_sorting(target_sorting, num_samples, max_injected_per_unit,
-                                                              injected_rate, refractory_period_ms)
+            assert (
+                injected_sorting_folder is not None
+            ), "Provide injected_sorting_folder to save generated injected sorting object"
+            num_samples = [
+                target_recording.get_num_frames(seg_index) for seg_index in range(target_recording.get_num_segments())
+            ]
+            self.injected_sorting = generate_injected_sorting(
+                target_sorting, num_samples, max_injected_per_unit, injected_rate, refractory_period_ms
+            )
         else:
             self.injected_sorting = injected_sorting
 
         # save injected sorting if necessary
-        if not self.injected_sorting.is_dumpable:
-            assert injected_sorting_folder is not None, \
-                "Provide injected_sorting_folder to injected sorting object"
+        if not self.injected_sorting.check_if_json_serializable():
+            assert injected_sorting_folder is not None, "Provide injected_sorting_folder to injected sorting object"
             self.injected_sorting = self.injected_sorting.save(folder=injected_sorting_folder)
 
         InjectTemplatesRecording.__init__(
-            self, self.injected_sorting, templates, wvf_extractor.nbefore, parent_recording=target_recording)
+            self, self.injected_sorting, templates, wvf_extractor.nbefore, parent_recording=target_recording
+        )
 
         self._kwargs = dict(
             wvf_extractor=str(wvf_extractor.folder.absolute()),
             injected_sorting=self.injected_sorting.to_dict(),
             unit_ids=unit_ids,
             max_injected_per_unit=max_injected_per_unit,
             injected_rate=injected_rate,
             refractory_period_ms=refractory_period_ms,
-            injected_sorting_folder=None
+            injected_sorting_folder=None,
         )
 
 
-def generate_injected_sorting(sorting: BaseSorting, num_samples: List[int], max_injected_per_unit: int = 1000,
-                              injected_rate: float = 0.05, refractory_period_ms: float = 1.5) -> NumpySorting:
+def generate_injected_sorting(
+    sorting: BaseSorting,
+    num_samples: List[int],
+    max_injected_per_unit: int = 1000,
+    injected_rate: float = 0.05,
+    refractory_period_ms: float = 1.5,
+) -> NumpySorting:
     injected_spike_trains = [{} for seg_index in range(sorting.get_num_segments())]
     t_r = int(round(refractory_period_ms * sorting.get_sampling_frequency() * 1e-3))
 
     for segment_index in range(sorting.get_num_segments()):
         for unit_id in sorting.unit_ids:
             spike_train = sorting.get_unit_spike_train(unit_id, segment_index=segment_index)
             n_injection = min(max_injected_per_unit, int(round(injected_rate * len(spike_train))))
             # Inject more, then take out all that violate the refractory period.
             n = int(n_injection + 10 * np.sqrt(n_injection))
-            injected_spike_train = np.sort(np.random.uniform(
-                low=0, high=num_samples[segment_index], size=n).astype(np.int64))
+            injected_spike_train = np.sort(
+                np.random.uniform(low=0, high=num_samples[segment_index], size=n).astype(np.int64)
+            )
 
             # Remove spikes that are in the refractory period.
             violations = np.where(np.diff(injected_spike_train) < t_r)[0]
             injected_spike_train = np.delete(injected_spike_train, violations)
 
             # Remove spikes that violate the refractory period of the real spikes.
             # TODO: Need a better & faster way than this.
@@ -201,10 +241,12 @@
 
             injected_spike_trains[segment_index][unit_id] = injected_spike_train
 
     return NumpySorting.from_dict(injected_spike_trains, sorting.get_sampling_frequency())
 
 
 create_hybrid_units_recording = define_function_from_class(
-    source_class=HybridUnitsRecording, name="create_hybrid_units_recording")
+    source_class=HybridUnitsRecording, name="create_hybrid_units_recording"
+)
 create_hybrid_spikes_recording = define_function_from_class(
-    source_class=HybridSpikesRecording, name="create_hybrid_spikes_recording")
+    source_class=HybridSpikesRecording, name="create_hybrid_spikes_recording"
+)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/comparison/multicomparisons.py` & `spikeinterface-0.98.0/src/spikeinterface/comparison/multicomparisons.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,20 +1,19 @@
-import numpy as np
 from pathlib import Path
 import json
 import pickle
 
+import numpy as np
+
 from spikeinterface.core import load_extractor, BaseSorting, BaseSortingSegment
 from spikeinterface.core.core_tools import define_function_from_class
 from .basecomparison import BaseMultiComparison, MixinSpikeTrainComparison, MixinTemplateComparison
 from .paircomparisons import SymmetricSortingComparison, TemplateComparison
 from .comparisontools import compare_spike_trains
 
-import networkx as nx
-
 
 class MultiSortingComparison(BaseMultiComparison, MixinSpikeTrainComparison):
     """
     Compares multiple spike sorting outputs based on spike trains.
 
     - Pair-wise comparisons are made
     - An agreement graph is built based on the agreement score
@@ -45,105 +44,117 @@
 
     Returns
     -------
     multi_sorting_comparison: MultiSortingComparison
         MultiSortingComparison object with the multiple sorter comparison
     """
 
-    def __init__(self, sorting_list, name_list=None, delta_time=0.4,  # sampling_frequency=None,
-                 match_score=0.5, chance_score=0.1, n_jobs=-1, spiketrain_mode='union', verbose=False,
-                 do_matching=True):
+    def __init__(
+        self,
+        sorting_list,
+        name_list=None,
+        delta_time=0.4,  # sampling_frequency=None,
+        match_score=0.5,
+        chance_score=0.1,
+        n_jobs=-1,
+        spiketrain_mode="union",
+        verbose=False,
+        do_matching=True,
+    ):
         if name_list is None:
             name_list = [f"sorting{i}" for i in range(len(sorting_list))]
-        BaseMultiComparison.__init__(self, object_list=sorting_list, name_list=name_list,
-                                     match_score=match_score, chance_score=chance_score,
-                                     verbose=verbose)
+        BaseMultiComparison.__init__(
+            self,
+            object_list=sorting_list,
+            name_list=name_list,
+            match_score=match_score,
+            chance_score=chance_score,
+            verbose=verbose,
+        )
         MixinSpikeTrainComparison.__init__(self, delta_time=delta_time, n_jobs=n_jobs)
         self.set_frames_and_frequency(self.object_list)
         self._spiketrain_mode = spiketrain_mode
         self._spiketrains = None
         self._num_segments = sorting_list[0].get_num_segments()
 
         if do_matching:
             self._compute_all()
             self._populate_spiketrains()
 
     def _compare_ij(self, i, j):
-        comp = SymmetricSortingComparison(self.object_list[i], self.object_list[j],
-                                          sorting1_name=self.name_list[i],
-                                          sorting2_name=self.name_list[j],
-                                          delta_time=self.delta_time,
-                                          match_score=self.match_score,
-                                          n_jobs=self.n_jobs,
-                                          verbose=False)
+        comp = SymmetricSortingComparison(
+            self.object_list[i],
+            self.object_list[j],
+            sorting1_name=self.name_list[i],
+            sorting2_name=self.name_list[j],
+            delta_time=self.delta_time,
+            match_score=self.match_score,
+            n_jobs=self.n_jobs,
+            verbose=False,
+        )
         return comp
 
     def _populate_nodes(self):
         for i, sorting in enumerate(self.object_list):
             sorter_name = self.name_list[i]
             for unit_id in sorting.get_unit_ids():
                 node = sorter_name, unit_id
                 self.graph.add_node(node)
 
     def _populate_spiketrains(self):
         self._spiketrains = []
         for seg_index in range(self._num_segments):
             spike_trains_segment = dict()
-            for (unit_id, sg) in zip(self._new_units, self.subgraphs):
+            for unit_id, sg in zip(self._new_units, self.subgraphs):
                 sorter_unit_ids = self._new_units[unit_id]["unit_ids"]
                 edges = list(sg.edges(data=True))
                 # Append correct spike train
                 if len(sorter_unit_ids.keys()) == 1:
                     sorting = self.object_list[self.name_list.index(list(sorter_unit_ids.keys())[0])]
-                    unit_id = list(sorter_unit_ids.values())[0]
-                    spike_train = sorting.get_unit_spike_train(unit_id, seg_index)
+                    this_sorting_unit_id = list(sorter_unit_ids.values())[0]
+                    spike_train = sorting.get_unit_spike_train(this_sorting_unit_id, seg_index)
                 else:
-                    max_edge = edges[int(np.argmax([d['weight']
-                                        for u, v, d in edges]))]
+                    max_edge = edges[int(np.argmax([d["weight"] for u, v, d in edges]))]
                     node1, node2, weight = max_edge
                     sorter1, unit1 = node1
                     sorter2, unit2 = node2
 
                     sorting1 = self.object_list[self.name_list.index(sorter1)]
                     sorting2 = self.object_list[self.name_list.index(sorter2)]
                     sp1 = sorting1.get_unit_spike_train(unit1, seg_index)
                     sp2 = sorting2.get_unit_spike_train(unit2, seg_index)
-                    if self._spiketrain_mode == 'union':
+                    if self._spiketrain_mode == "union":
                         lab1, lab2 = compare_spike_trains(sp1, sp2)
                         # add FP to spike train 1 (FP are the only spikes outside the union)
-                        fp_idx2 = np.where(np.array(lab2) == 'FP')[0]
+                        fp_idx2 = np.where(np.array(lab2) == "FP")[0]
                         spike_train = np.sort(np.concatenate((sp1, sp2[fp_idx2])))
-                    elif self._spiketrain_mode == 'intersection':
+                    elif self._spiketrain_mode == "intersection":
                         lab1, lab2 = compare_spike_trains(sp1, sp2)
                         # TP are the spikes in the intersection
-                        tp_idx1 = np.where(np.array(lab1) == 'TP')[0]
+                        tp_idx1 = np.where(np.array(lab1) == "TP")[0]
                         spike_train = np.array(sp1)[tp_idx1]
                 spike_trains_segment[unit_id] = spike_train
             self._spiketrains.append(spike_trains_segment)
 
-
     def _do_agreement_matrix(self, minimum_agreement=1):
         sorted_name_list = sorted(self.name_list)
-        sorting_agr = AgreementSortingExtractor(
-            self.sampling_frequency, self, minimum_agreement)
+        sorting_agr = AgreementSortingExtractor(self.sampling_frequency, self, minimum_agreement)
         unit_ids = sorting_agr.get_unit_ids()
         agreement_matrix = np.zeros((len(unit_ids), len(sorted_name_list)))
 
         for u_i, unit in enumerate(unit_ids):
             for sort_name, sorter in enumerate(sorted_name_list):
-                if sorter in sorting_agr.get_unit_property(unit, 'unit_ids').keys():
-                    assigned_unit = sorting_agr.get_unit_property(unit, 'unit_ids')[
-                        sorter]
+                if sorter in sorting_agr.get_unit_property(unit, "unit_ids").keys():
+                    assigned_unit = sorting_agr.get_unit_property(unit, "unit_ids")[sorter]
                 else:
                     assigned_unit = -1
                 if assigned_unit == -1:
                     agreement_matrix[u_i, sort_name] = np.nan
                 else:
-                    agreement_matrix[u_i, sort_name] = sorting_agr.get_unit_property(
-                        unit, 'avg_agreement')
+                    agreement_matrix[u_i, sort_name] = sorting_agr.get_unit_property(unit, "avg_agreement")
         return agreement_matrix
 
     def get_agreement_sorting(self, minimum_agreement_count=1, minimum_agreement_count_only=False):
         """
         Returns AgreementSortingExtractor with units with a 'minimum_matching' agreement.
 
         Parameters
@@ -156,87 +167,104 @@
 
         Returns
         -------
         agreement_sorting: AgreementSortingExtractor
             The output AgreementSortingExtractor
         """
         assert minimum_agreement_count > 0, "'minimum_agreement_count' should be greater than 0"
-        sorting = AgreementSortingExtractor(self.sampling_frequency, self,
-                                            min_agreement_count=minimum_agreement_count,
-                                            min_agreement_count_only=minimum_agreement_count_only)
+        sorting = AgreementSortingExtractor(
+            self.sampling_frequency,
+            self,
+            min_agreement_count=minimum_agreement_count,
+            min_agreement_count_only=minimum_agreement_count_only,
+        )
         return sorting
 
     def save_to_folder(self, save_folder):
         for sorting in self.object_list:
-            assert sorting.check_if_dumpable(
-            ), 'MultiSortingComparison.save_to_folder() need dumpable sortings'
+            assert (
+                sorting.check_if_json_serializable()
+            ), "MultiSortingComparison.save_to_folder() need json serializable sortings"
 
         save_folder = Path(save_folder)
         save_folder.mkdir(parents=True, exist_ok=True)
-        filename = str(save_folder / 'multicomparison.gpickle')
-        with open(filename, 'wb') as f:
+        filename = str(save_folder / "multicomparison.gpickle")
+        with open(filename, "wb") as f:
             pickle.dump(self.graph, f, pickle.HIGHEST_PROTOCOL)
-        kwargs = {'delta_time': float(self.delta_time),
-                  'match_score': float(self.match_score), 'chance_score': float(self.chance_score)}
-        with (save_folder / 'kwargs.json').open('w') as f:
+        kwargs = {
+            "delta_time": float(self.delta_time),
+            "match_score": float(self.match_score),
+            "chance_score": float(self.chance_score),
+        }
+        with (save_folder / "kwargs.json").open("w") as f:
             json.dump(kwargs, f)
         sortings = {}
-        for (name, sorting) in zip(self.name_list, self.object_list):
+        for name, sorting in zip(self.name_list, self.object_list):
             sortings[name] = sorting.to_dict()
-        with (save_folder / 'sortings.json').open('w') as f:
+        with (save_folder / "sortings.json").open("w") as f:
             json.dump(sortings, f)
 
     @staticmethod
     def load_from_folder(folder_path):
         folder_path = Path(folder_path)
-        with (folder_path / 'kwargs.json').open() as f:
+        with (folder_path / "kwargs.json").open() as f:
             kwargs = json.load(f)
-        with (folder_path / 'sortings.json').open() as f:
+        with (folder_path / "sortings.json").open() as f:
             dict_sortings = json.load(f)
         name_list = list(dict_sortings.keys())
         sorting_list = [load_extractor(v) for v in dict_sortings.values()]
-        mcmp = MultiSortingComparison(sorting_list=sorting_list, name_list=list(
-            name_list), do_matching=False, **kwargs)
-        filename = str(folder_path / 'multicomparison.gpickle')
-        with open(filename, 'rb') as f:
+        mcmp = MultiSortingComparison(sorting_list=sorting_list, name_list=list(name_list), do_matching=False, **kwargs)
+        filename = str(folder_path / "multicomparison.gpickle")
+        with open(filename, "rb") as f:
             mcmp.graph = pickle.load(f)
         # do step 3 and 4
         mcmp._clean_graph()
         mcmp._do_agreement()
         mcmp._populate_spiketrains()
         return mcmp
 
 
 class AgreementSortingExtractor(BaseSorting):
-
-    def __init__(self, sampling_frequency, multisortingcomparison,
-                 min_agreement_count=1, min_agreement_count_only=False):
-
+    def __init__(
+        self, sampling_frequency, multisortingcomparison, min_agreement_count=1, min_agreement_count_only=False
+    ):
         self._msc = multisortingcomparison
-        self.is_dumpable = False
+        self._is_json_serializable = False
 
         if min_agreement_count_only:
-            unit_ids = list(u for u in self._msc._new_units.keys()
-                            if self._msc._new_units[u]['agreement_number'] == min_agreement_count)
+            unit_ids = list(
+                u
+                for u in self._msc._new_units.keys()
+                if self._msc._new_units[u]["agreement_number"] == min_agreement_count
+            )
         else:
-            unit_ids = list(u for u in self._msc._new_units.keys()
-                            if self._msc._new_units[u]['agreement_number'] >= min_agreement_count)
+            unit_ids = list(
+                u
+                for u in self._msc._new_units.keys()
+                if self._msc._new_units[u]["agreement_number"] >= min_agreement_count
+            )
 
         BaseSorting.__init__(self, sampling_frequency=sampling_frequency, unit_ids=unit_ids)
 
         if len(unit_ids) > 0:
-            for k in ('agreement_number', 'avg_agreement', 'unit_ids'):
-                values = [self._msc._new_units[unit_id][k]
-                          for unit_id in unit_ids]
+            for k in ("agreement_number", "avg_agreement", "unit_ids"):
+                values = [self._msc._new_units[unit_id][k] for unit_id in unit_ids]
                 self.set_property(k, values, ids=unit_ids)
 
         for segment_index in range(multisortingcomparison._num_segments):
             sorting_segment = AgreementSortingSegment(multisortingcomparison._spiketrains[segment_index])
             self.add_sorting_segment(sorting_segment)
 
+        self._kwargs = dict(
+            sampling_frequency=sampling_frequency,
+            multisortingcomparison=multisortingcomparison,
+            min_agreement_count=min_agreement_count,
+            min_agreement_count_only=min_agreement_count_only,
+        )
+
 
 class AgreementSortingSegment(BaseSortingSegment):
     def __init__(self, spiketrains_segment):
         BaseSortingSegment.__init__(self)
         self.spiketrains = spiketrains_segment
 
     def get_unit_spike_train(self, unit_id, start_frame, end_frame):
@@ -244,16 +272,17 @@
         if start_frame is not None:
             spiketrain = spiketrain[spiketrain >= start_frame]
         if end_frame is not None:
             spiketrain = spiketrain[spiketrain < end_frame]
         return spiketrain
 
 
-compare_multiple_sorters = define_function_from_class(source_class=MultiSortingComparison, 
-                                                      name="compare_multiple_sorters")
+compare_multiple_sorters = define_function_from_class(
+    source_class=MultiSortingComparison, name="compare_multiple_sorters"
+)
 
 
 class MultiTemplateComparison(BaseMultiComparison, MixinTemplateComparison):
     """
     Compares multiple waveform extractors using template similarity.
 
     - Pair-wise comparisons are made
@@ -274,39 +303,55 @@
 
     Returns
     -------
     multi_template_comparison: MultiTemplateComparison
         MultiTemplateComparison object with the multiple template comparisons
     """
 
-    def __init__(self, waveform_list, name_list=None,
-                 match_score=0.8, chance_score=0.3, verbose=False,
-                 similarity_method='cosine_similarity', sparsity_dict=None,
-                 do_matching=True):
+    def __init__(
+        self,
+        waveform_list,
+        name_list=None,
+        match_score=0.8,
+        chance_score=0.3,
+        verbose=False,
+        similarity_method="cosine_similarity",
+        sparsity_dict=None,
+        do_matching=True,
+    ):
         if name_list is None:
             name_list = [f"sess{i}" for i in range(len(waveform_list))]
-        BaseMultiComparison.__init__(self, object_list=waveform_list, name_list=name_list, 
-                                     match_score=match_score, chance_score=chance_score, 
-                                     verbose=verbose)
+        BaseMultiComparison.__init__(
+            self,
+            object_list=waveform_list,
+            name_list=name_list,
+            match_score=match_score,
+            chance_score=chance_score,
+            verbose=verbose,
+        )
         MixinTemplateComparison.__init__(self, similarity_method=similarity_method, sparsity_dict=sparsity_dict)
 
         if do_matching:
             self._compute_all()
 
     def _compare_ij(self, i, j):
-        comp = TemplateComparison(self.object_list[i], self.object_list[j],
-                                  we1_name=self.name_list[i],
-                                  we2_name=self.name_list[j],
-                                  match_score=self.match_score,
-                                  verbose=False)
+        comp = TemplateComparison(
+            self.object_list[i],
+            self.object_list[j],
+            we1_name=self.name_list[i],
+            we2_name=self.name_list[j],
+            match_score=self.match_score,
+            verbose=False,
+        )
         return comp
 
     def _populate_nodes(self):
         for i, we in enumerate(self.object_list):
             session_name = self.name_list[i]
             for unit_id in we.unit_ids:
                 node = session_name, unit_id
                 self.graph.add_node(node)
 
 
-compare_multiple_templates = define_function_from_class(source_class=MultiTemplateComparison, 
-                                                        name="compare_multiple_templates")
+compare_multiple_templates = define_function_from_class(
+    source_class=MultiTemplateComparison, name="compare_multiple_templates"
+)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/comparison/paircomparisons.py` & `spikeinterface-0.98.0/src/spikeinterface/comparison/paircomparisons.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,43 +1,63 @@
 import numpy as np
-import pandas as pd
 
 from spikeinterface.core.core_tools import define_function_from_class
 from .basecomparison import BasePairComparison, MixinSpikeTrainComparison, MixinTemplateComparison
-from .comparisontools import (do_count_event, make_match_count_matrix, 
-                              make_agreement_scores_from_count, do_score_labels, do_confusion_matrix, 
-                              do_count_score, compute_performance)
+from .comparisontools import (
+    do_count_event,
+    make_match_count_matrix,
+    make_agreement_scores_from_count,
+    do_score_labels,
+    do_confusion_matrix,
+    do_count_score,
+    compute_performance,
+)
 from ..postprocessing import compute_template_similarity
 
 
 class BasePairSorterComparison(BasePairComparison, MixinSpikeTrainComparison):
     """
     Base class shared by SymmetricSortingComparison and GroundTruthComparison
     """
 
-    def __init__(self, sorting1, sorting2, sorting1_name=None, sorting2_name=None,
-                 delta_time=0.4, match_score=0.5, chance_score=0.1, n_jobs=1, 
-                 verbose=False):
+    def __init__(
+        self,
+        sorting1,
+        sorting2,
+        sorting1_name=None,
+        sorting2_name=None,
+        delta_time=0.4,
+        match_score=0.5,
+        chance_score=0.1,
+        n_jobs=1,
+        verbose=False,
+    ):
         if sorting1_name is None:
-            sorting1_name = 'sorting1'
+            sorting1_name = "sorting1"
         if sorting2_name is None:
-            sorting2_name = 'sorting2'
-        assert sorting1.get_num_segments() == sorting2.get_num_segments(), ("The two sortings must have the same "
-                                                                            "number of segments! ")
-
-        BasePairComparison.__init__(self, object1=sorting1, object2=sorting2, 
-                                    name1=sorting1_name, name2=sorting2_name,
-                                    match_score=match_score, chance_score=chance_score, 
-                                    verbose=verbose)
+            sorting2_name = "sorting2"
+        assert sorting1.get_num_segments() == sorting2.get_num_segments(), (
+            "The two sortings must have the same " "number of segments! "
+        )
+
+        BasePairComparison.__init__(
+            self,
+            object1=sorting1,
+            object2=sorting2,
+            name1=sorting1_name,
+            name2=sorting2_name,
+            match_score=match_score,
+            chance_score=chance_score,
+            verbose=verbose,
+        )
         MixinSpikeTrainComparison.__init__(self, delta_time=delta_time, n_jobs=n_jobs)
         self.set_frames_and_frequency(self.object_list)
 
         self.unit1_ids = self.sorting1.get_unit_ids()
         self.unit2_ids = self.sorting2.get_unit_ids()
-        
 
         self._do_agreement()
         self._do_matching()
 
     @property
     def sorting1(self):
         return self.object_list[0]
@@ -52,28 +72,30 @@
 
     @property
     def sorting2_name(self):
         return self.name_list[1]
 
     def _do_agreement(self):
         if self._verbose:
-            print('Agreement scores...')
+            print("Agreement scores...")
 
         # common to GroundTruthComparison and SymmetricSortingComparison
         # spike count for each spike train
         self.event_counts1 = do_count_event(self.sorting1)
         self.event_counts2 = do_count_event(self.sorting2)
 
         # matrix of  event match count for each pair
-        self.match_event_count = make_match_count_matrix(self.sorting1, self.sorting2, self.delta_frames,
-                                                         n_jobs=self.n_jobs)
+        self.match_event_count = make_match_count_matrix(
+            self.sorting1, self.sorting2, self.delta_frames, n_jobs=self.n_jobs
+        )
 
         # agreement matrix score for each pair
-        self.agreement_scores = make_agreement_scores_from_count(self.match_event_count, self.event_counts1,
-                                                                 self.event_counts2)
+        self.agreement_scores = make_agreement_scores_from_count(
+            self.match_event_count, self.event_counts1, self.event_counts2
+        )
 
 
 class SymmetricSortingComparison(BasePairSorterComparison):
     """
     Compares two spike sorter outputs.
 
     - Spike trains are matched based on their agreement scores
@@ -106,32 +128,48 @@
 
     Returns
     -------
     sorting_comparison: SortingComparison
         The SortingComparison object
     """
 
-    def __init__(self, sorting1, sorting2, sorting1_name=None, sorting2_name=None,
-                 delta_time=0.4, sampling_frequency=None, match_score=0.5, chance_score=0.1,
-                 n_jobs=-1, verbose=False):
-        BasePairSorterComparison.__init__(self, sorting1, sorting2, sorting1_name=sorting1_name,
-                                          sorting2_name=sorting2_name,
-                                          delta_time=delta_time,
-                                          match_score=match_score, chance_score=chance_score,
-                                          n_jobs=n_jobs, verbose=verbose)
+    def __init__(
+        self,
+        sorting1,
+        sorting2,
+        sorting1_name=None,
+        sorting2_name=None,
+        delta_time=0.4,
+        sampling_frequency=None,
+        match_score=0.5,
+        chance_score=0.1,
+        n_jobs=-1,
+        verbose=False,
+    ):
+        BasePairSorterComparison.__init__(
+            self,
+            sorting1,
+            sorting2,
+            sorting1_name=sorting1_name,
+            sorting2_name=sorting2_name,
+            delta_time=delta_time,
+            match_score=match_score,
+            chance_score=chance_score,
+            n_jobs=n_jobs,
+            verbose=verbose,
+        )
 
     def get_matching(self):
         return self.hungarian_match_12, self.hungarian_match_21
 
     def get_matching_event_count(self, unit1, unit2):
         if (unit1 is not None) and (unit2 is not None):
             return self.match_event_count.at[unit1, unit2]
         else:
-            raise Exception(
-                'get_matching_event_count: unit1 and unit2 must not be None.')
+            raise Exception("get_matching_event_count: unit1 and unit2 must not be None.")
 
     def get_best_unit_match1(self, unit1):
         return self.best_match_12[unit1]
 
     def get_best_unit_match2(self, unit2):
         return self.best_match_21[unit2]
 
@@ -208,35 +246,60 @@
 
     Returns
     -------
     sorting_comparison: SortingComparison
         The SortingComparison object
     """
 
-    def __init__(self, gt_sorting, tested_sorting, gt_name=None, tested_name=None,
-                 delta_time=0.4, sampling_frequency=None, match_score=0.5, well_detected_score=0.8,
-                 redundant_score=0.2, overmerged_score=0.2, chance_score=0.1, exhaustive_gt=False, n_jobs=-1,
-                 match_mode='hungarian', compute_labels=False, compute_misclassifications=False, verbose=False):
+    def __init__(
+        self,
+        gt_sorting,
+        tested_sorting,
+        gt_name=None,
+        tested_name=None,
+        delta_time=0.4,
+        sampling_frequency=None,
+        match_score=0.5,
+        well_detected_score=0.8,
+        redundant_score=0.2,
+        overmerged_score=0.2,
+        chance_score=0.1,
+        exhaustive_gt=False,
+        n_jobs=-1,
+        match_mode="hungarian",
+        compute_labels=False,
+        compute_misclassifications=False,
+        verbose=False,
+    ):
+        import pandas as pd
 
         if gt_name is None:
-            gt_name = 'ground truth'
+            gt_name = "ground truth"
         if tested_name is None:
-            tested_name = 'tested'
-        BasePairSorterComparison.__init__(self, gt_sorting, tested_sorting, sorting1_name=gt_name,
-                                          sorting2_name=tested_name, delta_time=delta_time,
-                                          match_score=match_score, chance_score=chance_score, 
-                                          n_jobs=n_jobs, verbose=verbose)
+            tested_name = "tested"
+        BasePairSorterComparison.__init__(
+            self,
+            gt_sorting,
+            tested_sorting,
+            sorting1_name=gt_name,
+            sorting2_name=tested_name,
+            delta_time=delta_time,
+            match_score=match_score,
+            chance_score=chance_score,
+            n_jobs=n_jobs,
+            verbose=verbose,
+        )
         self.exhaustive_gt = exhaustive_gt
 
         self._compute_misclassifications = compute_misclassifications
         self.redundant_score = redundant_score
         self.overmerged_score = overmerged_score
         self.well_detected_score = well_detected_score
 
-        assert match_mode in ['hungarian', 'best']
+        assert match_mode in ["hungarian", "best"]
         self.match_mode = match_mode
         self._compute_labels = compute_labels
 
         self._do_count()
 
         self._labels_st1 = None
         self._labels_st2 = None
@@ -266,33 +329,33 @@
 
     def _do_count(self):
         """
         Do raw count into a dataframe.
 
         Internally use hungarian match or best match.
         """
-        if self.match_mode == 'hungarian':
+        if self.match_mode == "hungarian":
             match_12 = self.hungarian_match_12
-        elif self.match_mode == 'best':
+        elif self.match_mode == "best":
             match_12 = self.best_match_12
 
-        self.count_score = do_count_score(self.event_counts1, self.event_counts2,
-                                          match_12, self.match_event_count)
+        self.count_score = do_count_score(self.event_counts1, self.event_counts2, match_12, self.match_event_count)
 
     def _do_confusion_matrix(self):
         if self._verbose:
             print("Computing confusion matrix...")
 
-        if self.match_mode == 'hungarian':
+        if self.match_mode == "hungarian":
             match_12 = self.hungarian_match_12
-        elif self.match_mode == 'best':
+        elif self.match_mode == "best":
             match_12 = self.best_match_12
 
-        self._confusion_matrix = do_confusion_matrix(self.event_counts1, self.event_counts2, match_12,
-                                                     self.match_event_count)
+        self._confusion_matrix = do_confusion_matrix(
+            self.event_counts1, self.event_counts2, match_12, self.match_event_count
+        )
 
     def get_confusion_matrix(self):
         """
         Computes the confusion matrix.
 
         Returns
         -------
@@ -300,25 +363,24 @@
             The confusion matrix
         """
         if self._confusion_matrix is None:
             self._do_confusion_matrix()
         return self._confusion_matrix
 
     def _do_score_labels(self):
-        assert self.match_mode == 'hungarian', \
-            'Labels (TP, FP, FN) can be computed only with hungarian match'
+        assert self.match_mode == "hungarian", "Labels (TP, FP, FN) can be computed only with hungarian match"
 
         if self._verbose:
             print("Adding labels...")
 
-        self._labels_st1, self._labels_st2 = do_score_labels(self.sorting1, self.sorting2,
-                                                             self.delta_frames, self.hungarian_match_12,
-                                                             self._compute_misclassifications)
+        self._labels_st1, self._labels_st2 = do_score_labels(
+            self.sorting1, self.sorting2, self.delta_frames, self.hungarian_match_12, self._compute_misclassifications
+        )
 
-    def get_performance(self, method='by_unit', output='pandas'):
+    def get_performance(self, method="by_unit", output="pandas"):
         """
         Get performance rate with several method:
           * 'raw_count' : just render the raw count table
           * 'by_unit' : render perf as rate unit by unit of the GT
           * 'pooled_with_average' : compute rate unit by unit and average
 
         Parameters
@@ -329,51 +391,50 @@
             'pandas' or 'dict'
 
         Returns
         -------
         perf: pandas dataframe/series (or dict)
             dataframe/series (based on 'output') with performance entries
         """
-        possibles = ('raw_count', 'by_unit', 'pooled_with_average')
+        possibles = ("raw_count", "by_unit", "pooled_with_average")
         if method not in possibles:
-            raise Exception("'method' can be " + ' or '.join(possibles))
+            raise Exception("'method' can be " + " or ".join(possibles))
 
-        if method == 'raw_count':
+        if method == "raw_count":
             perf = self.count_score
 
-        elif method == 'by_unit':
+        elif method == "by_unit":
             perf = compute_performance(self.count_score)
 
-        elif method == 'pooled_with_average':
-            perf = self.get_performance(method='by_unit').mean(axis=0)
+        elif method == "pooled_with_average":
+            perf = self.get_performance(method="by_unit").mean(axis=0)
 
-        if output == 'dict' and isinstance(perf, pd.Series):
+        if output == "dict" and isinstance(perf, pd.Series):
             perf = perf.to_dict()
 
         return perf
 
-    def print_performance(self, method='pooled_with_average'):
+    def print_performance(self, method="pooled_with_average"):
         """
         Print performance with the selected method
         """
 
         template_txt_performance = _template_txt_performance
 
-        if method == 'by_unit':
-            perf = self.get_performance(method=method, output='pandas')
+        if method == "by_unit":
+            perf = self.get_performance(method=method, output="pandas")
             perf = perf * 100
             d = {k: perf[k].tolist() for k in perf.columns}
             txt = template_txt_performance.format(method=method, **d)
             print(txt)
 
-        elif method == 'pooled_with_average':
-            perf = self.get_performance(method=method, output='pandas')
+        elif method == "pooled_with_average":
+            perf = self.get_performance(method=method, output="pandas")
             perf = perf * 100
-            txt = template_txt_performance.format(
-                method=method, **perf.to_dict())
+            txt = template_txt_performance.format(method=method, **perf.to_dict())
             print(txt)
 
     def print_summary(self, well_detected_score=None, redundant_score=None, overmerged_score=None):
         """
         Print a global performance summary that depend on the context:
           * exhaustive= True/False
           * how many gt units (one or several)
@@ -381,24 +442,23 @@
         This summary mix several performance metrics.
         """
         txt = _template_summary_part1
 
         d = dict(
             num_gt=len(self.unit1_ids),
             num_tested=len(self.unit2_ids),
-            num_well_detected=self.count_well_detected_units(
-                well_detected_score),
+            num_well_detected=self.count_well_detected_units(well_detected_score),
             num_redundant=self.count_redundant_units(redundant_score),
             num_overmerged=self.count_overmerged_units(overmerged_score),
         )
 
         if self.exhaustive_gt:
             txt = txt + _template_summary_part2
-            d['num_false_positive_units'] = self.count_false_positive_units()
-            d['num_bad'] = self.count_bad_units()
+            d["num_false_positive_units"] = self.count_false_positive_units()
+            d["num_bad"] = self.count_bad_units()
 
         txt = txt.format(**d)
 
         print(txt)
 
     def get_well_detected_units(self, well_detected_score=None):
         """
@@ -445,15 +505,15 @@
 
         Parameters
         ----------
         redundant_score: float (default 0.2)
             The agreement score below which tested units
             are counted as "false positive"" (and not "redundant").
         """
-        assert self.exhaustive_gt, 'false_positive_units list is valid only if exhaustive_gt=True'
+        assert self.exhaustive_gt, "false_positive_units list is valid only if exhaustive_gt=True"
 
         if redundant_score is not None:
             self.redundant_score = redundant_score
 
         matched_units2 = list(self.hungarian_match_12.values)
         false_positive_ids = []
         for u2 in self.unit2_ids:
@@ -485,15 +545,15 @@
 
         Parameters
         ----------
         redundant_score=None: float (default 0.2)
             The agreement score above which tested units
             are counted as "redundant" (and not "false positive" ).
         """
-        assert self.exhaustive_gt, 'redundant_units list is valid only if exhaustive_gt=True'
+        assert self.exhaustive_gt, "redundant_units list is valid only if exhaustive_gt=True"
 
         if redundant_score is not None:
             self.redundant_score = redundant_score
         matched_units2 = list(self.hungarian_match_12.values)
         redundant_ids = []
         for u2 in self.unit2_ids:
             if u2 not in matched_units2 and self.best_match_21[u2] != -1:
@@ -520,15 +580,15 @@
 
         Parameters
         ----------
         overmerged_score: float (default 0.4)
             Tested units with 2 or more agreement scores above 'overmerged_score'
             are counted as "overmerged".
         """
-        assert self.exhaustive_gt, 'overmerged_units list is valid only if exhaustive_gt=True'
+        assert self.exhaustive_gt, "overmerged_units list is valid only if exhaustive_gt=True"
 
         if overmerged_score is not None:
             self.overmerged_score = overmerged_score
 
         overmerged_ids = []
         for u2 in self.unit2_ids:
             scores = self.agreement_scores.loc[:, u2]
@@ -550,44 +610,50 @@
         "bad units" are defined as units in tested that are not
         in the best match list of GT units.
 
         So it is the union of "false positive units" + "redundant units".
 
         Need exhaustive_gt=True
         """
-        assert self.exhaustive_gt, 'bad_units list is valid only if exhaustive_gt=True'
+        assert self.exhaustive_gt, "bad_units list is valid only if exhaustive_gt=True"
         matched_units2 = list(self.hungarian_match_12.values)
         bad_ids = []
         for u2 in self.unit2_ids:
             if u2 not in matched_units2:
                 bad_ids.append(u2)
         return bad_ids
 
     def count_bad_units(self):
         """
         See get_bad_units
         """
         return len(self.get_bad_units())
 
-    def count_units_categories(self, well_detected_score=None, overmerged_score=None, redundant_score=None, ):
-        count = pd.Series(dtype='int64')
-
-        count['num_gt'] = len(self.sorting1.get_unit_ids())
-        count['num_sorter'] = len(self.sorting2.get_unit_ids())
-        count['num_well_detected'] = self.count_well_detected_units(well_detected_score)
+    def count_units_categories(
+        self,
+        well_detected_score=None,
+        overmerged_score=None,
+        redundant_score=None,
+    ):
+        import pandas as pd
+
+        count = pd.Series(dtype="int64")
+
+        count["num_gt"] = len(self.sorting1.get_unit_ids())
+        count["num_sorter"] = len(self.sorting2.get_unit_ids())
+        count["num_well_detected"] = self.count_well_detected_units(well_detected_score)
         if self.exhaustive_gt:
-            count['num_overmerged'] = self.count_overmerged_units(overmerged_score)
-            count['num_redundant'] = self.count_redundant_units(redundant_score)
-            count['num_false_positive'] = self.count_false_positive_units(redundant_score)
-            count['num_bad'] = self.count_bad_units()
+            count["num_overmerged"] = self.count_overmerged_units(overmerged_score)
+            count["num_redundant"] = self.count_redundant_units(redundant_score)
+            count["num_false_positive"] = self.count_false_positive_units(redundant_score)
+            count["num_bad"] = self.count_bad_units()
 
         return count
 
 
-
 # usefull also for gathercomparison
 
 
 _template_txt_performance = """PERFORMANCE ({method})
 -----------
 ACCURACY: {accuracy}
 RECALL: {recall}
@@ -606,16 +672,17 @@
 """
 
 _template_summary_part2 = """num_false_positive_units {num_false_positive_units}
 num_bad: {num_bad}
 """
 
 
-compare_sorter_to_ground_truth = define_function_from_class(source_class=GroundTruthComparison, 
-                                                            name="compare_sorter_to_ground_truth")
+compare_sorter_to_ground_truth = define_function_from_class(
+    source_class=GroundTruthComparison, name="compare_sorter_to_ground_truth"
+)
 
 
 class TemplateComparison(BasePairComparison, MixinTemplateComparison):
     """
     Compares units from different sessions based on template similarity
 
     Parameters
@@ -636,27 +703,43 @@
         If True, output is verbose, by default False
 
     Returns
     -------
     comparison : TemplateComparison
         The output TemplateComparison object
     """
-    def __init__(self, we1, we2, we1_name=None, we2_name=None,
-                 unit_ids1=None, unit_ids2=None,
-                 match_score=0.7, chance_score=0.3,
-                 similarity_method="cosine_similarity", sparsity_dict=None,
-                 verbose=False):
+
+    def __init__(
+        self,
+        we1,
+        we2,
+        we1_name=None,
+        we2_name=None,
+        unit_ids1=None,
+        unit_ids2=None,
+        match_score=0.7,
+        chance_score=0.3,
+        similarity_method="cosine_similarity",
+        sparsity_dict=None,
+        verbose=False,
+    ):
         if we1_name is None:
             we1_name = "sess1"
         if we2_name is None:
             we2_name = "sess2"
-        BasePairComparison.__init__(self, object1=we1, object2=we2,
-                                    name1=we1_name, name2=we2_name,
-                                    match_score=match_score, chance_score=chance_score,
-                                    verbose=verbose)
+        BasePairComparison.__init__(
+            self,
+            object1=we1,
+            object2=we2,
+            name1=we1_name,
+            name2=we2_name,
+            match_score=match_score,
+            chance_score=chance_score,
+            verbose=verbose,
+        )
         MixinTemplateComparison.__init__(self, similarity_method=similarity_method, sparsity_dict=sparsity_dict)
 
         self.we1 = we1
         self.we2 = we2
         channel_ids1 = we1.recording.get_channel_ids()
         channel_ids2 = we2.recording.get_channel_ids()
 
@@ -682,17 +765,18 @@
             self.sparsity = None
 
         self._do_agreement()
         self._do_matching()
 
     def _do_agreement(self):
         if self._verbose:
-            print('Agreement scores...')
+            print("Agreement scores...")
+
+        agreement_scores = compute_template_similarity(
+            self.we1, waveform_extractor_other=self.we2, method=self.similarity_method
+        )
+        import pandas as pd
 
-        agreement_scores = compute_template_similarity(self.we1, self.we2,
-                                                       method=self.similarity_method)
-        self.agreement_scores = pd.DataFrame(agreement_scores,
-                                             index=self.unit_ids[0],
-                                             columns=self.unit_ids[1])
+        self.agreement_scores = pd.DataFrame(agreement_scores, index=self.unit_ids[0], columns=self.unit_ids[1])
 
 
 compare_templates = define_function_from_class(source_class=TemplateComparison, name="compare_templates")
```

### Comparing `spikeinterface-0.97.1/spikeinterface/comparison/studytools.py` & `spikeinterface-0.98.0/src/spikeinterface/comparison/studytools.py`

 * *Files 8% similar despite different names*

```diff
@@ -13,15 +13,14 @@
 """
 
 from pathlib import Path
 import shutil
 import json
 import os
 
-import pandas as pd
 
 from spikeinterface.core import load_extractor
 from spikeinterface.core.job_tools import fix_job_kwargs
 from spikeinterface.extractors import NpzSortingExtractor
 from spikeinterface.sorters import sorter_dict
 from spikeinterface.sorters.launcher import iter_working_folder, iter_sorting_output
 
@@ -41,31 +40,31 @@
         Dict of tuple that contain recording and sorting ground truth
     """
     job_kwargs = fix_job_kwargs(job_kwargs)
     study_folder = Path(study_folder)
     assert not study_folder.is_dir(), "'study_folder' already exists. Please remove it"
 
     study_folder.mkdir(parents=True, exist_ok=True)
-    sorting_folders = study_folder / 'sortings'
-    log_folder = sorting_folders / 'run_log'
+    sorting_folders = study_folder / "sortings"
+    log_folder = sorting_folders / "run_log"
     log_folder.mkdir(parents=True, exist_ok=True)
-    tables_folder = study_folder / 'tables'
+    tables_folder = study_folder / "tables"
     tables_folder.mkdir(parents=True, exist_ok=True)
 
     for rec_name, (recording, sorting_gt) in gt_dict.items():
         # write recording using save with binary
-        folder = study_folder / 'ground_truth' / rec_name
-        sorting_gt.save(folder=folder, format='npz')
-        folder = study_folder / 'raw_files' / rec_name
-        recording.save(folder=folder, format='binary', **job_kwargs)
+        folder = study_folder / "ground_truth" / rec_name
+        sorting_gt.save(folder=folder, format="npz")
+        folder = study_folder / "raw_files" / rec_name
+        recording.save(folder=folder, format="binary", **job_kwargs)
 
     # make an index of recording names
-    with open(study_folder / 'names.txt', mode='w', encoding='utf8') as f:
+    with open(study_folder / "names.txt", mode="w", encoding="utf8") as f:
         for rec_name in gt_dict:
-            f.write(rec_name + '\n')
+            f.write(rec_name + "\n")
 
 
 def get_rec_names(study_folder):
     """
     Get list of keys of recordings.
     Read from the 'names.txt' file in study folder.
 
@@ -76,16 +75,16 @@
 
     Returns
     -------
     rec_names: list
         List of names.
     """
     study_folder = Path(study_folder)
-    with open(study_folder / 'names.txt', mode='r', encoding='utf8') as f:
-        rec_names = f.read()[:-1].split('\n')
+    with open(study_folder / "names.txt", mode="r", encoding="utf8") as f:
+        rec_names = f.read()[:-1].split("\n")
     return rec_names
 
 
 def get_recordings(study_folder):
     """
     Get ground recording as a dict.
 
@@ -102,15 +101,15 @@
         Dict of recording.
     """
     study_folder = Path(study_folder)
 
     rec_names = get_rec_names(study_folder)
     recording_dict = {}
     for rec_name in rec_names:
-        rec = load_extractor(study_folder / 'raw_files' / rec_name)
+        rec = load_extractor(study_folder / "raw_files" / rec_name)
         recording_dict[rec_name] = rec
 
     return recording_dict
 
 
 def get_ground_truths(study_folder):
     """
@@ -128,63 +127,65 @@
     ground_truths: dict
         Dict of sorting_gt.
     """
     study_folder = Path(study_folder)
     rec_names = get_rec_names(study_folder)
     ground_truths = {}
     for rec_name in rec_names:
-        sorting = load_extractor(study_folder / 'ground_truth' / rec_name)
+        sorting = load_extractor(study_folder / "ground_truth" / rec_name)
         ground_truths[rec_name] = sorting
     return ground_truths
 
 
 def iter_computed_names(study_folder):
-    sorting_folder = Path(study_folder) / 'sortings'
+    sorting_folder = Path(study_folder) / "sortings"
     for filename in os.listdir(sorting_folder):
-        if filename.endswith('.npz') and '[#]' in filename:
-            rec_name, sorter_name = filename.replace('.npz', '').split('[#]')
+        if filename.endswith(".npz") and "[#]" in filename:
+            rec_name, sorter_name = filename.replace(".npz", "").split("[#]")
             yield rec_name, sorter_name
 
 
 def iter_computed_sorting(study_folder):
     """
     Iter over sorting files.
     """
-    sorting_folder = Path(study_folder) / 'sortings'
+    sorting_folder = Path(study_folder) / "sortings"
     for filename in os.listdir(sorting_folder):
-        if filename.endswith('.npz') and '[#]' in filename:
-            rec_name, sorter_name = filename.replace('.npz', '').split('[#]')
+        if filename.endswith(".npz") and "[#]" in filename:
+            rec_name, sorter_name = filename.replace(".npz", "").split("[#]")
             sorting = NpzSortingExtractor(sorting_folder / filename)
             yield rec_name, sorter_name, sorting
 
 
 def collect_run_times(study_folder):
     """
     Collect run times in a working folder and store it in CVS files.
 
     The output is list of (rec_name, sorter_name, run_time)
     """
+    import pandas as pd
+
     study_folder = Path(study_folder)
-    sorting_folders = study_folder / 'sortings'
-    log_folder = sorting_folders / 'run_log'
-    tables_folder = study_folder / 'tables'
+    sorting_folders = study_folder / "sortings"
+    log_folder = sorting_folders / "run_log"
+    tables_folder = study_folder / "tables"
 
     tables_folder.mkdir(parents=True, exist_ok=True)
 
     run_times = []
     for filename in os.listdir(log_folder):
-        if filename.endswith('.json') and '[#]' in filename:
-            rec_name, sorter_name = filename.replace('.json', '').split('[#]')
-            with open(log_folder / filename, encoding='utf8', mode='r') as logfile:
+        if filename.endswith(".json") and "[#]" in filename:
+            rec_name, sorter_name = filename.replace(".json", "").split("[#]")
+            with open(log_folder / filename, encoding="utf8", mode="r") as logfile:
                 log = json.load(logfile)
-                run_time = log.get('run_time', None)
+                run_time = log.get("run_time", None)
             run_times.append((rec_name, sorter_name, run_time))
 
-    run_times = pd.DataFrame(run_times, columns=['rec_name', 'sorter_name', 'run_time'])
-    run_times = run_times.set_index(['rec_name', 'sorter_name'])
+    run_times = pd.DataFrame(run_times, columns=["rec_name", "sorter_name", "run_time"])
+    run_times = run_times.set_index(["rec_name", "sorter_name"])
 
     return run_times
 
 
 def aggregate_sorting_comparison(study_folder, exhaustive_gt=False):
     """
     Loop over output folder in a tree to collect sorting output and run
@@ -238,70 +239,78 @@
 
     Returns
     -------
     dataframes: a dict of DataFrame
         Return several useful DataFrame to compare all results.
         Note that count_units depend on karg_thresh.
     """
+    import pandas as pd
+
     study_folder = Path(study_folder)
-    sorter_folders = study_folder / 'sorter_folders'
-    tables_folder = study_folder / 'tables'
+    sorter_folders = study_folder / "sorter_folders"
+    tables_folder = study_folder / "tables"
 
     comparisons = aggregate_sorting_comparison(study_folder, exhaustive_gt=exhaustive_gt)
     ground_truths = get_ground_truths(study_folder)
     results = collect_study_sorting(study_folder)
 
     study_folder = Path(study_folder)
 
     dataframes = {}
 
     # get run times:
-    run_times = pd.read_csv(str(tables_folder / 'run_times.csv'), sep='\t')
-    run_times.columns = ['rec_name', 'sorter_name', 'run_time']
-    run_times = run_times.set_index(['rec_name', 'sorter_name', ])
-    dataframes['run_times'] = run_times
+    run_times = pd.read_csv(str(tables_folder / "run_times.csv"), sep="\t")
+    run_times.columns = ["rec_name", "sorter_name", "run_time"]
+    run_times = run_times.set_index(
+        [
+            "rec_name",
+            "sorter_name",
+        ]
+    )
+    dataframes["run_times"] = run_times
 
     perf_pooled_with_sum = pd.DataFrame(index=run_times.index, columns=_perf_keys)
-    dataframes['perf_pooled_with_sum'] = perf_pooled_with_sum
+    dataframes["perf_pooled_with_sum"] = perf_pooled_with_sum
 
     perf_pooled_with_average = pd.DataFrame(index=run_times.index, columns=_perf_keys)
-    dataframes['perf_pooled_with_average'] = perf_pooled_with_average
+    dataframes["perf_pooled_with_average"] = perf_pooled_with_average
 
-    count_units = pd.DataFrame(index=run_times.index,
-                               columns=['num_gt', 'num_sorter', 'num_well_detected', 'num_redundant'])
-    dataframes['count_units'] = count_units
+    count_units = pd.DataFrame(
+        index=run_times.index, columns=["num_gt", "num_sorter", "num_well_detected", "num_redundant"]
+    )
+    dataframes["count_units"] = count_units
     if exhaustive_gt:
-        count_units['num_false_positive'] = None
-        count_units['num_bad'] = None
+        count_units["num_false_positive"] = None
+        count_units["num_bad"] = None
 
     perf_by_spiketrain = []
 
     for (rec_name, sorter_name), comp in comparisons.items():
         gt_sorting = ground_truths[rec_name]
         sorting = results[(rec_name, sorter_name)]
 
-        perf = comp.get_performance(method='pooled_with_sum', output='pandas')
+        perf = comp.get_performance(method="pooled_with_sum", output="pandas")
         perf_pooled_with_sum.loc[(rec_name, sorter_name), :] = perf
 
-        perf = comp.get_performance(method='pooled_with_average', output='pandas')
+        perf = comp.get_performance(method="pooled_with_average", output="pandas")
         perf_pooled_with_average.loc[(rec_name, sorter_name), :] = perf
 
-        perf = comp.get_performance(method='by_spiketrain', output='pandas')
-        perf['rec_name'] = rec_name
-        perf['sorter_name'] = sorter_name
+        perf = comp.get_performance(method="by_spiketrain", output="pandas")
+        perf["rec_name"] = rec_name
+        perf["sorter_name"] = sorter_name
         perf = perf.reset_index()
 
         perf_by_spiketrain.append(perf)
 
-        count_units.loc[(rec_name, sorter_name), 'num_gt'] = len(gt_sorting.get_unit_ids())
-        count_units.loc[(rec_name, sorter_name), 'num_sorter'] = len(sorting.get_unit_ids())
-        count_units.loc[(rec_name, sorter_name), 'num_well_detected'] = comp.count_well_detected_units(**karg_thresh)
-        count_units.loc[(rec_name, sorter_name), 'num_redundant'] = comp.count_redundant_units()
+        count_units.loc[(rec_name, sorter_name), "num_gt"] = len(gt_sorting.get_unit_ids())
+        count_units.loc[(rec_name, sorter_name), "num_sorter"] = len(sorting.get_unit_ids())
+        count_units.loc[(rec_name, sorter_name), "num_well_detected"] = comp.count_well_detected_units(**karg_thresh)
+        count_units.loc[(rec_name, sorter_name), "num_redundant"] = comp.count_redundant_units()
         if exhaustive_gt:
-            count_units.loc[(rec_name, sorter_name), 'num_false_positive'] = comp.count_false_positive_units()
-            count_units.loc[(rec_name, sorter_name), 'num_bad'] = comp.count_bad_units()
+            count_units.loc[(rec_name, sorter_name), "num_false_positive"] = comp.count_false_positive_units()
+            count_units.loc[(rec_name, sorter_name), "num_bad"] = comp.count_bad_units()
 
     perf_by_spiketrain = pd.concat(perf_by_spiketrain)
-    perf_by_spiketrain = perf_by_spiketrain.set_index(['rec_name', 'sorter_name', 'gt_unit_id'])
-    dataframes['perf_by_spiketrain'] = perf_by_spiketrain
+    perf_by_spiketrain = perf_by_spiketrain.set_index(["rec_name", "sorter_name", "gt_unit_id"])
+    dataframes["perf_by_spiketrain"] = perf_by_spiketrain
 
     return dataframes
```

### Comparing `spikeinterface-0.97.1/spikeinterface/core/__init__.py` & `spikeinterface-0.98.0/src/spikeinterface/core/__init__.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,8 +1,7 @@
-
 from .base import load_extractor  # , load_extractor_from_dict, load_extractor_from_json, load_extractor_from_pickle
 from .baserecording import BaseRecording, BaseRecordingSegment
 from .basesorting import BaseSorting, BaseSortingSegment
 from .baseevent import BaseEvent, BaseEventSegment
 from .basesnippets import BaseSnippets, BaseSnippetsSegment
 from .baserecordingsnippets import BaseRecordingSnippets
 
@@ -22,69 +21,104 @@
 from .frameslicerecording import FrameSliceRecording
 from .frameslicesorting import FrameSliceSorting
 
 from .channelsaggregationrecording import ChannelsAggregationRecording, aggregate_channels
 from .unitsaggregationsorting import UnitsAggregationSorting, aggregate_units
 
 # generator of simple object for testing or examples
-from .generate import (generate_recording, generate_sorting,
-  create_sorting_npz, generate_snippets,
-  synthesize_random_firings,  inject_some_duplicate_units,
-  inject_some_split_units, synthetize_spike_train_bad_isi)
+from .generate import (
+    generate_recording,
+    generate_sorting,
+    create_sorting_npz,
+    generate_snippets,
+    synthesize_random_firings,
+    inject_some_duplicate_units,
+    inject_some_split_units,
+    synthetize_spike_train_bad_isi,
+)
 
 # utils to append and concatenate segment (equivalent to OLD MultiRecordingTimeExtractor)
 from .segmentutils import (
     append_recordings,
     AppendSegmentRecording,
     concatenate_recordings,
     ConcatenateSegmentRecording,
     split_recording,
     select_segment_recording,
     SelectSegmentRecording,
     append_sortings,
     AppendSegmentSorting,
+    concatenate_sortings,
+    ConcatenateSegmentSorting,
     split_sorting,
     SplitSegmentSorting,
     select_segment_sorting,
-    SelectSegmentSorting
+    SelectSegmentSorting,
 )
 
 # default folder
-from .globals import (set_global_tmp_folder, get_global_tmp_folder,
-                      is_set_global_tmp_folder, reset_global_tmp_folder,
-                      get_global_dataset_folder, set_global_dataset_folder,
-                      is_set_global_dataset_folder,
-                      get_global_job_kwargs, set_global_job_kwargs, reset_global_job_kwargs)
-
-# tools 
-from .core_tools import write_binary_recording, write_to_h5_dataset_format, write_binary_recording, read_python, \
-    write_python
+from .globals import (
+    set_global_tmp_folder,
+    get_global_tmp_folder,
+    is_set_global_tmp_folder,
+    reset_global_tmp_folder,
+    get_global_dataset_folder,
+    set_global_dataset_folder,
+    is_set_global_dataset_folder,
+    get_global_job_kwargs,
+    set_global_job_kwargs,
+    reset_global_job_kwargs,
+)
+
+# tools
+from .core_tools import (
+    write_binary_recording,
+    write_to_h5_dataset_format,
+    write_binary_recording,
+    read_python,
+    write_python,
+)
 from .job_tools import ensure_n_jobs, ensure_chunk_size, ChunkRecordingExecutor, split_job_kwargs, fix_job_kwargs
-from .recording_tools import (get_random_data_chunks, get_channel_distances, get_closest_channels,
-                              get_noise_levels, get_chunk_with_margin, order_channels_by_depth)
+from .recording_tools import (
+    get_random_data_chunks,
+    get_channel_distances,
+    get_closest_channels,
+    get_noise_levels,
+    get_chunk_with_margin,
+    order_channels_by_depth,
+)
 from .waveform_tools import extract_waveforms_to_buffers
 from .snippets_tools import snippets_from_sorting
 
 # waveform extractor
-from .waveform_extractor import (WaveformExtractor, BaseWaveformExtractorExtension,
-                                 extract_waveforms, load_waveforms, precompute_sparsity)
+from .waveform_extractor import (
+    WaveformExtractor,
+    BaseWaveformExtractorExtension,
+    extract_waveforms,
+    load_waveforms,
+    precompute_sparsity,
+)
 
 # retrieve datasets
 from .datasets import download_dataset
 
-from .old_api_utils import (create_recording_from_old_extractor, create_sorting_from_old_extractor,
-                            create_extractor_from_new_recording, create_extractor_from_new_sorting)
+from .old_api_utils import (
+    create_recording_from_old_extractor,
+    create_sorting_from_old_extractor,
+    create_extractor_from_new_recording,
+    create_extractor_from_new_sorting,
+)
 
 # templates addition
 from .injecttemplates import InjectTemplatesRecording, InjectTemplatesRecordingSegment, inject_templates
 
 # template tools
 from .template_tools import (
     get_template_amplitudes,
     get_template_extremum_channel,
     get_template_extremum_channel_peak_shift,
     get_template_extremum_amplitude,
     get_template_channel_sparsity,
 )
 
 # channel sparsity
-from .sparsity import ChannelSparsity, compute_sparsity
+from .sparsity import ChannelSparsity, compute_sparsity
```

### Comparing `spikeinterface-0.97.1/spikeinterface/core/base.py` & `spikeinterface-0.98.0/src/spikeinterface/core/base.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,96 +1,95 @@
 from pathlib import Path
+from typing import Any, Iterable, List, Optional, Sequence, Union
 import importlib
 import warnings
 import weakref
 import json
 import pickle
 import os
 import random
 import string
 from packaging.version import parse
 from copy import deepcopy
 
 import numpy as np
 
 from .globals import get_global_tmp_folder, is_set_global_tmp_folder
-from .core_tools import check_json, is_dict_extractor, recursive_path_modifier
+from .core_tools import check_json, is_dict_extractor, recursive_path_modifier, SIJsonEncoder
 from .job_tools import _shared_job_kwargs_doc
 
+
 class BaseExtractor:
     """
     Base class for Recording/Sorting
 
     Handle serialization save/load to/from a folder.
 
     """
 
     default_missing_property_values = {"f": np.nan, "O": None, "S": "", "U": ""}
 
     # This replaces the old key_properties
-    # These are annotations/properties/features that always need to be
+    # These are annotations/properties that always need to be
     # dumped (for instance locations, groups, is_fileterd, etc.)
     _main_annotations = []
     _main_properties = []
-    _main_features = []
+
+    # these properties are skipped by default in copy_metadata
+    _skip_properties = []
 
     installed = True
     installation_mesg = ""
-    is_writable = False
-
 
-    def __init__(self, main_ids):
+    def __init__(self, main_ids: Sequence) -> None:
         # store init kwargs for nested serialisation
         self._kwargs = {}
 
         # 'main_ids' will either be channel_ids or units_ids
-        # They is used for properties and features
+        # They is used for properties
         self._main_ids = np.array(main_ids)
 
         # dict at object level
         self._annotations = {}
 
         # properties is a dict of arrays
         # array length is :
         #  * number of channel for recording
         #  * number of units for sorting
         self._properties = {}
 
-        # features is a dict of arrays (at spike level)
-        self._features = {}
-
-        self.is_dumpable = True
+        self._is_dumpable = True
+        self._is_json_serializable = True
 
         # extractor specific list of pip extra requirements
         self.extra_requirements = []
-        
+
         # preferred context for multiprocessing
         self._preferred_mp_context = None
 
-    def get_num_segments(self):
+    def get_num_segments(self) -> int:
         # This is implemented in BaseRecording or BaseSorting
         raise NotImplementedError
 
-    def _check_segment_index(self, segment_index=None):
+    def _check_segment_index(self, segment_index: Optional[int] = None) -> int:
         if segment_index is None:
             if self.get_num_segments() == 1:
                 return 0
             else:
                 raise ValueError("Multi-segment object. Provide 'segment_index'")
         else:
             return segment_index
 
-    def ids_to_indices(self, ids, prefer_slice=False):
+    def ids_to_indices(self, ids: Iterable, prefer_slice: bool = False) -> Union[np.ndarray, slice]:
         """
         Transform a ids list (aka channel_ids or unit_ids)
         into a indices array.
         Useful to manipulate:
           * data
           * properties
-          * features
 
         'prefer_slice' is an efficient option that tries to make a slice object
         when indices are consecutive.
 
         """
         if ids is None:
             if prefer_slice:
@@ -101,22 +100,22 @@
             _main_ids = self._main_ids.tolist()
             indices = np.array([_main_ids.index(id) for id in ids], dtype=int)
             if prefer_slice:
                 if np.all(np.diff(indices) == 1):
                     indices = slice(indices[0], indices[-1] + 1)
         return indices
 
-    def id_to_index(self, id):
+    def id_to_index(self, id) -> int:
         ind = list(self._main_ids).index(id)
         return ind
 
-    def annotate(self, **new_annotations):
+    def annotate(self, **new_annotations) -> None:
         self._annotations.update(new_annotations)
 
-    def set_annotation(self, annotation_key, value, overwrite=False):
+    def set_annotation(self, annotation_key, value: Any, overwrite=False) -> None:
         """This function adds an entry to the annotations dictionary.
 
         Parameters
         ----------
         annotation_key: str
             An annotation stored by the Extractor
         value:
@@ -135,224 +134,283 @@
 
     def get_preferred_mp_context(self):
         """
         Get the preferred context for multiprocessing.
         If None, the context is set by the multiprocessing package.
         """
         return self._preferred_mp_context
-    
-    def get_annotation(self, key, copy=True):
+
+    def get_annotation(self, key, copy: bool = True) -> Any:
         """
         Get a annotation.
         Return a copy by default
         """
         v = self._annotations.get(key, None)
         if copy:
             v = deepcopy(v)
         return v
 
-    def get_annotation_keys(self):
+    def get_annotation_keys(self) -> List:
         return list(self._annotations.keys())
 
-    def set_property(self, key, values, ids=None, missing_value=None):
+    def set_property(self, key, values: Sequence, ids: Optional[Sequence] = None, missing_value: Any = None) -> None:
         """
         Set property vector for main ids.
 
         If ids is given AND property already exists,
         then it is modified only on a subset of channels/units.
-        missing_values allows to specify the values of unset 
+        missing_values allows to specify the values of unset
         properties if ids is used
 
 
         Parameters
         ----------
         key : str
             The property name
         values : np.array
             Array of values for the property
         ids : list/np.array, optional
             List of subset of ids to set the values, by default None
         missing_value : object, optional
-            In case the property is set on a subset of values ('ids' not None), 
+            In case the property is set on a subset of values ('ids' not None),
             it specifies the how the missing values should be filled, by default None.
             The missing_value has to be specified for types int and unsigned int.
         """
-        
+
         if values is None:
             if key in self._properties:
                 self._properties.pop(key)
             return
 
         size = self._main_ids.size
         values = np.asarray(values)
         dtype = values.dtype
         dtype_kind = dtype.kind
-        
+
         if ids is None:
             assert values.shape[0] == size
             self._properties[key] = values
         else:
             ids = np.array(ids)
             assert np.unique(ids).size == ids.size, "'ids' are not unique!"
-            
+
             if ids.size < size:
                 if key not in self._properties:
                     # create the property with nan or empty
                     shape = (size,) + values.shape[1:]
-                    
-        
+
                     if missing_value is None:
                         if dtype_kind not in self.default_missing_property_values.keys():
-                            raise Exception("For values dtypes other than float, string, object or unicode, the missing value "
-                                            "cannot be automatically inferred. Please specify it with the 'missing_value' "
-                                            "argument.")
+                            raise Exception(
+                                "For values dtypes other than float, string, object or unicode, the missing value "
+                                "cannot be automatically inferred. Please specify it with the 'missing_value' "
+                                "argument."
+                            )
                         else:
                             missing_value = self.default_missing_property_values[dtype_kind]
                     else:
-                        assert dtype_kind == np.array(missing_value).dtype.kind, ("Mismatch between values and "
-                                                                                  "missing_value types. Provide a "
-                                                                                  "missing_value with the same type "
-                                                                                  "as the values.")
-                        
+                        assert dtype_kind == np.array(missing_value).dtype.kind, (
+                            "Mismatch between values and "
+                            "missing_value types. Provide a "
+                            "missing_value with the same type "
+                            "as the values."
+                        )
+
                     empty_values = np.zeros(shape, dtype=dtype)
                     empty_values[:] = missing_value
                     self._properties[key] = empty_values
-                    if ids.size==0:
+                    if ids.size == 0:
                         return
                 else:
-                    assert dtype_kind == self._properties[key].dtype.kind, ("Mismatch between existing property dtype "
-                                                                            "values dtype.")
+                    assert dtype_kind == self._properties[key].dtype.kind, (
+                        "Mismatch between existing property dtype " "values dtype."
+                    )
 
                 indices = self.ids_to_indices(ids)
                 self._properties[key][indices] = values
             else:
                 indices = self.ids_to_indices(ids)
                 self._properties[key] = np.zeros_like(values, dtype=values.dtype)
                 self._properties[key][indices] = values
 
-    def get_property(self, key, ids=None):
+    def get_property(self, key, ids: Optional[Iterable] = None) -> np.ndarray:
         values = self._properties.get(key, None)
         if ids is not None and values is not None:
             inds = self.ids_to_indices(ids)
             values = values[inds]
         return values
 
-    def get_property_keys(self):
+    def get_property_keys(self) -> List:
         return list(self._properties.keys())
 
-    def delete_property(self, key):
+    def delete_property(self, key) -> None:
         if key in self._properties:
             del self._properties[key]
         else:
             raise Exception(f"{key} is not a property key")
 
-    def copy_metadata(self, other, only_main=False, ids=None):
+    def copy_metadata(
+        self,
+        other: "BaseExtractor",
+        only_main: bool = False,
+        ids: Union[Iterable, slice, None] = None,
+        skip_properties: Optional[Iterable[str]] = None,
+    ) -> None:
         """
-        Copy annotations/properties/features to another extractor.
+        Copy metadata (annotations/properties) to another extractor (`other`).
 
-        If 'only main' is True, then only "main" annotations/properties/features one are copied.
+        Parameters
+        ----------
+        other: BaseExtractor
+            The extractor to copy the metadata to.
+        only_main: bool
+            If True, only the main annotations/properties are copied.
+        ids: list
+            List of ids to copy the metadata to. If None, all ids are copied.
+        skip_properties: list
+            List of properties to skip. Default is None.
         """
 
         if ids is None:
             inds = slice(None)
         elif len(ids) == 0:
             inds = slice(0, 0)
         else:
             inds = self.ids_to_indices(ids)
 
         if only_main:
             ann_keys = BaseExtractor._main_annotations
             prop_keys = BaseExtractor._main_properties
-            # feat_keys = BaseExtractor._main_features
         else:
             ann_keys = self._annotations.keys()
             prop_keys = self._properties.keys()
-            # TODO include features
-            # feat_keys = ExtractorBase._features.keys()
 
         other._annotations = deepcopy({k: self._annotations[k] for k in ann_keys})
+
+        # skip properties based on target "other" extractor
+        skip_properties_all = other._skip_properties
+        if skip_properties is not None:
+            skip_properties_all = skip_properties_all + skip_properties
+
         for k in prop_keys:
+            if k in skip_properties_all:
+                continue
             values = self._properties[k]
             if values is not None:
                 other.set_property(k, values[inds])
 
         other.extra_requirements.extend(self.extra_requirements)
-        
+
         if self._preferred_mp_context is not None:
             other._preferred_mp_context = self._preferred_mp_context
 
-    def to_dict(self, include_annotations=False, include_properties=False,
-                relative_to=None, folder_metadata=None):
+    def to_dict(
+        self,
+        include_annotations: bool = False,
+        include_properties: bool = False,
+        relative_to: Union[str, Path, None] = None,
+        folder_metadata=None,
+        recursive: bool = False,
+    ) -> dict:
         """
-        Make a nested serialized dictionary out of the extractor. The dictionary produced can be used to re-initialize 
+        Make a nested serialized dictionary out of the extractor. The dictionary produced can be used to re-initialize
         an extractor using load_extractor_from_dict(dump_dict)
 
         Parameters
         ----------
         include_annotations: bool
             If True, all annotations are added to the dict, by default False
         include_properties: bool
             If True, all properties are added to the dict, by default False
         relative_to: str, Path, or None
             If not None, file_paths are serialized relative to this path, by default None
+            Used in waveform extractor to maintain relative paths to binary files even if the
+            containing folder / diretory is moved
+        folder_metadata: str, Path, or None
+            Folder with numpy `npy` files containing additional information (e.g. probe in BaseRecording) and properties.
+        recursive: bool
+            If True, all dicitionaries in the kwargs are expanded with `to_dict` as well, by default False.
 
         Returns
         -------
         dump_dict: dict
-            Serialized dictionary
+            A dictionary representation of the extractor.
         """
-        class_name = str(type(self)).replace("<class '", "").replace("'>", '')
-        module = class_name.split('.')[0]
+
+        kwargs = self._kwargs
+
+        if recursive:
+            to_dict_kwargs = dict(
+                include_annotations=include_annotations,
+                include_properties=include_properties,
+                relative_to=None,  # '_make_paths_relative' is already recursive!
+                folder_metadata=folder_metadata,
+                recursive=recursive,
+            )
+
+            new_kwargs = dict()
+            transform_extractors_to_dict = lambda x: x.to_dict(**to_dict_kwargs) if isinstance(x, BaseExtractor) else x
+
+            for name, value in self._kwargs.items():
+                if isinstance(value, list):
+                    new_kwargs[name] = [transform_extractors_to_dict(element) for element in value]
+                elif isinstance(value, dict):
+                    new_kwargs[name] = {k: transform_extractors_to_dict(v) for k, v in value.items()}
+                else:
+                    new_kwargs[name] = transform_extractors_to_dict(value)
+
+            kwargs = new_kwargs
+        class_name = str(type(self)).replace("<class '", "").replace("'>", "")
+        module = class_name.split(".")[0]
         imported_module = importlib.import_module(module)
 
         try:
             version = imported_module.__version__
         except AttributeError:
-            version = 'unknown'
+            version = "unknown"
 
         dump_dict = {
-            'class': class_name,
-            'module': module,
-            'kwargs': self._kwargs,
-            'dumpable': self.is_dumpable,
-            'version': version,
-            'relative_paths': (relative_to is not None),
+            "class": class_name,
+            "module": module,
+            "kwargs": kwargs,
+            "version": version,
+            "relative_paths": (relative_to is not None),
         }
 
         try:
-            dump_dict['version'] = imported_module.__version__
+            dump_dict["version"] = imported_module.__version__
         except AttributeError:
-            dump_dict['version'] = 'unknown'
+            dump_dict["version"] = "unknown"
 
         if include_annotations:
-            dump_dict['annotations'] = self._annotations
+            dump_dict["annotations"] = self._annotations
         else:
             # include only main annotations
-            dump_dict['annotations'] = {k: self._annotations.get(k, None) for k in self._main_annotations}
+            dump_dict["annotations"] = {k: self._annotations.get(k, None) for k in self._main_annotations}
 
         if include_properties:
-            dump_dict['properties'] = self._properties
+            dump_dict["properties"] = self._properties
         else:
             # include only main properties
-            dump_dict['properties'] = {k: self._properties.get(k, None) for k in self._main_properties}
+            dump_dict["properties"] = {k: self._properties.get(k, None) for k in self._main_properties}
 
         if relative_to is not None:
             relative_to = Path(relative_to).absolute()
             assert relative_to.is_dir(), "'relative_to' must be an existing directory"
             dump_dict = _make_paths_relative(dump_dict, relative_to)
 
         if folder_metadata is not None:
             if relative_to is not None:
                 folder_metadata = Path(folder_metadata).absolute().relative_to(relative_to)
-            dump_dict['folder_metadata'] = str(folder_metadata)
+            dump_dict["folder_metadata"] = str(folder_metadata)
 
         return dump_dict
 
     @staticmethod
-    def from_dict(d, base_folder=None):
+    def from_dict(dictionary: dict, base_folder: Optional[Union[Path, str]] = None) -> "BaseExtractor":
         """
         Instantiate extractor from dictionary
 
         Parameters
         ----------
         d: dictionary
             Python dictionary
@@ -360,64 +418,101 @@
             If given, the parent folder of the file and folder paths
 
         Returns
         -------
         extractor: RecordingExtractor or SortingExtractor
             The loaded extractor object
         """
-        if d['relative_paths']:
-            assert base_folder is not None, 'When  relative_paths=True, need to provide base_folder'
-            d = _make_paths_absolute(d, base_folder)
-        extractor = _load_extractor_from_dict(d)
-        folder_metadata = d.get('folder_metadata', None)
+        if dictionary["relative_paths"]:
+            assert base_folder is not None, "When  relative_paths=True, need to provide base_folder"
+            dictionary = _make_paths_absolute(dictionary, base_folder)
+        extractor = _load_extractor_from_dict(dictionary)
+        folder_metadata = dictionary.get("folder_metadata", None)
         if folder_metadata is not None:
             folder_metadata = Path(folder_metadata)
-            if d['relative_paths']:
+            if dictionary["relative_paths"]:
                 folder_metadata = base_folder / folder_metadata
             extractor.load_metadata_from_folder(folder_metadata)
         return extractor
 
     def load_metadata_from_folder(self, folder_metadata):
         # hack to load probe for recording
         folder_metadata = Path(folder_metadata)
 
         self._extra_metadata_from_folder(folder_metadata)
 
         # load properties
-        prop_folder = folder_metadata / 'properties'
+        prop_folder = folder_metadata / "properties"
         if prop_folder.is_dir():
             for prop_file in prop_folder.iterdir():
-                if prop_file.suffix == '.npy':
+                if prop_file.suffix == ".npy":
                     values = np.load(prop_file, allow_pickle=True)
                     key = prop_file.stem
                     self.set_property(key, values)
 
     def save_metadata_to_folder(self, folder_metadata):
         self._extra_metadata_to_folder(folder_metadata)
 
         # save properties
-        prop_folder = folder_metadata / 'properties'
+        prop_folder = folder_metadata / "properties"
         prop_folder.mkdir(parents=True, exist_ok=False)
         for key in self.get_property_keys():
             values = self.get_property(key)
-            np.save(prop_folder / (key + '.npy'), values)
+            np.save(prop_folder / (key + ".npy"), values)
 
-    def clone(self):
+    def clone(self) -> "BaseExtractor":
         """
         Clones an existing extractor into a new instance.
         """
         d = self.to_dict(include_annotations=True, include_properties=True)
+        d = deepcopy(d)
         clone = BaseExtractor.from_dict(d)
         return clone
 
     def check_if_dumpable(self):
-        return _check_if_dumpable(self.to_dict())
+        """Check if the object is dumpable, including nested objects.
+
+        Returns
+        -------
+        bool
+            True if the object is dumpable, False otherwise.
+        """
+        kwargs = self._kwargs
+        for value in kwargs.values():
+            # here we check if the value is a BaseExtractor, a list of BaseExtractors, or a dict of BaseExtractors
+            if isinstance(value, BaseExtractor):
+                return value.check_if_dumpable()
+            elif isinstance(value, list) and (len(value) > 0) and isinstance(value[0], BaseExtractor):
+                return all([v.check_if_dumpable() for v in value])
+            elif isinstance(value, dict) and isinstance(value[list(value.keys())[0]], BaseExtractor):
+                return all([v.check_if_dumpable() for k, v in value.items()])
+        return self._is_dumpable
+
+    def check_if_json_serializable(self):
+        """
+        Check if the object is json serializable, including nested objects.
+
+        Returns
+        -------
+        bool
+            True if the object is json serializable, False otherwise.
+        """
+        kwargs = self._kwargs
+        for value in kwargs.values():
+            # here we check if the value is a BaseExtractor, a list of BaseExtractors, or a dict of BaseExtractors
+            if isinstance(value, BaseExtractor):
+                return value.check_if_json_serializable()
+            elif isinstance(value, list) and (len(value) > 0) and isinstance(value[0], BaseExtractor):
+                return all([v.check_if_json_serializable() for v in value])
+            elif isinstance(value, dict) and isinstance(value[list(value.keys())[0]], BaseExtractor):
+                return all([v.check_if_json_serializable() for k, v in value.items()])
+        return self._is_json_serializable
 
     @staticmethod
-    def _get_file_path(file_path, extensions):
+    def _get_file_path(file_path: Union[str, Path], extensions: Sequence) -> Path:
         """
         Helper function to be used by various dump_to_file utilities.
 
         Returns default file_path (if not specified), makes sure that target
         directory exists, adds correct file extension if none, and checks
         that the provided file extension is allowed.
 
@@ -433,208 +528,224 @@
         Path
             Path object with file path to the file
         """
         ext = extensions[0]
         file_path = Path(file_path)
         file_path.parent.mkdir(parents=True, exist_ok=True)
         folder_path = file_path.parent
-        if Path(file_path).suffix == '':
+        if Path(file_path).suffix == "":
             file_path = folder_path / (str(file_path) + ext)
-        assert file_path.suffix in extensions, \
-            "'file_path' should have one of the following extensions:" \
-            " %s" % (', '.join(extensions))
+        assert file_path.suffix in extensions, "'file_path' should have one of the following extensions:" " %s" % (
+            ", ".join(extensions)
+        )
         return file_path
 
-    def dump(self, file_path, relative_to=None, folder_metadata=None):
+    def dump(self, file_path: Union[str, Path], relative_to=None, folder_metadata=None) -> None:
         """
         Dumps extractor to json or pickle
 
         Parameters
         ----------
         file_path: str or Path
             The output file (either .json or .pkl/.pickle)
         relative_to: str, Path, or None
             If not None, file_paths are serialized relative to this path
         """
-        if str(file_path).endswith('.json'):
+        if str(file_path).endswith(".json"):
             self.dump_to_json(file_path, relative_to=relative_to, folder_metadata=folder_metadata)
-        elif str(file_path).endswith('.pkl') or str(file_path).endswith('.pickle'):
+        elif str(file_path).endswith(".pkl") or str(file_path).endswith(".pickle"):
             self.dump_to_pickle(file_path, relative_to=relative_to, folder_metadata=folder_metadata)
         else:
-            raise ValueError('Dump: file must .json or .pkl')
+            raise ValueError("Dump: file must .json or .pkl")
 
-    def dump_to_json(self, file_path=None, relative_to=None, folder_metadata=None):
+    def dump_to_json(self, file_path: Union[str, Path, None] = None, relative_to=None, folder_metadata=None) -> None:
         """
         Dump recording extractor to json file.
         The extractor can be re-loaded with load_extractor_from_json(json_file)
 
         Parameters
         ----------
         file_path: str
             Path of the json file
         relative_to: str, Path, or None
             If not None, file_paths are serialized relative to this path
         """
         assert self.check_if_dumpable()
-        dump_dict = self.to_dict(include_annotations=True,
-                                 include_properties=False,
-                                 relative_to=relative_to,
-                                 folder_metadata=folder_metadata)
-        file_path = self._get_file_path(file_path, ['.json'])
+        dump_dict = self.to_dict(
+            include_annotations=True, include_properties=False, relative_to=relative_to, folder_metadata=folder_metadata
+        )
+        file_path = self._get_file_path(file_path, [".json"])
+
         file_path.write_text(
-            json.dumps(check_json(dump_dict), indent=4),
-            encoding='utf8'
+            json.dumps(dump_dict, indent=4, cls=SIJsonEncoder),
+            encoding="utf8",
         )
 
-    def dump_to_pickle(self, file_path=None, include_properties=True,
-                       relative_to=None, folder_metadata=None):
+    def dump_to_pickle(
+        self,
+        file_path: Union[str, Path, None] = None,
+        include_properties: bool = True,
+        relative_to=None,
+        folder_metadata=None,
+        recursive: bool = False,
+    ):
         """
         Dump recording extractor to a pickle file.
         The extractor can be re-loaded with load_extractor_from_json(json_file)
 
         Parameters
         ----------
         file_path: str
             Path of the json file
         include_properties: bool
             If True, all properties are dumped
         relative_to: str, Path, or None
             If not None, file_paths are serialized relative to this path
+        recursive: bool
+            If True, all dicitionaries in the kwargs are expanded with `to_dict` as well, by default False.
         """
         assert self.check_if_dumpable()
-        dump_dict = self.to_dict(include_annotations=True,
-                                 include_properties=include_properties,
-                                 relative_to=relative_to,
-                                 folder_metadata=folder_metadata)
-        file_path = self._get_file_path(file_path, ['.pkl', '.pickle'])
+        dump_dict = self.to_dict(
+            include_annotations=True,
+            include_properties=include_properties,
+            relative_to=relative_to,
+            folder_metadata=folder_metadata,
+            recursive=recursive,
+        )
+        file_path = self._get_file_path(file_path, [".pkl", ".pickle"])
 
         file_path.write_bytes(pickle.dumps(dump_dict))
 
     @staticmethod
-    def load(file_path, base_folder=None):
+    def load(file_path: Union[str, Path], base_folder=None) -> "BaseExtractor":
         """
         Load extractor from file path (.json or .pkl)
 
         Used both after:
           * dump(...) json or pickle file
           * save (...)  a folder which contain data  + json (or pickle) + metadata.
         """
 
         file_path = Path(file_path)
         if file_path.is_file():
             # standard case based on a file (json or pickle)
-            if str(file_path).endswith('.json'):
-                with open(str(file_path), 'r') as f:
+            if str(file_path).endswith(".json"):
+                with open(str(file_path), "r") as f:
                     d = json.load(f)
-            elif str(file_path).endswith('.pkl') or str(file_path).endswith('.pickle'):
-                with open(str(file_path), 'rb') as f:
+            elif str(file_path).endswith(".pkl") or str(file_path).endswith(".pickle"):
+                with open(str(file_path), "rb") as f:
                     d = pickle.load(f)
             else:
-                raise ValueError(f'Impossible to load {file_path}')
-            if 'warning' in d and 'not dumpable' in d['warning']:
-                print('The extractor was not dumpable')
+                raise ValueError(f"Impossible to load {file_path}")
+            if "warning" in d and "not dumpable" in d["warning"]:
+                print("The extractor was not dumpable")
                 return None
             extractor = BaseExtractor.from_dict(d, base_folder=base_folder)
             return extractor
 
         elif file_path.is_dir():
             # case from a folder after a calling extractor.save(...)
             folder = file_path
             file = None
-            
-            # the is spikeinterface<=0.94.0
-            # a folder came with 'cached.sjon'
-            for dump_ext in ('json', 'pkl', 'pickle'):
-                f = folder / f'cached.{dump_ext}'
+
+            if folder.suffix == ".zarr":
+                from .zarrrecordingextractor import read_zarr
+
+                extractor = read_zarr(folder)
+            else:
+                # the is spikeinterface<=0.94.0
+                # a folder came with 'cached.json'
+                for dump_ext in ("json", "pkl", "pickle"):
+                    f = folder / f"cached.{dump_ext}"
+                    if f.is_file():
+                        file = f
+
+                # spikeinterface>=0.95.0
+                f = folder / f"si_folder.json"
                 if f.is_file():
                     file = f
-            
-            # spikeinterface>=0.95.0
-            f = folder / f'si_folder.json'
-            if f.is_file():
-                file = f
-
-            if file is None:
-                raise ValueError(f'This folder is not a cached folder {file_path}')
-            extractor = BaseExtractor.load(file, base_folder=folder)
+
+                if file is None:
+                    raise ValueError(f"This folder is not a cached folder {file_path}")
+                extractor = BaseExtractor.load(file, base_folder=folder)
 
             return extractor
 
         else:
-            raise ValueError('spikeinterface.Base.load() file_path must be an existing folder or file')
+            raise ValueError("spikeinterface.Base.load() file_path must be an existing folder or file")
 
     def __reduce__(self):
         """
         This function is used by pickle to serialize the object.
         """
         instance_constructor = self.from_dict
-        intialization_args = (self.to_dict(), )
+        intialization_args = (self.to_dict(),)
         return (instance_constructor, intialization_args)
 
     @staticmethod
-    def load_from_folder(folder):
+    def load_from_folder(folder) -> "BaseExtractor":
         return BaseExtractor.load(folder)
-    
+
     def _save(self, folder, **save_kwargs):
         # This implemented in BaseRecording or baseSorting
         # this is internally call by cache(...) main function
         raise NotImplementedError
 
     def _extra_metadata_from_folder(self, folder):
         # This implemented in BaseRecording for probe
         pass
 
     def _extra_metadata_to_folder(self, folder):
         # This implemented in BaseRecording for probe
         pass
 
-    def save(self, **kwargs):
+    def save(self, **kwargs) -> "BaseExtractor":
         """
-        Save a SpikeInterface object. 
+        Save a SpikeInterface object.
 
         Parameters
         ----------
         kwargs: Keyword arguments for saving.
             * format: "memory", "zarr", or "binary" (for recording) / "memory" or "npz" for sorting.
-                In case format is not memory, the recording is saved to a folder. See format specific functions for 
+                In case format is not memory, the recording is saved to a folder. See format specific functions for
                 more info (`save_to_memory()`, `save_to_folder()`, `save_to_zarr()`)
             * folder: if provided, the folder path where the object is saved
             * name: if provided and folder is not given, the name of the folder in the global temporary
                     folder (use set_global_tmp_folder() to change this folder) where the object is saved.
-              If folder and name are not given, the object is saved in the global temporary folder with 
+              If folder and name are not given, the object is saved in the global temporary folder with
               a random string
             * dump_ext: 'json' or 'pkl', default 'json' (if format is "folder")
             * verbose: if True output is verbose
             * **save_kwargs: additional kwargs format-dependent and job kwargs for recording
             {}
 
         Returns
         -------
         loaded_extractor: BaseRecording or BaseSorting
             The reference to the saved object after it is loaded back
         """
-        format = kwargs.get('format', None)
-        if format == 'memory':
+        format = kwargs.get("format", None)
+        if format == "memory":
             loaded_extractor = self.save_to_memory(**kwargs)
-        elif format == 'zarr':
+        elif format == "zarr":
             loaded_extractor = self.save_to_zarr(**kwargs)
         else:
             loaded_extractor = self.save_to_folder(**kwargs)
         return loaded_extractor
 
     save.__doc__ = save.__doc__.format(_shared_job_kwargs_doc)
 
-    def save_to_memory(self, **kwargs):
+    def save_to_memory(self, **kwargs) -> "BaseExtractor":
         # used only by recording at the moment
         cached = self._save(**kwargs)
         self.copy_metadata(cached)
         return cached
 
     # TODO rename to saveto_binary_folder
-    def save_to_folder(self, name=None, folder=None,  verbose=True, **save_kwargs):
+    def save_to_folder(self, name=None, folder=None, verbose=True, **save_kwargs):
         """
         Save extractor to folder.
 
         The save consist of:
           * extracting traces by calling get_trace() method in chunks
           * saving data into file (memmap with BinaryRecordingExtractor)
           * dumping to json/pickle the original extractor for provenance
@@ -662,251 +773,264 @@
             Name of the folder.
             If 'folder' is given, 'name' must be None.
 
         Returns
         -------
         cached: saved copy of the extractor.
         """
-        
+
         if folder is None:
             cache_folder = get_global_tmp_folder()
             if name is None:
-                name = ''.join(random.choices(string.ascii_uppercase + string.digits, k=8))
+                name = "".join(random.choices(string.ascii_uppercase + string.digits, k=8))
                 folder = cache_folder / name
                 if verbose:
-                    print(f'Use cache_folder={folder}')
+                    print(f"Use cache_folder={folder}")
             else:
                 folder = cache_folder / name
                 if not is_set_global_tmp_folder():
                     if verbose:
-                        print(f'Use cache_folder={folder}')
+                        print(f"Use cache_folder={folder}")
         else:
             folder = Path(folder)
-        assert not folder.exists(), f'folder {folder} already exists, choose another name'
+        assert not folder.exists(), f"folder {folder} already exists, choose another name"
         folder.mkdir(parents=True, exist_ok=False)
 
         # dump provenance
-        provenance_file = folder / f'provenance.json'
-        if self.check_if_dumpable():
+        provenance_file = folder / f"provenance.json"
+        if self.check_if_json_serializable():
             self.dump(provenance_file)
         else:
-            provenance_file.write_text(
-                json.dumps({'warning': 'the provenace is not dumpable!!!'}),
-                encoding='utf8'
-            )
-        
+            provenance_file.write_text(json.dumps({"warning": "the provenace is not dumpable!!!"}), encoding="utf8")
+
         self.save_metadata_to_folder(folder)
-        
+
         # save data (done the subclass)
         cached = self._save(folder=folder, verbose=verbose, **save_kwargs)
 
         # copy properties/
         self.copy_metadata(cached)
 
         # dump
-        # cached.dump(folder / f'cached.json', relative_to=folder, folder_metadata=folder)
-        cached.dump(folder / f'si_folder.json', relative_to=folder)
+        # cached.dump(folder / f'cached.json', relative_to=folder, folder_metadata=folder)
+        cached.dump(folder / f"si_folder.json", relative_to=folder)
 
         return cached
 
-    def save_to_zarr(self, name=None, folder=None, storage_options=None, 
-                     channel_chunk_size=None, verbose=True, zarr_path=None, **save_kwargs):
+    def save_to_zarr(
+        self,
+        name=None,
+        folder=None,
+        storage_options=None,
+        channel_chunk_size=None,
+        verbose=True,
+        zarr_path=None,
+        **save_kwargs,
+    ):
         """
         Save extractor to zarr.
 
         The save consist of:
             * extracting traces by calling get_trace() method in chunks
-            * saving data into a zarr file 
+            * saving data into a zarr file
             * dumping the original extractor for provenance in attributes
 
         Parameters
         ----------
         name: str or None
             Name of the subfolder in get_global_tmp_folder()
             If 'name' is given, 'folder' must be None.
         folder: str, Path, or None
-            The folder used to save the zarr output. If the folder does not have a '.zarr' suffix, 
+            The folder used to save the zarr output. If the folder does not have a '.zarr' suffix,
             it will be automatically appended.
         storage_options: dict or None
             Storage options for zarr `store`. E.g., if "s3://" or "gcs://" they can provide authentication methods, etc.
             For cloud storage locations, this should not be None (in case of default values, use an empty dict)
         channel_chunk_size: int or None
             Channels per chunk. Default None (chunking in time only)
         verbose: bool
             If True (default), the output is verbose.
         zarr_path: str, Path, or None
-            (Deprecated) Name of the zarr folder (.zarr). 
-        
+            (Deprecated) Name of the zarr folder (.zarr).
+
         Returns
         -------
         cached: ZarrRecordingExtractor
             Saved copy of the extractor.
         """
         import zarr
+        from .zarrrecordingextractor import read_zarr
 
         if zarr_path is not None:
-            warnings.warn("The 'zarr_path' argument is deprecated. "
-                          "Use 'folder' instead",
-                          DeprecationWarning, stacklevel=2)
+            warnings.warn(
+                "The 'zarr_path' argument is deprecated. " "Use 'folder' instead", DeprecationWarning, stacklevel=2
+            )
             folder = zarr_path
 
         if folder is None:
             cache_folder = get_global_tmp_folder()
             if name is None:
-                name = ''.join(random.choices(
-                    string.ascii_uppercase + string.digits, k=8))
+                name = "".join(random.choices(string.ascii_uppercase + string.digits, k=8))
                 zarr_path = cache_folder / f"{name}.zarr"
                 if verbose:
-                    print(f'Use zarr_path={zarr_path}')
+                    print(f"Use zarr_path={zarr_path}")
             else:
                 zarr_path = cache_folder / f"{name}.zarr"
                 if not is_set_global_tmp_folder():
                     if verbose:
-                        print(f'Use zarr_path={zarr_path}')
+                        print(f"Use zarr_path={zarr_path}")
         else:
             if storage_options is None:
                 folder = Path(folder)
                 if folder.suffix != ".zarr":
                     folder = folder.parent / f"{folder.stem}.zarr"
                 zarr_path = folder
                 zarr_path_init = str(zarr_path)
             else:
                 zarr_path = folder
                 zarr_path_init = zarr_path
 
         if isinstance(zarr_path, Path):
-            assert not zarr_path.exists(), f'Path {zarr_path} already exists, choose another name'
-        
+            assert not zarr_path.exists(), f"Path {zarr_path} already exists, choose another name"
+
         zarr_root = zarr.open(zarr_path_init, mode="w", storage_options=storage_options)
 
         if self.check_if_dumpable():
             zarr_root.attrs["provenance"] = check_json(self.to_dict())
         else:
             zarr_root.attrs["provenance"] = None
 
         # save data (done the subclass)
-        save_kwargs['zarr_root'] = zarr_root
-        save_kwargs['zarr_path'] = zarr_path
-        save_kwargs['storage_options'] = storage_options
-        save_kwargs['channel_chunk_size'] = channel_chunk_size
+        save_kwargs["zarr_root"] = zarr_root
+        save_kwargs["zarr_path"] = zarr_path
+        save_kwargs["storage_options"] = storage_options
+        save_kwargs["channel_chunk_size"] = channel_chunk_size
         cached = self._save(verbose=verbose, **save_kwargs)
-        cached_annotations = deepcopy(cached._annotations)
 
         # save properties
-        prop_group = zarr_root.create_group('properties')
+        prop_group = zarr_root.create_group("properties")
         for key in self.get_property_keys():
             values = self.get_property(key)
             prop_group.create_dataset(name=key, data=values, compressor=None)
 
         # save annotations
         zarr_root.attrs["annotations"] = check_json(self._annotations)
 
-        # copy properties/
-        self.copy_metadata(cached)
-        # append annotations on compression
-        cached._annotations.update(cached_annotations)
+        cached = read_zarr(zarr_path)
 
         return cached
 
 
-def _make_paths_relative(d, relative):
+def _make_paths_relative(d, relative) -> dict:
     relative = str(Path(relative).absolute())
     func = lambda p: os.path.relpath(str(p), start=relative)
-    return recursive_path_modifier(d,  func, target='path', copy=True)
+    return recursive_path_modifier(d, func, target="path", copy=True)
 
 
 def _make_paths_absolute(d, base):
     base = Path(base)
     func = lambda p: str((base / p).resolve().absolute())
-    return recursive_path_modifier(d,  func, target='path', copy=True)
+    return recursive_path_modifier(d, func, target="path", copy=True)
 
-def _check_if_dumpable(d):
-    kwargs = d['kwargs']
-    if np.any([isinstance(v, dict) and 'dumpable' in v.keys() for (k, v) in kwargs.items()]):
-        # check nested
-        for k, v in kwargs.items():
-            if isinstance(v, dict) and 'dumpable' in v.keys():
-                return _check_if_dumpable(v)
-    else:
-        return d['dumpable']
 
+def _load_extractor_from_dict(dic) -> BaseExtractor:
+    """
+    Convert a dictionary into an instance of BaseExtractor or its subclass.
+
+    This function takes a dictionary that represents the state of an extractor,
+    and reconstructs the extractor from the dictionary.
 
-def _load_extractor_from_dict(dic):
-    cls = None
+    Parameters
+    ----------
+    dic : dict
+        A dictionary representation of the extractor which  must contain the following keys:
+        - "kwargs": A dictionary of keyword arguments to be passed to the extractor class upon instantiation.
+        - "class": The full name of the extractor class, including the module name.
+        - "version": The version of the extractor class used when the dictionary was created.
+        - "annotations": A dictionary of annotations.
+        - "properties": A dictionary of properties.
+
+    Returns
+    -------
+    BaseExtractor
+        An instance of BaseExtractor or its subclass.
+    """
+    extractor_class = None
     class_name = None
 
-    if 'kwargs' not in dic:
-        raise Exception(f'This dict cannot be load into extractor {dic}')
-    kwargs = deepcopy(dic['kwargs'])
-
-    # handle nested
-    for k, v in kwargs.items():
-
-        if isinstance(v, dict) and is_dict_extractor(v):
-            kwargs[k] = _load_extractor_from_dict(v)
-
-            # handle list of extractors list
-    for k, v in kwargs.items():
-        if isinstance(v, list):
-            if all(is_dict_extractor(e) for e in v):
-                kwargs[k] = [_load_extractor_from_dict(e) for e in v]
-
-    class_name = dic['class']
-    cls = _get_class_from_string(class_name)
-
-    assert cls is not None and class_name is not None, "Could not load spikeinterface class"
-    if not _check_same_version(class_name, dic['version']):
-        print('Versions are not the same. This might lead to errors. Use ', class_name.split('.')[0],
-              'version', dic['version'])
-
-    # instantiate extrator object
-    extractor = cls(**kwargs)
-
-    extractor._annotations.update(dic['annotations'])
-    for k, v in dic['properties'].items():
-        # print(k, v)
+    if "kwargs" not in dic:
+        raise Exception(f"This dict cannot be load into extractor {dic}")
+
+    # Create new kwargs to avoid modifying the original dict["kwargs"]
+    new_kwargs = dict()
+    transform_dict_to_extractor = lambda x: _load_extractor_from_dict(x) if is_dict_extractor(x) else x
+    for name, value in dic["kwargs"].items():
+        if is_dict_extractor(value):
+            new_kwargs[name] = _load_extractor_from_dict(value)
+        elif isinstance(value, dict):
+            new_kwargs[name] = {k: transform_dict_to_extractor(v) for k, v in value.items()}
+        elif isinstance(value, list):
+            new_kwargs[name] = [transform_dict_to_extractor(e) for e in value]
+        else:
+            new_kwargs[name] = value
+
+    class_name = dic["class"]
+    extractor_class = _get_class_from_string(class_name)
+
+    assert extractor_class is not None and class_name is not None, "Could not load spikeinterface class"
+    if not _check_same_version(class_name, dic["version"]):
+        warnings.warn(
+            f"Versions are not the same. This might lead compatibility errors. "
+            f"Using {class_name.split('.')[0]}=={dic['version']} is recommended"
+        )
+
+    # Initialize the extractor
+    extractor = extractor_class(**new_kwargs)
+
+    extractor._annotations.update(dic["annotations"])
+    for k, v in dic["properties"].items():
         extractor.set_property(k, v)
-    # TODO features
 
     return extractor
 
 
 def _get_class_from_string(class_string):
-    class_name = class_string.split('.')[-1]
-    module = '.'.join(class_string.split('.')[:-1])
+    class_name = class_string.split(".")[-1]
+    module = ".".join(class_string.split(".")[:-1])
     imported_module = importlib.import_module(module)
 
     try:
         imported_class = getattr(imported_module, class_name)
     except:
         imported_class = None
 
     return imported_class
 
 
 def _check_same_version(class_string, version):
-    module = class_string.split('.')[0]
+    module = class_string.split(".")[0]
     imported_module = importlib.import_module(module)
-    
+
     current_version = parse(imported_module.__version__)
     saved_version = parse(version)
 
     try:
         return current_version.major == saved_version.major and current_version.minor == saved_version.minor
     except AttributeError:
-        return 'unknown'
+        return "unknown"
 
 
-def load_extractor(file_or_folder_or_dict, base_folder=None):
+def load_extractor(file_or_folder_or_dict, base_folder=None) -> BaseExtractor:
     """
     Instantiate extractor from:
       * a dict
       * a json file
       * a pickle file
       * folder (after save)
+      * a zarr folder (after save)
 
     Parameters
     ----------
     file_or_folder_or_dict: dictionary or folder or file (json, pickle)
 
     Returns
     -------
@@ -915,32 +1039,32 @@
     """
     if isinstance(file_or_folder_or_dict, dict):
         return BaseExtractor.from_dict(file_or_folder_or_dict, base_folder=base_folder)
     else:
         return BaseExtractor.load(file_or_folder_or_dict, base_folder=base_folder)
 
 
-def load_extractor_from_dict(d, base_folder=None):
-    print('Use load_extractor(..) instead')
+def load_extractor_from_dict(d, base_folder=None) -> BaseExtractor:
+    warnings.warn("Use load_extractor(..) instead")
     return BaseExtractor.from_dict(d, base_folder=base_folder)
 
 
-def load_extractor_from_json(json_file, base_folder=None):
-    print('Use load_extractor(..) instead')
+def load_extractor_from_json(json_file, base_folder=None) -> "BaseExtractor":
+    warnings.warn("Use load_extractor(..) instead")
     return BaseExtractor.load(json_file, base_folder=base_folder)
 
 
-def load_extractor_from_pickle(pkl_file, base_folder=None):
-    print('Use load_extractor(..) instead')
+def load_extractor_from_pickle(pkl_file, base_folder=None) -> "BaseExtractor":
+    warnings.warn("Use load_extractor(..) instead")
     return BaseExtractor.load(pkl_file, base_folder=base_folder)
 
 
 class BaseSegment:
     def __init__(self):
         self._parent_extractor = None
 
     @property
-    def parent_extractor(self):
+    def parent_extractor(self) -> Union[BaseExtractor, None]:
         return self._parent_extractor()
 
-    def set_parent_extractor(self, parent_extractor):
+    def set_parent_extractor(self, parent_extractor: BaseExtractor) -> None:
         self._parent_extractor = weakref.ref(parent_extractor)
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `spikeinterface-0.97.1/spikeinterface/core/baseevent.py` & `spikeinterface-0.98.0/src/spikeinterface/core/baseevent.py`

 * *Files 5% similar despite different names*

```diff
@@ -16,39 +16,42 @@
         The channel ids
     structured_dtype : dtype or dict
         The dtype of the events. If dict, each key is the channel_id and values must be
         the dtype of the channel (also structured). If dtype, each channel is assigned the
         same dtype.
         In case of structured dtypes, the "time" or "timestamp" field name must be present.
     """
+
     def __init__(self, channel_ids, structured_dtype):
         BaseExtractor.__init__(self, channel_ids)
         self._event_segments: List[BaseEventSegment] = []
 
         if not isinstance(structured_dtype, dict):
             structured_dtype = {chan_id: structured_dtype for chan_id in channel_ids}
         else:
-            assert all(chan_id in structured_dtype for chan_id in channel_ids), \
-                ("Missing some channel_ids from structured_dtype dict keys")
+            assert all(
+                chan_id in structured_dtype for chan_id in channel_ids
+            ), "Missing some channel_ids from structured_dtype dict keys"
 
         # check dtype fields (if present)
         for _, dtype in structured_dtype.items():
             if dtype.names is not None:
-                assert "time" in dtype.names or "timestamp" in dtype.names, \
-                    ("The event dtype need to have the 'time' or 'timestamp' field")
+                assert (
+                    "time" in dtype.names or "timestamp" in dtype.names
+                ), "The event dtype need to have the 'time' or 'timestamp' field"
 
         self.structured_dtype = structured_dtype
 
     def __repr__(self):
         clsname = self.__class__.__name__
         nseg = self.get_num_segments()
         nchannels = self.get_num_channels()
-        txt = f'{clsname}: {nchannels} channels - {nseg} segments'
-        if 'file_path' in self._kwargs:
-            txt += '\n  file_path: {}'.format(self._kwargs['file_path'])
+        txt = f"{clsname}: {nchannels} channels - {nseg} segments"
+        if "file_path" in self._kwargs:
+            txt += "\n  file_path: {}".format(self._kwargs["file_path"])
         return txt
 
     @property
     def channel_ids(self):
         return self._main_ids
 
     def get_dtype(self, channel_id):
@@ -61,20 +64,21 @@
         # todo: check consistency with unit ids and freq
         self._event_segments.append(event_segment)
         event_segment.set_parent_extractor(self)
 
     def get_num_segments(self):
         return len(self._event_segments)
 
-    def get_events(self,
-                   channel_id=None,
-                   segment_index=None,
-                   start_time=None,
-                   end_time=None,
-                   ):
+    def get_events(
+        self,
+        channel_id=None,
+        segment_index=None,
+        start_time=None,
+        end_time=None,
+    ):
         """
         Return events of a channel in its native structured type.
 
         Parameters
         ----------
         channel_id : int or str, optional
             The event channel id, by default None
@@ -90,20 +94,21 @@
         np.array
             Structured np.array of dtype `get_dtype(channel_id)`
         """
         segment_index = self._check_segment_index(segment_index)
         seg_ev = self._event_segments[segment_index]
         return seg_ev.get_events(channel_id, start_time, end_time)
 
-    def get_event_times(self,
-                        channel_id=None,
-                        segment_index=None,
-                        start_time=None,
-                        end_time=None,
-                        ):
+    def get_event_times(
+        self,
+        channel_id=None,
+        segment_index=None,
+        start_time=None,
+        end_time=None,
+    ):
         """
         Return events timestamps of a channel in seconds.
 
         Parameters
         ----------
         channel_id : int or str, optional
             The event channel id, by default None
@@ -141,8 +146,8 @@
                 times = events["time"]
             else:
                 times = events["timestamp"]
         return times
 
     def get_events(self, channel_id, start_time, end_time):
         # must be implemented in subclass
-        raise NotImplementedError
+        raise NotImplementedError
```

### Comparing `spikeinterface-0.97.1/spikeinterface/core/baserecording.py` & `spikeinterface-0.98.0/src/spikeinterface/core/baserecording.py`

 * *Files 16% similar despite different names*

```diff
@@ -5,56 +5,111 @@
 import numpy as np
 
 from probeinterface import Probe, ProbeGroup, write_probeinterface, read_probeinterface, select_axes
 
 from .base import BaseSegment
 from .baserecordingsnippets import BaseRecordingSnippets
 from .core_tools import write_binary_recording, write_memory_recording, write_traces_to_zarr, check_json
-from .job_tools import split_job_kwargs, fix_job_kwargs
+from .job_tools import split_job_kwargs
+from .core_tools import convert_bytes_to_str, convert_seconds_to_str
 
 from warnings import warn
 
 
 class BaseRecording(BaseRecordingSnippets):
     """
     Abstract class representing several a multichannel timeseries (or block of raw ephys traces).
     Internally handle list of RecordingSegment
     """
-    _main_annotations = ['is_filtered']
-    _main_properties = ['group', 'location', 'gain_to_uV', 'offset_to_uV']
+
+    _main_annotations = ["is_filtered"]
+    _main_properties = ["group", "location", "gain_to_uV", "offset_to_uV"]
     _main_features = []  # recording do not handle features
-    
-    def __init__(self, sampling_frequency: float, channel_ids: List, dtype):
-        BaseRecordingSnippets.__init__(self, 
-                                       channel_ids=channel_ids, 
-                                       sampling_frequency=sampling_frequency, 
-                                       dtype=dtype)
 
-        self.is_dumpable = True
+    _skip_properties = ["noise_level_raw", "noise_level_scaled"]
+
+    def __init__(self, sampling_frequency: float, channel_ids: List, dtype):
+        BaseRecordingSnippets.__init__(
+            self, channel_ids=channel_ids, sampling_frequency=sampling_frequency, dtype=dtype
+        )
 
         self._recording_segments: List[BaseRecordingSegment] = []
 
         # initialize main annotation and properties
         self.annotate(is_filtered=False)
 
     def __repr__(self):
-        clsname = self.__class__.__name__
-        nseg = self.get_num_segments()
-        nchan = self.get_num_channels()
-        sf_khz = self.get_sampling_frequency() / 1000.
-        duration = self.get_total_duration()
-        txt = f'{clsname}: {nchan} channels - {nseg} segments - {sf_khz:0.1f}kHz - {duration:0.3f}s'
-        if 'file_paths' in self._kwargs:
-            txt += '\n  file_paths: {}'.format(self._kwargs['file_paths'])
-        if 'file_path' in self._kwargs:
-            txt += '\n  file_path: {}'.format(self._kwargs['file_path'])
+        extractor_name = self.__class__.__name__
+        num_segments = self.get_num_segments()
+        num_channels = self.get_num_channels()
+        sf_khz = self.get_sampling_frequency() / 1000.0
+        dtype = self.get_dtype()
+
+        total_samples = self.get_total_samples()
+        total_duration = self.get_total_duration()
+        total_memory_size = self.get_total_memory_size()
+
+        txt = (
+            f"{extractor_name}: "
+            f"{num_channels} channels - "
+            f"{sf_khz:0.1f}kHz - "
+            f"{num_segments} segments - "
+            f"{total_samples:,} samples - "
+            f"{convert_seconds_to_str(total_duration)} - "
+            f"{dtype} dtype - "
+            f"{convert_bytes_to_str(total_memory_size)}"
+        )
+
+        # Split if too long
+        if len(txt) > 100:
+            split_index = txt.rfind("-", 0, 100)  # Find the last "-" before character 100
+            if split_index != -1:
+                first_line = txt[:split_index]
+                recording_string_space = len(extractor_name) + 2  # Length of extractor_name plus ": "
+                white_space_to_align_with_first_line = " " * recording_string_space
+                second_line = white_space_to_align_with_first_line + txt[split_index + 1 :].lstrip()
+                txt = first_line + "\n" + second_line
+
+        # Add segments info for multisegment
+        if num_segments > 1:
+            samples_per_segment = [self.get_num_samples(segment_index) for segment_index in range(num_segments)]
+            memory_per_segment_bytes = (self.get_memory_size(segment_index) for segment_index in range(num_segments))
+            durations = [self.get_duration(segment_index) for segment_index in range(num_segments)]
+
+            samples_per_segment_formated = [f"{samples:,}" for samples in samples_per_segment]
+            durations_per_segment_formated = [convert_seconds_to_str(d) for d in durations]
+            memory_per_segment_formated = [convert_bytes_to_str(mem) for mem in memory_per_segment_bytes]
+
+            def list_to_string(lst, max_size=6):
+                """Add elipsis ... notation in the middle if recording has more than six segments"""
+                if len(lst) <= max_size:
+                    return " | ".join(x for x in lst)
+                else:
+                    half = max_size // 2
+                    return " | ".join(x for x in lst[:half]) + " | ... | " + " | ".join(x for x in lst[-half:])
+
+            txt += (
+                f"\n"
+                f"Segments:"
+                f"\nSamples:   {list_to_string(samples_per_segment_formated)}"
+                f"\nDurations: {list_to_string(durations_per_segment_formated)}"
+                f"\nMemory:    {list_to_string(memory_per_segment_formated)}"
+            )
+
+        # Display where path from where recording was loaded
+        if "file_paths" in self._kwargs:
+            txt += f"\n  file_paths: {self._kwargs['file_paths']}"
+        if "file_path" in self._kwargs:
+            txt += f"\n  file_path: {self._kwargs['file_path']}"
+
         return txt
 
-    def get_num_segments(self):
-        """Returns the number of segments.
+    def get_num_segments(self) -> int:
+        """
+        Returns the number of segments.
 
         Returns
         -------
         int
             Number of segments in the recording
         """
         return len(self._recording_segments)
@@ -67,66 +122,129 @@
         recording_segment : BaseRecordingSegment
             The recording segment to add
         """
         # todo: check channel count and sampling frequency
         self._recording_segments.append(recording_segment)
         recording_segment.set_parent_extractor(self)
 
-    def get_num_samples(self, segment_index=None):
-        """Returns the number of samples for a segment.
+    def get_num_samples(self, segment_index=None) -> int:
+        """
+        Returns the number of samples for a segment.
 
         Parameters
         ----------
         segment_index : int, optional
-            The segment index to retrieve the number of samples for. 
+            The segment index to retrieve the number of samples for.
             For multi-segment objects, it is required, by default None
+            With single segment recording returns the number of samples in the segment
 
         Returns
         -------
         int
             The number of samples
         """
         segment_index = self._check_segment_index(segment_index)
         return self._recording_segments[segment_index].get_num_samples()
 
     get_num_frames = get_num_samples
 
-    def get_total_samples(self):
-        """Returns the total number of samples
+    def get_total_samples(self) -> int:
+        """
+        Returns the sum of the number of samples in each segment.
 
         Returns
         -------
         int
             The total number of samples
         """
-        s = 0
-        for segment_index in range(self.get_num_segments()):
-            s += self.get_num_samples(segment_index)
-        return s
+        num_segments = self.get_num_segments()
+        samples_per_segment = (self.get_num_samples(segment_index) for segment_index in range(num_segments))
+
+        return sum(samples_per_segment)
+
+    def get_duration(self, segment_index=None) -> float:
+        """
+        Returns the duration in seconds.
+
+        Parameters
+        ----------
+        segment_index : int, optional
+            The sample index to retrieve the duration for.
+            For multi-segment objects, it is required, by default None
+            With single segment recording returns the duration of the single segment
+
+        Returns
+        -------
+        float
+            The duration in seconds
+        """
+        segment_index = self._check_segment_index(segment_index)
+        segment_num_samples = self.get_num_samples(segment_index=segment_index)
+        segment_duration = segment_num_samples / self.get_sampling_frequency()
+        return segment_duration
 
-    def get_total_duration(self):
-        """Returns the total duration in s
+    def get_total_duration(self) -> float:
+        """
+        Returns the total duration in seconds
 
         Returns
         -------
         float
             The duration in seconds
         """
         duration = self.get_total_samples() / self.get_sampling_frequency()
         return duration
 
-    def get_traces(self,
-                   segment_index: Union[int, None] = None,
-                   start_frame: Union[int, None] = None,
-                   end_frame: Union[int, None] = None,
-                   channel_ids: Union[Iterable, None] = None,
-                   order: Union[str, None] = None,
-                   return_scaled=False,
-                   cast_unsigned=False
-                   ):
+    def get_memory_size(self, segment_index=None) -> int:
+        """
+        Returns the memory size of segment_index in bytes.
+
+        Parameters
+        ----------
+        segment_index : int, optional
+            The index of the segment for which the memory size should be calculated.
+            For multi-segment objects, it is required, by default None
+            With single segment recording returns the memory size of the single segment
+
+        Returns
+        -------
+        int
+            The memory size of the specified segment in bytes.
+        """
+        segment_index = self._check_segment_index(segment_index)
+        num_samples = self.get_num_samples(segment_index=segment_index)
+        num_channels = self.get_num_channels()
+        dtype_size_bytes = self.get_dtype().itemsize
+
+        memory_bytes = num_samples * num_channels * dtype_size_bytes
+
+        return memory_bytes
+
+    def get_total_memory_size(self) -> int:
+        """
+        Returns the sum in bytes of all the memory sizes of the segments.
+
+        Returns
+        -------
+        int
+            The total memory size in bytes for all segments.
+        """
+        memory_per_segment = (self.get_memory_size(segment_index) for segment_index in range(self.get_num_segments()))
+        return sum(memory_per_segment)
+
+    def get_traces(
+        self,
+        segment_index: Union[int, None] = None,
+        start_frame: Union[int, None] = None,
+        end_frame: Union[int, None] = None,
+        channel_ids: Union[Iterable, None] = None,
+        order: Union[str, None] = None,
+        return_scaled=False,
+        cast_unsigned=False,
+    ):
         """Returns traces from recording.
 
         Parameters
         ----------
         segment_index : Union[int, None], optional
             The segment index to get traces from. If recording is multi-segment, it is required, by default None
         start_frame : Union[int, None], optional
@@ -137,15 +255,15 @@
             The channel ids. If None, all channels are used, by default None
         order : Union[str, None], optional
             The order of the traces ("C" | "F"). If None, traces are returned as they are, by default None
         return_scaled : bool, optional
             If True and the recording has scaling (gain_to_uV and offset_to_uV properties),
             traces are scaled to uV, by default False
         cast_unsigned : bool, optional
-            If True and the traces are unsigned, they are cast to integer and centered 
+            If True and the traces are unsigned, they are cast to integer and centered
             (an offset of (2**nbits) is subtracted), by default False
 
         Returns
         -------
         np.array
             The traces (num_samples, num_channels)
 
@@ -172,40 +290,67 @@
                 # upcast to int with double itemsize
                 traces = traces.astype(f"int{2 * (dtype.itemsize) * 8}") - 2 ** (nbits - 1)
                 traces = traces.astype(f"int{dtype.itemsize * 8}")
 
         if return_scaled:
             if hasattr(self, "NeoRawIOClass"):
                 if self.has_non_standard_units:
-                    message = ( 
-                    f'This extractor based on neo.{self.NeoRawIOClass} has channels with units not in (V, mV, uV)'
+                    message = (
+                        f"This extractor based on neo.{self.NeoRawIOClass} has channels with units not in (V, mV, uV)"
                     )
                     warnings.warn(message)
-            
+
             if not self.has_scaled():
-                raise ValueError('This recording do not support return_scaled=True (need gain_to_uV and offset_'
-                                 'to_uV properties)')
+                raise ValueError(
+                    "This recording do not support return_scaled=True (need gain_to_uV and offset_" "to_uV properties)"
+                )
             else:
-                gains = self.get_property('gain_to_uV')
-                offsets = self.get_property('offset_to_uV')
-                gains = gains[channel_indices].astype('float32')
-                offsets = offsets[channel_indices].astype('float32')
-                traces = traces.astype('float32') * gains + offsets
+                gains = self.get_property("gain_to_uV")
+                offsets = self.get_property("offset_to_uV")
+                gains = gains[channel_indices].astype("float32")
+                offsets = offsets[channel_indices].astype("float32")
+                traces = traces.astype("float32") * gains + offsets
         return traces
-    
+
     def has_scaled_traces(self):
         """Checks if the recording has scaled traces
 
         Returns
         -------
         bool
             True if the recording has scaled traces, False otherwise
         """
         return self.has_scaled()
 
+    def get_time_info(self, segment_index=None) -> dict:
+        """
+        Retrieves the timing attributes for a given segment index. As with
+        other recorders this method only needs a segment index in the case
+        of multi-segment recordings.
+
+        Returns
+        -------
+        dict
+            A dictionary containing the following key-value pairs:
+
+            - 'sampling_frequency': The sampling frequency of the RecordingSegment.
+            - 't_start': The start time of the RecordingSegment.
+            - 'time_vector': The time vector of the RecordingSegment.
+
+        Notes
+        -----
+        The keys are always present, but the values may be None.
+        """
+
+        segment_index = self._check_segment_index(segment_index)
+        rs = self._recording_segments[segment_index]
+        time_kwargs = rs.get_times_kwargs()
+
+        return time_kwargs
+
     def get_times(self, segment_index=None):
         """Get time vector for a recording segment.
 
         If the segment has a time_vector, then it is returned. Otherwise
         a time_vector is constructed on the fly with sampling frequency.
         If t_start is defined and the time vector is constructed on the fly,
         the first time will be t_start. Otherwise it will start from 0.
@@ -237,15 +382,15 @@
         -------
         bool
             True if the recording has time vectors, False otherwise
         """
         segment_index = self._check_segment_index(segment_index)
         rs = self._recording_segments[segment_index]
         d = rs.get_times_kwargs()
-        return d['time_vector'] is not None
+        return d["time_vector"] is not None
 
     def set_times(self, times, segment_index=None, with_warning=True):
         """Set times for a recording segment.
 
         Parameters
         ----------
         times : 1d np.array
@@ -254,231 +399,260 @@
             The segment index (required for multi-segment), by default None
         with_warning : bool, optional
             If True, a warning is printed, by default True
         """
         segment_index = self._check_segment_index(segment_index)
         rs = self._recording_segments[segment_index]
 
-        assert times.ndim == 1, 'Time must have ndim=1'
-        assert rs.get_num_samples() == times.shape[0], 'times have wrong shape'
+        assert times.ndim == 1, "Time must have ndim=1"
+        assert rs.get_num_samples() == times.shape[0], "times have wrong shape"
 
         rs.t_start = None
-        rs.time_vector = times.astype('float64')
+        rs.time_vector = times.astype("float64")
 
         if with_warning:
-            warn('Setting times with Recording.set_times() is not recommended because '
-                          'times are not always propagated to across preprocessing'
-                          'Use use this carefully!')
+            warn(
+                "Setting times with Recording.set_times() is not recommended because "
+                "times are not always propagated to across preprocessing"
+                "Use use this carefully!"
+            )
 
-    def _save(self, format='binary', **save_kwargs):
+    def _save(self, format="binary", **save_kwargs):
         """
         This function replaces the old CacheRecordingExtractor, but enables more engines
         for caching a results. At the moment only 'binary' with memmap is supported.
         We plan to add other engines, such as zarr and NWB.
         """
 
         # handle t_starts
         t_starts = []
         has_time_vectors = []
         for segment_index, rs in enumerate(self._recording_segments):
             d = rs.get_times_kwargs()
-            t_starts.append(d['t_start'])
-            has_time_vectors.append(d['time_vector'] is not None)
+            t_starts.append(d["t_start"])
+            has_time_vectors.append(d["time_vector"] is not None)
 
         if all(t_start is None for t_start in t_starts):
             t_starts = None
 
         kwargs, job_kwargs = split_job_kwargs(save_kwargs)
 
-        if format == 'binary':
-            folder = kwargs['folder']
-            file_paths = [folder / f'traces_cached_seg{i}.raw' for i in range(self.get_num_segments())]
-            dtype = kwargs.get('dtype', None) or self.get_dtype()
+        if format == "binary":
+            folder = kwargs["folder"]
+            file_paths = [folder / f"traces_cached_seg{i}.raw" for i in range(self.get_num_segments())]
+            dtype = kwargs.get("dtype", None) or self.get_dtype()
 
             write_binary_recording(self, file_paths=file_paths, dtype=dtype, **job_kwargs)
 
             from .binaryrecordingextractor import BinaryRecordingExtractor
-            binary_rec = BinaryRecordingExtractor(file_paths=file_paths, sampling_frequency=self.get_sampling_frequency(),
-                                                  num_chan=self.get_num_channels(), dtype=dtype,
-                                                  t_starts=t_starts, channel_ids=self.get_channel_ids(), time_axis=0,
-                                                  file_offset=0, gain_to_uV=self.get_channel_gains(),
-                                                  offset_to_uV=self.get_channel_offsets())
-            binary_rec.dump(folder / 'binary.json', relative_to=folder)
+
+            binary_rec = BinaryRecordingExtractor(
+                file_paths=file_paths,
+                sampling_frequency=self.get_sampling_frequency(),
+                num_channels=self.get_num_channels(),
+                dtype=dtype,
+                t_starts=t_starts,
+                channel_ids=self.get_channel_ids(),
+                time_axis=0,
+                file_offset=0,
+                gain_to_uV=self.get_channel_gains(),
+                offset_to_uV=self.get_channel_offsets(),
+            )
+            binary_rec.dump(folder / "binary.json", relative_to=folder)
 
             from .binaryfolder import BinaryFolderRecording
+
             cached = BinaryFolderRecording(folder_path=folder)
 
-        elif format == 'memory':
+        elif format == "memory":
             traces_list = write_memory_recording(self, dtype=None, **job_kwargs)
             from .numpyextractors import NumpyRecording
-            cached = NumpyRecording(traces_list, self.get_sampling_frequency(), t_starts=t_starts, channel_ids=self.channel_ids)
 
-        elif format == 'zarr':
+            cached = NumpyRecording(
+                traces_list, self.get_sampling_frequency(), t_starts=t_starts, channel_ids=self.channel_ids
+            )
+
+        elif format == "zarr":
             from .zarrrecordingextractor import get_default_zarr_compressor, ZarrRecordingExtractor
+
             zarr_kwargs = kwargs.copy()
-            
-            zarr_root = zarr_kwargs['zarr_root']
+
+            zarr_root = zarr_kwargs["zarr_root"]
             zarr_root.attrs["sampling_frequency"] = float(self.get_sampling_frequency())
             zarr_root.attrs["num_segments"] = int(self.get_num_segments())
             zarr_root.create_dataset(name="channel_ids", data=self.get_channel_ids(), compressor=None)
 
-            zarr_kwargs['dataset_paths'] = [f'traces_seg{i}' for i in range(self.get_num_segments())]
-            zarr_kwargs['dtype'] = kwargs.get('dtype', None) or self.get_dtype()
-            
-            if 'compressor' not in zarr_kwargs:
-                zarr_kwargs['compressor'] = compressor = get_default_zarr_compressor()
-                print(f"Using default zarr compressor: {compressor}. To use a different compressor, use the "
-                      f"'compressor' argument")
+            zarr_kwargs["dataset_paths"] = [f"traces_seg{i}" for i in range(self.get_num_segments())]
+            zarr_kwargs["dtype"] = kwargs.get("dtype", None) or self.get_dtype()
+
+            if "compressor" not in zarr_kwargs:
+                zarr_kwargs["compressor"] = compressor = get_default_zarr_compressor()
+                print(
+                    f"Using default zarr compressor: {compressor}. To use a different compressor, use the "
+                    f"'compressor' argument"
+                )
 
             write_traces_to_zarr(self, **zarr_kwargs, **job_kwargs)
 
             # save probe
-            if self.get_property('contact_vector') is not None:
+            if self.get_property("contact_vector") is not None:
                 probegroup = self.get_probegroup()
                 zarr_root.attrs["probe"] = check_json(probegroup.to_dict(array_as_list=True))
 
             # save time vector if any
-            t_starts = np.zeros(self.get_num_segments(), dtype='float64') * np.nan
+            t_starts = np.zeros(self.get_num_segments(), dtype="float64") * np.nan
             for segment_index, rs in enumerate(self._recording_segments):
                 d = rs.get_times_kwargs()
-                time_vector = d['time_vector']
+                time_vector = d["time_vector"]
                 if time_vector is not None:
-                    _ = zarr_root.create_dataset(name=f'times_seg{segment_index}', data=time_vector,
-                                                 filters=zarr_kwargs.get('filters', None),
-                                                 compressor=zarr_kwargs['compressor'])
+                    _ = zarr_root.create_dataset(
+                        name=f"times_seg{segment_index}",
+                        data=time_vector,
+                        filters=zarr_kwargs.get("filters", None),
+                        compressor=zarr_kwargs["compressor"],
+                    )
                 elif d["t_start"] is not None:
                     t_starts[segment_index] = d["t_start"]
 
             if np.any(~np.isnan(t_starts)):
-                zarr_root.create_dataset(name="t_starts", data=t_starts,
-                                         compressor=None)
+                zarr_root.create_dataset(name="t_starts", data=t_starts, compressor=None)
 
-            cached = ZarrRecordingExtractor(zarr_kwargs['zarr_path'], zarr_kwargs['storage_options'])
+            cached = ZarrRecordingExtractor(zarr_kwargs["zarr_path"], zarr_kwargs["storage_options"])
 
-        elif format == 'nwb':
+        elif format == "nwb":
             # TODO implement a format based on zarr
             raise NotImplementedError
 
         else:
-            raise ValueError(f'format {format} not supported')
+            raise ValueError(f"format {format} not supported")
 
-        if self.get_property('contact_vector') is not None:
+        if self.get_property("contact_vector") is not None:
             probegroup = self.get_probegroup()
             cached.set_probegroup(probegroup)
 
         for segment_index, rs in enumerate(self._recording_segments):
             d = rs.get_times_kwargs()
-            time_vector = d['time_vector']
+            time_vector = d["time_vector"]
             if time_vector is not None:
                 cached._recording_segments[segment_index].time_vector = time_vector
 
         return cached
 
     def _extra_metadata_from_folder(self, folder):
         # load probe
         folder = Path(folder)
-        if (folder / 'probe.json').is_file():
-            probegroup = read_probeinterface(folder / 'probe.json')
+        if (folder / "probe.json").is_file():
+            probegroup = read_probeinterface(folder / "probe.json")
             self.set_probegroup(probegroup, in_place=True)
 
         # load time vector if any
         for segment_index, rs in enumerate(self._recording_segments):
-            time_file = folder / f'times_cached_seg{segment_index}.npy'
+            time_file = folder / f"times_cached_seg{segment_index}.npy"
             if time_file.is_file():
                 time_vector = np.load(time_file)
                 rs.time_vector = time_vector
 
     def _extra_metadata_to_folder(self, folder):
         # save probe
-        if self.get_property('contact_vector') is not None:
+        if self.get_property("contact_vector") is not None:
             probegroup = self.get_probegroup()
-            write_probeinterface(folder / 'probe.json', probegroup)
+            write_probeinterface(folder / "probe.json", probegroup)
 
         # save time vector if any
         for segment_index, rs in enumerate(self._recording_segments):
             d = rs.get_times_kwargs()
-            time_vector = d['time_vector']
+            time_vector = d["time_vector"]
             if time_vector is not None:
-                np.save(folder / f'times_cached_seg{segment_index}.npy', time_vector)
+                np.save(folder / f"times_cached_seg{segment_index}.npy", time_vector)
 
     def _channel_slice(self, channel_ids, renamed_channel_ids=None):
         from .channelslice import ChannelSliceRecording
+
         sub_recording = ChannelSliceRecording(self, channel_ids, renamed_channel_ids=renamed_channel_ids)
         return sub_recording
-    
+
     def _remove_channels(self, remove_channel_ids):
         from .channelslice import ChannelSliceRecording
+
         new_channel_ids = self.channel_ids[~np.in1d(self.channel_ids, remove_channel_ids)]
         sub_recording = ChannelSliceRecording(self, new_channel_ids)
         return sub_recording
 
     def _frame_slice(self, start_frame, end_frame):
         from .frameslicerecording import FrameSliceRecording
+
         sub_recording = FrameSliceRecording(self, start_frame=start_frame, end_frame=end_frame)
         return sub_recording
 
     def _select_segments(self, segment_indices):
         from .segmentutils import SelectSegmentRecording
+
         return SelectSegmentRecording(self, segment_indices=segment_indices)
 
     def is_binary_compatible(self):
         """
         Inform is this recording is "binary" compatible.
         To be used before calling `rec.get_binary_description()`
-        
+
         Returns
         -------
         bool
             True if the underlying recording is binary
         """
         # has to be changed in subclass if yes
         return False
-        
+
     def get_binary_description(self):
         """
         When `rec.is_binary_compatible()` is True
         this returns a dictionary describing the binary format.
         """
         if not self.is_binary_compatible:
             raise NotImplementedError
-    
-    def binary_compatible_with(self, dtype=None, time_axis=None, file_paths_lenght=None, 
-                               file_offset=None, file_suffix=None):
+
+    def binary_compatible_with(
+        self, dtype=None, time_axis=None, file_paths_lenght=None, file_offset=None, file_suffix=None
+    ):
         """
         Check is the recording is binary compatible with some constrain on
 
           * dtype
           * tim_axis
           * len(file_paths)
           * file_offset
           * file_suffix
         """
         if not self.is_binary_compatible():
             return False
-        
+
         d = self.get_binary_description()
-        
-        if dtype is not None and dtype != d['dtype']:
+
+        if dtype is not None and dtype != d["dtype"]:
             return False
-        
-        if time_axis is not None and time_axis != d['time_axis']:
+
+        if time_axis is not None and time_axis != d["time_axis"]:
             return False
-        
-        if file_paths_lenght is not None and file_paths_lenght != len(d['file_paths']):
+
+        if file_paths_lenght is not None and file_paths_lenght != len(d["file_paths"]):
             return False
-        
-        if file_offset is not None and file_offset != d['file_offset']:
+
+        if file_offset is not None and file_offset != d["file_offset"]:
             return False
-        
-        if file_suffix is not None and not all(Path(e).suffix == file_suffix  for e in d['file_paths']):
+
+        if file_suffix is not None and not all(Path(e).suffix == file_suffix for e in d["file_paths"]):
             return False
 
         # good job you pass all crucible
         return True
 
+    def astype(self, dtype):
+        from ..preprocessing.astype import astype
+
+        return astype(self, dtype=dtype)
+
 
 class BaseRecordingSegment(BaseSegment):
     """
     Abstract class representing a multichannel timeseries, or block of raw ephys traces
     """
 
     def __init__(self, sampling_frequency=None, t_start=None, time_vector=None):
@@ -499,25 +673,41 @@
     def get_times(self):
         if self.time_vector is not None:
             if isinstance(self.time_vector, np.ndarray):
                 return self.time_vector
             else:
                 return np.array(self.time_vector)
         else:
-            time_vector = np.arange(self.get_num_samples(), dtype='float64')
+            time_vector = np.arange(self.get_num_samples(), dtype="float64")
             time_vector /= self.sampling_frequency
             if self.t_start is not None:
                 time_vector += self.t_start
-            return time_vector 
+            return time_vector
+
+    def get_times_kwargs(self) -> dict:
+        """
+        Retrieves the timing attributes characterizing a RecordingSegment
+
+        Returns
+        -------
+        dict
+            A dictionary containing the following key-value pairs:
 
-    def get_times_kwargs(self):
-        # useful for other internal RecordingSegment
-        d = dict(sampling_frequency=self.sampling_frequency, t_start=self.t_start,
-                 time_vector=self.time_vector)
-        return d
+            - 'sampling_frequency': The sampling frequency of the RecordingSegment.
+            - 't_start': The start time of the RecordingSegment.
+            - 'time_vector': The time vector of the RecordingSegment.
+
+        Notes
+        -----
+        The keys are always present, but the values may be None.
+        """
+        time_kwargs = dict(
+            sampling_frequency=self.sampling_frequency, t_start=self.t_start, time_vector=self.time_vector
+        )
+        return time_kwargs
 
     def sample_index_to_time(self, sample_ind):
         """
         Transform sample index into time in seconds
         """
         if self.time_vector is None:
             time_s = sample_ind / self.sampling_frequency
@@ -533,31 +723,32 @@
         """
         if self.time_vector is None:
             if self.t_start is None:
                 sample_index = time_s * self.sampling_frequency
             else:
                 sample_index = (time_s - self.t_start) * self.sampling_frequency
         else:
-            sample_index = np.searchsorted(self.time_vector, time_s, side='right') - 1
+            sample_index = np.searchsorted(self.time_vector, time_s, side="right") - 1
         return int(sample_index)
 
     def get_num_samples(self) -> int:
         """Returns the number of samples in this signal segment
 
         Returns:
             SampleIndex: Number of samples in the signal segment
         """
         # must be implemented in subclass
         raise NotImplementedError
 
-    def get_traces(self,
-                   start_frame: Union[int, None] = None,
-                   end_frame: Union[int, None] = None,
-                   channel_indices: Union[List, None] = None,
-                   ) -> np.ndarray:
+    def get_traces(
+        self,
+        start_frame: Union[int, None] = None,
+        end_frame: Union[int, None] = None,
+        channel_indices: Union[List, None] = None,
+    ) -> np.ndarray:
         """
         Return the raw traces, optionally for a subset of samples and/or channels
 
         Parameters
         ----------
         start_frame: (Union[int, None], optional)
             start sample index, or zero if None. Defaults to None.
```

### Comparing `spikeinterface-0.97.1/spikeinterface/core/baserecordingsnippets.py` & `spikeinterface-0.98.0/src/spikeinterface/core/baserecordingsnippets.py`

 * *Files 6% similar despite different names*

```diff
@@ -12,75 +12,81 @@
 from warnings import warn
 
 
 class BaseRecordingSnippets(BaseExtractor):
     """
     Mixin that handles all probe and channel operations
     """
+
     has_default_locations = False
 
     def __init__(self, sampling_frequency: float, channel_ids: List, dtype):
         BaseExtractor.__init__(self, channel_ids)
         self._sampling_frequency = sampling_frequency
         self._dtype = np.dtype(dtype)
 
     @property
     def channel_ids(self):
         return self._main_ids
-    
+
     @property
     def sampling_frequency(self):
         return self._sampling_frequency
-    
+
     @property
     def dtype(self):
         return self._dtype
-    
+
     def get_sampling_frequency(self):
         return self._sampling_frequency
 
     def get_channel_ids(self):
         return self._main_ids
 
     def get_num_channels(self):
         return len(self.get_channel_ids())
-        
+
     def get_dtype(self):
         return self._dtype
-    
+
     def has_scaled(self):
-        if self.get_property('gain_to_uV') is None or self.get_property('offset_to_uV') is None:
+        if self.get_property("gain_to_uV") is None or self.get_property("offset_to_uV") is None:
             return False
         else:
             return True
 
+    def has_probe(self):
+        return "contact_vector" in self.get_property_keys()
+
+    def has_channel_location(self):
+        return self.has_probe() or "channel_location" in self.get_property_keys()
+
     def is_filtered(self):
         # the is_filtered is handle with annotation
-        return self._annotations.get('is_filtered', False)
-    
-    
+        return self._annotations.get("is_filtered", False)
+
     def _channel_slice(self, channel_ids, renamed_channel_ids=None):
         raise NotImplementedError
-    
+
     def _frame_slice(self, channel_ids, renamed_channel_ids=None):
         raise NotImplementedError
-    
-    def set_probe(self, probe, group_mode='by_probe', in_place=False):
+
+    def set_probe(self, probe, group_mode="by_probe", in_place=False):
         """
         Wrapper on top on set_probes when there one unique probe.
         """
-        assert isinstance(probe, Probe), 'must give Probe'
+        assert isinstance(probe, Probe), "must give Probe"
         probegroup = ProbeGroup()
         probegroup.add_probe(probe)
         return self.set_probes(probegroup, group_mode=group_mode, in_place=in_place)
 
-    def set_probegroup(self, probegroup, group_mode='by_probe', in_place=False):
+    def set_probegroup(self, probegroup, group_mode="by_probe", in_place=False):
         return self.set_probes(probegroup, group_mode=group_mode, in_place=in_place)
 
-    def set_probes(self, probe_or_probegroup, group_mode='by_probe', in_place=False):
+    def set_probes(self, probe_or_probegroup, group_mode="by_probe", in_place=False):
         """
         Attach a Probe to a recording.
         For this Probe.device_channel_indices is used to link contacts to recording channels.
         If some contacts of the Probe are not connected (device_channel_indices=-1)
         then the recording is "sliced" and only connected channel are kept.
 
         The probe order is not kept. Channel ids are re-ordered to match the channel_ids of the recording.
@@ -98,263 +104,280 @@
             Useful internally when extractor do self.set_probegroup(probe)
 
         Returns
         -------
         sub_recording: BaseRecording
             A view of the recording (ChannelSlice or clone or itself)
         """
-        assert group_mode in (
-            'by_probe', 'by_shank'), "'group_mode' can be 'by_probe' or 'by_shank'"
+        assert group_mode in ("by_probe", "by_shank"), "'group_mode' can be 'by_probe' or 'by_shank'"
 
         # handle several input possibilities
         if isinstance(probe_or_probegroup, Probe):
             probegroup = ProbeGroup()
             probegroup.add_probe(probe_or_probegroup)
         elif isinstance(probe_or_probegroup, ProbeGroup):
             probegroup = probe_or_probegroup
         elif isinstance(probe_or_probegroup, list):
             assert all([isinstance(e, Probe) for e in probe_or_probegroup])
             probegroup = ProbeGroup()
             for probe in probe_or_probegroup:
                 probegroup.add_probe(probe)
         else:
-            raise ValueError('must give Probe or ProbeGroup or list of Probe')
+            raise ValueError("must give Probe or ProbeGroup or list of Probe")
 
         # handle not connected channels
-        assert all(probe.device_channel_indices is not None for probe in probegroup.probes), \
-            'Probe must have device_channel_indices'
+        assert all(
+            probe.device_channel_indices is not None for probe in probegroup.probes
+        ), "Probe must have device_channel_indices"
 
         # this is a vector with complex fileds (dataframe like) that handle all contact attr
-        arr = probegroup.to_numpy(complete=True)
+        probe_as_numpy_array = probegroup.to_numpy(complete=True)
 
         # keep only connected contact ( != -1)
-        keep = arr['device_channel_indices'] >= 0
+        keep = probe_as_numpy_array["device_channel_indices"] >= 0
         if np.any(~keep):
-            warn('The given probes have unconnected contacts: they are removed')
+            warn("The given probes have unconnected contacts: they are removed")
 
-        arr = arr[keep]
-        inds = arr['device_channel_indices']
-        order = np.argsort(inds)
-        inds = inds[order]
-
-        # check
-        if np.max(list(inds) + [0]) >= self.get_num_channels():
-            raise ValueError(
-                'The given Probe have "device_channel_indices" that do not match channel count')
-        new_channel_ids = self.get_channel_ids()[inds]
-        arr = arr[order]
-        arr['device_channel_indices'] = np.arange(arr.size, dtype='int64')
+        probe_as_numpy_array = probe_as_numpy_array[keep]
+        device_channel_indices = probe_as_numpy_array["device_channel_indices"]
+        order = np.argsort(device_channel_indices)
+        device_channel_indices = device_channel_indices[order]
+
+        # check TODO: Where did this came from?
+        number_of_device_channel_indices = np.max(list(device_channel_indices) + [0])
+        if number_of_device_channel_indices >= self.get_num_channels():
+            error_msg = (
+                f"The given Probe have 'device_channel_indices' that do not match channel count \n"
+                f"{number_of_device_channel_indices} vs {self.get_num_channels()} \n"
+                f"device_channel_indices are the following: {device_channel_indices} \n"
+                f"recording channels are the following: {self.get_channel_ids()} \n"
+            )
+            raise ValueError(error_msg)
+
+        new_channel_ids = self.get_channel_ids()[device_channel_indices]
+        probe_as_numpy_array = probe_as_numpy_array[order]
+        probe_as_numpy_array["device_channel_indices"] = np.arange(probe_as_numpy_array.size, dtype="int64")
 
         # create recording : channel slice or clone or self
         if in_place:
             if not np.array_equal(new_channel_ids, self.get_channel_ids()):
-                raise Exception(
-                    'set_probe(inplace=True) must have all channel indices')
+                raise Exception("set_probe(inplace=True) must have all channel indices")
             sub_recording = self
         else:
             if np.array_equal(new_channel_ids, self.get_channel_ids()):
                 sub_recording = self.clone()
             else:
                 sub_recording = self.channel_slice(new_channel_ids)
 
         # create a vector that handle all contacts in property
-        sub_recording.set_property('contact_vector', arr, ids=None)
+        sub_recording.set_property("contact_vector", probe_as_numpy_array, ids=None)
 
         # planar_contour is saved in annotations
         for probe_index, probe in enumerate(probegroup.probes):
             contour = probe.probe_planar_contour
             if contour is not None:
-                sub_recording.set_annotation(
-                    f'probe_{probe_index}_planar_contour', contour, overwrite=True)
+                sub_recording.set_annotation(f"probe_{probe_index}_planar_contour", contour, overwrite=True)
 
         # duplicate positions to "locations" property
         ndim = probegroup.ndim
-        locations = np.zeros((arr.size, ndim), dtype='float64')
-        for i, dim in enumerate(['x', 'y', 'z'][:ndim]):
-            locations[:, i] = arr[dim]
-        sub_recording.set_property('location', locations, ids=None)
+        locations = np.zeros((probe_as_numpy_array.size, ndim), dtype="float64")
+        for i, dim in enumerate(["x", "y", "z"][:ndim]):
+            locations[:, i] = probe_as_numpy_array[dim]
+        sub_recording.set_property("location", locations, ids=None)
 
         # handle groups
-        groups = np.zeros(arr.size, dtype='int64')
-        if group_mode == 'by_probe':
-            for group, probe_index in enumerate(np.unique(arr['probe_index'])):
-                mask = arr['probe_index'] == probe_index
+        groups = np.zeros(probe_as_numpy_array.size, dtype="int64")
+        if group_mode == "by_probe":
+            for group, probe_index in enumerate(np.unique(probe_as_numpy_array["probe_index"])):
+                mask = probe_as_numpy_array["probe_index"] == probe_index
                 groups[mask] = group
-        elif group_mode == 'by_shank':
-            assert all(probe.shank_ids is not None for probe in probegroup.probes), \
-                'shank_ids is None in probe, you cannot group by shank'
-            for group, a in enumerate(np.unique(arr[['probe_index', 'shank_ids']])):
-                mask = (arr['probe_index'] == a['probe_index']) & (
-                    arr['shank_ids'] == a['shank_ids'])
+        elif group_mode == "by_shank":
+            assert all(
+                probe.shank_ids is not None for probe in probegroup.probes
+            ), "shank_ids is None in probe, you cannot group by shank"
+            for group, a in enumerate(np.unique(probe_as_numpy_array[["probe_index", "shank_ids"]])):
+                mask = (probe_as_numpy_array["probe_index"] == a["probe_index"]) & (
+                    probe_as_numpy_array["shank_ids"] == a["shank_ids"]
+                )
                 groups[mask] = group
-        sub_recording.set_property('group', groups, ids=None)
+        sub_recording.set_property("group", groups, ids=None)
 
         # add probe annotations to recording
         probes_info = []
         for probe in probegroup.probes:
-            probes_info.append(check_json(probe.annotations))
+            probes_info.append(probe.annotations)
         self.annotate(probes_info=probes_info)
 
         return sub_recording
 
     def get_probe(self):
         probes = self.get_probes()
-        assert len(
-            probes) == 1, 'there are several probe use .get_probes() or get_probegroup()'
+        assert len(probes) == 1, "there are several probe use .get_probes() or get_probegroup()"
         return probes[0]
 
     def get_probes(self):
         probegroup = self.get_probegroup()
         return probegroup.probes
 
     def get_probegroup(self):
-        arr = self.get_property('contact_vector')
+        arr = self.get_property("contact_vector")
         if arr is None:
-            positions = self.get_property('location')
+            positions = self.get_property("location")
             if positions is None:
-                raise ValueError(
-                    'There is no Probe attached to this recording. Use set_probe(...) to attach one.')
+                raise ValueError("There is no Probe attached to this recording. Use set_probe(...) to attach one.")
             else:
-                warn(
-                    'There is no Probe attached to this recording. Creating a dummy one with contact positions')
-                ndim = positions.shape[1]
-                probe = Probe(ndim=ndim)
-                probe.set_contacts(positions=positions,
-                                   shapes='circle', shape_params={'radius': 5})
-                probe.set_device_channel_indices(
-                    np.arange(self.get_num_channels(), dtype='int64'))
+                warn("There is no Probe attached to this recording. Creating a dummy one with contact positions")
+                probe = self.create_dummy_probe_from_locations(positions)
                 #  probe.create_auto_shape()
                 probegroup = ProbeGroup()
                 probegroup.add_probe(probe)
         else:
             probegroup = ProbeGroup.from_numpy(arr)
             for probe_index, probe in enumerate(probegroup.probes):
-                contour = self.get_annotation(
-                    f'probe_{probe_index}_planar_contour')
+                contour = self.get_annotation(f"probe_{probe_index}_planar_contour")
                 if contour is not None:
                     probe.set_planar_contour(contour)
         return probegroup
-    
+
     def _extra_metadata_from_folder(self, folder):
         # load probe
         folder = Path(folder)
-        if (folder / 'probe.json').is_file():
-            probegroup = read_probeinterface(folder / 'probe.json')
+        if (folder / "probe.json").is_file():
+            probegroup = read_probeinterface(folder / "probe.json")
             self.set_probegroup(probegroup, in_place=True)
 
     def _extra_metadata_to_folder(self, folder):
         # save probe
-        if self.get_property('contact_vector') is not None:
+        if self.get_property("contact_vector") is not None:
             probegroup = self.get_probegroup()
-            write_probeinterface(folder / 'probe.json', probegroup)
+            write_probeinterface(folder / "probe.json", probegroup)
 
-    def set_dummy_probe_from_locations(self, locations, shape="circle", shape_params={"radius": 1},
-                                       axes="xy"):
+    def create_dummy_probe_from_locations(self, locations, shape="circle", shape_params={"radius": 1}, axes="xy"):
         """
-        Sets a 'dummy' probe based on locations.
+        Creates a 'dummy' probe based on locations.
 
         Parameters
         ----------
         locations : np.array
             Array with channel locations (num_channels, ndim) [ndim can be 2 or 3]
         shape : str, optional
             Electrode shapes, by default "circle"
         shape_params : dict, optional
             Shape parameters, by default {"radius": 1}
         axes : str, optional
             If ndim is 3, indicates the axes that define the plane of the electrodes, by default "xy"
+
+        Returns
+        -------
+        probe : Probe
+            The created probe
         """
         ndim = locations.shape[1]
         probe = Probe(ndim=2)
         if ndim == 3:
             locations_2d = select_axes(locations, axes)
         else:
             locations_2d = locations
-        probe.set_contacts(locations_2d, shapes=shape,
-                           shape_params=shape_params)
+        probe.set_contacts(locations_2d, shapes=shape, shape_params=shape_params)
         probe.set_device_channel_indices(np.arange(self.get_num_channels()))
 
         if ndim == 3:
             probe = probe.to_3d(axes=axes)
 
+        return probe
+
+    def set_dummy_probe_from_locations(self, locations, shape="circle", shape_params={"radius": 1}, axes="xy"):
+        """
+        Sets a 'dummy' probe based on locations.
+
+        Parameters
+        ----------
+        locations : np.array
+            Array with channel locations (num_channels, ndim) [ndim can be 2 or 3]
+        shape : str, optional
+            Electrode shapes, by default "circle"
+        shape_params : dict, optional
+            Shape parameters, by default {"radius": 1}
+        axes : str, optional
+            If ndim is 3, indicates the axes that define the plane of the electrodes, by default "xy"
+        """
+        probe = self.create_dummy_probe_from_locations(locations, shape=shape, shape_params=shape_params, axes=axes)
         self.set_probe(probe, in_place=True)
 
     def set_channel_locations(self, locations, channel_ids=None):
-        if self.get_property('contact_vector') is not None:
-            raise ValueError(
-                'set_channel_locations(..) destroy the probe description, prefer set_probes(..)')
-        self.set_property('location', locations, ids=channel_ids)
+        if self.get_property("contact_vector") is not None:
+            raise ValueError("set_channel_locations(..) destroy the probe description, prefer set_probes(..)")
+        self.set_property("location", locations, ids=channel_ids)
 
-    def get_channel_locations(self, channel_ids=None, axes: str = 'xy'):
+    def get_channel_locations(self, channel_ids=None, axes: str = "xy"):
         if channel_ids is None:
             channel_ids = self.get_channel_ids()
         channel_indices = self.ids_to_indices(channel_ids)
-        if self.get_property('contact_vector') is not None:
+        if self.get_property("contact_vector") is not None:
             if len(self.get_probes()) == 1:
                 probe = self.get_probe()
                 positions = probe.contact_positions[channel_indices]
             else:
                 all_probes = self.get_probes()
                 # check that multiple probes are non-overlapping
                 check_probe_do_not_overlap(all_probes)
                 all_positions = np.vstack([probe.contact_positions for probe in all_probes])
                 positions = all_positions[channel_indices]
             return select_axes(positions, axes)
         else:
-            locations = self.get_property('location')
+            locations = self.get_property("location")
             if locations is None:
-                raise Exception('There are no channel locations')
+                raise Exception("There are no channel locations")
             locations = np.asarray(locations)[channel_indices]
             return select_axes(locations, axes)
 
     def has_3d_locations(self):
-        return self.get_property('location').shape[1] == 3
+        return self.get_property("location").shape[1] == 3
 
     def clear_channel_locations(self, channel_ids=None):
         if channel_ids is None:
             n = self.get_num_channel()
         else:
             n = len(channel_ids)
         locations = np.zeros((n, 2)) * np.nan
-        self.set_property('location', locations, ids=channel_ids)
+        self.set_property("location", locations, ids=channel_ids)
 
     def set_channel_groups(self, groups, channel_ids=None):
-        if 'probes' in self._annotations:
-            warn(
-                'set_channel_groups() destroys the probe description. Using set_probe() is preferable')
-            self._annotations.pop('probes')
-        self.set_property('group', groups, ids=channel_ids)
+        if "probes" in self._annotations:
+            warn("set_channel_groups() destroys the probe description. Using set_probe() is preferable")
+            self._annotations.pop("probes")
+        self.set_property("group", groups, ids=channel_ids)
 
     def get_channel_groups(self, channel_ids=None):
-        groups = self.get_property('group', ids=channel_ids)
+        groups = self.get_property("group", ids=channel_ids)
         return groups
 
     def clear_channel_groups(self, channel_ids=None):
         if channel_ids is None:
             n = self.get_num_channels()
         else:
             n = len(channel_ids)
-        groups = np.zeros(n, dtype='int64')
-        self.set_property('group', groups, ids=channel_ids)
+        groups = np.zeros(n, dtype="int64")
+        self.set_property("group", groups, ids=channel_ids)
 
     def set_channel_gains(self, gains, channel_ids=None):
         if np.isscalar(gains):
             gains = [gains] * self.get_num_channels()
-        self.set_property('gain_to_uV', gains, ids=channel_ids)
+        self.set_property("gain_to_uV", gains, ids=channel_ids)
 
     def get_channel_gains(self, channel_ids=None):
-        return self.get_property('gain_to_uV', ids=channel_ids)
+        return self.get_property("gain_to_uV", ids=channel_ids)
 
     def set_channel_offsets(self, offsets, channel_ids=None):
         if np.isscalar(offsets):
             offsets = [offsets] * self.get_num_channels()
-        self.set_property('offset_to_uV', offsets, ids=channel_ids)
+        self.set_property("offset_to_uV", offsets, ids=channel_ids)
 
     def get_channel_offsets(self, channel_ids=None):
-        return self.get_property('offset_to_uV', ids=channel_ids)
+        return self.get_property("offset_to_uV", ids=channel_ids)
 
     def get_channel_property(self, channel_id, key):
         values = self.get_property(key)
         v = values[self.id_to_index(channel_id)]
         return v
 
     def planarize(self, axes: str = "xy"):
@@ -368,16 +391,15 @@
 
         Returns
         -------
         BaseRecording
             The recording with 2D positions
         """
         assert self.has_3d_locations, "The 'planarize' function needs a recording with 3d locations"
-        assert len(
-            axes) == 2, "You need to specify 2 dimensions (e.g. 'xy', 'zy')"
+        assert len(axes) == 2, "You need to specify 2 dimensions (e.g. 'xy', 'zy')"
 
         probe2d = self.get_probe().to_2d(axes=axes)
         recording2d = self.clone()
         recording2d.set_probe(probe2d, in_place=True)
 
         return recording2d
 
@@ -395,15 +417,15 @@
 
         Returns
         -------
         BaseRecordingSnippets
             The object with sliced channels
         """
         return self._channel_slice(channel_ids, renamed_channel_ids=renamed_channel_ids)
-    
+
     def remove_channels(self, remove_channel_ids):
         """
         Returns a new object with removed channels.
 
 
         Parameters
         ----------
@@ -412,15 +434,15 @@
 
         Returns
         -------
         BaseRecordingSnippets
             The object with removed channels
         """
         return self._remove_channels(remove_channel_ids)
-    
+
     def frame_slice(self, start_frame, end_frame):
         """
         Returns a new object with sliced frames.
 
         Parameters
         ----------
         start_frame : int
@@ -430,15 +452,15 @@
 
         Returns
         -------
         BaseRecordingSnippets
             The object with sliced frames
         """
         return self._frame_slice(start_frame, end_frame)
-    
+
     def select_segments(self, segment_indices):
         """
         Return a new object with the segments specified by 'segment_indices'.
 
         Parameters
         ----------
         segment_indices : list of int
@@ -446,16 +468,16 @@
 
         Returns
         -------
         BaseRecordingSnippets
             The onject with the selected segments
         """
         return self._select_segments(segment_indices)
-        
-    def split_by(self, property='group', outputs='dict'):
+
+    def split_by(self, property="group", outputs="dict"):
         """
         Splits object based on a certain property (e.g. 'group')
 
         Parameters
         ----------
         property : str, optional
             The property to use to split the object, by default 'group'
@@ -468,25 +490,25 @@
             A dict or list with grouped objects based on property
 
         Raises
         ------
         ValueError
             Raised when property is not present
         """
-        assert outputs in ('list', 'dict')
+        assert outputs in ("list", "dict")
         values = self.get_property(property)
         if values is None:
-            raise ValueError(f'property {property} is not set')
+            raise ValueError(f"property {property} is not set")
 
-        if outputs == 'list':
+        if outputs == "list":
             recordings = []
-        elif outputs == 'dict':
+        elif outputs == "dict":
             recordings = {}
         for value in np.unique(values):
-            inds, = np.nonzero(values == value)
+            (inds,) = np.nonzero(values == value)
             new_channel_ids = self.get_channel_ids()[inds]
             subrec = self.channel_slice(new_channel_ids)
-            if outputs == 'list':
+            if outputs == "list":
                 recordings.append(subrec)
-            elif outputs == 'dict':
+            elif outputs == "dict":
                 recordings[value] = subrec
-        return recordings
+        return recordings
```

### Comparing `spikeinterface-0.97.1/spikeinterface/core/basesnippets.py` & `spikeinterface-0.98.0/src/spikeinterface/core/basesnippets.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,51 +1,51 @@
 from typing import List, Union
 from pathlib import Path
 from .base import BaseSegment
 from .baserecordingsnippets import BaseRecordingSnippets
 import numpy as np
 from warnings import warn
 from probeinterface import Probe, ProbeGroup, write_probeinterface, read_probeinterface, select_axes
+
 # snippets segments?
 
 
 class BaseSnippets(BaseRecordingSnippets):
     """
     Abstract class representing several multichannel snippets.
     """
+
     _main_annotations = []
-    _main_properties = ['group', 'location', 'gain_to_uV', 'offset_to_uV']
+    _main_properties = ["group", "location", "gain_to_uV", "offset_to_uV"]
     _main_features = []
 
-    def __init__(self, sampling_frequency: float, nbefore: Union[int, None], snippet_len: int,
-                 channel_ids: List, dtype):
-
-        BaseRecordingSnippets.__init__(self,
-                                       channel_ids=channel_ids,
-                                       sampling_frequency=sampling_frequency,
-                                       dtype=dtype)
+    def __init__(
+        self, sampling_frequency: float, nbefore: Union[int, None], snippet_len: int, channel_ids: List, dtype
+    ):
+        BaseRecordingSnippets.__init__(
+            self, channel_ids=channel_ids, sampling_frequency=sampling_frequency, dtype=dtype
+        )
         self._nbefore = nbefore
         self._snippet_len = snippet_len
-        self.is_dumpable = True
 
         self._snippets_segments: List[BaseSnippetsSegment] = []
         # initialize main annotation and properties
 
     def __repr__(self):
         clsname = self.__class__.__name__
         nchan = self.get_num_channels()
         nseg = self.get_num_segments()
-        sf_khz = self.get_sampling_frequency() / 1000.
-        txt = f'{clsname}: {nchan} channels - {nseg} segments -  {sf_khz:0.1f}kHz \n snippet_len:{self._snippet_len} before peak:{self._nbefore}'
+        sf_khz = self.get_sampling_frequency() / 1000.0
+        txt = f"{clsname}: {nchan} channels - {nseg} segments -  {sf_khz:0.1f}kHz \n snippet_len:{self._snippet_len} before peak:{self._nbefore}"
         return txt
-    
+
     @property
     def nbefore(self):
         return self._nbefore
-    
+
     @property
     def snippet_len(self):
         return self._snippet_len
 
     def get_num_segments(self):
         return len(self._snippets_segments)
 
@@ -75,149 +75,160 @@
         return s
 
     def is_aligned(self):
         return self._nbefore is not None
 
     def get_num_segments(self):
         return len(self._snippets_segments)
-    
+
     def has_scaled_snippets(self):
         return self.has_scaled()
 
-    def get_frames(self,
-                   indices=None,
-                   segment_index: Union[int, None] = None
-                   ):
+    def get_frames(self, indices=None, segment_index: Union[int, None] = None):
         segment_index = self._check_segment_index(segment_index)
         spts = self._snippets_segments[segment_index]
         return spts.get_frames(indices)
 
-    def get_snippets(self,
-                     indices=None,
-                     segment_index: Union[int, None] = None,
-                     channel_ids: Union[List, None] = None,
-                     return_scaled=False,
-                     ):
-
+    def get_snippets(
+        self,
+        indices=None,
+        segment_index: Union[int, None] = None,
+        channel_ids: Union[List, None] = None,
+        return_scaled=False,
+    ):
         segment_index = self._check_segment_index(segment_index)
         spts = self._snippets_segments[segment_index]
         channel_indices = self.ids_to_indices(channel_ids, prefer_slice=True)
         wfs = spts.get_snippets(indices, channel_indices=channel_indices)
 
         if return_scaled:
             if not self.has_scaled():
-                raise ValueError('These snippets do not support return_scaled=True (need gain_to_uV and offset_'
-                                 'to_uV properties)')
+                raise ValueError(
+                    "These snippets do not support return_scaled=True (need gain_to_uV and offset_" "to_uV properties)"
+                )
             else:
-                gains = self.get_property('gain_to_uV')
-                offsets = self.get_property('offset_to_uV')
-                gains = gains[channel_indices].astype('float32')
-                offsets = offsets[channel_indices].astype('float32')
-                wfs = wfs.astype('float32') * gains + offsets
+                gains = self.get_property("gain_to_uV")
+                offsets = self.get_property("offset_to_uV")
+                gains = gains[channel_indices].astype("float32")
+                offsets = offsets[channel_indices].astype("float32")
+                wfs = wfs.astype("float32") * gains + offsets
         return wfs
 
-    def get_snippets_from_frames(self,
-                                 segment_index: Union[int, None] = None,
-                                 start_frame: Union[int, None] = None,
-                                 end_frame: Union[int, None] = None,
-                                 channel_ids: Union[List, None] = None,
-                                 return_scaled=False,
-                                 ):
-
+    def get_snippets_from_frames(
+        self,
+        segment_index: Union[int, None] = None,
+        start_frame: Union[int, None] = None,
+        end_frame: Union[int, None] = None,
+        channel_ids: Union[List, None] = None,
+        return_scaled=False,
+    ):
         segment_index = self._check_segment_index(segment_index)
         spts = self._snippets_segments[segment_index]
         indices = spts.frames_to_indices(start_frame, end_frame)
 
         return self.get_snippets(indices, channel_ids=channel_ids, return_scaled=return_scaled)
 
-    def _save(self, format='binary', **save_kwargs):
+    def _save(self, format="binary", **save_kwargs):
         raise NotImplementedError
-    
+
     def _channel_slice(self, channel_ids, renamed_channel_ids=None):
         from .channelslice import ChannelSliceSnippets
+
         sub_recording = ChannelSliceSnippets(self, channel_ids, renamed_channel_ids=renamed_channel_ids)
         return sub_recording
-    
+
     def _remove_channels(self, remove_channel_ids):
         from .channelslice import ChannelSliceSnippets
+
         new_channel_ids = self.channel_ids[~np.in1d(self.channel_ids, remove_channel_ids)]
         sub_recording = ChannelSliceSnippets(self, new_channel_ids)
         return sub_recording
 
     def _frame_slice(self, start_frame, end_frame):
         raise NotImplementedError
-    
+
     def _select_segments(self, segment_indices):
         from .segmentutils import SelectSegmentSnippets
+
         return SelectSegmentSnippets(self, segment_indices=segment_indices)
 
-    def _save(self, format='npy', **save_kwargs):
+    def _save(self, format="npy", **save_kwargs):
         """
         At the moment only 'npy' and 'memory' avaiable:
         """
 
-        if format == 'npy':
+        if format == "npy":
             from spikeinterface.core.npysnippetsextractor import NpySnippetsExtractor
 
-            folder = save_kwargs['folder']
-            file_paths = [folder / f'traces_cached_seg{i}.npy' for i in range(self.get_num_segments())]
-            dtype = save_kwargs.get('dtype', None)
+            folder = save_kwargs["folder"]
+            file_paths = [folder / f"traces_cached_seg{i}.npy" for i in range(self.get_num_segments())]
+            dtype = save_kwargs.get("dtype", None)
             if dtype is None:
                 dtype = self.get_dtype()
 
-
             from spikeinterface.core.npysnippetsextractor import NpySnippetsExtractor
-            
+
             NpySnippetsExtractor.write_snippets(snippets=self, file_paths=file_paths, dtype=dtype)
-            cached = NpySnippetsExtractor(file_paths=file_paths, sampling_frequency=self.get_sampling_frequency(),
-                                              channel_ids=self.get_channel_ids(),
-                                              nbefore=self.nbefore, gain_to_uV=self.get_channel_gains(),
-                                              offset_to_uV=self.get_channel_offsets())
-            cached.dump(folder / 'npy.json', relative_to=folder)
+            cached = NpySnippetsExtractor(
+                file_paths=file_paths,
+                sampling_frequency=self.get_sampling_frequency(),
+                channel_ids=self.get_channel_ids(),
+                nbefore=self.nbefore,
+                gain_to_uV=self.get_channel_gains(),
+                offset_to_uV=self.get_channel_offsets(),
+            )
+            cached.dump(folder / "npy.json", relative_to=folder)
 
             from spikeinterface.core.npyfoldersnippets import NpyFolderSnippets
+
             cached = NpyFolderSnippets(folder_path=folder)
 
-        elif format == 'memory':
+        elif format == "memory":
             snippets_list = []
             spikesframes_list = []
             for i in range(self.get_num_segments()):
                 spikesframes_list.append(self.get_frames(segment_index=i))
                 snippets_list.append(self.get_snippets(segment_index=i))
 
             from .numpyextractors import NumpySnippets
-            cached = NumpySnippets(snippets_list=snippets_list, spikesframes_list=spikesframes_list,
-                                sampling_frequency = self.get_sampling_frequency(), nbefore=self.nbefore,
-                                channel_ids=self.channel_ids)
+
+            cached = NumpySnippets(
+                snippets_list=snippets_list,
+                spikesframes_list=spikesframes_list,
+                sampling_frequency=self.get_sampling_frequency(),
+                nbefore=self.nbefore,
+                channel_ids=self.channel_ids,
+            )
 
         else:
-            raise ValueError(f'format {format} not supported')
+            raise ValueError(f"format {format} not supported")
 
-        if self.get_property('contact_vector') is not None:
+        if self.get_property("contact_vector") is not None:
             probegroup = self.get_probegroup()
             cached.set_probegroup(probegroup)
 
         return cached
 
     def get_times(self):
-        return self.get_frames()/self.sampling_frequency
+        return self.get_frames() / self.sampling_frequency
 
 
 class BaseSnippetsSegment(BaseSegment):
     """
     Abstract class representing multichannel snippets
     """
 
     def __init__(self):
         BaseSegment.__init__(self)
 
-    def get_snippets(self,
-                    indices = None,
-                    channel_indices: Union[List, None] = None,
-                    ) -> np.ndarray:
+    def get_snippets(
+        self,
+        indices=None,
+        channel_indices: Union[List, None] = None,
+    ) -> np.ndarray:
         """
         Return the snippets, optionally for a subset of samples and/or channels
 
         Parameters
         ----------
         indexes: (Union[int, None], optional)
             indices of the snippets to return, or all if None. Defaults to None.
@@ -243,16 +254,15 @@
         """Returns the frames of the snippets in this  segment
 
         Returns:
             SampleIndex: Number of samples in the  segment
         """
         raise NotImplementedError
 
-    def frames_to_indices(self, start_frame: Union[int, None] = None,
-                          end_frame: Union[int, None] = None):
+    def frames_to_indices(self, start_frame: Union[int, None] = None, end_frame: Union[int, None] = None):
         """
         Return the slice of snippets
 
         Parameters
         ----------
         start_frame: (Union[int, None], optional)
             start sample index, or zero if None. Defaults to None.
@@ -260,8 +270,8 @@
             end_sample, or number of samples if None. Defaults to None.
 
         Returns
         -------
         snippets: slice
             slice of selected snippets
         """
-        raise NotImplementedError
+        raise NotImplementedError
```

### Comparing `spikeinterface-0.97.1/spikeinterface/core/basesorting.py` & `spikeinterface-0.98.0/src/spikeinterface/core/basesorting.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,36 +1,37 @@
-from typing import List, Union, Optional
-import numpy as np
 import warnings
+from typing import List, Optional, Union
+
+import numpy as np
 
 from .base import BaseExtractor, BaseSegment
+from .waveform_tools import has_exceeding_spikes
 
 
 class BaseSorting(BaseExtractor):
     """
     Abstract class representing several segment several units and relative spiketrains.
     """
 
     def __init__(self, sampling_frequency: float, unit_ids: List):
-
         BaseExtractor.__init__(self, unit_ids)
         self._sampling_frequency = sampling_frequency
         self._sorting_segments: List[BaseSortingSegment] = []
         # this weak link is to handle times from a recording object
         self._recording = None
         self._sorting_info = None
 
     def __repr__(self):
         clsname = self.__class__.__name__
         nseg = self.get_num_segments()
         nunits = self.get_num_units()
-        sf_khz = self.get_sampling_frequency() / 1000.
-        txt = f'{clsname}: {nunits} units - {nseg} segments - {sf_khz:0.1f}kHz'
-        if 'file_path' in self._kwargs:
-            txt += '\n  file_path: {}'.format(self._kwargs['file_path'])
+        sf_khz = self.get_sampling_frequency() / 1000.0
+        txt = f"{clsname}: {nunits} units - {nseg} segments - {sf_khz:0.1f}kHz"
+        if "file_path" in self._kwargs:
+            txt += "\n  file_path: {}".format(self._kwargs["file_path"])
         return txt
 
     @property
     def unit_ids(self):
         return self._main_ids
 
     @property
@@ -49,56 +50,119 @@
 
     def get_sampling_frequency(self):
         return self._sampling_frequency
 
     def get_num_segments(self):
         return len(self._sorting_segments)
 
+    def get_num_samples(self, segment_index=None):
+        """Returns the number of samples of the associated recording for a segment.
+
+        Parameters
+        ----------
+        segment_index : int, optional
+            The segment index to retrieve the number of samples for.
+            For multi-segment objects, it is required, by default None
+
+        Returns
+        -------
+        int
+            The number of samples
+        """
+        assert (
+            self.has_recording()
+        ), "This methods requires an associated recording. Call self.register_recording() first."
+        return self._recording.get_num_samples(segment_index=segment_index)
+
+    def get_total_samples(self):
+        """Returns the total number of samples of the associated recording.
+
+        Returns
+        -------
+        int
+            The total number of samples
+        """
+        s = 0
+        for segment_index in range(self.get_num_segments()):
+            s += self.get_num_samples(segment_index)
+        return s
+
+    def get_total_duration(self):
+        """Returns the total duration in s of the associated recording.
+
+        Returns
+        -------
+        float
+            The duration in seconds
+        """
+        assert (
+            self.has_recording()
+        ), "This methods requires an associated recording. Call self.register_recording() first."
+        return self._recording.get_total_duration()
+
     def get_unit_spike_train(
         self,
         unit_id,
         segment_index: Union[int, None] = None,
         start_frame: Union[int, None] = None,
         end_frame: Union[int, None] = None,
         return_times: bool = False,
     ):
         segment_index = self._check_segment_index(segment_index)
         segment = self._sorting_segments[segment_index]
         spike_frames = segment.get_unit_spike_train(
-            unit_id=unit_id, start_frame=start_frame, end_frame=end_frame).astype("int64")
+            unit_id=unit_id, start_frame=start_frame, end_frame=end_frame
+        ).astype("int64")
         if return_times:
             if self.has_recording():
                 times = self.get_times(segment_index=segment_index)
                 return times[spike_frames]
             else:
                 t_start = segment._t_start if segment._t_start is not None else 0
-                spike_times =  spike_frames / self.get_sampling_frequency()
+                spike_times = spike_frames / self.get_sampling_frequency()
                 return t_start + spike_times
         else:
             return spike_frames
 
-    def register_recording(self, recording):
-        assert np.isclose(self.get_sampling_frequency(),
-                          recording.get_sampling_frequency(),
-                          atol=0.1), "The recording has a different sampling frequency than the sorting!"
+    def register_recording(self, recording, check_spike_frames=True):
+        """Register a recording to the sorting.
+
+        Parameters
+        ----------
+        recording : BaseRecording
+            Recording with the same number of segments as current sorting.
+            Assigned to self._recording.
+        check_spike_frames : bool, optional
+            If True, assert for each segment that all spikes are within the recording's range.
+            By default True.
+        """
+        assert np.isclose(
+            self.get_sampling_frequency(), recording.get_sampling_frequency(), atol=0.1
+        ), "The recording has a different sampling frequency than the sorting!"
+        assert (
+            self.get_num_segments() == recording.get_num_segments()
+        ), "The recording has a different number of segments than the sorting!"
+        if check_spike_frames:
+            if has_exceeding_spikes(recording, self):
+                warnings.warn(
+                    "Some spikes are exceeding the recording's duration! "
+                    "Removing these excess spikes with `spikeinterface.curation.remove_excess_spikes()` "
+                    "Might be necessary for further postprocessing."
+                )
         self._recording = recording
 
     @property
     def sorting_info(self):
         if "__sorting_info__" in self.get_annotation_keys():
             return self.get_annotation("__sorting_info__")
         else:
             return None
 
     def set_sorting_info(self, recording_dict, params_dict, log_dict):
-        sorting_info = dict(
-            recording=recording_dict,
-            params=params_dict,
-            log=log_dict
-        )
+        sorting_info = dict(recording=recording_dict, params=params_dict, log=log_dict)
         self.annotate(__sorting_info__=sorting_info)
 
     def has_recording(self):
         return self._recording is not None
 
     def has_time_vector(self, segment_index=None):
         """
@@ -122,39 +186,41 @@
         """
         segment_index = self._check_segment_index(segment_index)
         if self.has_recording():
             return self._recording.get_times(segment_index=segment_index)
         else:
             return None
 
-    def _save(self, format='npz', **save_kwargs):
+    def _save(self, format="npz", **save_kwargs):
         """
         This function replaces the old CachesortingExtractor, but enables more engines
         for caching a results. At the moment only 'npz' is supported.
         """
-        if format == 'npz':
-            folder = save_kwargs.pop('folder')
+        if format == "npz":
+            folder = save_kwargs.pop("folder")
             # TODO save properties/features as npz!!!!!
             from .npzsortingextractor import NpzSortingExtractor
-            save_path = folder / 'sorting_cached.npz'
+
+            save_path = folder / "sorting_cached.npz"
             NpzSortingExtractor.write_sorting(self, save_path)
             cached = NpzSortingExtractor(save_path)
-            cached.dump(folder / 'npz.json', relative_to=folder)
+            cached.dump(folder / "npz.json", relative_to=folder)
 
             from .npzfolder import NpzFolderSorting
+
             cached = NpzFolderSorting(folder_path=folder)
             if self.has_recording():
-                warnings.warn(
-                    "The registered recording will not be persistent on disk, but only available in memory")
+                warnings.warn("The registered recording will not be persistent on disk, but only available in memory")
                 cached.register_recording(self._recording)
-        elif format == 'memory':
+        elif format == "memory":
             from .numpyextractors import NumpySorting
+
             cached = NumpySorting.from_extractor(self)
         else:
-            raise ValueError(f'format {format} not supported')            
+            raise ValueError(f"format {format} not supported")
         return cached
 
     def get_unit_property(self, unit_id, key):
         values = self.get_property(key)
         v = values[self.id_to_index(unit_id)]
         return v
 
@@ -189,18 +255,18 @@
 
         Returns
         -------
         BaseSorting
             Sorting object with selected units
         """
         from spikeinterface import UnitsSelectionSorting
-        sub_sorting = UnitsSelectionSorting(
-            self, unit_ids, renamed_unit_ids=renamed_unit_ids)
+
+        sub_sorting = UnitsSelectionSorting(self, unit_ids, renamed_unit_ids=renamed_unit_ids)
         return sub_sorting
-    
+
     def remove_units(self, remove_unit_ids):
         """
         Removes a subset of units
 
         Parameters
         ----------
         remove_unit_ids :  numpy.array or list
@@ -208,58 +274,69 @@
 
         Returns
         -------
         BaseSorting
             Sorting object without removed units
         """
         from spikeinterface import UnitsSelectionSorting
+
         new_unit_ids = self.unit_ids[~np.in1d(self.unit_ids, remove_unit_ids)]
         new_sorting = UnitsSelectionSorting(self, new_unit_ids)
         return new_sorting
 
     def remove_empty_units(self):
         """
         Removes units with empty spike trains
 
         Returns
         -------
         BaseSorting
             Sorting object with non-empty units
         """
-        units_to_keep = []
+        non_empty_units = self.get_non_empty_unit_ids()
+        return self.select_units(non_empty_units)
+
+    def get_non_empty_unit_ids(self):
+        non_empty_units = []
         for segment_index in range(self.get_num_segments()):
             for unit in self.get_unit_ids():
                 if len(self.get_unit_spike_train(unit, segment_index=segment_index)) > 0:
-                    units_to_keep.append(unit)
-        units_to_keep = np.unique(units_to_keep)
-        return self.select_units(units_to_keep)
+                    non_empty_units.append(unit)
+        non_empty_units = np.unique(non_empty_units)
+        return non_empty_units
+
+    def get_empty_unit_ids(self):
+        unit_ids = self.get_unit_ids()
+        empty_units = unit_ids[~np.isin(unit_ids, self.get_non_empty_unit_ids())]
+        return empty_units
 
-    def frame_slice(self, start_frame, end_frame):
+    def frame_slice(self, start_frame, end_frame, check_spike_frames=True):
         from spikeinterface import FrameSliceSorting
+
         sub_sorting = FrameSliceSorting(
-            self, start_frame=start_frame, end_frame=end_frame)
+            self, start_frame=start_frame, end_frame=end_frame, check_spike_frames=check_spike_frames
+        )
         return sub_sorting
 
-    def get_all_spike_trains(self, outputs='unit_id'):
+    def get_all_spike_trains(self, outputs="unit_id"):
         """
         Return all spike trains concatenated
         """
-        assert outputs in ('unit_id', 'unit_index')
+        assert outputs in ("unit_id", "unit_index")
         spikes = []
         for segment_index in range(self.get_num_segments()):
             spike_times = []
             spike_labels = []
             for i, unit_id in enumerate(self.unit_ids):
-                st = self.get_unit_spike_train(
-                    unit_id=unit_id, segment_index=segment_index)
+                st = self.get_unit_spike_train(unit_id=unit_id, segment_index=segment_index)
                 spike_times.append(st)
-                if outputs == 'unit_id':
+                if outputs == "unit_id":
                     spike_labels.append(np.array([unit_id] * st.size))
-                elif outputs == 'unit_index':
-                    spike_labels.append(np.zeros(st.size, dtype='int64') + i)
+                elif outputs == "unit_index":
+                    spike_labels.append(np.zeros(st.size, dtype="int64") + i)
 
             if len(spike_times) > 0:
                 spike_times = np.concatenate(spike_times)
                 spike_labels = np.concatenate(spike_labels)
                 order = np.argsort(spike_times)
                 spike_times = spike_times[order]
                 spike_labels = spike_labels[order]
@@ -268,61 +345,57 @@
                 spike_labels = np.array([], dtype=np.int64)
 
             spikes.append((spike_times, spike_labels))
         return spikes
 
     def to_spike_vector(self, extremum_channel_inds=None):
         """
-        Construct a unique structured numpy vector concatenating all spikes 
-        with several fields: sample_ind, unit_index, segment_index.
+        Construct a unique structured numpy vector concatenating all spikes
+        with several fields: sample_index, unit_index, segment_index.
 
         See also `get_all_spike_trains()`
 
         Parameters
         ----------
         extremum_channel_inds: None or dict
-            If a dictionnary of unit_id to channel_ind is given then an extra field 'channel_ind'.
+            If a dictionnary of unit_id to channel_ind is given then an extra field 'channel_index'.
             This can be convinient for computing spikes postion after sorter.
-            
+
             This dict can be computed with `get_template_extremum_channel(we, outputs="index")`
-        
+
         Returns
         -------
         spikes: np.array
-            Structured numpy array ('sample_ind', 'unit_index', 'segment_index') with all spikes
-            Or ('sample_ind', 'unit_index', 'segment_index', 'channel_ind') if extremum_channel_inds
+            Structured numpy array ('sample_index', 'unit_index', 'segment_index') with all spikes
+            Or ('sample_index', 'unit_index', 'segment_index', 'channel_index') if extremum_channel_inds
             is given
-            
+
         """
-        spikes_ = self.get_all_spike_trains(outputs='unit_index')
+        spikes_ = self.get_all_spike_trains(outputs="unit_index")
 
         n = np.sum([e[0].size for e in spikes_])
-        spike_dtype = [('sample_ind', 'int64'), ('unit_ind',
-                                                 'int64'), ('segment_ind', 'int64')]
-        
+        spike_dtype = [("sample_index", "int64"), ("unit_index", "int64"), ("segment_index", "int64")]
+
         if extremum_channel_inds is not None:
-            spike_dtype += [('channel_ind', 'int64')]
-        
-        
+            spike_dtype += [("channel_index", "int64")]
+
         spikes = np.zeros(n, dtype=spike_dtype)
 
         pos = 0
         for segment_index, (spike_times, spike_labels) in enumerate(spikes_):
             n = spike_times.size
-            spikes[pos:pos+n]['sample_ind'] = spike_times
-            spikes[pos:pos+n]['unit_ind'] = spike_labels
-            spikes[pos:pos+n]['segment_ind'] = segment_index
+            spikes[pos : pos + n]["sample_index"] = spike_times
+            spikes[pos : pos + n]["unit_index"] = spike_labels
+            spikes[pos : pos + n]["segment_index"] = segment_index
             pos += n
-        
 
         if extremum_channel_inds is not None:
             ext_channel_inds = np.array([extremum_channel_inds[unit_id] for unit_id in self.unit_ids])
             # vector way
-            spikes['channel_ind'] = ext_channel_inds[spikes['unit_ind']]
-
+            spikes["channel_index"] = ext_channel_inds[spikes["unit_index"]]
 
         return spikes
 
 
 class BaseSortingSegment(BaseSegment):
     """
     Abstract class representing several units and relative spiketrain inside a segment.
```

### Comparing `spikeinterface-0.97.1/spikeinterface/core/binaryfolder.py` & `spikeinterface-0.98.0/src/spikeinterface/core/binaryfolder.py`

 * *Files 20% similar despite different names*

```diff
@@ -4,67 +4,67 @@
 import numpy as np
 
 from .base import _make_paths_absolute
 from .binaryrecordingextractor import BinaryRecordingExtractor
 from .core_tools import define_function_from_class
 
 
-
 class BinaryFolderRecording(BinaryRecordingExtractor):
     """
     BinaryFolderRecording is an internal format used in spikeinterface.
     It is a BinaryRecordingExtractor + metadata contained in a folder.
-    
+
     It is created with the function: `recording.save(format='binary', folder='/myfolder')`
-    
+
     Parameters
     ----------
     folder_path: str or Path
-    
+
     Returns
     -------
     recording: BinaryFolderRecording
         The recording
     """
-    extractor_name = 'BinaryFolder'
-    has_default_locations = True
-    mode = 'folder'
+
+    extractor_name = "BinaryFolder"
+    mode = "folder"
     name = "binaryfolder"
 
-    def __init__(self,  folder_path):
-        
+    def __init__(self, folder_path):
         folder_path = Path(folder_path)
-        
-        with open(folder_path / 'binary.json', 'r') as f:
+
+        with open(folder_path / "binary.json", "r") as f:
             d = json.load(f)
 
-        if not d['class'].endswith('.BinaryRecordingExtractor'):
-            raise ValueError('This folder is not a binary spikeinterface folder')
+        if not d["class"].endswith(".BinaryRecordingExtractor"):
+            raise ValueError("This folder is not a binary spikeinterface folder")
 
-        assert d['relative_paths']
+        assert d["relative_paths"]
 
         d = _make_paths_absolute(d, folder_path)
 
-        BinaryRecordingExtractor.__init__(self, **d['kwargs'])
+        BinaryRecordingExtractor.__init__(self, **d["kwargs"])
 
         folder_metadata = folder_path
         self.load_metadata_from_folder(folder_metadata)
-        
+
         self._kwargs = dict(folder_path=str(folder_path.absolute()))
-        self._bin_kwargs = d['kwargs']
+        self._bin_kwargs = d["kwargs"]
+        if "num_channels" not in self._bin_kwargs:
+            assert "num_chan" in self._bin_kwargs, "Cannot find num_channels or num_chan in binary.json"
+            self._bin_kwargs["num_channels"] = self._bin_kwargs["num_chan"]
 
     def is_binary_compatible(self):
         return True
-        
+
     def get_binary_description(self):
         d = dict(
-            file_paths=self._bin_kwargs['file_paths'],
-            dtype=np.dtype(self._bin_kwargs['dtype']),
-            num_channels=self._bin_kwargs['num_chan'],
-            time_axis=self._bin_kwargs['time_axis'],
-            file_offset=self._bin_kwargs['file_offset'],
+            file_paths=self._bin_kwargs["file_paths"],
+            dtype=np.dtype(self._bin_kwargs["dtype"]),
+            num_channels=self._bin_kwargs["num_channels"],
+            time_axis=self._bin_kwargs["time_axis"],
+            file_offset=self._bin_kwargs["file_offset"],
         )
         return d
 
 
 read_binary_folder = define_function_from_class(source_class=BinaryFolderRecording, name="read_binary_folder")
-
```

### Comparing `spikeinterface-0.97.1/spikeinterface/core/binaryrecordingextractor.py` & `spikeinterface-0.98.0/src/spikeinterface/core/binaryrecordingextractor.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,30 +1,32 @@
 from typing import List, Union
-
-import shutil
+import mmap
+import warnings
 from pathlib import Path
 
 import numpy as np
 
 from .baserecording import BaseRecording, BaseRecordingSegment
-from .core_tools import read_binary_recording, write_binary_recording, define_function_from_class
+from .core_tools import write_binary_recording, define_function_from_class
 from .job_tools import _shared_job_kwargs_doc
 
 
 class BinaryRecordingExtractor(BaseRecording):
     """
     RecordingExtractor for a binary format
 
     Parameters
     ----------
     file_paths: str or Path or list
         Path to the binary file
     sampling_frequency: float
         The sampling frequency
-    num_chan: int
+    num_channels: int
+        Number of channels
+    num_chan: int [deprecated, use num_channels instead, will be removed as early as v0.100.0]
         Number of channels
     dtype: str or dtype
         The dtype of the binary file
     time_axis: int
         The axis of the time dimension (default 0: F order)
     t_starts: None or list of float
         Times in seconds of the first sample for each segment
@@ -35,73 +37,101 @@
     gain_to_uV: float or array-like (optional)
         The gain to apply to the traces
     offset_to_uV: float or array-like
         The offset to apply to the traces
     is_filtered: bool or None
         If True, the recording is assumed to be filtered. If None, is_filtered is not set.
 
+    Notes
+    -----
+    When both num_channels and num_chan are provided, `num_channels` is used and `num_chan` is ignored.
+
     Returns
     -------
     recording: BinaryRecordingExtractor
         The recording Extractor
     """
-    extractor_name = 'BinaryRecording'
-    is_writable = True
-    mode = 'file'
+
+    extractor_name = "BinaryRecording"
+    mode = "file"
     name = "binary"
-    
-    def __init__(self, file_paths, sampling_frequency, num_chan, dtype, t_starts=None, channel_ids=None,
-                 time_axis=0, file_offset=0, gain_to_uV=None, offset_to_uV=None,
-                 is_filtered=None):
+
+    def __init__(
+        self,
+        file_paths,
+        sampling_frequency,
+        dtype,
+        num_channels=None,
+        t_starts=None,
+        channel_ids=None,
+        time_axis=0,
+        file_offset=0,
+        gain_to_uV=None,
+        offset_to_uV=None,
+        is_filtered=None,
+        num_chan=None,
+    ):
+        # This assigns num_channels if num_channels is not None, otherwise num_chan is assigned
+        num_channels = num_channels or num_chan
+        assert num_channels is not None, "You must provide num_channels or num_chan"
+        if num_chan is not None:
+            warnings.warn("`num_chan` is to be deprecated in version 0.100, please use `num_channels` instead")
 
         if channel_ids is None:
-            channel_ids = list(range(num_chan))
+            channel_ids = list(range(num_channels))
         else:
-            assert len(channel_ids) == num_chan, 'Provided recording channels have the wrong length'
+            assert len(channel_ids) == num_channels, "Provided recording channels have the wrong length"
 
         BaseRecording.__init__(self, sampling_frequency, channel_ids, dtype)
 
         if isinstance(file_paths, list):
             # several segment
-            datfiles = [Path(p) for p in file_paths]
+            file_path_list = [Path(p) for p in file_paths]
         else:
             # one segment
-            datfiles = [Path(file_paths)]
+            file_path_list = [Path(file_paths)]
 
         if t_starts is not None:
-            assert len(t_starts) == len(datfiles), 't_starts must be a list of same size than file_paths'
+            assert len(t_starts) == len(file_path_list), "t_starts must be a list of same size than file_paths"
             t_starts = [float(t_start) for t_start in t_starts]
 
         dtype = np.dtype(dtype)
 
-        for i, datfile in enumerate(datfiles):
+        for i, file_path in enumerate(file_path_list):
             if t_starts is None:
                 t_start = None
             else:
                 t_start = t_starts[i]
-            rec_segment = BinaryRecordingSegment(datfile, sampling_frequency, t_start, num_chan, dtype, time_axis, file_offset)
+            rec_segment = BinaryRecordingSegment(
+                file_path, sampling_frequency, t_start, num_channels, dtype, time_axis, file_offset
+            )
             self.add_recording_segment(rec_segment)
 
         if is_filtered is not None:
             self.annotate(is_filtered=is_filtered)
 
         if gain_to_uV is not None:
             self.set_channel_gains(gain_to_uV)
 
         if offset_to_uV is not None:
             self.set_channel_offsets(offset_to_uV)
 
-        self._kwargs = {'file_paths': [str(e.absolute()) for e in datfiles],
-                        'sampling_frequency': sampling_frequency,
-                        't_starts': t_starts,
-                        'num_chan': num_chan, 'dtype': dtype.str,
-                        'channel_ids': channel_ids, 'time_axis': time_axis, 'file_offset': file_offset,
-                        'gain_to_uV': gain_to_uV, 'offset_to_uV': offset_to_uV,
-                        'is_filtered': is_filtered
-                        }
+        self._kwargs = {
+            "file_paths": [str(e.absolute()) for e in file_path_list],
+            "sampling_frequency": sampling_frequency,
+            "t_starts": t_starts,
+            "num_channels": num_channels,
+            "dtype": dtype.str,
+            "channel_ids": channel_ids,
+            "time_axis": time_axis,
+            "file_offset": file_offset,
+            "gain_to_uV": gain_to_uV,
+            "offset_to_uV": offset_to_uV,
+            "is_filtered": is_filtered,
+        }
 
     @staticmethod
     def write_recording(recording, file_paths, dtype=None, **job_kwargs):
         """
         Save the traces of a recording extractor in binary .dat format.
 
         Parameters
@@ -114,51 +144,86 @@
             Type of the saved data. Default float32.
         {}
         """
         write_binary_recording(recording, file_paths=file_paths, dtype=dtype, **job_kwargs)
 
     def is_binary_compatible(self):
         return True
-        
+
     def get_binary_description(self):
         d = dict(
-            file_paths=self._kwargs['file_paths'],
-            dtype=np.dtype(self._kwargs['dtype']),
-            num_channels=self._kwargs['num_chan'],
-            time_axis=self._kwargs['time_axis'],
-            file_offset=self._kwargs['file_offset'],
+            file_paths=self._kwargs["file_paths"],
+            dtype=np.dtype(self._kwargs["dtype"]),
+            num_channels=self._kwargs["num_channels"],
+            time_axis=self._kwargs["time_axis"],
+            file_offset=self._kwargs["file_offset"],
         )
         return d
 
 
 BinaryRecordingExtractor.write_recording.__doc__ = BinaryRecordingExtractor.write_recording.__doc__.format(
-    _shared_job_kwargs_doc)
+    _shared_job_kwargs_doc
+)
 
 
 class BinaryRecordingSegment(BaseRecordingSegment):
-    def __init__(self, datfile, sampling_frequency, t_start, num_chan, dtype, time_axis, file_offset):
+    def __init__(self, datfile, sampling_frequency, t_start, num_channels, dtype, time_axis, file_offset):
         BaseRecordingSegment.__init__(self, sampling_frequency=sampling_frequency, t_start=t_start)
-        self._timeseries = read_binary_recording(datfile, num_chan, dtype, time_axis, file_offset)
+        self.num_channels = num_channels
+        self.dtype = np.dtype(dtype)
+        self.file_offset = file_offset
+        self.time_axis = time_axis
+        self.datfile = datfile
+        self.file = open(self.datfile, "r")
+        self.num_samples = (Path(datfile).stat().st_size - file_offset) // (num_channels * np.dtype(dtype).itemsize)
+        if self.time_axis == 0:
+            self.shape = (self.num_samples, self.num_channels)
+        else:
+            self.shape = (self.num_channels, self.num_samples)
+
+        byte_offset = self.file_offset
+        dtype_size_bytes = self.dtype.itemsize
+        data_size_bytes = dtype_size_bytes * self.num_samples * self.num_channels
+        self.memmap_offset, self.array_offset = divmod(byte_offset, mmap.ALLOCATIONGRANULARITY)
+        self.memmap_length = data_size_bytes + self.array_offset
 
     def get_num_samples(self) -> int:
         """Returns the number of samples in this signal block
 
         Returns:
             SampleIndex: Number of samples in the signal block
         """
-        return self._timeseries.shape[0]
+        return self.num_samples
+
+    def get_traces(
+        self,
+        start_frame: Union[int, None] = None,
+        end_frame: Union[int, None] = None,
+        channel_indices: Union[List, None] = None,
+    ) -> np.ndarray:
+        length = self.memmap_length
+        memmap_offset = self.memmap_offset
+        memmap_obj = mmap.mmap(self.file.fileno(), length=length, access=mmap.ACCESS_READ, offset=memmap_offset)
+
+        array = np.ndarray.__new__(
+            np.ndarray,
+            shape=self.shape,
+            dtype=self.dtype,
+            buffer=memmap_obj,
+            order="C",
+            offset=self.array_offset,
+        )
+
+        if self.time_axis == 1:
+            array = array.T
 
-    def get_traces(self,
-                   start_frame: Union[int, None] = None,
-                   end_frame: Union[int, None] = None,
-                   channel_indices: Union[List, None] = None,
-                   ) -> np.ndarray:
-        traces = self._timeseries[start_frame:end_frame]
+        traces = array[start_frame:end_frame]
         if channel_indices is not None:
             traces = traces[:, channel_indices]
+
         return traces
 
 
 # For backward compatibility (old good time)
 BinDatRecordingExtractor = BinaryRecordingExtractor
 
 read_binary = define_function_from_class(source_class=BinaryRecordingExtractor, name="read_binary")
```

### Comparing `spikeinterface-0.97.1/spikeinterface/core/channelsaggregationrecording.py` & `spikeinterface-0.98.0/src/spikeinterface/core/channelsaggregationrecording.py`

 * *Files 3% similar despite different names*

```diff
@@ -8,32 +8,34 @@
 class ChannelsAggregationRecording(BaseRecording):
     """
     Class that handles aggregating channels from different recordings, e.g. from different channel groups.
 
     Do not use this class directly but use `si.aggregate_channels(...)`
 
     """
+
     def __init__(self, recording_list, renamed_channel_ids=None):
         channel_map = {}
 
         num_all_channels = sum([rec.get_num_channels() for rec in recording_list])
         if renamed_channel_ids is not None:
-            assert len(np.unique(renamed_channel_ids)) == num_all_channels, "'renamed_channel_ids' doesn't have the " \
-                                                                            "right size or has duplicates!"
+            assert len(np.unique(renamed_channel_ids)) == num_all_channels, (
+                "'renamed_channel_ids' doesn't have the " "right size or has duplicates!"
+            )
             channel_ids = list(renamed_channel_ids)
         else:
             channel_ids = list(np.arange(num_all_channels))
 
         # channel map maps channel indices that are used to get traces
         ch_id = 0
         for r_i, recording in enumerate(recording_list):
             single_channel_ids = recording.get_channel_ids()
             single_channel_indices = recording.ids_to_indices(single_channel_ids)
-            for (chan_id, chan_idx) in zip(single_channel_ids, single_channel_indices):
-                channel_map[ch_id] = {'recording_id': r_i, 'channel_index': chan_idx}
+            for chan_id, chan_idx in zip(single_channel_ids, single_channel_indices):
+                channel_map[ch_id] = {"recording_id": r_i, "channel_index": chan_idx}
                 ch_id += 1
 
         sampling_frequency = recording_list[0].get_sampling_frequency()
         num_segments = recording_list[0].get_num_segments()
         dtype = recording_list[0].get_dtype()
 
         ok1 = all(sampling_frequency == rec.get_sampling_frequency() for rec in recording_list)
@@ -57,99 +59,101 @@
             if all([prop_name in rec.get_property_keys() for rec in recording_list]):
                 for i_r, rec in enumerate(recording_list):
                     prop_value = rec.get_property(prop_name)
                     if i_r == 0:
                         property_dict[prop_name] = prop_value
                     else:
                         try:
-                            property_dict[prop_name] = np.concatenate((property_dict[prop_name],
-                                                                       rec.get_property(prop_name)))
+                            property_dict[prop_name] = np.concatenate(
+                                (property_dict[prop_name], rec.get_property(prop_name))
+                            )
                         except Exception as e:
                             print(f"Skipping property '{prop_name}' for shape inconsistency")
                             del property_dict[prop_name]
                             break
 
         for prop_name, prop_values in property_dict.items():
             if prop_name == "contact_vector":
                 # remap device channel indices correctly
                 prop_values["device_channel_indices"] = np.arange(self.get_num_channels())
             self.set_property(key=prop_name, values=prop_values)
 
         # if locations are present, check that they are all different!
-        if 'location' in self.get_property_keys():
-            location_tuple = [tuple(loc) for loc in self.get_property('location')]
-            assert len(set(location_tuple)) == self.get_num_channels(), "Locations are not unique! " \
-                                                                        "Cannot aggregate recordings!"
+        if "location" in self.get_property_keys():
+            location_tuple = [tuple(loc) for loc in self.get_property("location")]
+            assert len(set(location_tuple)) == self.get_num_channels(), (
+                "Locations are not unique! " "Cannot aggregate recordings!"
+            )
 
         # finally add segments
         for i_seg in range(num_segments):
             parent_segments = [rec._recording_segments[i_seg] for rec in recording_list]
             sub_segment = ChannelsAggregationRecordingSegment(channel_map, parent_segments)
             self.add_recording_segment(sub_segment)
 
         self._recordings = recording_list
-        self._kwargs = {'recording_list': [rec.to_dict() for rec in recording_list],
-                        'renamed_channel_ids': renamed_channel_ids}
+        self._kwargs = {"recording_list": [rec for rec in recording_list], "renamed_channel_ids": renamed_channel_ids}
 
 
 class ChannelsAggregationRecordingSegment(BaseRecordingSegment):
     """
     Class to return a aggregated segment traces.
     """
 
     def __init__(self, channel_map, parent_segments):
         parent_segment0 = parent_segments[0]
         times_kargs0 = parent_segment0.get_times_kwargs()
-        if times_kargs0['time_vector'] is None:
+        if times_kargs0["time_vector"] is None:
             for ps in parent_segments:
-                assert ps.get_times_kwargs()['time_vector'] is None, "All segment should not have times set"
+                assert ps.get_times_kwargs()["time_vector"] is None, "All segment should not have times set"
         else:
             for ps in parent_segments:
-                assert ps.get_times_kwargs()['t_start'] == times_kargs0['t_start'], "All segment should have the same "\
-                                                                                    "t_start"
-            
+                assert ps.get_times_kwargs()["t_start"] == times_kargs0["t_start"], (
+                    "All segment should have the same " "t_start"
+                )
+
         BaseRecordingSegment.__init__(self, **times_kargs0)
         self._channel_map = channel_map
         self._parent_segments = parent_segments
 
     def get_num_samples(self) -> int:
         # num samples are all the same
         return self._parent_segments[0].get_num_samples()
 
-    def get_traces(self,
-                   start_frame: Union[int, None] = None,
-                   end_frame: Union[int, None] = None,
-                   channel_indices: Union[List, None] = None,
-                   ) -> np.ndarray:
-
+    def get_traces(
+        self,
+        start_frame: Union[int, None] = None,
+        end_frame: Union[int, None] = None,
+        channel_indices: Union[List, None] = None,
+    ) -> np.ndarray:
         return_all_channels = False
         if channel_indices is None:
             return_all_channels = True
         elif isinstance(channel_indices, slice):
             if channel_indices == slice(None, None, None):
                 return_all_channels = True
 
         traces = []
         if not return_all_channels:
             if isinstance(channel_indices, slice):
                 # in case channel_indices is slice, it has step 1
                 step = channel_indices.step if channel_indices.step is not None else 1
                 channel_indices = list(range(channel_indices.start, channel_indices.stop, step))
             for channel_idx in channel_indices:
-                segment = self._parent_segments[self._channel_map[channel_idx]['recording_id']]
-                channel_index_recording = self._channel_map[channel_idx]['channel_index']
-                traces_recording = segment.get_traces(channel_indices=[channel_index_recording],
-                                                      start_frame=start_frame,
-                                                      end_frame=end_frame)
+                segment = self._parent_segments[self._channel_map[channel_idx]["recording_id"]]
+                channel_index_recording = self._channel_map[channel_idx]["channel_index"]
+                traces_recording = segment.get_traces(
+                    channel_indices=[channel_index_recording], start_frame=start_frame, end_frame=end_frame
+                )
                 traces.append(traces_recording)
         else:
             for segment in self._parent_segments:
-                traces_all_recording = segment.get_traces(channel_indices=channel_indices,
-                                                          start_frame=start_frame,
-                                                          end_frame=end_frame)
+                traces_all_recording = segment.get_traces(
+                    channel_indices=channel_indices, start_frame=start_frame, end_frame=end_frame
+                )
                 traces.append(traces_all_recording)
         return np.concatenate(traces, axis=1)
 
 
 def aggregate_channels(recording_list, renamed_channel_ids=None):
     """
     Aggregates channels of multiple recording into a single recording object
```

### Comparing `spikeinterface-0.97.1/spikeinterface/core/channelslice.py` & `spikeinterface-0.98.0/src/spikeinterface/core/channelslice.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 from typing import List, Union
 
 import numpy as np
 
 from .baserecording import BaseRecording, BaseRecordingSegment
-from .basesnippets import BaseSnippets, BaseSnippetsSegment 
+from .basesnippets import BaseSnippets, BaseSnippetsSegment
 
 
 class ChannelSliceRecording(BaseRecording):
     """
     Class to slice a Recording object based on channel_ids.
 
     Do not use this class directly but use `recording.channel_slice(...)`
@@ -23,44 +23,55 @@
         self._parent_recording = parent_recording
         self._channel_ids = np.asarray(channel_ids)
         self._renamed_channel_ids = np.asarray(renamed_channel_ids)
 
         parents_chan_ids = self._parent_recording.get_channel_ids()
 
         # some checks
-        assert all(chan_id in parents_chan_ids for chan_id in self._channel_ids), 'ChannelSliceRecording : channel ids are not all in parents'
-        assert len(self._channel_ids) == len(self._renamed_channel_ids), 'ChannelSliceRecording: renamed channel_ids must be the same size'
-        assert self._channel_ids.size == np.unique(self._channel_ids).size, 'ChannelSliceRecording : channel_ids not unique'
+        assert all(
+            chan_id in parents_chan_ids for chan_id in self._channel_ids
+        ), "ChannelSliceRecording : channel ids are not all in parents"
+        assert len(self._channel_ids) == len(
+            self._renamed_channel_ids
+        ), "ChannelSliceRecording: renamed channel_ids must be the same size"
+        assert (
+            self._channel_ids.size == np.unique(self._channel_ids).size
+        ), "ChannelSliceRecording : channel_ids not unique"
 
         sampling_frequency = parent_recording.get_sampling_frequency()
 
-        BaseRecording.__init__(self,
-                               sampling_frequency=sampling_frequency,
-                               channel_ids=self._renamed_channel_ids,
-                               dtype=parent_recording.get_dtype())
+        BaseRecording.__init__(
+            self,
+            sampling_frequency=sampling_frequency,
+            channel_ids=self._renamed_channel_ids,
+            dtype=parent_recording.get_dtype(),
+        )
 
         self._parent_channel_indices = parent_recording.ids_to_indices(self._channel_ids)
 
         # link recording segment
         for parent_segment in self._parent_recording._recording_segments:
             sub_segment = ChannelSliceRecordingSegment(parent_segment, self._parent_channel_indices)
             self.add_recording_segment(sub_segment)
 
         # copy annotation and properties
         parent_recording.copy_metadata(self, only_main=False, ids=self._channel_ids)
 
         # change the wiring of the probe
-        contact_vector = self.get_property('contact_vector')
+        contact_vector = self.get_property("contact_vector")
         if contact_vector is not None:
-            contact_vector['device_channel_indices'] = np.arange(len(channel_ids), dtype='int64')
-            self.set_property('contact_vector', contact_vector)
+            contact_vector["device_channel_indices"] = np.arange(len(channel_ids), dtype="int64")
+            self.set_property("contact_vector", contact_vector)
 
         # update dump dict
-        self._kwargs = {'parent_recording': parent_recording.to_dict(), 'channel_ids': channel_ids,
-                        'renamed_channel_ids': renamed_channel_ids}
+        self._kwargs = {
+            "parent_recording": parent_recording,
+            "channel_ids": channel_ids,
+            "renamed_channel_ids": renamed_channel_ids,
+        }
 
 
 class ChannelSliceRecordingSegment(BaseRecordingSegment):
     """
     Class to return a channel-sliced segment traces.
     """
 
@@ -68,19 +79,20 @@
         BaseRecordingSegment.__init__(self, **parent_recording_segment.get_times_kwargs())
         self._parent_recording_segment = parent_recording_segment
         self._parent_channel_indices = parent_channel_indices
 
     def get_num_samples(self) -> int:
         return self._parent_recording_segment.get_num_samples()
 
-    def get_traces(self,
-                   start_frame: Union[int, None] = None,
-                   end_frame: Union[int, None] = None,
-                   channel_indices: Union[List, None] = None,
-                   ) -> np.ndarray:
+    def get_traces(
+        self,
+        start_frame: Union[int, None] = None,
+        end_frame: Union[int, None] = None,
+        channel_indices: Union[List, None] = None,
+    ) -> np.ndarray:
         parent_indices = self._parent_channel_indices[channel_indices]
         traces = self._parent_recording_segment.get_traces(start_frame, end_frame, parent_indices)
         return traces
 
 
 class ChannelSliceSnippets(BaseSnippets):
     """
@@ -99,73 +111,83 @@
         self._parent_snippets = parent_snippets
         self._channel_ids = np.asarray(channel_ids)
         self._renamed_channel_ids = np.asarray(renamed_channel_ids)
 
         parents_chan_ids = self._parent_snippets.get_channel_ids()
 
         # some checks
-        assert all(chan_id in parents_chan_ids for chan_id in self._channel_ids), 'ChannelSliceSnippets : channel ids are not all in parents'
-        assert len(self._channel_ids) == len(self._renamed_channel_ids), 'ChannelSliceSnippets: renamed channel_ids must be the same size'
-        assert self._channel_ids.size == np.unique(self._channel_ids).size, 'ChannelSliceSnippets : channel_ids not unique'
+        assert all(
+            chan_id in parents_chan_ids for chan_id in self._channel_ids
+        ), "ChannelSliceSnippets : channel ids are not all in parents"
+        assert len(self._channel_ids) == len(
+            self._renamed_channel_ids
+        ), "ChannelSliceSnippets: renamed channel_ids must be the same size"
+        assert (
+            self._channel_ids.size == np.unique(self._channel_ids).size
+        ), "ChannelSliceSnippets : channel_ids not unique"
 
         sampling_frequency = parent_snippets.get_sampling_frequency()
 
-        BaseSnippets.__init__(self,
-                              sampling_frequency=sampling_frequency,
-                              nbefore=parent_snippets.nbefore,
-                              snippet_len=parent_snippets.snippet_len,
-                              channel_ids=self._renamed_channel_ids,
-                              dtype=parent_snippets.get_dtype())
+        BaseSnippets.__init__(
+            self,
+            sampling_frequency=sampling_frequency,
+            nbefore=parent_snippets.nbefore,
+            snippet_len=parent_snippets.snippet_len,
+            channel_ids=self._renamed_channel_ids,
+            dtype=parent_snippets.get_dtype(),
+        )
 
         self._parent_channel_indices = parent_snippets.ids_to_indices(self._channel_ids)
 
         # link recording segment
         for parent_segment in self._parent_snippets._snippets_segments:
             sub_segment = ChannelSliceSnippetsSegment(parent_segment, self._parent_channel_indices)
             self.add_snippets_segment(sub_segment)
 
         # copy annotation and properties
         parent_snippets.copy_metadata(self, only_main=False, ids=self._channel_ids)
 
         # change the wiring of the probe
-        contact_vector = self.get_property('contact_vector')
+        contact_vector = self.get_property("contact_vector")
         if contact_vector is not None:
-            contact_vector['device_channel_indices'] = np.arange(len(channel_ids), dtype='int64')
-            self.set_property('contact_vector', contact_vector)
+            contact_vector["device_channel_indices"] = np.arange(len(channel_ids), dtype="int64")
+            self.set_property("contact_vector", contact_vector)
 
         # update dump dict
-        self._kwargs = {'parent_snippets': parent_snippets.to_dict(), 'channel_ids': channel_ids,
-                        'renamed_channel_ids': renamed_channel_ids}
+        self._kwargs = {
+            "parent_snippets": parent_snippets,
+            "channel_ids": channel_ids,
+            "renamed_channel_ids": renamed_channel_ids,
+        }
 
 
 class ChannelSliceSnippetsSegment(BaseSnippetsSegment):
     """
     Class to return a channel-sliced segment snippets.
     """
 
     def __init__(self, parent_snippets_segment, parent_channel_indices):
         BaseSnippetsSegment.__init__(self)
         self._parent_snippets_segment = parent_snippets_segment
         self._parent_channel_indices = parent_channel_indices
 
     def get_num_snippets(self) -> int:
         return self._parent_snippets_segment.get_num_snippets()
-    
-    def frames_to_indices(self,
-                          start_frame: Union[int, None] = None,
-                          end_frame: Union[int, None] = None):
+
+    def frames_to_indices(self, start_frame: Union[int, None] = None, end_frame: Union[int, None] = None):
         return self._parent_snippets_segment.frames_to_indices(start_frame, end_frame)
 
     def get_frames(self, indices=None):
         return self._parent_snippets_segment.get_frames(indices)
-    
-    def get_snippets(self,
-                     indices,
-                     channel_indices: Union[List, None] = None,
-                     ) -> np.ndarray:
+
+    def get_snippets(
+        self,
+        indices,
+        channel_indices: Union[List, None] = None,
+    ) -> np.ndarray:
         """
         Return the snippets, optionally for a subset of samples and/or channels
 
         Parameters
         ----------
         indexes: (Union[int, None], optional)
             start sample index, or zero if None. Defaults to None.
@@ -177,8 +199,8 @@
         Returns
         -------
         snippets: np.ndarray
             Array of snippets, num_snippets x num_samples x num_channels
         """
         parent_indices = self._parent_channel_indices[channel_indices]
         snippets = self._parent_snippets_segment.get_snippets(indices, parent_indices)
-        return snippets
+        return snippets
```

### Comparing `spikeinterface-0.97.1/spikeinterface/core/core_tools.py` & `spikeinterface-0.98.0/src/spikeinterface/core/core_tools.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,38 +1,35 @@
 from pathlib import Path
+from typing import Union
 import os
 import sys
 import datetime
+import json
 from copy import deepcopy
 import gc
+import mmap
+import inspect
 
 import numpy as np
 from tqdm import tqdm
-import inspect
-
-from .job_tools import (ensure_chunk_size, ensure_n_jobs, divide_segment_into_chunks, fix_job_kwargs, 
-                        ChunkRecordingExecutor, _shared_job_kwargs_doc)
 
-def copy_signature(source_fct):
-    def copy(target_fct):
-        target_fct.__signature__ = inspect.signature(source_fct)
-        return target_fct
-    return copy
+from .job_tools import (
+    ensure_chunk_size,
+    ensure_n_jobs,
+    divide_segment_into_chunks,
+    fix_job_kwargs,
+    ChunkRecordingExecutor,
+    _shared_job_kwargs_doc,
+)
 
 
 def define_function_from_class(source_class, name):
+    "Wrapper to change the name of a class"
 
-    @copy_signature(source_class)
-    def reader_func(*args, **kwargs):
-        return source_class(*args, **kwargs)
-
-    reader_func.__doc__ = source_class.__doc__
-    reader_func.__name__ = name
-
-    return reader_func
+    return source_class
 
 
 def read_python(path):
     """Parses python scripts in a dictionary
 
     Parameters
     ----------
@@ -43,19 +40,20 @@
     -------
     metadata:
         dictionary containing parsed file
 
     """
     from six import exec_
     import re
+
     path = Path(path).absolute()
     assert path.is_file()
-    with path.open('r') as f:
+    with path.open("r") as f:
         contents = f.read()
-    contents = re.sub(r'range\(([\d,]*)\)', r'list(range(\1))', contents)
+    contents = re.sub(r"range\(([\d,]*)\)", r"list(range(\1))", contents)
     metadata = {}
     exec_(contents, {}, metadata)
     metadata = {k.lower(): v for (k, v) in metadata.items()}
     return metadata
 
 
 def write_python(path, dict):
@@ -64,161 +62,199 @@
     Parameters
     ----------
     path: str or Path
         Path to save file
     dict: dict
         dictionary to save
     """
-    with Path(path).open('w') as f:
+    with Path(path).open("w") as f:
         for k, v in dict.items():
             if isinstance(v, str) and not v.startswith("'"):
-                if 'path' in k and 'win' in sys.platform:
+                if "path" in k and "win" in sys.platform:
                     f.write(str(k) + " = r'" + str(v) + "'\n")
                 else:
                     f.write(str(k) + " = '" + str(v) + "'\n")
             else:
                 f.write(str(k) + " = " + str(v) + "\n")
 
 
-def check_json(d):
-    dc = deepcopy(d)
-    # quick hack to ensure json writable
-    for k, v in d.items():
-        # take care of keys first
-        if isinstance(k, np.integer):
-            del dc[k]
-            dc[int(k)] = v
-        if isinstance(k, np.floating):
-            del dc[k]
-            dc[float(k)] = v
-        if isinstance(v, dict):
-            dc[k] = check_json(v)
-        elif isinstance(v, Path):
-            dc[k] = str(v.absolute())
-        elif isinstance(v, (bool, np.bool_)):
-            dc[k] = bool(v)
-        elif isinstance(v, np.integer):
-            dc[k] = int(v)
-        elif isinstance(v, np.floating):
-            dc[k] = float(v)
-        elif isinstance(v, bytes):
-            dc[k] = v.decode()
-        elif isinstance(v, datetime.datetime):
-            dc[k] = v.isoformat()
-        elif isinstance(v, (np.ndarray, list)):
-            if len(v) > 0:
-                if isinstance(v[0], dict):
-                    # these must be extractors for multi extractors
-                    dc[k] = [check_json(v_el) for v_el in v]
-                else:
-                    v_arr = np.array(v)
-                    if v_arr.dtype.kind not in ("b", "i", "u", "f", "S", "U", "O"):
-                        print(f'Skipping field {k}: only int, uint, bool, float, or str types can be serialized')
-                        continue
-                    # 64-bit types are not serializable
-                    if v_arr.dtype == np.dtype('int64'):
-                        v_arr = v_arr.astype('int32')
-                    if v_arr.dtype == np.dtype('float64'):
-                        v_arr = v_arr.astype('float32')
-                    # np.bool_ needs to be cast as bool
-                    if v_arr.dtype == np.bool_:
-                        v_arr = v_arr.astype(bool)
-                    # for object types O, if they are actually str cast it
-                    # this is the case when loading a pandas column
-                    if v_arr.dtype.kind == "O":
-                        if isinstance(v_arr[0], str):
-                            v_arr = v_arr.astype('str')
-                        else:
-                            print(f'Skipping field {k}: Object type cannot be serialized')
-                            continue
-                    dc[k] = v_arr.tolist()
-            else:
-                # this is for empty arrays
-                dc[k] = list(v)
-    return dc
+class SIJsonEncoder(json.JSONEncoder):
+    """
+    An encoder used to encode Spike interface objects to json
+    """
+
+    def default(self, obj):
+        from spikeinterface.core.base import BaseExtractor
+
+        # Over-write behaviors for datetime object
+        if isinstance(obj, datetime.datetime):
+            return obj.isoformat()
+
+        # This should transforms integer, floats and bool to their python counterparts
+        if isinstance(obj, np.generic):
+            return obj.item()
+
+        if np.issctype(obj):  # Cast numpy datatypes to their names
+            return np.dtype(obj).name
+
+        if isinstance(obj, np.ndarray):
+            return obj.tolist()
+
+        if isinstance(obj, BaseExtractor):
+            return obj.to_dict()
+
+        # The base-class handles the assertion
+        return super().default(obj)
+
+    # This machinery is necessary for overriding the default behavior of the json encoder with keys
+    # This is a deep issue that goes deep down to cpython: https://github.com/python/cpython/issues/63020
+    # This object is called before encoding (so it pre-processes the object to not have numpy scalars)
+    def iterencode(self, obj, _one_shot=False):
+        return super().iterencode(self.remove_numpy_scalars(obj), _one_shot=_one_shot)
+
+    def remove_numpy_scalars(self, object):
+        from spikeinterface.core.base import BaseExtractor
+
+        if isinstance(object, dict):
+            return self.remove_numpy_scalars_in_dict(object)
+        elif isinstance(object, (list, tuple, set)):
+            return self.remove_numpy_scalars_in_list(object)
+        elif isinstance(object, BaseExtractor):
+            return self.remove_numpy_scalars_in_dict(object.to_dict())
+        else:
+            return object.item() if isinstance(object, np.generic) else object
+
+    def remove_numpy_scalars_in_list(self, list_: Union[list, tuple, set]) -> list:
+        return [self.remove_numpy_scalars(obj) for obj in list_]
+
+    def remove_numpy_scalars_in_dict(self, dictionary: dict) -> dict:
+        dict_copy = dict()
+        for key, value in dictionary.items():
+            key = self.remove_numpy_scalars(key)
+            value = self.remove_numpy_scalars(value)
+            dict_copy[key] = value
+
+        return dict_copy
+
+
+def check_json(dictionary: dict) -> dict:
+    """
+    Function that transforms a dictionary with spikeinterface objects into a json writable dictionary
+
+    Parameters
+    ----------
+    dictionary : A dictionary
+
+    """
+
+    json_string = json.dumps(dictionary, indent=4, cls=SIJsonEncoder)
+    return json.loads(json_string)
 
 
 def add_suffix(file_path, possible_suffix):
     file_path = Path(file_path)
     if isinstance(possible_suffix, str):
         possible_suffix = [possible_suffix]
-    possible_suffix = [s if s.startswith('.') else '.' + s for s in possible_suffix]
+    possible_suffix = [s if s.startswith(".") else "." + s for s in possible_suffix]
     if file_path.suffix not in possible_suffix:
-        file_path = file_path.parent / (file_path.name + '.' + possible_suffix[0])
+        file_path = file_path.parent / (file_path.name + "." + possible_suffix[0])
     return file_path
 
 
-def read_binary_recording(file, num_chan, dtype, time_axis=0, offset=0):
-    '''
+def read_binary_recording(file, num_channels, dtype, time_axis=0, offset=0):
+    """
     Read binary .bin or .dat file.
 
     Parameters
     ----------
     file: str
         File name
-    num_chan: int
+    num_channels: int
         Number of channels
     dtype: dtype
         dtype of the file
     time_axis: 0 (default) or 1
         If 0 then traces are transposed to ensure (nb_sample, nb_channel) in the file.
         If 1, the traces shape (nb_channel, nb_sample) is kept in the file.
     offset: int
         number of offset bytes
 
-    '''
-    num_chan = int(num_chan)
+    """
+    num_channels = int(num_channels)
     with Path(file).open() as f:
-        nsamples = (os.fstat(f.fileno()).st_size - offset) // (num_chan * np.dtype(dtype).itemsize)
+        nsamples = (os.fstat(f.fileno()).st_size - offset) // (num_channels * np.dtype(dtype).itemsize)
     if time_axis == 0:
-        samples = np.memmap(file, np.dtype(dtype), mode='r', offset=offset, shape=(nsamples, num_chan))
+        samples = np.memmap(file, np.dtype(dtype), mode="r", offset=offset, shape=(nsamples, num_channels))
     else:
-        samples = np.memmap(file, np.dtype(dtype), mode='r', offset=offset, shape=(num_chan, nsamples)).T
+        samples = np.memmap(file, np.dtype(dtype), mode="r", offset=offset, shape=(num_channels, nsamples)).T
     return samples
 
 
 # used by write_binary_recording + ChunkRecordingExecutor
-def _init_binary_worker(recording, rec_memmaps_dict, dtype, cast_unsigned):
+def _init_binary_worker(recording, file_path_dict, dtype, byte_offest, cast_unsigned):
     # create a local dict per worker
     worker_ctx = {}
-    if isinstance(recording, dict):
-        from spikeinterface.core import load_extractor
-        worker_ctx['recording'] = load_extractor(recording)
-    else:
-        worker_ctx['recording'] = recording
-
-    rec_memmaps = []
-    for d in rec_memmaps_dict:
-        rec_memmaps.append(np.memmap(**d))
+    worker_ctx["recording"] = recording
+    worker_ctx["byte_offset"] = byte_offest
+    worker_ctx["dtype"] = np.dtype(dtype)
+    worker_ctx["cast_unsigned"] = cast_unsigned
 
-    worker_ctx['rec_memmaps'] = rec_memmaps
-    worker_ctx['dtype'] = np.dtype(dtype)
-    worker_ctx['cast_unsigned'] = cast_unsigned
+    file_dict = {segment_index: open(file_path, "r+") for segment_index, file_path in file_path_dict.items()}
+    worker_ctx["file_dict"] = file_dict
 
     return worker_ctx
 
 
 # used by write_binary_recording + ChunkRecordingExecutor
 def _write_binary_chunk(segment_index, start_frame, end_frame, worker_ctx):
     # recover variables of the worker
-    recording = worker_ctx['recording']
-    dtype = worker_ctx['dtype']
-    rec_memmap = worker_ctx['rec_memmaps'][segment_index]
-    cast_unsigned = worker_ctx['cast_unsigned']
-
-    # apply function
-    traces = recording.get_traces(start_frame=start_frame, end_frame=end_frame, segment_index=segment_index,
-                                  cast_unsigned=cast_unsigned)
-    traces = traces.astype(dtype)
-    rec_memmap[start_frame:end_frame, :] = traces
+    recording = worker_ctx["recording"]
+    dtype = worker_ctx["dtype"]
+    byte_offset = worker_ctx["byte_offset"]
+    cast_unsigned = worker_ctx["cast_unsigned"]
+    file = worker_ctx["file_dict"][segment_index]
 
+    # Open the memmap
+    # What we need is the file_path
+    num_channels = recording.get_num_channels()
+    num_frames = recording.get_num_frames(segment_index=segment_index)
+    shape = (num_frames, num_channels)
+    dtype_size_bytes = np.dtype(dtype).itemsize
+    data_size_bytes = dtype_size_bytes * num_frames * num_channels
+
+    # Offset (The offset needs to be multiple of the page size)
+    # The mmap offset is associated to be as big as possible but still a multiple of the page size
+    # The array offset takes care of the reminder
+    mmap_offset, array_offset = divmod(byte_offset, mmap.ALLOCATIONGRANULARITY)
+    mmmap_length = data_size_bytes + array_offset
+    memmap_obj = mmap.mmap(file.fileno(), length=mmmap_length, access=mmap.ACCESS_WRITE, offset=mmap_offset)
 
-def write_binary_recording(recording, file_paths=None, dtype=None, add_file_extension=True,
-                           verbose=False, byte_offset=0, auto_cast_uint=True, **job_kwargs):
-    '''
+    array = np.ndarray.__new__(np.ndarray, shape=shape, dtype=dtype, buffer=memmap_obj, order="C", offset=array_offset)
+    # apply function
+    traces = recording.get_traces(
+        start_frame=start_frame, end_frame=end_frame, segment_index=segment_index, cast_unsigned=cast_unsigned
+    )
+    if traces.dtype != dtype:
+        traces = traces.astype(dtype)
+    array[start_frame:end_frame, :] = traces
+
+    # Close the memmap
+    memmap_obj.flush()
+
+
+def write_binary_recording(
+    recording,
+    file_paths=None,
+    dtype=None,
+    add_file_extension=True,
+    byte_offset=0,
+    auto_cast_uint=True,
+    **job_kwargs,
+):
+    """
     Save the trace of a recording extractor in several binary .dat format.
 
     Note :
         time_axis is always 0 (contrary to previous version.
         to get time_axis=1 (which is a bad idea) use `write_binary_recording_file_handle()`
 
     Parameters
@@ -227,77 +263,76 @@
         The recording extractor object to be saved in .dat format
     file_path: str
         The path to the file.
     dtype: dtype
         Type of the saved data. Default float32.
     add_file_extension: bool
         If True (default), file the '.raw' file extension is added if the file name is not a 'raw', 'bin', or 'dat'
-    verbose: bool
-        If True, output is verbose (when chunks are used)
     byte_offset: int
         Offset in bytes (default 0) to for the binary file (e.g. to write a header)
     auto_cast_uint: bool
         If True (default), unsigned integers are automatically cast to int if the specified dtype is signed
     {}
-    '''
+    """
     assert file_paths is not None, "Provide 'file_path'"
     job_kwargs = fix_job_kwargs(job_kwargs)
 
-    if not isinstance(file_paths, list):
-        file_paths = [file_paths]
-    file_paths = [Path(e) for e in file_paths]
+    file_path_list = [file_paths] if not isinstance(file_paths, list) else file_paths
+    num_segments = recording.get_num_segments()
+    if len(file_path_list) != num_segments:
+        raise ValueError("'file_paths' must be a list of the same size as the number of segments in the recording")
+
+    file_path_list = [Path(file_path) for file_path in file_path_list]
     if add_file_extension:
-        file_paths = [add_suffix(file_path, ['raw', 'bin', 'dat']) for file_path in file_paths]
+        file_path_list = [add_suffix(file_path, ["raw", "bin", "dat"]) for file_path in file_path_list]
 
-    if dtype is None:
-        dtype = recording.get_dtype()
+    dtype = dtype if dtype is not None else recording.get_dtype()
+    cast_unsigned = False
     if auto_cast_uint:
         cast_unsigned = determine_cast_unsigned(recording, dtype)
-    else:
-        cast_unsigned = False
 
-    # create memmap files
-    rec_memmaps = []
-    rec_memmaps_dict = []
-    for segment_index in range(recording.get_num_segments()):
-        num_frames = recording.get_num_samples(segment_index)
-        num_channels = recording.get_num_channels()
-        file_path = file_paths[segment_index]
-        shape = (num_frames, num_channels)
-        rec_memmap = np.memmap(str(file_path), dtype=dtype, mode='w+', offset=byte_offset, shape=shape)
-        rec_memmaps.append(rec_memmap)
-        rec_memmaps_dict.append(dict(filename=str(file_path), dtype=dtype, mode='r+', offset=byte_offset, shape=shape))
+    dtype_size_bytes = np.dtype(dtype).itemsize
+    num_channels = recording.get_num_channels()
+
+    file_path_dict = {segment_index: file_path for segment_index, file_path in enumerate(file_path_list)}
+    for segment_index, file_path in file_path_dict.items():
+        num_frames = recording.get_num_frames(segment_index=segment_index)
+        data_size_bytes = dtype_size_bytes * num_frames * num_channels
+        file_size_bytes = data_size_bytes + byte_offset
+
+        file = open(file_path, "wb+")
+        file.truncate(file_size_bytes)
+        file.close()
+        assert Path(file_path).is_file()
 
     # use executor (loop or workers)
     func = _write_binary_chunk
     init_func = _init_binary_worker
-    n_jobs = ensure_n_jobs(recording, n_jobs=job_kwargs.get('n_jobs', 1))
-    if n_jobs == 1:
-        init_args = (recording, rec_memmaps_dict, dtype, cast_unsigned)
-    else:
-        init_args = (recording.to_dict(), rec_memmaps_dict, dtype, cast_unsigned)
-    executor = ChunkRecordingExecutor(recording, func, init_func, init_args, verbose=verbose,
-                                      job_name='write_binary_recording', **job_kwargs)
+    init_args = (recording, file_path_dict, dtype, byte_offset, cast_unsigned)
+    executor = ChunkRecordingExecutor(
+        recording, func, init_func, init_args, job_name="write_binary_recording", **job_kwargs
+    )
     executor.run()
 
 
 write_binary_recording.__doc__ = write_binary_recording.__doc__.format(_shared_job_kwargs_doc)
 
 
-def write_binary_recording_file_handle(recording, file_handle=None,
-                                       time_axis=0, dtype=None, byte_offset=0, verbose=False, **job_kwargs):
+def write_binary_recording_file_handle(
+    recording, file_handle=None, time_axis=0, dtype=None, byte_offset=0, verbose=False, **job_kwargs
+):
     """
     Old variant version of write_binary_recording with one file handle.
     Can be useful in some case ???
     Not used anymore at the moment.
 
     @ SAM useful for writing with time_axis=1!
     """
     assert file_handle is not None
-    assert recording.get_num_segments() == 1, 'If file_handle is given then only deals with one segment'
+    assert recording.get_num_segments() == 1, "If file_handle is given then only deals with one segment"
 
     if dtype is None:
         dtype = recording.get_dtype()
 
     job_kwargs = fix_job_kwargs(job_kwargs)
     chunk_size = ensure_chunk_size(recording, **job_kwargs)
 
@@ -310,79 +345,76 @@
         traces = recording.get_traces(segment_index=0)
         if time_axis == 1:
             traces = traces.T
         if dtype is not None:
             traces = traces.astype(dtype)
         traces.tofile(file_handle)
     else:
-
         num_frames = recording.get_num_samples(segment_index=0)
         chunks = divide_segment_into_chunks(num_frames, chunk_size)
 
         for start_frame, end_frame in chunks:
-            traces = recording.get_traces(segment_index=0,
-                                          start_frame=start_frame, end_frame=end_frame)
+            traces = recording.get_traces(segment_index=0, start_frame=start_frame, end_frame=end_frame)
             if time_axis == 1:
                 traces = traces.T
             if dtype is not None:
                 traces = traces.astype(dtype)
             file_handle.write(traces.tobytes())
 
 
 # used by write_memory_recording
 def _init_memory_worker(recording, arrays, shm_names, shapes, dtype, cast_unsigned):
     # create a local dict per worker
     worker_ctx = {}
     if isinstance(recording, dict):
         from spikeinterface.core import load_extractor
-        worker_ctx['recording'] = load_extractor(recording)
+
+        worker_ctx["recording"] = load_extractor(recording)
     else:
-        worker_ctx['recording'] = recording
+        worker_ctx["recording"] = recording
 
-    worker_ctx['dtype'] = np.dtype(dtype)
+    worker_ctx["dtype"] = np.dtype(dtype)
 
     if arrays is None:
         # create it from share memory name
         from multiprocessing.shared_memory import SharedMemory
+
         arrays = []
         # keep shm alive
-        worker_ctx['shms'] = []
+        worker_ctx["shms"] = []
         for i in range(len(shm_names)):
             shm = SharedMemory(shm_names[i])
-            worker_ctx['shms'].append(shm)
+            worker_ctx["shms"].append(shm)
             arr = np.ndarray(shape=shapes[i], dtype=dtype, buffer=shm.buf)
             arrays.append(arr)
 
-    worker_ctx['arrays'] = arrays
-    worker_ctx['cast_unsigned'] = cast_unsigned
+    worker_ctx["arrays"] = arrays
+    worker_ctx["cast_unsigned"] = cast_unsigned
 
     return worker_ctx
 
 
 # used by write_memory_recording
 def _write_memory_chunk(segment_index, start_frame, end_frame, worker_ctx):
     # recover variables of the worker
-    recording = worker_ctx['recording']
-    dtype = worker_ctx['dtype']
-    arr = worker_ctx['arrays'][segment_index]
-    cast_unsigned = worker_ctx['cast_unsigned']
+    recording = worker_ctx["recording"]
+    dtype = worker_ctx["dtype"]
+    arr = worker_ctx["arrays"][segment_index]
+    cast_unsigned = worker_ctx["cast_unsigned"]
 
     # apply function
-    traces = recording.get_traces(start_frame=start_frame, end_frame=end_frame, segment_index=segment_index,
-                                  cast_unsigned=cast_unsigned)
+    traces = recording.get_traces(
+        start_frame=start_frame, end_frame=end_frame, segment_index=segment_index, cast_unsigned=cast_unsigned
+    )
     traces = traces.astype(dtype)
     arr[start_frame:end_frame, :] = traces
 
 
 def make_shared_array(shape, dtype):
-    # https://docs.python.org/3/library/multiprocessing.shared_memory.html
-    try:
-        from multiprocessing.shared_memory import SharedMemory
-    except Exception as e:
-        raise Exception('SharedMemory is available only for python>=3.8')
+    from multiprocessing.shared_memory import SharedMemory
 
     dtype = np.dtype(dtype)
     nbytes = int(np.prod(shape) * dtype.itemsize)
     shm = SharedMemory(name=None, create=True, size=nbytes)
     arr = np.ndarray(shape=shape, dtype=dtype, buffer=shm.buf)
     arr[:] = 0
 
@@ -407,28 +439,28 @@
     {}
 
     Returns
     ---------
     arrays: one arrays per segment
     """
     job_kwargs = fix_job_kwargs(job_kwargs)
-    chunk_size = ensure_chunk_size(recording, **job_kwargs)
-    n_jobs = ensure_n_jobs(recording, n_jobs=job_kwargs.get('n_jobs', 1))
 
     if dtype is None:
         dtype = recording.get_dtype()
     if auto_cast_uint:
         cast_unsigned = determine_cast_unsigned(recording, dtype)
     else:
         cast_unsigned = False
 
     # create sharedmmep
     arrays = []
     shm_names = []
     shapes = []
+
+    n_jobs = ensure_n_jobs(recording, n_jobs=job_kwargs.get("n_jobs", 1))
     for segment_index in range(recording.get_num_segments()):
         num_frames = recording.get_num_samples(segment_index)
         num_channels = recording.get_num_channels()
         shape = (num_frames, num_channels)
         shapes.append(shape)
         if n_jobs > 1:
             arr, shm = make_shared_array(shape, dtype)
@@ -437,31 +469,44 @@
             arr = np.zeros(shape, dtype=dtype)
         arrays.append(arr)
 
     # use executor (loop or workers)
     func = _write_memory_chunk
     init_func = _init_memory_worker
     if n_jobs > 1:
-        init_args = (recording.to_dict(), None, shm_names, shapes, dtype, cast_unsigned)
+        init_args = (recording, None, shm_names, shapes, dtype, cast_unsigned)
     else:
         init_args = (recording, arrays, None, None, dtype, cast_unsigned)
 
-    executor = ChunkRecordingExecutor(recording, func, init_func, init_args, verbose=verbose,
-                                      job_name='write_memory_recording', **job_kwargs)
+    executor = ChunkRecordingExecutor(
+        recording, func, init_func, init_args, verbose=verbose, job_name="write_memory_recording", **job_kwargs
+    )
     executor.run()
 
     return arrays
 
 
 write_memory_recording.__doc__ = write_memory_recording.__doc__.format(_shared_job_kwargs_doc)
 
 
-def write_to_h5_dataset_format(recording, dataset_path, segment_index, save_path=None, file_handle=None,
-                               time_axis=0, single_axis=False, dtype=None, chunk_size=None, chunk_memory='500M',
-                               verbose=False, auto_cast_uint=True, return_scaled=False):
+def write_to_h5_dataset_format(
+    recording,
+    dataset_path,
+    segment_index,
+    save_path=None,
+    file_handle=None,
+    time_axis=0,
+    single_axis=False,
+    dtype=None,
+    chunk_size=None,
+    chunk_memory="500M",
+    verbose=False,
+    auto_cast_uint=True,
+    return_scaled=False,
+):
     """
     Save the traces of a recording extractor in an h5 dataset.
 
     Parameters
     ----------
     recording: RecordingExtractor
         The recording extractor object to be saved in .dat format
@@ -491,30 +536,31 @@
     auto_cast_uint: bool
         If True (default), unsigned integers are automatically cast to int if the specified dtype is signed
     return_scaled : bool, optional
         If True and the recording has scaling (gain_to_uV and offset_to_uV properties),
         traces are dumped to uV, by default False
     """
     import h5py
+
     # ~ assert HAVE_H5, "To write to h5 you need to install h5py: pip install h5py"
     assert save_path is not None or file_handle is not None, "Provide 'save_path' or 'file handle'"
 
     if save_path is not None:
         save_path = Path(save_path)
-        if save_path.suffix == '':
+        if save_path.suffix == "":
             # when suffix is already raw/bin/dat do not change it.
-            save_path = save_path.parent / (save_path.name + '.h5')
+            save_path = save_path.parent / (save_path.name + ".h5")
 
     num_channels = recording.get_num_channels()
     num_frames = recording.get_num_frames(segment_index=0)
 
     if file_handle is not None:
         assert isinstance(file_handle, h5py.File)
     else:
-        file_handle = h5py.File(save_path, 'w')
+        file_handle = h5py.File(save_path, "w")
 
     if dtype is None:
         dtype_file = recording.get_dtype()
     else:
         dtype_file = dtype
     if auto_cast_uint:
         cast_unsigned = determine_cast_unsigned(recording, dtype)
@@ -550,42 +596,54 @@
         if num_frames % chunk_size > 0:
             n_chunk += 1
         if verbose:
             chunks = tqdm(range(n_chunk), ascii=True, desc="Writing to .h5 file")
         else:
             chunks = range(n_chunk)
         for i in chunks:
-            traces = recording.get_traces(segment_index=segment_index,
-                                          start_frame=i * chunk_size,
-                                          end_frame=min((i + 1) * chunk_size, num_frames),
-                                          cast_unsigned=cast_unsigned, return_scaled=return_scaled)
+            traces = recording.get_traces(
+                segment_index=segment_index,
+                start_frame=i * chunk_size,
+                end_frame=min((i + 1) * chunk_size, num_frames),
+                cast_unsigned=cast_unsigned,
+                return_scaled=return_scaled,
+            )
             chunk_frames = traces.shape[0]
             if dtype is not None:
                 traces = traces.astype(dtype_file)
             if single_axis:
-                dset[chunk_start:chunk_start + chunk_frames] = traces[:, 0]
+                dset[chunk_start : chunk_start + chunk_frames] = traces[:, 0]
             else:
                 if time_axis == 0:
-                    dset[chunk_start:chunk_start + chunk_frames, :] = traces
+                    dset[chunk_start : chunk_start + chunk_frames, :] = traces
                 else:
-                    dset[:, chunk_start:chunk_start + chunk_frames] = traces.T
+                    dset[:, chunk_start : chunk_start + chunk_frames] = traces.T
 
             chunk_start += chunk_frames
 
     if save_path is not None:
         file_handle.close()
     return save_path
 
 
-def write_traces_to_zarr(recording, zarr_root, zarr_path, storage_options, 
-                         dataset_paths, channel_chunk_size=None, dtype=None,
-                         compressor=None, filters=None, 
-                         verbose=False, auto_cast_uint=True, 
-                         **job_kwargs):
-    '''
+def write_traces_to_zarr(
+    recording,
+    zarr_root,
+    zarr_path,
+    storage_options,
+    dataset_paths,
+    channel_chunk_size=None,
+    dtype=None,
+    compressor=None,
+    filters=None,
+    verbose=False,
+    auto_cast_uint=True,
+    **job_kwargs,
+):
+    """
     Save the trace of a recording extractor in several zarr format.
 
 
     Parameters
     ----------
     recording: RecordingExtractor
         The recording extractor object to be saved in .dat format
@@ -606,15 +664,15 @@
     filters: list
         List of zarr filters
     verbose: bool
         If True, output is verbose (when chunks are used)
     auto_cast_uint: bool
         If True (default), unsigned integers are automatically cast to int if the specified dtype is signed
     {}
-    '''
+    """
     assert dataset_paths is not None, "Provide 'file_path'"
 
     if not isinstance(dataset_paths, list):
         dataset_paths = [dataset_paths]
     assert len(dataset_paths) == recording.get_num_segments()
 
     if dtype is None:
@@ -622,52 +680,53 @@
     if auto_cast_uint:
         cast_unsigned = determine_cast_unsigned(recording, dtype)
     else:
         cast_unsigned = False
 
     job_kwargs = fix_job_kwargs(job_kwargs)
     chunk_size = ensure_chunk_size(recording, **job_kwargs)
-    n_jobs = ensure_n_jobs(recording, n_jobs=job_kwargs.get('n_jobs', 1))
 
     # create zarr datasets files
     for segment_index in range(recording.get_num_segments()):
         num_frames = recording.get_num_samples(segment_index)
         num_channels = recording.get_num_channels()
         dset_name = dataset_paths[segment_index]
         shape = (num_frames, num_channels)
-        _ = zarr_root.create_dataset(name=dset_name, shape=shape,
-                                     chunks=(chunk_size, channel_chunk_size), 
-                                     dtype=dtype,
-                                     filters=filters,
-                                     compressor=compressor,)
-                                # synchronizer=zarr.ThreadSynchronizer())
+        _ = zarr_root.create_dataset(
+            name=dset_name,
+            shape=shape,
+            chunks=(chunk_size, channel_chunk_size),
+            dtype=dtype,
+            filters=filters,
+            compressor=compressor,
+        )
+        # synchronizer=zarr.ThreadSynchronizer())
 
     # use executor (loop or workers)
     func = _write_zarr_chunk
     init_func = _init_zarr_worker
-    if n_jobs == 1:
-        init_args = (recording, zarr_path, storage_options, dataset_paths, dtype, cast_unsigned)
-    else:
-        init_args = (recording.to_dict(), zarr_path, storage_options, dataset_paths, dtype, cast_unsigned)
-    executor = ChunkRecordingExecutor(recording, func, init_func, init_args, verbose=verbose,
-                                      job_name='write_zarr_recording', **job_kwargs)
+    init_args = (recording, zarr_path, storage_options, dataset_paths, dtype, cast_unsigned)
+    executor = ChunkRecordingExecutor(
+        recording, func, init_func, init_args, verbose=verbose, job_name="write_zarr_recording", **job_kwargs
+    )
     executor.run()
 
 
 # used by write_zarr_recording + ChunkRecordingExecutor
 def _init_zarr_worker(recording, zarr_path, storage_options, dataset_paths, dtype, cast_unsigned):
     import zarr
 
     # create a local dict per worker
     worker_ctx = {}
     if isinstance(recording, dict):
         from spikeinterface.core import load_extractor
-        worker_ctx['recording'] = load_extractor(recording)
+
+        worker_ctx["recording"] = load_extractor(recording)
     else:
-        worker_ctx['recording'] = recording
+        worker_ctx["recording"] = recording
 
     # reload root and datasets
     if storage_options is None:
         if isinstance(zarr_path, str):
             zarr_path_init = zarr_path
             zarr_path = Path(zarr_path)
         else:
@@ -676,32 +735,33 @@
         zarr_path_init = zarr_path
 
     root = zarr.open(zarr_path_init, mode="r+", storage_options=storage_options)
     zarr_datasets = []
     for dset_name in dataset_paths:
         z = root[dset_name]
         zarr_datasets.append(z)
-    worker_ctx['zarr_datasets'] = zarr_datasets
-    worker_ctx['dtype'] = np.dtype(dtype)
-    worker_ctx['cast_unsigned'] = cast_unsigned
+    worker_ctx["zarr_datasets"] = zarr_datasets
+    worker_ctx["dtype"] = np.dtype(dtype)
+    worker_ctx["cast_unsigned"] = cast_unsigned
 
     return worker_ctx
 
 
 # used by write_zarr_recording + ChunkRecordingExecutor
 def _write_zarr_chunk(segment_index, start_frame, end_frame, worker_ctx):
     # recover variables of the worker
-    recording = worker_ctx['recording']
-    dtype = worker_ctx['dtype']
-    zarr_dataset = worker_ctx['zarr_datasets'][segment_index]
-    cast_unsigned = worker_ctx['cast_unsigned']
+    recording = worker_ctx["recording"]
+    dtype = worker_ctx["dtype"]
+    zarr_dataset = worker_ctx["zarr_datasets"][segment_index]
+    cast_unsigned = worker_ctx["cast_unsigned"]
 
     # apply function
-    traces = recording.get_traces(start_frame=start_frame, end_frame=end_frame, segment_index=segment_index,
-                                  cast_unsigned=cast_unsigned)
+    traces = recording.get_traces(
+        start_frame=start_frame, end_frame=end_frame, segment_index=segment_index, cast_unsigned=cast_unsigned
+    )
     traces = traces.astype(dtype)
     zarr_dataset[start_frame:end_frame, :] = traces
 
     # fix memory leak by forcing garbage collection
     del traces
     gc.collect()
 
@@ -718,19 +778,19 @@
 
 def is_dict_extractor(d):
     """
     Check if a dict describe an extractor.
     """
     if not isinstance(d, dict):
         return False
-    is_extractor = ('module' in d) and ('class' in d) and ('version' in d) and ('annotations' in d)
+    is_extractor = ("module" in d) and ("class" in d) and ("version" in d) and ("annotations" in d)
     return is_extractor
 
 
-def recursive_path_modifier(d, func, target='path', copy=True):
+def recursive_path_modifier(d, func, target="path", copy=True) -> dict:
     """
     Generic function for recursive modification of paths in an extractor dict.
     A recording can be nested and this function explores the dictionary recursively
     to find the parent file or folder paths.
 
     Useful for :
       * relative/absolute path change
@@ -754,21 +814,21 @@
     dict
         Modified dictionary
     """
     if copy:
         dc = deepcopy(d)
     else:
         dc = d
-    
+
     if "kwargs" in dc.keys():
         kwargs = dc["kwargs"]
-        
+
         # change in place (copy=False)
         recursive_path_modifier(kwargs, func, copy=False)
-        
+
         # find nested and also change inplace (copy=False)
         nested_extractor_dict = None
         for k, v in kwargs.items():
             if isinstance(v, dict) and is_dict_extractor(v):
                 nested_extractor_dict = v
                 recursive_path_modifier(nested_extractor_dict, func, copy=False)
             # deal with list of extractor objects (e.g. concatenate_recordings)
@@ -778,25 +838,101 @@
                         nested_extractor_dict = vl
                         recursive_path_modifier(nested_extractor_dict, func, copy=False)
 
         return dc
     else:
         for k, v in d.items():
             if target in k:
-                # paths can be str or list of str
-                if isinstance(v, str):
-                    dc[k] =func(v)
+                # paths can be str or list of str or None
+                if v is None:
+                    continue
+                if isinstance(v, (str, Path)):
+                    dc[k] = func(v)
                 elif isinstance(v, list):
                     dc[k] = [func(e) for e in v]
                 else:
-                    raise ValueError(
-                        f'{k} key for path  must be str or list[str]')
+                    raise ValueError(f"{k} key for path  must be str or list[str]")
 
 
 def recursive_key_finder(d, key):
     # Find all values for a key on a dictionary, even if nested
     for k, v in d.items():
         if isinstance(v, dict):
             yield from recursive_key_finder(v, key)
         else:
             if k == key:
                 yield v
+
+
+def convert_seconds_to_str(seconds: float, long_notation: bool = True) -> str:
+    """
+    Convert seconds to a human-readable string representation.
+    Parameters
+    ----------
+    seconds : float
+        The duration in seconds.
+    long_notation : bool, optional, default: True
+        Whether to display the time with additional units (such as milliseconds, minutes,
+        hours, or days). If set to True, the function will display a more detailed
+        representation of the duration, including other units alongside the primary
+        seconds representation.
+    Returns
+    -------
+    str
+        A string representing the duration, with additional units included if
+        requested by the `long_notation` parameter.
+    """
+    base_str = f"{seconds:,.2f}s"
+
+    if long_notation:
+        if seconds < 1.0:
+            base_str += f" ({seconds * 1000:.2f} ms)"
+        elif seconds < 60:
+            pass  # seconds is already the primary representation
+        elif seconds < 3600:
+            minutes = seconds / 60
+            base_str += f" ({minutes:.2f} minutes)"
+        elif seconds < 86400 * 2:  # 2 days
+            hours = seconds / 3600
+            base_str += f" ({hours:.2f} hours)"
+        else:
+            days = seconds / 86400
+            base_str += f" ({days:.2f} days)"
+
+    return base_str
+
+
+def convert_bytes_to_str(byte_value: int) -> str:
+    """
+    Convert a number of bytes to a human-readable string with an appropriate unit.
+
+    This function converts a given number of bytes into a human-readable string
+    representing the value in either bytes (B), kibibytes (KiB), mebibytes (MiB),
+    gibibytes (GiB), or tebibytes (TiB). The function uses the IEC binary prefixes
+    (1 KiB = 1024 B, 1 MiB = 1024 KiB, etc.) to determine the appropriate unit.
+
+    Parameters
+    ----------
+    byte_value : int
+        The number of bytes to convert.
+
+    Returns
+    -------
+    str
+        The converted value as a formatted string with two decimal places,
+        followed by a space and the appropriate unit (B, KiB, MiB, GiB, or TiB).
+
+    Examples
+    --------
+    >>> convert_bytes_to_str(1024)
+    '1.00 KiB'
+    >>> convert_bytes_to_str(1048576)
+    '1.00 MiB'
+    >>> convert_bytes_to_str(45056)
+    '43.99 KiB'
+    """
+    suffixes = ["B", "KiB", "MiB", "GiB", "TiB"]
+    i = 0
+    while byte_value >= 1024 and i < len(suffixes) - 1:
+        byte_value /= 1024
+        i += 1
+    return f"{byte_value:.2f} {suffixes[i]}"
```

### Comparing `spikeinterface-0.97.1/spikeinterface/core/datasets.py` & `spikeinterface-0.98.0/src/spikeinterface/core/datasets.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,66 +1,67 @@
 """
 Some simple function to retrieve public datasets with datalad
 """
+from __future__ import annotations
 
-import warnings
-from .globals import get_global_dataset_folder, is_set_global_dataset_folder
+from pathlib import Path
 
+from .globals import get_global_dataset_folder
 
-def download_dataset(repo=None, remote_path=None, local_folder=None, update_if_exists=False, unlock=False):
+
+def download_dataset(
+    repo: str = "https://gin.g-node.org/NeuralEnsemble/ephy_testing_data",
+    remote_path: str = "mearec/mearec_test_10s.h5",
+    local_folder: Path | None = None,
+    update_if_exists: bool = False,
+    unlock: bool = False,
+) -> Path:
     """
     Function to download dataset from a remote repository using datalad.
 
     Parameters
     ----------
     repo : str, optional
         The repository to download the dataset from,
         defaults to: 'https://gin.g-node.org/NeuralEnsemble/ephy_testing_data'
     remote_path : str
         A specific subdirectory in the repository to download (e.g. Mearec, SpikeGLX, etc)
-        If not provided, the function returns None
+        defaults to: "mearec/mearec_test_10s.h5"
     local_folder : str, optional
         The destination folder / directory to download the dataset to.
-        defaults to the path "get_global_dataset_folder()" / f{repo_name} (see `spikeinterface.core.globals`) 
+        defaults to the path "get_global_dataset_folder()" / f{repo_name} (see `spikeinterface.core.globals`)
     update_if_exists : bool, optional
         Forces re-download of the dataset if it already exists, by default False
     unlock : bool, optional
         Use to enable the edition of the downloaded file content, by default False
 
     Returns
     -------
-    str
+    Path
         The local path to the downloaded dataset
     """
     import datalad.api
     from datalad.support.gitrepo import GitRepo
 
-    if repo is None:
-        #  print('Use gin NeuralEnsemble/ephy_testing_data')
-        repo = "https://gin.g-node.org/NeuralEnsemble/ephy_testing_data"
-
     if local_folder is None:
         base_local_folder = get_global_dataset_folder()
-        base_local_folder.mkdir(exist_ok=True)
+        base_local_folder.mkdir(exist_ok=True, parents=True)
         local_folder = base_local_folder / repo.split("/")[-1]
 
+    local_folder = Path(local_folder)
     if local_folder.exists() and GitRepo.is_valid_repo(local_folder):
         dataset = datalad.api.Dataset(path=local_folder)
         # make sure git repo is in clean state
         repo = dataset.repo
         if update_if_exists:
             repo.call_git(["checkout", "--force", "master"])
             dataset.update(merge=True)
     else:
         dataset = datalad.api.install(path=local_folder, source=repo)
 
-    if remote_path is None:
-        warnings.warn(message="No remote path provided, returning None")
-        return
-
     local_path = local_folder / remote_path
 
     # This downloads the data set content
     dataset.get(remote_path)
 
     # Unlock files of a dataset in order to be able to edit the actual content
     if unlock:
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `spikeinterface-0.97.1/spikeinterface/core/frameslicerecording.py` & `spikeinterface-0.98.0/src/spikeinterface/core/frameslicerecording.py`

 * *Files 16% similar despite different names*

```diff
@@ -6,60 +6,77 @@
 class FrameSliceRecording(BaseRecording):
     """
     Class to get a lazy frame slice.
     Work only with mono segment recording.
 
     Do not use this class directly but use `recording.frame_slice(...)`
 
+    Parameters
+    ----------
+    parent_recording: BaseRecording
+    start_frame: None or int
+        Earliest included frame in the parent recording.
+        Times are re-referenced to start_frame in the
+        sliced object. Set to 0 by default.
+    end_frame: None or int
+        Latest frame in the parent recording. As for usual
+        python slicing, the end frame is excluded.
+        Set to the recording's total number of samples by
+        default
     """
 
     def __init__(self, parent_recording, start_frame=None, end_frame=None):
         channel_ids = parent_recording.get_channel_ids()
 
-        assert parent_recording.get_num_segments() == 1, 'FrameSliceRecording work only with one segment'
+        assert parent_recording.get_num_segments() == 1, "FrameSliceRecording work only with one segment"
 
         parent_size = parent_recording.get_num_samples(0)
         if start_frame is None:
             start_frame = 0
         else:
             assert 0 <= start_frame < parent_size
 
         if end_frame is None:
             end_frame = parent_size
         else:
             assert 0 < end_frame <= parent_size
 
         assert end_frame > start_frame, "'start_frame' must be smaller than 'end_frame'!"
 
-        BaseRecording.__init__(self,
-                               sampling_frequency=parent_recording.get_sampling_frequency(),
-                               channel_ids=channel_ids,
-                               dtype=parent_recording.get_dtype())
+        BaseRecording.__init__(
+            self,
+            sampling_frequency=parent_recording.get_sampling_frequency(),
+            channel_ids=channel_ids,
+            dtype=parent_recording.get_dtype(),
+        )
 
         # link recording segment
         parent_segment = parent_recording._recording_segments[0]
         sub_segment = FrameSliceRecordingSegment(parent_segment, int(start_frame), int(end_frame))
         self.add_recording_segment(sub_segment)
 
         # copy properties and annotations
         parent_recording.copy_metadata(self)
 
         # update dump dict
-        self._kwargs = {'parent_recording': parent_recording.to_dict(), 'start_frame': int(start_frame),
-                        'end_frame': int(end_frame)}
+        self._kwargs = {
+            "parent_recording": parent_recording,
+            "start_frame": int(start_frame),
+            "end_frame": int(end_frame),
+        }
 
 
 class FrameSliceRecordingSegment(BaseRecordingSegment):
     def __init__(self, parent_recording_segment, start_frame, end_frame):
         d = parent_recording_segment.get_times_kwargs()
         d = d.copy()
-        if d['time_vector'] is None:
-            d['t_start'] = parent_recording_segment.sample_index_to_time(start_frame)
+        if d["time_vector"] is None:
+            d["t_start"] = parent_recording_segment.sample_index_to_time(start_frame)
         else:
-            d['time_vector'] = d['time_vector'][start_frame:end_frame]
+            d["time_vector"] = d["time_vector"][start_frame:end_frame]
         BaseRecordingSegment.__init__(self, **d)
         self._parent_recording_segment = parent_recording_segment
         self.start_frame = start_frame
         self.end_frame = end_frame
 
     def get_num_samples(self):
         return self.end_frame - self.start_frame
@@ -67,11 +84,11 @@
     def get_traces(self, start_frame, end_frame, channel_indices):
         if start_frame is None:
             start_frame = 0
         if end_frame is None:
             end_frame = self.get_num_samples()
         parent_start = self.start_frame + start_frame
         parent_end = self.start_frame + end_frame
-        traces = self._parent_recording_segment.get_traces(start_frame=parent_start,
-                                                           end_frame=parent_end,
-                                                           channel_indices=channel_indices)
+        traces = self._parent_recording_segment.get_traces(
+            start_frame=parent_start, end_frame=parent_end, channel_indices=channel_indices
+        )
         return traces
```

### Comparing `spikeinterface-0.97.1/spikeinterface/core/globals.py` & `spikeinterface-0.98.0/src/spikeinterface/core/globals.py`

 * *Files 5% similar despite different names*

```diff
@@ -9,15 +9,15 @@
 from copy import deepcopy
 from .job_tools import job_keys, _shared_job_kwargs_doc
 
 ########################################
 
 global temp_folder
 global temp_folder_set
-base = Path(tempfile.gettempdir()) / 'spikeinterface_cache'
+base = Path(tempfile.gettempdir()) / "spikeinterface_cache"
 temp_folder_set = False
 
 
 def get_global_tmp_folder():
     """
     Get the global path temporary folder.
     """
@@ -57,15 +57,15 @@
     global temp_folder_set
     temp_folder_set = False
 
 
 ########################################
 
 global dataset_folder
-dataset_folder = Path.home() / 'spikeinterface_datasets'
+dataset_folder = Path.home() / "spikeinterface_datasets"
 global dataset_folder_set
 dataset_folder_set = False
 
 
 def get_global_dataset_folder():
     """
     Get the global dataset folder.
@@ -108,24 +108,25 @@
     global global_job_kwargs
     return deepcopy(global_job_kwargs)
 
 
 def set_global_job_kwargs(**job_kwargs):
     """
     Set the global job kwargs.
-    
+
     Parameters
     ----------
-    
+
     {}
     """
     global global_job_kwargs
     for k in job_kwargs:
-        assert k in job_keys, (f"{k} is not a valid job keyword argument. "
-                               f"Available keyword arguments are: {list(job_keys)}")
+        assert k in job_keys, (
+            f"{k} is not a valid job keyword argument. " f"Available keyword arguments are: {list(job_keys)}"
+        )
     global_job_kwargs.update(job_kwargs)
     global global_job_kwargs_set
     global_job_kwargs_set = True
 
 
 def reset_global_job_kwargs():
     """
@@ -136,8 +137,8 @@
 
 
 def is_set_global_job_kwargs_set():
     global global_job_kwargs_set
     return global_job_kwargs_set
 
 
-set_global_job_kwargs.__doc__ = set_global_job_kwargs.__doc__.format(_shared_job_kwargs_doc)
+set_global_job_kwargs.__doc__ = set_global_job_kwargs.__doc__.format(_shared_job_kwargs_doc)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/core/injecttemplates.py` & `spikeinterface-0.98.0/src/spikeinterface/core/injecttemplates.py`

 * *Files 10% similar despite different names*

```diff
@@ -32,42 +32,52 @@
 
     Returns
     -------
     injected_recording: InjectTemplatesRecording
         The recording with the templates injected.
     """
 
-    def __init__(self, sorting: BaseSorting, templates: np.ndarray, nbefore: Union[List[int], int, None] = None,
-                 amplitude_factor: Union[List[List[float]], List[float], float] = 1.0,
-                 parent_recording: Union[BaseRecording, None] = None, num_samples: Union[List[int], None] = None) -> None:
+    def __init__(
+        self,
+        sorting: BaseSorting,
+        templates: np.ndarray,
+        nbefore: Union[List[int], int, None] = None,
+        amplitude_factor: Union[List[List[float]], List[float], float] = 1.0,
+        parent_recording: Union[BaseRecording, None] = None,
+        num_samples: Union[List[int], None] = None,
+    ) -> None:
         templates = np.array(templates)
         self._check_templates(templates)
 
         channel_ids = parent_recording.channel_ids if parent_recording is not None else list(range(templates.shape[2]))
         dtype = parent_recording.dtype if parent_recording is not None else templates.dtype
         BaseRecording.__init__(self, sorting.get_sampling_frequency(), channel_ids, dtype)
-        
+
         n_units = len(sorting.unit_ids)
         assert len(templates) == n_units
         self.spike_vector = sorting.to_spike_vector()
 
         if nbefore is None:
             nbefore = np.argmax(np.max(np.abs(templates), axis=2), axis=1)
         elif isinstance(nbefore, (int, np.integer)):
-            nbefore = [nbefore]*n_units 
+            nbefore = [nbefore] * n_units
         else:
             assert len(nbefore) == n_units
 
         if isinstance(amplitude_factor, float):
-            amplitude_factor = np.array([1.0]*len(self.spike_vector), dtype=np.float32)
-        elif len(amplitude_factor) != len(self.spike_vector):  # In this case, it's a list of list for amplitude by unit by spike.
+            amplitude_factor = np.array([1.0] * len(self.spike_vector), dtype=np.float32)
+        elif len(amplitude_factor) != len(
+            self.spike_vector
+        ):  # In this case, it's a list of list for amplitude by unit by spike.
             tmp = np.array([], dtype=np.float32)
 
             for segment_index in range(sorting.get_num_segments()):
-                spike_times = [sorting.get_unit_spike_train(unit_id, segment_index=segment_index) for unit_id in sorting.unit_ids]
+                spike_times = [
+                    sorting.get_unit_spike_train(unit_id, segment_index=segment_index) for unit_id in sorting.unit_ids
+                ]
                 spike_times = np.concatenate(spike_times)
                 spike_amplitudes = np.concatenate(amplitude_factor[segment_index])
 
                 order = np.argsort(spike_times)
                 tmp = np.append(tmp, spike_amplitudes[order])
 
             amplitude_factor = tmp
@@ -76,114 +86,143 @@
             assert parent_recording.get_num_segments() == sorting.get_num_segments()
             assert parent_recording.get_sampling_frequency() == sorting.get_sampling_frequency()
             assert parent_recording.get_num_channels() == templates.shape[2]
             parent_recording.copy_metadata(self)
 
         if num_samples is None:
             if parent_recording is None:
-                num_samples = [self.spike_vector['sample_ind'][-1] + templates.shape[1]]
+                num_samples = [self.spike_vector["sample_index"][-1] + templates.shape[1]]
             else:
-                num_samples = [parent_recording.get_num_frames(segment_index) for segment_index in range(sorting.get_num_segments())]
+                num_samples = [
+                    parent_recording.get_num_frames(segment_index)
+                    for segment_index in range(sorting.get_num_segments())
+                ]
         if isinstance(num_samples, int):
             assert sorting.get_num_segments() == 1
             num_samples = [num_samples]
 
-
         for segment_index in range(sorting.get_num_segments()):
-            start = np.searchsorted(self.spike_vector['segment_ind'], segment_index, side="left")
-            end = np.searchsorted(self.spike_vector['segment_ind'], segment_index, side="right")
-            spikes = self.spike_vector[start : end]
-
-            parent_recording_segment = None if parent_recording is None else parent_recording._recording_segments[segment_index]
-            recording_segment = InjectTemplatesRecordingSegment(self.sampling_frequency, self.dtype, spikes, templates, nbefore,
-                                                             amplitude_factor[start:end], parent_recording_segment, num_samples[segment_index])
+            start = np.searchsorted(self.spike_vector["segment_index"], segment_index, side="left")
+            end = np.searchsorted(self.spike_vector["segment_index"], segment_index, side="right")
+            spikes = self.spike_vector[start:end]
+
+            parent_recording_segment = (
+                None if parent_recording is None else parent_recording._recording_segments[segment_index]
+            )
+            recording_segment = InjectTemplatesRecordingSegment(
+                self.sampling_frequency,
+                self.dtype,
+                spikes,
+                templates,
+                nbefore,
+                amplitude_factor[start:end],
+                parent_recording_segment,
+                num_samples[segment_index],
+            )
             self.add_recording_segment(recording_segment)
 
         self._kwargs = {
-            "sorting": sorting.to_dict(),
+            "sorting": sorting,
             "templates": templates.tolist(),
             "nbefore": nbefore,
-            "amplitude_factor": amplitude_factor
+            "amplitude_factor": amplitude_factor,
         }
         if parent_recording is None:
-            self._kwargs['num_samples'] = num_samples
+            self._kwargs["num_samples"] = num_samples
         else:
-            self._kwargs['parent_recording'] = parent_recording.to_dict()
+            self._kwargs["parent_recording"] = parent_recording
         self._kwargs = check_json(self._kwargs)
 
-
     @staticmethod
     def _check_templates(templates: np.ndarray):
         max_value = np.max(np.abs(templates))
         threshold = 0.01 * max_value
 
         if max(np.max(np.abs(templates[:, 0])), np.max(np.abs(templates[:, -1]))) > threshold:
-            raise Exception("Warning!\nYour templates do not go to 0 on the edges in InjectTemplatesRecording.__init__\nPlease make your window bigger.")
-
+            raise Exception(
+                "Warning!\nYour templates do not go to 0 on the edges in InjectTemplatesRecording.__init__\nPlease make your window bigger."
+            )
 
 
 class InjectTemplatesRecordingSegment(BaseRecordingSegment):
-
-    def __init__(self, sampling_frequency: float, dtype, spike_vector: np.ndarray, templates: np.ndarray, nbefore: List[int],
-                 amplitude_factor: List[List[float]], parent_recording_segment: Union[BaseRecordingSegment, None] = None, num_samples: Union[int, None] = None) -> None:
-
-        BaseRecordingSegment.__init__(self, sampling_frequency, t_start=0 if parent_recording_segment is None else parent_recording_segment.t_start)
+    def __init__(
+        self,
+        sampling_frequency: float,
+        dtype,
+        spike_vector: np.ndarray,
+        templates: np.ndarray,
+        nbefore: List[int],
+        amplitude_factor: List[List[float]],
+        parent_recording_segment: Union[BaseRecordingSegment, None] = None,
+        num_samples: Union[int, None] = None,
+    ) -> None:
+        BaseRecordingSegment.__init__(
+            self,
+            sampling_frequency,
+            t_start=0 if parent_recording_segment is None else parent_recording_segment.t_start,
+        )
         assert not (parent_recording_segment is None and num_samples is None)
 
         self.dtype = dtype
         self.spike_vector = spike_vector
         self.templates = templates
         self.nbefore = nbefore
         self.amplitude_factor = amplitude_factor
         self.parent_recording = parent_recording_segment
         self.num_samples = parent_recording_segment.get_num_frames() if num_samples is None else num_samples
 
-    def get_traces(self, start_frame: Union[int, None] = None, end_frame: Union[int, None] = None,
-                   channel_indices: Union[List, None] = None) -> np.ndarray:
+    def get_traces(
+        self,
+        start_frame: Union[int, None] = None,
+        end_frame: Union[int, None] = None,
+        channel_indices: Union[List, None] = None,
+    ) -> np.ndarray:
         start_frame = 0 if start_frame is None else start_frame
         end_frame = self.num_samples if end_frame is None else end_frame
         channel_indices = list(range(self.templates.shape[2])) if channel_indices is None else channel_indices
         if isinstance(channel_indices, slice):
             stop = channel_indices.stop if channel_indices.stop is not None else self.templates.shape[2]
             start = channel_indices.start if channel_indices.start is not None else 0
             step = channel_indices.step if channel_indices.step is not None else 1
-            n_channels = math.ceil((stop-start) / step)
+            n_channels = math.ceil((stop - start) / step)
         else:
             n_channels = len(channel_indices)
 
         if self.parent_recording is not None:
             traces = self.parent_recording.get_traces(start_frame, end_frame, channel_indices).copy()
         else:
             traces = np.zeros([end_frame - start_frame, n_channels], dtype=self.dtype)
 
-        start = np.searchsorted(self.spike_vector['sample_ind'], start_frame - self.templates.shape[1], side="left")
-        end   = np.searchsorted(self.spike_vector['sample_ind'], end_frame   + self.templates.shape[1], side="right")
+        start = np.searchsorted(self.spike_vector["sample_index"], start_frame - self.templates.shape[1], side="left")
+        end = np.searchsorted(self.spike_vector["sample_index"], end_frame + self.templates.shape[1], side="right")
 
         for i in range(start, end):
             spike = self.spike_vector[i]
-            t = spike['sample_ind']
-            unit_ind = spike['unit_ind']
+            t = spike["sample_index"]
+            unit_ind = spike["unit_index"]
             template = self.templates[unit_ind][:, channel_indices]
 
             start_traces = t - self.nbefore[unit_ind] - start_frame
             end_traces = start_traces + template.shape[0]
-            if start_traces >= end_frame-start_frame or end_traces <= 0:
+            if start_traces >= end_frame - start_frame or end_traces <= 0:
                 continue
 
             start_template = 0
             end_template = template.shape[0]
 
             if start_traces < 0:
                 start_template = -start_traces
                 start_traces = 0
             if end_traces > end_frame - start_frame:
                 end_template = template.shape[0] + end_frame - start_frame - end_traces
                 end_traces = end_frame - start_frame
 
-            traces[start_traces : end_traces] += (template[start_template : end_template].astype(np.float64) * self.amplitude_factor[i]).astype(traces.dtype)
+            traces[start_traces:end_traces] += (
+                template[start_template:end_template].astype(np.float64) * self.amplitude_factor[i]
+            ).astype(traces.dtype)
 
         return traces.astype(self.dtype)
 
     def get_num_samples(self) -> int:
         return self.num_samples
```

### Comparing `spikeinterface-0.97.1/spikeinterface/core/job_tools.py` & `spikeinterface-0.98.0/src/spikeinterface/core/job_tools.py`

 * *Files 13% similar despite different names*

```diff
@@ -27,39 +27,50 @@
                 - chunk_duration : str or float or None
                     Chunk duration in s if float or with units if str (e.g. '1s', '500ms')
             * n_jobs: int
                 Number of jobs to use. With -1 the number of jobs is the same as number of cores
             * progress_bar: bool
                 If True, a progress bar is printed
             * mp_context: str or None
-                Context for multiprocessing. It can be None (default), "fork" or "spawn". 
+                Context for multiprocessing. It can be None (default), "fork" or "spawn".
                 Note that "fork" is only available on UNIX systems
     """
 
 
-job_keys = ('n_jobs', 'total_memory', 'chunk_size', 'chunk_memory', 'chunk_duration', 'progress_bar', 
-            'mp_context', 'verbose', 'max_threads_per_process')
+job_keys = (
+    "n_jobs",
+    "total_memory",
+    "chunk_size",
+    "chunk_memory",
+    "chunk_duration",
+    "progress_bar",
+    "mp_context",
+    "verbose",
+    "max_threads_per_process",
+)
 
 
 def fix_job_kwargs(runtime_job_kwargs):
     from .globals import get_global_job_kwargs
+
     job_kwargs = get_global_job_kwargs()
 
     for k in runtime_job_kwargs:
-        assert k in job_keys, (f"{k} is not a valid job keyword argument. "
-                               f"Available keyword arguments are: {list(job_keys)}")
+        assert k in job_keys, (
+            f"{k} is not a valid job keyword argument. " f"Available keyword arguments are: {list(job_keys)}"
+        )
     # remove None
     runtime_job_kwargs_exclude_none = runtime_job_kwargs.copy()
     for job_key, job_value in runtime_job_kwargs.items():
         if job_value is None:
             del runtime_job_kwargs_exclude_none[job_key]
     job_kwargs.update(runtime_job_kwargs_exclude_none)
 
     # if n_jobs is -1, set to os.cpu_count() (n_jobs is always in global job_kwargs)
-    n_jobs = job_kwargs['n_jobs']
+    n_jobs = job_kwargs["n_jobs"]
     assert isinstance(n_jobs, (float, np.integer, int))
     if isinstance(n_jobs, float):
         n_jobs = int(n_jobs * os.cpu_count())
     elif n_jobs < 0:
         n_jobs = os.cpu_count() + 1 + n_jobs
     job_kwargs["n_jobs"] = max(n_jobs, 1)
 
@@ -83,14 +94,15 @@
     return specific_kwargs, job_kwargs
 
 
 # from https://stackoverflow.com/questions/24983493/tracking-progress-of-joblib-parallel-execution
 @contextlib.contextmanager
 def tqdm_joblib(tqdm_object):
     """Context manager to patch joblib to report into tqdm progress bar given as argument"""
+
     class TqdmBatchCompletionCallback(joblib.parallel.BatchCompletionCallBack):
         def __call__(self, *args, **kwargs):
             tqdm_object.update(n=self.batch_size)
             return super().__call__(*args, **kwargs)
 
     old_batch_callback = joblib.parallel.BatchCompletionCallBack
     joblib.parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback
@@ -127,15 +139,15 @@
     for segment_index in range(recording.get_num_segments()):
         num_frames = recording.get_num_samples(segment_index)
         chunks = divide_segment_into_chunks(num_frames, chunk_size)
         all_chunks.extend([(segment_index, frame_start, frame_stop) for frame_start, frame_stop in chunks])
     return all_chunks
 
 
-_exponents = {'k': 1e3, 'M': 1e6, 'G': 1e9}
+_exponents = {"k": 1e3, "M": 1e6, "G": 1e9}
 
 
 def _mem_to_int(mem):
     suffix = mem[-1]
     assert suffix in _exponents
     mem = int(float(mem[:-1]) * _exponents[suffix])
     return mem
@@ -161,16 +173,17 @@
                 "Recording is not dumpable and can't be processed in parallel. "
                 "You can use the `recording.save()` function to make it dumpable or set 'n_jobs' to 1."
             )
 
     return n_jobs
 
 
-def ensure_chunk_size(recording, total_memory=None, chunk_size=None, chunk_memory=None, chunk_duration=None, n_jobs=1, 
-                      **other_kwargs):
+def ensure_chunk_size(
+    recording, total_memory=None, chunk_size=None, chunk_memory=None, chunk_duration=None, n_jobs=1, **other_kwargs
+):
     """
     'chunk_size' is the traces.shape[0] for each worker.
 
     Flexible chunk_size setter with 3 ways:
         * "chunk_size": is the length in sample for each chunk independently of channel count and dtype.
         * "chunk_memory": total memory per chunk per worker
         * "total_memory": total memory over all workers.
@@ -207,29 +220,32 @@
         n_bytes = np.dtype(recording.get_dtype()).itemsize
         num_channels = recording.get_num_channels()
         chunk_size = int(total_memory / (num_channels * n_bytes * n_jobs))
     elif chunk_duration is not None:
         if isinstance(chunk_duration, float):
             chunk_size = int(chunk_duration * recording.get_sampling_frequency())
         elif isinstance(chunk_duration, str):
-            if chunk_duration.endswith('ms'):
-                chunk_duration = float(chunk_duration.replace('ms', '')) / 1000.
-            elif chunk_duration.endswith('s'):
-                chunk_duration = float(chunk_duration.replace('s', ''))
+            if chunk_duration.endswith("ms"):
+                chunk_duration = float(chunk_duration.replace("ms", "")) / 1000.0
+            elif chunk_duration.endswith("s"):
+                chunk_duration = float(chunk_duration.replace("s", ""))
             else:
-                raise ValueError('chunk_duration must ends with s or ms') 
+                raise ValueError("chunk_duration must ends with s or ms")
             chunk_size = int(chunk_duration * recording.get_sampling_frequency())
         else:
-            raise ValueError('chunk_duration must be str or float') 
+            raise ValueError("chunk_duration must be str or float")
     else:
         if n_jobs == 1:
             # not chunk computing
+            # TODO Discuss, Sam, is this something that we want to do?
+            # Even in single process mode, we should chunk the data to avoid loading the whole thing into memory I feel
+            # Am I wrong?
             chunk_size = None
         else:
-            raise ValueError('For n_jobs >1 you must specify total_memory or chunk_size or chunk_memory')
+            raise ValueError("For n_jobs >1 you must specify total_memory or chunk_size or chunk_memory")
 
     return chunk_size
 
 
 class ChunkRecordingExecutor:
     """
     Core class for parallel processing to run a "function" over chunks on a recording.
@@ -254,14 +270,17 @@
         Arguments for init_func
     verbose: bool
         If True, output is verbose
     progress_bar: bool
         If True, a progress bar is printed to monitor the progress of the process
     handle_returns: bool
         If True, the function can return values
+    gather_func: None or callable
+        Optional function that is called in the main thread and retrieves the results of each worker.
+        This function can be used instead of `handle_returns` to implement custom storage on-the-fly.
     n_jobs: int
         Number of jobs to be used (default 1). Use -1 to use as many jobs as number of cores
     total_memory: str
         Total memory (RAM) to use (e.g. "1G", "500M")
     chunk_memory: str
         Memory per chunk (RAM) to use (e.g. "1G", "500M")
     chunk_size: int or None
@@ -274,99 +293,119 @@
     job_name: str
         Job name
     max_threads_per_process: int or None
         Limit the number of thread per process using threadpoolctl modules.
         This used only when n_jobs>1
         If None, no limits.
 
+
     Returns
     -------
     res: list
         If 'handle_returns' is True, the results for each chunk process
     """
 
-    def __init__(self, recording, func, init_func, init_args, verbose=False, progress_bar=False, handle_returns=False,
-                 n_jobs=1, total_memory=None, chunk_size=None, chunk_memory=None, chunk_duration=None,
-                 mp_context=None, job_name='', max_threads_per_process=1):
+    def __init__(
+        self,
+        recording,
+        func,
+        init_func,
+        init_args,
+        verbose=False,
+        progress_bar=False,
+        handle_returns=False,
+        gather_func=None,
+        n_jobs=1,
+        total_memory=None,
+        chunk_size=None,
+        chunk_memory=None,
+        chunk_duration=None,
+        mp_context=None,
+        job_name="",
+        max_threads_per_process=1,
+    ):
         self.recording = recording
         self.func = func
         self.init_func = init_func
         self.init_args = init_args
-        
+
         if mp_context is None:
             mp_context = recording.get_preferred_mp_context()
         if mp_context is not None and platform.system() == "Windows":
             assert mp_context != "fork", "'fork' mp_context not supported on Windows!"
-                
+
         self.mp_context = mp_context
 
         self.verbose = verbose
         self.progress_bar = progress_bar
 
         self.handle_returns = handle_returns
+        self.gather_func = gather_func
 
         self.n_jobs = ensure_n_jobs(recording, n_jobs=n_jobs)
-        self.chunk_size = ensure_chunk_size(recording,
-                                            total_memory=total_memory, chunk_size=chunk_size,
-                                            chunk_memory=chunk_memory, chunk_duration=chunk_duration,
-                                            n_jobs=self.n_jobs)
+        self.chunk_size = ensure_chunk_size(
+            recording,
+            total_memory=total_memory,
+            chunk_size=chunk_size,
+            chunk_memory=chunk_memory,
+            chunk_duration=chunk_duration,
+            n_jobs=self.n_jobs,
+        )
         self.job_name = job_name
         self.max_threads_per_process = max_threads_per_process
-        
+
         if verbose:
-            print(self.job_name, 'with n_jobs =', self.n_jobs, 'and chunk_size =', self.chunk_size)
+            print(self.job_name, "with n_jobs =", self.n_jobs, "and chunk_size =", self.chunk_size)
 
     def run(self):
         """
         Runs the defined jobs.
         """
         all_chunks = divide_recording_into_chunks(self.recording, self.chunk_size)
 
         if self.handle_returns:
             returns = []
         else:
             returns = None
 
-        import sys
-        if self.n_jobs != 1 and not (sys.version_info >= (3, 8)):
-            self.n_jobs = 1
-
         if self.n_jobs == 1:
             if self.progress_bar:
                 all_chunks = tqdm(all_chunks, ascii=True, desc=self.job_name)
 
             worker_ctx = self.init_func(*self.init_args)
             for segment_index, frame_start, frame_stop in all_chunks:
                 res = self.func(segment_index, frame_start, frame_stop, worker_ctx)
                 if self.handle_returns:
                     returns.append(res)
+                if self.gather_func is not None:
+                    self.gather_func(res)
         else:
             n_jobs = min(self.n_jobs, len(all_chunks))
             ######## Do you want to limit the number of threads per process?
             ######## It has to be done to speed up numpy a lot if multicores
             ######## Otherwise, np.dot will be slow. How to do that, up to you
             ######## This is just a suggestion, but here it adds a dependency
 
             # parallel
-            with ProcessPoolExecutor(max_workers=n_jobs,
-                                     initializer=worker_initializer,
-                                     mp_context=mp.get_context(self.mp_context),
-                                     initargs=(self.func, self.init_func, self.init_args, self.max_threads_per_process)) as executor:
-
+            with ProcessPoolExecutor(
+                max_workers=n_jobs,
+                initializer=worker_initializer,
+                mp_context=mp.get_context(self.mp_context),
+                initargs=(self.func, self.init_func, self.init_args, self.max_threads_per_process),
+            ) as executor:
                 results = executor.map(function_wrapper, all_chunks)
 
                 if self.progress_bar:
                     results = tqdm(results, desc=self.job_name, total=len(all_chunks))
 
-                if self.handle_returns:
-                    for res in results:
+                for res in results:
+                    if self.handle_returns:
                         returns.append(res)
-                else:
-                    for res in results:
-                        pass
+                    if self.gather_func is not None:
+                        self.gather_func(res)
 
         return returns
 
 
 # see
 # https://stackoverflow.com/questions/10117073/how-to-use-initializer-to-set-up-my-multiprocess-pool
 # the tricks is : theses 2 variables are global per worker
@@ -378,22 +417,22 @@
 def worker_initializer(func, init_func, init_args, max_threads_per_process):
     global _worker_ctx
     if max_threads_per_process is None:
         _worker_ctx = init_func(*init_args)
     else:
         with threadpool_limits(limits=max_threads_per_process):
             _worker_ctx = init_func(*init_args)
-    _worker_ctx['max_threads_per_process'] = max_threads_per_process
+    _worker_ctx["max_threads_per_process"] = max_threads_per_process
     global _func
     _func = func
 
 
 def function_wrapper(args):
     segment_index, start_frame, end_frame = args
     global _func
     global _worker_ctx
-    max_threads_per_process = _worker_ctx['max_threads_per_process']
+    max_threads_per_process = _worker_ctx["max_threads_per_process"]
     if max_threads_per_process is None:
         return _func(segment_index, start_frame, end_frame, _worker_ctx)
     else:
         with threadpool_limits(limits=max_threads_per_process):
             return _func(segment_index, start_frame, end_frame, _worker_ctx)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/core/npyfoldersnippets.py` & `spikeinterface-0.98.0/src/spikeinterface/core/npyfoldersnippets.py`

 * *Files 16% similar despite different names*

```diff
@@ -21,35 +21,35 @@
         The path to the folder
 
     Returns
     -------
     snippets: NpyFolderSnippets
         The snippets
     """
-    extractor_name = 'NpyFolderSnippets'
-    mode = 'folder'
-    name = "npyfolder"
 
-    def __init__(self,  folder_path):
+    extractor_name = "NpyFolderSnippets"
+    mode = "folder"
+    name = "npyfolder"
 
+    def __init__(self, folder_path):
         folder_path = Path(folder_path)
 
-        with open(folder_path / 'npy.json', 'r') as f:
+        with open(folder_path / "npy.json", "r") as f:
             d = json.load(f)
 
-        if not d['class'].endswith('.NpySnippetsExtractor'):
-            raise ValueError('This folder is not a binary spikeinterface folder')
+        if not d["class"].endswith(".NpySnippetsExtractor"):
+            raise ValueError("This folder is not a binary spikeinterface folder")
 
-        assert d['relative_paths']
+        assert d["relative_paths"]
 
         d = _make_paths_absolute(d, folder_path)
 
-        NpySnippetsExtractor.__init__(self, **d['kwargs'])
+        NpySnippetsExtractor.__init__(self, **d["kwargs"])
 
         folder_metadata = folder_path
         self.load_metadata_from_folder(folder_metadata)
 
         self._kwargs = dict(folder_path=str(folder_path.absolute()))
-        self._bin_kwargs = d['kwargs']
+        self._bin_kwargs = d["kwargs"]
 
 
 read_npy_snippets_folder = define_function_from_class(source_class=NpyFolderSnippets, name="read_npy_snippets_folder")
```

### Comparing `spikeinterface-0.97.1/spikeinterface/core/npysnippetsextractor.py` & `spikeinterface-0.98.0/src/spikeinterface/core/npysnippetsextractor.py`

 * *Files 4% similar despite different names*

```diff
@@ -10,45 +10,54 @@
     """
     Dead simple and super light format based on the NPY numpy format.
 
     It is in fact an archive of several .npy format.
     All spike are store in two columns maner index+labels
     """
 
-    extractor_name = 'NpySnippets'
-    is_writable = True
-    mode = 'file'
+    extractor_name = "NpySnippets"
+    mode = "file"
     name = "npy"
 
-    def __init__(self, file_paths, sampling_frequency, channel_ids=None, nbefore=None,
-                 gain_to_uV=None, offset_to_uV=None):
-
+    def __init__(
+        self, file_paths, sampling_frequency, channel_ids=None, nbefore=None, gain_to_uV=None, offset_to_uV=None
+    ):
         if not isinstance(file_paths, list):
             file_paths = [file_paths]
 
         num_segments = len(file_paths)
-        data = np.load(file_paths[0], mmap_mode='r')
+        data = np.load(file_paths[0], mmap_mode="r")
 
-        BaseSnippets.__init__(self, sampling_frequency,  nbefore=nbefore,
-                              snippet_len=data['snippet'].shape[1],
-                              dtype=data['snippet'].dtype, channel_ids=channel_ids)
+        BaseSnippets.__init__(
+            self,
+            sampling_frequency,
+            nbefore=nbefore,
+            snippet_len=data["snippet"].shape[1],
+            dtype=data["snippet"].dtype,
+            channel_ids=channel_ids,
+        )
 
         for i in range(num_segments):
             snp_segment = NpySnippetsSegment(file_paths[i])
             self.add_snippets_segment(snp_segment)
 
         if gain_to_uV is not None:
             self.set_channel_gains(gain_to_uV)
 
         if offset_to_uV is not None:
             self.set_channel_offsets(offset_to_uV)
 
-        self._kwargs = {'file_paths': [str(f) for f in file_paths], 'sampling_frequency': sampling_frequency,
-                        'channel_ids': channel_ids, 'nbefore': nbefore, 'gain_to_uV': gain_to_uV,
-                        'offset_to_uV': offset_to_uV}
+        self._kwargs = {
+            "file_paths": [str(f) for f in file_paths],
+            "sampling_frequency": sampling_frequency,
+            "channel_ids": channel_ids,
+            "nbefore": nbefore,
+            "gain_to_uV": gain_to_uV,
+            "offset_to_uV": offset_to_uV,
+        }
 
     @staticmethod
     def write_snippets(snippets, file_paths, dtype=None):
         """
         Save snippet extractor in binary .npy format.
 
         Parameters
@@ -62,38 +71,40 @@
         {}
         """
         if not isinstance(file_paths, list):
             file_paths = [file_paths]
         if dtype is None:
             dtype = snippets.dtype
         assert len(file_paths) == snippets.get_num_segments()
-        snippets_t = np.dtype([('frame', np.int64),
-                               ('snippet', dtype, (snippets.snippet_len, snippets.get_num_channels()))])
+        snippets_t = np.dtype(
+            [("frame", np.int64), ("snippet", dtype, (snippets.snippet_len, snippets.get_num_channels()))]
+        )
 
         for i in range(snippets.get_num_segments()):
             n = snippets.get_num_snippets(i)
-            arr = np.empty(n, dtype=snippets_t, order='F')
-            arr['frame'] = snippets.get_frames(segment_index=i)
-            arr['snippet'] = snippets.get_snippets(segment_index=i).astype(dtype, copy=False)
+            arr = np.empty(n, dtype=snippets_t, order="F")
+            arr["frame"] = snippets.get_frames(segment_index=i)
+            arr["snippet"] = snippets.get_snippets(segment_index=i).astype(dtype, copy=False)
 
             np.save(file_paths[i], arr)
 
 
 class NpySnippetsSegment(BaseSnippetsSegment):
     def __init__(self, file):
         BaseSnippetsSegment.__init__(self)
 
-        npy = np.load(file, mmap_mode='r')
-        self._snippets = npy['snippet']
-        self._spikestimes = npy['frame']
-
-    def get_snippets(self,
-                     indices,
-                     channel_indices: Union[List, None] = None,
-                     ) -> np.ndarray:
+        npy = np.load(file, mmap_mode="r")
+        self._snippets = npy["snippet"]
+        self._spikestimes = npy["frame"]
+
+    def get_snippets(
+        self,
+        indices,
+        channel_indices: Union[List, None] = None,
+    ) -> np.ndarray:
         """
         Return the snippets, optionally for a subset of samples and/or channels
 
         Parameters
         ----------
         indexes: (Union[int, None], optional)
             start sample index, or zero if None. Defaults to None.
@@ -110,17 +121,15 @@
         if indices is None:
             return self._snippets[:, :, channel_indices]
         return self._snippets[indices, :, channel_indices]
 
     def get_num_snippets(self):
         return self._spikestimes.shape[0]
 
-    def frames_to_indices(self,
-                          start_frame: Union[int, None] = None,
-                          end_frame: Union[int, None] = None):
+    def frames_to_indices(self, start_frame: Union[int, None] = None, end_frame: Union[int, None] = None):
         """
         Return the slice of snippets
 
         Parameters
         ----------
         start_frame: (Union[int, None], optional)
             start sample index, or zero if None. Defaults to None.
@@ -132,19 +141,19 @@
         snippets: slice
             slice of selected snippets
         """
         # must be implemented in subclass
         if start_frame is None:
             init = 0
         else:
-            init = np.searchsorted(self._spikestimes, start_frame, side='left')
+            init = np.searchsorted(self._spikestimes, start_frame, side="left")
         if end_frame is None:
             endi = self._spikestimes.shape[0]
         else:
-            endi = np.searchsorted(self._spikestimes, end_frame, side='left')
+            endi = np.searchsorted(self._spikestimes, end_frame, side="left")
         return slice(init, endi, 1)
 
     def get_frames(self, indices=None):
         """Returns the frames of the snippets in this segment
 
         Returns:
             SampleIndex: Number of samples in the segment
```

### Comparing `spikeinterface-0.97.1/spikeinterface/core/npzfolder.py` & `spikeinterface-0.98.0/src/spikeinterface/core/npzfolder.py`

 * *Files 12% similar despite different names*

```diff
@@ -4,53 +4,51 @@
 import numpy as np
 
 from .base import _make_paths_absolute
 from .npzsortingextractor import NpzSortingExtractor
 from .core_tools import define_function_from_class
 
 
-
 class NpzFolderSorting(NpzSortingExtractor):
     """
     NpzFolderSorting is an internal format used in spikeinterface.
     It is a NpzSortingExtractor + metadata contained in a folder.
-    
+
     It is created with the function: `sorting.save(folder='/myfolder')`
-    
+
     Parameters
     ----------
     folder_path: str or Path
-    
+
     Returns
     -------
     sorting: NpzFolderSorting
         The sorting
     """
-    extractor_name = 'NpzFolder'
-    has_default_locations = True
-    mode = 'folder'
+
+    extractor_name = "NpzFolder"
+    mode = "folder"
     name = "npzfolder"
 
     def __init__(self, folder_path):
-        
         folder_path = Path(folder_path)
-        
-        with open(folder_path / 'npz.json', 'r') as f:
+
+        with open(folder_path / "npz.json", "r") as f:
             d = json.load(f)
 
-        if not d['class'].endswith('.NpzSortingExtractor'):
-            raise ValueError('This folder is not an npz spikeinterface folder')
+        if not d["class"].endswith(".NpzSortingExtractor"):
+            raise ValueError("This folder is not an npz spikeinterface folder")
 
-        assert d['relative_paths']
+        assert d["relative_paths"]
 
         d = _make_paths_absolute(d, folder_path)
 
-        NpzSortingExtractor.__init__(self, **d['kwargs'])
+        NpzSortingExtractor.__init__(self, **d["kwargs"])
 
         folder_metadata = folder_path
         self.load_metadata_from_folder(folder_metadata)
-        
+
         self._kwargs = dict(folder_path=str(folder_path.absolute()))
-        self._npz_kwargs = d['kwargs']
+        self._npz_kwargs = d["kwargs"]
 
 
 read_npz_folder = define_function_from_class(source_class=NpzFolderSorting, name="read_npz_folder")
```

### Comparing `spikeinterface-0.97.1/spikeinterface/core/npzsortingextractor.py` & `spikeinterface-0.98.0/src/spikeinterface/core/npzsortingextractor.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,77 +1,75 @@
 from .basesorting import BaseSorting, BaseSortingSegment
 
 from pathlib import Path
 import numpy as np
 
 from .core_tools import define_function_from_class
 
+
 class NpzSortingExtractor(BaseSorting):
     """
     Dead simple and super light format based on the NPZ numpy format.
     https://docs.scipy.org/doc/numpy/reference/generated/numpy.savez.html#numpy.savez
 
     It is in fact an archive of several .npy format.
     All spike are store in two columns maner index+labels
     """
 
-    extractor_name = 'NpzSortingExtractor'
-    is_writable = True
-    mode = 'file'
+    extractor_name = "NpzSortingExtractor"
+    mode = "file"
     name = "npz"
 
     def __init__(self, file_path):
-
         self.npz_filename = file_path
 
         npz = np.load(file_path)
-        num_segment = int(npz['num_segment'][0])
-        unit_ids = npz['unit_ids']
-        sampling_frequency = float(npz['sampling_frequency'][0])
+        num_segment = int(npz["num_segment"][0])
+        unit_ids = npz["unit_ids"]
+        sampling_frequency = float(npz["sampling_frequency"][0])
 
         BaseSorting.__init__(self, sampling_frequency, unit_ids)
 
         for seg_index in range(num_segment):
-            spike_indexes = npz[f'spike_indexes_seg{seg_index}']
-            spike_labels = npz[f'spike_labels_seg{seg_index}']
+            spike_indexes = npz[f"spike_indexes_seg{seg_index}"]
+            spike_labels = npz[f"spike_labels_seg{seg_index}"]
             sorting_segment = NpzSortingSegment(spike_indexes, spike_labels)
             self.add_sorting_segment(sorting_segment)
 
-        self._kwargs = {'file_path': str(Path(file_path).absolute())}
+        self._kwargs = {"file_path": str(Path(file_path).absolute())}
 
     @staticmethod
     def write_sorting(sorting, save_path):
         d = {}
         units_ids = np.array(sorting.get_unit_ids())
-        d['unit_ids'] = units_ids
-        d['num_segment'] = np.array([sorting.get_num_segments()], dtype='int64')
-        d['sampling_frequency'] = np.array([sorting.get_sampling_frequency()], dtype='float64')
+        d["unit_ids"] = units_ids
+        d["num_segment"] = np.array([sorting.get_num_segments()], dtype="int64")
+        d["sampling_frequency"] = np.array([sorting.get_sampling_frequency()], dtype="float64")
 
         for seg_index in range(sorting.get_num_segments()):
-
             spike_indexes = []
             spike_labels = []
             for unit_id in units_ids:
                 sp_ind = sorting.get_unit_spike_train(unit_id, segment_index=seg_index)
-                spike_indexes.append(sp_ind.astype('int64'))
+                spike_indexes.append(sp_ind.astype("int64"))
                 # spike_labels.append(np.ones(sp_ind.size, dtype='int64')*unit_id)
                 spike_labels.append(np.array([unit_id] * sp_ind.size))
 
             # order times
             if len(spike_indexes) > 0:
                 spike_indexes = np.concatenate(spike_indexes)
                 spike_labels = np.concatenate(spike_labels)
                 order = np.argsort(spike_indexes)
                 spike_indexes = spike_indexes[order]
                 spike_labels = spike_labels[order]
             else:
-                spike_indexes = np.array([], dtype='int64')
-                spike_labels = np.array([], dtype='int64')
-            d[f'spike_indexes_seg{seg_index}'] = spike_indexes
-            d[f'spike_labels_seg{seg_index}'] = spike_labels
+                spike_indexes = np.array([], dtype="int64")
+                spike_labels = np.array([], dtype="int64")
+            d[f"spike_indexes_seg{seg_index}"] = spike_indexes
+            d[f"spike_labels_seg{seg_index}"] = spike_labels
 
         np.savez(save_path, **d)
 
 
 class NpzSortingSegment(BaseSortingSegment):
     def __init__(self, spike_indexes, spike_labels):
         BaseSortingSegment.__init__(self)
@@ -81,11 +79,11 @@
 
     def get_unit_spike_train(self, unit_id, start_frame, end_frame):
         spike_times = self.spike_indexes[self.spike_labels == unit_id]
         if start_frame is not None:
             spike_times = spike_times[spike_times >= start_frame]
         if end_frame is not None:
             spike_times = spike_times[spike_times < end_frame]
-        return spike_times.astype('int64')
+        return spike_times.astype("int64")
 
 
 read_npz_sorting = define_function_from_class(source_class=NpzSortingExtractor, name="read_npz_sorting")
```

### Comparing `spikeinterface-0.97.1/spikeinterface/core/numpyextractors.py` & `spikeinterface-0.98.0/src/spikeinterface/core/numpyextractors.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,14 +1,21 @@
 import numpy as np
-from spikeinterface.core import (BaseRecording, BaseSorting,
-                                 BaseRecordingSegment, BaseSortingSegment,
-                                 BaseEvent, BaseEventSegment,
-                                 BaseSnippets, BaseSnippetsSegment)
+from spikeinterface.core import (
+    BaseRecording,
+    BaseSorting,
+    BaseRecordingSegment,
+    BaseSortingSegment,
+    BaseEvent,
+    BaseEventSegment,
+    BaseSnippets,
+    BaseSnippetsSegment,
+)
 from typing import List, Union
 
+
 class NumpyRecording(BaseRecording):
     """
     In memory recording.
     Contrary to previous version this class does not handle npy files.
 
     Parameters
     ----------
@@ -17,79 +24,89 @@
     sampling_frequency: float
         The sampling frequency in Hz
     t_starts: None or list of float
         Times in seconds of the first sample for each segment
     channel_ids: list
         An optional list of channel_ids. If None, linear channels are assumed
     """
-    extractor_name = 'Numpy'
-    mode = 'memory'
+
+    extractor_name = "Numpy"
+    mode = "memory"
     name = "numpy"
 
     def __init__(self, traces_list, sampling_frequency, t_starts=None, channel_ids=None):
         if isinstance(traces_list, list):
-            assert all(isinstance(e, np.ndarray) for e in traces_list), 'must give a list of numpy array'
+            all_elements_are_list = all(isinstance(e, list) for e in traces_list)
+            if all_elements_are_list:
+                traces_list = [np.array(trace) for trace in traces_list]
+            assert all(
+                isinstance(e, np.ndarray) for e in traces_list
+            ), f"must give a list of numpy array but gave {traces_list[0]}"
         else:
-            assert isinstance(traces_list, np.ndarray), 'must give a list of numpy array'
+            assert isinstance(traces_list, np.ndarray), "must give a list of numpy array"
             traces_list = [traces_list]
 
         dtype = traces_list[0].dtype
-        assert all(dtype == ts.dtype for ts in traces_list)
+        assert all(dtype == trace.dtype for trace in traces_list)
 
         if channel_ids is None:
             channel_ids = np.arange(traces_list[0].shape[1])
         else:
             channel_ids = np.asarray(channel_ids)
             assert channel_ids.size == traces_list[0].shape[1]
         BaseRecording.__init__(self, sampling_frequency, channel_ids, dtype)
 
         if t_starts is not None:
-            assert len(t_starts) == len(traces_list), 't_starts must be a list of same size than traces_list'
+            assert len(t_starts) == len(traces_list), "t_starts must be a list of same size than traces_list"
             t_starts = [float(t_start) for t_start in t_starts]
 
-        self.is_dumpable = False
+        self._is_json_serializable = False
 
         for i, traces in enumerate(traces_list):
             if t_starts is None:
                 t_start = None
             else:
                 t_start = t_starts[i]
             rec_segment = NumpyRecordingSegment(traces, sampling_frequency, t_start)
             self.add_recording_segment(rec_segment)
 
-        self._kwargs = {'traces_list': traces_list, 't_starts': t_starts,
-                        'sampling_frequency': sampling_frequency,
-                        }
+        self._kwargs = {
+            "traces_list": traces_list,
+            "t_starts": t_starts,
+            "sampling_frequency": sampling_frequency,
+        }
 
 
 class NumpyRecordingSegment(BaseRecordingSegment):
     def __init__(self, traces, sampling_frequency, t_start):
         BaseRecordingSegment.__init__(self, sampling_frequency=sampling_frequency, t_start=t_start)
         self._traces = traces
+        self.num_samples = traces.shape[0]
 
     def get_num_samples(self):
-        return self._traces.shape[0]
+        return self.num_samples
 
     def get_traces(self, start_frame, end_frame, channel_indices):
         traces = self._traces[start_frame:end_frame, :]
         if channel_indices is not None:
             traces = traces[:, channel_indices]
 
         return traces
 
 
 class NumpySorting(BaseSorting):
     name = "numpy"
 
     def __init__(self, sampling_frequency, unit_ids=[]):
         BaseSorting.__init__(self, sampling_frequency, unit_ids)
-        self.is_dumpable = False
+        self._is_dumpable = False
+        self._is_json_serializable = False
 
     @staticmethod
-    def from_extractor(source_sorting):
+    def from_extractor(source_sorting: BaseSorting) -> "NumpySorting":
         """
         Create a numpy sorting from another extractor
         """
         unit_ids = source_sorting.get_unit_ids()
         nseg = source_sorting.get_num_segments()
 
         sorting = NumpySorting(source_sorting.get_sampling_frequency(), unit_ids)
@@ -101,15 +118,15 @@
             sorting.add_sorting_segment(NumpySortingSegment(units_dict))
 
         sorting.copy_metadata(source_sorting)
 
         return sorting
 
     @staticmethod
-    def from_times_labels(times_list, labels_list, sampling_frequency, unit_ids=None):
+    def from_times_labels(times_list, labels_list, sampling_frequency, unit_ids=None) -> "NumpySorting":
         """
         Construct sorting extractor from:
           * an array of spike times (in frames)
           * an array of spike labels and adds all the
         In case of multisegment, it is a list of array.
 
         Parameters
@@ -142,18 +159,18 @@
                 mask = labels == unit_id
                 units_dict[unit_id] = times[mask]
             sorting.add_sorting_segment(NumpySortingSegment(units_dict))
 
         return sorting
 
     @staticmethod
-    def from_dict(units_dict_list, sampling_frequency):
+    def from_dict(units_dict_list, sampling_frequency) -> "NumpySorting":
         """
         Construct sorting extractor from a list of dict.
-        The list lenght is the segment count
+        The list length is the segment count
         Each dict have unit_ids as keys and spike times as values.
 
         Parameters
         ----------
         dict_list: list of dict
         """
         if isinstance(units_dict_list, dict):
@@ -164,15 +181,15 @@
         sorting = NumpySorting(sampling_frequency, unit_ids)
         for i, units_dict in enumerate(units_dict_list):
             sorting.add_sorting_segment(NumpySortingSegment(units_dict))
 
         return sorting
 
     @staticmethod
-    def from_neo_spiketrain_list(neo_spiketrains, sampling_frequency, unit_ids=None):
+    def from_neo_spiketrain_list(neo_spiketrains, sampling_frequency, unit_ids=None) -> "NumpySorting":
         """
         Construct a sorting with a neo spiketrain list.
 
         If this is a list of list, it is multi segment.
 
         Parameters
         ----------
@@ -188,29 +205,28 @@
         elif isinstance(neo_spiketrains[0], neo.SpikeTrain):
             # unique segment
             neo_spiketrains = [neo_spiketrains]
 
         nseg = len(neo_spiketrains)
 
         if unit_ids is None:
-            unit_ids = np.arange(len(neo_spiketrains[0]), dtype='int64')
+            unit_ids = np.arange(len(neo_spiketrains[0]), dtype="int64")
 
         sorting = NumpySorting(sampling_frequency, unit_ids)
         for seg_index in range(nseg):
-
             units_dict = {}
             for u, unit_id in enumerate(unit_ids):
                 st = neo_spiketrains[seg_index][u]
-                units_dict[unit_id] = (st.rescale('s').magnitude * sampling_frequency).astype('int64')
+                units_dict[unit_id] = (st.rescale("s").magnitude * sampling_frequency).astype("int64")
             sorting.add_sorting_segment(NumpySortingSegment(units_dict))
 
         return sorting
 
     @staticmethod
-    def from_peaks(peaks, sampling_frequency):
+    def from_peaks(peaks, sampling_frequency) -> "NumpySorting":
         """
         Construct a sorting from peaks returned by 'detect_peaks()' function.
         The unit ids correspond to the recording channel ids and spike trains are the
         detected spikes for each channel.
 
         Parameters
         ----------
@@ -220,23 +236,23 @@
             the sampling frequency in Hz
 
         Returns
         -------
         sorting
             The NumpySorting object
         """
-        return NumpySorting.from_times_labels(peaks['sample_ind'], peaks['channel_ind'], sampling_frequency)
+        return NumpySorting.from_times_labels(peaks["sample_index"], peaks["channel_index"], sampling_frequency)
 
 
 class NumpySortingSegment(BaseSortingSegment):
     def __init__(self, units_dict):
         BaseSortingSegment.__init__(self)
         for unit_id, times in units_dict.items():
-            assert times.dtype.kind == 'i', 'numpy array of spike times must be integer'
-            assert np.all(np.diff(times) >= 0), 'unsorted times'
+            assert times.dtype.kind == "i", "numpy array of spike times must be integer"
+            assert np.all(np.diff(times) >= 0), "unsorted times"
         self._units_dict = units_dict
 
     def get_unit_spike_train(self, unit_id, start_frame, end_frame):
         times = self._units_dict[unit_id]
         if start_frame is not None:
             times = times[times >= start_frame]
         if end_frame is not None:
@@ -323,64 +339,68 @@
 
     channel_ids: list
         An optional list of channel_ids. If None, linear channels are assumed
     """
 
     def __init__(self, snippets_list, spikesframes_list, sampling_frequency, nbefore=None, channel_ids=None):
         if isinstance(snippets_list, list):
-            assert all(isinstance(e, np.ndarray)
-                       for e in snippets_list), 'must give a list of numpy array'
+            assert all(isinstance(e, np.ndarray) for e in snippets_list), "must give a list of numpy array"
         else:
-            assert isinstance(
-                snippets_list, np.ndarray), 'must give a list of numpy array'
+            assert isinstance(snippets_list, np.ndarray), "must give a list of numpy array"
             snippets_list = [snippets_list]
         if isinstance(spikesframes_list, list):
-            assert all(isinstance(e, np.ndarray)
-                       for e in spikesframes_list), 'must give a list of numpy array'
+            assert all(isinstance(e, np.ndarray) for e in spikesframes_list), "must give a list of numpy array"
         else:
-            assert isinstance(spikesframes_list,
-                              np.ndarray), 'must give a list of numpy array'
+            assert isinstance(spikesframes_list, np.ndarray), "must give a list of numpy array"
             spikesframes_list = [spikesframes_list]
 
         dtype = snippets_list[0].dtype
         assert all(dtype == ts.dtype for ts in snippets_list)
 
         if channel_ids is None:
             channel_ids = np.arange(snippets_list[0].shape[2])
         else:
             channel_ids = np.asarray(channel_ids)
             assert channel_ids.size == snippets_list[0].shape[2]
-        BaseSnippets.__init__(self, sampling_frequency,  nbefore=nbefore,
-                              snippet_len=snippets_list[0].shape[1], channel_ids=channel_ids,
-                              dtype=dtype)
+        BaseSnippets.__init__(
+            self,
+            sampling_frequency,
+            nbefore=nbefore,
+            snippet_len=snippets_list[0].shape[1],
+            channel_ids=channel_ids,
+            dtype=dtype,
+        )
 
-        self.is_dumpable = False
+        self._is_dumpable = False
+        self._is_json_serializable = False
 
         for snippets, spikesframes in zip(snippets_list, spikesframes_list):
             snp_segment = NumpySnippetsSegment(snippets, spikesframes)
             self.add_snippets_segment(snp_segment)
 
-        self._kwargs = {'snippets_list': snippets_list,
-                        'spikesframes_list': spikesframes_list,
-                        'nbefore': nbefore,
-                        'sampling_frequency': sampling_frequency,
-                        'channel_ids': channel_ids
-                        }
+        self._kwargs = {
+            "snippets_list": snippets_list,
+            "spikesframes_list": spikesframes_list,
+            "nbefore": nbefore,
+            "sampling_frequency": sampling_frequency,
+            "channel_ids": channel_ids,
+        }
 
 
 class NumpySnippetsSegment(BaseSnippetsSegment):
     def __init__(self, snippets, spikesframes):
         BaseSnippetsSegment.__init__(self)
         self._snippets = snippets
         self._spikestimes = spikesframes
 
-    def get_snippets(self,
-                     indices,
-                     channel_indices: Union[List, None] = None,
-                     ) -> np.ndarray:
+    def get_snippets(
+        self,
+        indices,
+        channel_indices: Union[List, None] = None,
+    ) -> np.ndarray:
         """
         Return the snippets, optionally for a subset of samples and/or channels
 
         Parameters
         ----------
         indexes: (Union[int, None], optional)
             start sample index, or zero if None. Defaults to None.
@@ -391,23 +411,21 @@
 
         Returns
         -------
         snippets: np.ndarray
             Array of snippets, num_snippets x num_samples x num_channels
         """
         if indices is None:
-            return self._snippets[:,:,channel_indices]
-        return self._snippets[indices,:,channel_indices]
+            return self._snippets[:, :, channel_indices]
+        return self._snippets[indices, :, channel_indices]
 
     def get_num_snippets(self):
         return self._spikestimes.shape[0]
 
-    def frames_to_indices(self,
-                          start_frame: Union[int, None] = None,
-                          end_frame: Union[int, None] = None):
+    def frames_to_indices(self, start_frame: Union[int, None] = None, end_frame: Union[int, None] = None):
         """
         Return the slice of snippets
 
         Parameters
         ----------
         start_frame: (Union[int, None], optional)
             start sample index, or zero if None. Defaults to None.
@@ -419,23 +437,23 @@
         snippets: slice
             slice of selected snippets
         """
         # must be implemented in subclass
         if start_frame is None:
             init = 0
         else:
-            init = np.searchsorted(self._spikestimes, start_frame, side='left')
+            init = np.searchsorted(self._spikestimes, start_frame, side="left")
         if end_frame is None:
             endi = self._spikestimes.shape[0]
         else:
-            endi = np.searchsorted(self._spikestimes, end_frame, side='left')
+            endi = np.searchsorted(self._spikestimes, end_frame, side="left")
         return slice(init, endi, 1)
 
     def get_frames(self, indices=None):
         """Returns the frames of the snippets in this segment
 
         Returns:
             SampleIndex: Number of samples in the segment
         """
         if indices is None:
             return self._spikestimes
-        raise self._spikestimes[indices]
+        raise self._spikestimes[indices]
```

### Comparing `spikeinterface-0.97.1/spikeinterface/core/old_api_utils.py` & `spikeinterface-0.98.0/src/spikeinterface/core/old_api_utils.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,13 +1,12 @@
 from typing import Union
 
 import numpy as np
 import warnings
-from spikeinterface.core import (BaseRecording, BaseSorting,
-                                 BaseRecordingSegment, BaseSortingSegment)
+from spikeinterface.core import BaseRecording, BaseSorting, BaseRecordingSegment, BaseSortingSegment
 
 
 class NewToOldRecording:
     """
     This class mimic the old API of spikeextractors with:
       * reversed shape (channels, samples):
       * unique segment
@@ -18,17 +17,17 @@
     """
 
     def __init__(self, recording):
         assert recording.get_num_segments() == 1
         self._recording = recording
 
     def get_traces(self, channel_ids=None, start_frame=None, end_frame=None):
-        traces = self._recording.get_traces(channel_ids=channel_ids,
-                                            start_frame=start_frame, end_frame=end_frame,
-                                            segment_index=0)
+        traces = self._recording.get_traces(
+            channel_ids=channel_ids, start_frame=start_frame, end_frame=end_frame, segment_index=0
+        )
         return traces.T
 
     def get_num_frames(self):
         return self._recording.get_num_frames(segment_index=0)
 
     def get_num_channels(self):
         return self._recording.get_num_channels()
@@ -53,32 +52,36 @@
 
 
 class NewToOldSorting:
     """
     This class mimic the old API of spikeextractors with:
       * unique segment
     """
-    extractor_name = 'NewToOldSorting'
+
+    extractor_name = "NewToOldSorting"
 
     def __init__(self, sorting):
         assert sorting.get_num_segments() == 1
         self._sorting = sorting
         self._sampling_frequency = sorting.get_sampling_frequency()
-        self.is_dumpable = False
 
         unit_map = {}
         if np.all([isinstance(unit_id, int)] for unit_id in self._sorting.get_unit_ids()):
             for u in self._sorting.get_unit_ids():
                 unit_map[u] = u
         else:
-            print("Some unit IDs are not int but all unit IDs must be int in the old API SortingExtractor. Converting unit IDs to index...")
+            print(
+                "Some unit IDs are not int but all unit IDs must be int in the old API SortingExtractor. Converting unit IDs to index..."
+            )
             for i_u, u in enumerate(self._sorting.get_unit_ids()):
                 unit_map[i_u] = u
         self._unit_map = unit_map
 
+        self._kwargs = dict(sorting=sorting)
+
     def get_unit_ids(self):
         """This function returns a list of ids (ints) for each unit in the sorsted result.
 
         Returns
         -------
         unit_ids: array_like
             A list of the unit ids in the sorted result (ints).
@@ -111,16 +114,17 @@
 
         Returns
         -------
         spike_train: numpy.ndarray
             An 1D array containing all the frames for each spike in the
             specified unit given the range of start and end frames
         """
-        return self._sorting.get_unit_spike_train(unit_id=self._unit_map[unit_id], segment_index=0,
-                                                  start_frame=start_frame, end_frame=end_frame)
+        return self._sorting.get_unit_spike_train(
+            unit_id=self._unit_map[unit_id], segment_index=0, start_frame=start_frame, end_frame=end_frame
+        )
 
     def get_units_spike_train(self, unit_ids=None, start_frame=None, end_frame=None):
         """This function extracts spike frames from the specified units.
 
         Parameters
         ----------
         unit_ids: array_like
@@ -149,14 +153,15 @@
         Returns
         -------
         sampling_frequency: float
             The sampling frequency
         """
         return self._sampling_frequency
 
+
 def create_extractor_from_new_sorting(new_sorting):
     old_sorting = NewToOldSorting(new_sorting)
     return old_sorting
 
 
 class OldToNewRecording(BaseRecording):
     """Wrapper class to convert old RecordingExtractor to a
@@ -165,72 +170,81 @@
     Parameters
     ----------
     oldapi_recording_extractor : se.RecordingExtractor
         recording extractor from spikeinterface < v0.90
     """
 
     def __init__(self, oldapi_recording_extractor):
-        BaseRecording.__init__(self, sampling_frequency=float(oldapi_recording_extractor.get_sampling_frequency()),
-                               channel_ids=oldapi_recording_extractor.get_channel_ids(),
-                               dtype=oldapi_recording_extractor.get_dtype(return_scaled=False))
+        BaseRecording.__init__(
+            self,
+            sampling_frequency=float(oldapi_recording_extractor.get_sampling_frequency()),
+            channel_ids=oldapi_recording_extractor.get_channel_ids(),
+            dtype=oldapi_recording_extractor.get_dtype(return_scaled=False),
+        )
+
+        # set _is_dumpable to False to use dumping mechanism of old extractor
+        self._is_dumpable = False
+        self._is_json_serializable = False
 
-        # set is_dumpable to False to use dumping mechanism of old extractor
-        self.is_dumpable = False
         self.annotate(is_filtered=oldapi_recording_extractor.is_filtered)
 
         # add old recording as a recording segment
         recording_segment = OldToNewRecordingSegment(oldapi_recording_extractor)
         self.add_recording_segment(recording_segment)
         self.set_channel_locations(oldapi_recording_extractor.get_channel_locations())
 
         # add old properties
-        copy_properties(oldapi_extractor=oldapi_recording_extractor, new_extractor=self,
-                        skip_properties=["gain", "offset"])
+        copy_properties(
+            oldapi_extractor=oldapi_recording_extractor, new_extractor=self, skip_properties=["gain", "offset"]
+        )
         # set correct gains and offsets
-        gains, offsets = find_old_gains_offsets_recursively(
-            oldapi_recording_extractor.dump_to_dict())
+        gains, offsets = find_old_gains_offsets_recursively(oldapi_recording_extractor.dump_to_dict())
         if gains is not None:
             if np.any(gains != 1):
                 self.set_channel_gains(gains)
         if offsets is not None:
             if np.any(offsets != 0):
                 self.set_channel_offsets(offsets)
 
-        self._kwargs = {'oldapi_recording_extractor': oldapi_recording_extractor}
+        self._kwargs = {"oldapi_recording_extractor": oldapi_recording_extractor}
 
 
 class OldToNewRecordingSegment(BaseRecordingSegment):
     """Wrapper class to convert old RecordingExtractor to a
     RecordingSegment in spikeinterface > v0.90
 
     Parameters
     ----------
     oldapi_recording_extractor : se.RecordingExtractor
         recording extractor from spikeinterface < v0.90
     """
+
     def __init__(self, oldapi_recording_extractor):
-        BaseRecordingSegment.__init__(self, sampling_frequency=float(oldapi_recording_extractor.get_sampling_frequency()),
-                                      t_start=None, time_vector=None)
+        BaseRecordingSegment.__init__(
+            self,
+            sampling_frequency=float(oldapi_recording_extractor.get_sampling_frequency()),
+            t_start=None,
+            time_vector=None,
+        )
         self._oldapi_recording_extractor = oldapi_recording_extractor
         self._channel_ids = np.array(oldapi_recording_extractor.get_channel_ids())
 
-        self._kwargs = {'oldapi_recording_extractor': oldapi_recording_extractor}
+        self._kwargs = {"oldapi_recording_extractor": oldapi_recording_extractor}
 
     def get_num_samples(self):
         return self._oldapi_recording_extractor.get_num_frames()
 
     def get_traces(self, start_frame, end_frame, channel_indices):
         if channel_indices is None:
             channel_ids = self._channel_ids
         else:
             channel_ids = self._channel_ids[channel_indices]
-        return self._oldapi_recording_extractor.get_traces(channel_ids=channel_ids,
-                                                           start_frame=start_frame,
-                                                           end_frame=end_frame,
-                                                           return_scaled=False).T
+        return self._oldapi_recording_extractor.get_traces(
+            channel_ids=channel_ids, start_frame=start_frame, end_frame=end_frame, return_scaled=False
+        ).T
 
 
 def create_recording_from_old_extractor(oldapi_recording_extractor) -> OldToNewRecording:
     new_recording = OldToNewRecording(oldapi_recording_extractor)
     return new_recording
 
 
@@ -241,52 +255,57 @@
     Parameters
     ----------
     oldapi_sorting_extractor : se.SortingExtractor
         sorting extractor from spikeinterface < v0.90
     """
 
     def __init__(self, oldapi_sorting_extractor):
-        BaseSorting.__init__(self, sampling_frequency=float(oldapi_sorting_extractor.get_sampling_frequency()),
-                             unit_ids=oldapi_sorting_extractor.get_unit_ids())
+        BaseSorting.__init__(
+            self,
+            sampling_frequency=float(oldapi_sorting_extractor.get_sampling_frequency()),
+            unit_ids=oldapi_sorting_extractor.get_unit_ids(),
+        )
 
         sorting_segment = OldToNewSortingSegment(oldapi_sorting_extractor)
         self.add_sorting_segment(sorting_segment)
 
-        self.is_dumpable = False
+        self._is_dumpable = False
+        self._is_json_serializable = False
 
         # add old properties
         copy_properties(oldapi_extractor=oldapi_sorting_extractor, new_extractor=self)
 
-        self._kwargs = {'oldapi_sorting_extractor': oldapi_sorting_extractor}
+        self._kwargs = {"oldapi_sorting_extractor": oldapi_sorting_extractor}
 
 
 class OldToNewSortingSegment(BaseSortingSegment):
     """Wrapper class to convert old SortingExtractor to a
     SortingSegment in spikeinterface > v0.90
 
     Parameters
     ----------
     oldapi_sorting_extractor : se.SortingExtractor
         sorting extractor from spikeinterface < v0.90
     """
+
     def __init__(self, oldapi_sorting_extractor):
         BaseSortingSegment.__init__(self)
         self._oldapi_sorting_extractor = oldapi_sorting_extractor
 
-        self._kwargs = {'oldapi_sorting_extractor': oldapi_sorting_extractor}
+        self._kwargs = {"oldapi_sorting_extractor": oldapi_sorting_extractor}
 
-    def get_unit_spike_train(self,
-                             unit_id,
-                             start_frame: Union[int, None] = None,
-                             end_frame: Union[int, None] = None,
-                             ) -> np.ndarray:
-
-        return self._oldapi_sorting_extractor.get_unit_spike_train(unit_id=unit_id,
-                                                                   start_frame=start_frame,
-                                                                   end_frame=end_frame)
+    def get_unit_spike_train(
+        self,
+        unit_id,
+        start_frame: Union[int, None] = None,
+        end_frame: Union[int, None] = None,
+    ) -> np.ndarray:
+        return self._oldapi_sorting_extractor.get_unit_spike_train(
+            unit_id=unit_id, start_frame=start_frame, end_frame=end_frame
+        )
 
 
 def create_sorting_from_old_extractor(oldapi_sorting_extractor) -> OldToNewSorting:
     new_sorting = OldToNewSorting(oldapi_sorting_extractor)
     return new_sorting
 
 
@@ -321,33 +340,32 @@
             properties[prop_name]["ids"].append(id)
             properties[prop_name]["values"].append(prop_value)
 
     for property_name, prop_dict in properties.items():
         property_ids = np.array(prop_dict["ids"])
         property_values = np.array(prop_dict["values"])
         missing_value = None
-        
+
         # For back-compatibility, incomplete int/uint properties are upcast to float
         # and missing_value is set to np.nan
         if len(property_ids) < len(get_ids()):
             if property_values.dtype.kind in ("u", "i"):
                 property_values = property_values.astype("float")
                 missing_value = np.nan
         try:
-            new_extractor.set_property(key=property_name,
-                                       values=property_values, 
-                                       ids=property_ids,
-                                       missing_value=missing_value)
+            new_extractor.set_property(
+                key=property_name, values=property_values, ids=property_ids, missing_value=missing_value
+            )
         except Exception as e:
             warnings.warn(f"Property {property_name} cannot be ported to new API due to missing values.")
 
 
 def find_old_gains_offsets_recursively(oldapi_extractor_dict):
-    kwargs = oldapi_extractor_dict['kwargs']
-    if np.any([isinstance(v, dict) and 'dumpable' in v.keys() for (k, v) in kwargs.items()]):
+    kwargs = oldapi_extractor_dict["kwargs"]
+    if np.any([isinstance(v, dict) and "dumpable" in v.keys() for (k, v) in kwargs.items()]):
         # check nested
         for k, v in oldapi_extractor_dict["kwargs"].items():
             if isinstance(v, dict) and "dumpable" in v:
                 return find_old_gains_offsets_recursively(v)
     else:
         gains = oldapi_extractor_dict["key_properties"]["gain"]
         offsets = oldapi_extractor_dict["key_properties"]["offset"]
```

### Comparing `spikeinterface-0.97.1/spikeinterface/core/segmentutils.py` & `spikeinterface-0.98.0/src/spikeinterface/core/segmentutils.py`

 * *Files 19% similar despite different names*

```diff
@@ -7,88 +7,88 @@
 
 from typing import List, Union
 
 
 def _check_sampling_frequencies(sampling_frequency_list, sampling_frequency_max_diff):
     assert sampling_frequency_max_diff >= 0
     freq_0 = sampling_frequency_list[0]
-    max_diff = max( abs(freq - freq_0) for freq in sampling_frequency_list)
+    max_diff = max(abs(freq - freq_0) for freq in sampling_frequency_list)
     if max_diff > sampling_frequency_max_diff:
-        raise ValueError(f"Sampling frequencies across datasets differ by `{max_diff}`Hz which is more than "
-                         f"`sampling_frequency_max_diff`={sampling_frequency_max_diff}Hz")
+        raise ValueError(
+            f"Sampling frequencies across datasets differ by `{max_diff}`Hz which is more than "
+            f"`sampling_frequency_max_diff`={sampling_frequency_max_diff}Hz"
+        )
     elif max_diff > 0:
-        diff_sec = 24 * 3600 * max_diff / freq_0 
+        diff_ms = 24 * 3600000 * max_diff / freq_0
         import warnings
+
         warnings.warn(
             "Inconsistent sampling frequency across datasets."
             + f" Diff is below hard bound={sampling_frequency_max_diff}Hz: concatenating anyway."
-            + f" Expect ~{round(diff_sec, 5)}s shift over 24h dataset"
+            + f" Expect ~{round(diff_ms, 5)}ms shift over 24h dataset"
         )
 
 
-
 class AppendSegmentRecording(BaseRecording):
     """
     Takes as input a list of parent recordings each with multiple segments and
     returns a single multi-segment recording that "appends" all segments from
     all parent recordings.
 
-    For instance, given one recording with 2 segments and one recording with 3 segments, 
+    For instance, given one recording with 2 segments and one recording with 3 segments,
     this class will give one recording with 5 segments
 
     Parameters
     ----------
     recording_list : list of BaseRecording
         A list of recordings
     sampling_frequency_max_diff : float
         Maximum allowed difference of sampling frequencies across recordings (default 0)
     """
 
     def __init__(self, recording_list, sampling_frequency_max_diff=0):
-
         rec0 = recording_list[0]
         sampling_frequency = rec0.get_sampling_frequency()
         dtype = rec0.get_dtype()
         channel_ids = rec0.channel_ids
         self.recording_list = recording_list
 
         # check same characteristics
         ok1 = all(dtype == rec.get_dtype() for rec in recording_list)
         ok2 = all(np.array_equal(channel_ids, rec.channel_ids) for rec in recording_list)
         if not (ok1 and ok2):
             raise ValueError("Recording don't have the same dtype or channel_ids")
         _check_sampling_frequencies(
-            [rec.get_sampling_frequency() for rec in recording_list],
-            sampling_frequency_max_diff
+            [rec.get_sampling_frequency() for rec in recording_list], sampling_frequency_max_diff
         )
 
         BaseRecording.__init__(self, sampling_frequency, channel_ids, dtype)
         rec0.copy_metadata(self)
 
         for rec in recording_list:
             for parent_segment in rec._recording_segments:
                 rec_seg = ProxyAppendRecordingSegment(parent_segment)
                 self.add_recording_segment(rec_seg)
 
-        self._kwargs = {'recording_list': [rec.to_dict() for rec in recording_list]}
+        self._kwargs = {"recording_list": recording_list, "sampling_frequency_max_diff": sampling_frequency_max_diff}
 
 
 class ProxyAppendRecordingSegment(BaseRecordingSegment):
     def __init__(self, parent_segment):
         BaseRecordingSegment.__init__(self, **parent_segment.get_times_kwargs())
         self.parent_segment = parent_segment
 
     def get_num_samples(self):
         return self.parent_segment.get_num_samples()
 
     def get_traces(self, *args, **kwargs):
         return self.parent_segment.get_traces(*args, **kwargs)
 
 
-append_recordings = define_function_from_class(source_class=AppendSegmentRecording, name='append_segment_recording')
+append_recordings = define_function_from_class(source_class=AppendSegmentRecording, name="append_segment_recording")
 
 
 class ConcatenateSegmentRecording(BaseRecording):
     """
     Return a recording that "concatenates" all segments from all parent recordings
     into one recording with a single segment. The operation is lazy.
 
@@ -109,46 +109,53 @@
     ignore_times: bool
         If True (default), time information (t_start, time_vector) is ignored when concatenating recordings.
     sampling_frequency_max_diff : float
         Maximum allowed difference of sampling frequencies across recordings (default 0)
     """
 
     def __init__(self, recording_list, ignore_times=True, sampling_frequency_max_diff=0):
-
         one_rec = append_recordings(recording_list, sampling_frequency_max_diff=sampling_frequency_max_diff)
 
         BaseRecording.__init__(self, one_rec.get_sampling_frequency(), one_rec.channel_ids, one_rec.get_dtype())
         one_rec.copy_metadata(self)
         self.recording_list = recording_list
 
         parent_segments = []
         for rec in recording_list:
             for parent_segment in rec._recording_segments:
                 d = parent_segment.get_times_kwargs()
                 if not ignore_times:
-                    assert d['time_vector'] is None, ("ConcatenateSegmentRecording does not handle time_vector. "
-                                                      "Use ignore_times=True to ignore time information.")
-                    assert d['t_start'] is None, ("ConcatenateSegmentRecording does not handle t_start. "
-                                                  "Use ignore_times=True to ignore time information.")
+                    assert d["time_vector"] is None, (
+                        "ConcatenateSegmentRecording does not handle time_vector. "
+                        "Use ignore_times=True to ignore time information."
+                    )
+                    assert d["t_start"] is None, (
+                        "ConcatenateSegmentRecording does not handle t_start. "
+                        "Use ignore_times=True to ignore time information."
+                    )
                 parent_segments.append(parent_segment)
-        rec_seg = ProxyConcatenateRecordingSegment(parent_segments, one_rec.get_sampling_frequency(), 
-                                                   ignore_times=ignore_times)
+        rec_seg = ProxyConcatenateRecordingSegment(
+            parent_segments, one_rec.get_sampling_frequency(), ignore_times=ignore_times
+        )
         self.add_recording_segment(rec_seg)
 
-        self._kwargs = {'recording_list': [rec.to_dict() for rec in recording_list],
-                        'ignore_times': ignore_times}
+        self._kwargs = {
+            "recording_list": recording_list,
+            "ignore_times": ignore_times,
+            "sampling_frequency_max_diff": sampling_frequency_max_diff,
+        }
 
 
 class ProxyConcatenateRecordingSegment(BaseRecordingSegment):
     def __init__(self, parent_segments, sampling_frequency, ignore_times=True):
         if ignore_times:
             d = {}
-            d['t_start'] = None
-            d['time_vector'] = None
-            d['sampling_frequency'] = sampling_frequency
+            d["t_start"] = None
+            d["time_vector"] = None
+            d["sampling_frequency"] = sampling_frequency
         else:
             d = parent_segments[0].get_times_kwargs()
         BaseRecordingSegment.__init__(self, **d)
         self.parent_segments = parent_segments
         self.all_length = [rec_seg.get_num_samples() for rec_seg in self.parent_segments]
         self.cumsum_length = np.cumsum([0] + self.all_length)
         self.total_length = np.sum(self.all_length)
@@ -158,16 +165,16 @@
 
     def get_traces(self, start_frame, end_frame, channel_indices):
         if start_frame is None:
             start_frame = 0
         if end_frame is None:
             end_frame = self.get_num_samples()
 
-        i0 = np.searchsorted(self.cumsum_length, start_frame, side='right') - 1
-        i1 = np.searchsorted(self.cumsum_length, end_frame, side='right') - 1
+        i0 = np.searchsorted(self.cumsum_length, start_frame, side="right") - 1
+        i1 = np.searchsorted(self.cumsum_length, end_frame, side="right") - 1
 
         # several case:
         #  * come from one segment (i0 == i1)
         #  * come from several segment (i0 < i1)
 
         if i0 == i1:
             #  one segment
@@ -198,47 +205,49 @@
                     traces_chunk = rec_seg.get_traces(None, None, channel_indices)
                     all_traces.append(traces_chunk)
             traces = np.concatenate(all_traces, axis=0)
 
         return traces
 
 
-concatenate_recordings = define_function_from_class(source_class=ConcatenateSegmentRecording, name='concatenate_recordings')
+concatenate_recordings = define_function_from_class(
+    source_class=ConcatenateSegmentRecording, name="concatenate_recordings"
+)
+
 
 class SelectSegmentRecording(BaseRecording):
     """
     Return a new recording with a subset of segments from a multi-segment recording.
 
     Parameters
     ----------
     recording : BaseRecording
         The multi-segment recording
     segment_indices : list of int
         The segment indices to select
     """
 
     def __init__(self, recording: BaseRecording, segment_indices: Union[int, List[int]]):
-        BaseRecording.__init__(self, recording.get_sampling_frequency(), 
-                               recording.channel_ids, recording.get_dtype())
+        BaseRecording.__init__(self, recording.get_sampling_frequency(), recording.channel_ids, recording.get_dtype())
         recording.copy_metadata(self)
-        
+
         if isinstance(segment_indices, int):
             segment_indices = [segment_indices]
-        
+
         num_segments = recording.get_num_segments()
-        assert all(0 <= s < num_segments for s in segment_indices), \
-            f"'segment_index' must be between 0 and {num_segments - 1}"
+        assert all(
+            0 <= s < num_segments for s in segment_indices
+        ), f"'segment_index' must be between 0 and {num_segments - 1}"
 
         for segment_index in segment_indices:
             rec_seg = recording._recording_segments[segment_index]
             self.add_recording_segment(rec_seg)
 
-        self._kwargs = {'recording': recording.to_dict(),
-                        'segment_indices': [int(s) for s in segment_indices]}
-        
+        self._kwargs = {"recording": recording, "segment_indices": segment_indices}
+
 
 def split_recording(recording: BaseRecording):
     """
     Return a list of mono-segment recordings from a multi-segment recording.
 
     Parameters
     ----------
@@ -248,22 +257,22 @@
     Returns
     -------
     recording_list
         A list of mono-segment recordings
     """
     recording_list = []
     for segment_index in range(recording.get_num_segments()):
-        rec_mono = SelectSegmentRecording(
-            recording=recording, segment_indices=[segment_index])
+        rec_mono = SelectSegmentRecording(recording=recording, segment_indices=[segment_index])
         recording_list.append(rec_mono)
     return recording_list
 
 
-select_segment_recording = define_function_from_class(source_class=SelectSegmentRecording,
-                                                      name='select_segment_recording')
+select_segment_recording = define_function_from_class(
+    source_class=SelectSegmentRecording, name="select_segment_recording"
+)
 
 
 class AppendSegmentSorting(BaseSorting):
     """
     Return a sorting that "append" all segments from all sorting
     into one sorting multi segment.
 
@@ -272,50 +281,239 @@
     sorting_list : list of BaseSorting
         A list of sortings
     sampling_frequency_max_diff : float
         Maximum allowed difference of sampling frequencies across sortings (default 0)
     """
 
     def __init__(self, sorting_list, sampling_frequency_max_diff=0):
-
         sorting0 = sorting_list[0]
         sampling_frequency = sorting0.get_sampling_frequency()
         unit_ids = sorting0.unit_ids
         self.sorting_list = sorting_list
 
         # check same characteristics
         ok1 = all(np.array_equal(unit_ids, sorting.unit_ids) for sorting in sorting_list)
         if not ok1:
             raise ValueError("Sortings don't have the same unit_ids")
-        _check_sampling_frequencies(
-            [rec.get_sampling_frequency() for rec in sorting_list],
-            sampling_frequency_max_diff
-        )
+        _check_sampling_frequencies([rec.get_sampling_frequency() for rec in sorting_list], sampling_frequency_max_diff)
 
         BaseSorting.__init__(self, sampling_frequency, unit_ids)
         sorting0.copy_metadata(self)
 
         for sorting in sorting_list:
             for parent_segment in sorting._sorting_segments:
                 sorting_seg = ProxyAppendSortingSegment(parent_segment)
                 self.add_sorting_segment(sorting_seg)
 
-        self._kwargs = {'sorting_list': [sorting.to_dict() for sorting in sorting_list]}
+        self._kwargs = {"sorting_list": sorting_list, "sampling_frequency_max_diff": sampling_frequency_max_diff}
 
 
 class ProxyAppendSortingSegment(BaseSortingSegment):
     def __init__(self, parent_segment):
         BaseSortingSegment.__init__(self)
         self.parent_segment = parent_segment
 
     def get_unit_spike_train(self, *args, **kwargs):
         return self.parent_segment.get_unit_spike_train(*args, **kwargs)
 
 
-append_sortings = define_function_from_class(source_class=AppendSegmentSorting, name='append_sortings')
+append_sortings = define_function_from_class(source_class=AppendSegmentSorting, name="append_sortings")
+
+
+class ConcatenateSegmentSorting(BaseSorting):
+    """
+    Return a sorting that "concatenates" all segments from all sorting
+    into one sorting with a single segment. This operation is lazy.
+
+    For instance, given one recording with 2 segments and one recording with
+    3 segments, this class will give one recording with one large segment
+    made by concatenating the 5 segments. The returned spike times (originating
+    from each segment) are returned as relative to the start of the concatenated segment.
+
+    Time information is lost upon concatenation. By default `ignore_times` is True.
+    If it is False, you get an error unless:
+
+      * all segments DO NOT have times, AND
+      * all segment have t_start=None
+
+    Parameters
+    ----------
+    sorting_list : list of BaseSorting
+        A list of sortings. If `total_samples_list` is not provided, all
+        sortings should have an assigned recording.  Otherwise, all sortings
+        should be monosegments.
+    total_samples_list : list[int] or None
+        If the sortings have no assigned recording, the total number of samples
+        of each of the concatenated (monosegment) sortings is pulled from this
+        list.
+    ignore_times : bool
+        If True (default), time information (t_start, time_vector) is ignored
+        when concatenating the sortings' assigned recordings.
+    sampling_frequency_max_diff : float
+        Maximum allowed difference of sampling frequencies across sortings (default 0)
+    """
+
+    def __init__(self, sorting_list, total_samples_list=None, ignore_times=True, sampling_frequency_max_diff=0):
+        # Check that all sortings have a recording or that sortings' num_samples are provided
+        all_has_recording = all([sorting.has_recording() for sorting in sorting_list])
+        if not all_has_recording:
+            assert total_samples_list is not None, (
+                "Some concatenated sortings don't have a registered recording. "
+                "Call sorting.register_recording() or set `total_samples_list` kwarg."
+            )
+            assert len(total_samples_list) == len(
+                sorting_list
+            ), "`total_samples_list` should have the same number of elements as `sorting_list`"
+            assert all(
+                [s.get_num_segments() == 1 for s in sorting_list]
+            ), "All sortings are expected to be monosegment."
+            assert ignore_times, (
+                "Concatenating sortings without registered recordings: "
+                "Use ignore_times=True to ignore time information."
+            )
+        else:
+            assert total_samples_list is None, "Sortings have registered recordings: Use `total_samples_list=None`"
+
+        # Pull metadata from AppendSorting object
+        one_sorting = append_sortings(sorting_list, sampling_frequency_max_diff=sampling_frequency_max_diff)
+        BaseSorting.__init__(self, one_sorting.sampling_frequency, one_sorting.unit_ids)
+        one_sorting.copy_metadata(self)
+
+        # Check and pull n_samples from each segment
+        parent_segments = []
+        parent_num_samples = []
+        for sorting_i, sorting in enumerate(sorting_list):
+            for segment_i, parent_segment in enumerate(sorting._sorting_segments):
+                # Check t_start is not assigned
+                segment_t_start = parent_segment._t_start
+                if not ignore_times:
+                    assert segment_t_start is None, (
+                        "ConcatenateSegmentSorting does not handle Sorting.t_start. "
+                        "Set time information only in the sortings' assigned recordings, "
+                        "or use ignore_times=True to ignore time information."
+                    )
+                # Pull num samples for each segment
+                if sorting.has_recording():
+                    segment_num_samples = sorting.get_num_samples(segment_index=segment_i)
+                else:
+                    segment_num_samples = total_samples_list[sorting_i]
+                # Check consistency between num samples and spike frames
+                for unit_id in sorting.unit_ids:
+                    unit_segment_spikes = parent_segment.get_unit_spike_train(
+                        unit_id=unit_id,
+                        start_frame=None,
+                        end_frame=None,
+                    )
+                    if any([spike_frame >= segment_num_samples for spike_frame in unit_segment_spikes]):
+                        raise ValueError(
+                            "Sortings' spike frames exceed the provided number of samples for some segment. "
+                            "If the sortings have registered recordings, you can remove these excess "
+                            "spikes with `spikeinterface.curation.remove_excess_spikes(sorting, sorting._recording)`. "
+                            "Otherwise, the `total_num_samples` argument may contain invalid values."
+                        )
+                parent_segments.append(parent_segment)
+                parent_num_samples.append(segment_num_samples)
+        self.parent_num_samples = parent_num_samples
+
+        # Add a single Concatenated segment
+        sorting_seg = ProxyConcatenateSortingSegment(
+            parent_segments, parent_num_samples, one_sorting.get_sampling_frequency()
+        )
+        self.add_sorting_segment(sorting_seg)
+
+        # Assign concatenated recording if possible
+        if all_has_recording:
+            self.register_recording(
+                concatenate_recordings([s._recording for s in sorting_list], ignore_times=ignore_times)
+            )
+
+        self._kwargs = {
+            "sorting_list": sorting_list,
+            "ignore_times": ignore_times,
+            "total_samples_list": total_samples_list,
+            "sampling_frequency_max_diff": sampling_frequency_max_diff,
+        }
+
+    def get_num_samples(self, segment_index=None):
+        """Overrides the BaseSorting method, which requires a recording."""
+        segment_index = self._check_segment_index(segment_index)
+        n_samples = self._sorting_segments[segment_index].get_num_samples()
+        if self.has_recording():  # Sanity check
+            assert n_samples == self._recording.get_num_samples(segment_index)
+        return n_samples
+
+
+class ProxyConcatenateSortingSegment(BaseSortingSegment):
+    def __init__(self, parent_segments, parent_num_samples, sampling_frequency):
+        BaseSortingSegment.__init__(self)
+        self.parent_segments = parent_segments
+        self.parent_num_samples = parent_num_samples
+        self.cumsum_length = np.cumsum([0] + self.parent_num_samples)
+        self.total_num_samples = np.sum(self.parent_num_samples)
+
+    def get_num_samples(self):
+        return self.total_num_samples
+
+    def get_unit_spike_train(
+        self,
+        unit_id,
+        start_frame,
+        end_frame,
+    ):
+        if start_frame is None:
+            start_frame = 0
+        if end_frame is None:
+            end_frame = self.get_num_samples()
+
+        i0 = np.searchsorted(self.cumsum_length, start_frame, side="right") - 1
+        i1 = np.searchsorted(self.cumsum_length, end_frame, side="right") - 1
+
+        # several case:
+        #  * come from one segment (i0 == i1)
+        #  * come from several segment (i0 < i1)
+        if i0 == i1:
+            #  one segment
+            sorting_seg = self.parent_segments[i0]
+            seg_start = self.cumsum_length[i0]
+            spike_frames = (
+                sorting_seg.get_unit_spike_train(unit_id, start_frame - seg_start, end_frame - seg_start) + seg_start
+            )
+        else:
+            #  several segments
+            all_spike_frames = []
+            for i in range(i0, i1 + 1):
+                if i == len(self.parent_segments):
+                    # limit case
+                    continue
+
+                sorting_seg = self.parent_segments[i]
+                seg_start = self.cumsum_length[i]
+                if i == i0:
+                    # first
+                    spike_frames_chunk = (
+                        sorting_seg.get_unit_spike_train(unit_id, start_frame - seg_start, None) + seg_start
+                    )
+                    all_spike_frames.append(spike_frames_chunk)
+                elif i == i1:
+                    # last
+                    if (end_frame - seg_start) > 0:
+                        spike_frames_chunk = (
+                            sorting_seg.get_unit_spike_train(unit_id, None, end_frame - seg_start) + seg_start
+                        )
+                        all_spike_frames.append(spike_frames_chunk)
+                else:
+                    # in between
+                    spike_frames_chunk = sorting_seg.get_unit_spike_train(unit_id, None, None) + seg_start
+                    all_spike_frames.append(spike_frames_chunk)
+            spike_frames = np.concatenate(all_spike_frames, axis=0)
+
+        return spike_frames
+
+
+concatenate_sortings = define_function_from_class(source_class=ConcatenateSegmentSorting, name="concatenate_sortings")
 
 
 class SplitSegmentSorting(BaseSorting):
     """Splits a sorting with a single segment to multiple segments
     based on the given list of recordings (must be in order)
 
     Parameters
@@ -325,50 +523,55 @@
     recording_or_recording_list : list of recordings, ConcatenateSegmentRecording, or None
         If list of recordings, uses the lengths of those recordings to split the sorting
         into smaller segments
         If ConcatenateSegmentRecording, uses the associated list of recordings to split
         the sorting into smaller segments
         If None, looks for the recording associated with the sorting (default None)
     """
+
     def __init__(self, parent_sorting: BaseSorting, recording_or_recording_list=None):
         assert parent_sorting.get_num_segments() == 1, "The sorting must have only one segment."
         sampling_frequency = parent_sorting.get_sampling_frequency()
         unit_ids = parent_sorting.unit_ids
         BaseSorting.__init__(self, sampling_frequency, unit_ids)
         parent_sorting.copy_metadata(self)
 
         if recording_or_recording_list is None:
-            assert parent_sorting.has_recording(), ("There is no recording registered to the sorting object. "
-                                                    "Please specify the 'recording_or_recording_list' argument.")
+            assert parent_sorting.has_recording(), (
+                "There is no recording registered to the sorting object. "
+                "Please specify the 'recording_or_recording_list' argument."
+            )
             recording_list = [parent_sorting._recording]
         elif isinstance(recording_or_recording_list, list):
             # how to make sure this list only contains recordings (of possibly various types)?
             recording_list = recording_or_recording_list
         elif isinstance(recording_or_recording_list, ConcatenateSegmentRecording):
             recording_list = recording_or_recording_list.recording_list
         else:
-            raise TypeError("'recording_or_recording_list' must be a list of recordings, "
-                            "ConcatenateSegmentRecording, or None")
+            raise TypeError(
+                "'recording_or_recording_list' must be a list of recordings, " "ConcatenateSegmentRecording, or None"
+            )
 
         num_samples = [0]
         for recording in recording_list:
             for recording_segment in recording._recording_segments:
                 num_samples.append(recording_segment.get_num_samples())
 
         cumsum_num_samples = np.cumsum(num_samples)
-        for idx in range(len(cumsum_num_samples)-1):
-            sliced_parent_sorting = parent_sorting.frame_slice(start_frame=cumsum_num_samples[idx],
-                                                               end_frame=cumsum_num_samples[idx+1])
+        for idx in range(len(cumsum_num_samples) - 1):
+            sliced_parent_sorting = parent_sorting.frame_slice(
+                start_frame=cumsum_num_samples[idx], end_frame=cumsum_num_samples[idx + 1]
+            )
             sliced_segment = sliced_parent_sorting._sorting_segments[0]
             self.add_sorting_segment(sliced_segment)
 
-        self._kwargs = {'parent_sorting': parent_sorting.to_dict(),
-                        'recording_list': [recording.to_dict() for recording in recording_list]}
+        self._kwargs = {"parent_sorting": parent_sorting, "recording_or_recording_list": recording_list}
+
 
-split_sorting = define_function_from_class(source_class=SplitSegmentSorting, name='split_sorting')
+split_sorting = define_function_from_class(source_class=SplitSegmentSorting, name="split_sorting")
 
 
 class SelectSegmentSorting(BaseSorting):
     """
     Return a new sorting with a single segment from a multi-segment sorting.
 
     Parameters
@@ -376,28 +579,26 @@
     sorting : BaseSorting
         The multi-segment sorting
     segment_indices : list of int
         The segment indices to select
     """
 
     def __init__(self, sorting: BaseSorting, segment_indices: Union[int, List[int]]):
-        BaseSorting.__init__(self, sorting.get_sampling_frequency(), 
-                             sorting.unit_ids)
+        BaseSorting.__init__(self, sorting.get_sampling_frequency(), sorting.unit_ids)
         sorting.copy_metadata(self)
-        
+
         if isinstance(segment_indices, int):
             segment_indices = [segment_indices]
-        
+
         num_segments = sorting.get_num_segments()
-        assert all(0 <= s < num_segments for s in segment_indices), \
-            f"'segment_index' must be between 0 and {num_segments - 1}"
+        assert all(
+            0 <= s < num_segments for s in segment_indices
+        ), f"'segment_index' must be between 0 and {num_segments - 1}"
 
         for segment_index in segment_indices:
             sort_seg = sorting._sorting_segments[segment_index]
             self.add_sorting_segment(sort_seg)
 
-        self._kwargs = {'sorting': sorting.to_dict(),
-                        'segment_indices': [int(s) for s in segment_indices]}
+        self._kwargs = {"sorting": sorting, "segment_indices": [int(s) for s in segment_indices]}
 
 
-select_segment_sorting = define_function_from_class(source_class=SelectSegmentSorting,
-                                                    name='select_segment_sorting')
+select_segment_sorting = define_function_from_class(source_class=SelectSegmentSorting, name="select_segment_sorting")
```

### Comparing `spikeinterface-0.97.1/spikeinterface/core/snippets_tools.py` & `spikeinterface-0.98.0/src/spikeinterface/core/snippets_tools.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,15 +1,14 @@
 import numpy as np
 from .job_tools import fix_job_kwargs
 from .waveform_tools import extract_waveforms_to_buffers
 from .numpyextractors import NumpySnippets
 
 
-def snippets_from_sorting(recording, sorting, nbefore=20,
-                          nafter=44, wf_folder=None, **job_kwargs):
+def snippets_from_sorting(recording, sorting, nbefore=20, nafter=44, wf_folder=None, **job_kwargs):
     """
     Extract snippets from recording and sorting instances
 
     Parameters
     ----------
     recording: BaseRecording
         The recording to get snippets from
@@ -26,29 +25,43 @@
     snippets: NumpySnippets
         Snippets extractor created
     """
     job_kwargs = fix_job_kwargs(job_kwargs)
     strains = sorting.get_all_spike_trains()
 
     peaks2 = sorting.to_spike_vector()
-    peaks2['unit_ind'] = 0
+    peaks2["unit_index"] = 0
 
     if wf_folder is None:
         mode = "shared_memory"
         folder = None
     else:
         mode = "memmap"
         folder = wf_folder
 
-    wfs_arrays = extract_waveforms_to_buffers(recording, peaks2, [0], nbefore, nafter,
-                                              mode=mode, return_scaled=False, folder=folder,
-                                              dtype=recording.get_dtype(), sparsity_mask=None,
-                                              copy=True, **job_kwargs)
+    wfs_arrays = extract_waveforms_to_buffers(
+        recording,
+        peaks2,
+        [0],
+        nbefore,
+        nafter,
+        mode=mode,
+        return_scaled=False,
+        folder=folder,
+        dtype=recording.get_dtype(),
+        sparsity_mask=None,
+        copy=True,
+        **job_kwargs,
+    )
     wfs = []
     for i in range(recording.get_num_segments()):
-        wfs.append(wfs_arrays[0][peaks2['segment_ind'] == i, :, :])  # extract class zero
+        wfs.append(wfs_arrays[0][peaks2["segment_index"] == i, :, :])  # extract class zero
 
-    nse = NumpySnippets(snippets_list=wfs, spikesframes_list=[np.sort(s[0]) for s in strains],
-                        sampling_frequency=recording.get_sampling_frequency(),
-                        nbefore=nbefore, channel_ids=recording.get_channel_ids())
-    nse.set_property('gain_to_uV', recording.get_property('gain_to_uV'))
+    nse = NumpySnippets(
+        snippets_list=wfs,
+        spikesframes_list=[np.sort(s[0]) for s in strains],
+        sampling_frequency=recording.get_sampling_frequency(),
+        nbefore=nbefore,
+        channel_ids=recording.get_channel_ids(),
+    )
+    nse.set_property("gain_to_uV", recording.get_property("gain_to_uV"))
     return nse
```

### Comparing `spikeinterface-0.97.1/spikeinterface/core/sparsity.py` & `spikeinterface-0.98.0/src/spikeinterface/core/sparsity.py`

 * *Files 3% similar despite different names*

```diff
@@ -5,17 +5,20 @@
 
 _sparsity_doc = """
     method: str
         * "best_channels": N best channels with the largest amplitude. Use the 'num_channels' argument to specify the
                          number of channels.
         * "radius": radius around the best channel. Use the 'radius_um' argument to specify the radius in um
         * "snr": threshold based on template signal-to-noise ratio. Use the 'threshold' argument
-                 to specify the SNR threshold.
-        * "energy": threshold based on the expected energy that should be present on the channels, 
+                 to specify the SNR threshold (in units of noise levels)
+        * "ptp": threshold based on the peak-to-peak values on every channels. Use the 'threshold' argument
+                to specify the ptp threshold (in units of noise levels)
+        * "energy": threshold based on the expected energy that should be present on the channels,
                     given their noise levels. Use the 'threshold' argument to specify the SNR threshold
+                    (in units of noise levels)
         * "by_property": sparsity is given by a property of the recording and sorting(e.g. 'group').
                          Use the 'by_property' argument to specify the property name.
 
     peak_sign: str
         Sign of the template to compute best channels ('neg', 'pos', 'both')
     num_channels: int
         Number of channels for 'best_channels' method
@@ -54,15 +57,15 @@
     unit_ids: list or array
         Unit ids vector or list
     channel_ids: list or array
         Channel ids vector or list
 
     Examples
     --------
-    
+
     The class can also be used to construct/estimate the sparsity from a Waveformextractor
     with several methods:
 
     Using the N best channels (largest template amplitude):
 
     >>> sparsity = ChannelSparsity.from_best_channels(we, num_channels, peak_sign='neg')
 
@@ -70,46 +73,47 @@
 
     >>> sparsity = ChannelSparsity.from_radius(we, radius_um, peak_sign='neg')
 
     Using a SNR threshold:
     >>> sparsity = ChannelSparsity.from_snr(we, threshold, peak_sign='neg')
 
     Using a template energy threshold:
-    >>> sparsity = ChannelSparsity.from_energy(we, threshold, peak_sign='neg')
+    >>> sparsity = ChannelSparsity.from_energy(we, threshold)
 
     Using a recording/sorting property (e.g. 'group'):
-    
+
     >>> sparsity = ChannelSparsity.from_property(we, by_property="group")
 
     """
+
     def __init__(self, mask, unit_ids, channel_ids):
         self.unit_ids = np.asarray(unit_ids)
         self.channel_ids = np.asarray(channel_ids)
-        self.mask = np.asarray(mask, dtype='bool')
+        self.mask = np.asarray(mask, dtype="bool")
         assert self.mask.shape[0] == self.unit_ids.shape[0]
         assert self.mask.shape[1] == self.channel_ids.shape[0]
 
         # some precomputed dict
         self._unit_id_to_channel_ids = None
         self._unit_id_to_channel_indices = None
 
     def __repr__(self):
         ratio = np.mean(self.mask)
-        txt = f'ChannelSparsity - units: {self.unit_ids.size} - channels: {self.channel_ids.size} - ratio: {ratio:0.2f}'
+        txt = f"ChannelSparsity - units: {self.unit_ids.size} - channels: {self.channel_ids.size} - ratio: {ratio:0.2f}"
         return txt
 
     @property
     def unit_id_to_channel_ids(self):
         if self._unit_id_to_channel_ids is None:
             self._unit_id_to_channel_ids = {}
             for unit_ind, unit_id in enumerate(self.unit_ids):
                 channel_inds = np.flatnonzero(self.mask[unit_ind, :])
                 self._unit_id_to_channel_ids[unit_id] = self.channel_ids[channel_inds]
         return self._unit_id_to_channel_ids
-    
+
     @property
     def unit_id_to_channel_indices(self):
         if self._unit_id_to_channel_indices is None:
             self._unit_id_to_channel_indices = {}
             for unit_ind, unit_id in enumerate(self.unit_ids):
                 channel_inds = np.flatnonzero(self.mask[unit_ind, :])
                 self._unit_id_to_channel_indices[unit_id] = channel_inds
@@ -118,15 +122,15 @@
     @classmethod
     def from_unit_id_to_channel_ids(cls, unit_id_to_channel_ids, unit_ids, channel_ids):
         """
         Create a sparsity object from dict unit_id to channel_ids.
         """
         unit_ids = list(unit_ids)
         channel_ids = list(channel_ids)
-        mask = np.zeros((len(unit_ids), len(channel_ids)), dtype='bool')
+        mask = np.zeros((len(unit_ids), len(channel_ids)), dtype="bool")
         for unit_id, chan_ids in unit_id_to_channel_ids.items():
             unit_ind = unit_ids.index(unit_id)
             channel_inds = [channel_ids.index(chan_id) for chan_id in chan_ids]
             mask[unit_ind, channel_inds] = True
         return cls(mask, unit_ids, channel_ids)
 
     def to_dict(self):
@@ -138,82 +142,97 @@
             channel_ids=list(self.channel_ids),
             unit_ids=list(self.unit_ids),
         )
 
     @classmethod
     def from_dict(cls, d):
         unit_id_to_channel_ids_corrected = {}
-        for unit_id in d['unit_ids']:
-            if unit_id in d['unit_id_to_channel_ids']:
-                unit_id_to_channel_ids_corrected[unit_id] = d['unit_id_to_channel_ids'][unit_id]
+        for unit_id in d["unit_ids"]:
+            if unit_id in d["unit_id_to_channel_ids"]:
+                unit_id_to_channel_ids_corrected[unit_id] = d["unit_id_to_channel_ids"][unit_id]
             else:
-                unit_id_to_channel_ids_corrected[unit_id] = d['unit_id_to_channel_ids'][str(unit_id)]
-        d['unit_id_to_channel_ids'] = unit_id_to_channel_ids_corrected
+                unit_id_to_channel_ids_corrected[unit_id] = d["unit_id_to_channel_ids"][str(unit_id)]
+        d["unit_id_to_channel_ids"] = unit_id_to_channel_ids_corrected
 
         return cls.from_unit_id_to_channel_ids(**d)
 
     ## Some convinient function to compute sparsity from several strategy
     @classmethod
-    def from_best_channels(cls, we, num_channels, peak_sign='neg'):
+    def from_best_channels(cls, we, num_channels, peak_sign="neg"):
         """
         Construct sparsity from N best channels with the largest amplitude.
         Use the 'num_channels' argument to specify the number of channels.
         """
         from .template_tools import get_template_amplitudes
 
-        mask = np.zeros((we.unit_ids.size, we.channel_ids.size), dtype='bool')
+        mask = np.zeros((we.unit_ids.size, we.channel_ids.size), dtype="bool")
         peak_values = get_template_amplitudes(we, peak_sign=peak_sign)
         for unit_ind, unit_id in enumerate(we.unit_ids):
             chan_inds = np.argsort(np.abs(peak_values[unit_id]))[::-1]
             chan_inds = chan_inds[:num_channels]
             mask[unit_ind, chan_inds] = True
         return cls(mask, we.unit_ids, we.channel_ids)
 
     @classmethod
-    def from_radius(cls, we, radius_um, peak_sign='neg'):
+    def from_radius(cls, we, radius_um, peak_sign="neg"):
         """
         Construct sparsity from a radius around the best channel.
         Use the 'radius_um' argument to specify the radius in um
         """
         from .template_tools import get_template_extremum_channel
 
-        mask = np.zeros((we.unit_ids.size, we.channel_ids.size), dtype='bool')
+        mask = np.zeros((we.unit_ids.size, we.channel_ids.size), dtype="bool")
         locations = we.get_channel_locations()
         distances = np.linalg.norm(locations[:, np.newaxis] - locations[np.newaxis, :], axis=2)
         best_chan = get_template_extremum_channel(we, peak_sign=peak_sign, outputs="index")
         for unit_ind, unit_id in enumerate(we.unit_ids):
             chan_ind = best_chan[unit_id]
-            chan_inds, = np.nonzero(distances[chan_ind, :] <= radius_um)
+            (chan_inds,) = np.nonzero(distances[chan_ind, :] <= radius_um)
             mask[unit_ind, chan_inds] = True
         return cls(mask, we.unit_ids, we.channel_ids)
 
     @classmethod
-    def from_snr(cls, we, threshold, peak_sign='neg'):
+    def from_snr(cls, we, threshold, peak_sign="neg"):
         """
         Construct sparsity from a thresholds based on template signal-to-noise ratio.
         Use the 'threshold' argument to specify the SNR threshold.
         """
         from .template_tools import get_template_amplitudes
 
-        mask = np.zeros((we.unit_ids.size, we.channel_ids.size), dtype='bool')
+        mask = np.zeros((we.unit_ids.size, we.channel_ids.size), dtype="bool")
 
         peak_values = get_template_amplitudes(we, peak_sign=peak_sign, mode="extremum")
         noise = get_noise_levels(we.recording, return_scaled=we.return_scaled)
         for unit_ind, unit_id in enumerate(we.unit_ids):
             chan_inds = np.nonzero((np.abs(peak_values[unit_id]) / noise) >= threshold)
             mask[unit_ind, chan_inds] = True
         return cls(mask, we.unit_ids, we.channel_ids)
 
     @classmethod
+    def from_ptp(cls, we, threshold):
+        """
+        Construct sparsity from a thresholds based on template peak-to-peak values.
+        Use the 'threshold' argument to specify the SNR threshold.
+        """
+
+        mask = np.zeros((we.unit_ids.size, we.channel_ids.size), dtype="bool")
+        templates_ptps = np.ptp(we.get_all_templates(), axis=1)
+        noise = get_noise_levels(we.recording, return_scaled=we.return_scaled)
+        for unit_ind, unit_id in enumerate(we.unit_ids):
+            chan_inds = np.nonzero(templates_ptps[unit_ind] / noise >= threshold)
+            mask[unit_ind, chan_inds] = True
+        return cls(mask, we.unit_ids, we.channel_ids)
+
+    @classmethod
     def from_energy(cls, we, threshold):
         """
         Construct sparsity from a threshold based on per channel energy ratio.
         Use the 'threshold' argument to specify the SNR threshold.
         """
-        mask = np.zeros((we.unit_ids.size, we.channel_ids.size), dtype='bool')
+        mask = np.zeros((we.unit_ids.size, we.channel_ids.size), dtype="bool")
         noise = np.sqrt(we.nsamples) * get_noise_levels(we.recording, return_scaled=we.return_scaled)
         for unit_ind, unit_id in enumerate(we.unit_ids):
             wfs = we.get_waveforms(unit_id)
             energies = np.linalg.norm(wfs, axis=(0, 1))
             chan_inds = np.nonzero(energies / (noise * np.sqrt(len(wfs))) >= threshold)
             mask[unit_ind, chan_inds] = True
         return cls(mask, we.unit_ids, we.channel_ids)
@@ -224,69 +243,73 @@
         Construct sparsity witha property of the recording and sorting(e.g. 'group').
         Use the 'by_property' argument to specify the property name.
         """
         # check consistency
         assert by_property in we.recording.get_property_keys(), f"Property {by_property} is not a recording property"
         assert by_property in we.sorting.get_property_keys(), f"Property {by_property} is not a sorting property"
 
-        mask = np.zeros((we.unit_ids.size, we.channel_ids.size), dtype='bool')
+        mask = np.zeros((we.unit_ids.size, we.channel_ids.size), dtype="bool")
         rec_by = we.recording.split_by(by_property)
         for unit_ind, unit_id in enumerate(we.unit_ids):
             unit_property = we.sorting.get_property(by_property)[unit_ind]
-            assert unit_property in rec_by.keys(), (
-                   f"Unit property {unit_property} cannot be found in the recording properties")
+            assert (
+                unit_property in rec_by.keys()
+            ), f"Unit property {unit_property} cannot be found in the recording properties"
             chan_inds = we.recording.ids_to_indices(rec_by[unit_property].get_channel_ids())
             mask[unit_ind, chan_inds] = True
         return cls(mask, we.unit_ids, we.channel_ids)
 
     @classmethod
     def create_dense(cls, we):
         """
         Create a sparsity object with all selected channel for all units.
         """
-        mask = np.ones((we.unit_ids.size, we.channel_ids.size), dtype='bool')
+        mask = np.ones((we.unit_ids.size, we.channel_ids.size), dtype="bool")
         return cls(mask, we.unit_ids, we.channel_ids)
 
 
 def compute_sparsity(
     waveform_extractor,
     method="radius",
     peak_sign="neg",
     num_channels=5,
-    radius_um=100.,
+    radius_um=100.0,
     threshold=5,
     by_property=None,
 ):
     """
-    Get channel sparsity (subset of channels) for each template with several methods.
-
-    Parameters
-    ----------
-    waveform_extractor: WaveformExtractor
-        The waveform extractor
-
-{}
+        Get channel sparsity (subset of channels) for each template with several methods.
 
-    Returns
-    -------
-    sparsity: ChannelSparsity
-        The estimated sparsity
+        Parameters
+        ----------
+        waveform_extractor: WaveformExtractor
+            The waveform extractor
+
+    {}
+
+        Returns
+        -------
+        sparsity: ChannelSparsity
+            The estimated sparsity
     """
     if method == "best_channels":
         assert num_channels is not None, "For the 'best_channels' method, 'num_channels' needs to be given"
         sparsity = ChannelSparsity.from_best_channels(waveform_extractor, num_channels, peak_sign=peak_sign)
     elif method == "radius":
         assert radius_um is not None, "For the 'radius' method, 'radius_um' needs to be given"
         sparsity = ChannelSparsity.from_radius(waveform_extractor, radius_um, peak_sign=peak_sign)
     elif method == "snr":
         assert threshold is not None, "For the 'snr' method, 'threshold' needs to be given"
         sparsity = ChannelSparsity.from_snr(waveform_extractor, threshold, peak_sign=peak_sign)
     elif method == "energy":
         assert threshold is not None, "For the 'energy' method, 'threshold' needs to be given"
         sparsity = ChannelSparsity.from_energy(waveform_extractor, threshold)
+    elif method == "ptp":
+        assert threshold is not None, "For the 'ptp' method, 'threshold' needs to be given"
+        sparsity = ChannelSparsity.from_ptp(waveform_extractor, threshold)
     elif method == "by_property":
         assert by_property is not None, "For the 'by_property' method, 'by_property' needs to be given"
         sparsity = ChannelSparsity.from_property(waveform_extractor, by_property)
     else:
         raise ValueError(f"compute_sparsity() method={method} do not exists")
 
     return sparsity
```

### Comparing `spikeinterface-0.97.1/spikeinterface/core/template_tools.py` & `spikeinterface-0.98.0/src/spikeinterface/core/template_tools.py`

 * *Files 1% similar despite different names*

```diff
@@ -52,15 +52,17 @@
                 values = template[before, :]
 
         peak_values[unit_id] = values
 
     return peak_values
 
 
-def get_template_extremum_channel(waveform_extractor, peak_sign: str = "neg", mode: str = "extremum", outputs: str = "id"):
+def get_template_extremum_channel(
+    waveform_extractor, peak_sign: str = "neg", mode: str = "extremum", outputs: str = "id"
+):
     """
     Compute the channel with the extremum peak for each unit.
 
     Parameters
     ----------
     waveform_extractor: WaveformExtractor
         The waveform extractor
@@ -101,47 +103,55 @@
 
 
 def get_template_channel_sparsity(
     waveform_extractor,
     method="radius",
     peak_sign="neg",
     num_channels=5,
-    radius_um=100.,
+    radius_um=100.0,
     threshold=5,
     by_property=None,
     outputs="id",
 ):
     """
-    Get channel sparsity (subset of channels) for each template with several methods.
-
-    Parameters
-    ----------
-    waveform_extractor: WaveformExtractor
-        The waveform extractor
-{}
-    outputs: str
-        * 'id': channel id
-        * 'index': channel index
+        Get channel sparsity (subset of channels) for each template with several methods.
 
-    Returns
-    -------
-    sparsity: dict
-        Dictionary with unit ids as keys and sparse channel ids or indices (id or index based on 'outputs')
-        as values
+        Parameters
+        ----------
+        waveform_extractor: WaveformExtractor
+            The waveform extractor
+    {}
+        outputs: str
+            * 'id': channel id
+            * 'index': channel index
+
+        Returns
+        -------
+        sparsity: dict
+            Dictionary with unit ids as keys and sparse channel ids or indices (id or index based on 'outputs')
+            as values
     """
     from spikeinterface.core.sparsity import compute_sparsity
 
-    warnings.warn("The 'get_template_channel_sparsity()' function is deprecated. "
-                  "Use 'compute_sparsity()' instead",
-                  DeprecationWarning, stacklevel=2)
-
-    assert outputs in ('id', 'index'), "'outputs' can either be 'id' or 'index'"
-    sparsity = compute_sparsity(waveform_extractor, method=method, peak_sign=peak_sign,
-                                 num_channels=num_channels, radius_um=radius_um, threshold=threshold,
-                                 by_property=by_property)
+    warnings.warn(
+        "The 'get_template_channel_sparsity()' function is deprecated. " "Use 'compute_sparsity()' instead",
+        DeprecationWarning,
+        stacklevel=2,
+    )
+
+    assert outputs in ("id", "index"), "'outputs' can either be 'id' or 'index'"
+    sparsity = compute_sparsity(
+        waveform_extractor,
+        method=method,
+        peak_sign=peak_sign,
+        num_channels=num_channels,
+        radius_um=radius_um,
+        threshold=threshold,
+        by_property=by_property,
+    )
 
     # handle output ids or indexes
     if outputs == "id":
         return sparsity.unit_id_to_channel_ids
     elif outputs == "index":
         return sparsity.unit_id_to_channel_indices
 
@@ -215,24 +225,18 @@
     """
     assert peak_sign in ("both", "neg", "pos")
     assert mode in ("extremum", "at_index")
     unit_ids = waveform_extractor.sorting.unit_ids
 
     before = waveform_extractor.nbefore
 
-    extremum_channels_ids = get_template_extremum_channel(
-        waveform_extractor, peak_sign=peak_sign, mode=mode
-    )
+    extremum_channels_ids = get_template_extremum_channel(waveform_extractor, peak_sign=peak_sign, mode=mode)
 
-    extremum_amplitudes = get_template_amplitudes(
-        waveform_extractor, peak_sign=peak_sign, mode=mode
-    )
+    extremum_amplitudes = get_template_amplitudes(waveform_extractor, peak_sign=peak_sign, mode=mode)
 
     unit_amplitudes = {}
     for unit_id in unit_ids:
         channel_id = extremum_channels_ids[unit_id]
         best_channel = waveform_extractor.channel_ids_to_indices([channel_id])[0]
         unit_amplitudes[unit_id] = extremum_amplitudes[unit_id][best_channel]
 
     return unit_amplitudes
-
-
```

### Comparing `spikeinterface-0.97.1/spikeinterface/core/unitsaggregationsorting.py` & `spikeinterface-0.98.0/src/spikeinterface/core/unitsaggregationsorting.py`

 * *Files 6% similar despite different names*

```diff
@@ -19,31 +19,33 @@
         If given, unit ids are renamed as provided. If None, unit ids are sequential integers.
 
     Returns
     -------
     aggregate_sorting: UnitsAggregationSorting
         The aggregated sorting object
     """
+
     def __init__(self, sorting_list, renamed_unit_ids=None):
         unit_map = {}
 
         num_all_units = sum([sort.get_num_units() for sort in sorting_list])
         if renamed_unit_ids is not None:
-            assert len(np.unique(renamed_unit_ids)) == num_all_units, "'renamed_unit_ids' doesn't have the right size" \
-                                                                      "or has duplicates!"
+            assert len(np.unique(renamed_unit_ids)) == num_all_units, (
+                "'renamed_unit_ids' doesn't have the right size" "or has duplicates!"
+            )
             unit_ids = list(renamed_unit_ids)
         else:
             unit_ids = list(np.arange(num_all_units))
 
         # unit map maps unit ids that are used to get spike trains
         u_id = 0
         for s_i, sorting in enumerate(sorting_list):
             single_unit_ids = sorting.get_unit_ids()
             for unit_id in single_unit_ids:
-                unit_map[unit_ids[u_id]] = {'sorting_id': s_i, 'unit_id': unit_id}
+                unit_map[unit_ids[u_id]] = {"sorting_id": s_i, "unit_id": unit_id}
                 u_id += 1
 
         sampling_frequency = sorting_list[0].get_sampling_frequency()
         num_segments = sorting_list[0].get_num_segments()
 
         ok1 = all(sampling_frequency == sort.get_sampling_frequency() for sort in sorting_list)
         ok2 = all(num_segments == sort.get_num_segments() for sort in sorting_list)
@@ -82,15 +84,17 @@
             for sort in sorting_list:
                 if prop_name in sort.get_property_keys():
                     values = sort.get_property(prop_name)
                 else:
                     if dtype.kind not in BaseExtractor.default_missing_property_values:
                         del property_dict[prop_name]
                         break
-                    values = np.full(sort.get_num_units(), BaseExtractor.default_missing_property_values[dtype.kind], dtype=dtype)
+                    values = np.full(
+                        sort.get_num_units(), BaseExtractor.default_missing_property_values[dtype.kind], dtype=dtype
+                    )
 
                 try:
                     property_dict[prop_name] = np.concatenate((property_dict[prop_name], values))
                 except Exception as e:
                     print(f"Skipping property '{prop_name}' for shape inconsistency")
                     del property_dict[prop_name]
                     break
@@ -100,33 +104,33 @@
         # add segments
         for i_seg in range(num_segments):
             parent_segments = [sort._sorting_segments[i_seg] for sort in sorting_list]
             sub_segment = UnitsAggregationSortingSegment(unit_map, parent_segments)
             self.add_sorting_segment(sub_segment)
 
         self._sortings = sorting_list
-        self._kwargs = {'sorting_list': [sort.to_dict() for sort in sorting_list],
-                        'renamed_unit_ids': renamed_unit_ids}
+        self._kwargs = {"sorting_list": sorting_list, "renamed_unit_ids": renamed_unit_ids}
 
     @property
     def sortings(self):
         return self._sortings
 
 
 class UnitsAggregationSortingSegment(BaseSortingSegment):
     def __init__(self, unit_map, parent_segments):
         BaseSortingSegment.__init__(self)
         self._unit_map = unit_map
         self._parent_segments = parent_segments
 
-    def get_unit_spike_train(self,
-                             unit_id,
-                             start_frame: Union[int, None] = None,
-                             end_frame: Union[int, None] = None,
-                             ) -> np.ndarray:
-        sorting_id = self._unit_map[unit_id]['sorting_id']
-        unit_id_sorting = self._unit_map[unit_id]['unit_id']
+    def get_unit_spike_train(
+        self,
+        unit_id,
+        start_frame: Union[int, None] = None,
+        end_frame: Union[int, None] = None,
+    ) -> np.ndarray:
+        sorting_id = self._unit_map[unit_id]["sorting_id"]
+        unit_id_sorting = self._unit_map[unit_id]["unit_id"]
         times = self._parent_segments[sorting_id].get_unit_spike_train(unit_id_sorting, start_frame, end_frame)
         return times
 
 
 aggregate_units = define_function_from_class(UnitsAggregationSorting, "aggregate_units")
```

### Comparing `spikeinterface-0.97.1/spikeinterface/core/unitsselectionsorting.py` & `spikeinterface-0.98.0/src/spikeinterface/core/unitsselectionsorting.py`

 * *Files 3% similar despite different names*

```diff
@@ -23,41 +23,41 @@
         self._unit_ids = np.asarray(unit_ids)
         self._renamed_unit_ids = np.asarray(renamed_unit_ids)
 
         parents_unit_ids = parent_sorting.get_unit_ids()
         sampling_frequency = parent_sorting.get_sampling_frequency()
 
         # some checks
-        assert all(unit_id in parents_unit_ids for unit_id in self._unit_ids), 'unit ids are not all in parents'
-        assert len(self._unit_ids) == len(self._renamed_unit_ids), 'renamed channel_ids must be the same size'
+        assert all(unit_id in parents_unit_ids for unit_id in self._unit_ids), "unit ids are not all in parents"
+        assert len(self._unit_ids) == len(self._renamed_unit_ids), "renamed channel_ids must be the same size"
 
         ids_conversion = dict(zip(self._renamed_unit_ids, self._unit_ids))
 
         BaseSorting.__init__(self, sampling_frequency, self._renamed_unit_ids)
 
         for parent_segment in self._parent_sorting._sorting_segments:
             sub_segment = UnitsSelectionSortingSegment(parent_segment, ids_conversion)
             self.add_sorting_segment(sub_segment)
 
         parent_sorting.copy_metadata(self, only_main=False, ids=self._unit_ids)
 
         if parent_sorting.has_recording():
             self.register_recording(parent_sorting._recording)
 
-        self._kwargs = dict(parent_sorting=parent_sorting.to_dict(), unit_ids=unit_ids,
-                            renamed_unit_ids=renamed_unit_ids)
+        self._kwargs = dict(parent_sorting=parent_sorting, unit_ids=unit_ids, renamed_unit_ids=renamed_unit_ids)
 
 
 class UnitsSelectionSortingSegment(BaseSortingSegment):
     def __init__(self, parent_segment, ids_conversion):
         BaseSortingSegment.__init__(self)
         self._parent_segment = parent_segment
         self._ids_conversion = ids_conversion
 
-    def get_unit_spike_train(self,
-                             unit_id,
-                             start_frame: Union[int, None] = None,
-                             end_frame: Union[int, None] = None,
-                             ) -> np.ndarray:
+    def get_unit_spike_train(
+        self,
+        unit_id,
+        start_frame: Union[int, None] = None,
+        end_frame: Union[int, None] = None,
+    ) -> np.ndarray:
         unit_id_parent = self._ids_conversion[unit_id]
         times = self._parent_segment.get_unit_spike_train(unit_id_parent, start_frame, end_frame)
         return times
```

### Comparing `spikeinterface-0.97.1/spikeinterface/core/waveform_extractor.py` & `spikeinterface-0.98.0/src/spikeinterface/core/waveform_extractor.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,36 +1,41 @@
+import math
 import pickle
 from pathlib import Path
 import shutil
+from typing import Iterable, Literal, Optional
 import json
 
 import numpy as np
 from copy import deepcopy
 from warnings import warn
 
 import probeinterface
 
 from .base import load_extractor
+from .baserecording import BaseRecording
+from .basesorting import BaseSorting
 from .core_tools import check_json
 from .job_tools import _shared_job_kwargs_doc, split_job_kwargs, fix_job_kwargs
-from .recording_tools import check_probe_do_not_overlap
-from .waveform_tools import extract_waveforms_to_buffers, has_exceeding_spikes
+from .numpyextractors import NumpySorting
+from .recording_tools import check_probe_do_not_overlap, get_rec_attributes
 from .sparsity import ChannelSparsity, compute_sparsity, _sparsity_doc
+from .waveform_tools import extract_waveforms_to_buffers, has_exceeding_spikes
 
-_possible_template_modes = ('average', 'std', 'median')
+_possible_template_modes = ("average", "std", "median")
 
 
 class WaveformExtractor:
     """
     Class to extract waveform on paired Recording-Sorting objects.
     Waveforms are persistent on disk and cached in memory.
 
     Parameters
     ----------
-    recording: Recording
+    recording: Recording | None
         The recording object
     sorting: Sorting
         The sorting object
     folder: Path
         The folder where waveforms are cached
     rec_attributes: None or dict
         When recording is None then a minimal dict with some attributes
@@ -57,429 +62,427 @@
     >>> waveforms = we.get_waveforms(unit_id)
     >>> template = we.get_template(unit_id, mode='median')
 
     >>> # Load  from folder (in another session)
     >>> we = WaveformExtractor.load(folder)
 
     """
+
     extensions = []
-    def __init__(self, recording, sorting, folder=None, rec_attributes=None, allow_unfiltered=False,
-                 sparsity=None):
-        if recording is None:
-            # this is for the mode when recording is not accessible anymore
-            if rec_attributes is None:
-                raise ValueError('WaveformExtractor: if recording is None then rec_attributes must be provied')
-            # some check on minimal attributes (probegroup is not mandatory)
-            for k in ('channel_ids', 'sampling_frequency', 'num_channels'):
-                if k not in rec_attributes:
-                    raise ValueError(f'Missing key in rec_attributes {k}')
-            for k in ('num_samples', 'properties', 'is_filtered'):
-                if k not in rec_attributes:
-                    warn(f'Missing optional key in rec_attributes {k}: '
-                         f'some recordingless functions might not be available')
-            self._rec_attributes = rec_attributes
-        else:
-            assert recording.get_num_segments() == sorting.get_num_segments(), \
-                "The recording and sorting objects must have the same number of segments!"
-            np.testing.assert_almost_equal(recording.get_sampling_frequency(),
-                                           sorting.get_sampling_frequency(), decimal=2)
-            if not recording.is_filtered() and not allow_unfiltered:
-                raise Exception('The recording is not filtered, you must filter it using `bandpass_filter()`.'
-                                'If the recording is already filtered, you can also do '
-                                '`recording.annotate(is_filtered=True).\n'
-                                'If you trully want to extract unfiltered waveforms, use `allow_unfiltered=True`.')
-            self._rec_attributes = rec_attributes
 
-        self._recording = recording
+    def __init__(
+        self,
+        recording: Optional[BaseRecording],
+        sorting: BaseSorting,
+        folder=None,
+        rec_attributes=None,
+        allow_unfiltered: bool = False,
+        sparsity=None,
+    ) -> None:
         self.sorting = sorting
+        self._rec_attributes = None
+        self.set_recording(recording, rec_attributes, allow_unfiltered)
 
         # cache in memory
         self._waveforms = {}
         self._template_cache = {}
         self._params = {}
         self._loaded_extensions = dict()
         self.sparsity = sparsity
 
         self.folder = folder
         if self.folder is not None:
             self.folder = Path(self.folder)
             if self.folder.suffix == ".zarr":
                 import zarr
+
                 self.format = "zarr"
                 self._waveforms_root = zarr.open(self.folder, mode="r")
                 self._params = self._waveforms_root.attrs["params"]
             else:
                 self.format = "binary"
-                if (self.folder / 'params.json').is_file():
-                    with open(str(self.folder / 'params.json'), 'r') as f:
+                if (self.folder / "params.json").is_file():
+                    with open(str(self.folder / "params.json"), "r") as f:
                         self._params = json.load(f)
         else:
             # this is in case of in-memory
             self.format = "memory"
             self._memory_objects = None
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         clsname = self.__class__.__name__
         nseg = self.get_num_segments()
         nchan = self.get_num_channels()
         nunits = self.sorting.get_num_units()
-        txt = f'{clsname}: {nchan} channels - {nunits} units - {nseg} segments'
+        txt = f"{clsname}: {nchan} channels - {nunits} units - {nseg} segments"
         if len(self._params) > 0:
-            max_spikes_per_unit = self._params['max_spikes_per_unit']
-            txt = txt + f'\n  before:{self.nbefore} after:{self.nafter} n_per_units:{max_spikes_per_unit}'
+            max_spikes_per_unit = self._params["max_spikes_per_unit"]
+            txt = txt + f"\n  before:{self.nbefore} after:{self.nafter} n_per_units:{max_spikes_per_unit}"
         if self.is_sparse():
-            txt += ' - sparse'
+            txt += " - sparse"
         return txt
 
     @classmethod
-    def load(cls, folder, with_recording=True, sorting=None):
+    def load(cls, folder, with_recording: bool = True, sorting: Optional[BaseSorting] = None) -> "WaveformExtractor":
         folder = Path(folder)
         assert folder.is_dir(), "Waveform folder does not exists"
         if folder.suffix == ".zarr":
-            return WaveformExtractor.load_from_zarr(folder, with_recording=with_recording,
-                                                    sorting=sorting)
+            return WaveformExtractor.load_from_zarr(folder, with_recording=with_recording, sorting=sorting)
         else:
-            return WaveformExtractor.load_from_folder(folder, with_recording=with_recording,
-                                                      sorting=sorting)
+            return WaveformExtractor.load_from_folder(folder, with_recording=with_recording, sorting=sorting)
 
     @classmethod
-    def load_from_folder(cls, folder, with_recording=True, sorting=None):
+    def load_from_folder(
+        cls, folder, with_recording: bool = True, sorting: Optional[BaseSorting] = None
+    ) -> "WaveformExtractor":
         folder = Path(folder)
-        assert folder.is_dir(), f'This waveform folder does not exists {folder}'
+        assert folder.is_dir(), f"This waveform folder does not exists {folder}"
 
         if not with_recording:
             # load
             recording = None
-            rec_attributes_file = folder / 'recording_info' / 'recording_attributes.json'
+            rec_attributes_file = folder / "recording_info" / "recording_attributes.json"
             if not rec_attributes_file.exists():
-                raise ValueError('This WaveformExtractor folder was created with an older version of spikeinterface'
-                                 '\nYou cannot use the mode with_recording=False')
-            with open(rec_attributes_file, 'r') as f:
+                raise ValueError(
+                    "This WaveformExtractor folder was created with an older version of spikeinterface"
+                    "\nYou cannot use the mode with_recording=False"
+                )
+            with open(rec_attributes_file, "r") as f:
                 rec_attributes = json.load(f)
             # the probe is handle ouside the main json
-            probegroup_file = folder / 'recording_info' / 'probegroup.json'
+            probegroup_file = folder / "recording_info" / "probegroup.json"
             if probegroup_file.is_file():
-                rec_attributes['probegroup'] = probeinterface.read_probeinterface(probegroup_file)
+                rec_attributes["probegroup"] = probeinterface.read_probeinterface(probegroup_file)
             else:
-                rec_attributes['probegroup'] = None
+                rec_attributes["probegroup"] = None
         else:
             try:
-                recording = load_extractor(folder / 'recording.json',
-                                        base_folder=folder)
+                recording = load_extractor(folder / "recording.json", base_folder=folder)
                 rec_attributes = None
             except:
                 raise Exception("The recording could not be loaded. You can use the `with_recording=False` argument")
 
         if sorting is None:
-            sorting = load_extractor(folder / 'sorting.json',
-                                     base_folder=folder)
+            sorting = load_extractor(folder / "sorting.json", base_folder=folder)
 
         # the sparsity is the sparsity of the saved/cached waveforms arrays
-        sparsity_file = folder / 'sparsity.json'
+        sparsity_file = folder / "sparsity.json"
         if sparsity_file.is_file():
-            with open(sparsity_file, mode='r')as f:
+            with open(sparsity_file, mode="r") as f:
                 sparsity = ChannelSparsity.from_dict(json.load(f))
         else:
             sparsity = None
 
-        we = cls(recording, sorting, folder=folder, rec_attributes=rec_attributes, allow_unfiltered=True,
-                 sparsity=sparsity)
+        we = cls(
+            recording, sorting, folder=folder, rec_attributes=rec_attributes, allow_unfiltered=True, sparsity=sparsity
+        )
 
         for mode in _possible_template_modes:
             # load cached templates
-            template_file = folder / f'templates_{mode}.npy'
+            template_file = folder / f"templates_{mode}.npy"
             if template_file.is_file():
                 we._template_cache[mode] = np.load(template_file)
 
         return we
 
     @classmethod
-    def load_from_zarr(cls, folder, with_recording=True, sorting=None):
+    def load_from_zarr(
+        cls, folder, with_recording: bool = True, sorting: Optional[BaseSorting] = None
+    ) -> "WaveformExtractor":
         import zarr
+
         folder = Path(folder)
-        assert folder.is_dir(), f'This waveform folder does not exists {folder}'
+        assert folder.is_dir(), f"This waveform folder does not exists {folder}"
         assert folder.suffix == ".zarr"
 
         waveforms_root = zarr.open(folder, mode="r+")
 
         if not with_recording:
             # load
             recording = None
-            rec_attributes = waveforms_root.require_group('recording_info').attrs['recording_attributes']
+            rec_attributes = waveforms_root.require_group("recording_info").attrs["recording_attributes"]
             # the probe is handle ouside the main json
-            if "probegroup" in waveforms_root.require_group('recording_info').attrs:
-                probegroup_dict = waveforms_root.require_group('recording_info').attrs['probegroup']
-                rec_attributes['probegroup'] = probeinterface.Probe.from_dict(probegroup_dict)
+            if "probegroup" in waveforms_root.require_group("recording_info").attrs:
+                probegroup_dict = waveforms_root.require_group("recording_info").attrs["probegroup"]
+                rec_attributes["probegroup"] = probeinterface.Probe.from_dict(probegroup_dict)
             else:
-                rec_attributes['probegroup'] = None
+                rec_attributes["probegroup"] = None
         else:
             try:
-                recording_dict = waveforms_root.attrs['recording']
+                recording_dict = waveforms_root.attrs["recording"]
                 recording = load_extractor(recording_dict, base_folder=folder)
                 rec_attributes = None
             except:
                 raise Exception("The recording could not be loaded. You can use the `with_recording=False` argument")
 
         if sorting is None:
-            sorting_dict = waveforms_root.attrs['sorting']
+            sorting_dict = waveforms_root.attrs["sorting"]
             sorting = load_extractor(sorting_dict, base_folder=folder)
 
-        if 'sparsity' in waveforms_root.attrs:
-            sparsity = waveforms_root.attrs['sparsity']
+        if "sparsity" in waveforms_root.attrs:
+            sparsity = waveforms_root.attrs["sparsity"]
         else:
             sparsity = None
 
-        we = cls(recording, sorting, folder=folder, rec_attributes=rec_attributes, allow_unfiltered=True,
-                 sparsity=sparsity)
+        we = cls(
+            recording, sorting, folder=folder, rec_attributes=rec_attributes, allow_unfiltered=True, sparsity=sparsity
+        )
 
         for mode in _possible_template_modes:
             # load cached templates
-            if f'templates_{mode}' in waveforms_root.keys():
-                we._template_cache[mode] = waveforms_root[f'templates_{mode}']
+            if f"templates_{mode}" in waveforms_root.keys():
+                we._template_cache[mode] = waveforms_root[f"templates_{mode}"]
         return we
 
     @classmethod
-    def create(cls, recording, sorting, folder, mode="folder", remove_if_exists=False,
-               use_relative_path=False, allow_unfiltered=False, sparsity=None):
+    def create(
+        cls,
+        recording: BaseRecording,
+        sorting: BaseSorting,
+        folder,
+        mode: Literal["folder", "memory"] = "folder",
+        remove_if_exists: bool = False,
+        use_relative_path: bool = False,
+        allow_unfiltered: bool = False,
+        sparsity=None,
+    ) -> "WaveformExtractor":
         assert mode in ("folder", "memory")
         # create rec_attributes
-        properties_to_attrs = deepcopy(recording._properties)
-        if 'contact_vector' in properties_to_attrs:
-            del properties_to_attrs['contact_vector']
         if has_exceeding_spikes(recording, sorting):
             raise ValueError(
-                    "The sorting object has spikes exceeding the recording duration. You have to remove those spikes "
-                    "with the `spikeinterface.curation.remove_excess_spikes()` function"
-                )
-        rec_attributes = dict(
-            channel_ids=recording.channel_ids,
-            sampling_frequency=recording.get_sampling_frequency(),
-            num_channels=recording.get_num_channels(),
-            num_samples=[recording.get_num_samples(seg_index)
-                            for seg_index in range(recording.get_num_segments())],
-            is_filtered=recording.is_filtered(),
-            properties=properties_to_attrs
-        )
+                "The sorting object has spikes exceeding the recording duration. You have to remove those spikes "
+                "with the `spikeinterface.curation.remove_excess_spikes()` function"
+            )
+        rec_attributes = get_rec_attributes(recording)
         if mode == "folder":
             folder = Path(folder)
             if folder.is_dir():
                 if remove_if_exists:
                     shutil.rmtree(folder)
                 else:
-                    raise FileExistsError('Folder already exists')
+                    raise FileExistsError("Folder already exists")
             folder.mkdir(parents=True)
 
             if use_relative_path:
                 relative_to = folder
             else:
                 relative_to = None
 
-            if recording.is_dumpable:
-                recording.dump(folder / 'recording.json', relative_to=relative_to)
-            if sorting.is_dumpable:
-                sorting.dump(folder / 'sorting.json', relative_to=relative_to)
+            if recording.check_if_json_serializable():
+                recording.dump(folder / "recording.json", relative_to=relative_to)
+            if sorting.check_if_json_serializable():
+                sorting.dump(folder / "sorting.json", relative_to=relative_to)
             else:
-                warn("Sorting object is not dumpable, which might result in downstream errors for "
-                     "parallel processing. To make the sorting dumpable, use the `sorting.save()` function.")
-            
+                warn(
+                    "Sorting object is not dumpable, which might result in downstream errors for "
+                    "parallel processing. To make the sorting dumpable, use the `sorting.save()` function."
+                )
+
             # dump some attributes of the recording for the mode with_recording=False at next load
-            rec_attributes_file = folder / 'recording_info' / 'recording_attributes.json'
+            rec_attributes_file = folder / "recording_info" / "recording_attributes.json"
             rec_attributes_file.parent.mkdir()
-            rec_attributes_file.write_text(
-                json.dumps(check_json(rec_attributes), indent=4),
-                encoding='utf8'
-            )
+            rec_attributes_file.write_text(json.dumps(check_json(rec_attributes), indent=4), encoding="utf8")
             if recording.get_probegroup() is not None:
-                probegroup_file = folder / 'recording_info' / 'probegroup.json'
+                probegroup_file = folder / "recording_info" / "probegroup.json"
                 probeinterface.write_probeinterface(probegroup_file, recording.get_probegroup())
 
-            with open(rec_attributes_file, 'r') as f:
+            with open(rec_attributes_file, "r") as f:
                 rec_attributes = json.load(f)
-            
+
             if sparsity is not None:
-                with open(folder / 'sparsity.json', mode='w') as f:
+                with open(folder / "sparsity.json", mode="w") as f:
                     json.dump(check_json(sparsity.to_dict()), f)
 
-        return cls(recording, sorting, folder, allow_unfiltered=allow_unfiltered, sparsity=sparsity,
-                   rec_attributes=rec_attributes)
+        return cls(
+            recording,
+            sorting,
+            folder,
+            allow_unfiltered=allow_unfiltered,
+            sparsity=sparsity,
+            rec_attributes=rec_attributes,
+        )
 
-    def is_sparse(self):
+    def is_sparse(self) -> bool:
         return self.sparsity is not None
 
-    def has_waveforms(self):
+    def has_waveforms(self) -> bool:
         if self.folder is not None:
             if self.format == "binary":
                 return (self.folder / "waveforms").is_dir()
             elif self.format == "zarr":
                 import zarr
+
                 root = zarr.open(self.folder)
                 return "waveforms" in root.keys()
         else:
             return self._memory_objects is not None
 
-    def delete_waveforms(self):
+    def delete_waveforms(self) -> None:
         """
         Deletes waveforms folder.
         """
         assert self.has_waveforms(), "WaveformExtractor object doesn't have waveforms already!"
         if self.folder is not None:
             if self.format == "binary":
                 shutil.rmtree(self.folder / "waveforms")
             elif self.format == "zarr":
                 import zarr
+
                 root = zarr.open(self.folder)
                 del root["waveforms"]
         else:
             self._memory_objects = None
 
     @classmethod
-    def register_extension(cls, extension_class):
+    def register_extension(cls, extension_class) -> None:
         """
         This maintains a list of possible extensions that are available.
         It depends on the imported submodules (e.g. for postprocessing module).
 
         For instance:
         import spikeinterface as si
         si.WaveformExtractor.extensions == []
 
         from spikeinterface.postprocessing import WaveformPrincipalComponent
         si.WaveformExtractor.extensions == [WaveformPrincipalComponent, ...]
 
         """
         assert issubclass(extension_class, BaseWaveformExtractorExtension)
-        assert extension_class.extension_name is not None, 'extension_name must not be None'
-        assert all(extension_class.extension_name != ext.extension_name for ext in cls.extensions), \
-            'Extension name already exists'
+        assert extension_class.extension_name is not None, "extension_name must not be None"
+        assert all(
+            extension_class.extension_name != ext.extension_name for ext in cls.extensions
+        ), "Extension name already exists"
         cls.extensions.append(extension_class)
-    
+
     # map some method from recording and sorting
     @property
-    def recording(self):
+    def recording(self) -> BaseRecording:
         if not self.has_recording():
-            raise ValueError('WaveformExtractor is used in mode "with_recording=False" '
-                             'this operation needs the recording')
+            raise ValueError(
+                'WaveformExtractor is used in mode "with_recording=False" ' "this operation needs the recording"
+            )
         return self._recording
 
     @property
-    def channel_ids(self):
+    def channel_ids(self) -> np.ndarray:
         if self.has_recording():
             return self.recording.channel_ids
         else:
-            return np.array(self._rec_attributes['channel_ids'])
-    
+            return np.array(self._rec_attributes["channel_ids"])
+
     @property
-    def sampling_frequency(self):
+    def sampling_frequency(self) -> float:
         return self.sorting.get_sampling_frequency()
 
     @property
-    def unit_ids(self):
+    def unit_ids(self) -> np.ndarray:
         return self.sorting.unit_ids
 
     @property
-    def nbefore(self):
-        nbefore = int(self._params['ms_before'] * self.sampling_frequency / 1000.)
+    def nbefore(self) -> int:
+        nbefore = int(self._params["ms_before"] * self.sampling_frequency / 1000.0)
         return nbefore
 
     @property
-    def nafter(self):
-        nafter = int(self._params['ms_after'] * self.sampling_frequency / 1000.)
+    def nafter(self) -> int:
+        nafter = int(self._params["ms_after"] * self.sampling_frequency / 1000.0)
         return nafter
 
     @property
-    def nsamples(self):
+    def nsamples(self) -> int:
         return self.nbefore + self.nafter
 
     @property
-    def return_scaled(self):
-        return self._params['return_scaled']
-    
+    def return_scaled(self) -> bool:
+        return self._params["return_scaled"]
+
     @property
     def dtype(self):
-        return self._params['dtype']
+        return self._params["dtype"]
 
-    def has_recording(self):
+    def has_recording(self) -> bool:
         return self._recording is not None
 
-    def get_num_samples(self, segment_index=None):
+    def get_num_samples(self, segment_index: Optional[int] = None) -> int:
         if self.has_recording():
             return self.recording.get_num_samples(segment_index)
         else:
             assert "num_samples" in self._rec_attributes, "'num_samples' is not available"
             # we use self.sorting to check segment_index
             segment_index = self.sorting._check_segment_index(segment_index)
-            return self._rec_attributes['num_samples'][segment_index]
+            return self._rec_attributes["num_samples"][segment_index]
 
-    def get_total_samples(self):
+    def get_total_samples(self) -> int:
         s = 0
         for segment_index in range(self.get_num_segments()):
             s += self.get_num_samples(segment_index)
         return s
 
-    def get_total_duration(self):
+    def get_total_duration(self) -> float:
         duration = self.get_total_samples() / self.sampling_frequency
         return duration
 
-    def get_num_channels(self):
+    def get_num_channels(self) -> int:
         if self.has_recording():
             return self.recording.get_num_channels()
         else:
-            return self._rec_attributes['num_channels']
+            return self._rec_attributes["num_channels"]
 
-    def get_num_segments(self):
+    def get_num_segments(self) -> int:
         return self.sorting.get_num_segments()
 
     def get_probegroup(self):
         if self.has_recording():
             return self.recording.get_probegroup()
         else:
-            return self._rec_attributes['probegroup']
+            return self._rec_attributes["probegroup"]
 
-    def is_filtered(self):
+    def is_filtered(self) -> bool:
         if self.has_recording():
             return self.recording.is_filtered()
         else:
-            return self._rec_attributes['is_filtered']
-    
+            return self._rec_attributes["is_filtered"]
+
     def get_probe(self):
         probegroup = self.get_probegroup()
-        assert len(probegroup.probes) == 1, 'There are several probes. Use `get_probegroup()`'
+        assert len(probegroup.probes) == 1, "There are several probes. Use `get_probegroup()`"
         return probegroup.probes[0]
 
-    def get_channel_locations(self):
+    def get_channel_locations(self) -> np.ndarray:
         # important note : contrary to recording
         # this give all channel locations, so no kwargs like channel_ids and axes
         if self.has_recording():
             return self.recording.get_channel_locations()
         else:
             if self.get_probegroup() is not None:
                 all_probes = self.get_probegroup().probes
                 # check that multiple probes are non-overlapping
                 check_probe_do_not_overlap(all_probes)
                 all_positions = np.vstack([probe.contact_positions for probe in all_probes])
                 return all_positions
             else:
-                raise Exception('There are no channel locations')
+                raise Exception("There are no channel locations")
 
-    def channel_ids_to_indices(self, channel_ids):
+    def channel_ids_to_indices(self, channel_ids) -> np.ndarray:
         if self.has_recording():
             return self.recording.ids_to_indices(channel_ids)
         else:
-            all_channel_ids = self._rec_attributes['channel_ids']
+            all_channel_ids = self._rec_attributes["channel_ids"]
             indices = np.array([all_channel_ids.index(id) for id in channel_ids], dtype=int)
             return indices
 
-    def get_recording_property(self, key):
+    def get_recording_property(self, key) -> np.ndarray:
         if self.has_recording():
             return self.recording.get_property(key)
         else:
             assert "properties" in self._rec_attributes, "'properties' are not available"
-            values = np.array(self._rec_attributes['properties'].get(key, None))
+            values = np.array(self._rec_attributes["properties"].get(key, None))
             return values
 
-    def get_sorting_property(self, key):
+    def get_sorting_property(self, key) -> np.ndarray:
         return self.sorting.get_property(key)
 
     def get_extension_class(self, extension_name):
         """
         Get extension class from name and check if registered.
 
         Parameters
@@ -489,20 +492,19 @@
 
         Returns
         -------
         ext_class:
             The class of the extension.
         """
         extensions_dict = {ext.extension_name: ext for ext in self.extensions}
-        assert extension_name in extensions_dict, \
-            'Extension is not registered, please import related module before'
+        assert extension_name in extensions_dict, "Extension is not registered, please import related module before"
         ext_class = extensions_dict[extension_name]
         return ext_class
 
-    def is_extension(self, extension_name):
+    def is_extension(self, extension_name) -> bool:
         """
         Check if the extension exists in memory or in the folder.
 
         Parameters
         ----------
         extension_name: str
             The extension name.
@@ -512,44 +514,47 @@
         exists: bool
             Whether the extension exists or not
         """
         if self.folder is None:
             return extension_name in self._loaded_extensions
         else:
             if self.format == "binary":
-                return (self.folder / extension_name).is_dir() and \
-                    (self.folder / extension_name / 'params.json').is_file()
+                return (self.folder / extension_name).is_dir() and (
+                    self.folder / extension_name / "params.json"
+                ).is_file()
             elif self.format == "zarr":
-                return extension_name in self._waveforms_root.keys() and \
-                    "params" in self._waveforms_root[extension_name].attrs.keys()
+                return (
+                    extension_name in self._waveforms_root.keys()
+                    and "params" in self._waveforms_root[extension_name].attrs.keys()
+                )
 
     def load_extension(self, extension_name):
         """
         Load an extension from its name.
         The module of the extension must be loaded and registered.
 
         Parameters
         ----------
         extension_name: str
             The extension name.
 
         Returns
         -------
-        ext_instanace: 
+        ext_instanace:
             The loaded instance of the extension
         """
         if self.folder is not None and extension_name not in self._loaded_extensions:
             if self.is_extension(extension_name):
                 ext_class = self.get_extension_class(extension_name)
                 ext = ext_class.load(self.folder, self)
         if extension_name not in self._loaded_extensions:
-            raise Exception(f'Extension {extension_name} not available')
+            raise Exception(f"Extension {extension_name} not available")
         return self._loaded_extensions[extension_name]
 
-    def delete_extension(self, extension_name):
+    def delete_extension(self, extension_name) -> None:
         """
         Deletes an existing extension.
 
         Parameters
         ----------
         extension_name: str
             The extension name.
@@ -576,34 +581,107 @@
         """
         extension_names_in_folder = []
         for extension_class in self.extensions:
             if self.is_extension(extension_class.extension_name):
                 extension_names_in_folder.append(extension_class.extension_name)
         return extension_names_in_folder
 
-    def _reset(self):
+    def _reset(self) -> None:
         self._waveforms = {}
         self._template_cache = {}
         self._params = {}
 
         if self.folder is not None:
-            waveform_folder = self.folder / 'waveforms'
+            waveform_folder = self.folder / "waveforms"
             if waveform_folder.is_dir():
                 shutil.rmtree(waveform_folder)
             for mode in _possible_template_modes:
-                template_file = self.folder / f'templates_{mode}.npy'
+                template_file = self.folder / f"templates_{mode}.npy"
                 if template_file.is_file():
                     template_file.unlink()
 
             waveform_folder.mkdir()
         else:
             # remove shared objects
             self._memory_objects = None
 
-    def set_params(self, ms_before=1., ms_after=2., max_spikes_per_unit=500, return_scaled=False, dtype=None):
+    def set_recording(
+        self, recording: Optional[BaseRecording], rec_attributes: Optional[dict] = None, allow_unfiltered: bool = False
+    ) -> None:
+        """
+        Sets the recording object and attributes for the WaveformExtractor.
+
+        Parameters
+        ----------
+        recording: Recording | None
+            The recording object
+        rec_attributes: None or dict
+            When recording is None then a minimal dict with some attributes
+            is needed.
+        allow_unfiltered: bool
+            If true, will accept unfiltered recording.
+            False by default.
+        """
+
+        if recording is None:  # Recordless mode.
+            if rec_attributes is None:
+                raise ValueError("WaveformExtractor: if recording is None, then rec_attributes must be provided.")
+            for k in (
+                "channel_ids",
+                "sampling_frequency",
+                "num_channels",
+            ):  # Some check on minimal attributes (probegroup is not mandatory)
+                if k not in rec_attributes:
+                    raise ValueError(f"WaveformExtractor: Missing key '{k}' in rec_attributes")
+            for k in ("num_samples", "properties", "is_filtered"):
+                if k not in rec_attributes:
+                    warn(
+                        f"Missing optional key in rec_attributes {k}: "
+                        f"some recordingless functions might not be available"
+                    )
+        else:
+            if rec_attributes is None:
+                rec_attributes = get_rec_attributes(recording)
+
+            if recording.get_num_segments() != self.get_num_segments():
+                raise ValueError(
+                    f"Couldn't set the WaveformExtractor recording: num_segments do not match!\n{self.get_num_segments()} != {recording.get_num_segments()}"
+                )
+            if not math.isclose(recording.sampling_frequency, self.sampling_frequency, abs_tol=1e-2, rel_tol=1e-5):
+                raise ValueError(
+                    f"Couldn't set the WaveformExtractor recording: sampling frequency doesn't match!\n{self.sampling_frequency} != {recording.sampling_frequency}"
+                )
+            if self._rec_attributes is not None:
+                reference_channel_ids = self._rec_attributes["channel_ids"]
+            else:
+                reference_channel_ids = rec_attributes["channel_ids"]
+            if not np.array_equal(reference_channel_ids, recording.channel_ids):
+                raise ValueError(
+                    f"Couldn't set the WaveformExtractor recording: channel_ids do not match!\n{reference_channel_ids}"
+                )
+
+            if not recording.is_filtered() and not allow_unfiltered:
+                raise Exception(
+                    "The recording is not filtered, you must filter it using `bandpass_filter()`."
+                    "If the recording is already filtered, you can also do "
+                    "`recording.annotate(is_filtered=True).\n"
+                    "If you trully want to extract unfiltered waveforms, use `allow_unfiltered=True`."
+                )
+
+        self._recording = recording
+        self._rec_attributes = rec_attributes
+
+    def set_params(
+        self,
+        ms_before: float = 1.0,
+        ms_after: float = 2.0,
+        max_spikes_per_unit: int = 500,
+        return_scaled: bool = False,
+        dtype=None,
+    ) -> None:
         """
         Set parameters for waveform extraction
 
         Parameters
         ----------
         ms_before: float
             Cut out in ms before spike time
@@ -636,33 +714,33 @@
             max_spikes_per_unit = int(max_spikes_per_unit)
 
         self._params = dict(
             ms_before=float(ms_before),
             ms_after=float(ms_after),
             max_spikes_per_unit=max_spikes_per_unit,
             return_scaled=return_scaled,
-            dtype=dtype.str)
+            dtype=dtype.str,
+        )
 
         if self.folder is not None:
-            (self.folder / 'params.json').write_text(
-                json.dumps(check_json(self._params), indent=4), encoding='utf8')
-        
-    def select_units(self, unit_ids, new_folder=None, use_relative_path=False):
+            (self.folder / "params.json").write_text(json.dumps(check_json(self._params), indent=4), encoding="utf8")
+
+    def select_units(self, unit_ids, new_folder=None, use_relative_path: bool = False) -> "WaveformExtractor":
         """
         Filters units by creating a new waveform extractor object in a new folder.
-        
+
         Extensions are also updated to filter the selected unit ids.
 
         Parameters
         ----------
         unit_ids : list or array
             The unit ids to keep in the new WaveformExtractor object
         new_folder : Path or None
             The new folder where selected waveforms are copied
-            
+
         Returns
         -------
         we :  WaveformExtractor
             The newly create waveform extractor with the selected units
         """
         sorting = self.sorting.select_units(unit_ids)
         unit_indices = self.sorting.ids_to_indices(unit_ids)
@@ -670,36 +748,34 @@
         if self.folder is not None and new_folder is not None:
             if self.format == "binary":
                 new_folder = Path(new_folder)
                 assert not new_folder.is_dir(), f"{new_folder} already exists!"
                 new_folder.mkdir(parents=True)
 
                 # create new waveform extractor folder
-                shutil.copyfile(self.folder / "params.json",
-                                new_folder / "params.json")
+                shutil.copyfile(self.folder / "params.json", new_folder / "params.json")
 
                 if use_relative_path:
                     relative_to = new_folder
                 else:
                     relative_to = None
 
                 if self.has_recording():
-                    self.recording.dump(new_folder / 'recording.json', relative_to=relative_to)
-                sorting.dump(new_folder / 'sorting.json', relative_to=relative_to)
+                    self.recording.dump(new_folder / "recording.json", relative_to=relative_to)
+                sorting.dump(new_folder / "sorting.json", relative_to=relative_to)
 
                 # create and populate waveforms folder
                 new_waveforms_folder = new_folder / "waveforms"
                 new_waveforms_folder.mkdir()
-            
+
                 waveforms_files = [f for f in (self.folder / "waveforms").iterdir() if f.suffix == ".npy"]
                 for unit in sorting.get_unit_ids():
                     for wf_file in waveforms_files:
-                        if f"waveforms_{unit}.npy" in wf_file.name or f'sampled_index_{unit}.npy' in wf_file.name:
-                            shutil.copyfile(
-                                wf_file, new_waveforms_folder / wf_file.name)
+                        if f"waveforms_{unit}.npy" in wf_file.name or f"sampled_index_{unit}.npy" in wf_file.name:
+                            shutil.copyfile(wf_file, new_waveforms_folder / wf_file.name)
 
                 template_files = [f for f in self.folder.iterdir() if "template" in f.name and f.suffix == ".npy"]
                 for tmp_file in template_files:
                     templates_data_sliced = np.load(tmp_file)[unit_indices]
                     np.save(new_waveforms_folder / tmp_file.name, templates_data_sliced)
 
                 # slice masks
@@ -707,27 +783,28 @@
                     mask = self.sparsity.mask[unit_indices]
                     new_sparsity = ChannelSparsity(mask, unit_ids, self.channel_ids)
                     with (new_folder / "sparsity.json").open("w") as f:
                         json.dump(check_json(new_sparsity.to_dict()), f)
 
                 we = WaveformExtractor.load(new_folder)
             elif self.format == "zarr":
-                raise NotImplementedError("For zarr format, `select_units()` to a folder is not supported yet. "
-                                          "You can select units in two steps:\n"
-                                          "1. `we_new = select_units(unit_ids, new_folder=None)`\n"
-                                          "2. `we_new.save(folder='new_folder', format='zarr')`")
+                raise NotImplementedError(
+                    "For zarr format, `select_units()` to a folder is not supported yet. "
+                    "You can select units in two steps:\n"
+                    "1. `we_new = select_units(unit_ids, new_folder=None)`\n"
+                    "2. `we_new.save(folder='new_folder', format='zarr')`"
+                )
         else:
             sorting = self.sorting.select_units(unit_ids)
             if self.is_sparse():
                 mask = self.sparsity.mask[unit_indices]
                 sparsity = ChannelSparsity(mask, unit_ids, self.channel_ids)
             else:
                 sparsity = None
-            we = WaveformExtractor.create(self.recording, sorting, folder=None, mode="memory",
-                                          sparsity=sparsity)
+            we = WaveformExtractor.create(self.recording, sorting, folder=None, mode="memory", sparsity=sparsity)
             we.set_params(**self._params)
             # copy memory objects
             if self.has_waveforms():
                 we._memory_objects = {"wfs_arrays": {}, "sampled_indices": {}}
                 for unit_id in unit_ids:
                     we._memory_objects["wfs_arrays"][unit_id] = self._memory_objects["wfs_arrays"][unit_id]
                     we._memory_objects["sampled_indices"][unit_id] = self._memory_objects["sampled_indices"][unit_id]
@@ -735,30 +812,31 @@
         # finally select extensions data
         for ext_name in self.get_available_extension_names():
             ext = self.load_extension(ext_name)
             ext.select_units(unit_ids, new_waveform_extractor=we)
 
         return we
 
-    def save(self, folder, format="binary", use_relative_path=False, 
-             overwrite=False, sparsity=None, **kwargs):
+    def save(
+        self, folder, format="binary", use_relative_path: bool = False, overwrite: bool = False, sparsity=None, **kwargs
+    ) -> "WaveformExtractor":
         """
         Save WaveformExtractor object to disk.
 
         Parameters
         ----------
         folder : str or Path
             The output waveform folder
         format : str, optional
             "binary", "zarr", by default "binary"
         overwrite : bool
             If True and folder exists, it is deleted, by default False
         use_relative_path : bool, optional
-            If True, the recording and sorting paths are relative to the waveforms folder. 
-            This allows portability of the waveform folder provided that the relative paths are the same, 
+            If True, the recording and sorting paths are relative to the waveforms folder.
+            This allows portability of the waveform folder provided that the relative paths are the same,
             but forces all the data files to be in the same drive, by default False
         sparsity : ChannelSparsity, optional
             If given and WaveformExtractor is not sparse, it makes the returned WaveformExtractor sparse
         """
         folder = Path(folder)
         if use_relative_path:
             relative_to = folder
@@ -783,145 +861,157 @@
 
         if format == "binary":
             if folder.is_dir() and overwrite:
                 shutil.rmtree(folder)
             assert not folder.is_dir(), "Folder already exists. Use 'overwrite=True'"
             folder.mkdir(parents=True)
             # write metadata
-            (folder / 'params.json').write_text(
-                json.dumps(check_json(self._params), indent=4), encoding='utf8')
+            (folder / "params.json").write_text(json.dumps(check_json(self._params), indent=4), encoding="utf8")
 
             if self.has_recording():
-                if self.recording.is_dumpable:
-                    self.recording.dump(folder / 'recording.json', relative_to=relative_to)
-            if self.sorting.is_dumpable:
-                self.sorting.dump(folder / 'sorting.json', relative_to=relative_to)
+                if self.recording.check_if_json_serializable():
+                    self.recording.dump(folder / "recording.json", relative_to=relative_to)
+            if self.sorting.check_if_json_serializable():
+                self.sorting.dump(folder / "sorting.json", relative_to=relative_to)
             else:
-                warn("Sorting object is not dumpable, which might result in downstream errors for "
-                     "parallel processing. To make the sorting dumpable, use the `sorting.save()` function.")
+                warn(
+                    "Sorting object is not dumpable, which might result in downstream errors for "
+                    "parallel processing. To make the sorting dumpable, use the `sorting.save()` function."
+                )
 
             # dump some attributes of the recording for the mode with_recording=False at next load
-            rec_attributes_file = folder / 'recording_info' / 'recording_attributes.json'
+            rec_attributes_file = folder / "recording_info" / "recording_attributes.json"
             rec_attributes_file.parent.mkdir()
-            rec_attributes_file.write_text(
-                json.dumps(check_json(rec_attributes), indent=4),
-                encoding='utf8'
-            )
+            rec_attributes_file.write_text(json.dumps(check_json(rec_attributes), indent=4), encoding="utf8")
             if probegroup is not None:
-                probegroup_file = folder / 'recording_info' / 'probegroup.json'
-                probeinterface.write_probeinterface(probegroup_file, 
-                                                    probegroup)
-            with open(rec_attributes_file, 'r') as f:
+                probegroup_file = folder / "recording_info" / "probegroup.json"
+                probeinterface.write_probeinterface(probegroup_file, probegroup)
+            with open(rec_attributes_file, "r") as f:
                 rec_attributes = json.load(f)
             for mode, templates in self._template_cache.items():
                 templates_save = templates.copy()
                 if sparsity is not None:
                     expanded_mask = np.tile(sparsity.mask[:, np.newaxis, :], (1, templates_save.shape[1], 1))
                     templates_save[~expanded_mask] = 0
-                template_file = folder / f'templates_{mode}.npy'
+                template_file = folder / f"templates_{mode}.npy"
                 np.save(template_file, templates_save)
             if sparsity is not None:
                 with (folder / "sparsity.json").open("w") as f:
                     json.dump(check_json(sparsity.to_dict()), f)
             # now waveforms and templates
             if self.has_waveforms():
                 waveform_folder = folder / "waveforms"
                 waveform_folder.mkdir()
                 for unit_ind, unit_id in enumerate(self.unit_ids):
                     waveforms, sampled_indices = self.get_waveforms(unit_id, with_index=True)
                     if sparsity is not None:
                         waveforms = waveforms[:, :, sparsity.mask[unit_ind]]
-                    np.save(waveform_folder / f'waveforms_{unit_id}.npy', waveforms)
-                    np.save(waveform_folder / f'sampled_index_{unit_id}.npy', sampled_indices)
+                    np.save(waveform_folder / f"waveforms_{unit_id}.npy", waveforms)
+                    np.save(waveform_folder / f"sampled_index_{unit_id}.npy", sampled_indices)
         elif format == "zarr":
             import zarr
             from .zarrrecordingextractor import get_default_zarr_compressor
 
             if folder.suffix != ".zarr":
                 folder = folder.parent / f"{folder.stem}.zarr"
             if folder.is_dir() and overwrite:
                 shutil.rmtree(folder)
             assert not folder.is_dir(), "Folder already exists. Use 'overwrite=True'"
             zarr_root = zarr.open(str(folder), mode="w")
             # write metadata
-            zarr_root.attrs['params'] = check_json(self._params)
+            zarr_root.attrs["params"] = check_json(self._params)
             if self.has_recording():
-                if self.recording.is_dumpable:
+                if self.recording.check_if_json_serializable():
                     rec_dict = self.recording.to_dict(relative_to=relative_to)
-                    zarr_root.attrs['recording'] = check_json(rec_dict)
-            if self.sorting.is_dumpable:
+                    zarr_root.attrs["recording"] = check_json(rec_dict)
+            if self.sorting.check_if_json_serializable():
                 sort_dict = self.sorting.to_dict(relative_to=relative_to)
-                zarr_root.attrs['sorting'] = check_json(sort_dict)
+                zarr_root.attrs["sorting"] = check_json(sort_dict)
             else:
-                warn("Sorting object is not dumpable, which might result in downstream errors for "
-                     "parallel processing. To make the sorting dumpable, use the `sorting.save()` function.")
-            recording_info = zarr_root.create_group('recording_info')
-            recording_info.attrs['recording_attributes'] = check_json(rec_attributes)
+                warn(
+                    "Sorting object is not dumpable, which might result in downstream errors for "
+                    "parallel processing. To make the sorting dumpable, use the `sorting.save()` function."
+                )
+            recording_info = zarr_root.create_group("recording_info")
+            recording_info.attrs["recording_attributes"] = check_json(rec_attributes)
             if probegroup is not None:
-                recording_info.attrs['probegroup'] = check_json(probegroup.to_dict())
+                recording_info.attrs["probegroup"] = check_json(probegroup.to_dict())
             # save waveforms and templates
             compressor = kwargs.get("compressor", None)
             if compressor is None:
                 compressor = get_default_zarr_compressor()
-                print(f"Using default zarr compressor: {compressor}. To use a different compressor, use the "
-                      f"'compressor' argument")
+                print(
+                    f"Using default zarr compressor: {compressor}. To use a different compressor, use the "
+                    f"'compressor' argument"
+                )
             for mode, templates in self._template_cache.items():
                 templates_save = templates.copy()
                 if sparsity is not None:
                     expanded_mask = np.tile(sparsity.mask[:, np.newaxis, :], (1, templates_save.shape[1], 1))
                     templates_save[~expanded_mask] = 0
-                zarr_root.create_dataset(name=f'templates_{mode}', data=templates_save,
-                                         compressor=compressor)
+                zarr_root.create_dataset(name=f"templates_{mode}", data=templates_save, compressor=compressor)
             if sparsity is not None:
                 zarr_root.attrs["sparsity"] = check_json(sparsity.to_dict())
             if self.has_waveforms():
                 waveform_group = zarr_root.create_group("waveforms")
                 for unit_ind, unit_id in enumerate(self.unit_ids):
                     waveforms, sampled_indices = self.get_waveforms(unit_id, with_index=True)
                     if sparsity is not None:
                         waveforms = waveforms[:, :, sparsity.mask[unit_ind]]
-                    waveform_group.create_dataset(name=f'waveforms_{unit_id}',data=waveforms,
-                                                  compressor=compressor)
-                    waveform_group.create_dataset(name=f'sampled_index_{unit_id}', data=sampled_indices,
-                                                  compressor=compressor)
-        
+                    waveform_group.create_dataset(name=f"waveforms_{unit_id}", data=waveforms, compressor=compressor)
+                    waveform_group.create_dataset(
+                        name=f"sampled_index_{unit_id}", data=sampled_indices, compressor=compressor
+                    )
+
         new_we = WaveformExtractor.load(folder)
-        
+
         # save waveform extensions
         for ext_name in self.get_available_extension_names():
             ext = self.load_extension(ext_name)
             if sparsity is None:
                 ext.copy(new_we)
             else:
                 if ext.handle_sparsity:
-                    print(f"WaveformExtractor.save() : {ext.extension_name} cannot be propagated with sparsity"
-                          f"It is recommended to recompute {ext.extension_name} to properly handle sparsity")
+                    print(
+                        f"WaveformExtractor.save() : {ext.extension_name} cannot be propagated with sparsity"
+                        f"It is recommended to recompute {ext.extension_name} to properly handle sparsity"
+                    )
                 else:
                     ext.copy(new_we)
 
         return new_we
 
-    def get_waveforms(self, unit_id, with_index=False, cache=False, lazy=True, sparsity=None):
+    def get_waveforms(
+        self,
+        unit_id,
+        with_index: bool = False,
+        cache: bool = False,
+        lazy: bool = True,
+        sparsity=None,
+        force_dense: bool = False,
+    ):
         """
         Return waveforms for the specified unit id.
 
         Parameters
         ----------
         unit_id: int or str
             Unit id to retrieve waveforms for
         with_index: bool
             If True, spike indices of extracted waveforms are returned (default False)
         cache: bool
             If True, waveforms are cached to the self._waveforms dictionary (default False)
         lazy: bool
-            If True, waveforms are loaded as memmap objects (when format="binary") or Zarr datasets 
+            If True, waveforms are loaded as memmap objects (when format="binary") or Zarr datasets
             (when format="zarr").
             If False, waveforms are loaded as np.array objects (default True)
         sparsity: ChannelSparsity, optional
             Sparsity to apply to the waveforms (if WaveformExtractor is not sparse)
+        force_dense: bool (False)
+            Return dense waveforms even if the waveform extractor is sparse
 
         Returns
         -------
         wfs: np.array
             The returned waveform (num_spikes, num_samples, num_channels)
         indices: np.array
             If 'with_index' is True, the spike indices corresponding to the waveforms extracted
@@ -929,40 +1019,55 @@
         assert unit_id in self.sorting.unit_ids, "'unit_id' is invalid"
         assert self.has_waveforms(), "Waveforms have been deleted!"
 
         wfs = self._waveforms.get(unit_id, None)
         if wfs is None:
             if self.folder is not None:
                 if self.format == "binary":
-                    waveform_file = self.folder / 'waveforms' / f'waveforms_{unit_id}.npy'
+                    waveform_file = self.folder / "waveforms" / f"waveforms_{unit_id}.npy"
                     if not waveform_file.is_file():
-                        raise Exception('Waveforms not extracted yet: '
-                                        'please do WaveformExtractor.run_extract_waveforms() first')
+                        raise Exception(
+                            "Waveforms not extracted yet: " "please do WaveformExtractor.run_extract_waveforms() first"
+                        )
                     if lazy:
                         wfs = np.load(str(waveform_file), mmap_mode="r")
                     else:
                         wfs = np.load(waveform_file)
                 elif self.format == "zarr":
                     waveforms_group = self._waveforms_root["waveforms"]
-                    if f'waveforms_{unit_id}' not in waveforms_group.keys():
-                        raise Exception('Waveforms not extracted yet: '
-                                        'please do WaveformExtractor.run_extract_waveforms() first')
+                    if f"waveforms_{unit_id}" not in waveforms_group.keys():
+                        raise Exception(
+                            "Waveforms not extracted yet: " "please do WaveformExtractor.run_extract_waveforms() first"
+                        )
                     if lazy:
-                        wfs = waveforms_group[f'waveforms_{unit_id}']
+                        wfs = waveforms_group[f"waveforms_{unit_id}"]
                     else:
-                        wfs = waveforms_group[f'waveforms_{unit_id}'][:]
+                        wfs = waveforms_group[f"waveforms_{unit_id}"][:]
                 if cache:
                     self._waveforms[unit_id] = wfs
             else:
                 wfs = self._memory_objects["wfs_arrays"][unit_id]
 
         if sparsity is not None:
             assert not self.is_sparse(), "Waveforms are alreayd sparse! Cannot apply an additional sparsity."
             wfs = wfs[:, :, sparsity.mask[self.sorting.id_to_index(unit_id)]]
 
+        if force_dense:
+            num_channels = self.get_num_channels()
+            dense_wfs = np.zeros((wfs.shape[0], wfs.shape[1], num_channels), dtype=np.float32)
+            unit_ind = self.sorting.id_to_index(unit_id)
+            if sparsity is not None:
+                unit_sparsity = sparsity.mask[unit_ind]
+                dense_wfs[:, :, unit_sparsity] = wfs
+                wfs = dense_wfs
+            elif self.is_sparse():
+                unit_sparsity = self.sparsity.mask[unit_ind]
+                dense_wfs[:, :, unit_sparsity] = wfs
+                wfs = dense_wfs
+
         if with_index:
             sampled_index = self.get_sampled_indices(unit_id)
             return wfs, sampled_index
         else:
             return wfs
 
     def get_sampled_indices(self, unit_id):
@@ -978,27 +1083,28 @@
         -------
         sampled_indices: np.array
             The sampled indices
         """
         assert self.has_waveforms(), "Sample indices and waveforms have been deleted!"
         if self.folder is not None:
             if self.format == "binary":
-                sampled_index_file = self.folder / 'waveforms' / f'sampled_index_{unit_id}.npy'
+                sampled_index_file = self.folder / "waveforms" / f"sampled_index_{unit_id}.npy"
                 sampled_index = np.load(sampled_index_file)
             elif self.format == "zarr":
                 waveforms_group = self._waveforms_root["waveforms"]
-                if f'sampled_index_{unit_id}' not in waveforms_group.keys():
-                    raise Exception('Waveforms not extracted yet: '
-                                    'please do WaveformExtractor.run_extract_waveforms() first')
-                sampled_index = waveforms_group[f'sampled_index_{unit_id}'][:]
+                if f"sampled_index_{unit_id}" not in waveforms_group.keys():
+                    raise Exception(
+                        "Waveforms not extracted yet: " "please do WaveformExtractor.run_extract_waveforms() first"
+                    )
+                sampled_index = waveforms_group[f"sampled_index_{unit_id}"][:]
         else:
             sampled_index = self._memory_objects["sampled_indices"][unit_id]
         return sampled_index
 
-    def get_waveforms_segment(self, segment_index, unit_id, sparsity):
+    def get_waveforms_segment(self, segment_index: int, unit_id, sparsity):
         """
         Return waveforms from a specified segment and unit_id.
 
         Parameters
         ----------
         segment_index: int
             The segment index to retrieve waveforms from
@@ -1009,18 +1115,18 @@
 
         Returns
         -------
         wfs: np.array
             The returned waveform (num_spikes, num_samples, num_channels)
         """
         wfs, index_ar = self.get_waveforms(unit_id, with_index=True, sparsity=sparsity)
-        mask = index_ar['segment_index'] == segment_index
+        mask = index_ar["segment_index"] == segment_index
         return wfs[mask, :, :]
 
-    def precompute_templates(self, modes=('average', 'std')):
+    def precompute_templates(self, modes=("average", "std")) -> None:
         """
         Precompute all template for different "modes":
           * average
           * std
           * median
 
         The results is cache in memory as 3d ndarray (nunits, nsamples, nchans)
@@ -1028,44 +1134,44 @@
         """
         # TODO : run this in parralel
 
         unit_ids = self.unit_ids
         num_chans = self.get_num_channels()
 
         for mode in modes:
-            dtype = self._params['dtype'] if mode == 'median' else np.float32
+            dtype = self._params["dtype"] if mode == "median" else np.float32
             templates = np.zeros((len(unit_ids), self.nsamples, num_chans), dtype=dtype)
             self._template_cache[mode] = templates
 
         for unit_ind, unit_id in enumerate(unit_ids):
             wfs = self.get_waveforms(unit_id, cache=False)
             if self.sparsity is not None:
                 mask = self.sparsity.mask[unit_ind]
             else:
                 mask = slice(None)
             for mode in modes:
                 if len(wfs) == 0:
                     arr = np.zeros(wfs.shape[1:], dtype=wfs.dtype)
-                elif mode == 'median':
+                elif mode == "median":
                     arr = np.median(wfs, axis=0)
-                elif mode == 'average':
+                elif mode == "average":
                     arr = np.average(wfs, axis=0)
-                elif mode == 'std':
+                elif mode == "std":
                     arr = np.std(wfs, axis=0)
                 else:
-                    raise ValueError('mode must in median/average/std')
+                    raise ValueError("mode must in median/average/std")
                 self._template_cache[mode][unit_ind][:, mask] = arr
 
         for mode in modes:
             templates = self._template_cache[mode]
             if self.folder is not None:
-                template_file = self.folder / f'templates_{mode}.npy'
+                template_file = self.folder / f"templates_{mode}.npy"
                 np.save(template_file, templates)
 
-    def get_all_templates(self, unit_ids=None, mode='average'):
+    def get_all_templates(self, unit_ids: Optional[Iterable] = None, mode="average"):
         """
         Return  templates (average waveform) for multiple units.
 
         Parameters
         ----------
         unit_ids: list or None
             Unit ids to retrieve waveforms for
@@ -1084,66 +1190,71 @@
 
         if unit_ids is not None:
             unit_indices = self.sorting.ids_to_indices(unit_ids)
             templates = templates[unit_indices, :, :]
 
         return np.array(templates)
 
-    def get_template(self, unit_id, mode='average', sparsity=None):
+    def get_template(self, unit_id, mode="average", sparsity=None, force_dense: bool = False):
         """
         Return template (average waveform).
 
         Parameters
         ----------
         unit_id: int or str
             Unit id to retrieve waveforms for
         mode: str
             'average' (default), 'median' , 'std'(standard deviation)
         sparsity: ChannelSparsity, optional
             Sparsity to apply to the waveforms (if WaveformExtractor is not sparse)
+        force_dense: bool (False)
+            Return a dense template even if the waveform extractor is sparse
 
         Returns
         -------
         template: np.array
             The returned template (num_samples, num_channels)
         """
         assert mode in _possible_template_modes
         assert unit_id in self.sorting.unit_ids
 
         if sparsity is not None:
-            assert not self.is_sparse(), "Waveforms are alreayd sparse! Cannot apply an additional sparsity."
+            assert not self.is_sparse(), "Waveforms are already sparse! Cannot apply an additional sparsity."
 
         unit_ind = self.sorting.id_to_index(unit_id)
-        
+
         if mode in self._template_cache:
             # already in the global cache
             templates = self._template_cache[mode]
             template = templates[unit_ind, :, :]
             if sparsity is not None:
                 unit_sparsity = sparsity.mask[unit_ind]
             elif self.sparsity is not None:
                 unit_sparsity = self.sparsity.mask[unit_ind]
             else:
                 unit_sparsity = slice(None)
-            template = template[:, unit_sparsity]
+            if not force_dense:
+                template = template[:, unit_sparsity]
             return template
 
         # compute from waveforms
-        wfs = self.get_waveforms(unit_id)
-        if sparsity is not None:
+        wfs = self.get_waveforms(unit_id, force_dense=force_dense)
+        if sparsity is not None and not force_dense:
             wfs = wfs[:, :, sparsity.mask[unit_ind]]
-        if mode == 'median':
+
+        if mode == "median":
             template = np.median(wfs, axis=0)
-        elif mode == 'average':
+        elif mode == "average":
             template = np.average(wfs, axis=0)
-        elif mode == 'std':
+        elif mode == "std":
             template = np.std(wfs, axis=0)
+
         return np.array(template)
 
-    def get_template_segment(self, unit_id, segment_index, mode='average', sparsity=None):
+    def get_template_segment(self, unit_id, segment_index, mode="average", sparsity=None):
         """
         Return template for the specified unit id computed from waveforms of a specific segment.
 
         Parameters
         ----------
         unit_id: int or str
             Unit id to retrieve waveforms for
@@ -1156,110 +1267,107 @@
 
         Returns
         -------
         template: np.array
             The returned template (num_samples, num_channels)
 
         """
-        assert mode in ('median', 'average', 'std', )
+        assert mode in (
+            "median",
+            "average",
+            "std",
+        )
         assert unit_id in self.sorting.unit_ids
         waveforms_segment = self.get_waveforms_segment(segment_index, unit_id, sparsity=sparsity)
-        if mode == 'median':
+        if mode == "median":
             return np.median(waveforms_segment, axis=0)
-        elif mode == 'average':
+        elif mode == "average":
             return np.mean(waveforms_segment, axis=0)
-        elif mode == 'std':
+        elif mode == "std":
             return np.std(waveforms_segment, axis=0)
 
     def sample_spikes(self, seed=None):
         nbefore = self.nbefore
         nafter = self.nafter
 
-        selected_spikes = select_random_spikes_uniformly(self.recording, self.sorting,
-                                                         self._params['max_spikes_per_unit'], 
-                                                         nbefore, nafter, seed)
+        selected_spikes = select_random_spikes_uniformly(
+            self.recording, self.sorting, self._params["max_spikes_per_unit"], nbefore, nafter, seed
+        )
 
         # store in a 2 columns (spike_index, segment_index) in a npy file
         for unit_id in self.sorting.unit_ids:
             n = np.sum([e.size for e in selected_spikes[unit_id]])
-            sampled_index = np.zeros(n, dtype=[('spike_index', 'int64'), ('segment_index', 'int64')])
+            sampled_index = np.zeros(n, dtype=[("spike_index", "int64"), ("segment_index", "int64")])
             pos = 0
             for segment_index in range(self.sorting.get_num_segments()):
                 inds = selected_spikes[unit_id][segment_index]
-                sampled_index[pos:pos + inds.size]['spike_index'] = inds
-                sampled_index[pos:pos + inds.size]['segment_index'] = segment_index
+                sampled_index[pos : pos + inds.size]["spike_index"] = inds
+                sampled_index[pos : pos + inds.size]["segment_index"] = segment_index
                 pos += inds.size
 
             if self.folder is not None:
-                sampled_index_file = self.folder / 'waveforms' / f'sampled_index_{unit_id}.npy'
+                sampled_index_file = self.folder / "waveforms" / f"sampled_index_{unit_id}.npy"
                 np.save(sampled_index_file, sampled_index)
             else:
                 self._memory_objects["sampled_indices"][unit_id] = sampled_index
 
         return selected_spikes
 
-
     def run_extract_waveforms(self, seed=None, **job_kwargs):
         job_kwargs = fix_job_kwargs(job_kwargs)
         p = self._params
         nbefore = self.nbefore
         nafter = self.nafter
         return_scaled = self.return_scaled
         unit_ids = self.sorting.unit_ids
 
         if self.folder is None:
             self._memory_objects = {"wfs_arrays": {}, "sampled_indices": {}}
 
         selected_spikes = self.sample_spikes(seed=seed)
 
-        selected_spike_times = {}
-        for unit_id in self.sorting.unit_ids:
-            selected_spike_times[unit_id] = []
-            for segment_index in range(self.sorting.get_num_segments()):
-                spike_times = self.sorting.get_unit_spike_train(unit_id=unit_id, segment_index=segment_index)
-                sel = selected_spikes[unit_id][segment_index]
-                selected_spike_times[unit_id].append(spike_times[sel])
-        
-        spikes = []
+        selected_spike_times = []
         for segment_index in range(self.sorting.get_num_segments()):
-            num_in_seg = np.sum([selected_spikes[unit_id][segment_index].size for unit_id in unit_ids], dtype=np.int64)
-            spike_dtype = [('sample_ind', 'int64'), ('unit_ind', 'int64'), ('segment_ind', 'int64')]
-            spikes_ = np.zeros(num_in_seg,  dtype=spike_dtype)
-            pos = 0
-            for unit_ind, unit_id in enumerate(unit_ids):
+            selected_spike_times.append({})
+
+            for unit_id in self.sorting.unit_ids:
                 spike_times = self.sorting.get_unit_spike_train(unit_id=unit_id, segment_index=segment_index)
                 sel = selected_spikes[unit_id][segment_index]
-                n = sel.size
-                spikes_[pos:pos+n]['sample_ind'] = spike_times[sel]
-                spikes_[pos:pos+n]['unit_ind'] = unit_ind
-                spikes_[pos:pos+n]['segment_ind'] = segment_index
-                pos += n
-            order = np.argsort(spikes_)
-            spikes_ = spikes_[order]
-            spikes.append(spikes_)
-        spikes = np.concatenate(spikes)
+                selected_spike_times[segment_index][unit_id] = spike_times[sel]
+
+        spikes = NumpySorting.from_dict(selected_spike_times, self.sampling_frequency).to_spike_vector()
 
         if self.folder is not None:
-            wf_folder = self.folder / 'waveforms'
+            wf_folder = self.folder / "waveforms"
             mode = "memmap"
             copy = False
         else:
             wf_folder = None
             mode = "shared_memory"
             copy = True
-        
+
         if self.sparsity is None:
             sparsity_mask = None
         else:
             sparsity_mask = self.sparsity.mask
 
-        wfs_arrays = extract_waveforms_to_buffers(self.recording, spikes, unit_ids, nbefore, nafter,
-                                                  mode=mode, return_scaled=return_scaled, folder=wf_folder,
-                                                  dtype=p['dtype'], sparsity_mask=sparsity_mask, copy=copy,
-                                                  **job_kwargs)
+        wfs_arrays = extract_waveforms_to_buffers(
+            self.recording,
+            spikes,
+            unit_ids,
+            nbefore,
+            nafter,
+            mode=mode,
+            return_scaled=return_scaled,
+            folder=wf_folder,
+            dtype=p["dtype"],
+            sparsity_mask=sparsity_mask,
+            copy=copy,
+            **job_kwargs,
+        )
         if self.folder is None:
             self._memory_objects["wfs_arrays"] = wfs_arrays
 
 
 def select_random_spikes_uniformly(recording, sorting, max_spikes_per_unit, nbefore=None, nafter=None, seed=None):
     """
     Uniform random selection of spike across segment per units.
@@ -1276,60 +1384,65 @@
     for unit_id in unit_ids:
         # spike per segment
         n_per_segment = [sorting.get_unit_spike_train(unit_id, segment_index=i).size for i in range(num_seg)]
         cum_sum = [0] + np.cumsum(n_per_segment).tolist()
         total = np.sum(n_per_segment)
         if max_spikes_per_unit is not None:
             if total > max_spikes_per_unit:
-                global_inds = np.random.choice(total, size=max_spikes_per_unit, replace=False)
-                global_inds = np.sort(global_inds)
+                global_indices = np.random.choice(total, size=max_spikes_per_unit, replace=False)
+                global_indices = np.sort(global_indices)
             else:
-                global_inds = np.arange(total)
+                global_indices = np.arange(total)
         else:
-            global_inds = np.arange(total)
+            global_indices = np.arange(total)
         sel_spikes = []
         for segment_index in range(num_seg):
-            in_segment = (global_inds >= cum_sum[segment_index]) & (global_inds < cum_sum[segment_index + 1])
-            inds = global_inds[in_segment] - cum_sum[segment_index]
+            in_segment = (global_indices >= cum_sum[segment_index]) & (global_indices < cum_sum[segment_index + 1])
+            indices = global_indices[in_segment] - cum_sum[segment_index]
 
             if max_spikes_per_unit is not None:
                 # clean border when sub selection
                 assert nafter is not None
                 spike_times = sorting.get_unit_spike_train(unit_id=unit_id, segment_index=segment_index)
-                sampled_spike_times = spike_times[inds]
+                sampled_spike_times = spike_times[indices]
                 num_samples = recording.get_num_samples(segment_index=segment_index)
                 mask = (sampled_spike_times >= nbefore) & (sampled_spike_times < (num_samples - nafter))
-                inds = inds[mask]
+                indices = indices[mask]
 
-            sel_spikes.append(inds)
+            sel_spikes.append(indices)
         selected_spikes[unit_id] = sel_spikes
     return selected_spikes
 
 
-def extract_waveforms(recording, sorting, folder=None,
-                      mode='folder',
-                      precompute_template=('average', ),
-                      ms_before=3., ms_after=4.,
-                      max_spikes_per_unit=500,
-                      overwrite=False,
-                      return_scaled=True,
-                      dtype=None,
-                      sparse=False,
-                      sparsity=None,
-                      num_spikes_for_sparsity=100,
-                      allow_unfiltered=False,
-                      use_relative_path=False,
-                      seed=None,
-                      load_if_exists=None,
-                      **kwargs):
+def extract_waveforms(
+    recording,
+    sorting,
+    folder=None,
+    mode="folder",
+    precompute_template=("average",),
+    ms_before=3.0,
+    ms_after=4.0,
+    max_spikes_per_unit=500,
+    overwrite=False,
+    return_scaled=True,
+    dtype=None,
+    sparse=False,
+    sparsity=None,
+    num_spikes_for_sparsity=100,
+    allow_unfiltered=False,
+    use_relative_path=False,
+    seed=None,
+    load_if_exists=None,
+    **kwargs,
+):
     """
     Extracts waveform on paired Recording-Sorting objects.
     Waveforms can be persistent on disk (`mode`="folder") or in-memory (`mode`="memory").
     By default, waveforms are extracted on a subset of the spikes (`max_spikes_per_unit`) and on all channels (dense).
-    If the `sparse` parameter is set to True, a sparsity is estimated using a small number of spikes 
+    If the `sparse` parameter is set to True, a sparsity is estimated using a small number of spikes
     (`num_spikes_for_sparsity`) and waveforms are extracted and saved in sparse mode.
 
 
     Parameters
     ----------
     recording: Recording
         The recording object
@@ -1353,51 +1466,51 @@
         If True and 'folder' exists, the folder is removed and waveforms are recomputed.
         Otherwise an error is raised.
     return_scaled: bool
         If True and recording has gain_to_uV/offset_to_uV properties, waveforms are converted to uV.
     dtype: dtype or None
         Dtype of the output waveforms. If None, the recording dtype is maintained.
     sparse: bool (default False)
-        If True, before extracting all waveforms the `precompute_sparsity()` functio is run using 
-        a few spikes to get an estimate of dense templates to create a ChannelSparsity object. 
+        If True, before extracting all waveforms the `precompute_sparsity()` function is run using
+        a few spikes to get an estimate of dense templates to create a ChannelSparsity object.
         Then, the waveforms will be sparse at extraction time, which saves a lot of memory.
-        When True, you must some provide kwargs handle `precompute_sparsity()` to control the kind of 
+        When True, you must some provide kwargs handle `precompute_sparsity()` to control the kind of
         sparsity you want to apply (by radius, by best channels, ...).
     sparsity: ChannelSparsity or None
         The sparsity used to compute waveforms. If this is given, `sparse` is ignored. Default None.
     num_spikes_for_sparsity: int (default 100)
         The number of spikes to use to estimate sparsity (if sparse=True).
     allow_unfiltered: bool
         If true, will accept an allow_unfiltered recording.
         False by default.
     use_relative_path: bool
-        If True, the recording and sorting paths are relative to the waveforms folder. 
-        This allows portability of the waveform folder provided that the relative paths are the same, 
+        If True, the recording and sorting paths are relative to the waveforms folder.
+        This allows portability of the waveform folder provided that the relative paths are the same,
         but forces all the data files to be in the same drive.
         Default is False.
     seed: int or None
         Random seed for spike selection
 
     sparsity kwargs:
     {}
-    
+
 
     job kwargs:
     {}
 
 
     Returns
     -------
     we: WaveformExtractor
         The WaveformExtractor object
 
     Examples
     --------
     >>> import spikeinterface as si
-    
+
     >>> # Extract dense waveforms and save to disk
     >>> we = si.extract_waveforms(recording, sorting, folder="waveforms")
 
     >>> # Extract dense waveforms with parallel processing and save to disk
     >>> job_kwargs = dict(n_jobs=8, chunk_duration="1s", progress_bar=True)
     >>> we = si.extract_waveforms(recording, sorting, folder="waveforms", **job_kwargs)
 
@@ -1414,78 +1527,101 @@
     if load_if_exists is None:
         load_if_exists = False
     else:
         warn("load_if_exists=True/false is deprcated. Use load_waveforms() instead.", DeprecationWarning, stacklevel=2)
 
     estimate_kwargs, job_kwargs = split_job_kwargs(kwargs)
 
+    assert (
+        recording.has_channel_location()
+    ), "Recording must have a probe  or channel location to extract waveforms. Use the `set_probe()` or `set_dummy_probe_from_locations()` methods."
+
     if mode == "folder":
         assert folder is not None
         folder = Path(folder)
         assert not (overwrite and load_if_exists), "Use either 'overwrite=True' or 'load_if_exists=True'"
         if overwrite and folder.is_dir():
             shutil.rmtree(folder)
         if load_if_exists and folder.is_dir():
             we = WaveformExtractor.load_from_folder(folder)
             return we
 
     if sparsity is not None:
         assert isinstance(sparsity, ChannelSparsity), "'sparsity' must be a ChannelSparsity object"
         unit_id_to_channel_ids = sparsity.unit_id_to_channel_ids
-        assert all(u in sorting.unit_ids for u in unit_id_to_channel_ids), \
-                "Invalid unit ids in sparsity"
+        assert all(u in sorting.unit_ids for u in unit_id_to_channel_ids), "Invalid unit ids in sparsity"
         for channels in unit_id_to_channel_ids.values():
-            assert all(ch in recording.channel_ids for ch in channels), \
-                "Invalid channel ids in sparsity"
+            assert all(ch in recording.channel_ids for ch in channels), "Invalid channel ids in sparsity"
     elif sparse:
-        sparsity = precompute_sparsity(recording, sorting, ms_before=ms_before, ms_after=ms_after,
-                                       num_spikes_for_sparsity=num_spikes_for_sparsity,
-                                       **estimate_kwargs, **job_kwargs)
+        sparsity = precompute_sparsity(
+            recording,
+            sorting,
+            ms_before=ms_before,
+            ms_after=ms_after,
+            num_spikes_for_sparsity=num_spikes_for_sparsity,
+            **estimate_kwargs,
+            **job_kwargs,
+        )
     else:
         sparsity = None
 
-    we = WaveformExtractor.create(recording, sorting, folder, mode=mode, use_relative_path=use_relative_path,
-                                  allow_unfiltered=allow_unfiltered, sparsity=sparsity)
-    we.set_params(ms_before=ms_before, ms_after=ms_after, max_spikes_per_unit=max_spikes_per_unit, dtype=dtype,
-                  return_scaled=return_scaled)
+    we = WaveformExtractor.create(
+        recording,
+        sorting,
+        folder,
+        mode=mode,
+        use_relative_path=use_relative_path,
+        allow_unfiltered=allow_unfiltered,
+        sparsity=sparsity,
+    )
+    we.set_params(
+        ms_before=ms_before,
+        ms_after=ms_after,
+        max_spikes_per_unit=max_spikes_per_unit,
+        dtype=dtype,
+        return_scaled=return_scaled,
+    )
     we.run_extract_waveforms(seed=seed, **job_kwargs)
 
     if precompute_template is not None:
         we.precompute_templates(modes=precompute_template)
 
     return we
 
+
 extract_waveforms.__doc__ = extract_waveforms.__doc__.format(_sparsity_doc, _shared_job_kwargs_doc)
 
 
-def load_waveforms(folder, with_recording=True, sorting=None):
+def load_waveforms(folder, with_recording: bool = True, sorting: Optional[BaseSorting] = None) -> WaveformExtractor:
     """
     Load a waveform extractor object from disk.
 
     Parameters
     ----------
     folder : str or Path
         The folder / zarr folder where the waveform extractor is stored
     with_recording : bool, optional
-        If True, the recording is loaded, by default True
+        If True, the recording is loaded, by default True.
+        If False, the WaveformExtractor object in recordingless mode.
     sorting : BaseSorting, optional
         If passed, the sorting object associated to the waveform extractor, by default None
 
     Returns
     -------
     we: WaveformExtractor
         The loaded waveform extractor
     """
     return WaveformExtractor.load(folder, with_recording, sorting)
 
 
-def precompute_sparsity(recording, sorting, num_spikes_for_sparsity=100, unit_batch_size=200,
-                         ms_before=2., ms_after=3., **kwargs):
+def precompute_sparsity(
+    recording, sorting, num_spikes_for_sparsity=100, unit_batch_size=200, ms_before=2.0, ms_after=3.0, **kwargs
+):
     """
-    Pre-estimate sparsity with few spikes and by unit batch. 
+    Pre-estimate sparsity with few spikes and by unit batch.
     This equivalent to compute a dense waveform extractor (with all units at once) and so
     can be less memory agressive.
 
     Parameters
     ----------
     recording: Recording
         The recording object
@@ -1499,15 +1635,15 @@
     ms_before: float
         Time in ms to cut before spike peak
     ms_after: float
         Time in ms to cut after spike peak
 
     kwargs for sparsity strategy:
     {}
-    
+
 
     Job kwargs:
     {}
 
     Returns
     -------
     sparsity : ChannelSparsity
@@ -1518,24 +1654,33 @@
 
     unit_ids = sorting.unit_ids
     channel_ids = recording.channel_ids
 
     if unit_batch_size is None:
         unit_batch_size = len(unit_ids)
 
-    mask = np.zeros((len(unit_ids), len(channel_ids)), dtype='bool')
+    mask = np.zeros((len(unit_ids), len(channel_ids)), dtype="bool")
 
     nloop = int(np.ceil((unit_ids.size / unit_batch_size)))
     for i in range(nloop):
         sl = slice(i * unit_batch_size, (i + 1) * unit_batch_size)
         local_ids = unit_ids[sl]
         local_sorting = sorting.select_units(local_ids)
-        local_we = extract_waveforms(recording, local_sorting, folder=None, mode='memory',
-                                     precompute_template=('average', ), ms_before=ms_before, ms_after=ms_after,
-                                     max_spikes_per_unit=num_spikes_for_sparsity, return_scaled=False, **job_kwargs)
+        local_we = extract_waveforms(
+            recording,
+            local_sorting,
+            folder=None,
+            mode="memory",
+            precompute_template=("average",),
+            ms_before=ms_before,
+            ms_after=ms_after,
+            max_spikes_per_unit=num_spikes_for_sparsity,
+            return_scaled=False,
+            **job_kwargs,
+        )
         local_sparsity = compute_sparsity(local_we, **sparse_kwargs)
         mask[sl, :] = local_sparsity.mask
 
     sparsity = ChannelSparsity(mask, unit_ids, channel_ids)
     return sparsity
 
 
@@ -1543,52 +1688,53 @@
 
 
 class BaseWaveformExtractorExtension:
     """
     This the base class to extend the waveform extractor.
     It handles persistency to disk any computations related
     to a waveform extractor.
-    
+
     For instance:
       * principal components
       * spike amplitudes
       * quality metrics
 
     The design is done via a `WaveformExtractor.register_extension(my_extension_class)`,
     so that only imported modules can be used as *extension*.
 
     It also enables any custum computation on top on waveform extractor to be implemented by the user.
-    
+
     An extension needs to inherit from this class and implement some abstract methods:
       * _reset
       * _set_params
       * _run
-    
+
     The subclass must also save to the `self.extension_folder` any file that needs
     to be reloaded when calling `_load_extension_data`
 
     The subclass must also set an `extension_name` attribute which is not None by default.
     """
-    
-    # must be set in inherited in subclass 
+
+    # must be set in inherited in subclass
     extension_name = None
     handle_sparsity = False
-    
+
     def __init__(self, waveform_extractor):
         self.waveform_extractor = waveform_extractor
-        
+
         if self.waveform_extractor.folder is not None:
             self.folder = self.waveform_extractor.folder
             self.format = self.waveform_extractor.format
             if self.format == "binary":
                 self.extension_folder = self.folder / self.extension_name
                 if not self.extension_folder.is_dir():
                     self.extension_folder.mkdir()
             else:
                 import zarr
+
                 zarr_root = zarr.open(self.folder, mode="r+")
                 if self.extension_name not in zarr_root.keys():
                     self.extension_group = zarr_root.create_group(self.extension_name)
                 else:
                     self.extension_group = zarr_root[self.extension_name]
         else:
             self.format = "memory"
@@ -1605,16 +1751,16 @@
         folder = Path(folder)
         assert folder.is_dir(), "Waveform folder does not exists"
         if folder.suffix == ".zarr":
             params = cls.load_params_from_zarr(folder)
         else:
             params = cls.load_params_from_folder(folder)
 
-        if 'sparsity' in params and params['sparsity'] is not None:
-            params['sparsity'] = ChannelSparsity.from_dict(params['sparsity'])
+        if "sparsity" in params and params["sparsity"] is not None:
+            params["sparsity"] = ChannelSparsity.from_dict(params["sparsity"])
 
         if waveform_extractor is None:
             waveform_extractor = WaveformExtractor.load(folder)
 
         # make instance with params
         ext = cls(waveform_extractor)
         ext._params = params
@@ -1627,88 +1773,93 @@
         """
         Load extension params from Zarr folder.
         'folder' is the waveform extractor zarr folder.
         """
         import zarr
 
         zarr_root = zarr.open(folder, mode="r+")
-        assert cls.extension_name in zarr_root.keys(), f'WaveformExtractor: extension {cls.extension_name} ' \
-                                                        f'is not in folder {folder}'
+        assert cls.extension_name in zarr_root.keys(), (
+            f"WaveformExtractor: extension {cls.extension_name} " f"is not in folder {folder}"
+        )
         extension_group = zarr_root[cls.extension_name]
-        assert 'params' in extension_group.attrs, f'No params file in extension {cls.extension_name} folder'
-        params = extension_group.attrs['params']
+        assert "params" in extension_group.attrs, f"No params file in extension {cls.extension_name} folder"
+        params = extension_group.attrs["params"]
 
         return params
 
     @classmethod
     def load_params_from_folder(cls, folder):
         """
         Load extension params from folder.
         'folder' is the waveform extractor folder.
         """
         ext_folder = Path(folder) / cls.extension_name
-        assert ext_folder.is_dir(), f'WaveformExtractor: extension {cls.extension_name} is not in folder {folder}'
-        
-        params_file = ext_folder / 'params.json'
-        assert params_file.is_file(), f'No params file in extension {cls.extension_name} folder'
-        
-        with open(str(params_file), 'r') as f:
+        assert ext_folder.is_dir(), f"WaveformExtractor: extension {cls.extension_name} is not in folder {folder}"
+
+        params_file = ext_folder / "params.json"
+        assert params_file.is_file(), f"No params file in extension {cls.extension_name} folder"
+
+        with open(str(params_file), "r") as f:
             params = json.load(f)
 
         return params
 
     # use load instead
     def _load_extension_data(self):
         if self.format == "binary":
             for ext_data_file in self.extension_folder.iterdir():
-                if ext_data_file.name == 'params.json':
+                if ext_data_file.name == "params.json":
                     continue
                 ext_data_name = ext_data_file.stem
-                if ext_data_file.suffix == '.json':
-                    ext_data = json.load(ext_data_file.open('r'))
-                elif ext_data_file.suffix == '.npy':
-                    ext_data = np.load(ext_data_file, mmap_mode='r')
-                elif ext_data_file.suffix == '.csv':
+                if ext_data_file.suffix == ".json":
+                    ext_data = json.load(ext_data_file.open("r"))
+                elif ext_data_file.suffix == ".npy":
+                    ext_data = np.load(ext_data_file, mmap_mode="r")
+                elif ext_data_file.suffix == ".csv":
                     import pandas as pd
+
                     ext_data = pd.read_csv(ext_data_file, index_col=0)
-                elif ext_data_file.suffix == '.pkl':
-                    ext_data = pickle.load(ext_data_file.open('rb'))
+                elif ext_data_file.suffix == ".pkl":
+                    ext_data = pickle.load(ext_data_file.open("rb"))
                 self._extension_data[ext_data_name] = ext_data
-        elif self.format == "zarr":            
+        elif self.format == "zarr":
             for ext_data_name in self.extension_group.keys():
                 ext_data_ = self.extension_group[ext_data_name]
                 if "dict" in ext_data_.attrs:
                     ext_data = ext_data_[0]
                 elif "dataframe" in ext_data_.attrs:
                     import xarray
-                    ext_data = xarray.open_zarr(ext_data_.store,
-                                                group=f"{self.extension_group.name}/{ext_data_name}").to_pandas()
+
+                    ext_data = xarray.open_zarr(
+                        ext_data_.store, group=f"{self.extension_group.name}/{ext_data_name}"
+                    ).to_pandas()
                     ext_data.index.rename("", inplace=True)
                 else:
                     ext_data = ext_data_
-                self._extension_data[ext_data_name] = ext_data                    
+                self._extension_data[ext_data_name] = ext_data
 
     def run(self, **kwargs):
         self._run(**kwargs)
         self._save(**kwargs)
 
     def _run(self, **kwargs):
         # must be implemented in subclass
         # must populate the self._extension_data dictionary
         raise NotImplementedError
-    
+
     def save(self, **kwargs):
         self._save(**kwargs)
-    
+
     def _save(self, **kwargs):
         if self.format == "binary":
             import pandas as pd
+
             for ext_data_name, ext_data in self._extension_data.items():
                 if isinstance(ext_data, dict):
-                    with (self.extension_folder / f"{ext_data_name}.json").open('w') as f:
+                    with (self.extension_folder / f"{ext_data_name}.json").open("w") as f:
                         json.dump(ext_data, f)
                 elif isinstance(ext_data, np.ndarray):
                     np.save(self.extension_folder / f"{ext_data_name}.npy", ext_data)
                 elif isinstance(ext_data, pd.DataFrame):
                     ext_data.to_csv(self.extension_folder / f"{ext_data_name}.csv", index=True)
                 else:
                     try:
@@ -1724,55 +1875,56 @@
             compressor = kwargs.get("compressor", None)
             if compressor is None:
                 compressor = get_default_zarr_compressor()
             for ext_data_name, ext_data in self._extension_data.items():
                 if ext_data_name in self.extension_group:
                     del self.extension_group[ext_data_name]
                 if isinstance(ext_data, dict):
-                    self.extension_group.create_dataset(name=ext_data_name, data=[ext_data],
-                                                        object_codec=numcodecs.JSON())
+                    self.extension_group.create_dataset(
+                        name=ext_data_name, data=[ext_data], object_codec=numcodecs.JSON()
+                    )
                     self.extension_group[ext_data_name].attrs["dict"] = True
                 elif isinstance(ext_data, np.ndarray):
-                    self.extension_group.create_dataset(name=ext_data_name, data=ext_data,
-                                                        compressor=compressor)
+                    self.extension_group.create_dataset(name=ext_data_name, data=ext_data, compressor=compressor)
                 elif isinstance(ext_data, pd.DataFrame):
-                    ext_data.to_xarray().to_zarr(store=self.extension_group.store,
-                                                 group=f"{self.extension_group.name}/{ext_data_name}",
-                                                 mode="a")
+                    ext_data.to_xarray().to_zarr(
+                        store=self.extension_group.store, group=f"{self.extension_group.name}/{ext_data_name}", mode="a"
+                    )
                     self.extension_group[ext_data_name].attrs["dataframe"] = True
                 else:
                     try:
-                        self.extension_group.create_dataset(name=ext_data_name, data=ext_data,
-                                                            object_codec=numcodecs.Pickle())
+                        self.extension_group.create_dataset(
+                            name=ext_data_name, data=ext_data, object_codec=numcodecs.Pickle()
+                        )
                     except:
                         raise Exception(f"Could not save {ext_data_name} as extension data")
 
     def reset(self):
         """
         Reset the waveform extension.
         Delete the sub folder and create a new empty one.
-        """        
+        """
         if self.extension_folder is not None:
             if self.format == "binary":
                 if self.extension_folder.is_dir():
                     shutil.rmtree(self.extension_folder)
                 self.extension_folder.mkdir()
             elif self.format == "zarr":
                 del self.extension_group
-        
+
         self._params = None
         self._extension_data = dict()
-    
+
     def select_units(self, unit_ids, new_waveform_extractor):
         new_extension = self.__class__(new_waveform_extractor)
         new_extension.set_params(**self._params)
         new_extension_data = self._select_extension_data(unit_ids=unit_ids)
         new_extension._extension_data = new_extension_data
         new_extension._save()
-    
+
     def copy(self, new_waveform_extractor):
         new_extension = self.__class__(new_waveform_extractor)
         new_extension.set_params(**self._params)
         new_extension._extension_data = self._extension_data
         new_extension._save()
 
     def _select_extension_data(self, unit_ids):
@@ -1784,29 +1936,29 @@
         Set parameters for the extension and
         make it persistent in json.
         """
         params = self._set_params(**params)
         self._params = params
 
         params_to_save = params.copy()
-        if 'sparsity' in params and params['sparsity'] is not None:
-            assert isinstance(params['sparsity'], ChannelSparsity), \
-                "'sparsity' parameter must be a ChannelSparsity object!"
-            params_to_save['sparsity'] = params['sparsity'].to_dict()
+        if "sparsity" in params and params["sparsity"] is not None:
+            assert isinstance(
+                params["sparsity"], ChannelSparsity
+            ), "'sparsity' parameter must be a ChannelSparsity object!"
+            params_to_save["sparsity"] = params["sparsity"].to_dict()
         if self.format == "binary":
             if self.extension_folder is not None:
-                param_file = self.extension_folder / 'params.json'
-                param_file.write_text(json.dumps(check_json(params_to_save), indent=4), encoding='utf8')
+                param_file = self.extension_folder / "params.json"
+                param_file.write_text(json.dumps(check_json(params_to_save), indent=4), encoding="utf8")
         elif self.format == "zarr":
-            self.extension_group.attrs['params'] = check_json(params_to_save)
+            self.extension_group.attrs["params"] = check_json(params_to_save)
 
     def _set_params(self, **params):
         # must be implemented in subclass
         # must return a cleaned version of params dict
         raise NotImplementedError
 
     @staticmethod
     def get_extension_function():
         # must be implemented in subclass
         # must return extension function
         raise NotImplementedError
-
```

### Comparing `spikeinterface-0.97.1/spikeinterface/core/waveform_tools.py` & `spikeinterface-0.98.0/src/spikeinterface/core/waveform_tools.py`

 * *Files 14% similar despite different names*

```diff
@@ -8,23 +8,33 @@
   2. extract and distribute snippets into buffers (optionally in parallel)
 
 """
 from pathlib import Path
 
 import numpy as np
 
-from .job_tools import ChunkRecordingExecutor, ensure_n_jobs, _shared_job_kwargs_doc
+from .job_tools import ChunkRecordingExecutor, _shared_job_kwargs_doc
 from .core_tools import make_shared_array
 from .job_tools import fix_job_kwargs
 
 
-
-def extract_waveforms_to_buffers(recording, spikes, unit_ids, nbefore, nafter,
-                                 mode='memmap', return_scaled=False, folder=None, dtype=None,
-                                 sparsity_mask=None, copy=False, **job_kwargs):
+def extract_waveforms_to_buffers(
+    recording,
+    spikes,
+    unit_ids,
+    nbefore,
+    nafter,
+    mode="memmap",
+    return_scaled=False,
+    folder=None,
+    dtype=None,
+    sparsity_mask=None,
+    copy=False,
+    **job_kwargs,
+):
     """
     Allocate buffers (memmap or or shared memory) and then distribute every waveform into theses buffers.
 
     Same as calling allocate_waveforms_buffers() and then distribute_waveforms_to_buffers().
 
     Important note: for the "shared_memory" mode wfs_arrays_info contains reference to
     the shared memmory buffer, this variable must be reference as long as arrays as used.
@@ -52,18 +62,19 @@
         In case of memmap mode, folder to save npy files
     dtype: numpy.dtype
         dtype for waveforms buffer
     sparsity_mask: None or array of bool
         If not None shape must be must be (len(unit_ids), len(channel_ids))
     copy: bool
         If True (default), the output shared memory object is copied to a numpy standard array.
-        If copy=False then wfs_arrays_info is also return. Please keep in mind that wfs_arrays_info 
+        If copy=False then wfs_arrays_info is also return. Please keep in mind that wfs_arrays_info
         need to be referenced as long as wfs_arrays will be used otherwise it will be very hard to debug.
+        Also when copy=False the SharedMemory will need to be unlink manually
     {}
-    
+
     Returns
     -------
     wfs_arrays: dict of arrays
         Arrays for all units (memmap or shared_memmep)
 
     wfs_arrays_info: dict of info
         Optionally return in case of shared_memory if copy=False.
@@ -71,41 +82,56 @@
     """
     job_kwargs = fix_job_kwargs(job_kwargs)
 
     if dtype is None:
         if return_scaled:
             dtype = recording.get_dtype()
         else:
-            dtype = 'float32'
+            dtype = "float32"
     dtype = np.dtype(dtype)
 
+    wfs_arrays, wfs_arrays_info = allocate_waveforms_buffers(
+        recording, spikes, unit_ids, nbefore, nafter, mode=mode, folder=folder, dtype=dtype, sparsity_mask=sparsity_mask
+    )
+
+    distribute_waveforms_to_buffers(
+        recording,
+        spikes,
+        unit_ids,
+        wfs_arrays_info,
+        nbefore,
+        nafter,
+        return_scaled,
+        mode=mode,
+        sparsity_mask=sparsity_mask,
+        **job_kwargs,
+    )
 
-    wfs_arrays, wfs_arrays_info = allocate_waveforms_buffers(recording, spikes, unit_ids, nbefore, nafter, mode=mode,
-                                                             folder=folder, dtype=dtype, sparsity_mask=sparsity_mask)
-    
-    distribute_waveforms_to_buffers(recording, spikes, unit_ids, wfs_arrays_info, nbefore, nafter, return_scaled,
-                                    mode=mode, sparsity_mask=sparsity_mask, **job_kwargs)
-
-    if mode == 'memmap':
+    if mode == "memmap":
         return wfs_arrays
-    elif mode == 'shared_memory':
+    elif mode == "shared_memory":
         if copy:
             wfs_arrays = {unit_id: arr.copy() for unit_id, arr in wfs_arrays.items()}
-            # clear shared mem buffer
-            del wfs_arrays_info
+            # release all sharedmem buffer
+            for unit_id in unit_ids:
+                shm = wfs_arrays_info[unit_id][0]
+                if shm is not None:
+                    # empty array have None
+                    shm.unlink()
             return wfs_arrays
         else:
             return wfs_arrays, wfs_arrays_info
 
 
 extract_waveforms_to_buffers.__doc__ = extract_waveforms_to_buffers.__doc__.format(_shared_job_kwargs_doc)
 
 
-def allocate_waveforms_buffers(recording, spikes, unit_ids, nbefore, nafter, mode='memmap', folder=None, dtype=None,
-                               sparsity_mask=None):
+def allocate_waveforms_buffers(
+    recording, spikes, unit_ids, nbefore, nafter, mode="memmap", folder=None, dtype=None, sparsity_mask=None
+):
     """
     Allocate memmap or shared memory buffers before snippet extraction.
 
     Important note: for the shared memory mode wfs_arrays_info contains reference to
     the shared memmory buffer, this variable must be reference as long as arrays as used.
 
     Parameters
@@ -137,54 +163,63 @@
     wfs_arrays_info: dict of info
         Dictionary to "construct" array in workers process (memmap file or sharemem)
     """
 
     nsamples = nbefore + nafter
 
     dtype = np.dtype(dtype)
-    if mode == 'shared_memory':
+    if mode == "shared_memory":
         assert folder is None
     else:
         folder = Path(folder)
 
     # prepare buffers
     wfs_arrays = {}
     wfs_arrays_info = {}
     for unit_ind, unit_id in enumerate(unit_ids):
-        n_spikes = np.sum(spikes['unit_ind'] == unit_ind)
+        n_spikes = np.sum(spikes["unit_index"] == unit_ind)
         if sparsity_mask is None:
             num_chans = recording.get_num_channels()
         else:
             num_chans = np.sum(sparsity_mask[unit_ind, :])
         shape = (n_spikes, nsamples, num_chans)
 
-        if mode == 'memmap':
-            filename = str(folder / f'waveforms_{unit_id}.npy')
-            arr = np.lib.format.open_memmap(
-                filename, mode='w+', dtype=dtype, shape=shape)
+        if mode == "memmap":
+            filename = str(folder / f"waveforms_{unit_id}.npy")
+            arr = np.lib.format.open_memmap(filename, mode="w+", dtype=dtype, shape=shape)
             wfs_arrays[unit_id] = arr
             wfs_arrays_info[unit_id] = filename
-        elif mode == 'shared_memory':
+        elif mode == "shared_memory":
             if n_spikes == 0:
                 arr = np.zeros(shape, dtype=dtype)
                 shm = None
                 shm_name = None
             else:
                 arr, shm = make_shared_array(shape, dtype)
                 shm_name = shm.name
             wfs_arrays[unit_id] = arr
             wfs_arrays_info[unit_id] = (shm, shm_name, dtype.str, shape)
         else:
-            raise ValueError('allocate_waveforms_buffers bad mode')
+            raise ValueError("allocate_waveforms_buffers bad mode")
 
     return wfs_arrays, wfs_arrays_info
 
 
-def distribute_waveforms_to_buffers(recording, spikes, unit_ids, wfs_arrays_info, nbefore, nafter, return_scaled,
-                                    mode='memmap', sparsity_mask=None, **job_kwargs):
+def distribute_waveforms_to_buffers(
+    recording,
+    spikes,
+    unit_ids,
+    wfs_arrays_info,
+    nbefore,
+    nafter,
+    return_scaled,
+    mode="memmap",
+    sparsity_mask=None,
+    **job_kwargs,
+):
     """
     Distribute snippets of traces into corresponding buffers.
 
     Buffers must be pre-allocated with the `allocate_waveforms_buffers()` function.
 
     Important note, for "shared_memory" mode wfs_arrays_info contain reference to
     the shared memmory buffer, this variable must be reference as long as arrays as used.
@@ -211,146 +246,154 @@
     sparsity_mask: None or array of bool
         If not None shape must be must be (len(unit_ids), len(channel_ids)
 
     {}
 
     """
     job_kwargs = fix_job_kwargs(job_kwargs)
-    n_jobs = ensure_n_jobs(recording, job_kwargs.get('n_jobs', None))
 
     inds_by_unit = {}
     for unit_ind, unit_id in enumerate(unit_ids):
-        inds,  = np.nonzero(spikes['unit_ind'] == unit_ind)
+        (inds,) = np.nonzero(spikes["unit_index"] == unit_ind)
         inds_by_unit[unit_id] = inds
 
     # and run
     func = _waveform_extractor_chunk
     init_func = _init_worker_waveform_extractor
-    if n_jobs == 1:
-        init_args = (recording, )
-    else:
-        init_args = (recording.to_dict(), )
-    init_args = init_args + (unit_ids, spikes, wfs_arrays_info, nbefore,
-                             nafter, return_scaled, inds_by_unit, mode, sparsity_mask)
-    processor = ChunkRecordingExecutor(recording, func, init_func, init_args, job_name=f'extract waveforms {mode}',
-                                       **job_kwargs)
+
+    init_args = (
+        recording,
+        unit_ids,
+        spikes,
+        wfs_arrays_info,
+        nbefore,
+        nafter,
+        return_scaled,
+        inds_by_unit,
+        mode,
+        sparsity_mask,
+    )
+    processor = ChunkRecordingExecutor(
+        recording, func, init_func, init_args, job_name=f"extract waveforms {mode}", **job_kwargs
+    )
     processor.run()
 
 
 distribute_waveforms_to_buffers.__doc__ = distribute_waveforms_to_buffers.__doc__.format(_shared_job_kwargs_doc)
 
 
 # used by ChunkRecordingExecutor
-def _init_worker_waveform_extractor(recording, unit_ids, spikes, wfs_arrays_info, nbefore, nafter, return_scaled,
-                                    inds_by_unit, mode, sparsity_mask):
+def _init_worker_waveform_extractor(
+    recording, unit_ids, spikes, wfs_arrays_info, nbefore, nafter, return_scaled, inds_by_unit, mode, sparsity_mask
+):
     # create a local dict per worker
     worker_ctx = {}
     if isinstance(recording, dict):
         from spikeinterface.core import load_extractor
+
         recording = load_extractor(recording)
-    worker_ctx['recording'] = recording
+    worker_ctx["recording"] = recording
 
-    if mode == 'memmap':
+    if mode == "memmap":
         # in memmap mode we have the "too many open file" problem with linux
         # memmap file will be open on demand and not globally per worker
-        worker_ctx['wfs_arrays_info'] = wfs_arrays_info
-    elif mode == 'shared_memory':
-
+        worker_ctx["wfs_arrays_info"] = wfs_arrays_info
+    elif mode == "shared_memory":
         from multiprocessing.shared_memory import SharedMemory
+
         wfs_arrays = {}
         shms = {}
         for unit_id, (shm, shm_name, dtype, shape) in wfs_arrays_info.items():
             if shm_name is None:
                 arr = np.zeros(shape=shape, dtype=dtype)
             else:
                 shm = SharedMemory(shm_name)
                 arr = np.ndarray(shape=shape, dtype=dtype, buffer=shm.buf)
             wfs_arrays[unit_id] = arr
             # we need a reference to all sham otherwise we get segment fault!!!
             shms[unit_id] = shm
-        worker_ctx['shms'] = shms
-        worker_ctx['wfs_arrays'] = wfs_arrays
+        worker_ctx["shms"] = shms
+        worker_ctx["wfs_arrays"] = wfs_arrays
 
-    worker_ctx['unit_ids'] = unit_ids
-    worker_ctx['spikes'] = spikes
-    
-    worker_ctx['nbefore'] = nbefore
-    worker_ctx['nafter'] = nafter
-    worker_ctx['return_scaled'] = return_scaled
-    worker_ctx['inds_by_unit'] = inds_by_unit
-    worker_ctx['sparsity_mask'] = sparsity_mask
-    worker_ctx['mode'] = mode
-    
+    worker_ctx["unit_ids"] = unit_ids
+    worker_ctx["spikes"] = spikes
+
+    worker_ctx["nbefore"] = nbefore
+    worker_ctx["nafter"] = nafter
+    worker_ctx["return_scaled"] = return_scaled
+    worker_ctx["inds_by_unit"] = inds_by_unit
+    worker_ctx["sparsity_mask"] = sparsity_mask
+    worker_ctx["mode"] = mode
 
     return worker_ctx
 
 
 # used by ChunkRecordingExecutor
 def _waveform_extractor_chunk(segment_index, start_frame, end_frame, worker_ctx):
     # recover variables of the worker
-    recording = worker_ctx['recording']
-    unit_ids = worker_ctx['unit_ids']
-    spikes = worker_ctx['spikes']
-    nbefore = worker_ctx['nbefore']
-    nafter = worker_ctx['nafter']
-    return_scaled = worker_ctx['return_scaled']
-    inds_by_unit = worker_ctx['inds_by_unit']
-    sparsity_mask = worker_ctx['sparsity_mask']
+    recording = worker_ctx["recording"]
+    unit_ids = worker_ctx["unit_ids"]
+    spikes = worker_ctx["spikes"]
+    nbefore = worker_ctx["nbefore"]
+    nafter = worker_ctx["nafter"]
+    return_scaled = worker_ctx["return_scaled"]
+    inds_by_unit = worker_ctx["inds_by_unit"]
+    sparsity_mask = worker_ctx["sparsity_mask"]
 
     seg_size = recording.get_num_samples(segment_index=segment_index)
 
-    # take only spikes with the correct segment_ind
+    # take only spikes with the correct segment_index
     # this is a slice so no copy!!
-    s0 = np.searchsorted(spikes['segment_ind'], segment_index)
-    s1 = np.searchsorted(spikes['segment_ind'], segment_index + 1)
+    s0 = np.searchsorted(spikes["segment_index"], segment_index)
+    s1 = np.searchsorted(spikes["segment_index"], segment_index + 1)
     in_seg_spikes = spikes[s0:s1]
 
     # take only spikes in range [start_frame, end_frame]
     # this is a slice so no copy!!
-    i0 = np.searchsorted(in_seg_spikes['sample_ind'], start_frame)
-    i1 = np.searchsorted(in_seg_spikes['sample_ind'], end_frame)
+    i0 = np.searchsorted(in_seg_spikes["sample_index"], start_frame)
+    i1 = np.searchsorted(in_seg_spikes["sample_index"], end_frame)
     if i0 != i1:
         # protect from spikes on border :  spike_time<0 or spike_time>seg_size
         # useful only when max_spikes_per_unit is not None
         # waveform will not be extracted and a zeros will be left in the memmap file
-        while (in_seg_spikes[i0]['sample_ind'] - nbefore) < 0 and (i0 != i1):
+        while (in_seg_spikes[i0]["sample_index"] - nbefore) < 0 and (i0 != i1):
             i0 = i0 + 1
-        while (in_seg_spikes[i1-1]['sample_ind'] + nafter) > seg_size and (i0 != i1):
+        while (in_seg_spikes[i1 - 1]["sample_index"] + nafter) > seg_size and (i0 != i1):
             i1 = i1 - 1
 
     # slice in absolut in spikes vector
     l0 = i0 + s0
     l1 = i1 + s0
 
     if l1 > l0:
-        start = spikes[l0]['sample_ind'] - nbefore
-        end = spikes[l1-1]['sample_ind'] + nafter
+        start = spikes[l0]["sample_index"] - nbefore
+        end = spikes[l1 - 1]["sample_index"] + nafter
 
         # load trace in memory
-        traces = recording.get_traces(start_frame=start, end_frame=end, segment_index=segment_index,
-                                      return_scaled=return_scaled)
+        traces = recording.get_traces(
+            start_frame=start, end_frame=end, segment_index=segment_index, return_scaled=return_scaled
+        )
 
         for unit_ind, unit_id in enumerate(unit_ids):
             # find pos
             inds = inds_by_unit[unit_id]
-            in_chunk_pos,  = np.nonzero((inds >= l0) & (inds < l1))
-            if in_chunk_pos.size ==0:
+            (in_chunk_pos,) = np.nonzero((inds >= l0) & (inds < l1))
+            if in_chunk_pos.size == 0:
                 continue
-            
-            if worker_ctx['mode'] == 'memmap':
+
+            if worker_ctx["mode"] == "memmap":
                 # open file in demand (and also autoclose it after)
-                filename = worker_ctx['wfs_arrays_info'][unit_id]
-                wfs = np.load(str(filename), mmap_mode='r+')
-            elif worker_ctx['mode'] == 'shared_memory':
-                wfs = worker_ctx['wfs_arrays'][unit_id]
+                filename = worker_ctx["wfs_arrays_info"][unit_id]
+                wfs = np.load(str(filename), mmap_mode="r+")
+            elif worker_ctx["mode"] == "shared_memory":
+                wfs = worker_ctx["wfs_arrays"][unit_id]
 
             for pos in in_chunk_pos:
-                sample_ind = spikes[inds[pos]]['sample_ind']
-                wf = traces[sample_ind - start -
-                            nbefore:sample_ind - start + nafter, :]
+                sample_index = spikes[inds[pos]]["sample_index"]
+                wf = traces[sample_index - start - nbefore : sample_index - start + nafter, :]
 
                 if sparsity_mask is None:
                     wfs[pos, :, :] = wf
                 else:
                     wfs[pos, :, :] = wf[:, sparsity_mask[unit_ind]]
 
 
@@ -368,14 +411,14 @@
     Returns
     -------
     bool
         True if exceeding spikes, False otherwise
     """
     spike_vector = sorting.to_spike_vector()
     for segment_index in range(recording.get_num_segments()):
-        start_seg_ind = np.searchsorted(spike_vector["segment_ind"], segment_index)
-        end_seg_ind = np.searchsorted(spike_vector["segment_ind"], segment_index + 1)
+        start_seg_ind = np.searchsorted(spike_vector["segment_index"], segment_index)
+        end_seg_ind = np.searchsorted(spike_vector["segment_index"], segment_index + 1)
         spike_vector_seg = spike_vector[start_seg_ind:end_seg_ind]
         if len(spike_vector_seg) > 0:
-            if spike_vector_seg["sample_ind"][-1] > recording.get_num_samples(segment_index=segment_index) - 1:
+            if spike_vector_seg["sample_index"][-1] > recording.get_num_samples(segment_index=segment_index) - 1:
                 return True
     return False
```

### Comparing `spikeinterface-0.97.1/spikeinterface/core/zarrrecordingextractor.py` & `spikeinterface-0.98.0/src/spikeinterface/core/zarrrecordingextractor.py`

 * *Files 8% similar despite different names*

```diff
@@ -6,14 +6,15 @@
 import numpy as np
 
 from .baserecording import BaseRecording, BaseRecordingSegment
 from .core_tools import define_function_from_class
 
 try:
     import zarr
+
     HAVE_ZARR = True
 except ImportError:
     HAVE_ZARR = False
 
 
 class ZarrRecordingExtractor(BaseRecording):
     """
@@ -27,61 +28,63 @@
         Storage options for zarr `store`. E.g., if "s3://" or "gcs://" they can provide authentication methods, etc.
 
     Returns
     -------
     recording: ZarrRecordingExtractor
         The recording Extractor
     """
-    extractor_name = 'ZarrRecording'
+
+    extractor_name = "ZarrRecording"
     installed = HAVE_ZARR  # check at class level if installed or not
-    mode = 'file'
+    mode = "file"
     # error message when not installed
     installation_mesg = "To use the ZarrRecordingExtractor install zarr: \n\n pip install zarr\n\n"
     name = "zarr"
 
     def __init__(self, root_path: Union[Path, str], storage_options=None):
         assert self.installed, self.installation_mesg
-        
+
         if storage_options is None:
             if isinstance(root_path, str):
                 root_path_init = root_path
                 root_path = Path(root_path)
             else:
                 root_path_init = str(root_path)
             root_path_kwarg = str(root_path.absolute())
         else:
             root_path_init = root_path
             root_path_kwarg = root_path_init
-        
+
         self._root = zarr.open(root_path_init, mode="r", storage_options=storage_options)
 
         sampling_frequency = self._root.attrs.get("sampling_frequency", None)
         num_segments = self._root.attrs.get("num_segments", None)
         assert "channel_ids" in self._root.keys(), "'channel_ids' dataset not found!"
         channel_ids = self._root["channel_ids"][:]
 
         assert sampling_frequency is not None, "'sampling_frequency' attiribute not found!"
         assert num_segments is not None, "'num_segments' attiribute not found!"
 
         channel_ids = np.array(channel_ids)
 
-        dtype = self._root['traces_seg0'].dtype
+        dtype = self._root["traces_seg0"].dtype
 
         BaseRecording.__init__(self, sampling_frequency, channel_ids, dtype)
 
         dtype = np.dtype(dtype)
         t_starts = self._root.get("t_starts", None)
 
         total_nbytes = 0
         total_nbytes_stored = 0
         cr_by_segment = {}
         for segment_index in range(num_segments):
             trace_name = f"traces_seg{segment_index}"
-            assert len(channel_ids) == self._root[trace_name].shape[1], \
-                f'Segment {segment_index} has the wrong number of channels!'
+            assert (
+                len(channel_ids) == self._root[trace_name].shape[1]
+            ), f"Segment {segment_index} has the wrong number of channels!"
 
             time_kwargs = {}
             time_vector = self._root.get(f"times_seg{segment_index}", None)
             if time_vector is not None:
                 time_kwargs["time_vector"] = time_vector
             else:
                 if t_starts is None:
@@ -90,46 +93,45 @@
                     t_start = t_starts[segment_index]
                     if np.isnan(t_start):
                         t_start = None
                 time_kwargs["t_start"] = t_start
                 time_kwargs["sampling_frequency"] = sampling_frequency
 
             rec_segment = ZarrRecordingSegment(self._root, trace_name, **time_kwargs)
-            
+
             nbytes_segment = self._root[trace_name].nbytes
             nbytes_stored_segment = self._root[trace_name].nbytes_stored
             cr_by_segment[segment_index] = nbytes_segment / nbytes_stored_segment
-            
+
             total_nbytes += nbytes_segment
             total_nbytes_stored += nbytes_stored_segment
             self.add_recording_segment(rec_segment)
 
         # load probe
         probe_dict = self._root.attrs.get("probe", None)
         if probe_dict is not None:
             probegroup = ProbeGroup.from_dict(probe_dict)
             self.set_probegroup(probegroup, in_place=True)
 
         # load properties
-        if 'properties' in self._root:
-            prop_group = self._root['properties']
+        if "properties" in self._root:
+            prop_group = self._root["properties"]
             for key in prop_group.keys():
-                values = self._root['properties'][key]
+                values = self._root["properties"][key]
                 self.set_property(key, values)
 
         # load annotations
         annotations = self._root.attrs.get("annotations", None)
         if annotations is not None:
             self.annotate(**annotations)
         # annotate compression ratios
         cr = total_nbytes / total_nbytes_stored
         self.annotate(compression_ratio=cr, compression_ratio_segments=cr_by_segment)
-        
-        self._kwargs = {'root_path': root_path_kwarg,
-                        'storage_options': storage_options}
+
+        self._kwargs = {"root_path": root_path_kwarg, "storage_options": storage_options}
 
 
 class ZarrRecordingSegment(BaseRecordingSegment):
     def __init__(self, root, dataset_name, **time_kwargs):
         BaseRecordingSegment.__init__(self, **time_kwargs)
         self._timeseries = root[dataset_name]
 
@@ -137,31 +139,32 @@
         """Returns the number of samples in this signal block
 
         Returns:
             SampleIndex: Number of samples in the signal block
         """
         return self._timeseries.shape[0]
 
-    def get_traces(self,
-                   start_frame: Union[int, None] = None,
-                   end_frame: Union[int, None] = None,
-                   channel_indices: Union[List, None] = None,
-                   ) -> np.ndarray:
+    def get_traces(
+        self,
+        start_frame: Union[int, None] = None,
+        end_frame: Union[int, None] = None,
+        channel_indices: Union[List, None] = None,
+    ) -> np.ndarray:
         traces = self._timeseries[start_frame:end_frame]
         if channel_indices is not None:
             traces = traces[:, channel_indices]
         return traces
 
 
 read_zarr = define_function_from_class(source_class=ZarrRecordingExtractor, name="read_zarr")
 
 
 def get_default_zarr_compressor(clevel=5):
     """
-    Return default Zarr compressor object for good preformance in int16 
+    Return default Zarr compressor object for good preformance in int16
     electrophysiology data.
 
     cname: zstd (zstandard)
     clevel: 5
     shuffle: BITSHUFFLE
 
     Parameters
@@ -173,8 +176,9 @@
     Returns
     -------
     Blosc.compressor
         The compressor object that can be used with the save to zarr function
     """
     assert ZarrRecordingExtractor.installed, ZarrRecordingExtractor.installation_mesg
     from numcodecs import Blosc
+
     return Blosc(cname="zstd", clevel=clevel, shuffle=Blosc.BITSHUFFLE)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/curation/__init__.py` & `spikeinterface-0.98.0/src/spikeinterface/curation/__init__.py`

 * *Files 18% similar despite different names*

```diff
@@ -2,13 +2,13 @@
 
 from .remove_redundant import remove_redundant_units, find_redundant_units
 from .remove_duplicated_spikes import remove_duplicated_spikes
 from .remove_excess_spikes import remove_excess_spikes
 from .auto_merge import get_potential_auto_merge
 
 
-# manual sorting, 
+# manual sorting,
 from .curationsorting import CurationSorting
 from .mergeunitssorting import MergeUnitsSorting
 from .splitunitsorting import SplitUnitSorting
 
 from .sortingview_curation import apply_sortingview_curation
```

### Comparing `spikeinterface-0.97.1/spikeinterface/curation/auto_merge.py` & `spikeinterface-0.98.0/src/spikeinterface/curation/auto_merge.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,67 +1,68 @@
 from re import template
 import numpy as np
-import scipy.signal
-import scipy.spatial
+
 
 from ..core.template_tools import get_template_extremum_channel
 from ..postprocessing import compute_correlograms
 from ..qualitymetrics import compute_refrac_period_violations, compute_firing_rates
 
 from .mergeunitssorting import MergeUnitsSorting
 
 
 def get_potential_auto_merge(
-    waveform_extractor, 
-    minimum_spikes=1000, 
-    maximum_distance_um=150.,
+    waveform_extractor,
+    minimum_spikes=1000,
+    maximum_distance_um=150.0,
     peak_sign="neg",
-    bin_ms=0.25, window_ms=100.,
+    bin_ms=0.25,
+    window_ms=100.0,
     corr_diff_thresh=0.16,
     template_diff_thresh=0.25,
-    censored_period_ms=0., 
+    censored_period_ms=0.3,
     refractory_period_ms=1.0,
-    sigma_smooth_ms = 0.6,
+    sigma_smooth_ms=0.6,
     contamination_threshold=0.2,
     adaptative_window_threshold=0.5,
+    censor_correlograms_ms: float = 0.15,
     num_channels=5,
     num_shift=5,
     firing_contamination_balance=1.5,
     extra_outputs=False,
     steps=None,
 ):
     """
     Algorithm to find and check potential merges between units.
 
-    This is taken from lussac version 1 done by Aurelien Wyngaard.
+    This is taken from lussac version 1 done by Aurelien Wyngaard and Victor Llobet.
     https://github.com/BarbourLab/lussac/blob/v1.0.0/postprocessing/merge_units.py
 
 
     The merges are proposed when the following criteria are met:
 
         * STEP 1: enough spikes are found in each units for computing the correlogram (`minimum_spikes`)
         * STEP 2: each unit is not contaminated (by checking auto-correlogram - `contamination_threshold`)
         * STEP 3: estimated unit locations are close enough (`maximum_distance_um`)
         * STEP 4: the cross-correlograms of the two units are similar to each auto-corrleogram (`corr_diff_thresh`)
         * STEP 5: the templates of the two units are similar (`template_diff_thresh`)
         * STEP 6: the unit "quality score" is increased after the merge.
 
-    The "quality score" factors in the increase in firing rate (**f**) due to the merge and a possible increase in 
+    The "quality score" factors in the increase in firing rate (**f**) due to the merge and a possible increase in
     contamination (**C**), wheighted by a factor **k** (`firing_contamination_balance`).
 
     .. math::
 
         Q = f(1 - (k + 1)C)
 
 
     Parameters
     ----------
     waveform_extractor: WaveformExtractor
         The waveform extractor
-    minimum_spikes: int 
+    minimum_spikes: int
         Minimum number of spikes for each unit to consider a potential merge.
         Enough spikes are needed to estimate the correlogram, by default 1000
     maximum_distance_um: float
         Minimum distance between units for considering a merge, by default 150
     peak_sign: "neg"/"pos"/"both"
         Peak sign used to estimate the maximum channel of a template, by default "neg"
     bin_ms: float
@@ -80,183 +81,202 @@
         Used to compute the refractory period violations aka "contamination", by default 1
     sigma_smooth_ms: float
         Parameters to smooth the correlogram estimation, by default 0.6
     contamination_threshold: float
         Threshold for not taking in account a unit when it is too contaminated, by default 0.2
     adaptative_window_threshold:: float
         Parameter to detect the window size in correlogram estimation, by default 0.5
+    censor_correlograms_ms: float
+        The period to censor on the auto and cross-correlograms, by default 0.15 ms
     num_channels: int
         Number of channel to use for template similarity computation, by default 5
     num_shift: int
         Number of shifts in samles to be explored for template similarity computation, by default 5
     firing_contamination_balance: float
         Parameter to control the balance between firing rate and contamination in computing unit "quality score",
         by default 1.5
     extra_outputs: bool
         If True, an additional dictionary (`outs`) with processed data is returned, by default False
     steps: None or list of str
         which steps to run (gives flexibility to running just some steps)
         If None all steps are done.
         Pontential steps: 'min_spikes', 'remove_contaminated', 'unit_positions', 'correlogram', 'template_similarity',
         'check_increase_score'. Please check steps explanations above!
-        
+
     Returns
     -------
     potential_merges:
         A list of tuples of 2 elements.
         List of pairs that could be merged.
     outs:
         Returned only when extra_outputs=True
         A dictionary that contains data for debugging and plotting.
     """
-    
+    import scipy
+
     we = waveform_extractor
     sorting = we.sorting
     unit_ids = sorting.unit_ids
 
-    # to get fast computation we will not analyse pairs when:
+    # to get fast computation we will not analyse pairs when:
     #    * not enough spikes for one of theses
-    #    * auto correlogram is contaminated
+    #    * auto correlogram is contaminated
     #    * to far away one from each other
 
-    
     if steps is None:
-        steps = ['min_spikes', 'remove_contaminated', 'unit_positions', 'correlogram', 'template_similarity',
-                 'check_increase_score']
+        steps = [
+            "min_spikes",
+            "remove_contaminated",
+            "unit_positions",
+            "correlogram",
+            "template_similarity",
+            "check_increase_score",
+        ]
 
     n = unit_ids.size
-    pair_mask = np.ones((n, n), dtype='bool')
+    pair_mask = np.ones((n, n), dtype="bool")
 
     # STEP 1 :
-    if 'min_spikes' in steps:
+    if "min_spikes" in steps:
         num_spikes = np.array(list(sorting.get_total_num_spikes().values()))
         to_remove = num_spikes < minimum_spikes
         pair_mask[to_remove, :] = False
         pair_mask[:, to_remove] = False
-    
 
     # STEP 2 : remove contaminated auto corr
-    if 'remove_contaminated' in steps:
-        contaminations, nb_violations = compute_refrac_period_violations(we, refractory_period_ms=refractory_period_ms,
-                                                                         censored_period_ms=censored_period_ms)
+    if "remove_contaminated" in steps:
+        contaminations, nb_violations = compute_refrac_period_violations(
+            we, refractory_period_ms=refractory_period_ms, censored_period_ms=censored_period_ms
+        )
         nb_violations = np.array(list(nb_violations.values()))
         contaminations = np.array(list(contaminations.values()))
         to_remove = contaminations > contamination_threshold
         pair_mask[to_remove, :] = False
         pair_mask[:, to_remove] = False
 
     # STEP 3 : unit positions are estimated roughly with channel
-    if 'unit_positions' in steps:
+    if "unit_positions" in steps:
         chan_loc = we.get_channel_locations()
         unit_max_chan = get_template_extremum_channel(we, peak_sign=peak_sign, mode="extremum", outputs="index")
         unit_max_chan = list(unit_max_chan.values())
         unit_locations = chan_loc[unit_max_chan, :]
-        unit_distances = scipy.spatial.distance.cdist(unit_locations, unit_locations, metric='euclidean')
+        unit_distances = scipy.spatial.distance.cdist(unit_locations, unit_locations, metric="euclidean")
         pair_mask = pair_mask & (unit_distances <= maximum_distance_um)
 
     # STEP 4 : potential auto merge by correlogram
-    if 'correlogram' in steps:
-        correlograms, bins = compute_correlograms(sorting, window_ms=window_ms, bin_ms=bin_ms, method='numba')
+    if "correlogram" in steps:
+        correlograms, bins = compute_correlograms(sorting, window_ms=window_ms, bin_ms=bin_ms, method="numba")
+        mask = (bins[:-1] >= -censor_correlograms_ms) & (bins[:-1] < censor_correlograms_ms)
+        correlograms[:, :, mask] = 0
         correlograms_smoothed = smooth_correlogram(correlograms, bins, sigma_smooth_ms=sigma_smooth_ms)
         # find correlogram window for each units
         win_sizes = np.zeros(n, dtype=int)
         for unit_ind in range(n):
             auto_corr = correlograms_smoothed[unit_ind, unit_ind, :]
             thresh = np.max(auto_corr) * adaptative_window_threshold
             win_size = get_unit_adaptive_window(auto_corr, thresh)
             win_sizes[unit_ind] = win_size
-        correlogram_diff = compute_correlogram_diff(sorting, correlograms_smoothed, bins, win_sizes,
-                                                    adaptative_window_threshold=adaptative_window_threshold,
-                                                    pair_mask=pair_mask)
+        correlogram_diff = compute_correlogram_diff(
+            sorting,
+            correlograms_smoothed,
+            bins,
+            win_sizes,
+            adaptative_window_threshold=adaptative_window_threshold,
+            pair_mask=pair_mask,
+        )
         # print(correlogram_diff)
-        pair_mask = pair_mask & (correlogram_diff  < corr_diff_thresh)
+        pair_mask = pair_mask & (correlogram_diff < corr_diff_thresh)
 
     # STEP 5 : check if potential merge with CC also have template similarity
-    if 'template_similarity' in steps:
-        templates = we.get_all_templates(mode='average')
-        templates_diff = compute_templates_diff(sorting, templates, num_channels=num_channels, num_shift=num_shift, pair_mask=pair_mask)        
-        pair_mask = pair_mask & (templates_diff  < template_diff_thresh)
+    if "template_similarity" in steps:
+        templates = we.get_all_templates(mode="average")
+        templates_diff = compute_templates_diff(
+            sorting, templates, num_channels=num_channels, num_shift=num_shift, pair_mask=pair_mask
+        )
+        pair_mask = pair_mask & (templates_diff < template_diff_thresh)
 
     # STEP 6 : validate the potential merges with CC increase the contamination quality metrics
-    if 'check_increase_score' in steps:
-        pair_mask = check_improve_contaminations_score(we, pair_mask, contaminations,
-                                                       firing_contamination_balance, refractory_period_ms,
-                                                       censored_period_ms)
+    if "check_increase_score" in steps:
+        pair_mask, pairs_decreased_score = check_improve_contaminations_score(
+            we, pair_mask, contaminations, firing_contamination_balance, refractory_period_ms, censored_period_ms
+        )
 
     # FINAL STEP : create the final list from pair_mask boolean matrix
     ind1, ind2 = np.nonzero(pair_mask)
     potential_merges = list(zip(unit_ids[ind1], unit_ids[ind2]))
 
     if extra_outputs:
         outs = dict(
             correlograms=correlograms,
             bins=bins,
             correlograms_smoothed=correlograms_smoothed,
             correlogram_diff=correlogram_diff,
             win_sizes=win_sizes,
             templates_diff=templates_diff,
+            pairs_decreased_score=pairs_decreased_score,
         )
         return potential_merges, outs
     else:
         return potential_merges
 
 
-def compute_correlogram_diff(sorting, correlograms_smoothed, bins, win_sizes, adaptative_window_threshold=0.5,
-                             pair_mask=None):
+def compute_correlogram_diff(
+    sorting, correlograms_smoothed, bins, win_sizes, adaptative_window_threshold=0.5, pair_mask=None
+):
     """
     Original author: Aurelien Wyngaard (lussac)
-    
+
     Parameters
     ----------
     sorting: BaseSorting
         The sorting object
     correlograms_smoothed: array 3d
         The 3d array containing all cross and auto correlograms
         (smoothed by a convolution with a gaussian curve)
     bins: array
         Bins of the correlograms
-    win_sized: 
+    win_sized:
         TODO
     adaptative_window_threshold: float
         TODO
     pair_mask: None or boolean array
         A bool matrix of size (num_units, num_units) to select
         which pair to compute.
-    
+
     Returns
     -------
     corr_diff
     """
-    # bin_ms = bins[1] - bins[0] 
-    
+    # bin_ms = bins[1] - bins[0]
+
     unit_ids = sorting.unit_ids
     n = len(unit_ids)
 
     if pair_mask is None:
-        pair_mask = np.ones((n, n), dtype='bool')
+        pair_mask = np.ones((n, n), dtype="bool")
 
     # Index of the middle of the correlograms.
     m = correlograms_smoothed.shape[2] // 2
     num_spikes = sorting.get_total_num_spikes()
 
-    corr_diff = np.full((n, n), np.nan, dtype='float64')
+    corr_diff = np.full((n, n), np.nan, dtype="float64")
     for unit_ind1 in range(n):
         for unit_ind2 in range(unit_ind1 + 1, n):
             if not pair_mask[unit_ind1, unit_ind2]:
                 continue
 
             unit_id1, unit_id2 = unit_ids[unit_ind1], unit_ids[unit_ind2]
 
             num1, num2 = num_spikes[unit_id1], num_spikes[unit_id2]
             # Weighted window (larger unit imposes its window).
-            win_size = int(round((num1 * win_sizes[unit_ind1] + num2 * win_sizes[unit_ind2]) / (num1 + num2)))    
+            win_size = int(round((num1 * win_sizes[unit_ind1] + num2 * win_sizes[unit_ind2]) / (num1 + num2)))
             # Plage of indices where correlograms are inside the window.
             corr_inds = np.arange(m - win_size, m + win_size, dtype=int)
 
-            # TODO : for Aurelien 
+            # TODO : for Aurelien
             shift = 0
             auto_corr1 = normalize_correlogram(correlograms_smoothed[unit_ind1, unit_ind1, :])
             auto_corr2 = normalize_correlogram(correlograms_smoothed[unit_ind2, unit_ind2, :])
             cross_corr = normalize_correlogram(correlograms_smoothed[unit_ind1, unit_ind2, :])
             diff1 = np.sum(np.abs(cross_corr[corr_inds - shift] - auto_corr1[corr_inds])) / len(corr_inds)
             diff2 = np.sum(np.abs(cross_corr[corr_inds - shift] - auto_corr2[corr_inds])) / len(corr_inds)
             # Weighted difference (larger unit imposes its difference).
@@ -285,23 +305,28 @@
     return correlogram if mean == 0 else correlogram / mean
 
 
 def smooth_correlogram(correlograms, bins, sigma_smooth_ms=0.6):
     """
     Smooths cross-correlogram with a Gaussian kernel.
     """
+    import scipy.signal
+
     # OLD implementation : smooth correlogram by low pass filter
     # b, a = scipy.signal.butter(N=2, Wn = correlogram_low_pass / (1e3 / bin_ms /2), btype='low')
     # correlograms_smoothed = scipy.signal.filtfilt(b, a, correlograms, axis=2)
 
-    # new implementation smooth by convolution with a Gaussian kernel
-    smooth_kernel = np.exp( -bins**2 / ( 2 * sigma_smooth_ms **2))
+    # new implementation smooth by convolution with a Gaussian kernel
+    if len(correlograms) == 0:  # fftconvolve will not return the correct shape.
+        return np.empty(correlograms.shape, dtype=np.float64)
+
+    smooth_kernel = np.exp(-(bins**2) / (2 * sigma_smooth_ms**2))
     smooth_kernel /= np.sum(smooth_kernel)
     smooth_kernel = smooth_kernel[None, None, :]
-    correlograms_smoothed = scipy.signal.fftconvolve(correlograms, smooth_kernel, mode='same', axes=2)
+    correlograms_smoothed = scipy.signal.fftconvolve(correlograms, smooth_kernel, mode="same", axes=2)
 
     return correlograms_smoothed
 
 
 def get_unit_adaptive_window(auto_corr: np.ndarray, threshold: float):
     """
     Computes an adaptive window to correlogram (basically corresponds to the first peak).
@@ -316,28 +341,30 @@
         Minimum threshold of correlogram (all peaks under this threshold are discarded).
 
     Returns
     -------
     unit_window (int):
         Index at which the adaptive window has been calculated.
     """
+    import scipy.signal
+
     if np.sum(np.abs(auto_corr)) == 0:
         return 20.0
 
     derivative_2 = -np.gradient(np.gradient(auto_corr))
     peaks = scipy.signal.find_peaks(derivative_2)[0]
 
     keep = auto_corr[peaks] >= threshold
     peaks = peaks[keep]
     keep = peaks < (auto_corr.shape[0] // 2)
     peaks = peaks[keep]
 
     if peaks.size == 0:
         # If none of the peaks crossed the threshold, redo with threshold/2.
-        return get_unit_adaptive_window(auto_corr, threshold/2)
+        return get_unit_adaptive_window(auto_corr, threshold / 2)
 
     # keep the last peak (nearest to center)
     win_size = auto_corr.shape[0] // 2 - peaks[-1]
 
     return win_size
 
 
@@ -364,73 +391,76 @@
     templates_diff: np.array
         2D array with template differences
     """
     unit_ids = sorting.unit_ids
     n = len(unit_ids)
 
     if pair_mask is None:
-        pair_mask = np.ones((n, n), dtype='bool')
+        pair_mask = np.ones((n, n), dtype="bool")
 
-    templates_diff = np.full((n, n), np.nan, dtype='float64')
+    templates_diff = np.full((n, n), np.nan, dtype="float64")
     for unit_ind1 in range(n):
         for unit_ind2 in range(unit_ind1 + 1, n):
             if not pair_mask[unit_ind1, unit_ind2]:
                 continue
-            
+
             template1 = templates[unit_ind1]
             template2 = templates[unit_ind2]
             # take best channels
             chan_inds = np.argsort(np.max(np.abs(template1 + template2), axis=0))[::-1][:num_channels]
             template1 = template1[:, chan_inds]
             template2 = template2[:, chan_inds]
 
             num_samples = template1.shape[0]
             norm = np.sum(np.abs(template1)) + np.sum(np.abs(template2))
             all_shift_diff = []
-            for shift in range(-num_shift, num_shift+1):
-                temp1 = template1[num_shift:num_samples - num_shift, :]
-                temp2 = template2[num_shift + shift:num_samples - num_shift + shift, :]
+            for shift in range(-num_shift, num_shift + 1):
+                temp1 = template1[num_shift : num_samples - num_shift, :]
+                temp2 = template2[num_shift + shift : num_samples - num_shift + shift, :]
                 d = np.sum(np.abs(temp1 - temp2)) / (norm)
                 all_shift_diff.append(d)
             templates_diff[unit_ind1, unit_ind2] = np.min(all_shift_diff)
 
     return templates_diff
 
 
 class MockWaveformExtractor:
     """
-    Mock WaveformExtractor to be able to run compute_refrac_period_violations() 
+    Mock WaveformExtractor to be able to run compute_refrac_period_violations()
     needed for the auto_merge() function.
     """
+
     def __init__(self, recording, sorting):
         self.recording = recording
         self.sorting = sorting
 
     def get_total_samples(self):
         return self.recording.get_total_samples()
 
     def get_total_duration(self):
         return self.recording.get_total_duration()
 
 
-def check_improve_contaminations_score(we, pair_mask, contaminations, firing_contamination_balance,
-                                       refractory_period_ms, censored_period_ms):
+def check_improve_contaminations_score(
+    we, pair_mask, contaminations, firing_contamination_balance, refractory_period_ms, censored_period_ms
+):
     """
     Check that the score is improve afeter a potential merge
 
     The score is a balance between:
       * contamination decrease
       * firing increase
 
-    Check that the contamination score is improved (decrease)  after 
+    Check that the contamination score is improved (decrease)  after
     a potential merge
     """
     recording = we.recording
     sorting = we.sorting
     pair_mask = pair_mask.copy()
+    pairs_removed = []
 
     firing_rates = list(compute_firing_rates(we).values())
 
     inds1, inds2 = np.nonzero(pair_mask)
     for i in range(inds1.size):
         ind1, ind2 = inds1[i], inds2[i]
 
@@ -438,27 +468,31 @@
         c_2 = contaminations[ind2]
 
         f_1 = firing_rates[ind1]
         f_2 = firing_rates[ind2]
 
         # make a merged sorting and tale one unit (unit_id1 is used)
         unit_id1, unit_id2 = sorting.unit_ids[ind1], sorting.unit_ids[ind2]
-        sorting_merged = MergeUnitsSorting(sorting, [[unit_id1, unit_id2]], new_unit_ids=[unit_id1]).select_units([unit_id1])
+        sorting_merged = MergeUnitsSorting(
+            sorting, [[unit_id1, unit_id2]], new_unit_ids=[unit_id1], delta_time_ms=censored_period_ms
+        ).select_units([unit_id1])
         # make a lazy fake WaveformExtractor to compute contamination and firing rate
         we_new = MockWaveformExtractor(recording, sorting_merged)
 
-        new_contaminations, _ = compute_refrac_period_violations(we_new, refractory_period_ms=refractory_period_ms,
-                                    censored_period_ms=censored_period_ms)
+        new_contaminations, _ = compute_refrac_period_violations(
+            we_new, refractory_period_ms=refractory_period_ms, censored_period_ms=censored_period_ms
+        )
         c_new = new_contaminations[unit_id1]
         f_new = compute_firing_rates(we_new)[unit_id1]
 
         # old and new scores
         k = 1 + firing_contamination_balance
-        score_1 = f_1 * ( 1 - k * c_1)
-        score_2 = f_2 * ( 1 - k * c_2)
-        score_new = f_new * ( 1 - k * c_new)
+        score_1 = f_1 * (1 - k * c_1)
+        score_2 = f_2 * (1 - k * c_2)
+        score_new = f_new * (1 - k * c_new)
 
         if score_new < score_1 or score_new < score_2:
             # the score is not improved
             pair_mask[ind1, ind2] = False
+            pairs_removed.append((sorting.unit_ids[ind1], sorting.unit_ids[ind2]))
 
-    return pair_mask
+    return pair_mask, pairs_removed
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `spikeinterface-0.97.1/spikeinterface/curation/curation_tools.py` & `spikeinterface-0.98.0/src/spikeinterface/curation/curation_tools.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,95 +1,99 @@
 from typing import Optional
 import numpy as np
 
 
 try:
     import numba
+
     HAVE_NUMBA = True
 except ModuleNotFoundError as err:
     HAVE_NUMBA = False
 
 
-def _find_duplicated_spikes_numpy(spike_train: np.ndarray, censored_period: int, seed: Optional[int] = None,
-                                  method: str = "keep_first") -> np.ndarray:
-    indices_of_duplicates, = np.where(np.diff(spike_train) <= censored_period)
+def _find_duplicated_spikes_numpy(
+    spike_train: np.ndarray, censored_period: int, seed: Optional[int] = None, method: str = "keep_first"
+) -> np.ndarray:
+    (indices_of_duplicates,) = np.where(np.diff(spike_train) <= censored_period)
 
     if method == "keep_first":
         indices_of_duplicates += 1
     elif method == "random":
         rand_state = np.random.get_state()
         np.random.seed(seed)
         mask = np.ones(len(spike_train), dtype=bool)
 
-        while(np.sum(np.diff(spike_train[mask]) <= censored_period) > 0):
+        while np.sum(np.diff(spike_train[mask]) <= censored_period) > 0:
             shift = np.random.randint(low=0, high=2, size=len(indices_of_duplicates))
-            mask[indices_of_duplicates+shift] = False
+            mask[indices_of_duplicates + shift] = False
         np.random.set_state(rand_state)
 
-        indices_of_duplicates, = np.where(~mask)
+        (indices_of_duplicates,) = np.where(~mask)
     elif method != "keep_last":
         raise ValueError(f"Method '{method}' isn't a valid method for _find_duplicated_spikes_numpy.")
 
     return indices_of_duplicates
 
 
 def _find_duplicated_spikes_random(spike_train: np.ndarray, censored_period: int, seed: int) -> np.ndarray:
     # random seed
     rng = np.random.RandomState(seed=seed)
 
     indices_of_duplicates = []
     while not np.all(np.diff(np.delete(spike_train, indices_of_duplicates)) > censored_period):
-        duplicates, = np.where(np.diff(spike_train) <= censored_period)
+        (duplicates,) = np.where(np.diff(spike_train) <= censored_period)
         duplicates = np.unique(np.concatenate((duplicates, duplicates + 1)))
         duplicate = rng.choice(duplicates)
         indices_of_duplicates.append(duplicate)
 
     return np.array(indices_of_duplicates, dtype=np.int64)
 
 
-
 if HAVE_NUMBA:
+
     @numba.jit((numba.int64[::1], numba.int32), nopython=True, nogil=True, cache=True)
     def _find_duplicated_spikes_keep_first_iterative(spike_train, censored_period):
         indices_of_duplicates = numba.typed.List()
         N = len(spike_train)
 
-        for i in range(N-1):
+        for i in range(N - 1):
             if i in indices_of_duplicates:
                 continue
 
-            for j in range(i+1, N):
+            for j in range(i + 1, N):
                 if spike_train[j] - spike_train[i] > censored_period:
                     break
                 indices_of_duplicates.append(j)
 
         return np.asarray(indices_of_duplicates)
 
     @numba.jit((numba.int64[::1], numba.int32), nopython=True, nogil=True, cache=True)
     def _find_duplicated_spikes_keep_last_iterative(spike_train, censored_period):
         indices_of_duplicates = numba.typed.List()
         N = len(spike_train)
 
-        for i in range(N-1, 0, -1):
+        for i in range(N - 1, 0, -1):
             if i in indices_of_duplicates:
                 continue
 
-            for j in range(i-1, -1, -1):
+            for j in range(i - 1, -1, -1):
                 if spike_train[i] - spike_train[j] > censored_period:
                     break
                 indices_of_duplicates.append(j)
 
         return np.asarray(indices_of_duplicates)
 
 
-def find_duplicated_spikes(spike_train, censored_period: int, method: str = "random", seed: Optional[int] = None) -> np.ndarray:
+def find_duplicated_spikes(
+    spike_train, censored_period: int, method: str = "random", seed: Optional[int] = None
+) -> np.ndarray:
     """
-    Finds the indices where there a spike in considered a duplicate.
+    Finds the indices where spikes should be considered duplicates.
     When two spikes are closer together than the censored period,
-    one of them taken out based on the method provided.
+    one of them is taken out based on the method provided.
 
     Parameters
     ----------
     spike_train: np.ndarray
         The spike train on which to look for duplicated spikes.
     censored_period: int
         The censored period for duplicates (in sample time).
@@ -113,8 +117,7 @@
         assert HAVE_NUMBA, "'keep_first' method requires numba. Install it with >>> pip install numba"
         return _find_duplicated_spikes_keep_first_iterative(spike_train.astype(np.int64), censored_period)
     elif method == "keep_last_iterative":
         assert HAVE_NUMBA, "'keep_last' method requires numba. Install it with >>> pip install numba"
         return _find_duplicated_spikes_keep_last_iterative(spike_train.astype(np.int64), censored_period)
     else:
         raise ValueError(f"Method '{method}' isn't a valid method for find_duplicated_spikes.")
-
```

### Comparing `spikeinterface-0.97.1/spikeinterface/curation/curationsorting.py` & `spikeinterface-0.98.0/src/spikeinterface/curation/curationsorting.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,181 +1,194 @@
 from .mergeunitssorting import MergeUnitsSorting
 from .splitunitsorting import SplitUnitSorting
 from collections import namedtuple
 import numpy as np
-node_t = namedtuple('node_t', 'unit_id stage_id')
+
+node_t = namedtuple("node_t", "unit_id stage_id")
+
 
 class CurationSorting:
     """
     Class that handles curation of a Sorting object.
 
     Parameters
     ----------
     parent_sorting: Recording
         The recording object
     properties_policy: str
         Policy used to propagate properties after split and merge operation. If 'keep' the properties will be
-        passed to the new units (if the original units have the same value). If 'remove' the new units will have 
+        passed to the new units (if the original units have the same value). If 'remove' the new units will have
         an empty value for all the properties. Default: 'keep'
     make_graph: bool
-        True to keep a networkx graph with the curation history
+        True to keep a Networkx graph instance with the curation history
     Returns
     -------
     sorting: Sorting
         Sorting object with the selected units merged
     """
 
-
-    def __init__(self, parent_sorting, make_graph=False, properties_policy='keep'):
-        #to allow undo and redo a list of sortingextractors is keep
+    def __init__(self, parent_sorting, make_graph=False, properties_policy="keep"):
+        # to allow undo and redo a list of sortingextractors is keep
         self._sorting_stages = [parent_sorting]
         self._sorting_stages_i = 0
         self._properties_policy = properties_policy
         parent_units = parent_sorting.get_unit_ids()
         self._make_graph = make_graph
         if make_graph:
-            #to easily allow undo and redo a list of graphs with the history of the curation is keep
+            # to easily allow undo and redo a list of graphs with the history of the curation is keep
             import networkx as nx
+
             self._nx = nx
             self._graphs = [self._nx.DiGraph()]
-            self._graphs[0].add_nodes_from([node_t(u,0) for u in parent_units])
-        # check the maximum numeric id used, strings with digits will be casted to int to reduce confusion
+            self._graphs[0].add_nodes_from([node_t(u, 0) for u in parent_units])
+        # check the maximum numeric id used, strings with digits will be cast to int to reduce confusion
         if np.issubdtype(parent_units.dtype, np.character):
-            self.max_used_id = max([-1]+[int(p) for p in parent_units if p.isdigit()])
+            self.max_used_id = max([-1] + [int(p) for p in parent_units if p.isdigit()])
         else:
-            self.max_used_id = max(parent_units)
-        
-        self._kwargs = dict(parent_sorting=parent_sorting.to_dict(), make_graph=make_graph, properties_policy=properties_policy)
+            self.max_used_id = max(parent_units) if len(parent_units) > 0 else 0
+
+        self._kwargs = dict(parent_sorting=parent_sorting, make_graph=make_graph, properties_policy=properties_policy)
 
     def _get_unused_id(self, n=1):
-        #check units in the graph to the next unused unit id
-        ids =[self.max_used_id+i for i in range(1,1+n)]
-        if  np.issubdtype(self.sorting.get_unit_ids().dtype, np.character):
+        # check units in the graph to the next unused unit id
+        ids = [self.max_used_id + i for i in range(1, 1 + n)]
+        if np.issubdtype(self.sorting.get_unit_ids().dtype, np.character):
             ids = [str(i) for i in ids]
         return ids
 
-    def split(self,split_unit_id, indices_list):
+    def split(self, split_unit_id, indices_list):
         current_sorting = self._sorting_stages[self._sorting_stages_i]
         new_unit_ids = self._get_unused_id(2)
-        new_sorting = SplitUnitSorting(current_sorting, split_unit_id=split_unit_id, indices_list=indices_list, new_unit_ids=new_unit_ids,properties_policy=self._properties_policy)
+        new_sorting = SplitUnitSorting(
+            current_sorting,
+            split_unit_id=split_unit_id,
+            indices_list=indices_list,
+            new_unit_ids=new_unit_ids,
+            properties_policy=self._properties_policy,
+        )
 
         if self._make_graph:
             units = current_sorting.get_unit_ids()
             i = self._sorting_stages_i
-            edges = [(node_t(u,i),node_t(u,i+1)) for u in units if u !=split_unit_id]
-            edges = edges + [(node_t(split_unit_id,i), node_t(u,i+1)) for u in new_unit_ids]
+            edges = [(node_t(u, i), node_t(u, i + 1)) for u in units if u != split_unit_id]
+            edges = edges + [(node_t(split_unit_id, i), node_t(u, i + 1)) for u in new_unit_ids]
         else:
             edges = None
-        self.max_used_id = self.max_used_id+2
+        self.max_used_id = self.max_used_id + 2
         self._add_new_stage(new_sorting, edges)
 
     def merge(self, units_to_merge, new_unit_id=None, delta_time_ms=0.4):
         current_sorting = self._sorting_stages[self._sorting_stages_i]
         if new_unit_id is None:
             new_unit_id = self._get_unused_id()[0]
-        else:
+        elif new_unit_id not in units_to_merge:
             assert new_unit_id not in current_sorting.unit_ids, f"new_unit_id already exists!"
-        new_sorting = MergeUnitsSorting(parent_sorting=current_sorting, units_to_merge=units_to_merge, 
-                new_unit_ids=[new_unit_id], delta_time_ms=delta_time_ms, properties_policy=self._properties_policy)
+        new_sorting = MergeUnitsSorting(
+            parent_sorting=current_sorting,
+            units_to_merge=units_to_merge,
+            new_unit_ids=[new_unit_id],
+            delta_time_ms=delta_time_ms,
+            properties_policy=self._properties_policy,
+        )
         if self._make_graph:
             units = current_sorting.get_unit_ids()
             i = self._sorting_stages_i
-            edges = [(node_t(u,i),node_t(u,i+1)) for u in units if u not in units_to_merge]
-            edges = edges + [(node_t(u,i),node_t(new_unit_id,i+1)) for u in units_to_merge]
+            edges = [(node_t(u, i), node_t(u, i + 1)) for u in units if u not in units_to_merge]
+            edges = edges + [(node_t(u, i), node_t(new_unit_id, i + 1)) for u in units_to_merge]
         else:
             edges = None
         self.max_used_id = self.max_used_id + 1
         self._add_new_stage(new_sorting, edges)
 
     def remove_units(self, unit_ids):
         current_sorting = self._sorting_stages[self._sorting_stages_i]
         unit2keep = [u for u in current_sorting.get_unit_ids() if u not in unit_ids]
         if self._make_graph:
             i = self._sorting_stages_i
-            edges = [(node_t(u,i),node_t(u,i+1)) for u in unit2keep]
+            edges = [(node_t(u, i), node_t(u, i + 1)) for u in unit2keep]
         else:
             edges = None
-        self._add_new_stage(current_sorting.select_units(unit2keep),edges)
+        self._add_new_stage(current_sorting.select_units(unit2keep), edges)
 
     def remove_unit(self, unit_id):
         self.remove_units([unit_id])
 
     def select_units(self, unit_ids, renamed_unit_ids=None):
         new_sorting = self._sorting_stages[self._sorting_stages_i].select_units(unit_ids, renamed_unit_ids)
         if self._make_graph:
             i = self._sorting_stages_i
             if renamed_unit_ids is None:
-                edges = [(node_t(u,i), node_t(u,i+1)) for u in unit_ids] 
+                edges = [(node_t(u, i), node_t(u, i + 1)) for u in unit_ids]
             else:
-                edges = [(node_t(u,i), node_t(v,i+1)) for u,v in zip(unit_ids, renamed_unit_ids)] 
+                edges = [(node_t(u, i), node_t(v, i + 1)) for u, v in zip(unit_ids, renamed_unit_ids)]
         else:
-            edges = None        
+            edges = None
         self._add_new_stage(new_sorting, edges)
-    
+
     def rename(self, renamed_unit_ids):
         self.select_units(self.current_sorting.unit_ids, renamed_unit_ids=renamed_unit_ids)
 
     def _add_new_stage(self, new_sorting, edges):
-        #adds the stage to the stage list and creates the associated new graph
-        self._sorting_stages = self._sorting_stages[0:self._sorting_stages_i+1]
+        # adds the stage to the stage list and creates the associated new graph
+        self._sorting_stages = self._sorting_stages[0 : self._sorting_stages_i + 1]
         self._sorting_stages.append(new_sorting)
         if self._make_graph:
-            self._graphs = self._graphs[0:self._sorting_stages_i+1]
+            self._graphs = self._graphs[0 : self._sorting_stages_i + 1]
             new_graph = self._graphs[self._sorting_stages_i].copy()
             new_graph.add_edges_from(edges)
             self._graphs.append(new_graph)
         self._sorting_stages_i += 1
 
     def remove_empty_units(self):
         i = self._sorting_stages_i
         new_sorting = self._sorting_stages[i].remove_empty_units()
         if self._make_graph:
             curr_ids = self._sorting_stages[i].get_unit_ids()
-            edges = [(node_t(u,i), node_t(u,i+1)) for u in curr_ids] 
+            edges = [(node_t(u, i), node_t(u, i + 1)) for u in curr_ids]
         else:
-            edges = None        
-        self._add_new_stage(new_sorting, edges)        
+            edges = None
+        self._add_new_stage(new_sorting, edges)
 
     def redo_avaiable(self):
-        #useful function for a gui
+        # useful function for a gui
         return self._sorting_stages_i < len(self._sorting_stages)
-    
+
     def undo_avaiable(self):
-        #useful function for a gui
+        # useful function for a gui
         return self._sorting_stages_i > 0
 
     def undo(self):
         if self.undo_avaiable():
-            self._sorting_stages_i -=1
-    
+            self._sorting_stages_i -= 1
+
     def redo(self):
         if self.redo_avaiable():
-            self._sorting_stages_i +=1
+            self._sorting_stages_i += 1
 
     def draw_graph(self, **kwargs):
-        assert self._make_graph, 'to make graph make_graph=True'
+        assert self._make_graph, "to make a graph make_graph=True"
         graph = self.graph
         ids = [c.unit_id for c in graph.nodes]
-        pos = {n:(n.stage_id, -ids.index(n.unit_id)) for n in graph.nodes}
-        labels = {n:str(n.unit_id) for n in graph.nodes}
+        pos = {n: (n.stage_id, -ids.index(n.unit_id)) for n in graph.nodes}
+        labels = {n: str(n.unit_id) for n in graph.nodes}
         self._nx.draw(graph, pos=pos, labels=labels, **kwargs)
 
     @property
     def graph(self):
-        assert self._make_graph, 'to have a graph make_graph=True'
+        assert self._make_graph, "to have a graph make_graph=True"
         return self._graphs[self._sorting_stages_i]
 
     @property
     def sorting(self):
         return self.current_sorting
 
     @property
     def current_sorting(self):
         return self._sorting_stages[self._sorting_stages_i]
 
     # def __getattr__(self,name):
-    #     #any method not define for this class will try to use the current 
+    #     #any method not define for this class will try to use the current
     #     #  sorting stage. In that whay this class will behave as a sortingextractor
     #     current_sorting = self._sorting_stages[self._sorting_stages_i]
 
     #     attr = object.__getattribute__(current_sorting, name)
     #     return attr
```

### Comparing `spikeinterface-0.97.1/spikeinterface/curation/mergeunitssorting.py` & `spikeinterface-0.98.0/src/spikeinterface/curation/mergeunitssorting.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,182 +1,191 @@
 from typing import List, Union
 import numpy as np
 from spikeinterface.core.basesorting import BaseSorting, BaseSortingSegment
 from copy import deepcopy
 
+
 class MergeUnitsSorting(BaseSorting):
     """
-    Class that handles several merges of units from a Sorting object based on a list of list of unit_ids.
+    Class that handles several merges of units from a Sorting object based on a list of lists of unit_ids.
 
     Parameters
     ----------
     parent_sorting: Recording
         The sorting object
     units_to_merge: list of lists
         A list of lists for every merge group. Each element needs to have at least two elements (two units to merge),
         but it can also have more (merge multiple units at once).
     new_unit_ids: None or list
         A new unit_ids for merged units. If given, it needs to have the same length as `units_to_merge`
     properties_policy: str ('keep', 'remove')
-        Policy used to propagate propierties. If 'keep' the properties will be pass to the new units
-         (if the units_to_merge have the same value). If 'remove' the new units will have an empty 
+        Policy used to propagate properties. If 'keep' the properties will be passed to the new units
+         (if the units_to_merge have the same value). If 'remove' the new units will have an empty
          value for all the properties of the new unit.
          Default: 'keep'
     delta_time_ms: float or None
-        Number of ms to consider duplicated spikes. None to don't check duplications
+        Number of ms to consider for duplicated spikes. None won't check for duplications
     Returns
     -------
     sorting: Sorting
         Sorting object with the selected units merged
     """
 
-    def __init__(self, parent_sorting, units_to_merge, new_unit_ids=None, properties_policy='keep', delta_time_ms=0.4):
+    def __init__(self, parent_sorting, units_to_merge, new_unit_ids=None, properties_policy="keep", delta_time_ms=0.4):
         self._parent_sorting = parent_sorting
-        
+
         if not isinstance(units_to_merge[0], list):
             # keep backward compatibility : the previous behavior was only one merge
             units_to_merge = [units_to_merge]
-        
+
         num_merge = len(units_to_merge)
 
         parents_unit_ids = parent_sorting.unit_ids
         sampling_frequency = parent_sorting.get_sampling_frequency()
-        
+
         all_removed_ids = []
         for ids in units_to_merge:
             all_removed_ids.extend(ids)
         keep_unit_ids = [u for u in parents_unit_ids if u not in all_removed_ids]
 
         if new_unit_ids is None:
             dtype = parents_unit_ids.dtype
-            #select new_units_ids grater that the max id, event greater than the numerical str ids 
+            # select new_units_ids greater that the max id, event greater than the numerical str ids
             if np.issubdtype(dtype, np.character):
                 # dtype str
                 if all(p.isdigit() for p in parents_unit_ids):
                     # All str are digit : we can generate a max
                     m = max(int(p) for p in parents_unit_ids) + 1
-                    new_unit_ids = [str(m + i ) for i in range(num_merge)]
+                    new_unit_ids = [str(m + i) for i in range(num_merge)]
                 else:
                     # we cannot automatically find new names
-                    new_unit_ids = [ f'merge{i}' for i in range(num_merge)]
+                    new_unit_ids = [f"merge{i}" for i in range(num_merge)]
                     if np.any(np.in1d(new_unit_ids, keep_unit_ids)):
-                        raise ValueError("Unable to find 'new_unit_ids' because it is a string and parents "
-                                         "already contain merges. Pass a list of 'new_unit_ids' as an argument.")
+                        raise ValueError(
+                            "Unable to find 'new_unit_ids' because it is a string and parents "
+                            "already contain merges. Pass a list of 'new_unit_ids' as an argument."
+                        )
             else:
                 # dtype int
                 new_unit_ids = list(max(parents_unit_ids) + 1 + np.arange(num_merge, dtype=dtype))
         else:
             if np.any(np.in1d(new_unit_ids, keep_unit_ids)):
-                raise ValueError("'new_unit_ids' are already exesting in the sorting.unit_ids. Provide new ones")
-        
-        assert len(new_unit_ids) == num_merge, 'new_unit_ids must have the same size as units_to_merge'
+                raise ValueError("'new_unit_ids' already exist in the sorting.unit_ids. Provide new ones")
+
+        assert len(new_unit_ids) == num_merge, "new_unit_ids must have the same size as units_to_merge"
 
         # some checks
         for ids in units_to_merge:
-            assert all(u in parents_unit_ids for u in ids), 'units to merge are not all in parent'
-        assert properties_policy in ('keep', 'remove'), 'properties_policy must be ''keep'' or ''remove'''
-        
+            assert all(u in parents_unit_ids for u in ids), "units to merge are not all in parent"
+        assert properties_policy in ("keep", "remove"), "properties_policy must be " "keep" " or " "remove" ""
+
         # new units are put at the end
-        units_ids = keep_unit_ids + new_unit_ids 
+        units_ids = keep_unit_ids + new_unit_ids
         BaseSorting.__init__(self, sampling_frequency, units_ids)
-        #assert all(np.isin(keep_unit_ids, self.unit_ids)), 'new_unit_id should have a compatible format with the parent ids'
-        
+        # assert all(np.isin(keep_unit_ids, self.unit_ids)), 'new_unit_id should have a compatible format with the parent ids'
+
         if delta_time_ms is None:
             rm_dup_delta = None
         else:
             rm_dup_delta = int(delta_time_ms / 1000 * sampling_frequency)
         for parent_segment in self._parent_sorting._sorting_segments:
             sub_segment = MergeUnitsSortingSegment(parent_segment, units_to_merge, new_unit_ids, rm_dup_delta)
             self.add_sorting_segment(sub_segment)
 
         ann_keys = parent_sorting._annotations.keys()
         self._annotations = deepcopy({k: parent_sorting._annotations[k] for k in ann_keys})
-                
-        #copy properties for unchanged units, and check if units propierties are the same
+
+        # copy properties for unchanged units, and check if units propierties are the same
         keep_parent_inds = parent_sorting.ids_to_indices(keep_unit_ids)
-        #~ all_removed_inds = parent_sorting.ids_to_indices(all_removed_ids)
+        # ~ all_removed_inds = parent_sorting.ids_to_indices(all_removed_ids)
         keep_inds = self.ids_to_indices(keep_unit_ids)
-        #~ merge_inds = self.ids_to_indices(new_unit_ids)
+        # ~ merge_inds = self.ids_to_indices(new_unit_ids)
         prop_keys = parent_sorting._properties.keys()
         for k in prop_keys:
             parent_values = parent_sorting._properties[k]
-            
-            if properties_policy=='keep':
+
+            if properties_policy == "keep":
                 # propagate keep values
                 new_values = np.empty(shape=len(units_ids), dtype=parent_values.dtype)
                 new_values[keep_inds] = parent_values[keep_parent_inds]
-                for new_id, ids in zip(new_unit_ids, units_to_merge): 
+                for new_id, ids in zip(new_unit_ids, units_to_merge):
                     removed_inds = parent_sorting.ids_to_indices(ids)
                     merge_values = parent_values[removed_inds]
-                    if all(merge_values==merge_values[0]):
+                    if all(merge_values == merge_values[0]):
                         # and new values only if they are all similar
                         ind = self.id_to_index(new_id)
                         new_values[ind] = merge_values[0]
                 self.set_property(k, new_values)
 
-            elif properties_policy=='remove':
+            elif properties_policy == "remove":
                 self.set_property(k, parent_values[keep_parent_inds], keep_unit_ids)
 
         if parent_sorting.has_recording():
             self.register_recording(parent_sorting._recording)
-        
+
         # make it jsonable
         units_to_merge = [list(e) for e in units_to_merge]
-        self._kwargs = dict(parent_sorting=parent_sorting.to_dict(), units_to_merge=units_to_merge,
-                            new_unit_id=new_unit_ids, properties_policy=properties_policy, delta_time_ms=delta_time_ms)
-
+        self._kwargs = dict(
+            parent_sorting=parent_sorting,
+            units_to_merge=units_to_merge,
+            new_unit_ids=new_unit_ids,
+            properties_policy=properties_policy,
+            delta_time_ms=delta_time_ms,
+        )
 
 
 class MergeUnitsSortingSegment(BaseSortingSegment):
     def __init__(self, parent_segment, units_to_merge, new_unit_ids, rm_dup_delta):
         BaseSortingSegment.__init__(self)
         self._parent_segment = parent_segment
         self._units_to_merge = units_to_merge
         self.new_unit_ids = new_unit_ids
         self._dup_delta = rm_dup_delta
         # if cache compute
         self._merged_spike_times = []
         for ids in units_to_merge:
-            spike_times = get_non_duplicated_events([self._parent_segment.get_unit_spike_train(u, None, None) for u in ids], rm_dup_delta)
+            spike_times = get_non_duplicated_events(
+                [self._parent_segment.get_unit_spike_train(u, None, None) for u in ids], rm_dup_delta
+            )
             self._merged_spike_times.append(spike_times)
-        
-    def get_unit_spike_train(self,
-                             unit_id,
-                             start_frame: Union[int, None] = None,
-                             end_frame: Union[int, None] = None,
-                             ) -> np.ndarray:
-        
+
+    def get_unit_spike_train(
+        self,
+        unit_id,
+        start_frame: Union[int, None] = None,
+        end_frame: Union[int, None] = None,
+    ) -> np.ndarray:
         if unit_id in self.new_unit_ids:
             ind = self.new_unit_ids.index(unit_id)
             spike_times = self._merged_spike_times[ind]
 
             if start_frame is not None:
-                start_i = np.searchsorted(spike_times, start_frame, side='left')
+                start_i = np.searchsorted(spike_times, start_frame, side="left")
             else:
                 start_i = 0
             if end_frame is not None:
-                end_i = np.searchsorted(spike_times, start_frame, side='right')
+                end_i = np.searchsorted(spike_times, start_frame, side="right")
             else:
                 end_i = len(spike_times)
             return spike_times[start_i:end_i]
         else:
             spike_times = self._parent_segment.get_unit_spike_train(unit_id, start_frame, end_frame)
             return spike_times
 
-#TODO move this function to postprocessing or similar
-def get_non_duplicated_events(times_list, delta): 
 
+# TODO move this function to postprocessing or similar
+def get_non_duplicated_events(times_list, delta):
     times_concat = np.concatenate(times_list)
-    if len(times_concat)==0:
+    if len(times_concat) == 0:
         return times_concat
-    indices = times_concat.argsort(kind='mergesort')
+    indices = times_concat.argsort(kind="mergesort")
     times_concat_sorted = times_concat[indices]
-    
+
     if delta is None:
         return times_concat_sorted
-    membership = np.concatenate([np.ones(t.shape)*i for i,t in enumerate(times_list)])    
-    membership_sorted = membership[indices]   
-    
+    membership = np.concatenate([np.ones(t.shape) * i for i, t in enumerate(times_list)])
+    membership_sorted = membership[indices]
+
     inds = np.nonzero((np.diff(times_concat_sorted) > delta) | (np.diff(membership_sorted) == 0))[0]
-    #always add the first one and realing counting
-    inds = np.concatenate([[0],inds+1])
+    # always add the first one and realing counting
+    inds = np.concatenate([[0], inds + 1])
     return times_concat_sorted[inds]
```

### Comparing `spikeinterface-0.97.1/spikeinterface/curation/remove_duplicated_spikes.py` & `spikeinterface-0.98.0/src/spikeinterface/curation/remove_duplicated_spikes.py`

 * *Files 11% similar despite different names*

```diff
@@ -5,82 +5,89 @@
 from spikeinterface.curation.curation_tools import find_duplicated_spikes
 
 
 class RemoveDuplicatedSpikesSorting(BaseSorting):
     """
     Class to remove duplicated spikes from the spike trains.
     Spikes are considered duplicated if they are less than x
-    ms appart where x is the censored period.
+    ms apart where x is the censored period.
 
     Parameters
     ----------
     sorting: BaseSorting
         The parent sorting.
     censored_period_ms: float
         The censored period to consider 2 spikes to be duplicated (in ms).
     method: str in ("keep_first", "keep_last", "keep_first_iterative', 'keep_last_iterative", random")
         Method used to remove the duplicated spikes.
         If method = "random", will randomly choose to remove the first or last spike.
         If method = "keep_first", for each ISI violation, will remove the second spike.
         If method = "keep_last", for each ISI violation, will remove the first spike.
         If method = "keep_first_iterative", will iteratively keep the first spike and remove the following violations.
         If method = "keep_last_iterative", does the same as "keep_first_iterative" but starting from the end.
-        In the iterative methods, if there is a triplet A, B, C where (A, B) and (A, C) are in the censored period
-        (but not (A, C)), then only B is removed. In the non iterative method however, only one spike remains.
+        In the iterative methods, if there is a triplet A, B, C where (A, B) and (B, C) are in the censored period
+        (but not (A, C)), then only B is removed. In the non iterative methods however, only one spike remains.
 
     Returns
     -------
     sorting_without_duplicated_spikes: Remove_DuplicatedSpikesSorting
         The sorting without any duplicated spikes.
     """
 
     def __init__(self, sorting: BaseSorting, censored_period_ms: float = 0.3, method: str = "keep_first") -> None:
         super().__init__(sorting.get_sampling_frequency(), sorting.unit_ids)
         censored_period = int(round(censored_period_ms * 1e-3 * sorting.get_sampling_frequency()))
         seed = np.random.randint(low=0, high=np.iinfo(np.int32).max)
 
         for segment in sorting._sorting_segments:
-            self.add_sorting_segment(RemoveDuplicatedSpikesSortingSegment(segment, censored_period,
-                                                                          sorting.unit_ids, method, seed))
+            self.add_sorting_segment(
+                RemoveDuplicatedSpikesSortingSegment(segment, censored_period, sorting.unit_ids, method, seed)
+            )
 
         sorting.copy_metadata(self, only_main=False)
         if sorting.has_recording():
             self.register_recording(sorting._recording)
 
-        self._kwargs = {
-            'sorting': sorting.to_dict(),
-            'censored_period_ms': censored_period_ms,
-            'method': method
-        }
+        self._kwargs = {"sorting": sorting, "censored_period_ms": censored_period_ms, "method": method}
 
 
 class RemoveDuplicatedSpikesSortingSegment(BaseSortingSegment):
-
-    def __init__(self, parent_segment: BaseSortingSegment, censored_period: int, unit_ids, method: str, seed: Optional[int] = None) -> None:
+    def __init__(
+        self,
+        parent_segment: BaseSortingSegment,
+        censored_period: int,
+        unit_ids,
+        method: str,
+        seed: Optional[int] = None,
+    ) -> None:
         super().__init__()
         self._parent_segment = parent_segment
         self._duplicated_spikes = {}
 
         for unit_id in unit_ids:
-            self._duplicated_spikes[unit_id] = \
-                find_duplicated_spikes(parent_segment.get_unit_spike_train(unit_id, start_frame=None, end_frame=None),
-                                       censored_period, method=method, seed=seed)
-
-
-    def get_unit_spike_train(self, unit_id, start_frame: Optional[int] = None,
-                             end_frame: Optional[int] = None) -> np.ndarray:
+            self._duplicated_spikes[unit_id] = find_duplicated_spikes(
+                parent_segment.get_unit_spike_train(unit_id, start_frame=None, end_frame=None),
+                censored_period,
+                method=method,
+                seed=seed,
+            )
+
+    def get_unit_spike_train(
+        self, unit_id, start_frame: Optional[int] = None, end_frame: Optional[int] = None
+    ) -> np.ndarray:
         spike_train = self._parent_segment.get_unit_spike_train(unit_id, start_frame=None, end_frame=None)
         spike_train = np.delete(spike_train, self._duplicated_spikes[unit_id])
 
         if start_frame == None:
             start_frame = 0
         if end_frame == None:
-            end_frame = spike_train[-1]
+            end_frame = spike_train[-1] if len(spike_train) > 0 else 0
 
         start = np.searchsorted(spike_train, start_frame, side="left")
-        end   = np.searchsorted(spike_train, end_frame, side="right")
+        end = np.searchsorted(spike_train, end_frame, side="right")
 
-        return spike_train[start : end]
+        return spike_train[start:end]
 
 
-remove_duplicated_spikes = define_function_from_class(source_class=RemoveDuplicatedSpikesSorting,
-                                                      name="remove_duplicated_spikes")
+remove_duplicated_spikes = define_function_from_class(
+    source_class=RemoveDuplicatedSpikesSorting, name="remove_duplicated_spikes"
+)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/curation/remove_excess_spikes.py` & `spikeinterface-0.98.0/src/spikeinterface/curation/remove_excess_spikes.py`

 * *Files 13% similar despite different names*

```diff
@@ -4,74 +4,74 @@
 from ..core import BaseSorting, BaseSortingSegment, BaseRecording
 from ..core.waveform_tools import has_exceeding_spikes
 
 
 class RemoveExcessSpikesSorting(BaseSorting):
     """
     Class to remove excess spikes from the spike trains.
-    Excess spikes are the ones exceeding a recording number of samples, for each segment
+    Excess spikes are the ones exceeding a recording number of samples, for each segment.
 
     Parameters
     ----------
     sorting: BaseSorting
         The parent sorting.
     recording: BaseRecording
-        The recording to use to get number of samples.
+        The recording to use to get the number of samples.
 
     Returns
     -------
     sorting_without_excess_spikes: RemoveExcessSpikesSorting
         The sorting without any excess spikes.
     """
 
     def __init__(self, sorting: BaseSorting, recording: BaseRecording) -> None:
         super().__init__(sorting.get_sampling_frequency(), sorting.unit_ids)
 
-        assert sorting.get_num_segments() == recording.get_num_segments(), \
-            "The sorting and recording objects must have the same number of samples!"
+        assert (
+            sorting.get_num_segments() == recording.get_num_segments()
+        ), "The sorting and recording objects must have the same number of samples!"
 
         for segment_index in range(sorting.get_num_segments()):
             sorting_segment = sorting._sorting_segments[segment_index]
             num_samples = recording.get_num_samples(segment_index=segment_index)
             self.add_sorting_segment(RemoveExcessSpikesSortingSegment(sorting_segment, num_samples))
 
         sorting.copy_metadata(self, only_main=False)
         if sorting.has_recording():
             self.register_recording(sorting._recording)
 
-        self._kwargs = {
-            'sorting': sorting.to_dict(),
-            'recording': recording.to_dict()
-        }
+        self._kwargs = {"sorting": sorting, "recording": recording}
 
 
 class RemoveExcessSpikesSortingSegment(BaseSortingSegment):
     def __init__(self, parent_segment: BaseSortingSegment, num_samples: int) -> None:
         super().__init__()
         self._parent_segment = parent_segment
         self._num_samples = num_samples
 
-    def get_unit_spike_train(self, unit_id, start_frame: Optional[int] = None,
-                             end_frame: Optional[int] = None) -> np.ndarray:
+    def get_unit_spike_train(
+        self, unit_id, start_frame: Optional[int] = None, end_frame: Optional[int] = None
+    ) -> np.ndarray:
         spike_train = self._parent_segment.get_unit_spike_train(unit_id, start_frame=start_frame, end_frame=end_frame)
+        max_spike = np.searchsorted(spike_train, self._num_samples, side="left")
 
-        return spike_train[spike_train < self._num_samples]
+        return spike_train[:max_spike]
 
 
 def remove_excess_spikes(sorting, recording):
     """
     Remove excess spikes from the spike trains.
-    Excess spikes are the ones exceeding a recording number of samples, for each segment
+    Excess spikes are the ones exceeding a recording number of samples, for each segment.
 
     Parameters
     ----------
     sorting: BaseSorting
         The parent sorting.
     recording: BaseRecording
-        The recording to use to get number of samples.
+        The recording to use to get the number of samples.
 
     Returns
     -------
     sorting_without_excess_spikes: Sorting
         The sorting without any excess spikes.
     """
     if has_exceeding_spikes(recording=recording, sorting=sorting):
```

### Comparing `spikeinterface-0.97.1/spikeinterface/curation/remove_redundant.py` & `spikeinterface-0.98.0/src/spikeinterface/curation/remove_redundant.py`

 * *Files 3% similar despite different names*

```diff
@@ -2,27 +2,29 @@
 
 from spikeinterface import WaveformExtractor
 
 from ..core.template_tools import get_template_extremum_channel_peak_shift, get_template_amplitudes
 from ..postprocessing import align_sorting
 
 
-def remove_redundant_units(sorting_or_waveform_extractor, 
-                           align=True, 
-                           unit_peak_shifts=None,
-                           delta_time=0.4,
-                           agreement_threshold=0.2,
-                           duplicate_threshold=0.8,
-                           remove_strategy='minimum_shift',
-                           peak_sign="neg",
-                           extra_outputs=False):
+def remove_redundant_units(
+    sorting_or_waveform_extractor,
+    align=True,
+    unit_peak_shifts=None,
+    delta_time=0.4,
+    agreement_threshold=0.2,
+    duplicate_threshold=0.8,
+    remove_strategy="minimum_shift",
+    peak_sign="neg",
+    extra_outputs=False,
+):
     """
     Removes redundant or duplicate units by comparing the sorting output with itself.
-    
-    When a redundant pair is found, there are several strategy to choice which one the best:
+
+    When a redundant pair is found, there are several strategies to choose which unit is the best:
 
        * 'minimum_shift'
        * 'highest_amplitude'
        * 'max_spikes'
 
 
     Parameters
@@ -34,144 +36,147 @@
     align : bool, optional
         If True, spike trains are aligned (if a WaveformExtractor is used), by default False
     delta_time : float, optional
         The time in ms to consider matching spikes, by default 0.4
     agreement_threshold : float, optional
         Threshold on the agreement scores to flag possible redundant/duplicate units, by default 0.2
     duplicate_threshold : float, optional
-        Final threshold on the portion of coincident events over the number of spikes above which the  
-        unit is removed, by default 0.84
+        Final threshold on the portion of coincident events over the number of spikes above which the
+        unit is removed, by default 0.8
     remove_strategy: str
-        Which stragtegy to remove one of the two duplicated units:
+        Which strategy to remove one of the two duplicated units:
 
             * 'minimum_shift': keep the unit with best peak alignment (minimum shift)
-                             If shift are equal then the 'highest_amplitude' is used
-            * 'highest_amplitude': keep the unit with the best amplitude on un shifted max.
+                             If shifts are equal then the 'highest_amplitude' is used
+            * 'highest_amplitude': keep the unit with the best amplitude on unshifted max.
             * 'max_spikes': keep the unit with more spikes
 
     peak_sign: str  ('neg', 'pos', 'both')
         Used when remove_strategy='highest_amplitude'
     extra_outputs: bool
         If True, will return the redundant pairs.
 
     Returns
     -------
     BaseSorting
         Sorting object without redundant units
     """
-    
+
     if isinstance(sorting_or_waveform_extractor, WaveformExtractor):
         sorting = sorting_or_waveform_extractor.sorting
         we = sorting_or_waveform_extractor
     else:
-        assert not align, "The 'align' option is only available when a waveform extractor is used as input"
+        assert not align, "The 'align' option is only available when a WaveformExtractor is used as input"
         sorting = sorting_or_waveform_extractor
         we = None
 
     if align and unit_peak_shifts is None:
-        assert we is not None, 'For align=True must give a WaveformExtractor or explicit unit_peak_shifts'
+        assert we is not None, "For align=True must give a WaveformExtractor or explicit unit_peak_shifts"
         unit_peak_shifts = get_template_extremum_channel_peak_shift(we)
-    
+
     if align:
         sorting_aligned = align_sorting(sorting, unit_peak_shifts)
     else:
         sorting_aligned = sorting
 
-    redundant_unit_pairs = find_redundant_units(sorting_aligned, 
-                                                delta_time=delta_time,
-                                                agreement_threshold=agreement_threshold,
-                                                duplicate_threshold=duplicate_threshold)
-    
+    redundant_unit_pairs = find_redundant_units(
+        sorting_aligned,
+        delta_time=delta_time,
+        agreement_threshold=agreement_threshold,
+        duplicate_threshold=duplicate_threshold,
+    )
+
     remove_unit_ids = []
 
-    if remove_strategy in ('minimum_shift', 'highest_amplitude'):
+    if remove_strategy in ("minimum_shift", "highest_amplitude"):
         # this is the values at spike index !
         peak_values = get_template_amplitudes(we, peak_sign=peak_sign, mode="at_index")
         peak_values = {unit_id: np.max(np.abs(values)) for unit_id, values in peak_values.items()}
 
-    if remove_strategy == 'minimum_shift':
-        assert align, 'remove_strategy with minimum_shift need align=True'
+    if remove_strategy == "minimum_shift":
+        assert align, "remove_strategy with minimum_shift need align=True"
         for u1, u2 in redundant_unit_pairs:
             if np.abs(unit_peak_shifts[u1]) > np.abs(unit_peak_shifts[u2]):
                 remove_unit_ids.append(u1)
             elif np.abs(unit_peak_shifts[u1]) < np.abs(unit_peak_shifts[u2]):
                 remove_unit_ids.append(u2)
             else:
                 # equal shift use peak values
                 if np.abs(peak_values[u1]) < np.abs(peak_values[u2]):
                     remove_unit_ids.append(u1)
                 else:
                     remove_unit_ids.append(u2)
-    elif remove_strategy == 'highest_amplitude':
+    elif remove_strategy == "highest_amplitude":
         for u1, u2 in redundant_unit_pairs:
             if np.abs(peak_values[u1]) < np.abs(peak_values[u2]):
                 remove_unit_ids.append(u1)
             else:
                 remove_unit_ids.append(u2)
     elif remove_strategy == "max_spikes":
         num_spikes = sorting.get_total_num_spikes()
         for u1, u2 in redundant_unit_pairs:
             if num_spikes[u1] < num_spikes[u2]:
                 remove_unit_ids.append(u1)
             else:
                 remove_unit_ids.append(u2)
-    elif remove_strategy == 'with_metrics':
+    elif remove_strategy == "with_metrics":
         # TODO
         # @aurelien @alessio
         # here we can implement the choice of the best one given an external metrics table
         # this will be implemented in a futur PR by the first who need it!
         raise NotImplementedError()
     else:
-        raise ValueError(f'remove_strategy : {remove_strategy} is not implemented!')
+        raise ValueError(f"remove_strategy : {remove_strategy} is not implemented!")
 
     sorting_clean = sorting.remove_units(remove_unit_ids)
 
     if extra_outputs:
         return sorting_clean, redundant_unit_pairs
     else:
         return sorting_clean
 
-    
-def find_redundant_units(sorting, delta_time: float=0.4, agreement_threshold=0.2, duplicate_threshold=0.8):
+
+def find_redundant_units(sorting, delta_time: float = 0.4, agreement_threshold=0.2, duplicate_threshold=0.8):
     """
     Finds redundant or duplicate units by comparing the sorting output with itself.
 
     Parameters
     ----------
     sorting : BaseSorting
         The input sorting object
     delta_time : float, optional
         The time in ms to consider matching spikes, by default 0.4
     agreement_threshold : float, optional
         Threshold on the agreement scores to flag possible redundant/duplicate units, by default 0.2
     duplicate_threshold : float, optional
-        Final threshold on the portion of coincident events over the number of spikes above which the  
+        Final threshold on the portion of coincident events over the number of spikes above which the
         unit is flagged as duplicate/redundant, by default 0.8
-    
+
     Returns
     -------
     list
         The list of duplicate units
     list of 2-element lists
         The list of duplicate pairs
     """
     from spikeinterface.comparison import compare_two_sorters
+
     comparison = compare_two_sorters(sorting, sorting, delta_time=delta_time)
-    
+
     # make agreement triangular and exclude diagonal
     agreement_scores_cleaned = np.tril(comparison.agreement_scores.values, k=-1)
-    
+
     possible_pairs = np.where(agreement_scores_cleaned >= agreement_threshold)
-    
+
     rows, cols = possible_pairs
     redundant_unit_pairs = []
     for r, c in zip(rows, cols):
         unit_i = sorting.unit_ids[r]
         unit_j = sorting.unit_ids[c]
-        
+
         n_coincidents = comparison.match_event_count.at[unit_i, unit_j]
         event_counts = comparison.event_counts1
         shared = max(n_coincidents / event_counts[unit_i], n_coincidents / event_counts[unit_j])
         if shared > duplicate_threshold:
             redundant_unit_pairs.append([unit_i, unit_j])
 
     return redundant_unit_pairs
```

### Comparing `spikeinterface-0.97.1/spikeinterface/curation/sortingview_curation.py` & `spikeinterface-0.98.0/src/spikeinterface/curation/sortingview_curation.py`

 * *Files 1% similar despite different names*

```diff
@@ -2,32 +2,27 @@
 import numpy as np
 from pathlib import Path
 
 from .curationsorting import CurationSorting
 
 
 def apply_sortingview_curation(
-    sorting,
-    uri_or_json,
-    exclude_labels=None,
-    include_labels=None,
-    skip_merge=False,
-    verbose=False
+    sorting, uri_or_json, exclude_labels=None, include_labels=None, skip_merge=False, verbose=False
 ):
     """
     Apply curation from SortingView manual curation.
     First, merges (if present) are applied. Then labels are loaded and units
     are optionally filtered based on exclude_labels and include_labels.
 
     Parameters
     ----------
     sorting : BaseSorting
         The sorting object to be curated
     uri_or_json : str or Path
-        The URI curation link from sortingview or the path to the curation json file
+        The URI curation link from SortingView or the path to the curation json file
     exclude_labels : list, optional
         Optional list of labels to exclude (e.g. ["reject", "noise"]).
         Mutually exclusive with include_labels, by default None
     include_labels : list, optional
         Optional list of labels to include (e.g. ["accept"]).
         Mutually exclusive with exclude_labels,  by default None
     skip_merge : bool, optional
@@ -46,15 +41,15 @@
         raise ImportError(
             "To apply a SortingView manual curation, you need to have sortingview installed: "
             ">>> pip install sortingview"
         )
     curation_sorting = CurationSorting(sorting, make_graph=False, properties_policy="keep")
 
     # get sorting view curation
-    if Path(uri_or_json).suffix == ".json":
+    if Path(uri_or_json).suffix == ".json" and not str(uri_or_json).startswith("gh://"):
         with open(uri_or_json, "r") as f:
             sortingview_curation_dict = json.load(f)
     else:
         try:
             sortingview_curation_dict = kcl.load_json(uri=uri_or_json)
         except:
             raise Exception(f"Could not retrieve curation from SortingView uri: {uri_or_json}")
```

### Comparing `spikeinterface-0.97.1/spikeinterface/curation/splitunitsorting.py` & `spikeinterface-0.98.0/src/spikeinterface/curation/splitunitsorting.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,129 +1,142 @@
 from typing import List, Union
 
 import numpy as np
 from copy import deepcopy
 from spikeinterface.core.basesorting import BaseSorting, BaseSortingSegment
 
+
 class SplitUnitSorting(BaseSorting):
     """
     Class that handles spliting of a unit. It creates a new Sorting object linked to parent_sorting.
 
     Parameters
     ----------
     parent_sorting: Recording
         The recording object
     parent_unit_id: int
-        Unit id of the unit to split        
+        Unit id of the unit to split
     indices_list: list
-        A list of index arrays selecting the spikes to split in each segment. 
-        Each array can contain more than 2 indices (e.g. for splitting in 3 or more units) and it should 
-        be the exact same length as the spike train (for each segment)
+        A list of index arrays selecting the spikes to split in each segment.
+        Each array can contain more than 2 indices (e.g. for splitting in 3 or more units) and it should
+        be the same length as the spike train (for each segment)
     new_unit_ids: int
-        Units id of the new units to be created.
+        Unit ids of the new units to be created.
     properties_policy: str
-        Policy used to propagate propierties. If 'keep' the properties will be pass to the new units
-         (if the units_to_merge have the same value). If 'remove' the new units will have an empty 
+        Policy used to propagate properties. If 'keep' the properties will be passed to the new units
+         (if the units_to_merge have the same value). If 'remove' the new units will have an empty
          value for all the properties of the new unit.
          Default: 'keep'
     Returns
     -------
     sorting: Sorting
-        Sorting object with the selected unit splited
+        Sorting object with the selected units split
     """
 
-    def __init__(self, parent_sorting, split_unit_id, indices_list, new_unit_ids=None,properties_policy='keep'):
+    def __init__(self, parent_sorting, split_unit_id, indices_list, new_unit_ids=None, properties_policy="keep"):
         if type(indices_list) is not list:
             indices_list = [indices_list]
         parents_unit_ids = parent_sorting.get_unit_ids()
-        tot_splits = max([v.max() for v in indices_list])+1
-        unchanged_units = parents_unit_ids[parents_unit_ids!=split_unit_id]
+        tot_splits = max([v.max() for v in indices_list]) + 1
+        unchanged_units = parents_unit_ids[parents_unit_ids != split_unit_id]
 
         if new_unit_ids is None:
-            #select new_unit_ids grater that the max id, event greater than the numerical str ids 
+            # select new_unit_ids greater that the max id, event greater than the numerical str ids
             if np.issubdtype(parents_unit_ids.dtype, np.character):
-                new_unit_ids = max([0]+[int(p) for p in parents_unit_ids if p.isdigit()])+1
+                new_unit_ids = max([0] + [int(p) for p in parents_unit_ids if p.isdigit()]) + 1
             else:
-                new_unit_ids = max(parents_unit_ids)+1
-            new_unit_ids = np.array([u + new_unit_ids for u in range(tot_splits)],dtype=parents_unit_ids.dtype)
+                new_unit_ids = max(parents_unit_ids) + 1
+            new_unit_ids = np.array([u + new_unit_ids for u in range(tot_splits)], dtype=parents_unit_ids.dtype)
         else:
             new_unit_ids = np.array(new_unit_ids, dtype=parents_unit_ids.dtype)
-            assert len(np.unique(new_unit_ids))==len(new_unit_ids), 'Each element in new_unit_ids should be unique'
-            assert len(new_unit_ids) <= tot_splits , 'indices_list have more ids indices than the length of new_unit_ids'
-            
-
+            assert len(np.unique(new_unit_ids)) == len(new_unit_ids), "Each element in new_unit_ids should be unique"
+            assert len(new_unit_ids) <= tot_splits, "indices_list have more ids indices than the length of new_unit_ids"
 
-        assert parent_sorting.get_num_segments()==len(indices_list), 'The length of indices_list must be the same as parent_sorting.get_num_segments'
-        assert split_unit_id in parents_unit_ids, 'Unit to split should be in parent sorting'
-        assert properties_policy=='keep' or properties_policy=='remove', 'properties_policy must be ''keep'' or ''remove'''
-        assert not any(np.isin(new_unit_ids,unchanged_units)), 'new_unit_ids should be new units or one could be equal to split_unit_id'
+        assert parent_sorting.get_num_segments() == len(
+            indices_list
+        ), "The length of indices_list must be the same as parent_sorting.get_num_segments"
+        assert split_unit_id in parents_unit_ids, "Unit to split should be in parent sorting"
+        assert properties_policy == "keep" or properties_policy == "remove", (
+            "properties_policy must be " "keep" " or " "remove" ""
+        )
+        assert not any(
+            np.isin(new_unit_ids, unchanged_units)
+        ), "new_unit_ids should be new units or one could be equal to split_unit_id"
 
         sampling_frequency = parent_sorting.get_sampling_frequency()
         units_ids = np.concatenate([unchanged_units, new_unit_ids])
 
         self._parent_sorting = parent_sorting
         indices_list = deepcopy(indices_list)
-        
+
         BaseSorting.__init__(self, sampling_frequency, units_ids)
-        assert all(np.isin(unchanged_units, self.unit_ids)), 'new_unit_ids should have a compatible format with the parent ids'
+        assert all(
+            np.isin(unchanged_units, self.unit_ids)
+        ), "new_unit_ids should have a compatible format with the parent ids"
 
         for si, parent_segment in enumerate(self._parent_sorting._sorting_segments):
             sub_segment = SplitSortingUnitSegment(parent_segment, split_unit_id, indices_list[si], new_unit_ids)
             self.add_sorting_segment(sub_segment)
 
         # copy properties
         ann_keys = parent_sorting._annotations.keys()
         self._annotations = deepcopy({k: parent_sorting._annotations[k] for k in ann_keys})
-                        
-        #copy properties for unchanged units, and check if units propierties
+
+        # copy properties for unchanged units, and check if units propierties
         keep_parent_inds = parent_sorting.ids_to_indices(unchanged_units)
         split_unit_id_ind = parent_sorting.id_to_index(split_unit_id)
         keep_units_inds = self.ids_to_indices(unchanged_units)
         split_unit_ind = self.ids_to_indices(new_unit_ids)
-        #copy properties from original units to split ones
+        # copy properties from original units to split ones
         prop_keys = parent_sorting._properties.keys()
         for k in prop_keys:
             values = parent_sorting._properties[k]
-            if properties_policy=='keep':
+            if properties_policy == "keep":
                 new_values = np.empty_like(values, shape=len(units_ids))
                 new_values[keep_units_inds] = values[keep_parent_inds]
                 new_values[split_unit_ind] = values[split_unit_id_ind]
                 self.set_property(k, new_values)
                 continue
             self.set_property(k, values[keep_parent_inds], unchanged_units)
 
         if parent_sorting.has_recording():
             self.register_recording(parent_sorting._recording)
 
-        self._kwargs = dict(parent_sorting=parent_sorting.to_dict(), split_unit_id=split_unit_id, 
-                            indices_list=indices_list, new_unit_ids=new_unit_ids,properties_policy=properties_policy)
+        self._kwargs = dict(
+            parent_sorting=parent_sorting,
+            split_unit_id=split_unit_id,
+            indices_list=indices_list,
+            new_unit_ids=new_unit_ids,
+            properties_policy=properties_policy,
+        )
 
 
 class SplitSortingUnitSegment(BaseSortingSegment):
     def __init__(self, parent_segment, split_unit_id, indices, new_unit_ids):
         BaseSortingSegment.__init__(self)
         self._parent_segment = parent_segment
         self._new_unit_ids = new_unit_ids
         self._spike_trains = dict()
         times = self._parent_segment.get_unit_spike_train(split_unit_id, start_frame=None, end_frame=None)
         for idx, unit_id in enumerate(self._new_unit_ids):
-            self._spike_trains[unit_id] = times[indices==idx]          
+            self._spike_trains[unit_id] = times[indices == idx]
 
-    def get_unit_spike_train(self,
-                             unit_id,
-                             start_frame: Union[int, None] = None,
-                             end_frame: Union[int, None] = None,
-                             ) -> np.ndarray:
+    def get_unit_spike_train(
+        self,
+        unit_id,
+        start_frame: Union[int, None] = None,
+        end_frame: Union[int, None] = None,
+    ) -> np.ndarray:
         if unit_id in self._new_unit_ids:
             if start_frame is None:
                 init = 0
             else:
-                init = np.searchsorted(self._spike_trains[unit_id], start_frame, side='left')
+                init = np.searchsorted(self._spike_trains[unit_id], start_frame, side="left")
             if end_frame is None:
                 endf = len(self._spike_trains[unit_id])
             else:
-                endf = np.searchsorted(self._spike_trains[unit_id], end_frame, side='right')
+                endf = np.searchsorted(self._spike_trains[unit_id], end_frame, side="right")
             times = self._spike_trains[unit_id][init:endf]
         else:
             times = self._parent_segment.get_unit_spike_train(unit_id, start_frame, end_frame)
-        
+
         return times
```

### Comparing `spikeinterface-0.97.1/spikeinterface/exporters/report.py` & `spikeinterface-0.98.0/src/spikeinterface/exporters/report.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,30 +1,33 @@
 from pathlib import Path
 import shutil
-import pandas as pd
 
 from spikeinterface.core.job_tools import _shared_job_kwargs_doc, fix_job_kwargs
 import spikeinterface.widgets as sw
 from spikeinterface.core import get_template_extremum_channel, get_template_extremum_amplitude
-from spikeinterface.postprocessing import (compute_spike_amplitudes,
-                                           compute_unit_locations,
-                                           compute_correlograms)
+from spikeinterface.postprocessing import compute_spike_amplitudes, compute_unit_locations, compute_correlograms
 from spikeinterface.qualitymetrics import compute_quality_metrics
 
-import matplotlib.pyplot as plt
 
-
-def export_report(waveform_extractor, output_folder, remove_if_exists=False, format="png",
-                  show_figures=False, peak_sign='neg', force_computation=False, **job_kwargs):
+def export_report(
+    waveform_extractor,
+    output_folder,
+    remove_if_exists=False,
+    format="png",
+    show_figures=False,
+    peak_sign="neg",
+    force_computation=False,
+    **job_kwargs,
+):
     """
     Exports a SI spike sorting report. The report includes summary figures of the spike sorting output
     (e.g. amplitude distributions, unit localization and depth VS amplitude) as well as unit-specific reports,
     that include waveforms, templates, template maps, ISI distributions, and more.
-    
-    
+
+
     Parameters
     ----------
     waveform_extractor: a WaveformExtractor or None
         If WaveformExtractor is provide then the compute is faster otherwise
     output_folder: str
         The output folder where the report files are saved
     remove_if_exists: bool
@@ -35,98 +38,109 @@
         used to compute amplitudes and metrics
     show_figures: bool
         If True, figures are shown. If False (default), figures are closed after saving.
     force_computation:  bool default False
         Force or not some heavy computaion before exporting.
     {}
     """
+    import pandas as pd
+    import matplotlib.pyplot as plt
+
     job_kwargs = fix_job_kwargs(job_kwargs)
     we = waveform_extractor
     sorting = we.sorting
     unit_ids = sorting.unit_ids
-    
-    
+
     # load or compute spike_amplitudes
-    if we.is_extension('spike_amplitudes'):
-        spike_amplitudes = we.load_extension('spike_amplitudes').get_data(outputs='by_unit')
+    if we.is_extension("spike_amplitudes"):
+        spike_amplitudes = we.load_extension("spike_amplitudes").get_data(outputs="by_unit")
     elif force_computation:
-        spike_amplitudes = compute_spike_amplitudes(we, peak_sign=peak_sign, outputs='by_unit', **job_kwargs)
+        spike_amplitudes = compute_spike_amplitudes(we, peak_sign=peak_sign, outputs="by_unit", **job_kwargs)
     else:
         spike_amplitudes = None
-        print('export_report(): spike_amplitudes will not be exported. Use compute_spike_amplitudes() if you want to include them.')
+        print(
+            "export_report(): spike_amplitudes will not be exported. Use compute_spike_amplitudes() if you want to include them."
+        )
 
     # load or compute quality_metrics
-    if we.is_extension('quality_metrics'):
-        metrics = we.load_extension('quality_metrics').get_data()
+    if we.is_extension("quality_metrics"):
+        metrics = we.load_extension("quality_metrics").get_data()
     elif force_computation:
         metrics = compute_quality_metrics(we)
     else:
         metrics = None
-        print('export_report(): quality metrics will not be exported. Use compute_quality_metrics() if you want to include them.')
+        print(
+            "export_report(): quality metrics will not be exported. Use compute_quality_metrics() if you want to include them."
+        )
 
     # load or compute correlograms
-    if we.is_extension('correlograms'):
-        correlograms, bins = we.load_extension('correlograms').get_data()
+    if we.is_extension("correlograms"):
+        correlograms, bins = we.load_extension("correlograms").get_data()
     elif force_computation:
         correlograms, bins = compute_correlograms(we, window_ms=100.0, bin_ms=1.0)
     else:
         correlograms = None
-        print('export_report(): correlograms will not be exported. Use compute_correlograms() if you want to include them.')
+        print(
+            "export_report(): correlograms will not be exported. Use compute_correlograms() if you want to include them."
+        )
 
     # pre-compute unit locations if not done
-    if not we.is_extension('unit_locations'):
+    if not we.is_extension("unit_locations"):
         unit_locations = compute_unit_locations(we)
 
     output_folder = Path(output_folder).absolute()
     if output_folder.is_dir():
         if remove_if_exists:
             shutil.rmtree(output_folder)
         else:
-            raise FileExistsError(f'{output_folder} already exists')
+            raise FileExistsError(f"{output_folder} already exists")
     output_folder.mkdir(parents=True, exist_ok=True)
 
     # unit list
     units = pd.DataFrame(index=unit_ids)  #  , columns=['max_on_channel_id', 'amplitude'])
-    units.index.name = 'unit_id'
-    units['max_on_channel_id'] = pd.Series(get_template_extremum_channel(we, peak_sign='neg', outputs='id'))
-    units['amplitude'] = pd.Series(get_template_extremum_amplitude(we, peak_sign='neg'))
-    units.to_csv(output_folder / 'unit list.csv', sep='\t')
+    units.index.name = "unit_id"
+    units["max_on_channel_id"] = pd.Series(get_template_extremum_channel(we, peak_sign="neg", outputs="id"))
+    units["amplitude"] = pd.Series(get_template_extremum_amplitude(we, peak_sign="neg"))
+    units.to_csv(output_folder / "unit list.csv", sep="\t")
 
     unit_colors = sw.get_unit_colors(sorting)
 
     # global figures
     fig = plt.figure(figsize=(20, 10))
     w = sw.plot_unit_locations(we, figure=fig, unit_colors=unit_colors)
-    fig.savefig(output_folder / f'unit_localization.{format}')
+    fig.savefig(output_folder / f"unit_localization.{format}")
     if not show_figures:
         plt.close(fig)
 
     fig, ax = plt.subplots(figsize=(20, 10))
     sw.plot_unit_depths(we, ax=ax, unit_colors=unit_colors)
-    fig.savefig(output_folder / f'unit_depths.{format}')
+    fig.savefig(output_folder / f"unit_depths.{format}")
     if not show_figures:
         plt.close(fig)
-    
+
     if spike_amplitudes and len(unit_ids) < 100:
         fig = plt.figure(figsize=(20, 10))
         sw.plot_all_amplitudes_distributions(we, figure=fig, unit_colors=unit_colors)
-        fig.savefig(output_folder / f'amplitudes_distribution.{format}')
+        fig.savefig(output_folder / f"amplitudes_distribution.{format}")
         if not show_figures:
             plt.close(fig)
 
     if metrics is not None:
-        metrics.to_csv(output_folder / 'quality metrics.csv')
+        metrics.to_csv(output_folder / "quality metrics.csv")
 
     # units
-    units_folder = output_folder / 'units'
+    units_folder = output_folder / "units"
     units_folder.mkdir()
 
     for unit_id in unit_ids:
-        fig = plt.figure(constrained_layout=False, figsize=(15, 7), )
+        fig = plt.figure(
+            constrained_layout=False,
+            figsize=(15, 7),
+        )
         sw.plot_unit_summary(we, unit_id, figure=fig)
-        fig.suptitle(f'unit {unit_id}')
-        fig.savefig(units_folder / f'{unit_id}.{format}')
+        fig.suptitle(f"unit {unit_id}")
+        fig.savefig(units_folder / f"{unit_id}.{format}")
         if not show_figures:
             plt.close(fig)
 
 
 export_report.__doc__ = export_report.__doc__.format(_shared_job_kwargs_doc)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/exporters/to_phy.py` & `spikeinterface-0.98.0/src/spikeinterface/exporters/to_phy.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,33 +1,54 @@
+from __future__ import annotations
 from pathlib import Path
+from typing import Literal, Optional
 
 import numpy as np
+import numpy.typing as npt
 import shutil
-import pandas as pd
 import warnings
 
 import spikeinterface
-from spikeinterface.core import write_binary_recording, BinaryRecordingExtractor, ChannelSparsity
+from spikeinterface.core import (
+    write_binary_recording,
+    BinaryRecordingExtractor,
+    WaveformExtractor,
+    BinaryFolderRecording,
+    ChannelSparsity,
+)
 from spikeinterface.core.job_tools import _shared_job_kwargs_doc, fix_job_kwargs
-from spikeinterface.postprocessing import (compute_spike_amplitudes, compute_template_similarity,
-                                           compute_principal_components)
-
-
-def export_to_phy(waveform_extractor, output_folder, compute_pc_features=True,
-                  compute_amplitudes=True, sparsity=None, copy_binary=True,
-                  remove_if_exists=False, peak_sign='neg', template_mode='median',
-                  dtype=None, verbose=True, **job_kwargs):
+from spikeinterface.postprocessing import (
+    compute_spike_amplitudes,
+    compute_template_similarity,
+    compute_principal_components,
+)
+
+
+def export_to_phy(
+    waveform_extractor: WaveformExtractor,
+    output_folder: str | Path,
+    compute_pc_features: bool = True,
+    compute_amplitudes: bool = True,
+    sparsity: Optional[ChannelSparsity] = None,
+    copy_binary: bool = True,
+    remove_if_exists: bool = False,
+    peak_sign: Literal["both", "neg", "pos"] = "neg",
+    template_mode: str = "median",
+    dtype: Optional[npt.DTypeLike] = None,
+    verbose: bool = True,
+    **job_kwargs,
+):
     """
     Exports a waveform extractor to the phy template-gui format.
 
     Parameters
     ----------
     waveform_extractor: a WaveformExtractor or None
         If WaveformExtractor is provide then the compute is faster otherwise
-    output_folder: str
+    output_folder: str | Path
         The output folder where the phy template-gui files are saved
     compute_pc_features: bool
         If True (default), pc features are computed
     compute_amplitudes: bool
         If True (default), waveforms amplitudes are computed
     sparsity: ChannelSparsity or None
         The sparsity object.
@@ -40,18 +61,21 @@
     template_mode: str
         Parameter 'mode' to be given to WaveformExtractor.get_template()
     dtype: dtype or None
         Dtype to save binary data
     verbose: bool
         If True, output is verbose
     {}
-    
+
     """
-    assert isinstance(waveform_extractor, spikeinterface.core.waveform_extractor.WaveformExtractor), \
-        'waveform_extractor must be a WaveformExtractor object'
+    import pandas as pd
+
+    assert isinstance(
+        waveform_extractor, spikeinterface.core.waveform_extractor.WaveformExtractor
+    ), "waveform_extractor must be a WaveformExtractor object"
     sorting = waveform_extractor.sorting
 
     assert waveform_extractor.get_num_segments() == 1, "Export to phy only works with one segment"
     num_chans = waveform_extractor.get_num_channels()
     fs = waveform_extractor.sampling_frequency
 
     job_kwargs = fix_job_kwargs(job_kwargs)
@@ -59,15 +83,15 @@
     # check sparsity
     if (num_chans > 64) and (sparsity is None or not waveform_extractor.is_sparse()):
         warnings.warn(
             "Exporting to Phy with many channels and without sparsity might result in a heavy and less "
             "informative visualization. You can use use a sparse WaveformExtractor or you can use the 'sparsity' "
             "argument to enforce sparsity (see compute_sparsity())"
         )
-    
+
     if waveform_extractor.is_sparse():
         used_sparsity = waveform_extractor.sparsity
     elif sparsity is not None:
         used_sparsity = sparsity
     else:
         used_sparsity = ChannelSparsity.create_dense(waveform_extractor)
     # convinient sparsity dict for the 3 cases to retrieve channl_inds
@@ -78,153 +102,157 @@
     for unit in sorting.unit_ids:
         if len(sorting.get_unit_spike_train(unit)) > 0:
             non_empty_units.append(unit)
         else:
             empty_flag = True
     unit_ids = non_empty_units
     if empty_flag:
-        warnings.warn('Empty units have been removed when being exported to Phy')
+        warnings.warn("Empty units have been removed when being exported to Phy")
 
     if len(unit_ids) == 0:
         raise Exception("No non-empty units in the sorting result, can't save to Phy.")
 
     output_folder = Path(output_folder).absolute()
     if output_folder.is_dir():
         if remove_if_exists:
             shutil.rmtree(output_folder)
         else:
-            raise FileExistsError(f'{output_folder} already exists')
+            raise FileExistsError(f"{output_folder} already exists")
 
     output_folder.mkdir(parents=True)
 
     # save dat file
     if dtype is None:
         if waveform_extractor.has_recording():
             dtype = waveform_extractor.recording.get_dtype()
         else:
             dtype = waveform_extractor.dtype
 
     if waveform_extractor.has_recording():
         if copy_binary:
-            rec_path = output_folder / 'recording.dat'
-            write_binary_recording(waveform_extractor.recording, file_paths=rec_path, verbose=verbose, dtype=dtype, **job_kwargs)
+            rec_path = output_folder / "recording.dat"
+            write_binary_recording(waveform_extractor.recording, file_paths=rec_path, dtype=dtype, **job_kwargs)
         elif isinstance(waveform_extractor.recording, BinaryRecordingExtractor):
-            rec_path = waveform_extractor.recording._kwargs['file_paths'][0]
+            if isinstance(waveform_extractor.recording, BinaryFolderRecording):
+                bin_kwargs = waveform_extractor.recording._bin_kwargs
+            else:
+                bin_kwargs = waveform_extractor.recording._kwargs
+            rec_path = bin_kwargs["file_paths"][0]
             dtype = waveform_extractor.recording.get_dtype()
+        else:
+            rec_path = "None"
     else:  # don't save recording.dat
         if copy_binary:
             warnings.warn("Recording will not be copied since waveform extractor is recordingless.")
-        rec_path = 'None'
+        rec_path = "None"
 
     dtype_str = np.dtype(dtype).name
 
     # write params.py
-    with (output_folder / 'params.py').open('w') as f:
+    with (output_folder / "params.py").open("w") as f:
         f.write(f"dat_path = r'{str(rec_path)}'\n")
         f.write(f"n_channels_dat = {num_chans}\n")
         f.write(f"dtype = '{dtype_str}'\n")
         f.write(f"offset = 0\n")
         f.write(f"sample_rate = {fs}\n")
         f.write(f"hp_filtered = {waveform_extractor.is_filtered()}")
 
     # export spike_times/spike_templates/spike_clusters
     # here spike_labels is a remapping to unit_index
-    all_spikes = sorting.get_all_spike_trains(outputs='unit_index')
+    all_spikes = sorting.get_all_spike_trains(outputs="unit_index")
     spike_times, spike_labels = all_spikes[0]
-    np.save(str(output_folder / 'spike_times.npy'), spike_times[:, np.newaxis])
-    np.save(str(output_folder / 'spike_templates.npy'), spike_labels[:, np.newaxis])
-    np.save(str(output_folder / 'spike_clusters.npy'), spike_labels[:, np.newaxis])
+    np.save(str(output_folder / "spike_times.npy"), spike_times[:, np.newaxis])
+    np.save(str(output_folder / "spike_templates.npy"), spike_labels[:, np.newaxis])
+    np.save(str(output_folder / "spike_clusters.npy"), spike_labels[:, np.newaxis])
 
     # export templates/templates_ind/similar_templates
     # shape (num_units, num_samples, max_num_channels)
     max_num_channels = max(len(chan_inds) for chan_inds in sparse_dict.values())
     num_samples = waveform_extractor.nbefore + waveform_extractor.nafter
     templates = np.zeros((len(unit_ids), num_samples, max_num_channels), dtype=waveform_extractor.dtype)
     # here we pad template inds with -1 if len of sparse channels is unequal
-    templates_ind = -np.ones((len(unit_ids), max_num_channels), dtype='int64')
+    templates_ind = -np.ones((len(unit_ids), max_num_channels), dtype="int64")
     for unit_ind, unit_id in enumerate(unit_ids):
         chan_inds = sparse_dict[unit_id]
         template = waveform_extractor.get_template(unit_id, mode=template_mode, sparsity=sparsity)
-        templates[unit_ind, :, :][:, :len(chan_inds)] = template
-        templates_ind[unit_ind, :len(chan_inds)] = chan_inds
+        templates[unit_ind, :, :][:, : len(chan_inds)] = template
+        templates_ind[unit_ind, : len(chan_inds)] = chan_inds
 
-    template_similarity = compute_template_similarity(waveform_extractor, method='cosine_similarity')
+    template_similarity = compute_template_similarity(waveform_extractor, method="cosine_similarity")
 
-    np.save(str(output_folder / 'templates.npy'), templates)
-    np.save(str(output_folder / 'template_ind.npy'), templates_ind)
-    np.save(str(output_folder / 'similar_templates.npy'), template_similarity)
+    np.save(str(output_folder / "templates.npy"), templates)
+    np.save(str(output_folder / "template_ind.npy"), templates_ind)
+    np.save(str(output_folder / "similar_templates.npy"), template_similarity)
 
-    channel_maps = np.arange(num_chans, dtype='int32')
+    channel_maps = np.arange(num_chans, dtype="int32")
     channel_map_si = waveform_extractor.channel_ids
-    channel_positions = waveform_extractor.get_channel_locations().astype('float32')
+    channel_positions = waveform_extractor.get_channel_locations().astype("float32")
     channel_groups = waveform_extractor.get_recording_property("group")
     if channel_groups is None:
-        channel_groups = np.zeros(num_chans, dtype='int32')
-    np.save(str(output_folder / 'channel_map.npy'), channel_maps)
-    np.save(str(output_folder / 'channel_map_si.npy'), channel_map_si)
-    np.save(str(output_folder / 'channel_positions.npy'), channel_positions)
-    np.save(str(output_folder / 'channel_groups.npy'), channel_groups)
+        channel_groups = np.zeros(num_chans, dtype="int32")
+    np.save(str(output_folder / "channel_map.npy"), channel_maps)
+    np.save(str(output_folder / "channel_map_si.npy"), channel_map_si)
+    np.save(str(output_folder / "channel_positions.npy"), channel_positions)
+    np.save(str(output_folder / "channel_groups.npy"), channel_groups)
 
     if compute_amplitudes:
-        if waveform_extractor.is_extension('spike_amplitudes'):
-            sac = waveform_extractor.load_extension('spike_amplitudes')
-            amplitudes = sac.get_data(outputs='concatenated')
+        if waveform_extractor.is_extension("spike_amplitudes"):
+            sac = waveform_extractor.load_extension("spike_amplitudes")
+            amplitudes = sac.get_data(outputs="concatenated")
         else:
-            amplitudes = compute_spike_amplitudes(waveform_extractor, peak_sign=peak_sign, outputs='concatenated', 
-                                                  **job_kwargs)
+            amplitudes = compute_spike_amplitudes(
+                waveform_extractor, peak_sign=peak_sign, outputs="concatenated", **job_kwargs
+            )
         # one segment only
         amplitudes = amplitudes[0][:, np.newaxis]
-        np.save(str(output_folder / 'amplitudes.npy'), amplitudes)
+        np.save(str(output_folder / "amplitudes.npy"), amplitudes)
 
     if compute_pc_features:
-        if waveform_extractor.is_extension('principal_components'):
-            pc = waveform_extractor.load_extension('principal_components')
+        if waveform_extractor.is_extension("principal_components"):
+            pc = waveform_extractor.load_extension("principal_components")
         else:
-            pc = compute_principal_components(waveform_extractor, n_components=5, mode='by_channel_local',
-                                              sparsity=sparsity)
+            pc = compute_principal_components(
+                waveform_extractor, n_components=5, mode="by_channel_local", sparsity=sparsity
+            )
         pc_sparsity = pc.get_sparsity()
         if pc_sparsity is None:
             pc_sparsity = used_sparsity
         max_num_channels_pc = max(len(chan_inds) for chan_inds in pc_sparsity.unit_id_to_channel_indices.values())
 
-        pc.run_for_all_spikes(output_folder / 'pc_features.npy', **job_kwargs)
+        pc.run_for_all_spikes(output_folder / "pc_features.npy", **job_kwargs)
 
-        pc_feature_ind = -np.ones((len(unit_ids), max_num_channels_pc), dtype='int64')
+        pc_feature_ind = -np.ones((len(unit_ids), max_num_channels_pc), dtype="int64")
         for unit_ind, unit_id in enumerate(unit_ids):
             chan_inds = pc_sparsity.unit_id_to_channel_indices[unit_id]
-            pc_feature_ind[unit_ind, :len(chan_inds)] = chan_inds
-        np.save(str(output_folder / 'pc_feature_ind.npy'), pc_feature_ind)
+            pc_feature_ind[unit_ind, : len(chan_inds)] = chan_inds
+        np.save(str(output_folder / "pc_feature_ind.npy"), pc_feature_ind)
 
     # Save .tsv metadata
-    cluster_group = pd.DataFrame({'cluster_id': [i for i in range(len(unit_ids))],
-                                  'group': ['unsorted'] * len(unit_ids)})
-    cluster_group.to_csv(output_folder / 'cluster_group.tsv',
-                         sep="\t", index=False)
-    si_unit_ids = pd.DataFrame({'cluster_id': [i for i in range(len(unit_ids))],
-                                'si_unit_id': unit_ids})
-    si_unit_ids.to_csv(output_folder / 'cluster_si_unit_ids.tsv',
-                       sep="\t", index=False)
+    cluster_group = pd.DataFrame(
+        {"cluster_id": [i for i in range(len(unit_ids))], "group": ["unsorted"] * len(unit_ids)}
+    )
+    cluster_group.to_csv(output_folder / "cluster_group.tsv", sep="\t", index=False)
+    si_unit_ids = pd.DataFrame({"cluster_id": [i for i in range(len(unit_ids))], "si_unit_id": unit_ids})
+    si_unit_ids.to_csv(output_folder / "cluster_si_unit_ids.tsv", sep="\t", index=False)
 
-    unit_groups = sorting.get_property('group')
+    unit_groups = sorting.get_property("group")
     if unit_groups is None:
-        unit_groups = np.zeros(len(unit_ids), dtype='int32')
-    channel_group = pd.DataFrame({'cluster_id': [i for i in range(len(unit_ids))],
-                                  'channel_group': unit_groups})
-    channel_group.to_csv(output_folder / 'cluster_channel_group.tsv',
-                         sep="\t", index=False)
-    
-    if waveform_extractor.is_extension('quality_metrics'):
-        qm = waveform_extractor.load_extension('quality_metrics')
+        unit_groups = np.zeros(len(unit_ids), dtype="int32")
+    channel_group = pd.DataFrame({"cluster_id": [i for i in range(len(unit_ids))], "channel_group": unit_groups})
+    channel_group.to_csv(output_folder / "cluster_channel_group.tsv", sep="\t", index=False)
+
+    if waveform_extractor.is_extension("quality_metrics"):
+        qm = waveform_extractor.load_extension("quality_metrics")
         qm_data = qm.get_data()
         for column_name in qm_data.columns:
             # already computed by phy
             if column_name not in ["num_spikes", "firing_rate"]:
-                metric = pd.DataFrame({'cluster_id': [i for i in range(len(unit_ids))],
-                                       column_name: qm_data[column_name].values})
-                metric.to_csv(output_folder / f'cluster_{column_name}.tsv',
-                              sep="\t", index=False)
+                metric = pd.DataFrame(
+                    {"cluster_id": [i for i in range(len(unit_ids))], column_name: qm_data[column_name].values}
+                )
+                metric.to_csv(output_folder / f"cluster_{column_name}.tsv", sep="\t", index=False)
 
     if verbose:
-        print('Run:\nphy template-gui ', str(output_folder / 'params.py'))
+        print("Run:\nphy template-gui ", str(output_folder / "params.py"))
 
 
 export_to_phy.__doc__ = export_to_phy.__doc__.format(_shared_job_kwargs_doc)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/alfsortingextractor.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/alfsortingextractor.py`

 * *Files 6% similar despite different names*

```diff
@@ -3,14 +3,15 @@
 import numpy as np
 
 from spikeinterface.core import BaseSorting, BaseSortingSegment
 from spikeinterface.core.core_tools import define_function_from_class
 
 try:
     import pandas as pd
+
     HAVE_PANDAS = True
 except:
     HAVE_PANDAS = False
 
 
 class ALFSortingExtractor(BaseSorting):
     """Load ALF format data as a sorting extractor.
@@ -24,76 +25,77 @@
 
     Returns
     -------
     extractor : ALFSortingExtractor
         The loaded data.
     """
 
-    extractor_name = 'ALFSorting'
+    extractor_name = "ALFSorting"
     installed = HAVE_PANDAS
     installation_mesg = "To use the ALF extractors, install pandas: \n\n pip install pandas\n\n"
     name = "alf"
 
     def __init__(self, folder_path, sampling_frequency=30000):
-
         assert self.installed, self.installation_mesg
         # check correct parent folder:
         self._folder_path = Path(folder_path)
-        if 'probe' not in self._folder_path.name:
+        if "probe" not in self._folder_path.name:
             raise ValueError('folder name should contain "probe", containing channels, clusters.* .npy datasets')
         # load datasets as mmap into a dict:
-        required_alf_datasets = ['spikes.times', 'spikes.clusters']
+        required_alf_datasets = ["spikes.times", "spikes.clusters"]
         found_alf_datasets = dict()
         for alf_dataset_name in self.file_loc.iterdir():
-            if 'spikes' in alf_dataset_name.stem or 'clusters' in alf_dataset_name.stem:
-                if 'npy' in alf_dataset_name.suffix:
-                    dset = np.load(alf_dataset_name, mmap_mode='r', allow_pickle=True)
+            if "spikes" in alf_dataset_name.stem or "clusters" in alf_dataset_name.stem:
+                if "npy" in alf_dataset_name.suffix:
+                    dset = np.load(alf_dataset_name, mmap_mode="r", allow_pickle=True)
                     found_alf_datasets.update({alf_dataset_name.stem: dset})
-                elif 'metrics' in alf_dataset_name.stem:
+                elif "metrics" in alf_dataset_name.stem:
                     found_alf_datasets.update({alf_dataset_name.stem: pd.read_csv(alf_dataset_name)})
 
         # check existence of datasets:
         if not any([i in found_alf_datasets for i in required_alf_datasets]):
-            raise Exception(f'could not find {required_alf_datasets} in folder')
+            raise Exception(f"could not find {required_alf_datasets} in folder")
 
-        spike_clusters = found_alf_datasets['spikes.clusters']
-        spike_times = found_alf_datasets['spikes.times']
+        spike_clusters = found_alf_datasets["spikes.clusters"]
+        spike_times = found_alf_datasets["spikes.times"]
 
         # load units properties:
         total_units = 0
         properties = dict()
 
         for alf_dataset_name, alf_dataset in found_alf_datasets.items():
-            if 'clusters' in alf_dataset_name:
-                if 'clusters.metrics' in alf_dataset_name:
+            if "clusters" in alf_dataset_name:
+                if "clusters.metrics" in alf_dataset_name:
                     for property_name, property_values in found_alf_datasets[alf_dataset_name].iteritems():
                         properties[property_name] = property_values.tolist()
                 else:
-                    property_name = alf_dataset_name.split('.')[1]
+                    property_name = alf_dataset_name.split(".")[1]
                     properties[property_name] = alf_dataset
                     if total_units == 0:
                         total_units = alf_dataset.shape[0]
 
-        if 'clusters.metrics' in found_alf_datasets and \
-                found_alf_datasets['clusters.metrics'].get('cluster_id') is not None:
-            unit_ids = found_alf_datasets['clusters.metrics'].get('cluster_id').tolist()
+        if (
+            "clusters.metrics" in found_alf_datasets
+            and found_alf_datasets["clusters.metrics"].get("cluster_id") is not None
+        ):
+            unit_ids = found_alf_datasets["clusters.metrics"].get("cluster_id").tolist()
         else:
             unit_ids = list(range(total_units))
 
         BaseSorting.__init__(self, unit_ids=unit_ids, sampling_frequency=sampling_frequency)
         sorting_segment = ALFSortingSegment(spike_clusters, spike_times, sampling_frequency)
         self.add_sorting_segment(sorting_segment)
 
-        self.extra_requirements.append('pandas')
+        self.extra_requirements.append("pandas")
 
         # add properties
         for property_name, values in properties.items():
             self.set_property(property_name, values)
 
-        self._kwargs = {'folder_path': str(Path(folder_path).absolute()), 'sampling_frequency': sampling_frequency}
+        self._kwargs = {"folder_path": str(Path(folder_path).absolute()), "sampling_frequency": sampling_frequency}
 
     # @staticmethod
     # def write_sorting(sorting, save_path):
     #     assert HAVE_PANDAS, ALFSortingExtractor.installation_mesg
     #     # write cluster properties as clusters.<property_name>.npy
     #     save_path = Path(save_path)
     #     csv_property_names = ['cluster_id', 'cluster_id.1', 'num_spikes', 'firing_rate',
@@ -130,24 +132,25 @@
 class ALFSortingSegment(BaseSortingSegment):
     def __init__(self, spike_clusters, spike_times, sampling_frequency):
         self._spike_clusters = spike_clusters
         self._spike_times = spike_times
         self._sampling_frequency = sampling_frequency
         BaseSortingSegment.__init__(self)
 
-    def get_unit_spike_train(self,
-                             unit_id,
-                             start_frame,
-                             end_frame,
-                             ) -> np.ndarray:
+    def get_unit_spike_train(
+        self,
+        unit_id,
+        start_frame,
+        end_frame,
+    ) -> np.ndarray:
         # must be implemented in subclass
         if start_frame is None:
             start_frame = 0
         if end_frame is None:
             end_frame = np.inf
 
         spike_times = self._spike_time[np.where(self._spike_clusters == unit_id)]
-        spike_frames = (spike_times * self._sampling_frequency).astype('int64')
+        spike_frames = (spike_times * self._sampling_frequency).astype("int64")
         return spike_frames[(spike_frames >= start_frame) & (spike_frames < end_frame)]
 
 
 read_alf_sorting = define_function_from_class(source_class=ALFSortingExtractor, name="read_alf_sorting")
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/bids.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/bids.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,11 +1,10 @@
 from pathlib import Path
 
 import numpy as np
-import pandas as pd
 
 import neo
 from probeinterface import read_BIDS_probe
 
 from .nwbextractors import read_nwb
 from .neoextractors import read_nix
 
@@ -31,70 +30,71 @@
         The loaded data, with attached Probes.
     """
 
     folder_path = Path(folder_path)
 
     recordings = []
     for file_path in folder_path.iterdir():
-
         bids_name = file_path.stem
 
-        if file_path.suffix == '.nwb':
-            rec, = read_nwb(file_path, load_recording=True, load_sorting=False, electrical_series_name=None)
+        if file_path.suffix == ".nwb":
+            (rec,) = read_nwb(file_path, load_recording=True, load_sorting=False, electrical_series_name=None)
             rec.annotate(bids_name=bids_name)
-            rec.extra_requirements.extend('pandas')
+            rec.extra_requirements.extend("pandas")
             probegroup = _read_probe_group(file_path.parent, bids_name, rec.channel_ids)
             rec = rec.set_probegroup(probegroup)
             recordings.append(rec)
 
-        elif file_path.suffix == '.nix':
+        elif file_path.suffix == ".nix":
             neo_reader = neo.rawio.NIXRawIO(file_path)
             neo_reader.parse_header()
-            stream_ids = neo_reader.header['signal_streams']['id']
+            stream_ids = neo_reader.header["signal_streams"]["id"]
 
             for stream_id in stream_ids:
                 rec = read_nix(file_path, stream_id=stream_id)
-                rec.extra_requirements.extend('pandas')
+                rec.extra_requirements.extend("pandas")
                 probegroup = _read_probe_group(file_path.parent, bids_name, rec.channel_ids)
                 rec = rec.set_probegroup(probegroup)
                 recordings.append(rec)
 
     return recordings
 
 
 def _read_probe_group(folder, bids_name, recording_channel_ids):
     probegroup = read_BIDS_probe(folder)
 
     # make maps between : channel_id and contact_id using _channels.tsv
+    import pandas as pd
+
     for probe in probegroup.probes:
-        channels_file = folder / bids_name.replace('_ephys', '_channels.tsv')
-        channels = pd.read_csv(channels_file, sep='\t', dtype='str')
+        channels_file = folder / bids_name.replace("_ephys", "_channels.tsv")
+        channels = pd.read_csv(channels_file, sep="\t", dtype="str")
         # channel_ids are unique
-        channel_ids = channels['channel_id'].values.astype('U')
+        channel_ids = channels["channel_id"].values.astype("U")
         # contact ids are not unique
         # a single contact can be associated with multiple channels, contact_ids can be n/a
-        channels['contact_id'][channels['contact_id'].isnull()] = 'unconnected'
-        contact_ids = channels['contact_id'].values.astype('U')
+        channels["contact_id"][channels["contact_id"].isnull()] = "unconnected"
+        contact_ids = channels["contact_id"].values.astype("U")
 
         # extracting information of requested channels
         keep = np.in1d(channel_ids, recording_channel_ids)
         channel_ids = channel_ids[keep]
         contact_ids = contact_ids[keep]
 
-        rec_chan_ids = list(recording_channel_ids.astype('U'))
+        rec_chan_ids = list(recording_channel_ids.astype("U"))
 
         # contact_id > channel_id
         # this overwrites if there's multiple contact_ids = unconnected
         contact_id_to_channel_id = dict(zip(contact_ids, channel_ids))
         # remove unconnected contact entry
-        contact_id_to_channel_id.pop('unconnected', None)
+        contact_id_to_channel_id.pop("unconnected", None)
         # contact_id > channel_index within recording
-        contact_id_to_channel_index = {con_id: rec_chan_ids.index(chan_id)
-                                       for con_id, chan_id in
-                                       contact_id_to_channel_id.items()}
+        contact_id_to_channel_index = {
+            con_id: rec_chan_ids.index(chan_id) for con_id, chan_id in contact_id_to_channel_id.items()
+        }
 
         # vector of channel indices within recording ordered by probe contact_ids
         # needed for probe wiring
         device_channel_indices = []
         for contact_id in probe.contact_ids:
             if contact_id in contact_id_to_channel_index:
                 device_channel_indices.append(contact_id_to_channel_index[contact_id])
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/cbin_ibl.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/cbin_ibl.py`

 * *Files 14% similar despite different names*

```diff
@@ -4,14 +4,15 @@
 
 from spikeinterface.core import BaseRecording, BaseRecordingSegment
 from spikeinterface.extractors.neuropixels_utils import get_neuropixels_sample_shifts
 from spikeinterface.core.core_tools import define_function_from_class
 
 try:
     import mtscomp
+
     HAVE_MTSCOMP = True
 except:
     HAVE_MTSCOMP = False
 
 
 class CompressedBinaryIblExtractor(BaseRecording):
     """Load IBL data as an extractor object.
@@ -32,58 +33,59 @@
         If not then the probe is loaded.
 
     Returns
     -------
     recording : CompressedBinaryIblExtractor
         The loaded data.
     """
-    extractor_name = 'CompressedBinaryIbl'
-    has_default_locations = True
+
+    extractor_name = "CompressedBinaryIbl"
     installed = HAVE_MTSCOMP
-    mode = 'folder'
+    mode = "folder"
     installation_mesg = "To use the CompressedBinaryIblExtractor, install mtscomp: \n\n pip install mtscomp\n\n"
     name = "cbin_ibl"
 
     def __init__(self, folder_path, load_sync_channel=False):
-
         # this work only for future neo
         from neo.rawio.spikeglxrawio import read_meta_file, extract_stream_info
 
         assert HAVE_MTSCOMP
         folder_path = Path(folder_path)
 
         # explore files
-        cbin_files = list(folder_path.glob('*.cbin'))
+        cbin_files = list(folder_path.glob("*.cbin"))
         assert len(cbin_files) == 1
         cbin_file = cbin_files[0]
-        ch_file = cbin_file.with_suffix('.ch')
-        meta_file = cbin_file.with_suffix('.meta')
+        ch_file = cbin_file.with_suffix(".ch")
+        meta_file = cbin_file.with_suffix(".meta")
 
         # reader
         cbuffer = mtscomp.Reader()
         cbuffer.open(cbin_file, ch_file)
 
         # meta data
         meta = read_meta_file(meta_file)
         info = extract_stream_info(meta_file, meta)
-        channel_ids = info['channel_names']
-        gains = info['channel_gains']
-        offsets = info['channel_offsets']
+        channel_ids = info["channel_names"]
+        gains = info["channel_gains"]
+        offsets = info["channel_offsets"]
         if not load_sync_channel:
             channel_ids = channel_ids[:-1]
             gains = gains[:-1]
             offsets = offsets[:-1]
-        sampling_frequency = float(info['sampling_rate'])
+        sampling_frequency = float(info["sampling_rate"])
 
         # init
-        BaseRecording.__init__(self, channel_ids=channel_ids, sampling_frequency=sampling_frequency, dtype=cbuffer.dtype)
+        BaseRecording.__init__(
+            self, channel_ids=channel_ids, sampling_frequency=sampling_frequency, dtype=cbuffer.dtype
+        )
         recording_segment = CBinIblRecordingSegment(cbuffer, sampling_frequency, load_sync_channel)
         self.add_recording_segment(recording_segment)
 
-        self.extra_requirements.append('mtscomp')
+        self.extra_requirements.append("mtscomp")
 
         # set inplace meta data
         self.set_channel_gains(gains)
         self.set_channel_offsets(offsets)
 
         if not load_sync_channel:
             probe = pi.read_spikeglx(meta_file)
@@ -92,23 +94,23 @@
                 self.set_probe(probe, in_place=True, group_mode="by_shank")
             else:
                 self.set_probe(probe, in_place=True)
 
             # load num_channels_per_adc depending on probe type
             ptype = probe.annotations["probe_type"]
 
-            if ptype in [21, 24]: # NP2.0
+            if ptype in [21, 24]:  # NP2.0
                 num_channels_per_adc = 16
-            else: # NP1.0
+            else:  # NP1.0
                 num_channels_per_adc = 12
 
             sample_shifts = get_neuropixels_sample_shifts(self.get_num_channels(), num_channels_per_adc)
             self.set_property("inter_sample_shift", sample_shifts)
 
-        self._kwargs = {'folder_path': str(folder_path.absolute()), 'load_sync_channel': load_sync_channel}
+        self._kwargs = {"folder_path": str(folder_path.absolute()), "load_sync_channel": load_sync_channel}
 
 
 class CBinIblRecordingSegment(BaseRecordingSegment):
     def __init__(self, cbuffer, sampling_frequency, load_sync_channel):
         BaseRecordingSegment.__init__(self, sampling_frequency=sampling_frequency)
         self._cbuffer = cbuffer
         self._load_sync_channel = load_sync_channel
@@ -117,16 +119,18 @@
         return self._cbuffer.shape[0]
 
     def get_traces(self, start_frame, end_frame, channel_indices):
         if start_frame is None:
             start_frame = 0
         if end_frame is None:
             end_frame = self.get_num_samples()
+        if channel_indices is None:
+            channel_indices = slice(None)
 
         traces = self._cbuffer[start_frame:end_frame]
         if not self._load_sync_channel:
             traces = traces[:, :-1]
 
-        return traces
+        return traces[:, channel_indices]
 
 
 read_cbin_ibl = define_function_from_class(source_class=CompressedBinaryIblExtractor, name="read_cbin_ibl")
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/combinatoextractors.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/combinatoextractors.py`

 * *Files 11% similar despite different names*

```diff
@@ -3,14 +3,15 @@
 import numpy as np
 
 from spikeinterface.core import BaseSorting, BaseSortingSegment
 from spikeinterface.core.core_tools import define_function_from_class
 
 try:
     import h5py
+
     HAVE_H5PY = True
 except ImportError:
     HAVE_H5PY = False
 
 
 class CombinatoSortingExtractor(BaseSorting):
     """Load Combinato format data as a sorting extractor.
@@ -30,67 +31,67 @@
 
     Returns
     -------
     extractor : CombinatoSortingExtractor
         The loaded data.
     """
 
-    extractor_name = 'CombinatoSortingExtractor'
+    extractor_name = "CombinatoSortingExtractor"
     installed = HAVE_H5PY
     installation_mesg = "To use the CombinatoSortingExtractor install h5py: \n\n pip install h5py\n\n"
     name = "combinato"
 
-    def __init__(self, folder_path, sampling_frequency=None, user='simple', det_sign='both', keep_good_only=True):
-
+    def __init__(self, folder_path, sampling_frequency=None, user="simple", det_sign="both", keep_good_only=True):
         folder_path = Path(folder_path)
-        assert folder_path.is_dir(), 'Folder {} doesn\'t exist'.format(folder_path)
+        assert folder_path.is_dir(), "Folder {} doesn't exist".format(folder_path)
         if sampling_frequency is None:
-            h5_path = str(folder_path) + '.h5'
+            h5_path = str(folder_path) + ".h5"
             if Path(h5_path).exists():
-                with h5py.File(h5_path, mode='r') as f:
-                    sampling_frequency = f['sr'][0]
+                with h5py.File(h5_path, mode="r") as f:
+                    sampling_frequency = f["sr"][0]
 
         # ~ self.set_sampling_frequency(sampling_frequency)
-        det_file = str(folder_path / Path('data_' + folder_path.stem + '.h5'))
+        det_file = str(folder_path / Path("data_" + folder_path.stem + ".h5"))
         sort_cat_files = []
-        for sign in ['neg', 'pos']:
-            if det_sign in ['both', sign]:
-                sort_cat_file = folder_path / Path('sort_{}_{}/sort_cat.h5'.format(sign, user))
+        for sign in ["neg", "pos"]:
+            if det_sign in ["both", sign]:
+                sort_cat_file = folder_path / Path("sort_{}_{}/sort_cat.h5".format(sign, user))
                 if sort_cat_file.exists():
                     sort_cat_files.append((sign, str(sort_cat_file)))
 
         unit_counter = 0
         spiketrains = {}
         metadata = {}
         unsorted = []
-        with h5py.File(det_file, mode='r') as fdet:
+        with h5py.File(det_file, mode="r") as fdet:
             for sign, sfile in sort_cat_files:
-                with h5py.File(sfile, mode='r') as f:
-                    sp_class = f['classes'][()]
-                    gaux = f['groups'][()]
+                with h5py.File(sfile, mode="r") as f:
+                    sp_class = f["classes"][()]
+                    gaux = f["groups"][()]
                     groups = {g: gaux[gaux[:, 1] == g, 0] for g in np.unique(gaux[:, 1])}  # array of classes per group
-                    group_type = {group: g_type for group, g_type in f['types'][()]}
-                    sp_index = f['index'][()]
+                    group_type = {group: g_type for group, g_type in f["types"][()]}
+                    sp_index = f["index"][()]
 
-                times_css = fdet[sign]['times'][()]
+                times_css = fdet[sign]["times"][()]
                 for gr, cls in groups.items():
                     if keep_good_only and (group_type[gr] < 1):  # artifact or unsorted
                         continue
                     spiketrains[unit_counter] = np.rint(
-                        times_css[sp_index[np.isin(sp_class, cls)]] * (sampling_frequency / 1000))
-                    metadata[unit_counter] = {'group_type': group_type[gr]}
+                        times_css[sp_index[np.isin(sp_class, cls)]] * (sampling_frequency / 1000)
+                    )
+                    metadata[unit_counter] = {"group_type": group_type[gr]}
                     unit_counter = unit_counter + 1
-        unit_ids = np.arange(unit_counter, dtype='int64')
+        unit_ids = np.arange(unit_counter, dtype="int64")
         BaseSorting.__init__(self, sampling_frequency, unit_ids)
         self.add_sorting_segment(CombinatoSortingSegment(spiketrains))
-        self.set_property('unsorted', np.array([metadata[u]['group_type'] == 0 for u in range(unit_counter)]))
-        self.set_property('artifact', np.array([metadata[u]['group_type'] == -1 for u in range(unit_counter)]))
-        self._kwargs = {'folder_path': str(folder_path), 'user': user, 'det_sign': det_sign}
+        self.set_property("unsorted", np.array([metadata[u]["group_type"] == 0 for u in range(unit_counter)]))
+        self.set_property("artifact", np.array([metadata[u]["group_type"] == -1 for u in range(unit_counter)]))
+        self._kwargs = {"folder_path": str(folder_path), "user": user, "det_sign": det_sign}
 
-        self.extra_requirements.append('h5py')
+        self.extra_requirements.append("h5py")
 
 
 class CombinatoSortingSegment(BaseSortingSegment):
     def __init__(self, spiketrains):
         BaseSortingSegment.__init__(self)
         # spiketrains is dict
         self._spiketrains = spiketrains
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/extractorlist.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/extractorlist.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,24 +1,28 @@
 from typing import Type
 
 # most important extractor are in spikeinterface.core
-from spikeinterface.core import (BaseRecording, BaseSorting,
-                                 BinaryRecordingExtractor, NumpyRecording,
-                                 NpzSortingExtractor, NumpySorting,
-                                 NpySnippetsExtractor)
+from spikeinterface.core import (
+    BaseRecording,
+    BaseSorting,
+    BinaryRecordingExtractor,
+    NumpyRecording,
+    NpzSortingExtractor,
+    NumpySorting,
+    NpySnippetsExtractor,
+)
 
 # sorting/recording/event from neo
 from .neoextractors import *
 
 # non-NEO objects implemented in neo folder
 from .neoextractors import NeuroScopeSortingExtractor, MaxwellEventExtractor
 
 # NWB sorting/recording/event
-from .nwbextractors import (NwbRecordingExtractor, NwbSortingExtractor,
-                            read_nwb, read_nwb_recording, read_nwb_sorting)
+from .nwbextractors import NwbRecordingExtractor, NwbSortingExtractor, read_nwb, read_nwb_recording, read_nwb_sorting
 
 from .cbin_ibl import CompressedBinaryIblExtractor, read_cbin_ibl
 from .iblstreamingrecording import IblStreamingRecordingExtractor, read_ibl_streaming_recording
 from .mcsh5extractors import MCSH5RecordingExtractor, read_mcsh5
 
 # sorting extractors in relation with a sorter
 from .cellexplorersortingextractor import CellExplorerSortingExtractor, read_cellexplorer
@@ -31,52 +35,52 @@
 from .tridesclousextractors import TridesclousSortingExtractor, read_tridesclous
 from .spykingcircusextractors import SpykingCircusSortingExtractor, read_spykingcircus
 from .herdingspikesextractors import HerdingspikesSortingExtractor, read_herdingspikes
 from .mdaextractors import MdaRecordingExtractor, MdaSortingExtractor, read_mda_recording, read_mda_sorting
 from .phykilosortextractors import PhySortingExtractor, KiloSortSortingExtractor, read_phy, read_kilosort
 
 # sorting in relation with simulator
-from .shybridextractors import (SHYBRIDRecordingExtractor, SHYBRIDSortingExtractor,
-                                read_shybrid_recording, read_shybrid_sorting)
+from .shybridextractors import (
+    SHYBRIDRecordingExtractor,
+    SHYBRIDSortingExtractor,
+    read_shybrid_recording,
+    read_shybrid_sorting,
+)
 
 # snippers
 from .waveclussnippetstextractors import WaveClusSnippetsExtractor, read_waveclus_snippets
 
 
 # misc
 from .alfsortingextractor import ALFSortingExtractor, read_alf_sorting
 
 
 ########################################
 
 recording_extractor_full_list = [
     BinaryRecordingExtractor,
-
     # natively implemented in spikeinterface.extractors
     NumpyRecording,
     SHYBRIDRecordingExtractor,
     MdaRecordingExtractor,
     NwbRecordingExtractor,
-
     # others
     CompressedBinaryIblExtractor,
     IblStreamingRecordingExtractor,
     MCSH5RecordingExtractor,
 ]
 recording_extractor_full_list += neo_recording_extractors_list
 
 sorting_extractor_full_list = [
     NpzSortingExtractor,
-
     # natively implemented in spikeinterface.extractors
     NumpySorting,
     MdaSortingExtractor,
     SHYBRIDSortingExtractor,
     ALFSortingExtractor,
-
     KlustaSortingExtractor,
     HDSortSortingExtractor,
     MClustSortingExtractor,
     WaveClusSortingExtractor,
     YassSortingExtractor,
     CombinatoSortingExtractor,
     TridesclousSortingExtractor,
@@ -85,31 +89,25 @@
     KiloSortSortingExtractor,
     PhySortingExtractor,
     NwbSortingExtractor,
     NeuroScopeSortingExtractor,
 ]
 sorting_extractor_full_list += neo_sorting_extractors_list
 
-event_extractor_full_list = [
-    MaxwellEventExtractor
-]
+event_extractor_full_list = [MaxwellEventExtractor]
 event_extractor_full_list += neo_event_extractors_list
 
-snippets_extractor_full_list = [
-    NpySnippetsExtractor,
-    WaveClusSnippetsExtractor
-]
+snippets_extractor_full_list = [NpySnippetsExtractor, WaveClusSnippetsExtractor]
 
 
 recording_extractor_full_dict = {recext.name: recext for recext in recording_extractor_full_list}
 sorting_extractor_full_dict = {recext.name: recext for recext in sorting_extractor_full_list}
 snippets_extractor_full_dict = {recext.name: recext for recext in snippets_extractor_full_list}
 
 
-
 def get_recording_extractor_from_name(name: str) -> Type[BaseRecording]:
     """
     Returns the Recording Extractor class based on its name.
 
     Parameters
     ----------
     name: str
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/hdsortextractors.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/hdsortextractors.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,12 @@
 from pathlib import Path
 
 import numpy as np
 
-from spikeinterface.core import (BaseRecording, BaseSorting,
-                                 BaseRecordingSegment, BaseSortingSegment)
+from spikeinterface.core import BaseRecording, BaseSorting, BaseRecordingSegment, BaseSortingSegment
 from spikeinterface.core.core_tools import define_function_from_class
 from .matlabhelpers import MatlabHelper
 
 
 class HDSortSortingExtractor(MatlabHelper, BaseSorting):
     """Load HDSort format data as a sorting extractor.
 
@@ -19,46 +18,46 @@
         Whether to only keep good units.
 
     Returns
     -------
     extractor : HDSortSortingExtractor
         The loaded data.
     """
+
     extractor_name = "HDSortSortingExtractor"
-    mode = 'file'
+    mode = "file"
     name = "hdsort"
 
     def __init__(self, file_path, keep_good_only=True):
-
         MatlabHelper.__init__(self, file_path)
 
         if not self._old_style_mat:
-            _units = self._data['Units']
+            _units = self._data["Units"]
             units = _parse_units(self._data, _units)
 
             # Extracting MutliElectrode field by field:
             _ME = self._data["MultiElectrode"]
             multi_electrode = dict((k, _ME.get(k)[()]) for k in _ME.keys())
 
             # Extracting sampling_frequency:
             sr = self._data["samplingRate"]
             sampling_frequency = float(_squeeze_ds(sr))
 
             # Remove noise units if necessary:
             if keep_good_only:
                 units = [unit for unit in units if unit["ID"].flatten()[0].astype(int) % 1000 != 0]
 
-            if 'sortingInfo' in self._data.keys():
+            if "sortingInfo" in self._data.keys():
                 info = self._data["sortingInfo"]
-                start_frame = _squeeze_ds(info['startTimes'])
+                start_frame = _squeeze_ds(info["startTimes"])
                 self.start_frame = int(start_frame)
             else:
                 self.start_frame = 0
         else:
-            _units = self._getfield('Units').squeeze()
+            _units = self._getfield("Units").squeeze()
             fields = _units.dtype.fields.keys()
             units = []
 
             for unit in _units:
                 unit_dict = {}
                 for f in fields:
                     unit_dict[f] = unit[f]
@@ -70,29 +69,29 @@
             _ME = self._data["MultiElectrode"]
             multi_electrode = dict((k, _ME[k][0][0].T) for k in _ME.dtype.fields.keys())
 
             # Remove noise units if necessary:
             if keep_good_only:
                 units = [unit for unit in units if unit["ID"].flatten()[0].astype(int) % 1000 != 0]
 
-            if 'sortingInfo' in self._data.keys():
+            if "sortingInfo" in self._data.keys():
                 info = self._getfield("sortingInfo")
-                start_frame = _squeeze_ds(info['startTimes'])
+                start_frame = _squeeze_ds(info["startTimes"])
                 self.start_frame = int(start_frame)
             else:
                 self.start_frame = 0
 
         self._units = units
         self._multi_electrode = multi_electrode
 
         unit_ids = []
         spiketrains = []
         for uc, unit in enumerate(units):
             unit_id = int(_squeeze_ds(unit["ID"]))
-            spike_times = _squeeze(unit["spikeTrain"]).astype('int64') - self.start_frame
+            spike_times = _squeeze(unit["spikeTrain"]).astype("int64") - self.start_frame
             unit_ids.append(unit_id)
             spiketrains.append(spike_times)
 
         BaseSorting.__init__(self, sampling_frequency, unit_ids)
 
         self.add_sorting_segment(HDSortSortingSegment(unit_ids, spiketrains))
 
@@ -105,15 +104,15 @@
             else:
                 template = unit["footprint"]
             templates.append(template)
             templates_frames_cut_before.append(unit["cutLeft"].flatten())
         self.set_property("template", np.array(templates))
         self.set_property("template_frames_cut_before", np.array(templates_frames_cut_before))
 
-        self._kwargs = {'file_path': str(file_path), 'keep_good_only': keep_good_only}
+        self._kwargs = {"file_path": str(file_path), "keep_good_only": keep_good_only}
 
         # TODO features
         # ~ for uc, unit in enumerate(units):
         # ~ unit_id = int(_squeeze_ds(unit["ID"]))
         # ~ self.set_unit_spike_features(unit_id, "amplitudes", _squeeze(unit["spikeAmplitudes"]))
         # ~ self.set_unit_spike_features(unit_id, "detection_channel", _squeeze(unit["detectionChannel"]).astype(np.int))
         # ~ idx = unit["detectionChannel"].astype(int) - 1
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/herdingspikesextractors.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/herdingspikesextractors.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,16 +1,17 @@
 from pathlib import Path
 
 import numpy as np
 
-from spikeinterface.core import (BaseSorting, BaseSortingSegment)
+from spikeinterface.core import BaseSorting, BaseSortingSegment
 from spikeinterface.core.core_tools import define_function_from_class
 
 try:
     import h5py
+
     HAVE_HS2SX = True
 except ImportError:
     HAVE_HS2SX = False
 
 
 class HerdingspikesSortingExtractor(BaseSorting):
     """Load HerdingSpikes format data as a sorting extractor.
@@ -24,43 +25,45 @@
 
     Returns
     -------
     extractor : HerdingSpikesSortingExtractor
         The loaded data.
     """
 
-    extractor_name = 'HS2Sorting'
+    extractor_name = "HS2Sorting"
     installed = HAVE_HS2SX  # check at class level if installed or not
-    mode = 'file'
-    installation_mesg = "To use the HS2SortingExtractor install h5py: \n\n pip install h5py\n\n"  # error message when not installed
+    mode = "file"
+    installation_mesg = (
+        "To use the HS2SortingExtractor install h5py: \n\n pip install h5py\n\n"  # error message when not installed
+    )
     name = "herdingspikes"
 
     def __init__(self, file_path, load_unit_info=True):
         assert self.installed, self.installation_mesg
 
         self._recording_file = file_path
-        self._rf = h5py.File(self._recording_file, mode='r')
-        if 'Sampling' in self._rf:
-            if self._rf['Sampling'][()] == 0:
+        self._rf = h5py.File(self._recording_file, mode="r")
+        if "Sampling" in self._rf:
+            if self._rf["Sampling"][()] == 0:
                 sampling_frequency = None
             else:
-                sampling_frequency = self._rf['Sampling'][()]
+                sampling_frequency = self._rf["Sampling"][()]
 
-        spike_ids = self._rf['cluster_id'][()]
+        spike_ids = self._rf["cluster_id"][()]
         unit_ids = np.unique(spike_ids)
-        spike_times = self._rf['times'][()]
+        spike_times = self._rf["times"][()]
 
         if load_unit_info:
             self.load_unit_info()
 
         BaseSorting.__init__(self, sampling_frequency, unit_ids)
         self.add_sorting_segment(HerdingspikesSortingSegment(unit_ids, spike_times, spike_ids))
-        self._kwargs = {'file_path': str(Path(file_path).absolute()), 'load_unit_info': load_unit_info}
+        self._kwargs = {"file_path": str(Path(file_path).absolute()), "load_unit_info": load_unit_info}
 
-        self.extra_requirements.append('h5py')
+        self.extra_requirements.append("h5py")
 
     def load_unit_info(self):
         # TODO
         """
         if 'centres' in self._rf.keys() and len(self._spike_times) > 0:
             self._unit_locs = self._rf['centres'][()]  # cache for faster access
             for u_i, unit_id in enumerate(self._unit_ids):
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/iblstreamingrecording.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/iblstreamingrecording.py`

 * *Files 6% similar despite different names*

```diff
@@ -5,22 +5,14 @@
 
 import numpy as np
 import probeinterface as pi
 
 from spikeinterface.core import BaseRecording, BaseRecordingSegment
 from spikeinterface.core.core_tools import define_function_from_class
 
-try:
-    import brainbox
-    from one.api import ONE
-
-    HAVE_BRAINBOX_ONE = True
-except ModuleNotFoundError:
-    HAVE_BRAINBOX_ONE = False
-
 
 class IblStreamingRecordingExtractor(BaseRecording):
     """
     Stream IBL data as an extractor object.
 
     Parameters
     ----------
@@ -52,20 +44,16 @@
     Returns
     -------
     recording : IblStreamingRecordingExtractor
         The recording extractor which allows access to the traces.
     """
 
     extractor_name = "IblStreamingRecording"
-    has_default_locations = True
-    installed = HAVE_BRAINBOX_ONE
     mode = "folder"
-    installation_mesg = (
-        "To use the IblStreamingRecordingSegment, install ONE-api and ibllib: \n\n pip install ONE-api\npip install ibllib\n"
-    )
+    installation_mesg = "To use the IblStreamingRecordingSegment, install ONE-api and ibllib: \n\n pip install ONE-api\npip install ibllib\n"
     name = "ibl_streaming_recording"
 
     @classmethod
     def get_stream_names(cls, session: str, cache_folder: Optional[Union[Path, str]] = None) -> List[str]:
         """
         Convenient retrieval of available stream names.
 
@@ -83,18 +71,26 @@
             each returned value in `sessions` refers to it as the 'id'.
 
         Returns
         -------
         stream_names : list of str
             List of stream names as expected by the `stream_name` argument for the class initialization.
         """
-        assert HAVE_BRAINBOX_ONE, cls.installation_mesg
+        try:
+            from one.api import ONE
+        except ImportError:
+            raise ImportError(IblStreamingRecordingExtractor.installation_mesg)
 
         cache_folder = Path(cache_folder) if cache_folder is not None else cache_folder
-        one = ONE(base_url="https://openalyx.internationalbrainlab.org", password="international", silent=True, cache_dir=cache_folder)
+        one = ONE(
+            base_url="https://openalyx.internationalbrainlab.org",
+            password="international",
+            silent=True,
+            cache_dir=cache_folder,
+        )
 
         dataset_contents = one.list_datasets(eid=session, collection="raw_ephys_data/*")
         raw_contents = [dataset_content for dataset_content in dataset_contents if not dataset_content.endswith(".npy")]
         probe_labels = set([raw_content.split("/")[1] for raw_content in raw_contents])
 
         stream_names = list()
         for probe_label in probe_labels:
@@ -112,22 +108,29 @@
         self,
         session: str,
         stream_name: str,
         load_sync_channel: bool = False,
         cache_folder: Optional[Union[Path, str]] = None,
         remove_cached: bool = True,
     ):
-        assert HAVE_BRAINBOX_ONE, self.installation_mesg
+        try:
+            from brainbox.io.spikeglx import Streamer
+            from one.api import ONE
+        except ImportError:
+            raise ImportError(self.installation_mesg)
 
-        from brainbox.io.spikeglx import Streamer
-        from one.api import ONE
         from neo.rawio.spikeglxrawio import read_meta_file, extract_stream_info
 
         cache_folder = Path(cache_folder) if cache_folder is not None else cache_folder
-        one = ONE(base_url="https://openalyx.internationalbrainlab.org", password="international", silent=True, cache_dir=cache_folder)
+        one = ONE(
+            base_url="https://openalyx.internationalbrainlab.org",
+            password="international",
+            silent=True,
+            cache_dir=cache_folder,
+        )
 
         session_names = self.get_stream_names(session=session, cache_folder=cache_folder)
         assert stream_name in session_names, (
             f"The `stream_name` '{stream_name}' was not found in the available listing for session '{session}'! "
             f"Please choose one of {session_names}."
         )
         probe_label, stream_type = stream_name.split(".")
@@ -146,15 +149,15 @@
         channel_ids = info["channel_names"]
         channel_gains = info["channel_gains"]
         channel_offsets = info["channel_offsets"]
         if not load_sync_channel:
             channel_ids = channel_ids[:-1]
             channel_gains = channel_gains[:-1]
             channel_offsets = channel_offsets[:-1]
-            
+
         # initialize main extractor
         sampling_frequency = self._file_streamer.fs
         dtype = self._file_streamer.dtype
         BaseRecording.__init__(self, channel_ids=channel_ids, sampling_frequency=sampling_frequency, dtype=dtype)
         self.set_channel_gains(channel_gains)
         self.set_channel_offsets(channel_offsets)
         self.extra_requirements.append("ONE-api")
@@ -186,29 +189,28 @@
         else:
             shank = np.concatenate((electrodes_geometry["shank"], [np.nan]))
             shank_row = np.concatenate((electrodes_geometry["shank"], [np.nan]))
             shank_col = np.concatenate((electrodes_geometry["shank"], [np.nan]))
             inter_sample_shift = np.concatenate((electrodes_geometry["sample_shift"], [np.nan]))
             adc = np.concatenate((electrodes_geometry["adc"], [np.nan]))
             index_on_probe = np.concatenate((electrodes_geometry["ind"], [np.nan]))
-            good_channel = np.concatenate((electrodes_geometry["shank"], [1.]))
+            good_channel = np.concatenate((electrodes_geometry["shank"], [1.0]))
 
         self.set_property("shank", shank)
         self.set_property("shank_row", shank_row)
         self.set_property("shank_col", shank_col)
         self.set_property("inter_sample_shift", inter_sample_shift)
         self.set_property("adc", adc)
         self.set_property("index_on_probe", index_on_probe)
         if not all(good_channel):
             self.set_property("good_channel", good_channel)
 
         # init recording segment
         recording_segment = IblStreamingRecordingSegment(
-            file_streamer=self._file_streamer,
-            load_sync_channel=load_sync_channel
+            file_streamer=self._file_streamer, load_sync_channel=load_sync_channel
         )
         self.add_recording_segment(recording_segment)
 
         self._kwargs = {
             "session": session,
             "stream_name": stream_name,
             "load_sync_channel": load_sync_channel,
@@ -236,8 +238,10 @@
         traces = self._file_streamer.read(nsel=slice(start_frame, end_frame), volts=False)
         if not self._load_sync_channel:
             traces = traces[:, :-1]
 
         return traces[:, channel_indices]
 
 
-read_ibl_streaming_recording = define_function_from_class(source_class=IblStreamingRecordingExtractor, name="read_ibl_streaming_recording")
+read_ibl_streaming_recording = define_function_from_class(
+    source_class=IblStreamingRecordingExtractor, name="read_ibl_streaming_recording"
+)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/klustaextractors.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/klustaextractors.py`

 * *Files 14% similar despite different names*

```diff
@@ -8,21 +8,20 @@
 04/08/20
 """
 
 from pathlib import Path
 
 import numpy as np
 
-from spikeinterface.core import (BaseRecording, BaseSorting,
-                                 BaseRecordingSegment, BaseSortingSegment,
-                                 read_python)
+from spikeinterface.core import BaseRecording, BaseSorting, BaseRecordingSegment, BaseSortingSegment, read_python
 from spikeinterface.core.core_tools import define_function_from_class
 
 try:
     import h5py
+
     HAVE_H5PY = True
 except ImportError:
     HAVE_H5PY = False
 
 
 # noinspection SpellCheckingInspection
 class KlustaSortingExtractor(BaseSorting):
@@ -37,79 +36,84 @@
 
     Returns
     -------
     extractor : KlustaSortingExtractor
         The loaded data.
     """
 
-    extractor_name = 'KlustaSortingExtractor'
+    extractor_name = "KlustaSortingExtractor"
     installed = HAVE_H5PY  # check at class level if installed or not
-    installation_mesg = "To use the KlustaSortingExtractor install h5py: \n\n pip install h5py\n\n"  # error message when not installed
-    mode = 'file_or_folder'
+    installation_mesg = (
+        "To use the KlustaSortingExtractor install h5py: \n\n pip install h5py\n\n"  # error message when not installed
+    )
+    mode = "file_or_folder"
     name = "klusta"
 
-    default_cluster_groups = {0: 'Noise', 1: 'MUA', 2: 'Good', 3: 'Unsorted'}
+    default_cluster_groups = {0: "Noise", 1: "MUA", 2: "Good", 3: "Unsorted"}
 
     def __init__(self, file_or_folder_path, exclude_cluster_groups=None):
         assert HAVE_H5PY, self.installation_mesg
         # ~ SortingExtractor.__init__(self)
 
         kwik_file_or_folder = Path(file_or_folder_path)
         kwikfile = None
         klustafolder = None
         if kwik_file_or_folder.is_file():
-            assert kwik_file_or_folder.suffix == '.kwik', "Not a '.kwik' file"
+            assert kwik_file_or_folder.suffix == ".kwik", "Not a '.kwik' file"
             kwikfile = Path(kwik_file_or_folder).absolute()
             klustafolder = kwikfile.parent
         elif kwik_file_or_folder.is_dir():
             klustafolder = kwik_file_or_folder
-            kwikfiles = [f for f in kwik_file_or_folder.iterdir() if f.suffix == '.kwik']
+            kwikfiles = [f for f in kwik_file_or_folder.iterdir() if f.suffix == ".kwik"]
             if len(kwikfiles) == 1:
                 kwikfile = kwikfiles[0]
         assert kwikfile is not None, "Could not load '.kwik' file"
 
         try:
-            config_file = [f for f in klustafolder.iterdir() if f.suffix == '.prm'][0]
+            config_file = [f for f in klustafolder.iterdir() if f.suffix == ".prm"][0]
             config = read_python(str(config_file))
-            sampling_frequency = config['traces']['sample_rate']
+            sampling_frequency = config["traces"]["sample_rate"]
         except Exception as e:
             print("Could not load sampling frequency info")
 
-        kf_reader = h5py.File(kwikfile, 'r')
+        kf_reader = h5py.File(kwikfile, "r")
         spiketrains = []
         unit_ids = []
         unique_units = []
         klusta_units = []
         cluster_groups_name = []
         groups = []
         unit = 0
 
         cs_to_exclude = []
         valid_group_names = [i[1].lower() for i in self.default_cluster_groups.items()]
         if exclude_cluster_groups is not None:
-            assert isinstance(exclude_cluster_groups, list), 'exclude_cluster_groups should be a list'
+            assert isinstance(exclude_cluster_groups, list), "exclude_cluster_groups should be a list"
             for ec in exclude_cluster_groups:
-                assert ec in valid_group_names, f'select exclude names out of: {valid_group_names}'
+                assert ec in valid_group_names, f"select exclude names out of: {valid_group_names}"
                 cs_to_exclude.append(ec.lower())
 
-        for channel_group in kf_reader.get('/channel_groups'):
-            chan_cluster_id_arr = kf_reader.get(f'/channel_groups/{channel_group}/spikes/clusters/main')[()]
-            chan_cluster_times_arr = kf_reader.get(f'/channel_groups/{channel_group}/spikes/time_samples')[()]
+        for channel_group in kf_reader.get("/channel_groups"):
+            chan_cluster_id_arr = kf_reader.get(f"/channel_groups/{channel_group}/spikes/clusters/main")[()]
+            chan_cluster_times_arr = kf_reader.get(f"/channel_groups/{channel_group}/spikes/time_samples")[()]
             chan_cluster_ids = np.unique(chan_cluster_id_arr)  # if clusters were merged in gui,
             # the original id's are still in the kwiktree, but
             # in this array
 
             for cluster_id in chan_cluster_ids:
                 cluster_frame_idx = np.nonzero(chan_cluster_id_arr == cluster_id)  # the [()] is a h5py thing
                 st = chan_cluster_times_arr[cluster_frame_idx]
-                assert st.shape[0] > 0, 'no spikes in cluster'
-                cluster_group = kf_reader.get(f'/channel_groups/{channel_group}/clusters/main/{cluster_id}').attrs[
-                    'cluster_group']
-
-                assert cluster_group in self.default_cluster_groups.keys(), f'cluster_group not in "default_dict: {cluster_group}'
+                assert st.shape[0] > 0, "no spikes in cluster"
+                cluster_group = kf_reader.get(f"/channel_groups/{channel_group}/clusters/main/{cluster_id}").attrs[
+                    "cluster_group"
+                ]
+
+                assert (
+                    cluster_group in self.default_cluster_groups.keys()
+                ), f'cluster_group not in "default_dict: {cluster_group}'
                 cluster_group_name = self.default_cluster_groups[cluster_group]
 
                 if cluster_group_name.lower() in cs_to_exclude:
                     continue
 
                 spiketrains.append(st)
 
@@ -118,28 +122,31 @@
                 unit += 1
                 groups.append(int(channel_group))
                 cluster_groups_name.append(cluster_group_name)
 
         if len(np.unique(klusta_units)) == len(np.unique(unique_units)):
             unit_ids = klusta_units
         else:
-            print('Klusta units are not unique! Using unique unit ids')
+            print("Klusta units are not unique! Using unique unit ids")
             unit_ids = unique_units
 
         BaseSorting.__init__(self, sampling_frequency, unit_ids)
-        self.is_dumpable = False
-        self.extra_requirements.append('h5py')
+
+        self.extra_requirements.append("h5py")
 
         self.add_sorting_segment(KlustSortingSegment(unit_ids, spiketrains))
 
-        self.set_property('group', groups)
+        self.set_property("group", groups)
         quality = [e.lower() for e in cluster_groups_name]
-        self.set_property('quality', quality)
+        self.set_property("quality", quality)
 
-        self._kwargs = {'file_or_folder_path': str(Path(file_or_folder_path).absolute())}
+        self._kwargs = {
+            "file_or_folder_path": str(Path(file_or_folder_path).absolute()),
+            "exclude_cluster_groups": exclude_cluster_groups,
+        }
 
 
 class KlustSortingSegment(BaseSortingSegment):
     def __init__(self, unit_ids, spiketrains):
         BaseSortingSegment.__init__(self)
         self._unit_ids = list(unit_ids)
         self._spiketrains = spiketrains
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/mclustextractors.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/mclustextractors.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 from pathlib import Path
 import re
 import numpy as np
 
-from spikeinterface.core import (BaseSorting, BaseSortingSegment)
+from spikeinterface.core import BaseSorting, BaseSortingSegment
 from spikeinterface.core.core_tools import define_function_from_class
 
 
 class MClustSortingExtractor(BaseSorting):
     """Load MClust sorting solution as a sorting extractor.
 
     Parameters
@@ -14,73 +14,74 @@
     folder_path : str or Path
         Path to folder with t files.
     sampling_frequency : sampling frequency
         sampling frequency in Hz.
     sampling_frequency_raw: float or None
         Required to read files with raw formats. In that case, the samples are saved in the same
         unit as the input data. Default None
-        Examples: 
-            - If raw time is in tens of ms sampling_frequency_raw=10000 
-            - If raw time is in samples sampling_frequency_raw=sampling_frequency 
+        Examples:
+            - If raw time is in tens of ms sampling_frequency_raw=10000
+            - If raw time is in samples sampling_frequency_raw=sampling_frequency
     Returns
     -------
     extractor : MClustSortingExtractor
         Loaded data.
     """
 
     extractor_name = "MClustSortingExtractor"
     name = "mclust"
 
-    def __init__(self, folder_path, sampling_frequency, sampling_frequency_raw = None):
-        end_header_str = '%%ENDHEADER'
-        ext_list = ['t64', 't32', 't', 'raw64','raw32']
+    def __init__(self, folder_path, sampling_frequency, sampling_frequency_raw=None):
+        end_header_str = "%%ENDHEADER"
+        ext_list = ["t64", "t32", "t", "raw64", "raw32"]
         unit_ids = []
         ext = None
 
         for e in ext_list:
-            files = Path(folder_path).glob(f'*.{e}')
+            files = Path(folder_path).glob(f"*.{e}")
             if files:
                 ext = e
                 break
 
         if ext is None:
             raise Exception("Mclust files not found in path")
 
-        if ext.startswith('raw') and sampling_frequency_raw is None: 
+        if ext.startswith("raw") and sampling_frequency_raw is None:
             raise Exception(f"To load files with extension {ext} a sampling_frequency_raw input is required.")
-        
-        if ext.endswith('64'):
-            dataformat='>u8'
+
+        if ext.endswith("64"):
+            dataformat = ">u8"
         else:
-            dataformat='>u4'
-        
+            dataformat = ">u4"
+
         spiketrains = {}
 
         for filename in files:
-            unit = int(re.search('_([0-9]+?)$',filename.stem).group(1))
+            unit = int(re.search("_([0-9]+?)$", filename.stem).group(1))
             unit_ids.append(unit)
-            with open(filename, 'rb') as f:
+            with open(filename, "rb") as f:
                 reading_header = True
                 while reading_header:
                     line = f.readline()
-                    reading_header = not line.decode('utf-8').startswith(end_header_str)
-                times = np.fromfile(f,dtype=dataformat) 
-            if ext.startswith('t'):
-                times = times/10000
+                    reading_header = not line.decode("utf-8").startswith(end_header_str)
+                times = np.fromfile(f, dtype=dataformat)
+            if ext.startswith("t"):
+                times = times / 10000
             else:
-                times = times/sampling_frequency_raw
+                times = times / sampling_frequency_raw
             spiketrains[unit] = np.rint(times * sampling_frequency)
 
-            
-
         BaseSorting.__init__(self, sampling_frequency, unit_ids)
 
         self.add_sorting_segment(MClustSortingSegment(unit_ids, spiketrains))
-        self._kwargs = {'folder_path': str(Path(folder_path).absolute()), 'sampling_frequency':sampling_frequency,
-                        'sampling_frequency_raw':sampling_frequency_raw}
+        self._kwargs = {
+            "folder_path": str(Path(folder_path).absolute()),
+            "sampling_frequency": sampling_frequency,
+            "sampling_frequency_raw": sampling_frequency_raw,
+        }
 
 
 class MClustSortingSegment(BaseSortingSegment):
     def __init__(self, unit_ids, spiketrains):
         BaseSortingSegment.__init__(self)
         self._unit_ids = list(unit_ids)
         self._spiketrains = spiketrains
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/mcsh5extractors.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/mcsh5extractors.py`

 * *Files 8% similar despite different names*

```diff
@@ -3,14 +3,15 @@
 import numpy as np
 
 from spikeinterface.core import BaseRecording, BaseRecordingSegment
 from spikeinterface.core.core_tools import define_function_from_class
 
 try:
     import h5py
+
     HAVE_MCSH5 = True
 except ImportError:
     HAVE_MCSH5 = False
 
 
 class MCSH5RecordingExtractor(BaseRecording):
     """Load a MCS H5 file as a recording extractor.
@@ -23,122 +24,134 @@
         The stream ID to load.
 
     Returns
     -------
     recording : MCSH5RecordingExtractor
         The loaded data.
     """
-    extractor_name = 'MCSH5Recording'
+
+    extractor_name = "MCSH5Recording"
     installed = HAVE_MCSH5  # check at class level if installed or not
-    mode = 'file'
-    installation_mesg = "To use the MCSH5RecordingExtractor install h5py: \n\n pip install h5py\n\n"  # error message when not installed
+    mode = "file"
+    installation_mesg = (
+        "To use the MCSH5RecordingExtractor install h5py: \n\n pip install h5py\n\n"  # error message when not installed
+    )
     name = "mcsh5"
 
     def __init__(self, file_path, stream_id=0):
         assert self.installed, self.installation_mesg
         self._file_path = file_path
 
         mcs_info = openMCSH5File(self._file_path, stream_id)
         self._rf = mcs_info["filehandle"]
 
-        BaseRecording.__init__(self, sampling_frequency=mcs_info["sampling_frequency"], channel_ids=mcs_info["channel_ids"],
-                               dtype=mcs_info["dtype"])
-
-        self.extra_requirements.append('h5py')
-
-        recording_segment = MCSH5RecordingSegment(self._rf, stream_id, mcs_info["num_frames"],
-                                                  sampling_frequency=mcs_info["sampling_frequency"])
+        BaseRecording.__init__(
+            self,
+            sampling_frequency=mcs_info["sampling_frequency"],
+            channel_ids=mcs_info["channel_ids"],
+            dtype=mcs_info["dtype"],
+        )
+
+        self.extra_requirements.append("h5py")
+
+        recording_segment = MCSH5RecordingSegment(
+            self._rf, stream_id, mcs_info["num_frames"], sampling_frequency=mcs_info["sampling_frequency"]
+        )
         self.add_recording_segment(recording_segment)
 
         # set gain
         self.set_channel_gains(mcs_info["gain"])
 
         # set other properties
         self.set_property("electrode_labels", mcs_info["electrode_labels"])
 
-        self._kwargs = {'file_path': str(Path(file_path).absolute()), 'stream_id': stream_id}
+        self._kwargs = {"file_path": str(Path(file_path).absolute()), "stream_id": stream_id}
 
     def __del__(self):
         self._rf.close()
 
 
 class MCSH5RecordingSegment(BaseRecordingSegment):
-
     def __init__(self, rf, stream_id, num_frames, sampling_frequency):
         BaseRecordingSegment.__init__(self, sampling_frequency=sampling_frequency)
         self._rf = rf
         self._stream_id = stream_id
         self._num_samples = int(num_frames)
-        self._stream = self._rf.require_group('/Data/Recording_0/AnalogStream/Stream_' + str(self._stream_id))
+        self._stream = self._rf.require_group("/Data/Recording_0/AnalogStream/Stream_" + str(self._stream_id))
 
     def get_num_samples(self):
         return self._num_samples
 
-    def get_traces(self,
-                   start_frame=None,
-                   end_frame=None,
-                   channel_indices=None):
-
+    def get_traces(self, start_frame=None, end_frame=None, channel_indices=None):
         if isinstance(channel_indices, slice):
-            traces = self._stream.get('ChannelData')[channel_indices, start_frame:end_frame].T
+            traces = self._stream.get("ChannelData")[channel_indices, start_frame:end_frame].T
         else:
             # channel_indices is np.ndarray
             if np.array(channel_indices).size > 1 and np.any(np.diff(channel_indices) < 0):
                 # get around h5py constraint that it does not allow datasets
                 # to be indexed out of order
                 sorted_channel_indices = np.sort(channel_indices)
                 resorted_indices = np.array([list(sorted_channel_indices).index(ch) for ch in channel_indices])
-                recordings = self._stream.get('ChannelData')[sorted_channel_indices, start_frame:end_frame].T
+                recordings = self._stream.get("ChannelData")[sorted_channel_indices, start_frame:end_frame].T
                 traces = recordings[:, resorted_indices]
             else:
-                traces = self._stream.get('ChannelData')[channel_indices, start_frame:end_frame].T
+                traces = self._stream.get("ChannelData")[channel_indices, start_frame:end_frame].T
 
         return traces
 
 
 def openMCSH5File(filename, stream_id):
     """Open an MCS hdf5 file, read and return the recording info."""
-    rf = h5py.File(filename, 'r')
+    rf = h5py.File(filename, "r")
 
-    stream_name = 'Stream_' + str(stream_id)
-    analog_stream_names = list(rf.require_group('/Data/Recording_0/AnalogStream').keys())
-    assert stream_name in analog_stream_names, (f"Specified stream does not exist. "
-                                                f"Available streams: {analog_stream_names}")
-
-    stream = rf.require_group('/Data/Recording_0/AnalogStream/' + stream_name)
-    data = stream.get('ChannelData')
-    timestamps = np.array(stream.get('ChannelDataTimeStamps'))
-    info = np.array(stream.get('InfoChannel'))
+    stream_name = "Stream_" + str(stream_id)
+    analog_stream_names = list(rf.require_group("/Data/Recording_0/AnalogStream").keys())
+    assert stream_name in analog_stream_names, (
+        f"Specified stream does not exist. " f"Available streams: {analog_stream_names}"
+    )
+
+    stream = rf.require_group("/Data/Recording_0/AnalogStream/" + stream_name)
+    data = stream.get("ChannelData")
+    timestamps = np.array(stream.get("ChannelDataTimeStamps"))
+    info = np.array(stream.get("InfoChannel"))
     dtype = data.dtype
 
-    Unit = info['Unit'][0]
-    Tick = info['Tick'][0] / 1e6
-    exponent = info['Exponent'][0]
-    convFact = info['ConversionFactor'][0]
-    gain = convFact.astype(float) * (10.0 ** exponent)
+    Unit = info["Unit"][0]
+    Tick = info["Tick"][0] / 1e6
+    exponent = info["Exponent"][0]
+    convFact = info["ConversionFactor"][0]
+    gain = convFact.astype(float) * (10.0**exponent)
 
     nRecCh, nFrames = data.shape
-    channel_ids = [f"Ch{ch}" for ch in info['ChannelID']]
-    assert len(np.unique(channel_ids)) == len(channel_ids), 'Duplicate MCS channel IDs found'
-    electrodeLabels = [l.decode() for l in info['Label']]
+    channel_ids = [f"Ch{ch}" for ch in info["ChannelID"]]
+    assert len(np.unique(channel_ids)) == len(channel_ids), "Duplicate MCS channel IDs found"
+    electrodeLabels = [l.decode() for l in info["Label"]]
 
-    assert timestamps[0][0] < timestamps[0][2], 'Please check the validity of \'ChannelDataTimeStamps\' in the stream.'
+    assert timestamps[0][0] < timestamps[0][2], "Please check the validity of 'ChannelDataTimeStamps' in the stream."
     TimeVals = np.arange(timestamps[0][0], timestamps[0][2] + 1, 1) * Tick
 
-    if Unit != b'V':
+    if Unit != b"V":
         print(f"Unexpected units found, expected volts, found {Unit.decode('UTF-8')}. Assuming Volts.")
 
     timestep_avg = np.mean(TimeVals[1:] - TimeVals[0:-1])
     timestep_min = np.min(TimeVals[1:] - TimeVals[0:-1])
     timestep_max = np.min(TimeVals[1:] - TimeVals[0:-1])
-    assert all(np.abs(np.array(
-        (timestep_min, timestep_max)) - timestep_avg) / timestep_avg < 1e-6), 'Time steps vary by more than 1 ppm'
-    samplingRate = 1. / timestep_avg
-
-    mcs_info = {"filehandle": rf, "num_frames": nFrames, "sampling_frequency": samplingRate,
-                "num_channels": nRecCh, "channel_ids": channel_ids, "electrode_labels": electrodeLabels, "gain": gain,
-                "dtype": dtype}
+    assert all(
+        np.abs(np.array((timestep_min, timestep_max)) - timestep_avg) / timestep_avg < 1e-6
+    ), "Time steps vary by more than 1 ppm"
+    samplingRate = 1.0 / timestep_avg
+
+    mcs_info = {
+        "filehandle": rf,
+        "num_frames": nFrames,
+        "sampling_frequency": samplingRate,
+        "num_channels": nRecCh,
+        "channel_ids": channel_ids,
+        "electrode_labels": electrodeLabels,
+        "gain": gain,
+        "dtype": dtype,
+    }
 
     return mcs_info
 
 
 read_mcsh5 = define_function_from_class(source_class=MCSH5RecordingExtractor, name="read_mcsh5")
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/mdaextractors.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/mdaextractors.py`

 * *Files 7% similar despite different names*

```diff
@@ -30,44 +30,56 @@
 
     Returns
     -------
     extractor : MdaRecordingExtractor
         The loaded data.
     """
 
-    extractor_name = 'MdaRecording'
-    has_default_locations = True
-    is_writable = True
-    mode = 'folder'
+    extractor_name = "MdaRecording"
+    mode = "folder"
     name = "mda"
 
-    def __init__(self, folder_path, raw_fname='raw.mda', params_fname='params.json', geom_fname='geom.csv'):
+    def __init__(self, folder_path, raw_fname="raw.mda", params_fname="params.json", geom_fname="geom.csv"):
         folder_path = Path(folder_path)
         self._folder_path = folder_path
         self._dataset_params = read_dataset_params(self._folder_path, params_fname)
         self._timeseries_path = self._folder_path / raw_fname
-        geom = np.loadtxt(self._folder_path / geom_fname, delimiter=',', ndmin=2)
+        geom = np.loadtxt(self._folder_path / geom_fname, delimiter=",", ndmin=2)
         self._diskreadmda = DiskReadMda(str(self._timeseries_path))
         dtype = self._diskreadmda.dt()
         num_channels = self._diskreadmda.N1()
-        assert geom.shape[0] == self._diskreadmda.N1(), f'Incompatible dimensions between geom.csv and timeseries ' \
-                                                        f'file: {geom.shape[0]} <> {self._diskreadmda.N1()}'
-        sampling_frequency=float(self._dataset_params['samplerate'])
-        BaseRecording.__init__(self, sampling_frequency=sampling_frequency,
-                               channel_ids=np.arange(num_channels), dtype=dtype)
+        assert geom.shape[0] == self._diskreadmda.N1(), (
+            f"Incompatible dimensions between geom.csv and timeseries "
+            f"file: {geom.shape[0]} <> {self._diskreadmda.N1()}"
+        )
+        sampling_frequency = float(self._dataset_params["samplerate"])
+        BaseRecording.__init__(
+            self, sampling_frequency=sampling_frequency, channel_ids=np.arange(num_channels), dtype=dtype
+        )
         rec_segment = MdaRecordingSegment(self._diskreadmda, sampling_frequency)
         self.add_recording_segment(rec_segment)
         self.set_dummy_probe_from_locations(geom)
-        self._kwargs = {'folder_path': str(Path(folder_path).absolute()),
-                        'raw_fname': raw_fname, 'params_fname': params_fname,
-                        'geom_fname': geom_fname}
+        self._kwargs = {
+            "folder_path": str(Path(folder_path).absolute()),
+            "raw_fname": raw_fname,
+            "params_fname": params_fname,
+            "geom_fname": geom_fname,
+        }
 
     @staticmethod
-    def write_recording(recording, save_path, params=dict(), raw_fname='raw.mda', params_fname='params.json',
-                        geom_fname='geom.csv', verbose=True, dtype=None, **job_kwargs):
+    def write_recording(
+        recording,
+        save_path,
+        params=dict(),
+        raw_fname="raw.mda",
+        params_fname="params.json",
+        geom_fname="geom.csv",
+        dtype=None,
+        **job_kwargs,
+    ):
         """Write a recording to file in MDA format.
 
         Parameters
         ----------
         recording: RecordingExtractor
             The recording extractor to be saved.
         save_path: str or Path
@@ -79,84 +91,90 @@
             File name of raw file. Defaults to 'raw.mda'.
         params_fname: str
             File name of params file. Defaults to 'params.json'.
         geom_fname: str
             File name of geom file. Defaults to 'geom.csv'.
         dtype: dtype
             Data type to be used. If None dtype is same as recording traces.
-        verbose: bool
-            If True, output is verbose.
         **job_kwargs:
             Use by job_tools modules to set:
 
                 * chunk_size or chunk_memory, or total_memory
                 * n_jobs
                 * progress_bar
         """
         job_kwargs = fix_job_kwargs(job_kwargs)
-        assert recording.get_num_segments() == 1, "MdaRecording.write_recording() can only write a single segment " \
-                                                  "recording"
+        assert recording.get_num_segments() == 1, (
+            "MdaRecording.write_recording() can only write a single segment " "recording"
+        )
         save_path = Path(save_path)
         save_path.mkdir(parents=True, exist_ok=True)
         save_file_path = save_path / raw_fname
         parent_dir = save_path
-        num_chan = recording.get_num_channels()
+        num_channels = recording.get_num_channels()
         num_frames = recording.get_num_frames(0)
 
         geom = recording.get_channel_locations()
 
         if dtype is None:
             dtype = recording.get_dtype()
 
-        if dtype == 'float':
-            dtype = 'float32'
-        if dtype == 'int':
-            dtype = 'int16'
+        if dtype == "float":
+            dtype = "float32"
+        if dtype == "int":
+            dtype = "int16"
 
-        header = MdaHeader(dt0=dtype, dims0=(num_chan, num_frames))
+        header = MdaHeader(dt0=dtype, dims0=(num_channels, num_frames))
         header_size = header.header_size
 
-        write_binary_recording(recording, file_paths=save_file_path, dtype=dtype,
-                               byte_offset=header_size, verbose=verbose, add_file_extension=False, **job_kwargs)
+        write_binary_recording(
+            recording,
+            file_paths=save_file_path,
+            dtype=dtype,
+            byte_offset=header_size,
+            add_file_extension=False,
+            **job_kwargs,
+        )
 
-        with save_file_path.open('rb+') as f:
+        with save_file_path.open("rb+") as f:
             header.write(f)
 
         params["samplerate"] = float(recording.get_sampling_frequency())
-        with (parent_dir / params_fname).open('w') as f:
+        with (parent_dir / params_fname).open("w") as f:
             json.dump(params, f)
-        np.savetxt(str(parent_dir / geom_fname), geom, delimiter=',')
+        np.savetxt(str(parent_dir / geom_fname), geom, delimiter=",")
 
 
 class MdaRecordingSegment(BaseRecordingSegment):
-
     def __init__(self, diskreadmda, sampling_frequency):
         self._diskreadmda = diskreadmda
         BaseRecordingSegment.__init__(self, sampling_frequency=sampling_frequency)
         self._num_samples = self._diskreadmda.N2()
 
     def get_num_samples(self):
         """Returns the number of samples in this signal block
 
         Returns:
             SampleIndex: Number of samples in the signal block
         """
         return self._num_samples
 
-    def get_traces(self,
-                   start_frame: Union[int, None] = None,
-                   end_frame: Union[int, None] = None,
-                   channel_indices: Union[List, None] = None,
-                   ) -> np.ndarray:
+    def get_traces(
+        self,
+        start_frame: Union[int, None] = None,
+        end_frame: Union[int, None] = None,
+        channel_indices: Union[List, None] = None,
+    ) -> np.ndarray:
         if start_frame is None:
             start_frame = 0
         if end_frame is None:
             end_frame = self.get_num_samples()
-        recordings = self._diskreadmda.readChunk(i1=0, i2=start_frame, N1=self._diskreadmda.N1(),
-                                                 N2=end_frame - start_frame)
+        recordings = self._diskreadmda.readChunk(
+            i1=0, i2=start_frame, N1=self._diskreadmda.N1(), N2=end_frame - start_frame
+        )
         recordings = recordings[channel_indices, :].T
         return recordings
 
 
 class MdaSortingExtractor(BaseSorting):
     """Load MDA format data as a sorting extractor.
 
@@ -169,49 +187,48 @@
 
     Returns
     -------
     extractor : MdaRecordingExtractor
         The loaded data.
     """
 
-    extractor_name = 'MdaSorting'
-    is_writable = True
-    mode = 'file'
+    extractor_name = "MdaSorting"
+    mode = "file"
     name = "mda"
 
     def __init__(self, file_path, sampling_frequency):
         firings = readmda(str(file_path))
         labels = firings[2, :]
         unit_ids = np.unique(labels).astype(int)
         BaseSorting.__init__(self, unit_ids=unit_ids, sampling_frequency=sampling_frequency)
 
         sorting_segment = MdaSortingSegment(firings)
         self.add_sorting_segment(sorting_segment)
 
-        self._kwargs = {'file_path': str(Path(file_path).absolute()), 'sampling_frequency': sampling_frequency}
+        self._kwargs = {"file_path": str(Path(file_path).absolute()), "sampling_frequency": sampling_frequency}
 
     @staticmethod
     def write_sorting(sorting, save_path, write_primary_channels=False):
-        assert sorting.get_num_segments() == 1, "MdaSorting.write_sorting() can only write a single segment " \
-                                                "sorting"
+        assert sorting.get_num_segments() == 1, "MdaSorting.write_sorting() can only write a single segment " "sorting"
         unit_ids = sorting.get_unit_ids()
         times_list = []
         labels_list = []
         primary_channels_list = []
         for unit_id in unit_ids:
             times = sorting.get_unit_spike_train(unit_id=unit_id)
             times_list.append(times)
             labels_list.append(np.ones(times.shape) * unit_id)
             if write_primary_channels:
-                if 'max_channel' in sorting.get_unit_property_names(unit_id):
-                    primary_channels_list.append([sorting.get_unit_property(unit_id, 'max_channel')] * times.shape[0])
+                if "max_channel" in sorting.get_unit_property_names(unit_id):
+                    primary_channels_list.append([sorting.get_unit_property(unit_id, "max_channel")] * times.shape[0])
                 else:
                     raise ValueError(
-                        "Unable to write primary channels because 'max_channel' spike feature not set in unit " + str(
-                            unit_id))
+                        "Unable to write primary channels because 'max_channel' spike feature not set in unit "
+                        + str(unit_id)
+                    )
             else:
                 primary_channels_list.append(np.zeros(times.shape))
         all_times = _concatenate(times_list)
         all_labels = _concatenate(labels_list)
         all_primary_channels = _concatenate(primary_channels_list)
         sort_inds = np.argsort(all_times)
         all_times = all_times[sort_inds]
@@ -230,50 +247,53 @@
     def __init__(self, firings):
         self._firings = firings
         self._max_channels = self._firings[0, :]
         self._spike_times = self._firings[1, :]
         self._labels = self._firings[2, :]
         BaseSortingSegment.__init__(self)
 
-    def get_unit_spike_train(self,
-                             unit_id,
-                             start_frame: Union[int, None] = None,
-                             end_frame: Union[int, None] = None,
-                             ) -> np.ndarray:
+    def get_unit_spike_train(
+        self,
+        unit_id,
+        start_frame: Union[int, None] = None,
+        end_frame: Union[int, None] = None,
+    ) -> np.ndarray:
         # must be implemented in subclass
         if start_frame is None:
             start_frame = 0
         if end_frame is None:
             end_frame = np.inf
         inds = np.where(
-            (self._labels == unit_id) & (start_frame <= self._spike_times) & (self._spike_times < end_frame))
+            (self._labels == unit_id) & (start_frame <= self._spike_times) & (self._spike_times < end_frame)
+        )
         return np.rint(self._spike_times[inds]).astype(int)
 
 
 read_mda_recording = define_function_from_class(source_class=MdaRecordingExtractor, name="read_mda_recording")
 read_mda_sorting = define_function_from_class(source_class=MdaSortingExtractor, name="read_mda_sorting")
 
+
 def _concatenate(list):
     if len(list) == 0:
         return np.array([])
     return np.concatenate(list)
 
 
 def read_dataset_params(dsdir, params_fname):
     fname1 = dsdir / params_fname
     if not fname1.is_file():
-        raise Exception('Dataset parameter file does not exist: ' + fname1)
+        raise Exception("Dataset parameter file does not exist: " + fname1)
     with open(fname1) as f:
         return json.load(f)
 
 
 ######### MDAIO ###########
 class MdaHeader:
     def __init__(self, dt0, dims0):
-        uses64bitdims = (max(dims0) > 2e9)
+        uses64bitdims = max(dims0) > 2e9
         self.uses64bitdims = uses64bitdims
         self.dt_code = _dt_code_from_dt(dt0)
         self.dt = dt0
         self.num_bytes_per_entry = get_num_bytes_per_entry_from_dt(dt0)
         self.num_dims = len(dims0)
         self.dimprod = np.prod(dims0)
         self.dims = dims0
@@ -295,100 +315,100 @@
             for j in range(0, H.num_dims):
                 _write_int32(f, H.dims[j])
 
 
 def npy_dtype_to_string(dt):
     str = dt.str[1:]
     map = {
-        "f2": 'float16',
-        "f4": 'float32',
-        "f8": 'float64',
-        "i1": 'int8',
-        "i2": 'int16',
-        "i4": 'int32',
-        "u2": 'uint16',
-        "u4": 'uint32'
+        "f2": "float16",
+        "f4": "float32",
+        "f8": "float64",
+        "i1": "int8",
+        "i2": "int16",
+        "i4": "int32",
+        "u2": "uint16",
+        "u4": "uint32",
     }
     return map[str]
 
 
 class DiskReadMda:
     def __init__(self, path, header=None):
         self._npy_mode = False
         self._path = path
-        if file_extension(path) == '.npy':
-            raise Exception('DiskReadMda implementation has not been tested for npy files')
+        if file_extension(path) == ".npy":
+            raise Exception("DiskReadMda implementation has not been tested for npy files")
             self._npy_mode = True
             if header:
-                raise Exception('header not allowed in npy mode for DiskReadMda')
+                raise Exception("header not allowed in npy mode for DiskReadMda")
         if header:
             self._header = header
             self._header.header_size = 0
         else:
             self._header = _read_header(self._path)
 
     def dims(self):
         if self._npy_mode:
-            A = np.load(self._path, mmap_mode='r')
+            A = np.load(self._path, mmap_mode="r")
             return A.shape
         return self._header.dims
 
     def N1(self):
         return self.dims()[0]
 
     def N2(self):
         return self.dims()[1]
 
     def N3(self):
         return self.dims()[2]
 
     def dt(self):
         if self._npy_mode:
-            A = np.load(self._path, mmap_mode='r')
+            A = np.load(self._path, mmap_mode="r")
             return npy_dtype_to_string(A.dtype)
         return self._header.dt
 
     def numBytesPerEntry(self):
         if self._npy_mode:
-            A = np.load(self._path, mmap_mode='r')
+            A = np.load(self._path, mmap_mode="r")
             return A.itemsize
         return self._header.num_bytes_per_entry
 
     def readChunk(self, i1=-1, i2=-1, i3=-1, N1=1, N2=1, N3=1):
         # print("Reading chunk {} {} {} {} {} {}".format(i1,i2,i3,N1,N2,N3))
         if i2 < 0:
             if self._npy_mode:
-                A = np.load(self._path, mmap_mode='r')
-                return A[:, :, i1:i1 + N1]
+                A = np.load(self._path, mmap_mode="r")
+                return A[:, :, i1 : i1 + N1]
             return self._read_chunk_1d(i1, N1)
         elif i3 < 0:
             if N1 != self.N1():
                 print("Unable to support N1 {} != {}".format(N1, self.N1()))
                 return None
             X = self._read_chunk_1d(i1 + N1 * i2, N1 * N2)
 
             if X is None:
-                print('Problem reading chunk from file: ' + self._path)
+                print("Problem reading chunk from file: " + self._path)
                 return None
             if self._npy_mode:
-                A = np.load(self._path, mmap_mode='r')
-                return A[:, i2:i2 + N2]
-            return np.reshape(X, (N1, N2), order='F')
+                A = np.load(self._path, mmap_mode="r")
+                return A[:, i2 : i2 + N2]
+            return np.reshape(X, (N1, N2), order="F")
         else:
             if N1 != self.N1():
                 print("Unable to support N1 {} != {}".format(N1, self.N1()))
                 return None
             if N2 != self.N2():
                 print("Unable to support N2 {} != {}".format(N2, self.N2()))
                 return None
             if self._npy_mode:
-                A = np.load(self._path, mmap_mode='r')
-                return A[:, :, i3:i3 + N3]
+                A = np.load(self._path, mmap_mode="r")
+                return A[:, :, i3 : i3 + N3]
             X = self._read_chunk_1d(i1 + N1 * i2 + N1 * N2 * i3, N1 * N2 * N3)
-            return np.reshape(X, (N1, N2, N3), order='F')
+            return np.reshape(X, (N1, N2, N3), order="F")
 
     def _read_chunk_1d(self, i, N):
         offset = self._header.header_size + self._header.num_bytes_per_entry * i
         if is_url(self._path):
             tmp_fname = _download_bytes_to_tmpfile(self._path, offset, offset + self._header.num_bytes_per_entry * N)
             try:
                 ret = self._read_chunk_1d_helper(tmp_fname, N, offset=0)
@@ -407,37 +427,37 @@
         except Exception as e:  # catch *all* exceptions
             print(e)
             f.close()
             return None
 
 
 def is_url(path):
-    return path.startswith('http://') or path.startswith('https://')
+    return path.startswith("http://") or path.startswith("https://")
 
 
 def _download_bytes_to_tmpfile(url, start, end):
     try:
         import requests
     except:
-        raise Exception('Unable to import module: requests')
+        raise Exception("Unable to import module: requests")
     headers = {"Range": "bytes={}-{}".format(start, end - 1)}
     r = requests.get(url, headers=headers, stream=True)
     fd, tmp_fname = tempfile.mkstemp()
-    with open(tmp_fname, 'wb') as f:
+    with open(tmp_fname, "wb") as f:
         for chunk in r.iter_content(chunk_size=1024):
             if chunk:
                 f.write(chunk)
     return tmp_fname
 
 
 def _read_header(path):
     if is_url(path):
         tmp_fname = _download_bytes_to_tmpfile(path, 0, 200)
         if not tmp_fname:
-            raise Exception('Problem downloading bytes from ' + path)
+            raise Exception("Problem downloading bytes from " + path)
         try:
             ret = _read_header(tmp_fname)
         except:
             ret = None
         Path(tmp_fname).unlink()
         return ret
 
@@ -481,71 +501,71 @@
         print(e)
         f.close()
         return None
 
 
 def _dt_from_dt_code(dt_code):
     if dt_code == -2:
-        dt = 'uint8'
+        dt = "uint8"
     elif dt_code == -3:
-        dt = 'float32'
+        dt = "float32"
     elif dt_code == -4:
-        dt = 'int16'
+        dt = "int16"
     elif dt_code == -5:
-        dt = 'int32'
+        dt = "int32"
     elif dt_code == -6:
-        dt = 'uint16'
+        dt = "uint16"
     elif dt_code == -7:
-        dt = 'float64'
+        dt = "float64"
     elif dt_code == -8:
-        dt = 'uint32'
+        dt = "uint32"
     else:
         dt = None
     return dt
 
 
 def _dt_code_from_dt(dt):
-    if dt == 'uint8':
+    if dt == "uint8":
         return -2
-    if dt == 'float32':
+    if dt == "float32":
         return -3
-    if dt == 'int16':
+    if dt == "int16":
         return -4
-    if dt == 'int32':
+    if dt == "int32":
         return -5
-    if dt == 'uint16':
+    if dt == "uint16":
         return -6
-    if dt == 'float64':
+    if dt == "float64":
         return -7
-    if dt == 'uint32':
+    if dt == "uint32":
         return -8
     return None
 
 
 def get_num_bytes_per_entry_from_dt(dt):
-    if dt == 'uint8':
+    if dt == "uint8":
         return 1
-    if dt == 'float32':
+    if dt == "float32":
         return 4
-    if dt == 'int16':
+    if dt == "int16":
         return 2
-    if dt == 'int32':
+    if dt == "int32":
         return 4
-    if dt == 'uint16':
+    if dt == "uint16":
         return 2
-    if dt == 'float64':
+    if dt == "float64":
         return 8
-    if dt == 'uint32':
+    if dt == "uint32":
         return 4
     return None
 
 
 def readmda_header(path):
-    if file_extension(path) == '.npy':
-        raise Exception('Cannot read mda header for .npy file.')
+    if file_extension(path) == ".npy":
+        raise Exception("Cannot read mda header for .npy file.")
     return _read_header(path)
 
 
 def _write_header(path, H, rewrite=False):
     if rewrite:
         f = open(path, "r+b")
     else:
@@ -566,102 +586,102 @@
     except Exception as e:  # catch *all* exceptions
         print(e)
         f.close()
         return False
 
 
 def readmda(path):
-    if file_extension(path) == '.npy':
-        return readnpy(path);
+    if file_extension(path) == ".npy":
+        return readnpy(path)
     H = _read_header(path)
     if H is None:
         print("Problem reading header of: {}".format(path))
         return None
     f = open(path, "rb")
     try:
         f.seek(H.header_size)
         # This is how I do the column-major order
         ret = np.fromfile(f, dtype=H.dt, count=H.dimprod)
-        ret = np.reshape(ret, H.dims, order='F')
+        ret = np.reshape(ret, H.dims, order="F")
         f.close()
         return ret
     except Exception as e:  # catch *all* exceptions
         print(e)
         f.close()
         return None
 
 
 def writemda32(X, fname):
-    if file_extension(fname) == '.npy':
+    if file_extension(fname) == ".npy":
         return writenpy32(X, fname)
-    return _writemda(X, fname, 'float32')
+    return _writemda(X, fname, "float32")
 
 
 def writemda64(X, fname):
-    if file_extension(fname) == '.npy':
+    if file_extension(fname) == ".npy":
         return writenpy64(X, fname)
-    return _writemda(X, fname, 'float64')
+    return _writemda(X, fname, "float64")
 
 
 def writemda8(X, fname):
-    if file_extension(fname) == '.npy':
+    if file_extension(fname) == ".npy":
         return writenpy8(X, fname)
-    return _writemda(X, fname, 'uint8')
+    return _writemda(X, fname, "uint8")
 
 
 def writemda32i(X, fname):
-    if file_extension(fname) == '.npy':
+    if file_extension(fname) == ".npy":
         return writenpy32i(X, fname)
-    return _writemda(X, fname, 'int32')
+    return _writemda(X, fname, "int32")
 
 
 def writemda32ui(X, fname):
-    if file_extension(fname) == '.npy':
+    if file_extension(fname) == ".npy":
         return writenpy32ui(X, fname)
-    return _writemda(X, fname, 'uint32')
+    return _writemda(X, fname, "uint32")
 
 
 def writemda16i(X, fname):
-    if file_extension(fname) == '.npy':
+    if file_extension(fname) == ".npy":
         return writenpy16i(X, fname)
-    return _writemda(X, fname, 'int16')
+    return _writemda(X, fname, "int16")
 
 
 def writemda16ui(X, fname):
-    if file_extension(fname) == '.npy':
+    if file_extension(fname) == ".npy":
         return writenpy16ui(X, fname)
-    return _writemda(X, fname, 'uint16')
+    return _writemda(X, fname, "uint16")
 
 
 def writemda(X, fname, *, dtype):
     return _writemda(X, fname, dtype)
 
 
 def _writemda(X, fname, dt):
     num_bytes_per_entry = get_num_bytes_per_entry_from_dt(dt)
     dt_code = _dt_code_from_dt(dt)
     if dt_code is None:
         print("Unexpected data type: {}".format(dt))
         return False
 
     if type(fname) == str:
-        f = open(fname, 'wb')
+        f = open(fname, "wb")
     else:
         f = fname
     try:
         _write_int32(f, dt_code)
         _write_int32(f, num_bytes_per_entry)
         _write_int32(f, X.ndim)
         for j in range(0, X.ndim):
             _write_int32(f, X.shape[j])
         # This is how I do column-major order
         # A=np.reshape(X,X.size,order='F').astype(dt)
         # A.tofile(f)
 
-        bytes0 = X.astype(dt).tobytes(order='F')
+        bytes0 = X.astype(dt).tobytes(order="F")
         f.write(bytes0)
 
         if type(fname) == str:
             f.close()
         return True
     except Exception as e:  # catch *all* exceptions
         traceback.print_exc()
@@ -672,55 +692,55 @@
 
 
 def readnpy(path):
     return np.load(path)
 
 
 def writenpy8(X, path):
-    return _writenpy(X, path, dtype='int8')
+    return _writenpy(X, path, dtype="int8")
 
 
 def writenpy32(X, path):
-    return _writenpy(X, path, dtype='float32')
+    return _writenpy(X, path, dtype="float32")
 
 
 def writenpy64(X, path):
-    return _writenpy(X, path, dtype='float64')
+    return _writenpy(X, path, dtype="float64")
 
 
 def writenpy16i(X, path):
-    return _writenpy(X, path, dtype='int16')
+    return _writenpy(X, path, dtype="int16")
 
 
 def writenpy16ui(X, path):
-    return _writenpy(X, path, dtype='uint16')
+    return _writenpy(X, path, dtype="uint16")
 
 
 def writenpy32i(X, path):
-    return _writenpy(X, path, dtype='int32')
+    return _writenpy(X, path, dtype="int32")
 
 
 def writenpy32ui(X, path):
-    return _writenpy(X, path, dtype='uint32')
+    return _writenpy(X, path, dtype="uint32")
 
 
 def writenpy(X, path, *, dtype):
     return _writenpy(X, path, dtype=dtype)
 
 
 def _writenpy(X, path, *, dtype):
     np.save(path, X.astype(dtype=dtype, copy=False))  # astype will always create copy if dtype does not match
     # apparently allowing pickling is a security issue. (according to the docs) ??
     # np.save(path,X.astype(dtype=dtype,copy=False),allow_pickle=False) # astype will always create copy if dtype does not match
     return True
 
 
 def appendmda(X, path):
-    if file_extension(path) == '.npy':
-        raise Exception('appendmda not yet implemented for .npy files')
+    if file_extension(path) == ".npy":
+        raise Exception("appendmda not yet implemented for .npy files")
     H = _read_header(path)
     if H is None:
         print("Problem reading header of: {}".format(path))
         return None
     if len(H.dims) != len(X.shape):
         print("Incompatible number of dimensions in appendmda", H.dims, X.shape)
         return None
@@ -731,15 +751,15 @@
             print("Incompatible dimensions in appendmda", H.dims, X.shape)
             return None
     H.dims[num_dims - 1] = H.dims[num_dims - 1] + X.shape[num_dims - 1]
     try:
         _write_header(path, H, rewrite=True)
         f = open(path, "r+b")
         f.seek(H.header_size + H.num_bytes_per_entry * num_entries_old)
-        A = np.reshape(X, X.size, order='F').astype(H.dt)
+        A = np.reshape(X, X.size, order="F").astype(H.dt)
         A.tofile(f)
         f.close()
     except Exception as e:  # catch *all* exceptions
         print(e)
         f.close()
         return False
 
@@ -749,27 +769,27 @@
         filename, ext = os.path.splitext(fname)
         return ext
     else:
         return None
 
 
 def _read_int32(f):
-    return struct.unpack('<i', f.read(4))[0]
+    return struct.unpack("<i", f.read(4))[0]
 
 
 def _read_int64(f):
-    return struct.unpack('<q', f.read(8))[0]
+    return struct.unpack("<q", f.read(8))[0]
 
 
 def _write_int32(f, val):
-    f.write(struct.pack('<i', val))
+    f.write(struct.pack("<i", val))
 
 
 def _write_int64(f, val):
-    f.write(struct.pack('<q', val))
+    f.write(struct.pack("<q", val))
 
 
 def _header_from_file(f):
     try:
         dt_code = _read_int32(f)
         num_bytes_per_entry = _read_int32(f)
         num_dims = _read_int32(f)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/__init__.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/__init__.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,46 +1,38 @@
-from .alphaomega import (AlphaOmegaRecordingExtractor, AlphaOmegaEventExtractor, 
-                         read_alphaomega, read_alphaomega_event)
-from .axona import (AxonaRecordingExtractor, 
-                    read_axona)
-from .biocam import (BiocamRecordingExtractor, 
-                     read_biocam)
-from .blackrock import (BlackrockRecordingExtractor, BlackrockSortingExtractor,
-                        read_blackrock, read_blackrock_sorting)
-from .ced import (CedRecordingExtractor, 
-                  read_ced)
-from .edf import (EDFRecordingExtractor, 
-                  read_edf)
-from .intan import (IntanRecordingExtractor, 
-                    read_intan)
-from .maxwell import (MaxwellRecordingExtractor, MaxwellEventExtractor, 
-                      read_maxwell, read_maxwell_event)
-from .mearec import (MEArecRecordingExtractor, MEArecSortingExtractor,
-                     read_mearec)
-from .mcsraw import (MCSRawRecordingExtractor, 
-                     read_mcsraw)
-from .neuralynx import (NeuralynxRecordingExtractor, NeuralynxSortingExtractor,
-                        read_neuralynx, read_neuralynx_sorting)
-from .neuroscope import (NeuroScopeRecordingExtractor, NeuroScopeSortingExtractor,
-                         read_neuroscope_recording, read_neuroscope_sorting, read_neuroscope)
-from .nix import (NixRecordingExtractor, 
-                  read_nix)
-from .openephys import (OpenEphysLegacyRecordingExtractor,
-                        OpenEphysBinaryRecordingExtractor, OpenEphysBinaryEventExtractor, 
-                        read_openephys, read_openephys_event)
-from .plexon import (PlexonRecordingExtractor, PlexonSortingExtractor,
-                     read_plexon, read_plexon_sorting)
-from .spike2 import (Spike2RecordingExtractor, 
-                     read_spike2)
-from .spikegadgets import (SpikeGadgetsRecordingExtractor, 
-                           read_spikegadgets)
-from .spikeglx import (SpikeGLXRecordingExtractor, 
-                       read_spikeglx)
-from .tdt import (TdtRecordingExtractor, 
-                  read_tdt)
+from .alphaomega import AlphaOmegaRecordingExtractor, AlphaOmegaEventExtractor, read_alphaomega, read_alphaomega_event
+from .axona import AxonaRecordingExtractor, read_axona
+from .biocam import BiocamRecordingExtractor, read_biocam
+from .blackrock import BlackrockRecordingExtractor, BlackrockSortingExtractor, read_blackrock, read_blackrock_sorting
+from .ced import CedRecordingExtractor, read_ced
+from .edf import EDFRecordingExtractor, read_edf
+from .intan import IntanRecordingExtractor, read_intan
+from .maxwell import MaxwellRecordingExtractor, MaxwellEventExtractor, read_maxwell, read_maxwell_event
+from .mearec import MEArecRecordingExtractor, MEArecSortingExtractor, read_mearec
+from .mcsraw import MCSRawRecordingExtractor, read_mcsraw
+from .neuralynx import NeuralynxRecordingExtractor, NeuralynxSortingExtractor, read_neuralynx, read_neuralynx_sorting
+from .neuroscope import (
+    NeuroScopeRecordingExtractor,
+    NeuroScopeSortingExtractor,
+    read_neuroscope_recording,
+    read_neuroscope_sorting,
+    read_neuroscope,
+)
+from .nix import NixRecordingExtractor, read_nix
+from .openephys import (
+    OpenEphysLegacyRecordingExtractor,
+    OpenEphysBinaryRecordingExtractor,
+    OpenEphysBinaryEventExtractor,
+    read_openephys,
+    read_openephys_event,
+)
+from .plexon import PlexonRecordingExtractor, PlexonSortingExtractor, read_plexon, read_plexon_sorting
+from .spike2 import Spike2RecordingExtractor, read_spike2
+from .spikegadgets import SpikeGadgetsRecordingExtractor, read_spikegadgets
+from .spikeglx import SpikeGLXRecordingExtractor, read_spikeglx
+from .tdt import TdtRecordingExtractor, read_tdt
 
 from .neo_utils import get_neo_streams, get_neo_num_blocks
 
 neo_recording_extractors_list = [
     AlphaOmegaRecordingExtractor,
     AxonaRecordingExtractor,
     BiocamRecordingExtractor,
@@ -56,20 +48,13 @@
     NixRecordingExtractor,
     OpenEphysBinaryRecordingExtractor,
     OpenEphysLegacyRecordingExtractor,
     PlexonRecordingExtractor,
     Spike2RecordingExtractor,
     SpikeGadgetsRecordingExtractor,
     SpikeGLXRecordingExtractor,
-    TdtRecordingExtractor
+    TdtRecordingExtractor,
 ]
 
-neo_sorting_extractors_list = [
-    BlackrockSortingExtractor,
-    MEArecSortingExtractor,
-    NeuralynxSortingExtractor
-]
+neo_sorting_extractors_list = [BlackrockSortingExtractor, MEArecSortingExtractor, NeuralynxSortingExtractor]
 
-neo_event_extractors_list = [
-    AlphaOmegaEventExtractor,
-    OpenEphysBinaryEventExtractor
-]
+neo_event_extractors_list = [AlphaOmegaEventExtractor, OpenEphysBinaryEventExtractor]
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/alphaomega.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/alphaomega.py`

 * *Files 7% similar despite different names*

```diff
@@ -18,25 +18,24 @@
     stream_id: {'RAW', 'LFP', 'SPK', 'ACC', 'AI', 'UD'}, optional
         If there are several streams, specify the stream id you want to load.
     stream_name: str, optional
         If there are several streams, specify the stream name you want to load.
     all_annotations: bool, default: False
         Load exhaustively all annotations from neo.
     """
+
     mode = "folder"
     NeoRawIOClass = "AlphaOmegaRawIO"
     name = "alphaomega"
 
-    def __init__(self, folder_path, lsx_files=None, stream_id="RAW", 
-                 stream_name=None, all_annotations=False):
+    def __init__(self, folder_path, lsx_files=None, stream_id="RAW", stream_name=None, all_annotations=False):
         neo_kwargs = self.map_to_neo_kwargs(folder_path, lsx_files)
-        NeoBaseRecordingExtractor.__init__(self, stream_id=stream_id, 
-                                           stream_name=stream_name,
-                                           all_annotations=all_annotations,
-                                           **neo_kwargs)
+        NeoBaseRecordingExtractor.__init__(
+            self, stream_id=stream_id, stream_name=stream_name, all_annotations=all_annotations, **neo_kwargs
+        )
         self._kwargs.update(dict(folder_path=str(folder_path), lsx_files=lsx_files))
 
     @classmethod
     def map_to_neo_kwargs(cls, folder_path, lsx_files=None):
         neo_kwargs = {
             "dirname": str(folder_path),
             "lsx_files": lsx_files,
@@ -44,14 +43,15 @@
         return neo_kwargs
 
 
 class AlphaOmegaEventExtractor(NeoBaseEventExtractor):
     """
     Class for reading events from AlphaOmega MPX file format
     """
+
     mode = "folder"
     NeoRawIOClass = "AlphaOmegaRawIO"
     handle_event_frame_directly = True
 
     def __init__(self, folder_path):
         neo_kwargs = self.map_to_neo_kwargs(folder_path)
         NeoBaseEventExtractor.__init__(self, **neo_kwargs)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/axona.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/axona.py`

 * *Files 14% similar despite different names*

```diff
@@ -12,24 +12,24 @@
     Parameters
     ----------
     file_path: str
         The file path to load the recordings from.
     all_annotations: bool, default: False
         Load exhaustively all annotations from neo.
     """
-    mode = 'folder'
-    NeoRawIOClass = 'AxonaRawIO'
+
+    mode = "folder"
+    NeoRawIOClass = "AxonaRawIO"
     name = "axona"
 
     def __init__(self, file_path, all_annotations=False):
         neo_kwargs = self.map_to_neo_kwargs(file_path)
-        NeoBaseRecordingExtractor.__init__(self,
-                                           all_annotations=all_annotations,
-                                           **neo_kwargs)
-        self._kwargs.update({'file_path': file_path})
+        NeoBaseRecordingExtractor.__init__(self, all_annotations=all_annotations, **neo_kwargs)
+        self._kwargs.update({"file_path": file_path})
 
     @classmethod
     def map_to_neo_kwargs(cls, file_path):
-        neo_kwargs = {'filename': str(file_path)}
+        neo_kwargs = {"filename": str(file_path)}
         return neo_kwargs
 
+
 read_axona = define_function_from_class(source_class=AxonaRecordingExtractor, name="read_axona")
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/biocam.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/biocam.py`

 * *Files 9% similar despite different names*

```diff
@@ -22,39 +22,47 @@
     stream_id: str, optional
         If there are several streams, specify the stream id you want to load.
     stream_name: str, optional
         If there are several streams, specify the stream name you want to load.
     all_annotations: bool  (default False)
         Load exhaustively all annotations from neo.
     """
-    mode = 'file'
-    NeoRawIOClass = 'BiocamRawIO'
+
+    mode = "file"
+    NeoRawIOClass = "BiocamRawIO"
     name = "biocam"
-    has_default_locations = True
 
-    def __init__(self, file_path, mea_pitch=None, electrode_width=None, stream_id=None,
-                 stream_name=None, block_index=None, all_annotations=False):
+    def __init__(
+        self,
+        file_path,
+        mea_pitch=None,
+        electrode_width=None,
+        stream_id=None,
+        stream_name=None,
+        block_index=None,
+        all_annotations=False,
+    ):
         neo_kwargs = self.map_to_neo_kwargs(file_path)
-        NeoBaseRecordingExtractor.__init__(self, stream_id=stream_id, 
-                                           stream_name=stream_name,
-                                           all_annotations=all_annotations,
-                                           **neo_kwargs)
+        NeoBaseRecordingExtractor.__init__(
+            self, stream_id=stream_id, stream_name=stream_name, all_annotations=all_annotations, **neo_kwargs
+        )
 
         # load probe from probeinterface
         probe_kwargs = {}
         if mea_pitch is not None:
             probe_kwargs["mea_pitch"] = mea_pitch
         if electrode_width is not None:
             probe_kwargs["electrode_width"] = electrode_width
         probe = pi.read_3brain(file_path, **probe_kwargs)
         self.set_probe(probe, in_place=True)
         self.set_property("row", self.get_property("contact_vector")["row"])
         self.set_property("col", self.get_property("contact_vector")["col"])
 
-        self._kwargs.update( {'file_path': str(file_path), 'mea_pitch':mea_pitch, 'electrode_width':electrode_width})
+        self._kwargs.update({"file_path": str(file_path), "mea_pitch": mea_pitch, "electrode_width": electrode_width})
 
     @classmethod
     def map_to_neo_kwargs(cls, file_path):
-        neo_kwargs = {'filename': str(file_path)}
+        neo_kwargs = {"filename": str(file_path)}
         return neo_kwargs
 
+
 read_biocam = define_function_from_class(source_class=BiocamRecordingExtractor, name="read_biocam")
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/blackrock.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/blackrock.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,7 +1,13 @@
+from pathlib import Path
+from packaging import version
+from typing import Optional
+
+import neo
+
 from spikeinterface.core.core_tools import define_function_from_class
 
 from .neobaseextractor import NeoBaseRecordingExtractor, NeoBaseSortingExtractor
 
 
 class BlackrockRecordingExtractor(NeoBaseRecordingExtractor):
     """
@@ -16,59 +22,104 @@
     stream_id: str, optional
         If there are several streams, specify the stream id you want to load.
     stream_name: str, optional
         If there are several streams, specify the stream name you want to load.
     all_annotations: bool, default: False
         Load exhaustively all annotations from neo.
     """
-    mode = 'file'
-    NeoRawIOClass = 'BlackrockRawIO'
+
+    mode = "file"
+    NeoRawIOClass = "BlackrockRawIO"
     name = "blackrock"
 
-    def __init__(self, file_path, stream_id=None, stream_name=None, block_index=None, all_annotations=False):
+    def __init__(
+        self,
+        file_path,
+        stream_id=None,
+        stream_name=None,
+        block_index=None,
+        all_annotations=False,
+        use_names_as_ids=False,
+    ):
         neo_kwargs = self.map_to_neo_kwargs(file_path)
-        NeoBaseRecordingExtractor.__init__(self, stream_id=stream_id, 
-                                           stream_name=stream_name,
-                                           all_annotations=all_annotations,
-                                           **neo_kwargs)
-        self._kwargs.update({'file_path': str(file_path)})
+        if version.parse(neo.__version__) > version.parse("0.12.0"):
+            # do not load spike because this is slow but not released yet
+            neo_kwargs["load_nev"] = False
+        # trick to avoid to select automatically the correct stream_id
+        suffix = Path(file_path).suffix
+        if ".ns" in suffix:
+            neo_kwargs["nsx_to_load"] = int(suffix[-1])
+        NeoBaseRecordingExtractor.__init__(
+            self,
+            stream_id=stream_id,
+            stream_name=stream_name,
+            all_annotations=all_annotations,
+            use_names_as_ids=use_names_as_ids,
+            **neo_kwargs,
+        )
+        self._kwargs.update({"file_path": str(file_path)})
 
     @classmethod
     def map_to_neo_kwargs(cls, file_path):
-        neo_kwargs = {'filename': str(file_path)}
+        neo_kwargs = {"filename": str(file_path)}
         return neo_kwargs
 
 
 class BlackrockSortingExtractor(NeoBaseSortingExtractor):
     """
     Class for reading BlackRock spiking data.
 
     Based on :py:class:`neo.rawio.BlackrockRawIO`
 
+
     Parameters
     ----------
     file_path: str
         The file path to load the recordings from.
     sampling_frequency: float, None by default.
-        The sampling frequency for the sorting extractor. When the signal data is available (.ncs) those files will be 
-        used to extract the frequency automatically. Otherwise, the sampling frequency needs to be specified for 
+        The sampling frequency for the sorting extractor. When the signal data is available (.ncs) those files will be
+        used to extract the frequency automatically. Otherwise, the sampling frequency needs to be specified for
         this extractor to be initialized.
+    stream_id: str, optional
+        Used to extract information about the sampling frequency and t_start from the analog signal if provided.
+    stream_name: str, optional
+        Used to extract information about the sampling frequency and t_start from the analog signal if provided.
     """
-    
-    mode = 'file'
-    NeoRawIOClass = 'BlackrockRawIO'
-    handle_spike_frame_directly = False
+
+    mode = "file"
+    NeoRawIOClass = "BlackrockRawIO"
+    neo_returns_frames = False
     name = "blackrock"
 
-    def __init__(self, file_path, sampling_frequency=None):
+    def __init__(
+        self,
+        file_path,
+        sampling_frequency: Optional[float] = None,
+        stream_id: Optional[str] = None,
+        stream_name: Optional[str] = None,
+    ):
         neo_kwargs = self.map_to_neo_kwargs(file_path)
-        NeoBaseSortingExtractor.__init__(self, sampling_frequency=sampling_frequency, **neo_kwargs)
-        self._kwargs.update({'file_path': str(file_path)})
+        NeoBaseSortingExtractor.__init__(
+            self,
+            sampling_frequency=sampling_frequency,
+            stream_id=stream_id,
+            stream_name=stream_name,
+            **neo_kwargs,
+        )
+
+        self._kwargs = {
+            "file_path": file_path,
+            "sampling_frequency": sampling_frequency,
+            "stream_id": stream_id,
+            "stream_name": stream_name,
+        }
 
     @classmethod
     def map_to_neo_kwargs(cls, file_path):
-        neo_kwargs = {'filename': str(file_path)}
+        neo_kwargs = {"filename": str(file_path)}
         return neo_kwargs
 
+
 read_blackrock = define_function_from_class(source_class=BlackrockRecordingExtractor, name="read_blackrock")
-read_blackrock_sorting = define_function_from_class(source_class=BlackrockSortingExtractor,
-                                                    name="read_blackrock_sorting")
+read_blackrock_sorting = define_function_from_class(
+    source_class=BlackrockSortingExtractor, name="read_blackrock_sorting"
+)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/ced.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/ced.py`

 * *Files 4% similar despite different names*

```diff
@@ -20,26 +20,27 @@
     stream_name: str, optional
         If there are several streams, specify the stream name you want to load.
     block_index: int, optional
         If there are several blocks, specify the block index you want to load.
     all_annotations: bool, default: False
         Load exhaustively all annotations from neo.
     """
-    mode = 'file'
-    NeoRawIOClass = 'CedRawIO'
+
+    mode = "file"
+    NeoRawIOClass = "CedRawIO"
     name = "ced"
 
     def __init__(self, file_path, stream_id=None, stream_name=None, all_annotations=False):
         neo_kwargs = self.map_to_neo_kwargs(file_path)
-        NeoBaseRecordingExtractor.__init__(self, stream_id=stream_id, 
-                                           stream_name=stream_name,
-                                           all_annotations=all_annotations,
-                                           **neo_kwargs)
+        NeoBaseRecordingExtractor.__init__(
+            self, stream_id=stream_id, stream_name=stream_name, all_annotations=all_annotations, **neo_kwargs
+        )
         self._kwargs.update(dict(file_path=str(file_path)))
-        self.extra_requirements.append('neo[ced]')
+        self.extra_requirements.append("neo[ced]")
 
     @classmethod
     def map_to_neo_kwargs(cls, file_path):
-        neo_kwargs = {'filename': str(file_path)}
+        neo_kwargs = {"filename": str(file_path)}
         return neo_kwargs
 
+
 read_ced = define_function_from_class(source_class=CedRecordingExtractor, name="read_ced")
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/edf.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/edf.py`

 * *Files 6% similar despite different names*

```diff
@@ -17,27 +17,27 @@
         If there are several streams, specify the stream id you want to load.
         For this neo reader streams are defined by their sampling frequency.
     stream_name: str, optional
         If there are several streams, specify the stream name you want to load.
     all_annotations: bool, default: False
         Load exhaustively all annotations from neo.
     """
-    mode = 'file'
-    NeoRawIOClass = 'EDFRawIO'
+
+    mode = "file"
+    NeoRawIOClass = "EDFRawIO"
     name = "edf"
 
     def __init__(self, file_path, stream_id=None, stream_name=None, all_annotations=False):
-        neo_kwargs = {'filename': str(file_path)}
-        NeoBaseRecordingExtractor.__init__(self, stream_id=stream_id, 
-                                           stream_name=stream_name,
-                                           all_annotations=all_annotations,
-                                           **neo_kwargs)
-        self._kwargs.update({'file_path': str(file_path)})
-        self.extra_requirements.append('neo[edf]')
+        neo_kwargs = {"filename": str(file_path)}
+        NeoBaseRecordingExtractor.__init__(
+            self, stream_id=stream_id, stream_name=stream_name, all_annotations=all_annotations, **neo_kwargs
+        )
+        self._kwargs.update({"file_path": str(file_path)})
+        self.extra_requirements.append("neo[edf]")
 
     @classmethod
     def map_to_neo_kwargs(cls, file_path):
-        neo_kwargs = {'filename': str(file_path)}
+        neo_kwargs = {"filename": str(file_path)}
         return neo_kwargs
 
 
 read_edf = define_function_from_class(source_class=EDFRecordingExtractor, name="read_edf")
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/intan.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/intan.py`

 * *Files 6% similar despite different names*

```diff
@@ -16,28 +16,32 @@
     stream_id: str, optional
         If there are several streams, specify the stream id you want to load.
     stream_name: str, optional
         If there are several streams, specify the stream name you want to load.
     all_annotations: bool, default: False
         Load exhaustively all annotations from neo.
     """
-    mode = 'file'
-    NeoRawIOClass = 'IntanRawIO'
+
+    mode = "file"
+    NeoRawIOClass = "IntanRawIO"
     name = "intan"
 
-    def __init__(self, file_path, stream_id=None, stream_name=None, all_annotations=False):
+    def __init__(self, file_path, stream_id=None, stream_name=None, all_annotations=False, use_names_as_ids=False):
         neo_kwargs = self.map_to_neo_kwargs(file_path)
-        NeoBaseRecordingExtractor.__init__(self, stream_id=stream_id, 
-                                           stream_name=stream_name,
-                                           all_annotations=all_annotations,
-                                           **neo_kwargs)
+        NeoBaseRecordingExtractor.__init__(
+            self,
+            stream_id=stream_id,
+            stream_name=stream_name,
+            all_annotations=all_annotations,
+            use_names_as_ids=use_names_as_ids,
+            **neo_kwargs,
+        )
 
         self._kwargs.update(dict(file_path=str(file_path)))
 
     @classmethod
     def map_to_neo_kwargs(cls, file_path):
-        neo_kwargs = {'filename': str(file_path)}
+        neo_kwargs = {"filename": str(file_path)}
         return neo_kwargs
 
 
-
 read_intan = define_function_from_class(source_class=IntanRecordingExtractor, name="read_intan")
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/maxwell.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/maxwell.py`

 * *Files 9% similar despite different names*

```diff
@@ -29,100 +29,116 @@
         Load exhaustively all annotations from neo.
     rec_name: str, optional
         When the file contains several recordings you need to specify the one
         you want to extract. (rec_name='rec0000').
     install_maxwell_plugin: bool, default: False
         If True, install the maxwell plugin for neo.
     """
-    mode = 'file'
-    NeoRawIOClass = 'MaxwellRawIO'
+
+    mode = "file"
+    NeoRawIOClass = "MaxwellRawIO"
     name = "maxwell"
-    has_default_locations = True
 
-    def __init__(self, file_path, stream_id=None, stream_name=None, block_index=None, 
-                 all_annotations=False, rec_name=None, install_maxwell_plugin=False):
+    def __init__(
+        self,
+        file_path,
+        stream_id=None,
+        stream_name=None,
+        block_index=None,
+        all_annotations=False,
+        rec_name=None,
+        install_maxwell_plugin=False,
+    ):
         if install_maxwell_plugin:
             self.install_maxwell_plugin()
-        
+
         neo_kwargs = self.map_to_neo_kwargs(file_path, rec_name)
-        NeoBaseRecordingExtractor.__init__(self, stream_id=stream_id, 
-                                           stream_name=stream_name,
-                                           block_index=block_index,
-                                           all_annotations=all_annotations, 
-                                           **neo_kwargs)
+        NeoBaseRecordingExtractor.__init__(
+            self,
+            stream_id=stream_id,
+            stream_name=stream_name,
+            block_index=block_index,
+            all_annotations=all_annotations,
+            **neo_kwargs,
+        )
 
-        self.extra_requirements.append('h5py')
+        self.extra_requirements.append("h5py")
 
         # well_name is stream_id
         well_name = self.stream_id
         # rec_name auto set by neo
         rec_name = self.neo_reader.rec_name
         probe = pi.read_maxwell(file_path, well_name=well_name, rec_name=rec_name)
         self.set_probe(probe, in_place=True)
         self.set_property("electrode", self.get_property("contact_vector")["electrode"])
         self._kwargs.update(dict(file_path=str(file_path), rec_name=rec_name))
 
     @classmethod
     def map_to_neo_kwargs(cls, file_path, rec_name=None):
-        neo_kwargs = {'filename': str(file_path), 'rec_name': rec_name}
+        neo_kwargs = {"filename": str(file_path), "rec_name": rec_name}
         return neo_kwargs
 
     def install_maxwell_plugin(self, force_download=False):
         from neo.rawio.maxwellrawio import auto_install_maxwell_hdf5_compression_plugin
+
         auto_install_maxwell_hdf5_compression_plugin(force_download=False)
 
+
 _maxwell_event_dtype = np.dtype([("frame", "int64"), ("state", "int8"), ("time", "float64")])
 
+
 class MaxwellEventExtractor(BaseEvent):
     """
     Class for reading TTL events from Maxwell files.
     """
+
     name = "maxwell"
 
     def __init__(self, file_path):
         import h5py
+
         self.file_path = file_path
-        h5_file = h5py.File(self.file_path, mode='r')
+        h5_file = h5py.File(self.file_path, mode="r")
         version = int(h5_file["version"][0].decode())
         fs = 20000
 
-        bits = h5_file['bits']
-        bit_states = bits['bits']
+        bits = h5_file["bits"]
+        bit_states = bits["bits"]
         channel_ids = np.unique(bit_states[bit_states != 0])
 
         BaseEvent.__init__(self, channel_ids, structured_dtype=_maxwell_event_dtype)
         event_segment = MaxwellEventSegment(h5_file, version, fs)
         self.add_event_segment(event_segment)
 
 
 class MaxwellEventSegment(BaseEventSegment):
     def __init__(self, h5_file, version, fs):
         BaseEventSegment.__init__(self)
         self.h5_file = h5_file
         self.version = version
-        self.bits = self.h5_file['bits']
+        self.bits = self.h5_file["bits"]
         self.fs = fs
 
     def get_events(self, channel_id, start_time, end_time):
         if self.version != 20160704:
             raise NotImplementedError(f"Version {self.version} not supported")
 
         framevals = self.h5_file["sig"][-2:, 0]
         first_frame = framevals[1] << 16 | framevals[0]
-        ttl_frames = self.bits['frameno'] - first_frame
-        ttl_states = self.bits['bits']
+        ttl_frames = self.bits["frameno"] - first_frame
+        ttl_states = self.bits["bits"]
         if channel_id is not None:
             bits_channel_idx = np.where((ttl_states == channel_id) | (ttl_states == 0))[0]
             ttl_frames = ttl_frames[bits_channel_idx]
             ttl_states = ttl_states[bits_channel_idx]
         ttl_states[ttl_states == 0] = -1
         event = np.zeros(len(ttl_frames), dtype=_maxwell_event_dtype)
         event["frame"] = ttl_frames
         event["time"] = ttl_frames / self.fs
-        event["state"] = ttl_states            
+        event["state"] = ttl_states
 
         if start_time is not None:
             event = event[event["time"] >= start_time]
         if end_time is not None:
             event = event[event["time"] < end_time]
         return event
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/mcsraw.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/mcsraw.py`

 * *Files 8% similar despite different names*

```diff
@@ -21,25 +21,31 @@
     stream_name: str, optional
         If there are several streams, specify the stream name you want to load.
     block_index: int, optional
         If there are several blocks, specify the block index you want to load.
     all_annotations: bool, default: False
         Load exhaustively all annotations from neo.
     """
-    mode = 'file'
-    NeoRawIOClass = 'RawMCSRawIO'
+
+    mode = "file"
+    NeoRawIOClass = "RawMCSRawIO"
     name = "mcsraw"
 
     def __init__(self, file_path, stream_id=None, stream_name=None, block_index=None, all_annotations=False):
         neo_kwargs = self.map_to_neo_kwargs(file_path)
-        NeoBaseRecordingExtractor.__init__(self, stream_id=stream_id,
-                                           stream_name=stream_name,
-                                           block_index=block_index,
-                                           all_annotations=all_annotations, **neo_kwargs)
+        NeoBaseRecordingExtractor.__init__(
+            self,
+            stream_id=stream_id,
+            stream_name=stream_name,
+            block_index=block_index,
+            all_annotations=all_annotations,
+            **neo_kwargs,
+        )
         self._kwargs.update(dict(file_path=str(file_path)))
 
     @classmethod
     def map_to_neo_kwargs(cls, file_path):
-        neo_kwargs = {'filename': str(file_path)}
+        neo_kwargs = {"filename": str(file_path)}
         return neo_kwargs
 
+
 read_mcsraw = define_function_from_class(source_class=MCSRawRecordingExtractor, name="read_maxwell_event")
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/mearec.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/spike2.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,84 +1,43 @@
-import probeinterface as pi
+from spikeinterface.core.core_tools import define_function_from_class
 
 from .neobaseextractor import NeoBaseRecordingExtractor, NeoBaseSortingExtractor
 
 
-class MEArecRecordingExtractor(NeoBaseRecordingExtractor):
+class Spike2RecordingExtractor(NeoBaseRecordingExtractor):
     """
-    Class for reading data from a MEArec simulated data.
+    Class for reading spike2 smr files.
+    smrx are not supported with this, prefer CedRecordingExtractor instead.
 
-    Based on :py:class:`neo.rawio.MEArecRawIO`
+    Based on :py:class:`neo.rawio.Spike2RawIO`
 
     Parameters
     ----------
     file_path: str
         The file path to load the recordings from.
+    stream_id: str, optional
+        If there are several streams, specify the stream id you want to load.
+    stream_name: str, optional
+        If there are several streams, specify the stream name you want to load.
     all_annotations: bool, default: False
         Load exhaustively all annotations from neo.
     """
-    mode = 'file'
-    NeoRawIOClass = 'MEArecRawIO'
-    name = "mearec"
 
-    def __init__(self, file_path, all_annotations=False):
-        neo_kwargs = self.map_to_neo_kwargs(file_path)
-        NeoBaseRecordingExtractor.__init__(self, 
-                                           all_annotations=all_annotations,
-                                           **neo_kwargs)
-
-        self.extra_requirements.append('mearec')
-
-        probe = pi.read_mearec(file_path)
-        self.set_probe(probe, in_place=True)
-        self.annotate(is_filtered=True)
-
-        if hasattr(self.neo_reader._recgen, "gain_to_uV"):
-            self.set_channel_gains(self.neo_reader._recgen.gain_to_uV)
-
-        self._kwargs.update({'file_path': str(file_path)})
-
-    @classmethod
-    def map_to_neo_kwargs(cls, file_path):
-        neo_kwargs = {'filename': str(file_path)}
-        return neo_kwargs
+    mode = "file"
+    NeoRawIOClass = "Spike2RawIO"
+    name = "spike2"
 
-
-class MEArecSortingExtractor(NeoBaseSortingExtractor):
-    mode = 'file'
-    NeoRawIOClass = 'MEArecRawIO'
-    handle_spike_frame_directly = False
-    name = "mearec"
-
-    def __init__(self, file_path):
+    def __init__(self, file_path, stream_id=None, stream_name=None, all_annotations=False):
         neo_kwargs = self.map_to_neo_kwargs(file_path)
-        NeoBaseSortingExtractor.__init__(self,
-                                         sampling_frequency=None,  # auto guess is correct here
-                                         use_natural_unit_ids=True,
-                                         **neo_kwargs)
-
-        self._kwargs = {'file_path': str(file_path)}
+        NeoBaseRecordingExtractor.__init__(
+            self, stream_id=stream_id, stream_name=stream_name, all_annotations=all_annotations, **neo_kwargs
+        )
+        self._kwargs.update({"file_path": str(file_path)})
+        self.extra_requirements.append("sonpy")
 
     @classmethod
     def map_to_neo_kwargs(cls, file_path):
-        neo_kwargs = {'filename': str(file_path)}
+        neo_kwargs = {"filename": str(file_path)}
         return neo_kwargs
 
 
-def read_mearec(file_path):
-    """Read a MEArec file.
-
-    Parameters
-    ----------
-    file_path: str or Path
-        Path to MEArec h5 file
-
-    Returns
-    -------
-    recording: MEArecRecordingExtractor
-        The recording extractor object
-    sorting: MEArecSortingExtractor
-        The sorting extractor object
-    """
-    recording = MEArecRecordingExtractor(file_path)
-    sorting = MEArecSortingExtractor(file_path)
-    return recording, sorting
+read_spike2 = define_function_from_class(source_class=Spike2RecordingExtractor, name="read_spike2")
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/neo_utils.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/neo_utils.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,49 +1,49 @@
 from .neobaseextractor import NeoBaseRecordingExtractor
 
 
 def get_neo_streams(extractor_name, *args, **kwargs):
     """Returns the NEO streams (stream names and stream ids) associated to a dataset.
-    For multi-stream datasets, the `stream_id` or `stream_name` arguments can be used 
+    For multi-stream datasets, the `stream_id` or `stream_name` arguments can be used
     to select which stream to read with the `read_**extractor_name**()` function.
 
     Parameters
     ----------
     extractor_name : str
         The extractor name (available through the se.recording_extractor_full_dict).
     *args, **kwargs : arguments
         Extractor specific arguments. You can check extractor specific arguments with:
         `read_**extractor_name**?`
-    
+
 
     Returns
     -------
     list
         List of NEO stream names
     list
         List of NEO stream ids
     """
     neo_extractor = get_neo_extractor(extractor_name)
     return neo_extractor.get_streams(*args, **kwargs)
 
 
 def get_neo_num_blocks(extractor_name, *args, **kwargs):
-    """Returns the number of NEO blocks. 
-    For multi-block datasets, the `block_index` argument can be used to select 
+    """Returns the number of NEO blocks.
+    For multi-block datasets, the `block_index` argument can be used to select
     which bloack to read with the `read_**extractor_name**()` function.
-    
+
 
     Parameters
     ----------
     extractor_name : str
         The extractor name (available through the se.recording_extractor_full_dict).
     *args, **kwargs : arguments
         Extractor specific arguments. You can check extractor specific arguments with:
         `read_**extractor_name**?`
-    
+
     Returns
     -------
     int
         Number of NEO blocks
 
     Note
     ----
@@ -52,12 +52,13 @@
     neo_extractor = get_neo_extractor(extractor_name)
     return neo_extractor.get_num_blocks(*args, **kwargs)
 
 
 def get_neo_extractor(extractor_name):
     from ..extractorlist import recording_extractor_full_dict
 
-    assert extractor_name in recording_extractor_full_dict, (f"{extractor_name} not an extractor name:"
-                                                             f"\n{list(recording_extractor_full_dict.keys())}")
+    assert extractor_name in recording_extractor_full_dict, (
+        f"{extractor_name} not an extractor name:" f"\n{list(recording_extractor_full_dict.keys())}"
+    )
     neo_extractor = recording_extractor_full_dict[extractor_name]
     assert issubclass(neo_extractor, NeoBaseRecordingExtractor), f"{extractor_name} is not a NEO recording extractor!"
-    return neo_extractor
+    return neo_extractor
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/neuralynx.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/neuralynx.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,7 +1,9 @@
+from typing import Optional
+
 from spikeinterface.core.core_tools import define_function_from_class
 
 from .neobaseextractor import NeoBaseRecordingExtractor, NeoBaseSortingExtractor
 
 
 class NeuralynxRecordingExtractor(NeoBaseRecordingExtractor):
     """
@@ -16,58 +18,83 @@
     stream_id: str, optional
         If there are several streams, specify the stream id you want to load.
     stream_name: str, optional
         If there are several streams, specify the stream name you want to load.
     all_annotations: bool, default: False
         Load exhaustively all annotations from neo.
     """
-    mode = 'folder'
-    NeoRawIOClass = 'NeuralynxRawIO'
+
+    mode = "folder"
+    NeoRawIOClass = "NeuralynxRawIO"
     name = "neuralynx"
 
     def __init__(self, folder_path, stream_id=None, stream_name=None, all_annotations=False):
         neo_kwargs = self.map_to_neo_kwargs(folder_path)
-        NeoBaseRecordingExtractor.__init__(self, stream_id=stream_id, 
-                                           stream_name=stream_name,
-                                           all_annotations=all_annotations, 
-                                           **neo_kwargs)
+        NeoBaseRecordingExtractor.__init__(
+            self, stream_id=stream_id, stream_name=stream_name, all_annotations=all_annotations, **neo_kwargs
+        )
         self._kwargs.update(dict(folder_path=str(folder_path)))
 
     @classmethod
     def map_to_neo_kwargs(cls, folder_path):
-        neo_kwargs = {'dirname': str(folder_path)}
+        neo_kwargs = {"dirname": str(folder_path)}
         return neo_kwargs
 
 
 class NeuralynxSortingExtractor(NeoBaseSortingExtractor):
     """
     Class for reading spike data from a folder with neuralynx spiking data (i.e .nse and .ntt formats).
 
     Based on :py:class:`neo.rawio.NeuralynxRawIO`
 
     Parameters
     ----------
     folder_path: str
         The file path to load the recordings from.
     sampling_frequency: float
-        The sampling frequency for the spiking channels. When the signal data is available (.ncs) those files will be 
+        The sampling frequency for the spiking channels. When the signal data is available (.ncs) those files will be
         used to extract the frequency. Otherwise, the sampling frequency needs to be specified for this extractor.
+    stream_id: str, optional
+        Used to extract information about the sampling frequency and t_start from the analog signal if provided.
+    stream_name: str, optional
+        Used to extract information about the sampling frequency and t_start from the analog signal if provided.
     """
-    mode = 'folder'
-    NeoRawIOClass = 'NeuralynxRawIO'
-    handle_spike_frame_directly = False
+
+    mode = "folder"
+    NeoRawIOClass = "NeuralynxRawIO"
+    neo_returns_timestamps = False
+    need_t_start_from_signal_stream = True
     name = "neuralynx"
 
-    def __init__(self, folder_path, sampling_frequency=None):
+    def __init__(
+        self,
+        folder_path: str,
+        sampling_frequency: Optional[float] = None,
+        stream_id: Optional[str] = None,
+        stream_name: Optional[str] = None,
+    ):
         neo_kwargs = self.map_to_neo_kwargs(folder_path)
-        NeoBaseSortingExtractor.__init__(self, sampling_frequency=sampling_frequency, **neo_kwargs)
-        self._kwargs.update(dict(folder_path=str(folder_path)))
+        NeoBaseSortingExtractor.__init__(
+            self,
+            sampling_frequency=sampling_frequency,
+            stream_id=stream_id,
+            stream_name=stream_name,
+            **neo_kwargs,
+        )
+
+        self._kwargs = {
+            "folder_path": folder_path,
+            "sampling_frequency": sampling_frequency,
+            "stream_id": stream_id,
+            "stream_name": stream_name,
+        }
 
     @classmethod
     def map_to_neo_kwargs(cls, folder_path):
-        neo_kwargs = {'dirname': str(folder_path)}
+        neo_kwargs = {"dirname": str(folder_path)}
         return neo_kwargs
 
 
 read_neuralynx = define_function_from_class(source_class=NeuralynxRecordingExtractor, name="read_neuralynx")
-read_neuralynx_sorting = define_function_from_class(source_class=NeuralynxSortingExtractor,
-                                                    name="read_neuralynx_sorting")
+read_neuralynx_sorting = define_function_from_class(
+    source_class=NeuralynxSortingExtractor, name="read_neuralynx_sorting"
+)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/neuroscope.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/neuroscope.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,20 +1,21 @@
 import warnings
 from pathlib import Path
 from typing import Union, Optional
 
 import numpy as np
 
-from spikeinterface.core import (BaseSorting, BaseSortingSegment)
+from spikeinterface.core import BaseSorting, BaseSortingSegment
 from spikeinterface.core.core_tools import define_function_from_class
 
 from .neobaseextractor import NeoBaseRecordingExtractor
 
 try:
     from lxml import etree as et
+
     HAVE_LXML = True
 except ImportError:
     HAVE_LXML = False
 
 PathType = Union[str, Path]
 OptionalPathType = Optional[PathType]
 
@@ -25,37 +26,48 @@
     Ref: http://neuroscope.sourceforge.net
 
     Based on :py:class:`neo.rawio.NeuroScopeRawIO`
 
     Parameters
     ----------
     file_path: str
-        The file path to load the recordings from.
+        The file path to the binary container usually a .dat, .lfp, .eeg extension.
+    xml_file_path: str, optional
+        The path to the xml file. If None, the xml file is assumed to have the same name as the binary file.
     stream_id: str, optional
         If there are several streams, specify the stream id you want to load.
     stream_name: str, optional
         If there are several streams, specify the stream name you want to load.
     all_annotations: bool, default: False
         Load exhaustively all annotations from neo.
     """
-    mode = 'file'
-    NeoRawIOClass = 'NeuroScopeRawIO'
+
+    mode = "file"
+    NeoRawIOClass = "NeuroScopeRawIO"
     name = "neuroscope"
 
-    def __init__(self, file_path, stream_id=None, stream_name=None, all_annotations=False):
-        neo_kwargs = self.map_to_neo_kwargs(file_path)
-        NeoBaseRecordingExtractor.__init__(self, stream_id=stream_id, 
-                                           stream_name=stream_name,
-                                           all_annotations=all_annotations, 
-                                           **neo_kwargs)
-        self._kwargs.update(dict(file_path=str(file_path)))
+    def __init__(self, file_path, xml_file_path=None, stream_id=None, stream_name=None, all_annotations=False):
+        neo_kwargs = self.map_to_neo_kwargs(file_path, xml_file_path)
+
+        NeoBaseRecordingExtractor.__init__(
+            self, stream_id=stream_id, stream_name=stream_name, all_annotations=all_annotations, **neo_kwargs
+        )
+        self._kwargs.update(dict(file_path=str(file_path), xml_file_path=xml_file_path))
 
     @classmethod
-    def map_to_neo_kwargs(cls, file_path):
-        neo_kwargs = {'filename': str(file_path)}
+    def map_to_neo_kwargs(cls, file_path, xml_file_path=None):
+        # For this because of backwards compatibility we have a strange convention
+        # filename is the xml file
+        # binary_file is the binary file in .dat, .lfp, .eeg
+
+        if xml_file_path is not None:
+            neo_kwargs = {"binary_file": file_path, "filename": xml_file_path}
+        else:
+            neo_kwargs = {"filename": file_path}
+
         return neo_kwargs
 
 
 class NeuroScopeSortingExtractor(BaseSorting):
     """
     Extracts spiking information from an arbitrary number of .res.%i and .clu.%i files in the general folder path.
 
@@ -105,72 +117,77 @@
         clufile_path: OptionalPathType = None,
         keep_mua_units: bool = True,
         exclude_shanks: Optional[list] = None,
         xml_file_path: OptionalPathType = None,
     ):
         assert self.installed, self.installation_mesg
 
-        assert not (folder_path is None and resfile_path is None and clufile_path is None), \
-            "Either pass a single folder_path location, or a pair of resfile_path and clufile_path! None received."
+        assert not (
+            folder_path is None and resfile_path is None and clufile_path is None
+        ), "Either pass a single folder_path location, or a pair of resfile_path and clufile_path! None received."
 
         if resfile_path is not None:
             assert clufile_path is not None, "If passing resfile_path or clufile_path, both are required!"
             resfile_path = Path(resfile_path)
             clufile_path = Path(clufile_path)
-            assert resfile_path.is_file() and clufile_path.is_file(), \
-                f"The resfile_path ({resfile_path}) and clufile_path ({clufile_path}) must be .res and .clu files!"
-
-            assert folder_path is None, "Pass either a single folder_path location, " \
-                                        "or a pair of resfile_path and clufile_path! All received."
+            assert (
+                resfile_path.is_file() and clufile_path.is_file()
+            ), f"The resfile_path ({resfile_path}) and clufile_path ({clufile_path}) must be .res and .clu files!"
+
+            assert folder_path is None, (
+                "Pass either a single folder_path location, "
+                "or a pair of resfile_path and clufile_path! All received."
+            )
             folder_path_passed = False
             folder_path = resfile_path.parent
             res_files = [resfile_path]
             clu_files = [clufile_path]
         else:
             assert folder_path is not None, "Either pass resfile_path and clufile_path, or folder_path!"
             folder_path = Path(folder_path)
             assert folder_path.is_dir(), "The folder_path must be a directory!"
 
-            res_files = sorted([f for f in folder_path.iterdir() if f.is_file() and ".res" in f.suffixes and not
-                                f.name.endswith("~")])
-            clu_files = sorted([f for f in folder_path.iterdir() if f.is_file() and ".clu" in f.suffixes and not
-                                f.name.endswith("~")])
+            res_files = sorted(
+                [f for f in folder_path.iterdir() if f.is_file() and ".res" in f.suffixes and not f.name.endswith("~")]
+            )
+            clu_files = sorted(
+                [f for f in folder_path.iterdir() if f.is_file() and ".clu" in f.suffixes and not f.name.endswith("~")]
+            )
 
-            assert len(res_files) > 0 or len(clu_files) > 0, \
-                "No .res or .clu files found in the folder_path!"
+            assert len(res_files) > 0 or len(clu_files) > 0, "No .res or .clu files found in the folder_path!"
 
             folder_path_passed = True  # flag for setting kwargs for proper dumping
 
         if exclude_shanks is not None:  # dumping checks do not like having an empty list as default
-            assert all([isinstance(x, (int, np.integer)) and x >= 0 for x in
-                        exclude_shanks]), "Optional argument 'exclude_shanks' must contain positive integers only!"
+            assert all(
+                [isinstance(x, (int, np.integer)) and x >= 0 for x in exclude_shanks]
+            ), "Optional argument 'exclude_shanks' must contain positive integers only!"
         else:
             exclude_shanks = []
-        xml_file_path = _handle_xml_file_path(
-            folder_path=folder_path, initial_xml_file_path=xml_file_path)
+        xml_file_path = _handle_xml_file_path(folder_path=folder_path, initial_xml_file_path=xml_file_path)
         xml_root = et.parse(str(xml_file_path)).getroot()
-        sampling_frequency = float(xml_root.find(
-            'acquisitionSystem').find('samplingRate').text)
+        sampling_frequency = float(xml_root.find("acquisitionSystem").find("samplingRate").text)
 
         if len(res_files) > 1:
             res_ids = [int(x.suffix[1:]) for x in res_files]
             clu_ids = [int(x.suffix[1:]) for x in clu_files]
-            assert sorted(res_ids) == sorted(
-                clu_ids), "Unmatched .clu.%i and .res.%i files detected!"
+            assert sorted(res_ids) == sorted(clu_ids), "Unmatched .clu.%i and .res.%i files detected!"
             if any([x not in res_ids for x in exclude_shanks]):
                 warnings.warn(
-                    "Detected indices in exclude_shanks that are not in the directory! These will be ignored.")
+                    "Detected indices in exclude_shanks that are not in the directory! These will be ignored."
+                )
             shank_ids = res_ids
         else:
             shank_ids = None
 
-        resfile_names = [x.name[:x.name.find('.res')] for x in res_files]
-        clufile_names = [x.name[:x.name.find('.clu')] for x in clu_files]
-        assert np.all(r == c for (r, c) in zip(resfile_names, clufile_names)), \
-            "Some of the .res.%i and .clu.%i files do not share the same name!"
+        resfile_names = [x.name[: x.name.find(".res")] for x in res_files]
+        clufile_names = [x.name[: x.name.find(".clu")] for x in clu_files]
+        assert np.all(
+            r == c for (r, c) in zip(resfile_names, clufile_names)
+        ), "Some of the .res.%i and .clu.%i files do not share the same name!"
 
         all_unit_ids = []
         all_spiketrains = []
         all_unit_shank_ids = []
         for i, (res_file, clu_file) in enumerate(zip(res_files, clu_files)):
             if shank_ids is not None:
                 shank_id = shank_ids[i]
@@ -184,17 +201,17 @@
 
             n_spikes = len(res)
             if n_spikes > 0:
                 # Extract the number of unique IDs from the first line of the clufile then remove it from the list
                 n_clu = clu[0]
                 clu = np.delete(clu, 0)
                 unique_ids = np.unique(clu)
-                assert len(unique_ids) == n_clu, (
-                    "First value of .clu file ({clufile_path}) does not match number of unique IDs!"
-                )
+                assert (
+                    len(unique_ids) == n_clu
+                ), "First value of .clu file ({clufile_path}) does not match number of unique IDs!"
                 unit_map = dict(zip(unique_ids, list(range(1, n_clu + 1))))
 
                 if 0 in unique_ids:
                     unit_map.pop(0)
                 if not keep_mua_units and 1 in unique_ids:
                     unit_map.pop(1)
                 if len(all_unit_ids) > 0:
@@ -205,42 +222,40 @@
                 all_unit_ids += new_unit_ids
                 for s_id in unit_map:
                     all_spiketrains.append(res[(clu == s_id).nonzero()])
 
                 if shank_ids is not None:
                     all_unit_shank_ids += [shank_id] * len(new_unit_ids)
 
-        BaseSorting.__init__(self, sampling_frequency=sampling_frequency,
-                             unit_ids=all_unit_ids)
-        self.add_sorting_segment(
-            NeuroScopeSortingSegment(all_unit_ids, all_spiketrains))
+        BaseSorting.__init__(self, sampling_frequency=sampling_frequency, unit_ids=all_unit_ids)
+        self.add_sorting_segment(NeuroScopeSortingSegment(all_unit_ids, all_spiketrains))
 
-        self.extra_requirements.append('lxml')
+        self.extra_requirements.append("lxml")
 
         # set "group" property based on shank ids
         if len(all_unit_shank_ids) > 0:
             self.set_property("group", all_unit_shank_ids)
 
         if folder_path_passed:
             self._kwargs = dict(
                 folder_path=str(folder_path.absolute()),
                 resfile_path=None,
                 clufile_path=None,
                 keep_mua_units=keep_mua_units,
                 exclude_shanks=exclude_shanks,
-                xml_file_path=xml_file_path
+                xml_file_path=xml_file_path,
             )
         else:
             self._kwargs = dict(
                 folder_path=None,
                 resfile_path=str(resfile_path.absolute()),
                 clufile_path=str(clufile_path.absolute()),
                 keep_mua_units=keep_mua_units,
                 exclude_shanks=exclude_shanks,
-                xml_file_path=xml_file_path
+                xml_file_path=xml_file_path,
             )
 
 
 class NeuroScopeSortingSegment(BaseSortingSegment):
     def __init__(self, unit_ids, spiketrains):
         BaseSortingSegment.__init__(self)
         self._unit_ids = list(unit_ids)
@@ -253,41 +268,41 @@
             times = times[times >= start_frame]
         if end_frame is not None:
             times = times[times < end_frame]
         return times
 
 
 def _find_xml_file_path(folder_path: PathType):
-    xml_files = [f for f in folder_path.iterdir() if f.is_file()
-                 if f.suffix == ".xml"]
+    xml_files = [f for f in folder_path.iterdir() if f.is_file() if f.suffix == ".xml"]
     assert any(xml_files), "No .xml files found in the folder_path."
-    assert len(
-        xml_files) == 1, "More than one .xml file found in the folder_path! Specify xml_file_path."
+    assert len(xml_files) == 1, "More than one .xml file found in the folder_path! Specify xml_file_path."
     xml_file_path = xml_files[0]
     return xml_file_path
 
 
 def _handle_xml_file_path(folder_path: PathType, initial_xml_file_path: PathType):
     if initial_xml_file_path is None:
         xml_file_path = _find_xml_file_path(folder_path=folder_path)
     else:
-        assert Path(initial_xml_file_path).is_file(
-        ), f".xml file ({initial_xml_file_path}) not found!"
+        assert Path(initial_xml_file_path).is_file(), f".xml file ({initial_xml_file_path}) not found!"
         xml_file_path = initial_xml_file_path
     return xml_file_path
 
 
-read_neuroscope_recording = define_function_from_class(source_class=NeuroScopeRecordingExtractor,
-                                                   name="read_neuroscope_recording")
-read_neuroscope_sorting = define_function_from_class(source_class=NeuroScopeSortingExtractor,
-                                                 name="read_neuroscope_sorting")
+read_neuroscope_recording = define_function_from_class(
+    source_class=NeuroScopeRecordingExtractor, name="read_neuroscope_recording"
+)
+read_neuroscope_sorting = define_function_from_class(
+    source_class=NeuroScopeSortingExtractor, name="read_neuroscope_sorting"
+)
 
 
-def read_neuroscope(file_path, stream_id=None, keep_mua_units=False,
-                    exclude_shanks=None, load_recording=True, load_sorting=False):
+def read_neuroscope(
+    file_path, stream_id=None, keep_mua_units=False, exclude_shanks=None, load_recording=True, load_sorting=False
+):
     """
     Read neuroscope recording and sorting.
     This function assumses that all .res and .clu files are in the same folder as
     the .xml file.
 
     Parameters
     ----------
@@ -307,15 +322,16 @@
     outputs = ()
     # TODO add checks for recording and sorting existence
     if load_recording:
         recording = NeuroScopeRecordingExtractor(file_path=file_path, stream_id=stream_id)
         outputs = outputs + (recording,)
     if load_sorting:
         folder_path = Path(file_path).parent
-        sorting = NeuroScopeSortingExtractor(folder_path=folder_path, keep_mua_units=keep_mua_units,
-                                             exclude_shanks=exclude_shanks)
+        sorting = NeuroScopeSortingExtractor(
+            folder_path=folder_path, keep_mua_units=keep_mua_units, exclude_shanks=exclude_shanks
+        )
         outputs = outputs + (sorting,)
 
     if len(outputs) == 1:
         outputs = outputs[0]
 
     return outputs
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/nix.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/nix.py`

 * *Files 4% similar despite different names*

```diff
@@ -18,29 +18,32 @@
     stream_name: str, optional
         If there are several streams, specify the stream name you want to load.
     block_index: int, optional
         If there are several blocks, specify the block index you want to load.
     all_annotations: bool, default: False
         Load exhaustively all annotations from neo.
     """
-    mode = 'file'
-    NeoRawIOClass = 'NIXRawIO'
-    name = "nix"
 
+    mode = "file"
+    NeoRawIOClass = "NIXRawIO"
+    name = "nix"
 
     def __init__(self, file_path, stream_id=None, stream_name=None, block_index=None, all_annotations=False):
         neo_kwargs = self.map_to_neo_kwargs(file_path)
-        NeoBaseRecordingExtractor.__init__(self, stream_id=stream_id, 
-                                           stream_name=stream_name,
-                                           block_index=block_index,
-                                           all_annotations=all_annotations, 
-                                           **neo_kwargs)
+        NeoBaseRecordingExtractor.__init__(
+            self,
+            stream_id=stream_id,
+            stream_name=stream_name,
+            block_index=block_index,
+            all_annotations=all_annotations,
+            **neo_kwargs,
+        )
         self._kwargs.update(dict(file_path=str(file_path), stream_id=stream_id))
-        self.extra_requirements.append('neo[nixio]')
+        self.extra_requirements.append("neo[nixio]")
 
     @classmethod
     def map_to_neo_kwargs(cls, file_path):
-        neo_kwargs = {'filename': str(file_path)}
+        neo_kwargs = {"filename": str(file_path)}
         return neo_kwargs
 
 
 read_nix = define_function_from_class(source_class=NixRecordingExtractor, name="read_nix")
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/openephys.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/openephys.py`

 * *Files 5% similar despite different names*

```diff
@@ -13,17 +13,15 @@
 from pathlib import Path
 
 import numpy as np
 import warnings
 
 import probeinterface as pi
 
-from .neobaseextractor import (NeoBaseRecordingExtractor,
-                               NeoBaseSortingExtractor,
-                               NeoBaseEventExtractor)
+from .neobaseextractor import NeoBaseRecordingExtractor, NeoBaseSortingExtractor, NeoBaseEventExtractor
 
 from spikeinterface.extractors.neuropixels_utils import get_neuropixels_sample_shifts
 
 
 class OpenEphysLegacyRecordingExtractor(NeoBaseRecordingExtractor):
     """
     Class for reading data saved by the Open Ephys GUI.
@@ -44,30 +42,34 @@
     stream_name: str, optional
         If there are several streams, specify the stream name you want to load.
     block_index: int, optional
         If there are several blocks (experiments), specify the block index you want to load.
     all_annotations: bool  (default False)
         Load exhaustively all annotation from neo.
     """
-    mode = 'folder'
-    NeoRawIOClass = 'OpenEphysRawIO'
+
+    mode = "folder"
+    NeoRawIOClass = "OpenEphysRawIO"
     name = "openephyslegacy"
 
     def __init__(self, folder_path, stream_id=None, stream_name=None, block_index=None, all_annotations=False):
         neo_kwargs = self.map_to_neo_kwargs(folder_path)
-        NeoBaseRecordingExtractor.__init__(self, stream_id=stream_id,
-                                           stream_name=stream_name,
-                                           block_index=block_index,
-                                           all_annotations=all_annotations, 
-                                           **neo_kwargs)
+        NeoBaseRecordingExtractor.__init__(
+            self,
+            stream_id=stream_id,
+            stream_name=stream_name,
+            block_index=block_index,
+            all_annotations=all_annotations,
+            **neo_kwargs,
+        )
         self._kwargs.update(dict(folder_path=str(folder_path)))
 
     @classmethod
     def map_to_neo_kwargs(cls, folder_path):
-        neo_kwargs = {'dirname': str(folder_path)}
+        neo_kwargs = {"dirname": str(folder_path)}
         return neo_kwargs
 
 
 class OpenEphysBinaryRecordingExtractor(NeoBaseRecordingExtractor):
     """
     Class for reading data saved by the Open Ephys GUI.
 
@@ -77,119 +79,149 @@
     https://open-ephys.github.io/gui-docs/User-Manual/Recording-data/Binary-format.html
 
     Based on neo.rawio.OpenEphysBinaryRawIO
 
     Parameters
     ----------
     folder_path: str
-        The folder path to load the recordings from.
+        The folder path to the root folder (containing the record node folders).
     load_sync_channel : bool
         If False (default) and a SYNC channel is present (e.g. Neuropixels), this is not loaded.
         If True, the SYNC channel is loaded and can be accessed in the analog signals.
-    load_sync_channel : bool
+    load_sync_timestamps : bool
         If True, the synchronized_timestamps are loaded and set as times to the recording.
         If False (default), only the t_start and sampling rate are set, and timestamps are assumed
         to be uniform and linearly increasing.
-    experiment_name: str, list, or None
+    experiment_names: str, list, or None
         If multiple experiments are available, this argument allows users to select one
         or more experiments. If None, all experiements are loaded as blocks.
         E.g. 'experiment_names="experiment2"', 'experiment_names=["experiment1", "experiment2"]'
     stream_id: str, optional
         If there are several streams, specify the stream id you want to load.
     stream_name: str, optional
         If there are several streams, specify the stream name you want to load.
     block_index: int, optional
         If there are several blocks (experiments), specify the block index you want to load.
     all_annotations: bool  (default False)
         Load exhaustively all annotation from neo.
 
     """
-    mode = 'folder'
-    NeoRawIOClass = 'OpenEphysBinaryRawIO'
+
+    mode = "folder"
+    NeoRawIOClass = "OpenEphysBinaryRawIO"
     name = "openephys"
-    has_default_locations = True
 
-    def __init__(self, folder_path, load_sync_channel=False, load_sync_timestamps=False, experiment_names=None,
-                 stream_id=None, stream_name=None, block_index=None, all_annotations=False):
+    def __init__(
+        self,
+        folder_path,
+        load_sync_channel=False,
+        load_sync_timestamps=False,
+        experiment_names=None,
+        stream_id=None,
+        stream_name=None,
+        block_index=None,
+        all_annotations=False,
+    ):
         neo_kwargs = self.map_to_neo_kwargs(folder_path, load_sync_channel, experiment_names)
-        NeoBaseRecordingExtractor.__init__(self, stream_id=stream_id,
-                                           stream_name=stream_name,
-                                           block_index=block_index,
-                                           all_annotations=all_annotations, 
-                                           **neo_kwargs)
+        NeoBaseRecordingExtractor.__init__(
+            self,
+            stream_id=stream_id,
+            stream_name=stream_name,
+            block_index=block_index,
+            all_annotations=all_annotations,
+            **neo_kwargs,
+        )
         # get streams to find correct probe
         stream_names, stream_ids = self.get_streams(folder_path, experiment_names)
         if stream_name is None and stream_id is None:
             stream_name = stream_names[0]
         elif stream_name is None:
             stream_name = stream_names[stream_ids.index(stream_id)]
 
         # find settings file
         if "#" in stream_name:
             record_node, oe_stream = stream_name.split("#")
         else:
-            record_node = ''
+            record_node = ""
             oe_stream = stream_name
         exp_ids = sorted(list(self.neo_reader.folder_structure[record_node]["experiments"].keys()))
         if block_index is None:
             exp_id = exp_ids[0]
         else:
             exp_id = exp_ids[block_index]
 
         # do not load probe for NIDQ stream
         if "NI-DAQmx" not in stream_name:
             settings_file = self.neo_reader.folder_structure[record_node]["experiments"][exp_id]["settings_file"]
 
             if Path(settings_file).is_file():
-                probe = pi.read_openephys(settings_file=settings_file,
-                                          stream_name=stream_name, raise_error=False)
+                probe = pi.read_openephys(settings_file=settings_file, stream_name=stream_name, raise_error=False)
             else:
                 probe = None
 
             if probe is not None:
                 self = self.set_probe(probe, in_place=True)
                 probe_name = probe.annotations["probe_name"]
                 # load num_channels_per_adc depending on probe type
                 if "2.0" in probe_name:
                     num_channels_per_adc = 16
-                else:
+                    num_cycles_in_adc = 16
+                    total_channels = 384
+                else:  # NP1.0
                     num_channels_per_adc = 12
-                sample_shifts = get_neuropixels_sample_shifts(self.get_num_channels(), num_channels_per_adc)
+                    num_cycles_in_adc = 13 if "AP" in stream_name else 12
+                    total_channels = 384
+
+                # sample_shifts is generated from total channels (384) channels
+                # when only some channels are saved we need to slice this vector (like we do for the probe)
+                sample_shifts = get_neuropixels_sample_shifts(total_channels, num_channels_per_adc, num_cycles_in_adc)
+                if self.get_num_channels() != total_channels:
+                    # need slice because not all channel are saved
+                    chans = pi.get_saved_channel_indices_from_openephys_settings(settings_file, oe_stream)
+                    # lets clip to 384 because this contains also the synchro channel
+                    chans = chans[chans < total_channels]
+                    sample_shifts = sample_shifts[chans]
                 self.set_property("inter_sample_shift", sample_shifts)
 
         # load synchronized timestamps and set_times to recording
         if load_sync_timestamps:
             recording_folder = Path(folder_path) / record_node
             for segment_index in range(self.get_num_segments()):
-                stream_folder = recording_folder / f"experiment{exp_id}" / f"recording{segment_index+1}" / \
-                    "continuous" / oe_stream
+                stream_folder = (
+                    recording_folder / f"experiment{exp_id}" / f"recording{segment_index+1}" / "continuous" / oe_stream
+                )
                 if (stream_folder / "sample_numbers.npy").is_file():
                     # OE version>=v0.6
                     sync_times = np.load(stream_folder / "timestamps.npy")
                 elif (stream_folder / "synchronized_timestamps.npy").is_file():
                     # version<v0.6
                     sync_times = np.load(stream_folder / "synchronized_timestamps.npy")
                 else:
                     sync_times = None
                 try:
                     self.set_times(times=sync_times, segment_index=segment_index, with_warning=False)
                 except AssertionError:
                     warnings.warn(f"Could not load synchronized timestamps for {stream_name}")
 
-        self._kwargs.update(dict(folder_path=str(folder_path),
-                                 load_sync_channel=load_sync_channel,
-                                 load_sync_timestamps=load_sync_timestamps,
-                                 experiment_names=experiment_names))
-
+        self._kwargs.update(
+            dict(
+                folder_path=str(folder_path),
+                load_sync_channel=load_sync_channel,
+                load_sync_timestamps=load_sync_timestamps,
+                experiment_names=experiment_names,
+            )
+        )
 
     @classmethod
     def map_to_neo_kwargs(cls, folder_path, load_sync_channel=False, experiment_names=None):
-        neo_kwargs = {'dirname': str(folder_path),
-                      'load_sync_channel': load_sync_channel,
-                      'experiment_names': experiment_names}
+        neo_kwargs = {
+            "dirname": str(folder_path),
+            "load_sync_channel": load_sync_channel,
+            "experiment_names": experiment_names,
+        }
         return neo_kwargs
 
 
 class OpenEphysBinaryEventExtractor(NeoBaseEventExtractor):
     """
     Class for reading events saved by the Open Ephys GUI
 
@@ -201,26 +233,26 @@
     Based on neo.rawio.OpenEphysBinaryRawIO
 
     Parameters
     ----------
     folder_path: str
 
     """
-    mode = 'folder'
-    NeoRawIOClass = 'OpenEphysBinaryRawIO'
+
+    mode = "folder"
+    NeoRawIOClass = "OpenEphysBinaryRawIO"
     name = "openephys"
 
     def __init__(self, folder_path, block_index=None):
         neo_kwargs = self.map_to_neo_kwargs(folder_path)
-        NeoBaseEventExtractor.__init__(self, block_index=block_index,
-                                       **neo_kwargs)
+        NeoBaseEventExtractor.__init__(self, block_index=block_index, **neo_kwargs)
 
     @classmethod
     def map_to_neo_kwargs(cls, folder_path):
-        neo_kwargs = {'dirname': str(folder_path)}
+        neo_kwargs = {"dirname": str(folder_path)}
         return neo_kwargs
 
 
 def read_openephys(folder_path, **kwargs):
     """
     Read 'legacy' or 'binary' Open Ephys formats
 
@@ -239,15 +271,15 @@
 
     Returns
     -------
     recording: OpenEphysLegacyRecordingExtractor or OpenEphysBinaryExtractor
     """
     # auto guess format
     files = [str(f) for f in Path(folder_path).iterdir()]
-    if np.any([f.endswith('continuous') for f in files]):
+    if np.any([f.endswith("continuous") for f in files]):
         #  format = 'legacy'
         recording = OpenEphysLegacyRecordingExtractor(folder_path, **kwargs)
     else:
         # format = 'binary'
         recording = OpenEphysBinaryRecordingExtractor(folder_path, **kwargs)
     return recording
 
@@ -265,13 +297,13 @@
 
     Returns
     -------
     event: OpenEphysBinaryEventExtractor
     """
     # auto guess format
     files = [str(f) for f in Path(folder_path).iterdir()]
-    if np.any([f.startswith('Continuous') for f in files]):
+    if np.any([f.startswith("Continuous") for f in files]):
         raise Exception("Events can be read only from 'binary' format")
     else:
         # format = 'binary'
         event = OpenEphysBinaryEventExtractor(folder_path, block_index=block_index)
     return event
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/plexon.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/plexon.py`

 * *Files 6% similar despite different names*

```diff
@@ -16,29 +16,29 @@
     stream_id: str, optional
         If there are several streams, specify the stream id you want to load.
     stream_name: str, optional
         If there are several streams, specify the stream name you want to load.
     all_annotations: bool, default: False
         Load exhaustively all annotations from neo.
     """
-    mode = 'file'
-    NeoRawIOClass = 'PlexonRawIO'
+
+    mode = "file"
+    NeoRawIOClass = "PlexonRawIO"
     name = "plexon"
 
     def __init__(self, file_path, stream_id=None, stream_name=None, all_annotations=False):
         neo_kwargs = self.map_to_neo_kwargs(file_path)
-        NeoBaseRecordingExtractor.__init__(self, stream_id=stream_id, 
-                                           stream_name=stream_name,
-                                           all_annotations=all_annotations, 
-                                           **neo_kwargs)
-        self._kwargs.update({'file_path': str(file_path)})
+        NeoBaseRecordingExtractor.__init__(
+            self, stream_id=stream_id, stream_name=stream_name, all_annotations=all_annotations, **neo_kwargs
+        )
+        self._kwargs.update({"file_path": str(file_path)})
 
     @classmethod
     def map_to_neo_kwargs(cls, file_path):
-        neo_kwargs = {'filename': str(file_path)}
+        neo_kwargs = {"filename": str(file_path)}
         return neo_kwargs
 
 
 class PlexonSortingExtractor(NeoBaseSortingExtractor):
     """
     Class for reading plexon spiking data (.plx files).
 
@@ -48,26 +48,24 @@
     ----------
     file_path: str
         The file path to load the recordings from.
     """
 
     mode = "file"
     NeoRawIOClass = "PlexonRawIO"
-    handle_spike_frame_directly = False
     name = "plexon"
+    neo_returns_frames = True
 
     def __init__(self, file_path):
-        from neo.rawio import PlexonRawIO
         neo_kwargs = self.map_to_neo_kwargs(file_path)
-        neo_reader = PlexonRawIO(**neo_kwargs)
-        neo_reader.parse_header()
-        sampling_frequency = neo_reader._global_ssampling_rate
-        NeoBaseSortingExtractor.__init__(self, sampling_frequency=sampling_frequency,
-                                         **neo_kwargs)
-        self._kwargs.update({"file_path": str(file_path)})
+        self.neo_reader = NeoBaseSortingExtractor.get_neo_io_reader(self.NeoRawIOClass, **neo_kwargs)
+        self.neo_reader.parse_header()
+        sampling_frequency = self.neo_reader._global_ssampling_rate
+        NeoBaseSortingExtractor.__init__(self, sampling_frequency=sampling_frequency, **neo_kwargs)
+        self._kwargs = {"file_path": str(file_path)}
 
     @classmethod
     def map_to_neo_kwargs(cls, file_path):
         neo_kwargs = {"filename": str(file_path)}
         return neo_kwargs
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/spike2.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/tdt.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,43 +1,46 @@
 from spikeinterface.core.core_tools import define_function_from_class
 
-from .neobaseextractor import NeoBaseRecordingExtractor, NeoBaseSortingExtractor
+from .neobaseextractor import NeoBaseRecordingExtractor
 
 
-class Spike2RecordingExtractor(NeoBaseRecordingExtractor):
+class TdtRecordingExtractor(NeoBaseRecordingExtractor):
     """
-    Class for reading spike2 smr files.
-    smrx are not supported with this, prefer CedRecordingExtractor instead.
+    Class for reading TDT folder.
 
-    Based on :py:class:`neo.rawio.Spike2RawIO`
+    Based on :py:class:`neo.rawio.TdTRawIO`
 
     Parameters
     ----------
-    file_path: str
-        The file path to load the recordings from.
+    folder_path: str
+        The folder path to the tdt folder.
     stream_id: str, optional
         If there are several streams, specify the stream id you want to load.
     stream_name: str, optional
         If there are several streams, specify the stream name you want to load.
     all_annotations: bool, default: False
         Load exhaustively all annotations from neo.
     """
-    mode = 'file'
-    NeoRawIOClass = 'Spike2RawIO'
-    name = "spike2"
-
-    def __init__(self, file_path, stream_id=None, stream_name=None, all_annotations=False):
-        neo_kwargs = self.map_to_neo_kwargs(file_path)
-        NeoBaseRecordingExtractor.__init__(self, stream_id=stream_id, 
-                                           stream_name=stream_name,
-                                           all_annotations=all_annotations, 
-                                           **neo_kwargs)
-        self._kwargs.update({'file_path': str(file_path)})
-        self.extra_requirements.append('sonpy')
+
+    mode = "folder"
+    NeoRawIOClass = "TdtRawIO"
+    name = "tdt"
+
+    def __init__(self, folder_path, stream_id=None, stream_name=None, block_index=None, all_annotations=False):
+        neo_kwargs = self.map_to_neo_kwargs(folder_path)
+        NeoBaseRecordingExtractor.__init__(
+            self,
+            stream_id=stream_id,
+            stream_name=stream_name,
+            block_index=block_index,
+            all_annotations=all_annotations,
+            **neo_kwargs,
+        )
+        self._kwargs.update(dict(folder_path=str(folder_path)))
 
     @classmethod
-    def map_to_neo_kwargs(cls, file_path):
-        neo_kwargs = {'filename': str(file_path)}
+    def map_to_neo_kwargs(cls, folder_path):
+        neo_kwargs = {"dirname": str(folder_path)}
         return neo_kwargs
 
 
-read_spike2 = define_function_from_class(source_class=Spike2RecordingExtractor, name="read_spike2")
+read_tdt = define_function_from_class(source_class=TdtRecordingExtractor, name="read_tdt")
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/spikegadgets.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/spikegadgets.py`

 * *Files 22% similar despite different names*

```diff
@@ -16,26 +16,26 @@
     stream_id: str, optional
         If there are several streams, specify the stream id you want to load.
     stream_name: str, optional
         If there are several streams, specify the stream name you want to load.
     all_annotations: bool, default: False
         Load exhaustively all annotations from neo.
     """
-    mode = 'file'
-    NeoRawIOClass = 'SpikeGadgetsRawIO'
+
+    mode = "file"
+    NeoRawIOClass = "SpikeGadgetsRawIO"
     name = "spikegadgets"
 
     def __init__(self, file_path, stream_id=None, stream_name=None, block_index=None, all_annotations=False):
         neo_kwargs = self.map_to_neo_kwargs(file_path)
-        NeoBaseRecordingExtractor.__init__(self, stream_id=stream_id, 
-                                           stream_name=stream_name,
-                                           all_annotations=all_annotations, 
-                                           **neo_kwargs)
+        NeoBaseRecordingExtractor.__init__(
+            self, stream_id=stream_id, stream_name=stream_name, all_annotations=all_annotations, **neo_kwargs
+        )
         self._kwargs.update(dict(file_path=str(file_path), stream_id=stream_id))
 
     @classmethod
     def map_to_neo_kwargs(cls, file_path):
-        neo_kwargs = {'filename': str(file_path)}
+        neo_kwargs = {"filename": str(file_path)}
         return neo_kwargs
 
 
 read_spikegadgets = define_function_from_class(source_class=SpikeGadgetsRecordingExtractor, name="read_spikegadgets")
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/neoextractors/spikeglx.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/neoextractors/spikeglx.py`

 * *Files 8% similar despite different names*

```diff
@@ -10,15 +10,14 @@
 
 from spikeinterface.core.core_tools import define_function_from_class
 from spikeinterface.extractors.neuropixels_utils import get_neuropixels_sample_shifts
 
 from .neobaseextractor import NeoBaseRecordingExtractor
 
 
-
 class SpikeGLXRecordingExtractor(NeoBaseRecordingExtractor):
     """
     Class for reading data saved by SpikeGLX software.
     See https://billkarsh.github.io/SpikeGLX/
 
     Based on :py:class:`neo.rawio.SpikeGLXRawIO`
 
@@ -26,79 +25,77 @@
     So if the folder contain several streams ('imec0.ap' 'nidq' 'imec0.lf')
     then it has to be specified with 'stream_id'.
 
     Parameters
     ----------
     folder_path: str
         The folder path to load the recordings from.
-    load_sync_channel: bool dafult False
+    load_sync_channel: bool default False
         Whether or not to load the last channel in the stream, which is typically used for synchronization.
         If True, then the probe is not loaded.
     stream_id: str, optional
         If there are several streams, specify the stream id you want to load.
         For example, 'imec0.ap' 'nidq' or 'imec0.lf'.
     stream_name: str, optional
         If there are several streams, specify the stream name you want to load.
     all_annotations: bool, default: False
         Load exhaustively all annotations from neo.
     """
+
     mode = "folder"
     NeoRawIOClass = "SpikeGLXRawIO"
     name = "spikeglx"
-    has_default_locations = True
 
     def __init__(self, folder_path, load_sync_channel=False, stream_id=None, stream_name=None, all_annotations=False):
         neo_kwargs = self.map_to_neo_kwargs(folder_path, load_sync_channel=load_sync_channel)
-        NeoBaseRecordingExtractor.__init__(self, stream_id=stream_id, 
-                                           stream_name=stream_name,
-                                           all_annotations=all_annotations,
-                                           **neo_kwargs)
-
+        NeoBaseRecordingExtractor.__init__(
+            self, stream_id=stream_id, stream_name=stream_name, all_annotations=all_annotations, **neo_kwargs
+        )
 
         # open the corresponding stream probe for LF and AP
         # if load_sync_channel=False
         if "nidq" not in self.stream_id and not load_sync_channel:
-            signals_info_dict = {
-                e["stream_name"]: e for e in self.neo_reader.signals_info_list
-            }
+            signals_info_dict = {e["stream_name"]: e for e in self.neo_reader.signals_info_list}
             meta_filename = signals_info_dict[self.stream_id]["meta_file"]
             # Load probe geometry if available
             if "lf" in self.stream_id:
                 meta_filename = meta_filename.replace(".lf", ".ap")
             probe = pi.read_spikeglx(meta_filename)
 
             if probe.shank_ids is not None:
                 self.set_probe(probe, in_place=True, group_mode="by_shank")
             else:
                 self.set_probe(probe, in_place=True)
 
             # load num_channels_per_adc depending on probe type
             ptype = probe.annotations["probe_type"]
 
-            if ptype in [21, 24]: # NP2.0
+            if ptype in [21, 24]:  # NP2.0
                 num_channels_per_adc = 16
+                num_cycles_in_adc = 16  # TODO: Check this.
                 total_channels = 384
-            else: # NP1.0
+            else:  # NP1.0
                 num_channels_per_adc = 12
+                num_cycles_in_adc = 13 if "ap" in self.stream_id else 12
                 total_channels = 384
-            
+
             # sample_shifts is generated from total channels (384) channels
             # when only some channels are saved we need to slice this vector (like we do for the probe)
-            sample_shifts = get_neuropixels_sample_shifts(total_channels, num_channels_per_adc)
+            sample_shifts = get_neuropixels_sample_shifts(total_channels, num_channels_per_adc, num_cycles_in_adc)
             if self.get_num_channels() != total_channels:
                 # need slice because not all channel are saved
                 chans = pi.get_saved_channel_indices_from_spikeglx_meta(meta_filename)
                 # lets clip to 384 because this contains also the synchro channel
-                chans = chans[chans<total_channels]
+                chans = chans[chans < total_channels]
                 sample_shifts = sample_shifts[chans]
-            
+
             self.set_property("inter_sample_shift", sample_shifts)
 
         self._kwargs.update(dict(folder_path=str(folder_path), load_sync_channel=load_sync_channel))
 
     @classmethod
     def map_to_neo_kwargs(cls, folder_path, load_sync_channel=False):
-        neo_kwargs = {'dirname': str(folder_path), 'load_sync_channel': load_sync_channel}
+        neo_kwargs = {"dirname": str(folder_path), "load_sync_channel": load_sync_channel}
         return neo_kwargs
 
 
 read_spikeglx = define_function_from_class(source_class=SpikeGLXRecordingExtractor, name="read_spikeglx")
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/neuropixels_utils.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/neuropixels_utils.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,45 +1,53 @@
 import numpy as np
 
 
-def get_neuropixels_sample_shifts(num_channels=384, num_channels_per_adc=12):
+def get_neuropixels_sample_shifts(num_channels=384, num_channels_per_adc=12, num_cycles=None):
     """
     Calculates the relative sampling phase of each channel that results
     from Neuropixels ADC multiplexing.
 
     This information is needed to perform the preprocessing.phase_shift operation.
 
     See https://github.com/int-brain-lab/ibllib/blob/master/ibllib/ephys/neuropixel.py
-    
-    
+
+
     for the original implementation.
 
     Parameters
     ----------
     num_channels : int, default: 384
         The total number of channels in a recording.
         All currently available Neuropixels variants have 384 channels.
     num_channels_per_adc : int, default: 12
         The number of channels per ADC on the probe.
         Neuropixels 1.0 probes have 12 ADCs.
         Neuropixels 2.0 probes have 16 ADCs.
+    num_cycles: int or None, default: None
+        The number of cycles in the ADC on the probe.
+        Neuropixels 1.0 probes have 13 cycles for AP and 12 for LFP.
+        Neuropixels 2.0 probes have 16 cycles.
+        If None, the num_channels_per_adc is used.
 
     Returns
     -------
     sample_shifts : ndarray
         The relative phase (from 0-1) of each channel
     """
+    if num_cycles is None:
+        num_cycles = num_channels_per_adc
 
-    adc_indices = np.floor(np.arange(num_channels) /
-                           (num_channels_per_adc * 2)) * 2 + np.mod(np.arange(num_channels), 2)
+    adc_indices = np.floor(np.arange(num_channels) / (num_channels_per_adc * 2)) * 2 + np.mod(
+        np.arange(num_channels), 2
+    )
 
     sample_shifts = np.zeros_like(adc_indices)
 
     for a in adc_indices:
-        sample_shifts[adc_indices == a] = np.arange(num_channels_per_adc) / num_channels_per_adc
+        sample_shifts[adc_indices == a] = np.arange(num_channels_per_adc) / num_cycles
 
     return sample_shifts
 
 
 def get_neuropixels_channel_groups(num_channels=384, num_adcs=12):
     """
     Returns groups of simultaneously sampled channels on a Neuropixels probe.
@@ -76,49 +84,53 @@
     groups : list
         A list of lists of simultaneously sampled channel indices
     """
 
     groups = []
 
     for i in range(num_channels_per_adc):
-
         groups.append(
             list(
-                np.sort(np.concatenate([np.arange(i*2, num_channels, num_channels_per_adc*2),
-                                        np.arange(i*2+1, num_channels, num_channels_per_adc*2)]))
+                np.sort(
+                    np.concatenate(
+                        [
+                            np.arange(i * 2, num_channels, num_channels_per_adc * 2),
+                            np.arange(i * 2 + 1, num_channels, num_channels_per_adc * 2),
+                        ]
+                    )
+                )
             )
         )
 
     return groups
 
 
 def synchronize_neuropixel_streams(recording_ref, recording_other):
     """
     Use the last "sync" channel from spikeglx or openephys neuropixels to synchronize
     recordings.
-    
+
     Method used :
       1. detect pulse times on both streams.
       2. make a linear regression from 'other' to 'ref'.
           The slope is nclose to 1 and corresponds to the sample rate correction
           The intercept is close to 0 and corresponds to the delta time start
-    
+
     """
     # This will be done very very soon, I promise.
-    raise NotImplementedError    
-    
+    raise NotImplementedError
+
     synhcro_chan_id = recording_ref.channel_ids[-1]
     trig_ref = recording_ref.get_traces(channel_ids=[synhcro_chan_id], return_scaled=False)
     trig_ref = trig_ref[:, 0]
     times_ref = recording_ref.get_times()
-    
+
     synhcro_chan_id = recording_other.channel_ids[-1]
     trig_other = recording_other.get_traces(channel_ids=[synhcro_chan_id], return_scaled=False)
     trig_other = trig_other[:, 0]
     times_other = recording_other.get_times()
-    
-    # import matplotlib.pyplot as plt
-    # fig, ax = plt.subplots()
-    # ax.plot(times_ref, trig_ref)
-    # ax.plot(times_other, trig_other)
-    # plt.show()
 
+    # import matplotlib.pyplot as plt
+    # fig, ax = plt.subplots()
+    # ax.plot(times_ref, trig_ref)
+    # ax.plot(times_other, trig_other)
+    # plt.show()
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/phykilosortextractors.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/phykilosortextractors.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 from pathlib import Path
 
 import numpy as np
 
-from spikeinterface.core import (BaseSorting, BaseSortingSegment, read_python)
+from spikeinterface.core import BaseSorting, BaseSortingSegment, read_python
 from spikeinterface.core.core_tools import define_function_from_class
 
 
 class BasePhyKilosortSortingExtractor(BaseSorting):
     """Base SortingExtractor for Phy and Kilosort output folder.
 
     Parameters
@@ -14,76 +14,90 @@
     folder_path: str or Path
         Path to the output Phy folder (containing the params.py)
     exclude_cluster_groups: list or str, optional
         Cluster groups to exclude (e.g. "noise" or ["noise", "mua"]).
     keep_good_only : bool, default: True
         Whether to only keep good units.
     """
-    extractor_name = 'BasePhyKilosortSorting'
+
+    extractor_name = "BasePhyKilosortSorting"
     installed = False  # check at class level if installed or not
-    mode = 'folder'
-    installation_mesg = "To use the PhySortingExtractor install pandas: \n\n pip install pandas\n\n"  # error message when not installed
+    mode = "folder"
+    installation_mesg = (
+        "To use the PhySortingExtractor install pandas: \n\n pip install pandas\n\n"  # error message when not installed
+    )
     name = "phykilosort"
 
-    def __init__(self, folder_path, exclude_cluster_groups=None, keep_good_only=False,
-                 load_all_cluster_properties=True):
+    def __init__(
+        self,
+        folder_path,
+        exclude_cluster_groups=None,
+        keep_good_only=False,
+        remove_empty_units=False,
+        load_all_cluster_properties=True,
+    ):
         try:
             import pandas as pd
+
             HAVE_PD = True
         except ImportError:
             HAVE_PD = False
         assert HAVE_PD, self.installation_mesg
 
         phy_folder = Path(folder_path)
 
-        spike_times = np.load(phy_folder / 'spike_times.npy')
+        spike_times = np.load(phy_folder / "spike_times.npy").astype(int)
 
-        if (phy_folder / 'spike_clusters.npy').is_file():
-            spike_clusters = np.load(phy_folder / 'spike_clusters.npy')
+        if (phy_folder / "spike_clusters.npy").is_file():
+            spike_clusters = np.load(phy_folder / "spike_clusters.npy")
         else:
-            spike_clusters = np.load(phy_folder / 'spike_templates.npy')
+            spike_clusters = np.load(phy_folder / "spike_templates.npy")
+
+        # spike_times and spike_clusters can be 2d sometimes --> convert to 1d.
+        spike_times = np.atleast_1d(spike_times.squeeze())
+        spike_clusters = np.atleast_1d(spike_clusters.squeeze())
 
         clust_id = np.unique(spike_clusters)
-        unit_ids = list(clust_id)
-        spike_times = spike_times.astype(int)
-        params = read_python(str(phy_folder / 'params.py'))
-        sampling_frequency = params['sample_rate']
+        unique_unit_ids = list(clust_id)
+        params = read_python(str(phy_folder / "params.py"))
+        sampling_frequency = params["sample_rate"]
 
         # try to load cluster info
-        cluster_info_files = [p for p in phy_folder.iterdir() if p.suffix in ['.csv', '.tsv']
-                              and "cluster_info" in p.name]
+        cluster_info_files = [
+            p for p in phy_folder.iterdir() if p.suffix in [".csv", ".tsv"] and "cluster_info" in p.name
+        ]
 
         if len(cluster_info_files) == 1:
             # load properties from cluster_info file
             cluster_info_file = cluster_info_files[0]
             if cluster_info_file.suffix == ".tsv":
                 delimiter = "\t"
             else:
                 delimiter = ","
             cluster_info = pd.read_csv(cluster_info_file, delimiter=delimiter)
         else:
             # load properties from other tsv/csv files
-            all_property_files = [p for p in phy_folder.iterdir() if p.suffix in ['.csv', '.tsv']]
+            all_property_files = [p for p in phy_folder.iterdir() if p.suffix in [".csv", ".tsv"]]
 
             cluster_info = None
             for file in all_property_files:
                 if file.suffix == ".tsv":
                     delimiter = "\t"
                 else:
                     delimiter = ","
                 new_property = pd.read_csv(file, delimiter=delimiter)
                 if cluster_info is None:
                     cluster_info = new_property
                 else:
-                    cluster_info = pd.merge(cluster_info, new_property, on='cluster_id', suffixes=[None, '_repeat'])
+                    cluster_info = pd.merge(cluster_info, new_property, on="cluster_id", suffixes=[None, "_repeat"])
 
         # in case no tsv/csv files are found populate cluster info with minimal info
         if cluster_info is None:
-            cluster_info = pd.DataFrame({'cluster_id': unit_ids})
-            cluster_info['group'] = ['unsorted'] * len(unit_ids)
+            cluster_info = pd.DataFrame({"cluster_id": unique_unit_ids})
+            cluster_info["group"] = ["unsorted"] * len(unique_unit_ids)
 
         if exclude_cluster_groups is not None:
             if isinstance(exclude_cluster_groups, str):
                 cluster_info = cluster_info.query(f"group != '{exclude_cluster_groups}'")
             elif isinstance(exclude_cluster_groups, list):
                 if len(exclude_cluster_groups) > 0:
                     for exclude_group in exclude_cluster_groups:
@@ -93,77 +107,86 @@
             cluster_info = cluster_info.query("KSLabel == 'good'")
 
         if "cluster_id" not in cluster_info.columns:
             assert "id" in cluster_info.columns, "Couldn't find cluster ids in the tsv files!"
             cluster_info.loc[:, "cluster_id"] = cluster_info["id"].values
             del cluster_info["id"]
 
+        if remove_empty_units:
+            cluster_info = cluster_info.query(f"cluster_id in {unique_unit_ids}")
+
         # update spike clusters and times values
-        bad_clusters = [clust for clust in clust_id if clust not in cluster_info['cluster_id'].values]
+        bad_clusters = [clust for clust in clust_id if clust not in cluster_info["cluster_id"].values]
         spike_clusters_clean_idxs = ~np.isin(spike_clusters, bad_clusters)
         spike_clusters_clean = spike_clusters[spike_clusters_clean_idxs]
         spike_times_clean = spike_times[spike_clusters_clean_idxs]
 
-        if 'si_unit_id' in cluster_info.columns:
+        if "si_unit_id" in cluster_info.columns:
             unit_ids = cluster_info["si_unit_id"].values
 
             if np.all(np.isnan(unit_ids)):
                 max_si_unit_id = -1
             else:
                 max_si_unit_id = int(np.nanmax(unit_ids))
 
-            for i, (phy_id, si_id) in enumerate(zip(cluster_info["cluster_id"].values,
-                                                    cluster_info["si_unit_id"].values)):
-                if np.isnan(si_id):
+            for i, (phy_id, si_id) in enumerate(
+                zip(cluster_info["cluster_id"].values, cluster_info["si_unit_id"].values)
+            ):
+                if np.isnan(si_id) or np.count_nonzero(cluster_info["si_unit_id"].values == si_id) != 1:
                     max_si_unit_id += 1
                     new_si_id = int(max_si_unit_id)
                 else:
                     new_si_id = si_id
                 unit_ids[i] = new_si_id
 
             # Little hack to replace values in spike_clusters_clean to spike_clusters_new very efficiently.
-            from_values = cluster_info['cluster_id'].values
+            from_values = cluster_info["cluster_id"].values
             sort_idx = np.argsort(from_values)
             idx = np.searchsorted(from_values, spike_clusters_clean, sorter=sort_idx)
             spike_clusters_new = unit_ids[sort_idx][idx]
 
             unit_ids = unit_ids.astype(int)
             spike_clusters_clean = spike_clusters_new
             del cluster_info["si_unit_id"]
         else:
             unit_ids = cluster_info["cluster_id"].values
 
         BaseSorting.__init__(self, sampling_frequency, unit_ids)
-        self.extra_requirements.append('pandas')
+        self.extra_requirements.append("pandas")
 
-        del cluster_info["cluster_id"]
         for prop_name in cluster_info.columns:
-            if prop_name in ['chan_grp', 'ch_group']:
+            if prop_name in ["chan_grp", "ch_group"]:
                 self.set_property(key="group", values=cluster_info[prop_name])
+            elif prop_name == "cluster_id":
+                self.set_property(key="original_cluster_id", values=cluster_info[prop_name])
             elif prop_name != "group":
                 self.set_property(key=prop_name, values=cluster_info[prop_name])
             elif prop_name == "group":
                 # rename group property to 'quality'
                 self.set_property(key="quality", values=cluster_info[prop_name])
             else:
                 if load_all_cluster_properties:
                     self.set_property(key=prop_name, values=cluster_info[prop_name])
 
+        self.annotate(phy_folder=str(phy_folder.resolve()))
+
         self.add_sorting_segment(PhySortingSegment(spike_times_clean, spike_clusters_clean))
 
 
 class PhySortingSegment(BaseSortingSegment):
     def __init__(self, all_spikes, all_clusters):
         BaseSortingSegment.__init__(self)
         self._all_spikes = all_spikes
         self._all_clusters = all_clusters
 
     def get_unit_spike_train(self, unit_id, start_frame, end_frame):
         start = 0 if start_frame is None else np.searchsorted(self._all_spikes, start_frame, side="left")
-        end = len(self._all_spikes) if end_frame is None else np.searchsorted(self._all_spikes, end_frame, side="right")
+        end = (
+            len(self._all_spikes) if end_frame is None else np.searchsorted(self._all_spikes, end_frame, side="left")
+        )  # Exclude end frame
 
         spike_times = self._all_spikes[start:end][self._all_clusters[start:end] == unit_id]
         return np.atleast_1d(spike_times.copy().squeeze())
 
 
 class PhySortingExtractor(BasePhyKilosortSortingExtractor):
     """Load Phy format data as a sorting extractor.
@@ -176,48 +199,58 @@
         Cluster groups to exclude (e.g. "noise" or ["noise", "mua"]).
 
     Returns
     -------
     extractor : PhySortingExtractor
         The loaded data.
     """
-    extractor_name = 'PhySorting'
+
+    extractor_name = "PhySorting"
     name = "phy"
 
     def __init__(self, folder_path, exclude_cluster_groups=None):
         BasePhyKilosortSortingExtractor.__init__(self, folder_path, exclude_cluster_groups, keep_good_only=False)
 
-        self._kwargs = {'folder_path': str(Path(folder_path).absolute()),
-                        'exclude_cluster_groups': exclude_cluster_groups}
+        self._kwargs = {
+            "folder_path": str(Path(folder_path).absolute()),
+            "exclude_cluster_groups": exclude_cluster_groups,
+        }
 
 
 class KiloSortSortingExtractor(BasePhyKilosortSortingExtractor):
     """Load Kilosort format data as a sorting extractor.
 
     Parameters
     ----------
     folder_path: str or Path
         Path to the output Phy folder (containing the params.py).
     exclude_cluster_groups: list or str, optional
         Cluster groups to exclude (e.g. "noise" or ["noise", "mua"]).
     keep_good_only : bool, default: True
         Whether to only keep good units.
         If True, only Kilosort-labeled 'good' units are returned.
+    remove_empty_units : bool, default: True
+        If True, empty units are removed from the sorting extractor.
 
     Returns
     -------
     extractor : KiloSortSortingExtractor
         The loaded data.
     """
-    extractor_name = 'KiloSortSorting'
+
+    extractor_name = "KiloSortSorting"
     name = "kilosort"
 
-    def __init__(self, folder_path, keep_good_only=False):
-        BasePhyKilosortSortingExtractor.__init__(self, folder_path, exclude_cluster_groups=None,
-                                                 keep_good_only=keep_good_only)
+    def __init__(self, folder_path, keep_good_only=False, remove_empty_units=True):
+        BasePhyKilosortSortingExtractor.__init__(
+            self,
+            folder_path,
+            exclude_cluster_groups=None,
+            keep_good_only=keep_good_only,
+            remove_empty_units=remove_empty_units,
+        )
 
-        self._kwargs = {'folder_path': str(Path(folder_path).absolute()),
-                        'keep_good_only': keep_good_only}
+        self._kwargs = {"folder_path": str(Path(folder_path).absolute()), "keep_good_only": keep_good_only}
 
 
 read_phy = define_function_from_class(source_class=PhySortingExtractor, name="read_phy")
 read_kilosort = define_function_from_class(source_class=KiloSortSortingExtractor, name="read_kilosort")
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/shybridextractors.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/shybridextractors.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,25 +1,16 @@
-import json
 from pathlib import Path
 
 import numpy as np
 
 from probeinterface import read_prb, write_prb
 
 from spikeinterface.core import BinaryRecordingExtractor, BaseRecordingSegment, BaseSorting, BaseSortingSegment
 from spikeinterface.core.core_tools import write_binary_recording, define_function_from_class
 
-try:
-    import hybridizer.io as sbio
-    import hybridizer.probes as sbprb
-    import yaml
-    HAVE_SBEX = True
-except ImportError:
-    HAVE_SBEX = False
-
 
 class SHYBRIDRecordingExtractor(BinaryRecordingExtractor):
     """Load SHYBRID format data as a recording extractor.
 
     Parameters
     ----------
     file_path : str or Path
@@ -27,60 +18,68 @@
 
     Returns
     -------
     extractor : SHYBRIDRecordingExtractor
         Loaded data.
     """
 
-    extractor_name = 'SHYBRIDRecording'
-    has_default_locations = True
-    installed = HAVE_SBEX  # check at class level if installed or not
-    is_writable = True
-    mode = 'folder'
-    installation_mesg = "To use the SHYBRID extractors, install SHYBRID and pyyaml: " \
-                        "\n\n pip install shybrid pyyaml\n\n"
+    extractor_name = "SHYBRIDRecording"
+    mode = "folder"
+    installation_mesg = (
+        "To use the SHYBRID extractors, install SHYBRID and pyyaml: " "\n\n pip install shybrid pyyaml\n\n"
+    )
     name = "shybrid"
 
     def __init__(self, file_path):
+        try:
+            import hybridizer.io as sbio
+            import hybridizer.probes as sbprb
+            import yaml
+
+            HAVE_SBEX = True
+        except ImportError:
+            HAVE_SBEX = False
+
         # load params file related to the given shybrid recording
-        assert self.installed, self.installation_mesg
+        assert HAVE_SBEX, self.installation_mesg
         assert Path(file_path).suffix in [".yml", ".yaml"], "The 'file_path' should be a yaml file!"
-        params = sbio.get_params(file_path)['data']
+        params = sbio.get_params(file_path)["data"]
         file_path = Path(file_path)
 
         # create a shybrid probe object
-        probe = sbprb.Probe(params['probe'])
+        probe = sbprb.Probe(params["probe"])
         nb_channels = probe.total_nb_channels
 
         # translate the byte ordering
-        byte_order = params['order']
-        if byte_order == 'C':
+        byte_order = params["order"]
+        if byte_order == "C":
             time_axis = 1
-        elif byte_order == 'F':
+        elif byte_order == "F":
             time_axis = 0
 
         bin_file = file_path.parent / f"{file_path.stem}.bin"
 
         # piggyback on binary data recording extractor
-        BinaryRecordingExtractor.__init__(self,
-                                          file_paths=bin_file,
-                                          sampling_frequency=float(params['fs']),
-                                          num_chan=nb_channels,
-                                          dtype=params['dtype'],
-                                          time_axis=time_axis)
+        BinaryRecordingExtractor.__init__(
+            self,
+            file_paths=bin_file,
+            sampling_frequency=float(params["fs"]),
+            num_channels=nb_channels,
+            dtype=params["dtype"],
+            time_axis=time_axis,
+        )
 
         # load probe file
-        probegroup = read_prb(params['probe'])
+        probegroup = read_prb(params["probe"])
         self.set_probegroup(probegroup, in_place=True)
-        self._kwargs = {'file_path': str(Path(file_path).absolute())}
-        self.extra_requirements.extend(['hybridizer', 'pyyaml'])
+        self._kwargs = {"file_path": str(Path(file_path).absolute())}
+        self.extra_requirements.extend(["hybridizer", "pyyaml"])
 
     @staticmethod
-    def write_recording(recording, save_path, initial_sorting_fn, dtype='float32', verbose=True,
-                        **job_kwargs):
+    def write_recording(recording, save_path, initial_sorting_fn, dtype="float32", **job_kwargs):
         """Convert and save the recording extractor to SHYBRID format.
 
         Parameters
         ----------
         recording: RecordingExtractor
             The recording extractor to be converted and saved.
         save_path: str
@@ -88,45 +87,53 @@
         initial_sorting_fn: str
             Full path to the initial sorting csv file (can also be generated
             using write_sorting static method from the SHYBRIDSortingExtractor).
         dtype: dtype
             Type of the saved data. Default float32.
         **write_binary_kwargs: keyword arguments for write_to_binary_dat_format() function
         """
+        try:
+            import hybridizer.io as sbio
+            import hybridizer.probes as sbprb
+            import yaml
+
+            HAVE_SBEX = True
+        except ImportError:
+            HAVE_SBEX = False
+
         assert HAVE_SBEX, SHYBRIDRecordingExtractor.installation_mesg
         assert recording.get_num_segments() == 1, "SHYBRID can only write single segment recordings"
         save_path = Path(save_path)
-        recording_name = 'recording.bin'
-        probe_name = 'probe.prb'
-        params_name = 'recording.yml'
+        recording_name = "recording.bin"
+        probe_name = "probe.prb"
+        params_name = "recording.yml"
 
         # location information has to be present in order for shybrid to
         # be able to operate on the recording
         if recording.get_channel_locations() is None:
             raise GeometryNotLoadedError("Channel locations were not found")
 
         # write recording
         recording_fn = (save_path / recording_name).absolute()
-        write_binary_recording(recording, file_paths=recording_fn, dtype=dtype, verbose=verbose, **job_kwargs)
+        write_binary_recording(recording, file_paths=recording_fn, dtype=dtype, **job_kwargs)
 
         # write probe file
         probe_fn = (save_path / probe_name).absolute()
         probegroup = recording.get_probegroup()
         write_prb(probe_fn, probegroup, total_nb_channels=recording.get_num_channels())
 
         # create parameters file
-        parameters = dict(clusters=initial_sorting_fn,
-                          data=dict(dtype=dtype,
-                                    fs=str(recording.get_sampling_frequency()),
-                                    order='F',
-                                    probe=str(probe_fn)))
+        parameters = dict(
+            clusters=initial_sorting_fn,
+            data=dict(dtype=dtype, fs=str(recording.get_sampling_frequency()), order="F", probe=str(probe_fn)),
+        )
 
         # write parameters file
         parameters_fn = (save_path / params_name).absolute()
-        with parameters_fn.open('w') as fp:
+        with parameters_fn.open("w") as fp:
             yaml.dump(parameters, fp)
 
 
 class SHYBRIDSortingExtractor(BaseSorting):
     """Load SHYBRID format data as a sorting extractor.
 
     Parameters
@@ -140,99 +147,119 @@
 
     Returns
     -------
     extractor : SHYBRIDSortingExtractor
         Loaded data.
     """
 
-    extractor_name = 'SHYBRIDSorting'
-    installed = HAVE_SBEX
-    is_writable = True
+    extractor_name = "SHYBRIDSorting"
     installation_mesg = "To use the SHYBRID extractors, install SHYBRID: \n\n pip install shybrid\n\n"
     name = "shybrid"
 
-    def __init__(self, file_path, sampling_frequency, delimiter=','):
-        assert self.installed, self.installation_mesg
+    def __init__(self, file_path, sampling_frequency, delimiter=","):
+        try:
+            import hybridizer.io as sbio
+            import hybridizer.probes as sbprb
+
+            HAVE_SBEX = True
+        except ImportError:
+            HAVE_SBEX = False
+
+        assert HAVE_SBEX, self.installation_mesg
         assert Path(file_path).suffix == ".csv", "The 'file_path' should be a csv file!"
 
         if Path(file_path).is_file():
             spike_clusters = sbio.SpikeClusters()
             spike_clusters.fromCSV(str(file_path), None, delimiter=delimiter)
         else:
-            raise FileNotFoundError(f'The ground truth file {file_path} could not be found')
+            raise FileNotFoundError(f"The ground truth file {file_path} could not be found")
 
         BaseSorting.__init__(self, unit_ids=spike_clusters.keys(), sampling_frequency=sampling_frequency)
 
         sorting_segment = SHYBRIDSortingSegment(spike_clusters)
         self.add_sorting_segment(sorting_segment)
 
-        self._kwargs = {'file_path': str(Path(file_path).absolute()), 'sampling_frequency': sampling_frequency,
-                        'delimiter': delimiter}
-        self.extra_requirements.append('hybridizer')
+        self._kwargs = {
+            "file_path": str(Path(file_path).absolute()),
+            "sampling_frequency": sampling_frequency,
+            "delimiter": delimiter,
+        }
+        self.extra_requirements.append("hybridizer")
 
     @staticmethod
     def write_sorting(sorting, save_path):
         """Convert and save the sorting extractor to SHYBRID CSV format.
 
         Parameters
         ----------
         sorting : SortingExtractor
             The sorting extractor to be converted and saved.
         save_path : str
             Full path to the desired target folder.
         """
+        try:
+            import hybridizer.io as sbio
+            import hybridizer.probes as sbprb
+
+            HAVE_SBEX = True
+        except ImportError:
+            HAVE_SBEX = False
+
         assert HAVE_SBEX, SHYBRIDSortingExtractor.installation_mesg
         assert sorting.get_num_segments() == 1, "SHYBRID can only write single segment sortings"
         save_path = Path(save_path)
 
         dump = np.empty((0, 2))
 
         for unit_id in sorting.get_unit_ids():
             spikes = sorting.get_unit_spike_train(unit_id)[:, np.newaxis]
             expanded_id = (np.ones(spikes.size) * unit_id)[:, np.newaxis]
             tmp_concat = np.concatenate((expanded_id, spikes), axis=1)
 
             dump = np.concatenate((dump, tmp_concat), axis=0)
 
         save_path.mkdir(exist_ok=True, parents=True)
-        sorting_fn = save_path / 'initial_sorting.csv'
-        np.savetxt(sorting_fn, dump, delimiter=',', fmt='%i')
+        sorting_fn = save_path / "initial_sorting.csv"
+        np.savetxt(sorting_fn, dump, delimiter=",", fmt="%i")
 
 
 class SHYBRIDSortingSegment(BaseSortingSegment):
     def __init__(self, spike_clusters):
         self._spike_clusters = spike_clusters
         BaseSortingSegment.__init__(self)
 
-    def get_unit_spike_train(self,
-                             unit_id,
-                             start_frame,
-                             end_frame,
-                             ) -> np.ndarray:
+    def get_unit_spike_train(
+        self,
+        unit_id,
+        start_frame,
+        end_frame,
+    ) -> np.ndarray:
         # must be implemented in subclass
         if start_frame is None:
             start_frame = 0
         if end_frame is None:
             end_frame = np.inf
         train = self._spike_clusters[unit_id].get_actual_spike_train().spikes
         idxs = np.where((start_frame <= train) & (train < end_frame))
         return train[idxs]
 
 
-read_shybrid_recording = define_function_from_class(source_class=SHYBRIDRecordingExtractor, name="read_shybrid_recording")
+read_shybrid_recording = define_function_from_class(
+    source_class=SHYBRIDRecordingExtractor, name="read_shybrid_recording"
+)
 read_shybrid_sorting = define_function_from_class(source_class=SHYBRIDSortingExtractor, name="read_shybrid_sorting")
 
 
 class GeometryNotLoadedError(Exception):
     """Raised when the recording extractor has no associated channel locations."""
+
     pass
 
 
-params_template = \
-    """clusters:
+params_template = """clusters:
       csv: {initial_sorting_fn}
     data:
       dtype: {data_type}
       fs: {sampling_frequency}
       order: {byte_ordering}
       probe: {probe_fn}
     """
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/spykingcircusextractors.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/spykingcircusextractors.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,16 +1,17 @@
 from pathlib import Path
 
 import numpy as np
 
-from spikeinterface.core import (BaseSorting, BaseSortingSegment)
+from spikeinterface.core import BaseSorting, BaseSortingSegment
 from spikeinterface.core.core_tools import define_function_from_class
 
 try:
     import h5py
+
     HAVE_H5PY = True
 except ImportError:
     HAVE_H5PY = False
 
 
 class SpykingCircusSortingExtractor(BaseSorting):
     """Load SpykingCircus format data as a recording extractor.
@@ -22,74 +23,74 @@
 
     Returns
     -------
     extractor : SpykingCircusSortingExtractor
         Loaded data.
     """
 
-    extractor_name = 'SpykingCircusSortingExtractor'
+    extractor_name = "SpykingCircusSortingExtractor"
     installed = HAVE_H5PY  # check at class level if installed or not
-    mode = 'folder'
+    mode = "folder"
     installation_mesg = "To use the SpykingCircusSortingExtractor install h5py: \n\n pip install h5py\n\n"
     name = "spykingcircus"
 
     def __init__(self, folder_path):
         assert HAVE_H5PY, self.installation_mesg
 
         spykingcircus_folder = Path(folder_path)
         listfiles = spykingcircus_folder.iterdir()
 
         parent_folder = None
         result_folder = None
         for f in listfiles:
             if f.is_dir():
-                if any([f_.suffix == '.hdf5' for f_ in f.iterdir()]):
+                if any([f_.suffix == ".hdf5" for f_ in f.iterdir()]):
                     parent_folder = spykingcircus_folder
                     result_folder = f
 
         if parent_folder is None:
             parent_folder = spykingcircus_folder.parent
             for f in parent_folder.iterdir():
                 if f.is_dir():
-                    if any([f_.suffix == '.hdf5' for f_ in f.iterdir()]):
+                    if any([f_.suffix == ".hdf5" for f_ in f.iterdir()]):
                         result_folder = spykingcircus_folder
 
         assert isinstance(parent_folder, Path) and isinstance(result_folder, Path), "Not a valid spyking circus folder"
 
         # load files
         results = None
         for f in result_folder.iterdir():
-            if 'result.hdf5' in str(f):
+            if "result.hdf5" in str(f):
                 results = f
-            if 'result-merged.hdf5' in str(f):
+            if "result-merged.hdf5" in str(f):
                 results = f
                 break
         if results is None:
             raise Exception(spykingcircus_folder, " is not a spyking circus folder")
 
         # load params
         sample_rate = None
         for f in parent_folder.iterdir():
-            if f.suffix == '.params':
+            if f.suffix == ".params":
                 sample_rate = _load_sample_rate(f)
 
-        assert sample_rate is not None, 'sample rate not found'
+        assert sample_rate is not None, "sample rate not found"
 
-        with h5py.File(results, 'r') as f_results:
+        with h5py.File(results, "r") as f_results:
             spiketrains = []
             unit_ids = []
-            for temp in f_results['spiketimes'].keys():
-                spiketrains.append(np.array(f_results['spiketimes'][temp]).astype('int64'))
-                unit_ids.append(int(temp.split('_')[-1]))
+            for temp in f_results["spiketimes"].keys():
+                spiketrains.append(np.array(f_results["spiketimes"][temp]).astype("int64"))
+                unit_ids.append(int(temp.split("_")[-1]))
 
         BaseSorting.__init__(self, sample_rate, unit_ids)
         self.add_sorting_segment(SpykingcircustSortingSegment(unit_ids, spiketrains))
 
-        self._kwargs = {'folder_path': str(Path(folder_path).absolute())}
-        self.extra_requirements.append('h5py')
+        self._kwargs = {"folder_path": str(Path(folder_path).absolute())}
+        self.extra_requirements.append("h5py")
 
 
 class SpykingcircustSortingSegment(BaseSortingSegment):
     def __init__(self, unit_ids, spiketrains):
         BaseSortingSegment.__init__(self)
         self._unit_ids = list(unit_ids)
         self._spiketrains = spiketrains
@@ -102,18 +103,18 @@
         if end_frame is not None:
             times = times[times < end_frame]
         return times
 
 
 def _load_sample_rate(params_file):
     sample_rate = None
-    with params_file.open('r') as f:
+    with params_file.open("r") as f:
         for r in f.readlines():
-            if 'sampling_rate' in r:
-                sample_rate = r.split('=')[-1]
-                if '#' in sample_rate:
-                    sample_rate = sample_rate[:sample_rate.find('#')]
+            if "sampling_rate" in r:
+                sample_rate = r.split("=")[-1]
+                if "#" in sample_rate:
+                    sample_rate = sample_rate[: sample_rate.find("#")]
                 sample_rate = float(sample_rate)
     return sample_rate
 
 
 read_spykingcircus = define_function_from_class(source_class=SpykingCircusSortingExtractor, name="read_spykingcircus")
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/toy_example.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/toy_example.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,20 +1,30 @@
 import numpy as np
 
 from probeinterface import Probe
 
 from spikeinterface.core import NumpyRecording, NumpySorting, synthesize_random_firings
 
 
-def toy_example(duration=10, num_channels=4, num_units=10,
-                sampling_frequency=30000.0, num_segments=2,
-                average_peak_amplitude=-100, upsample_factor=13,
-                contact_spacing_um=40, num_columns=1,
-                spike_times=None, spike_labels=None,
-                score_detection=1, firing_rate=3., seed=None):
+def toy_example(
+    duration=10,
+    num_channels=4,
+    num_units=10,
+    sampling_frequency=30000.0,
+    num_segments=2,
+    average_peak_amplitude=-100,
+    upsample_factor=13,
+    contact_spacing_um=40,
+    num_columns=1,
+    spike_times=None,
+    spike_labels=None,
+    score_detection=1,
+    firing_rate=3.0,
+    seed=None,
+):
     """
     Creates a toy recording and sorting extractors.
 
     Parameters
     ----------
     duration: float (or list if multi segment)
         Duration in seconds (default 10).
@@ -61,85 +71,108 @@
         assert isinstance(spike_labels, list)
         assert len(spike_times) == len(spike_labels)
         assert len(spike_times) == num_segments
 
     assert num_channels > 0
     assert num_units > 0
 
-    waveforms, geometry = synthesize_random_waveforms(num_units=num_units, num_channels=num_channels,
-                                                      contact_spacing_um=contact_spacing_um, num_columns=num_columns,
-                                                      average_peak_amplitude=average_peak_amplitude,
-                                                      upsample_factor=upsample_factor, seed=seed)
+    waveforms, geometry = synthesize_random_waveforms(
+        num_units=num_units,
+        num_channels=num_channels,
+        contact_spacing_um=contact_spacing_um,
+        num_columns=num_columns,
+        average_peak_amplitude=average_peak_amplitude,
+        upsample_factor=upsample_factor,
+        seed=seed,
+    )
 
-    unit_ids = np.arange(num_units, dtype='int64')
+    unit_ids = np.arange(num_units, dtype="int64")
 
     traces_list = []
     times_list = []
     labels_list = []
     for segment_index in range(num_segments):
         if spike_times is None:
-            times, labels = synthesize_random_firings(num_units=num_units, duration=durations[segment_index],
-                                                  sampling_frequency=sampling_frequency, firing_rates=firing_rate, seed=seed)
+            times, labels = synthesize_random_firings(
+                num_units=num_units,
+                duration=durations[segment_index],
+                sampling_frequency=sampling_frequency,
+                firing_rates=firing_rate,
+                seed=seed,
+            )
         else:
             times = spike_times[segment_index]
             labels = spike_labels[segment_index]
 
-        traces = synthesize_timeseries(times, labels, unit_ids, waveforms, sampling_frequency, durations[segment_index],
-                                        noise_level=10, waveform_upsample_factor=upsample_factor, seed=seed)
+        traces = synthesize_timeseries(
+            times,
+            labels,
+            unit_ids,
+            waveforms,
+            sampling_frequency,
+            durations[segment_index],
+            noise_level=10,
+            waveform_upsample_factor=upsample_factor,
+            seed=seed,
+        )
 
-        amp_index= np.sort(np.argsort(np.max(np.abs(traces[times-10, :]), 1))[:int(score_detection*len(times))])
-        times_list.append(times[amp_index]) # Keep only a certain percentage of detected spike for sorting
+        amp_index = np.sort(np.argsort(np.max(np.abs(traces[times - 10, :]), 1))[: int(score_detection * len(times))])
+        times_list.append(times[amp_index])  # Keep only a certain percentage of detected spike for sorting
         labels_list.append(labels[amp_index])
         traces_list.append(traces)
 
     sorting = NumpySorting.from_times_labels(times_list, labels_list, sampling_frequency)
 
     recording = NumpyRecording(traces_list, sampling_frequency)
     recording.annotate(is_filtered=True)
 
     probe = Probe(ndim=2)
-    probe.set_contacts(positions=geometry,
-                       shapes='circle', shape_params={'radius': 5})
-    probe.create_auto_shape(probe_type='rect', margin=20)
-    probe.set_device_channel_indices(np.arange(num_channels, dtype='int64'))
+    probe.set_contacts(positions=geometry, shapes="circle", shape_params={"radius": 5})
+    probe.create_auto_shape(probe_type="rect", margin=20)
+    probe.set_device_channel_indices(np.arange(num_channels, dtype="int64"))
     recording = recording.set_probe(probe)
 
     return recording, sorting
 
 
-
-
-
-def synthesize_random_waveforms(num_channels=5, num_units=20, width=500,
-                                upsample_factor=13, timeshift_factor=0, average_peak_amplitude=-10,
-                                contact_spacing_um=40, num_columns=1, seed=None):
+def synthesize_random_waveforms(
+    num_channels=5,
+    num_units=20,
+    width=500,
+    upsample_factor=13,
+    timeshift_factor=0,
+    average_peak_amplitude=-10,
+    contact_spacing_um=40,
+    num_columns=1,
+    seed=None,
+):
     if seed is not None:
         np.random.seed(seed)
         seeds = np.random.RandomState(seed=seed).randint(0, 2147483647, num_units)
     else:
         seeds = np.random.randint(0, 2147483647, num_units)
 
     avg_durations = [200, 10, 30, 200]
     avg_amps = [0.5, 10, -1, 0]
     rand_durations_stdev = [10, 4, 6, 20]
     rand_amps_stdev = [0.2, 3, 0.5, 0]
     rand_amp_factor_range = [0.5, 1]
     geom_spread_coef1 = 1
-    geom_spread_coef2 =  0.1
+    geom_spread_coef2 = 0.1
 
     geometry = np.zeros((num_channels, 2))
     if num_columns == 1:
         geometry[:, 1] = np.arange(num_channels) * contact_spacing_um
     else:
-        assert num_channels % num_columns == 0, 'Invalid num_columns'
+        assert num_channels % num_columns == 0, "Invalid num_columns"
         num_contact_per_column = num_channels // num_columns
         j = 0
         for i in range(num_columns):
-            geometry[j:j+num_contact_per_column, 0] = i * contact_spacing_um
-            geometry[j:j+num_contact_per_column, 1] = np.arange(num_contact_per_column) * contact_spacing_um
+            geometry[j : j + num_contact_per_column, 0] = i * contact_spacing_um
+            geometry[j : j + num_contact_per_column, 1] = np.arange(num_contact_per_column) * contact_spacing_um
             j += num_contact_per_column
 
     avg_durations = np.array(avg_durations)
     avg_amps = np.array(avg_amps)
     rand_durations_stdev = np.array(rand_durations_stdev)
     rand_amps_stdev = np.array(rand_amps_stdev)
     rand_amp_factor_range = np.array(rand_amp_factor_range)
@@ -150,35 +183,40 @@
 
     ## The waveforms_out
     WW = np.zeros((num_channels, width * upsample_factor, num_units))
 
     for i, k in enumerate(range(num_units)):
         for m in range(num_channels):
             diff = neuron_locations[k, :] - geometry[m, :]
-            dist = np.sqrt(np.sum(diff ** 2))
-            durations0 = np.maximum(np.ones(avg_durations.shape),
-                                    avg_durations + np.random.RandomState(seed=seeds[i]).randn(1,
-                                                                                               4) * rand_durations_stdev) * upsample_factor
+            dist = np.sqrt(np.sum(diff**2))
+            durations0 = (
+                np.maximum(
+                    np.ones(avg_durations.shape),
+                    avg_durations + np.random.RandomState(seed=seeds[i]).randn(1, 4) * rand_durations_stdev,
+                )
+                * upsample_factor
+            )
             amps0 = avg_amps + np.random.RandomState(seed=seeds[i]).randn(1, 4) * rand_amps_stdev
             waveform0 = synthesize_single_waveform(full_width, durations0, amps0)
             waveform0 = np.roll(waveform0, int(timeshift_factor * dist * upsample_factor))
-            waveform0 = waveform0 * np.random.RandomState(seed=seeds[i]).uniform(rand_amp_factor_range[0],
-                                                                                 rand_amp_factor_range[1])
-            factor = (geom_spread_coef1 + dist * geom_spread_coef2)
+            waveform0 = waveform0 * np.random.RandomState(seed=seeds[i]).uniform(
+                rand_amp_factor_range[0], rand_amp_factor_range[1]
+            )
+            factor = geom_spread_coef1 + dist * geom_spread_coef2
             WW[m, :, k] = waveform0 / factor
 
     peaks = np.max(np.abs(WW), axis=(0, 1))
     WW = WW / np.mean(peaks) * average_peak_amplitude
 
     return WW, geometry
 
 
 def get_default_neuron_locations(num_channels, num_units, geometry):
     num_dims = geometry.shape[1]
-    neuron_locations = np.zeros((num_units, num_dims), dtype='float64')
+    neuron_locations = np.zeros((num_units, num_dims), dtype="float64")
 
     for k in range(num_units):
         ind = k / (num_units - 1) * (num_channels - 1) + 1
         ind0 = int(ind)
 
         if ind0 == num_channels:
             ind0 = num_channels - 1
@@ -192,15 +230,15 @@
 
 def exp_growth(amp1, amp2, dur1, dur2):
     t = np.arange(0, dur1)
     Y = np.exp(t / dur2)
     # Want Y[0]=amp1
     # Want Y[-1]=amp2
     Y = Y / (Y[-1] - Y[0]) * (amp2 - amp1)
-    Y = Y - Y[0] + amp1;
+    Y = Y - Y[0] + amp1
     return Y
 
 
 def exp_decay(amp1, amp2, dur1, dur2):
     Y = exp_growth(amp2, amp1, dur1, dur2)
     Y = np.flipud(Y)
     return Y
@@ -211,42 +249,53 @@
     for j in range(-t, t + 1):
         Z = Z + np.roll(Y, j)
     return Z
 
 
 def synthesize_single_waveform(full_width, durations, amps):
     durations = np.array(durations).ravel()
-    if (np.sum(durations) >= full_width - 2):
-        durations[-1] = full_width - 2 - np.sum(durations[0:durations.size - 1])
+    if np.sum(durations) >= full_width - 2:
+        durations[-1] = full_width - 2 - np.sum(durations[0 : durations.size - 1])
 
     amps = np.array(amps).ravel()
 
-    timepoints = np.round(np.hstack((0, np.cumsum(durations) - 1))).astype('int');
+    timepoints = np.round(np.hstack((0, np.cumsum(durations) - 1))).astype("int")
 
-    t = np.r_[0:np.sum(durations) + 1]
+    t = np.r_[0 : np.sum(durations) + 1]
 
     Y = np.zeros(len(t))
-    Y[timepoints[0]:timepoints[1] + 1] = exp_growth(0, amps[0], timepoints[1] + 1 - timepoints[0], durations[0] / 4)
-    Y[timepoints[1]:timepoints[2] + 1] = exp_growth(amps[0], amps[1], timepoints[2] + 1 - timepoints[1], durations[1])
-    Y[timepoints[2]:timepoints[3] + 1] = exp_decay(amps[1], amps[2], timepoints[3] + 1 - timepoints[2],
-                                                   durations[2] / 4)
-    Y[timepoints[3]:timepoints[4] + 1] = exp_decay(amps[2], amps[3], timepoints[4] + 1 - timepoints[3],
-                                                   durations[3] / 5)
+    Y[timepoints[0] : timepoints[1] + 1] = exp_growth(0, amps[0], timepoints[1] + 1 - timepoints[0], durations[0] / 4)
+    Y[timepoints[1] : timepoints[2] + 1] = exp_growth(amps[0], amps[1], timepoints[2] + 1 - timepoints[1], durations[1])
+    Y[timepoints[2] : timepoints[3] + 1] = exp_decay(
+        amps[1], amps[2], timepoints[3] + 1 - timepoints[2], durations[2] / 4
+    )
+    Y[timepoints[3] : timepoints[4] + 1] = exp_decay(
+        amps[2], amps[3], timepoints[4] + 1 - timepoints[3], durations[3] / 5
+    )
     Y = smooth_it(Y, 3)
     Y = Y - np.linspace(Y[0], Y[-1], len(t))
     Y = np.hstack((Y, np.zeros(full_width - len(t))))
     Nmid = int(np.floor(full_width / 2))
     peakind = np.argmax(np.abs(Y))
     Y = np.roll(Y, Nmid - peakind)
 
     return Y
 
 
-def synthesize_timeseries(spike_times, spike_labels, unit_ids, waveforms, sampling_frequency, duration,
-                          noise_level=10, waveform_upsample_factor=13, seed=None):
+def synthesize_timeseries(
+    spike_times,
+    spike_labels,
+    unit_ids,
+    waveforms,
+    sampling_frequency,
+    duration,
+    noise_level=10,
+    waveform_upsample_factor=13,
+    seed=None,
+):
     num_samples = np.int64(sampling_frequency * duration)
     waveform_upsample_factor = int(waveform_upsample_factor)
     W = waveforms
 
     num_channels, full_width, num_units = W.shape[0], W.shape[1], W.shape[2]
     width = int(full_width / waveform_upsample_factor)
     half_width = int(np.ceil((width + 1) / 2 - 1))
@@ -264,20 +313,16 @@
             amp0 = 1
             frac_offset = int(np.floor((t0 - np.floor(t0)) * waveform_upsample_factor))
             # note for later this frac_offset is supposed to mimic jitter but
             # is always 0 : TODO improve this
             i_start = np.int64(np.floor(t0)) - half_width
             if (0 <= i_start) and (i_start + width <= num_samples):
                 wf = waveform0[:, frac_offset::waveform_upsample_factor] * amp0
-                traces[i_start:i_start + width, :] += wf.T
+                traces[i_start : i_start + width, :] += wf.T
 
     return traces
 
 
-
-
-
-
-if __name__ == '__main__':
+if __name__ == "__main__":
     rec, sorting = toy_example(num_segments=2)
     print(rec)
     print(sorting)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/tridesclousextractors.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/tridesclousextractors.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,18 +1,12 @@
 from pathlib import Path
 
-from spikeinterface.core import (BaseSorting, BaseSortingSegment)
+from spikeinterface.core import BaseSorting, BaseSortingSegment
 from spikeinterface.core.core_tools import define_function_from_class
 
-try:
-    import tridesclous as tdc
-    HAVE_TDC = True
-except ImportError:
-    HAVE_TDC = False
-
 
 class TridesclousSortingExtractor(BaseSorting):
     """Load Tridesclous format data as a sorting extractor.
 
     Parameters
     ----------
     folder_path : str or Path
@@ -22,58 +16,62 @@
 
     Returns
     -------
     extractor : TridesclousSortingExtractor
         Loaded data.
     """
 
-    extractor_name = 'TridesclousSortingExtractor'
-    installed = HAVE_TDC
-    mode = 'folder'
+    extractor_name = "TridesclousSortingExtractor"
+    mode = "folder"
     installation_mesg = "To use the TridesclousSortingExtractor install tridesclous: \n\n pip install tridesclous\n\n"  # error message when not installed
     name = "tridesclous"
 
     def __init__(self, folder_path, chan_grp=None):
+        try:
+            import tridesclous as tdc
+        except ImportError:
+            raise ImportError(self.installation_mesg)
+
         assert self.installed, self.installation_mesg
         tdc_folder = Path(folder_path)
 
         dataio = tdc.DataIO(str(tdc_folder))
         if chan_grp is None:
             # if chan_grp is not provided, take the first one if unique
             chan_grps = list(dataio.channel_groups.keys())
-            assert len(chan_grps) == 1, 'There are several groups in the folder, specify chan_grp=...'
+            assert len(chan_grps) == 1, "There are several groups in the folder, specify chan_grp=..."
             chan_grp = chan_grps[0]
 
-        catalogue = dataio.load_catalogue(name='initial', chan_grp=chan_grp)
+        catalogue = dataio.load_catalogue(name="initial", chan_grp=chan_grp)
 
-        labels = catalogue['clusters']['cluster_label']
+        labels = catalogue["clusters"]["cluster_label"]
         labels = labels[labels >= 0]
         unit_ids = list(labels)
 
         sampling_frequency = dataio.sample_rate
 
         BaseSorting.__init__(self, sampling_frequency, unit_ids)
         for seg_num in range(dataio.nb_segment):
             # load all spike in memory (this avoid to lock the folder with memmap throug dataio
             all_spikes = dataio.get_spikes(seg_num=seg_num, chan_grp=chan_grp, i_start=None, i_stop=None).copy()
             self.add_sorting_segment(TridesclousSortingSegment(all_spikes))
 
-        self._kwargs = {'folder_path': str(Path(folder_path).absolute()), 'chan_grp': chan_grp}
-        self.extra_requirements.append('tridesclous')
+        self._kwargs = {"folder_path": str(Path(folder_path).absolute()), "chan_grp": chan_grp}
+        self.extra_requirements.append("tridesclous")
 
 
 class TridesclousSortingSegment(BaseSortingSegment):
     def __init__(self, all_spikes):
         BaseSortingSegment.__init__(self)
         self._all_spikes = all_spikes
 
     def get_unit_spike_train(self, unit_id, start_frame, end_frame):
         spikes = self._all_spikes
-        spikes = spikes[spikes['cluster_label'] == unit_id]
-        spike_times = spikes['index']
+        spikes = spikes[spikes["cluster_label"] == unit_id]
+        spike_times = spikes["index"]
         if start_frame is not None:
             spike_times = spike_times[spike_times >= start_frame]
         if end_frame is not None:
             spike_times = spike_times[spike_times < end_frame]
         return spike_times.copy()
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/waveclussnippetstextractors.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/waveclussnippetstextractors.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,31 +1,30 @@
 from pathlib import Path
 import numpy as np
 
-from spikeinterface.core import (BaseSnippets, BaseSnippetsSegment)
+from spikeinterface.core import BaseSnippets, BaseSnippetsSegment
 from spikeinterface.core.core_tools import define_function_from_class
 from .matlabhelpers import MatlabHelper
 from typing import List, Union
 
 
 class WaveClusSnippetsExtractor(MatlabHelper, BaseSnippets):
     extractor_name = "WaveClusSnippetsExtractor"
-    is_writable = True
     name = "waveclus"
 
     def __init__(self, file_path):
         file_path = Path(file_path) if isinstance(file_path, str) else file_path
         MatlabHelper.__init__(self, file_path)
         wc_snippets = self._getfield("spikes")
         # handle both types of waveclus results
 
         # the spikes can be in the times_file
-        if file_path.name.startswith('times_'):
+        if file_path.name.startswith("times_"):
             times = self._getfield("cluster_class")[:, 1]
-        elif file_path.name.endswith('_spikes.mat'):
+        elif file_path.name.endswith("_spikes.mat"):
             times = np.ravel(self._getfield("index"))
         else:
             raise ("Filename not compatible with waveclus file.")
 
         sampling_frequency = float(self._getfield("par/sr"))
         pre = int(self._getfield("par/w_pre")) - 1
         post = int(self._getfield("par/w_post")) + 1
@@ -33,54 +32,70 @@
         sp_len = pre + post
 
         nchannels = int(wc_snippets.shape[1] / sp_len)
         # waveclus use: #snippets,#concatenated_samples(sample * nchannels)
         # spikeinterface use: #snippets,#samples, #nchannels
         snippets = np.dstack(np.array_split(wc_snippets, nchannels, 1))
 
-        BaseSnippets.__init__(self, sampling_frequency=sampling_frequency,
-                              nbefore=pre, snippet_len=sp_len, dtype=wc_snippets.dtype,
-                              channel_ids=np.arange(nchannels))
+        BaseSnippets.__init__(
+            self,
+            sampling_frequency=sampling_frequency,
+            nbefore=pre,
+            snippet_len=sp_len,
+            dtype=wc_snippets.dtype,
+            channel_ids=np.arange(nchannels),
+        )
 
         snp_segment = WaveClustSnippetsSegment(
-            snippets=snippets, spikesframes=np.round(times*(sampling_frequency/1000)))
+            snippets=snippets, spikesframes=np.round(times * (sampling_frequency / 1000))
+        )
         self.add_snippets_segment(snp_segment)
 
-        self._kwargs = {'file_path': str(Path(file_path).absolute())}
+        self._kwargs = {"file_path": str(Path(file_path).absolute())}
 
     @staticmethod
     def write_snippets(snippets_extractor, save_file_path):
-        assert snippets_extractor.is_aligned(), 'Waveclus requires aligned spikes'
+        assert snippets_extractor.is_aligned(), "Waveclus requires aligned spikes"
         save_file_path = Path(save_file_path)
-        assert save_file_path.name.endswith('_spikes.mat'), 'Waveclus snippets files should end with _spikes.mat'
-        frame_to_ms = (snippets_extractor.get_sampling_frequency()/1000)
-        index = np.concatenate([snippets_extractor.get_frames(segment_index=sinx) * frame_to_ms
-                                for sinx in range(snippets_extractor.get_num_segments())])
-        spikes = np.concatenate([snippets_extractor.get_snippets(segment_index=sinx) * frame_to_ms
-                                for sinx in range(snippets_extractor.get_num_segments())])
-        spikes = np.swapaxes(spikes, 1, 2).reshape([spikes.shape[0], spikes.shape[1]*spikes.shape[2]], order='C')
-        par = dict(sr=snippets_extractor.get_sampling_frequency(),
-                   w_pre=snippets_extractor.nbefore+1,  # waveclus includes the peak in the pre samples
-                   w_post=snippets_extractor.nafter-1)
-        MatlabHelper.write_dict_to_mat(mat_file_path=save_file_path,
-                                       dict_to_write={'index': index,
-                                                      'spikes': spikes,
-                                                      'par': par})
+        assert save_file_path.name.endswith("_spikes.mat"), "Waveclus snippets files should end with _spikes.mat"
+        frame_to_ms = snippets_extractor.get_sampling_frequency() / 1000
+        index = np.concatenate(
+            [
+                snippets_extractor.get_frames(segment_index=sinx) * frame_to_ms
+                for sinx in range(snippets_extractor.get_num_segments())
+            ]
+        )
+        spikes = np.concatenate(
+            [
+                snippets_extractor.get_snippets(segment_index=sinx) * frame_to_ms
+                for sinx in range(snippets_extractor.get_num_segments())
+            ]
+        )
+        spikes = np.swapaxes(spikes, 1, 2).reshape([spikes.shape[0], spikes.shape[1] * spikes.shape[2]], order="C")
+        par = dict(
+            sr=snippets_extractor.get_sampling_frequency(),
+            w_pre=snippets_extractor.nbefore + 1,  # waveclus includes the peak in the pre samples
+            w_post=snippets_extractor.nafter - 1,
+        )
+        MatlabHelper.write_dict_to_mat(
+            mat_file_path=save_file_path, dict_to_write={"index": index, "spikes": spikes, "par": par}
+        )
 
 
 class WaveClustSnippetsSegment(BaseSnippetsSegment):
     def __init__(self, snippets, spikesframes):
         BaseSnippetsSegment.__init__(self)
         self._snippets = snippets
         self._spikestimes = spikesframes
 
-    def get_snippets(self,
-                     indices,
-                     channel_indices: Union[List, None] = None,
-                     ) -> np.ndarray:
+    def get_snippets(
+        self,
+        indices,
+        channel_indices: Union[List, None] = None,
+    ) -> np.ndarray:
         """
         Return the snippets, optionally for a subset of samples and/or channels
 
         Parameters
         ----------
         indexes: (Union[int, None], optional)
             start sample index, or zero if None. Defaults to None.
@@ -97,17 +112,15 @@
         if indices is None:
             return self._snippets[:, :, channel_indices]
         return self._snippets[indices, :, channel_indices]
 
     def get_num_snippets(self):
         return self._spikestimes.shape[0]
 
-    def frames_to_indices(self,
-                          start_frame: Union[int, None] = None,
-                          end_frame: Union[int, None] = None):
+    def frames_to_indices(self, start_frame: Union[int, None] = None, end_frame: Union[int, None] = None):
         """
         Return the slice of snippets
 
         Parameters
         ----------
         start_frame: (Union[int, None], optional)
             start sample index, or zero if None. Defaults to None.
@@ -119,26 +132,28 @@
         snippets: slice
             slice of selected snippets
         """
         # must be implemented in subclass
         if start_frame is None:
             init = 0
         else:
-            init = np.searchsorted(self._spikestimes, start_frame, side='left')
+            init = np.searchsorted(self._spikestimes, start_frame, side="left")
         if end_frame is None:
             endi = self._spikestimes.shape[0]
         else:
-            endi = np.searchsorted(self._spikestimes, end_frame, side='left')
+            endi = np.searchsorted(self._spikestimes, end_frame, side="left")
         return slice(init, endi, 1)
 
     def get_frames(self, indices=None):
         """Returns the frames of the snippets in this segment
 
         Returns:
             SampleIndex: Number of samples in the segment
         """
         if indices is None:
             return self._spikestimes
         return self._spikestimes[indices]
 
-read_waveclus_snippets = define_function_from_class(source_class=WaveClusSnippetsExtractor,
-                                                    name="read_waveclus_snippets")
+
+read_waveclus_snippets = define_function_from_class(
+    source_class=WaveClusSnippetsExtractor, name="read_waveclus_snippets"
+)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/waveclustextractors.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/waveclustextractors.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 from pathlib import Path
 
 import numpy as np
 
-from spikeinterface.core import (BaseSorting, BaseSortingSegment)
+from spikeinterface.core import BaseSorting, BaseSortingSegment
 from spikeinterface.core.core_tools import define_function_from_class
 from .matlabhelpers import MatlabHelper
 
 
 class WaveClusSortingExtractor(MatlabHelper, BaseSorting):
     """Load WaveClus format data as a sorting extractor.
 
@@ -29,27 +29,27 @@
     def __init__(self, file_path, keep_good_only=True):
         MatlabHelper.__init__(self, file_path)
 
         cluster_classes = self._getfield("cluster_class")
         classes = cluster_classes[:, 0]
         spike_times = cluster_classes[:, 1]
         sampling_frequency = float(self._getfield("par/sr"))
-        unit_ids = np.unique(classes).astype('int')
+        unit_ids = np.unique(classes).astype("int")
         if keep_good_only:
             unit_ids = unit_ids[unit_ids > 0]
         spiketrains = {}
         for unit_id in unit_ids:
-            mask = (classes == unit_id)
+            mask = classes == unit_id
             spiketrains[unit_id] = np.rint(spike_times[mask] * (sampling_frequency / 1000))
 
         BaseSorting.__init__(self, sampling_frequency, unit_ids)
 
         self.add_sorting_segment(WaveClustSortingSegment(unit_ids, spiketrains))
-        self.set_property('unsorted', np.array([c == 0 for c in unit_ids]))
-        self._kwargs = {'file_path': str(Path(file_path).absolute()), 'keep_good_only':keep_good_only}
+        self.set_property("unsorted", np.array([c == 0 for c in unit_ids]))
+        self._kwargs = {"file_path": str(Path(file_path).absolute()), "keep_good_only": keep_good_only}
 
 
 class WaveClustSortingSegment(BaseSortingSegment):
     def __init__(self, unit_ids, spiketrains):
         BaseSortingSegment.__init__(self)
         self._unit_ids = list(unit_ids)
         self._spiketrains = spiketrains
```

### Comparing `spikeinterface-0.97.1/spikeinterface/extractors/yassextractors.py` & `spikeinterface-0.98.0/src/spikeinterface/extractors/yassextractors.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,16 +1,17 @@
 from pathlib import Path
 
 import numpy as np
 
-from spikeinterface.core import (BaseSorting, BaseSortingSegment)
+from spikeinterface.core import BaseSorting, BaseSortingSegment
 from spikeinterface.core.core_tools import define_function_from_class
 
 try:
     import yaml
+
     HAVE_YAML = True
 except:
     HAVE_YAML = False
 
 
 class YassSortingExtractor(BaseSorting):
     """Load YASS format data as a sorting extractor.
@@ -22,43 +23,45 @@
 
     Returns
     -------
     extractor : YassSortingExtractor
         Loaded data.
     """
 
-    extractor_name = 'YassExtractor'
-    mode = 'folder'
+    extractor_name = "YassExtractor"
+    mode = "folder"
     installed = HAVE_YAML  # check at class level if installed or not
-    installation_mesg = "To use the Yass extractor, install pyyaml: \n\n pip install pyyaml\n\n"  # error message when not installed
+    installation_mesg = (
+        "To use the Yass extractor, install pyyaml: \n\n pip install pyyaml\n\n"  # error message when not installed
+    )
     name = "yass"
 
     def __init__(self, folder_path):
         assert HAVE_YAML, self.installation_mesg
 
         folder_path = Path(folder_path)
 
-        self.fname_spike_train = folder_path / 'tmp' / 'output' / 'spike_train.npy'
-        self.fname_templates = folder_path / 'tmp' / 'output' / 'templates' / 'templates_0sec.npy'
-        self.fname_config = folder_path / 'config.yaml'
+        self.fname_spike_train = folder_path / "tmp" / "output" / "spike_train.npy"
+        self.fname_templates = folder_path / "tmp" / "output" / "templates" / "templates_0sec.npy"
+        self.fname_config = folder_path / "config.yaml"
 
         # Read CONFIG File
-        with open(self.fname_config, 'r') as stream:
+        with open(self.fname_config, "r") as stream:
             self.config = yaml.safe_load(stream)
 
         spiketrains = np.load(self.fname_spike_train)
         unit_ids = np.unique(spiketrains[:, 1])
 
         # initialize
-        sampling_frequency = self.config['recordings']['sampling_rate']
+        sampling_frequency = self.config["recordings"]["sampling_rate"]
         BaseSorting.__init__(self, sampling_frequency, unit_ids)
         self.add_sorting_segment(YassSortingSegment(spiketrains))
 
-        self._kwargs = {'folder_path': str(folder_path)}
-        self.extra_requirements.append('pyyaml')
+        self._kwargs = {"folder_path": str(folder_path)}
+        self.extra_requirements.append("pyyaml")
 
 
 class YassSortingSegment(BaseSortingSegment):
     def __init__(self, spiketrains):
         BaseSortingSegment.__init__(self)
         # spiketrains is a 2 columns
         self._spiketrains = spiketrains
```

### Comparing `spikeinterface-0.97.1/spikeinterface/full.py` & `spikeinterface-0.98.0/src/spikeinterface/full.py`

 * *Files 1% similar despite different names*

```diff
@@ -6,14 +6,15 @@
 # this imports the core only
 import spikeinterface as si
 
 # this imports everything in a flat module
 import spieinterface.full as si
 """
 import importlib.metadata
+
 __version__ = importlib.metadata.version("spikeinterface")
 
 from .core import *
 from .extractors import *
 from .sorters import *
 from .preprocessing import *
 from .postprocessing import *
```

### Comparing `spikeinterface-0.97.1/spikeinterface/postprocessing/__init__.py` & `spikeinterface-0.98.0/src/spikeinterface/postprocessing/__init__.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,39 +1,60 @@
-
 # This is kept in 0.97.0 and then will be removed
 from .template_tools import (
     get_template_amplitudes,
     get_template_extremum_channel,
     get_template_extremum_channel_peak_shift,
     get_template_extremum_amplitude,
-    get_template_channel_sparsity
+    get_template_channel_sparsity,
 )
 
-from .template_metrics import (TemplateMetricsCalculator, compute_template_metrics,
-                               calculate_template_metrics, get_template_metric_names)
+from .template_metrics import (
+    TemplateMetricsCalculator,
+    compute_template_metrics,
+    calculate_template_metrics,
+    get_template_metric_names,
+)
 
-from .template_similarity import (TemplateSimilarityCalculator,
-                                  compute_template_similarity,
-                                  check_equal_template_with_distribution_overlap)
+from .template_similarity import (
+    TemplateSimilarityCalculator,
+    compute_template_similarity,
+    check_equal_template_with_distribution_overlap,
+)
 
-from .principal_component import WaveformPrincipalComponent, compute_principal_components
+from .principal_component import (
+    WaveformPrincipalComponent,
+    compute_principal_components,
+)
 
 from .spike_amplitudes import compute_spike_amplitudes, SpikeAmplitudesCalculator
 
-from .correlograms import (CorrelogramsCalculator,
-                           compute_autocorrelogram_from_spiketrain,
-                           compute_crosscorrelogram_from_spiketrain,
-                           compute_correlograms, correlogram_for_one_segment,
-                           compute_correlograms_numba, compute_correlograms_numpy)
-
-from .isi import (ISIHistogramsCalculator,
-                  compute_isi_histograms_from_spiketrain, compute_isi_histograms,
-                  compute_isi_histograms_numpy, compute_isi_histograms_numba)
+from .correlograms import (
+    CorrelogramsCalculator,
+    compute_autocorrelogram_from_spiketrain,
+    compute_crosscorrelogram_from_spiketrain,
+    compute_correlograms,
+    correlogram_for_one_segment,
+    compute_correlograms_numba,
+    compute_correlograms_numpy,
+)
+
+from .isi import (
+    ISIHistogramsCalculator,
+    compute_isi_histograms_from_spiketrain,
+    compute_isi_histograms,
+    compute_isi_histograms_numpy,
+    compute_isi_histograms_numba,
+)
 
 from .spike_locations import compute_spike_locations, SpikeLocationsCalculator
 
-from .unit_localization import (compute_unit_locations, UnitLocationsCalculator,
-                                compute_center_of_mass)
+from .unit_localization import (
+    compute_unit_locations,
+    UnitLocationsCalculator,
+    compute_center_of_mass,
+)
+
+from .amplitude_scalings import compute_amplitude_scalings, AmplitudeScalingsCalculator
 
 from .alignsorting import align_sorting, AlignSortingExtractor
 
-from .noise_level import compute_noise_levels, NoiseLevelsCalculator
+from .noise_level import compute_noise_levels, NoiseLevelsCalculator
```

### Comparing `spikeinterface-0.97.1/spikeinterface/postprocessing/alignsorting.py` & `spikeinterface-0.98.0/src/spikeinterface/postprocessing/alignsorting.py`

 * *Files 9% similar despite different names*

```diff
@@ -6,57 +6,52 @@
 from spikeinterface.core.template_tools import get_template_extremum_channel_peak_shift
 
 
 class AlignSortingExtractor(BaseSorting):
     """
     Class to shift a unit (generally to align the template on the peak) given
     the shifts for each unit.
-    
+
     Parameters
     ----------
     sorting: BaseSorting
         The sorting to align.
     unit_peak_shifts: dict
         Dictionary mapping the unit_id to the unit's shift (in number of samples).
         A positive shift means the spike train is shifted back in time, while
         a negative shift means the spike train is shifted forward.
-    
+
     Returns
     -------
     aligned_sorting: AlignSortingExtractor
         The aligned sorting.
     """
-    
-    is_dumpable = False
-    
+
     def __init__(self, sorting, unit_peak_shifts):
         super().__init__(sorting.get_sampling_frequency(), sorting.unit_ids)
-        
+
         for segment in sorting._sorting_segments:
             self.add_sorting_segment(AlignSortingSegment(segment, unit_peak_shifts))
 
         sorting.copy_metadata(self, only_main=False)
         if sorting.has_recording():
             self.register_recording(sorting._recording)
 
-        self._kwargs = {
-            'sorting': sorting.to_dict(),
-            'unit_peak_shifts': unit_peak_shifts
-        }
+        self._kwargs = {"sorting": sorting, "unit_peak_shifts": unit_peak_shifts}
 
 
 class AlignSortingSegment(BaseSortingSegment):
     def __init__(self, parent_segment, unit_peak_shifts):
         super().__init__()
         self._parent_segment = parent_segment
         self._unit_peak_shifts = unit_peak_shifts
-        
+
     def get_unit_spike_train(self, unit_id, start_frame, end_frame):
         if start_frame is not None:
             start_frame = start_frame + self._unit_peak_shifts[unit_id]
         if end_frame is not None:
             end_frame = end_frame + self._unit_peak_shifts[unit_id]
-        original_spike_train =  self._parent_segment.get_unit_spike_train(unit_id, start_frame, end_frame)
+        original_spike_train = self._parent_segment.get_unit_spike_train(unit_id, start_frame, end_frame)
         return original_spike_train - self._unit_peak_shifts[unit_id]
 
 
 align_sorting = define_function_from_class(source_class=AlignSortingExtractor, name="align_sorting")
```

### Comparing `spikeinterface-0.97.1/spikeinterface/postprocessing/correlograms.py` & `spikeinterface-0.98.0/src/spikeinterface/postprocessing/correlograms.py`

 * *Files 3% similar despite different names*

```diff
@@ -2,92 +2,87 @@
 import warnings
 import numpy as np
 from ..core import WaveformExtractor
 from ..core.waveform_extractor import BaseWaveformExtractorExtension
 
 try:
     import numba
+
     HAVE_NUMBA = True
 except ModuleNotFoundError as err:
     HAVE_NUMBA = False
-    
+
 
 class CorrelogramsCalculator(BaseWaveformExtractorExtension):
     """Compute correlograms of spike trains.
-    
+
     Parameters
     ----------
     waveform_extractor: WaveformExtractor
         A waveform extractor object
     """
-    extension_name = 'correlograms'
+
+    extension_name = "correlograms"
 
     def __init__(self, waveform_extractor):
         BaseWaveformExtractorExtension.__init__(self, waveform_extractor)
 
-    def _set_params(self, window_ms: float = 100.0,
-                    bin_ms: float = 5.0, method: str = "auto"):
-
-        params = dict(window_ms=window_ms, bin_ms=bin_ms, 
-                      method=method)
+    def _set_params(self, window_ms: float = 100.0, bin_ms: float = 5.0, method: str = "auto"):
+        params = dict(window_ms=window_ms, bin_ms=bin_ms, method=method)
 
         return params
 
     def _select_extension_data(self, unit_ids):
         # filter metrics dataframe
         unit_indices = self.waveform_extractor.sorting.ids_to_indices(unit_ids)
-        new_ccgs = self._extension_data['ccgs'][unit_indices][:, unit_indices]
-        new_bins = self._extension_data['bins']
+        new_ccgs = self._extension_data["ccgs"][unit_indices][:, unit_indices]
+        new_bins = self._extension_data["bins"]
         new_extension_data = dict(ccgs=new_ccgs, bins=new_bins)
         return new_extension_data
-        
+
     def _run(self):
         ccgs, bins = _compute_correlograms(self.waveform_extractor.sorting, **self._params)
-        self._extension_data['ccgs'] = ccgs
-        self._extension_data['bins'] = bins
+        self._extension_data["ccgs"] = ccgs
+        self._extension_data["bins"] = bins
 
     def get_data(self):
         """
         Get the computed ISI histograms.
-        
+
         Returns
         -------
         isi_histograms : np.array
             2D array with ISI histograms (num_units, num_bins)
         bins : np.array
             1D array with bins in ms
         """
         msg = "Crosscorrelograms are not computed. Use the 'run()' function."
-        assert self._extension_data['ccgs'] is not None and \
-            self._extension_data['bins'] is not None, msg
-        return self._extension_data['ccgs'], self._extension_data['bins']
+        assert self._extension_data["ccgs"] is not None and self._extension_data["bins"] is not None, msg
+        return self._extension_data["ccgs"], self._extension_data["bins"]
 
     @staticmethod
     def get_extension_function():
         return compute_correlograms
 
 
 WaveformExtractor.register_extension(CorrelogramsCalculator)
 
 
-
 def _make_bins(sorting, window_ms, bin_ms):
-
     fs = sorting.get_sampling_frequency()
 
-    window_size = int(round(fs * window_ms/2 * 1e-3))
+    window_size = int(round(fs * window_ms / 2 * 1e-3))
     bin_size = int(round(fs * bin_ms * 1e-3))
     window_size -= window_size % bin_size
     num_bins = 2 * int(window_size / bin_size)
     assert num_bins >= 1
 
-    bins = np.arange(-window_size, window_size+bin_size, bin_size) * 1e3 / fs
-    
+    bins = np.arange(-window_size, window_size + bin_size, bin_size) * 1e3 / fs
+
     return bins, window_size, bin_size
-    
 
 
 def compute_autocorrelogram_from_spiketrain(spike_times, window_size, bin_size):
     """
     Computes the auto-correlogram from a given spike train.
 
     This implementation only works if you have numba installed, to accelerate the
@@ -135,22 +130,21 @@
     auto_corr: np.ndarray[int64]
         The computed auto-correlogram.
     """
     assert HAVE_NUMBA
     return _compute_crosscorr_numba(spike_times1.astype(np.int64), spike_times2.astype(np.int64), window_size, bin_size)
 
 
-
-
-
-def compute_correlograms(waveform_or_sorting_extractor, 
-                         load_if_exists=False,
-                         window_ms: float = 100.0,
-                         bin_ms: float = 5.0,
-                         method: str = "auto"):
+def compute_correlograms(
+    waveform_or_sorting_extractor,
+    load_if_exists=False,
+    window_ms: float = 100.0,
+    bin_ms: float = 5.0,
+    method: str = "auto",
+):
     """Compute auto and cross correlograms.
 
     Parameters
     ----------
     waveform_or_sorting_extractor : WaveformExtractor or BaseSorting
         If WaveformExtractor, the correlograms are saved as WaveformExtensions.
     load_if_exists : bool, default: False
@@ -178,190 +172,183 @@
         else:
             ccc = CorrelogramsCalculator(waveform_or_sorting_extractor)
             ccc.set_params(window_ms=window_ms, bin_ms=bin_ms, method=method)
             ccc.run()
         ccgs, bins = ccc.get_data()
         return ccgs, bins
     else:
-        return _compute_correlograms(waveform_or_sorting_extractor, window_ms=window_ms,
-                                     bin_ms=bin_ms, method=method)
+        return _compute_correlograms(waveform_or_sorting_extractor, window_ms=window_ms, bin_ms=bin_ms, method=method)
 
 
 def _compute_correlograms(sorting, window_ms, bin_ms, method="auto"):
     """
     Computes several cross-correlogram in one course from several clusters.
-    """ 
+    """
     assert method in ("auto", "numba", "numpy")
 
     if method == "auto":
         method = "numba" if HAVE_NUMBA else "numpy"
 
-
     bins, window_size, bin_size = _make_bins(sorting, window_ms, bin_ms)
-    
+
     if method == "numpy":
-        correlograms =  compute_correlograms_numpy(sorting, window_size, bin_size)
+        correlograms = compute_correlograms_numpy(sorting, window_size, bin_size)
     if method == "numba":
         correlograms = compute_correlograms_numba(sorting, window_size, bin_size)
-    
+
     return correlograms, bins
 
+
 # LOW-LEVEL IMPLEMENTATIONS
 def compute_correlograms_numpy(sorting, window_size, bin_size):
     """
     Computes cross-correlograms for all units in a sorting object.
-    
+
     This very elegant implementation is copied from phy package written by Cyrille Rossant.
     https://github.com/cortex-lab/phylib/blob/master/phylib/stats/ccg.py
-    
+
     The main modification is way the positive and negative are handled explicitly
     for rounding reasons.
-    
+
     Other slight modifications have been made to fit the SpikeInterface
     data model (e.g. adding the ability to handle multiple segments).
-    
+
     Adaptation: Samuel Garcia
     """
     num_seg = sorting.get_num_segments()
     num_units = len(sorting.unit_ids)
-    spikes = sorting.get_all_spike_trains(outputs='unit_index')
+    spikes = sorting.get_all_spike_trains(outputs="unit_index")
 
     num_half_bins = int(window_size // bin_size)
     num_bins = int(2 * num_half_bins)
 
-    correlograms = np.zeros((num_units, num_units, num_bins), dtype='int64')
+    correlograms = np.zeros((num_units, num_units, num_bins), dtype="int64")
 
     for seg_index in range(num_seg):
         spike_times, spike_labels = spikes[seg_index]
 
         c0 = correlogram_for_one_segment(spike_times, spike_labels, window_size, bin_size)
 
         correlograms += c0
 
     return correlograms
 
+
 def correlogram_for_one_segment(spike_times, spike_labels, window_size, bin_size):
     """
     Called by compute_correlograms_numpy
     """
 
     num_half_bins = int(window_size // bin_size)
     num_bins = int(2 * num_half_bins)
     num_units = len(np.unique(spike_labels))
-    
-    correlograms = np.zeros((num_units, num_units, num_bins), dtype='int64')
+
+    correlograms = np.zeros((num_units, num_units, num_bins), dtype="int64")
 
     # At a given shift, the mask precises which spikes have matching spikes
     # within the correlogram time window.
-    mask = np.ones_like(spike_times, dtype='bool')
+    mask = np.ones_like(spike_times, dtype="bool")
 
     # The loop continues as long as there is at least one spike with
     # a matching spike.
     shift = 1
     while mask[:-shift].any():
         # Number of time samples between spike i and spike i+shift.
         spike_diff = spike_times[shift:] - spike_times[:-shift]
 
         for sign in (-1, 1):
             # Binarize the delays between spike i and spike i+shift for negative and positive
-            # the operator // is np.floor_divide
-            spike_diff_b = (spike_diff  * sign) // bin_size
+            # the operator // is np.floor_divide
+            spike_diff_b = (spike_diff * sign) // bin_size
 
             # Spikes with no matching spikes are masked.
             if sign == -1:
                 mask[:-shift][spike_diff_b < -num_half_bins] = False
             else:
                 mask[:-shift][spike_diff_b >= num_half_bins] = False
 
             m = mask[:-shift]
 
             # Find the indices in the raveled correlograms array that need
             # to be incremented, taking into account the spike clusters.
-            if sign ==1:
+            if sign == 1:
                 indices = np.ravel_multi_index(
-                                                    (
-                                                        spike_labels[+shift:][m],
-                                                        spike_labels[:-shift][m],
-                                                        spike_diff_b[m] + num_half_bins),
-                                                correlograms.shape)
+                    (spike_labels[+shift:][m], spike_labels[:-shift][m], spike_diff_b[m] + num_half_bins),
+                    correlograms.shape,
+                )
             else:
-                
                 indices = np.ravel_multi_index(
-                                                    (spike_labels[:-shift][m],
-                                                        spike_labels[+shift:][m],
-                                                        spike_diff_b[m] + num_half_bins),
-                                                correlograms.shape)
-                
+                    (spike_labels[:-shift][m], spike_labels[+shift:][m], spike_diff_b[m] + num_half_bins),
+                    correlograms.shape,
+                )
 
             # Increment the matching spikes in the correlograms array.
             bbins = np.bincount(indices)
-            correlograms.ravel()[:len(bbins)] += bbins
+            correlograms.ravel()[: len(bbins)] += bbins
 
         shift += 1
 
     return correlograms
 
 
 def compute_correlograms_numba(sorting, window_size, bin_size):
     """
     Computes several cross-correlogram in one course
     from several cluster.
-    
+
     This is a "brute force" method using compiled code (numba)
     to accelerate the computation.
-    
+
     Implementation: Aurélien Wyngaard
     """
 
     assert HAVE_NUMBA
-    
+
     num_bins = 2 * int(window_size / bin_size)
     num_units = len(sorting.unit_ids)
-    spikes = sorting.get_all_spike_trains(outputs='unit_index')
+    spikes = sorting.get_all_spike_trains(outputs="unit_index")
     correlograms = np.zeros((num_units, num_units, num_bins), dtype=np.int64)
 
     for seg_index in range(sorting.get_num_segments()):
         spike_times, spike_labels = spikes[seg_index]
-        _compute_correlograms_numba(correlograms,
-                                    spike_times.astype(np.int64),
-                                    spike_labels.astype(np.int32),
-                                    window_size, bin_size)
-    
+        _compute_correlograms_numba(
+            correlograms, spike_times.astype(np.int64), spike_labels.astype(np.int32), window_size, bin_size
+        )
+
     return correlograms
 
+
 if HAVE_NUMBA:
 
-    @numba.jit((numba.int64[::1], numba.int32, numba.int32), nopython=True,
-                nogil=True, cache=True)
+    @numba.jit((numba.int64[::1], numba.int32, numba.int32), nopython=True, nogil=True, cache=True)
     def _compute_autocorr_numba(spike_times, window_size, bin_size):
         num_half_bins = window_size // bin_size
         num_bins = 2 * num_half_bins
-        
+
         auto_corr = np.zeros(num_bins, dtype=np.int64)
 
         for i in range(len(spike_times)):
-            for j in range(i+1, len(spike_times)):
+            for j in range(i + 1, len(spike_times)):
                 diff = spike_times[j] - spike_times[i]
-                
+
                 if diff > window_size:
                     break
 
                 bin = int(math.floor(diff / bin_size))
-                #~ auto_corr[num_bins//2 - bin - 1] += 1
+                # ~ auto_corr[num_bins//2 - bin - 1] += 1
                 auto_corr[num_half_bins + bin] += 1
-                #~ print(diff, bin, num_half_bins + bin)
-                
+                # ~ print(diff, bin, num_half_bins + bin)
+
                 bin = int(math.floor(-diff / bin_size))
                 auto_corr[num_half_bins + bin] += 1
-                #~ print(diff, bin, num_half_bins + bin)
-                
+                # ~ print(diff, bin, num_half_bins + bin)
+
         return auto_corr
 
-    @numba.jit((numba.int64[::1], numba.int64[::1], numba.int32, numba.int32),
-                nopython=True, nogil=True, cache=True)
+    @numba.jit((numba.int64[::1], numba.int64[::1], numba.int32, numba.int32), nopython=True, nogil=True, cache=True)
     def _compute_crosscorr_numba(spike_times1, spike_times2, window_size, bin_size):
         num_half_bins = window_size // bin_size
         num_bins = 2 * num_half_bins
 
         cross_corr = np.zeros(num_bins, dtype=np.int64)
 
         start_j = 0
@@ -372,29 +359,32 @@
                 if diff >= window_size:
                     start_j += 1
                     continue
                 if diff < -window_size:
                     break
 
                 bin = int(math.floor(diff / bin_size))
-                #~ bin = diff // bin_size
+                # ~ bin = diff // bin_size
                 cross_corr[num_half_bins + bin] += 1
-                #~ print(diff, bin, num_half_bins + bin)
+                # ~ print(diff, bin, num_half_bins + bin)
 
         return cross_corr
 
-    
-    @numba.jit((numba.int64[:, :, ::1], numba.int64[::1], numba.int32[::1], numba.int32, numba.int32),
-                nopython=True, nogil=True, cache=True, parallel=True)
+    @numba.jit(
+        (numba.int64[:, :, ::1], numba.int64[::1], numba.int32[::1], numba.int32, numba.int32),
+        nopython=True,
+        nogil=True,
+        cache=True,
+        parallel=True,
+    )
     def _compute_correlograms_numba(correlograms, spike_times, spike_labels, window_size, bin_size):
-        
         n_units = correlograms.shape[0]
 
         for i in numba.prange(n_units):
-        #~ for i in range(n_units):
+            # ~ for i in range(n_units):
             spike_times1 = spike_times[spike_labels == i]
 
             for j in range(i, n_units):
                 spike_times2 = spike_times[spike_labels == j]
 
                 if i == j:
                     correlograms[i, j, :] += _compute_autocorr_numba(spike_times1, window_size, bin_size)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/postprocessing/isi.py` & `spikeinterface-0.98.0/src/spikeinterface/postprocessing/isi.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,78 +1,75 @@
 import numpy as np
 from ..core import WaveformExtractor
 from ..core.waveform_extractor import BaseWaveformExtractorExtension
 
 try:
     import numba
+
     HAVE_NUMBA = True
 except ModuleNotFoundError as err:
     HAVE_NUMBA = False
 
 
 class ISIHistogramsCalculator(BaseWaveformExtractorExtension):
     """Compute ISI histograms of spike trains.
-    
+
     Parameters
     ----------
     waveform_extractor: WaveformExtractor
         A waveform extractor object
     """
-    extension_name = 'isi_histograms'
+
+    extension_name = "isi_histograms"
 
     def __init__(self, waveform_extractor):
         BaseWaveformExtractorExtension.__init__(self, waveform_extractor)
 
-    def _set_params(self, window_ms: float = 100.0,
-                    bin_ms: float = 5.0, method: str = "auto"):
-
+    def _set_params(self, window_ms: float = 100.0, bin_ms: float = 5.0, method: str = "auto"):
         params = dict(window_ms=window_ms, bin_ms=bin_ms, method=method)
 
         return params
 
     def _select_extension_data(self, unit_ids):
         # filter metrics dataframe
         unit_indices = self.waveform_extractor.sorting.ids_to_indices(unit_ids)
-        new_isi_hists = self._extension_data['isi_histograms'][unit_indices, :]
-        new_bins = self._extension_data['bins']
+        new_isi_hists = self._extension_data["isi_histograms"][unit_indices, :]
+        new_bins = self._extension_data["bins"]
         new_extension_data = dict(isi_histograms=new_isi_hists, bins=new_bins)
         return new_extension_data
 
     def _run(self):
-        isi_histograms, bins = _compute_isi_histograms(
-            self.waveform_extractor.sorting, **self._params)
-        self._extension_data['isi_histograms'] = isi_histograms
-        self._extension_data['bins'] = bins
+        isi_histograms, bins = _compute_isi_histograms(self.waveform_extractor.sorting, **self._params)
+        self._extension_data["isi_histograms"] = isi_histograms
+        self._extension_data["bins"] = bins
 
     def get_data(self):
         """
         Get the computed ISI histograms.
-        
+
         Returns
         -------
         isi_histograms : np.array
             2D array with ISI histograms (num_units, num_bins)
         bins : np.array
             1D array with bins in ms
         """
         msg = "ISI histograms are not computed. Use the 'run()' function."
-        assert self._extension_data['isi_histograms'] is not None and \
-            self._extension_data['bins'] is not None, msg
-        return self._extension_data['isi_histograms'], self._extension_data['bins']
+        assert self._extension_data["isi_histograms"] is not None and self._extension_data["bins"] is not None, msg
+        return self._extension_data["isi_histograms"], self._extension_data["bins"]
 
     @staticmethod
     def get_extension_function():
         return compute_isi_histograms
 
 
 WaveformExtractor.register_extension(ISIHistogramsCalculator)
 
 
-def compute_isi_histograms_from_spiketrain(spike_train: np.ndarray, max_time: int,
-                                           bin_size: int, sampling_f: float):
+def compute_isi_histograms_from_spiketrain(spike_train: np.ndarray, max_time: int, bin_size: int, sampling_f: float):
     """
     Computes the Inter-Spike Intervals histogram from a given spike train.
 
     This implementation only works if you have numba installed, to accelerate the
     computation time.
 
     Parameters
@@ -99,39 +96,43 @@
         print("compute_ISI_from_spiketrain cannot run without numba.")
         return 0
 
     return _compute_isi_histograms_from_spiketrain(spike_train.astype(np.int64), max_time, bin_size, sampling_f)
 
 
 if HAVE_NUMBA:
-    @numba.jit((numba.int64[::1], numba.int32, numba.int32, numba.float32),
-               nopython=True, nogil=True, cache=True)
+
+    @numba.jit((numba.int64[::1], numba.int32, numba.int32, numba.float32), nopython=True, nogil=True, cache=True)
     def _compute_isi_histograms_from_spiketrain(spike_train, max_time, bin_size, sampling_f):
         n_bins = int(max_time / bin_size)
 
-        bins = np.arange(0, max_time+bin_size, bin_size) * 1e3 / sampling_f
+        bins = np.arange(0, max_time + bin_size, bin_size) * 1e3 / sampling_f
         ISI = np.zeros(n_bins, dtype=np.int64)
 
         for i in range(1, len(spike_train)):
-            diff = spike_train[i] - spike_train[i-1]
+            diff = spike_train[i] - spike_train[i - 1]
 
             if diff >= max_time:
                 continue
 
             bin = int(diff / bin_size)
             ISI[bin] += 1
 
         return ISI, bins
 
 
-def compute_isi_histograms(waveform_or_sorting_extractor, load_if_exists=False, 
-                           window_ms: float = 50.0, bin_ms: float = 1.0,
-                           method: str = "auto"):
+def compute_isi_histograms(
+    waveform_or_sorting_extractor,
+    load_if_exists=False,
+    window_ms: float = 50.0,
+    bin_ms: float = 1.0,
+    method: str = "auto",
+):
     """Compute ISI histograms.
-    
+
     Parameters
     ----------
     waveform_or_sorting_extractor : WaveformExtractor or BaseSorting
         If WaveformExtractor, the ISI histograms are saved as WaveformExtensions.
     load_if_exists : bool, default: False
         Whether to load precomputed crosscorrelograms, if they already exist.
     window_ms : float, optional
@@ -149,26 +150,23 @@
         The bin edges in ms
     """
     if isinstance(waveform_or_sorting_extractor, WaveformExtractor):
         if load_if_exists and waveform_or_sorting_extractor.is_extension(ISIHistogramsCalculator.extension_name):
             isic = waveform_or_sorting_extractor.load_extension(ISIHistogramsCalculator.extension_name)
         else:
             isic = ISIHistogramsCalculator(waveform_or_sorting_extractor)
-            isic.set_params(window_ms=window_ms, bin_ms=bin_ms,
-                            method=method)
+            isic.set_params(window_ms=window_ms, bin_ms=bin_ms, method=method)
             isic.run()
         isi_histograms, bins = isic.get_data()
         return isi_histograms, bins
     else:
-        return _compute_isi_histograms(waveform_or_sorting_extractor, window_ms=window_ms,
-                                       bin_ms=bin_ms, method=method)
+        return _compute_isi_histograms(waveform_or_sorting_extractor, window_ms=window_ms, bin_ms=bin_ms, method=method)
 
 
-def _compute_isi_histograms(sorting, window_ms: float = 50.0, bin_ms: float = 1.0,
-                            method: str = "auto"):
+def _compute_isi_histograms(sorting, window_ms: float = 50.0, bin_ms: float = 1.0, method: str = "auto"):
     """
     Computes the Inter-Spike Intervals histogram for all
     the units inside the given sorting.
     """
 
     assert method in ("auto", "numba", "numpy")
 
@@ -197,21 +195,21 @@
     window_size = int(round(fs * window_ms * 1e-3))
     bin_size = int(round(fs * bin_ms * 1e-3))
     window_size -= window_size % bin_size
     num_bins = int(window_size / bin_size)
     assert num_bins >= 1
 
     ISIs = np.zeros((num_units, num_bins), dtype=np.int64)
-    bins = np.arange(0, window_size+bin_size, bin_size) * 1e3 / fs
-    
+    bins = np.arange(0, window_size + bin_size, bin_size) * 1e3 / fs
+
     # TODO: There might be a better way than a double for loop?
     for i, unit_id in enumerate(sorting.unit_ids):
         for seg_index in range(sorting.get_num_segments()):
             spike_train = sorting.get_unit_spike_train(unit_id, segment_index=seg_index)
-            ISI = np.histogram(np.diff(spike_train), bins=num_bins, range=(0, window_size-1))[0]
+            ISI = np.histogram(np.diff(spike_train), bins=num_bins, range=(0, window_size - 1))[0]
             ISIs[i] += ISI
 
     return ISIs, bins
 
 
 def compute_isi_histograms_numba(sorting, window_ms: float = 50.0, bin_ms: float = 1.0):
     """
@@ -230,30 +228,41 @@
 
     window_size = int(round(fs * window_ms * 1e-3))
     bin_size = int(round(fs * bin_ms * 1e-3))
     window_size -= window_size % bin_size
     num_bins = int(window_size / bin_size)
     assert num_bins >= 1
 
-    bins = np.arange(0, window_size+bin_size, bin_size) * 1e3 / fs
+    bins = np.arange(0, window_size + bin_size, bin_size) * 1e3 / fs
     spikes = sorting.get_all_spike_trains(outputs="unit_index")
 
     ISIs = np.zeros((num_units, num_bins), dtype=np.int64)
 
     for seg_index in range(sorting.get_num_segments()):
-        _compute_isi_histograms_numba(ISIs, spikes[seg_index][0].astype(np.int64),
-                           spikes[seg_index][1].astype(np.int32),
-                           window_size, bin_size, fs)
+        _compute_isi_histograms_numba(
+            ISIs,
+            spikes[seg_index][0].astype(np.int64),
+            spikes[seg_index][1].astype(np.int32),
+            window_size,
+            bin_size,
+            fs,
+        )
 
     return ISIs, bins
 
 
 if HAVE_NUMBA:
-    @numba.jit((numba.int64[:, ::1], numba.int64[::1], numba.int32[::1], numba.int32, numba.int32, numba.float32),
-               nopython=True, nogil=True, cache=True, parallel=True)
+
+    @numba.jit(
+        (numba.int64[:, ::1], numba.int64[::1], numba.int32[::1], numba.int32, numba.int32, numba.float32),
+        nopython=True,
+        nogil=True,
+        cache=True,
+        parallel=True,
+    )
     def _compute_isi_histograms_numba(ISIs, spike_trains, spike_clusters, max_time, bin_size, sampling_f):
         n_units = ISIs.shape[0]
 
         for i in numba.prange(n_units):
             spike_train = spike_trains[spike_clusters == i]
 
             ISIs[i] += _compute_isi_histograms_from_spiketrain(spike_train, max_time, bin_size, sampling_f)[0]
```

### Comparing `spikeinterface-0.97.1/spikeinterface/postprocessing/noise_level.py` & `spikeinterface-0.98.0/src/spikeinterface/postprocessing/noise_level.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,61 +1,62 @@
 from spikeinterface.core.waveform_extractor import BaseWaveformExtractorExtension, WaveformExtractor
 from spikeinterface.core import get_noise_levels
 
 
 class NoiseLevelsCalculator(BaseWaveformExtractorExtension):
-    extension_name = 'noise_levels'
+    extension_name = "noise_levels"
 
     def __init__(self, waveform_extractor):
         BaseWaveformExtractorExtension.__init__(self, waveform_extractor)
 
     def _set_params(self, num_chunks_per_segment=20, chunk_size=10000, seed=None):
         params = dict(num_chunks_per_segment=num_chunks_per_segment, chunk_size=chunk_size, seed=seed)
         return params
 
     def _select_extension_data(self, unit_ids):
         # this do not depend on units
-        return  self._extension_data
-        
+        return self._extension_data
+
     def _run(self):
         return_scaled = self.waveform_extractor.return_scaled
-        self._extension_data['noise_levels'] = get_noise_levels(self.waveform_extractor.recording,
-                                                                return_scaled=return_scaled, **self._params)
-    
+        self._extension_data["noise_levels"] = get_noise_levels(
+            self.waveform_extractor.recording, return_scaled=return_scaled, **self._params
+        )
+
     def get_data(self):
         """
         Get computed noise levels.
 
         Returns
         -------
         noise_levels : np.array
             The noise levels associated to each channel.
         """
-        return self._extension_data['noise_levels']
+        return self._extension_data["noise_levels"]
 
     @staticmethod
     def get_extension_function():
-        return compute_noise_levels            
+        return compute_noise_levels
 
 
 WaveformExtractor.register_extension(NoiseLevelsCalculator)
 
 
 def compute_noise_levels(waveform_extractor, load_if_exists=False, **params):
     """
     Computes the noise level associated to each recording channel.
 
-    This function will wraps the `get_noise_levels(recording)` to make the noise levels persistent 
+    This function will wraps the `get_noise_levels(recording)` to make the noise levels persistent
     on disk (folder or zarr) as a `WaveformExtension`.
-    The noise levels do not depend on the unit list, only the recording, but it is a convenient way to 
+    The noise levels do not depend on the unit list, only the recording, but it is a convenient way to
     retrieve the noise levels directly ine the WaveformExtractor.
 
-    Note that the noise levels can be scaled or not, depending on the `return_scaled` parameter 
+    Note that the noise levels can be scaled or not, depending on the `return_scaled` parameter
     of the `WaveformExtractor`.
-    
+
     Parameters
     ----------
     waveform_extractor: WaveformExtractor
         A waveform extractor object.
     num_chunks_per_segment: int (deulf 20)
         Number of chunks to estimate the noise
     chunk_size: int (default 10000)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/postprocessing/principal_component.py` & `spikeinterface-0.98.0/src/spikeinterface/postprocessing/principal_component.py`

 * *Files 11% similar despite different names*

```diff
@@ -2,30 +2,27 @@
 import pickle
 import warnings
 from pathlib import Path
 from tqdm.auto import tqdm
 
 import numpy as np
 
-from sklearn.decomposition import IncrementalPCA
-from sklearn.exceptions import NotFittedError
 
-from spikeinterface.core.job_tools import (ChunkRecordingExecutor, ensure_n_jobs,
-                                           _shared_job_kwargs_doc, fix_job_kwargs)
+from spikeinterface.core.job_tools import ChunkRecordingExecutor, _shared_job_kwargs_doc, fix_job_kwargs
 from spikeinterface.core.waveform_extractor import WaveformExtractor, BaseWaveformExtractorExtension
 
-_possible_modes = ['by_channel_local', 'by_channel_global', 'concatenated']
+_possible_modes = ["by_channel_local", "by_channel_global", "concatenated"]
 
 
 class WaveformPrincipalComponent(BaseWaveformExtractorExtension):
     """
     Class to extract principal components from a WaveformExtractor object.
     """
 
-    extension_name = 'principal_components'
+    extension_name = "principal_components"
     handle_sparsity = True
 
     def __init__(self, waveform_extractor):
         BaseWaveformExtractorExtension.__init__(self, waveform_extractor)
 
     @classmethod
     def create(cls, waveform_extractor):
@@ -33,63 +30,67 @@
         return pc
 
     def __repr__(self):
         we = self.waveform_extractor
         clsname = self.__class__.__name__
         nseg = we.get_num_segments()
         nchan = we.get_num_channels()
-        txt = f'{clsname}: {nchan} channels - {nseg} segments'
+        txt = f"{clsname}: {nchan} channels - {nseg} segments"
         if len(self._params) > 0:
-            mode = self._params['mode']
-            n_components = self._params['n_components']
-            txt = txt + f'\n  mode: {mode} n_components: {n_components}'
-            if self._params['sparsity'] is not None:
-                txt += ' - sparse'
+            mode = self._params["mode"]
+            n_components = self._params["n_components"]
+            txt = txt + f"\n  mode: {mode} n_components: {n_components}"
+            if self._params["sparsity"] is not None:
+                txt += " - sparse"
         return txt
 
-    def _set_params(self, n_components=5, mode='by_channel_local',
-                    whiten=True, dtype='float32', sparsity=None):
+    def _set_params(
+        self, n_components=5, mode="by_channel_local", whiten=True, dtype="float32", sparsity=None, tmp_folder=None
+    ):
         assert mode in _possible_modes, "Invalid mode!"
-        
+
         if self.waveform_extractor.is_sparse():
             assert sparsity is None, "WaveformExtractor is already sparse, sparsity must be None"
-        
+
         # the sparsity in params is ONLY the injected sparsity and not the waveform_extractor one
-        params = dict(n_components=int(n_components),
-                      mode=str(mode),
-                      whiten=bool(whiten),
-                      dtype=np.dtype(dtype).str,
-                      sparsity=sparsity)
-        
+        params = dict(
+            n_components=int(n_components),
+            mode=str(mode),
+            whiten=bool(whiten),
+            dtype=np.dtype(dtype).str,
+            sparsity=sparsity,
+            tmp_folder=tmp_folder,
+        )
+
         return params
-    
+
     def _select_extension_data(self, unit_ids):
         new_extension_data = dict()
         for unit_id in unit_ids:
-            new_extension_data[f'pca_{unit_id}'] = self._extension_data[f'pca_{unit_id}']
+            new_extension_data[f"pca_{unit_id}"] = self._extension_data[f"pca_{unit_id}"]
         for k, v in self._extension_data.items():
             if "model" in k:
                 new_extension_data[k] = v
         return new_extension_data
 
     def get_projections(self, unit_id):
         """
         Returns the computed projections for the sampled waveforms of a unit id.
 
         Parameters
         ----------
-        unit_id : int or str 
+        unit_id : int or str
             The unit id to return PCA projections for
 
         Returns
         -------
         proj: np.array
             The PCA projections (num_waveforms, num_components, num_channels)
         """
-        return self._extension_data[f'pca_{unit_id}']
+        return self._extension_data[f"pca_{unit_id}"]
 
     def get_pca_model(self):
         """
         Returns the scikit-learn PCA model objects.
 
         Returns
         -------
@@ -102,15 +103,15 @@
             pca_models = []
             for chan_id in self.waveform_extractor.channel_ids:
                 pca_models.append(self._extension_data[f"pca_model_{mode}_{chan_id}"])
         else:
             pca_models = self._extension_data[f"pca_model_{mode}"]
         return pca_models
 
-    def get_all_projections(self, channel_ids=None, unit_ids=None, outputs='id'):
+    def get_all_projections(self, channel_ids=None, unit_ids=None, outputs="id"):
         """
         Returns the computed projections for the sampled waveforms of all units.
 
         Parameters
         ----------
         channel_ids : list, optional
             List of channel ids on which projections are computed
@@ -134,18 +135,18 @@
         all_projections = []
         for unit_index, unit_id in enumerate(unit_ids):
             proj = self.get_projections(unit_id)
             if channel_ids is not None:
                 chan_inds = self.waveform_extractor.channel_ids_to_indices(channel_ids)
                 proj = proj[:, :, chan_inds]
             n = proj.shape[0]
-            if outputs == 'id':
+            if outputs == "id":
                 labels = np.array([unit_id] * n)
-            elif outputs == 'index':
-                labels = np.ones(n, dtype='int64')
+            elif outputs == "index":
+                labels = np.ones(n, dtype="int64")
                 labels[:] = unit_index
             all_labels.append(labels)
             all_projections.append(proj)
         all_labels = np.concatenate(all_labels, axis=0)
         all_projections = np.concatenate(all_projections, axis=0)
 
         return all_labels, all_projections
@@ -168,42 +169,45 @@
 
         """
         p = self._params
         mode = p["mode"]
         sparsity = p["sparsity"]
 
         wfs0 = self.waveform_extractor.get_waveforms(unit_id=self.waveform_extractor.sorting.unit_ids[0])
-        assert wfs0.shape[1] == new_waveforms.shape[1], \
-            ("Mismatch in number of samples between waveforms used to fit the pca model and 'new_waveforms")
+        assert (
+            wfs0.shape[1] == new_waveforms.shape[1]
+        ), "Mismatch in number of samples between waveforms used to fit the pca model and 'new_waveforms"
         num_channels = len(self.waveform_extractor.channel_ids)
 
         # check waveform shapes
         if sparsity is not None:
-            assert unit_id is not None, \
-                    "The unit_id of the new_waveforms is needed to apply the waveforms transformation"
+            assert (
+                unit_id is not None
+            ), "The unit_id of the new_waveforms is needed to apply the waveforms transformation"
             channel_inds = sparsity.unit_id_to_channel_indices[unit_id]
             if new_waveforms.shape[2] != len(channel_inds):
                 new_waveforms = new_waveforms.copy()[:, :, channel_inds]
-        else:            
-            assert wfs0.shape[2] == new_waveforms.shape[2], \
-                ("Mismatch in number of channels between waveforms used to fit the pca model and 'new_waveforms")
+        else:
+            assert (
+                wfs0.shape[2] == new_waveforms.shape[2]
+            ), "Mismatch in number of channels between waveforms used to fit the pca model and 'new_waveforms"
             channel_inds = np.arange(num_channels, dtype=int)
 
         # get channel ids and pca models
         pca_model = self.get_pca_model()
         projections = None
 
         if mode == "by_channel_local":
-            shape = (new_waveforms.shape[0], p['n_components'], num_channels)
+            shape = (new_waveforms.shape[0], p["n_components"], num_channels)
             projections = np.zeros(shape)
             for wf_ind, chan_ind in enumerate(channel_inds):
                 pca = pca_model[chan_ind]
                 projections[:, :, chan_ind] = pca.transform(new_waveforms[:, :, wf_ind])
         elif mode == "by_channel_global":
-            shape = (new_waveforms.shape[0], p['n_components'], num_channels)
+            shape = (new_waveforms.shape[0], p["n_components"], num_channels)
             projections = np.zeros(shape)
             for wf_ind, chan_ind in enumerate(channel_inds):
                 projections[:, :, chan_ind] = pca_model.transform(new_waveforms[:, :, wf_ind])
         elif mode == "concatenated":
             wfs_flat = new_waveforms.reshape(new_waveforms.shape[0], -1)
             projections = pca_model.transform(wfs_flat)
 
@@ -214,53 +218,53 @@
             return self.waveform_extractor.sparsity
         return self._params["sparsity"]
 
     def _run(self, **job_kwargs):
         """
         Compute the PCs on waveforms extacted within the WaveformExtarctor.
         Projections are computed only on the waveforms sampled by the WaveformExtractor.
-        
+
         The index of spikes come from the WaveformExtarctor.
         This will be cached in the same folder than WaveformExtarctor
         in extension subfolder.
         """
         p = self._params
         we = self.waveform_extractor
         num_chans = we.get_num_channels()
 
         # update job_kwargs with global ones
         job_kwargs = fix_job_kwargs(job_kwargs)
-        n_jobs = job_kwargs['n_jobs']
-        progress_bar = job_kwargs['progress_bar']
+        n_jobs = job_kwargs["n_jobs"]
+        progress_bar = job_kwargs["progress_bar"]
 
         # prepare memmap files with npy
         projection_objects = {}
         unit_ids = we.unit_ids
 
         for unit_id in unit_ids:
             n_spike = we.get_waveforms(unit_id).shape[0]
-            if p['mode'] in ('by_channel_local', 'by_channel_global'):
-                shape = (n_spike, p['n_components'], num_chans)
-            elif p['mode'] == 'concatenated':
-                shape = (n_spike, p['n_components'])
-            proj = np.zeros(shape, dtype=p['dtype'])          
+            if p["mode"] in ("by_channel_local", "by_channel_global"):
+                shape = (n_spike, p["n_components"], num_chans)
+            elif p["mode"] == "concatenated":
+                shape = (n_spike, p["n_components"])
+            proj = np.zeros(shape, dtype=p["dtype"])
             projection_objects[unit_id] = proj
 
         # run ...
-        if p['mode'] == 'by_channel_local':
+        if p["mode"] == "by_channel_local":
             self._run_by_channel_local(projection_objects, n_jobs, progress_bar)
-        elif p['mode'] == 'by_channel_global':
+        elif p["mode"] == "by_channel_global":
             self._run_by_channel_global(projection_objects, n_jobs, progress_bar)
-        elif p['mode'] == 'concatenated':
+        elif p["mode"] == "concatenated":
             self._run_concatenated(projection_objects, n_jobs, progress_bar)
 
         # add projections to extension data
         for unit_id in unit_ids:
-            self._extension_data[f'pca_{unit_id}'] = projection_objects[unit_id]
-            
+            self._extension_data[f"pca_{unit_id}"] = projection_objects[unit_id]
+
     def get_data(self):
         """
         Get computed PCA projections.
 
         Returns
         -------
         all_labels : 1d np.array
@@ -274,126 +278,136 @@
     def get_extension_function():
         return compute_principal_components
 
     def run_for_all_spikes(self, file_path=None, **job_kwargs):
         """
         Project all spikes from the sorting on the PCA model.
         This is a long computation because waveform need to be extracted from each spikes.
-        
+
         Used mainly for `export_to_phy()`
-        
+
         PCs are exported to a .npy single file.
 
         Parameters
         ----------
         file_path : str or Path or None
             Path to npy file that will store the PCA projections.
             If None, output is saved in principal_components/all_pcs.npy
         {}
         """
         job_kwargs = fix_job_kwargs(job_kwargs)
         p = self._params
         we = self.waveform_extractor
         sorting = we.sorting
-        assert we.has_recording(), (
-            "To compute PCA projections for all spikes, the waveform extractor needs the recording"
-        )
+        assert (
+            we.has_recording()
+        ), "To compute PCA projections for all spikes, the waveform extractor needs the recording"
         recording = we.recording
 
         assert sorting.get_num_segments() == 1
-        assert p['mode'] in ('by_channel_local', 'by_channel_global')
+        assert p["mode"] in ("by_channel_local", "by_channel_global")
 
         if file_path is None:
             file_path = self.extension_folder / "all_pcs.npy"
         file_path = Path(file_path)
 
-        all_spikes = sorting.get_all_spike_trains(outputs='unit_index')
+        all_spikes = sorting.get_all_spike_trains(outputs="unit_index")
         spike_times, spike_labels = all_spikes[0]
 
         sparsity = self.get_sparsity()
         if sparsity is None:
             sparse_channels_indices = {unit_id: np.arange(we.get_num_channels()) for unit_id in we.unit_ids}
             max_channels_per_template = we.get_num_channels()
         else:
             sparse_channels_indices = sparsity.unit_id_to_channel_indices
             max_channels_per_template = max([chan_inds.size for chan_inds in sparse_channels_indices.values()])
 
         unit_channels = [sparse_channels_indices[unit_id] for unit_id in sorting.unit_ids]
 
         pca_model = self.get_pca_model()
-        if p['mode'] in ['by_channel_global', 'concatenated']:
+        if p["mode"] in ["by_channel_global", "concatenated"]:
             pca_model = [pca_model] * recording.get_num_channels()
 
         # nSpikes, nFeaturesPerChannel, nPCFeatures
         # this comes from  phy template-gui
         # https://github.com/kwikteam/phy-contrib/blob/master/docs/template-gui.md#datasets
-        shape = (spike_times.size, p['n_components'], max_channels_per_template)
-        all_pcs = np.lib.format.open_memmap(filename=file_path, mode='w+', dtype='float32', shape=shape)
-        all_pcs_args = dict(filename=file_path, mode='r+', dtype='float32', shape=shape)
+        shape = (spike_times.size, p["n_components"], max_channels_per_template)
+        all_pcs = np.lib.format.open_memmap(filename=file_path, mode="w+", dtype="float32", shape=shape)
+        all_pcs_args = dict(filename=file_path, mode="r+", dtype="float32", shape=shape)
 
         # and run
         func = _all_pc_extractor_chunk
         init_func = _init_work_all_pc_extractor
-        n_jobs = ensure_n_jobs(recording, job_kwargs.get('n_jobs', None))
-        if n_jobs == 1:
-            init_args = (recording,)
-        else:
-            init_args = (recording.to_dict(),)
-        init_args = init_args + (all_pcs_args, spike_times, spike_labels, we.nbefore, we.nafter, 
-                                 unit_channels, pca_model)
-        processor = ChunkRecordingExecutor(recording, func, init_func, init_args, job_name='extract PCs', **job_kwargs)
+        init_args = (
+            recording,
+            all_pcs_args,
+            spike_times,
+            spike_labels,
+            we.nbefore,
+            we.nafter,
+            unit_channels,
+            pca_model,
+        )
+        processor = ChunkRecordingExecutor(recording, func, init_func, init_args, job_name="extract PCs", **job_kwargs)
         processor.run()
 
     def _fit_by_channel_local(self, n_jobs, progress_bar):
+        from sklearn.decomposition import IncrementalPCA
         from joblib import delayed, Parallel
-        
+
         we = self.waveform_extractor
         p = self._params
 
         unit_ids = we.unit_ids
         channel_ids = we.channel_ids
-
         # there is one PCA per channel for independent fit per channel
-        pca_models = [IncrementalPCA(n_components=p['n_components'], whiten=p['whiten']) for _ in channel_ids]
-        
+        pca_models = [IncrementalPCA(n_components=p["n_components"], whiten=p["whiten"]) for _ in channel_ids]
+
         mode = p["mode"]
         pca_model_files = []
-        tmp_folder = Path("tmp")
+
+        tmp_folder = p["tmp_folder"]
+        if tmp_folder is None:
+            tmp_folder = "tmp"
+        tmp_folder = Path(tmp_folder)
+
         for chan_ind, chan_id in enumerate(channel_ids):
             pca_model = pca_models[chan_ind]
             if n_jobs > 1:
                 tmp_folder.mkdir(exist_ok=True)
                 pca_model_file = tmp_folder / f"tmp_pca_model_{mode}_{chan_id}.pkl"
                 with pca_model_file.open("wb") as f:
                     pickle.dump(pca_model, f)
                 pca_model_files.append(pca_model_file)
-        
+
         # fit
         units_loop = enumerate(unit_ids)
         if progress_bar:
             units_loop = tqdm(units_loop, desc="Fitting PCA", total=len(unit_ids))
 
         for unit_ind, unit_id in units_loop:
             wfs, channel_inds = self._get_sparse_waveforms(unit_id)
-            if len(wfs) < p['n_components']:
+            if len(wfs) < p["n_components"]:
                 continue
             if n_jobs in (0, 1):
                 for wf_ind, chan_ind in enumerate(channel_inds):
                     pca = pca_models[chan_ind]
                     pca.partial_fit(wfs[:, :, wf_ind])
             else:
-                Parallel(n_jobs=n_jobs)(delayed(partial_fit_one_channel)(pca_model_files[chan_ind], wfs[:, :, wf_ind])
-                                                        for wf_ind, chan_ind in enumerate(channel_inds))
+                Parallel(n_jobs=n_jobs)(
+                    delayed(partial_fit_one_channel)(pca_model_files[chan_ind], wfs[:, :, wf_ind])
+                    for wf_ind, chan_ind in enumerate(channel_inds)
+                )
 
         # reload the models (if n_jobs > 1)
         if n_jobs not in (0, 1):
             pca_models = []
             for chan_ind, chan_id in enumerate(channel_ids):
                 pca_model_file = pca_model_files[chan_ind]
-                with open(pca_model_file, 'rb') as fid:
+                with open(pca_model_file, "rb") as fid:
                     pca_models.append(pickle.load(fid))
                 pca_model_file.unlink()
             shutil.rmtree(tmp_folder)
 
         # add models to extension data
         for chan_ind, chan_id in enumerate(channel_ids):
             pca_model = pca_models[chan_ind]
@@ -402,14 +416,16 @@
         return pca_models
 
     def _run_by_channel_local(self, projection_memmap, n_jobs, progress_bar):
         """
         In this mode each PCA is "fit" and "transform" by channel.
         The output is then (n_spike, n_components, n_channels)
         """
+        from sklearn.exceptions import NotFittedError
+
         we = self.waveform_extractor
         unit_ids = we.unit_ids
 
         pca_model = self._fit_by_channel_local(n_jobs, progress_bar)
 
         # transform
         units_loop = enumerate(unit_ids)
@@ -426,35 +442,39 @@
                 try:
                     proj = pca.transform(wfs[:, :, wf_ind])
                     projection_memmap[unit_id][:, :, chan_ind] = proj
                 except NotFittedError as e:
                     # this could happen if len(wfs) is less then n_comp for a channel
                     project_on_non_fitted = True
         if project_on_non_fitted:
-            warnings.warn("Projection attempted on unfitted PCA models. This could be due to a small "
-                          "number of waveforms for a particular unit.")
+            warnings.warn(
+                "Projection attempted on unfitted PCA models. This could be due to a small "
+                "number of waveforms for a particular unit."
+            )
 
     def _fit_by_channel_global(self, progress_bar):
         we = self.waveform_extractor
         p = self._params
         unit_ids = we.unit_ids
 
         # there is one unique PCA accross channels
-        pca_model = IncrementalPCA(n_components=p['n_components'], whiten=p['whiten'])
+        from sklearn.decomposition import IncrementalPCA
+
+        pca_model = IncrementalPCA(n_components=p["n_components"], whiten=p["whiten"])
 
         # fit
         units_loop = enumerate(unit_ids)
         if progress_bar:
             units_loop = tqdm(units_loop, desc="Fitting PCA", total=len(unit_ids))
 
         # with 'by_channel_global' we can't parallelize over channels
         for unit_ind, unit_id in units_loop:
             wfs, _ = self._get_sparse_waveforms(unit_id)
             shape = wfs.shape
-            if shape[0] * shape[2] < p['n_components']:
+            if shape[0] * shape[2] < p["n_components"]:
                 continue
             # avoid loop with reshape
             wfs_concat = wfs.transpose(0, 2, 1).reshape(shape[0] * shape[2], shape[1])
             pca_model.partial_fit(wfs_concat)
 
         # save
         mode = p["mode"]
@@ -482,38 +502,41 @@
         for unit_ind, unit_id in units_loop:
             wfs, channel_inds = self._get_sparse_waveforms(unit_id)
             if wfs.size == 0:
                 continue
             for wf_ind, chan_ind in enumerate(channel_inds):
                 proj = pca_model.transform(wfs[:, :, wf_ind])
                 projection_objects[unit_id][:, :, chan_ind] = proj
-                
+
     def _fit_concatenated(self, progress_bar):
         we = self.waveform_extractor
         p = self._params
         unit_ids = we.unit_ids
-        
+
         sparsity = self.get_sparsity()
         if sparsity is not None:
             sparsity0 = sparsity.unit_id_to_channel_indices[unit_ids[0]]
-            assert all(len(chans) == len(sparsity0) for u, chans in sparsity.unit_id_to_channel_indices.items()), \
-                "When using sparsity in concatenated mode, make sure each unit has the same number of sparse channels"
-        
+            assert all(
+                len(chans) == len(sparsity0) for u, chans in sparsity.unit_id_to_channel_indices.items()
+            ), "When using sparsity in concatenated mode, make sure each unit has the same number of sparse channels"
+
         # there is one unique PCA accross channels
-        pca_model = IncrementalPCA(n_components=p['n_components'], whiten=p['whiten'])
+        from sklearn.decomposition import IncrementalPCA
+
+        pca_model = IncrementalPCA(n_components=p["n_components"], whiten=p["whiten"])
 
         # fit
         units_loop = enumerate(unit_ids)
         if progress_bar:
             units_loop = tqdm(units_loop, desc="Fitting PCA", total=len(unit_ids))
 
         for unit_ind, unit_id in units_loop:
             wfs, _ = self._get_sparse_waveforms(unit_id)
             wfs_flat = wfs.reshape(wfs.shape[0], -1)
-            if len(wfs_flat) < p['n_components']:
+            if len(wfs_flat) < p["n_components"]:
                 continue
             pca_model.partial_fit(wfs_flat)
 
         # save
         mode = p["mode"]
         self._extension_data[f"pca_model_{mode}"] = pca_model
 
@@ -542,38 +565,39 @@
             wfs_flat = wfs.reshape(wfs.shape[0], -1)
             proj = pca_model.transform(wfs_flat)
             projection_objects[unit_id][:, :] = proj
 
     def _get_sparse_waveforms(self, unit_id):
         # get waveforms : dense or sparse
         we = self.waveform_extractor
-        sparsity = self._params['sparsity']
+        sparsity = self._params["sparsity"]
         if we.is_sparse():
             # natural sparsity
             wfs = we.get_waveforms(unit_id, lazy=False)
             channel_inds = we.sparsity.unit_id_to_channel_indices[unit_id]
         elif sparsity is not None:
             # injected sparsity
             wfs = self.waveform_extractor.get_waveforms(unit_id, sparsity=sparsity, lazy=False)
             channel_inds = sparsity.unit_id_to_channel_indices[unit_id]
         else:
             # dense
             wfs = self.waveform_extractor.get_waveforms(unit_id, sparsity=None, lazy=False)
             channel_inds = np.arange(we.channel_ids.size, dtype=int)
         return wfs, channel_inds
 
+
 def _all_pc_extractor_chunk(segment_index, start_frame, end_frame, worker_ctx):
-    recording = worker_ctx['recording']
-    all_pcs = worker_ctx['all_pcs']
-    spike_times = worker_ctx['spike_times']
-    spike_labels = worker_ctx['spike_labels']
-    nbefore = worker_ctx['nbefore']
-    nafter = worker_ctx['nafter']
-    unit_channels = worker_ctx['unit_channels']
-    pca_model = worker_ctx['pca_model']
+    recording = worker_ctx["recording"]
+    all_pcs = worker_ctx["all_pcs"]
+    spike_times = worker_ctx["spike_times"]
+    spike_labels = worker_ctx["spike_labels"]
+    nbefore = worker_ctx["nbefore"]
+    nafter = worker_ctx["nafter"]
+    unit_channels = worker_ctx["unit_channels"]
+    pca_model = worker_ctx["pca_model"]
 
     seg_size = recording.get_num_samples(segment_index=segment_index)
 
     i0 = np.searchsorted(spike_times, start_frame)
     i1 = np.searchsorted(spike_times, end_frame)
 
     if i0 != i1:
@@ -595,53 +619,64 @@
     for i in range(i0, i1):
         st = spike_times[i]
         if st - start - nbefore < 0:
             continue
         if st - start + nafter > traces.shape[0]:
             continue
 
-        wf = traces[st - start - nbefore:st - start + nafter, :]
+        wf = traces[st - start - nbefore : st - start + nafter, :]
 
         unit_index = spike_labels[i]
         chan_inds = unit_channels[unit_index]
 
         for c, chan_ind in enumerate(chan_inds):
             w = wf[:, chan_ind]
             if w.size > 0:
                 w = w[None, :]
                 all_pcs[i, :, c] = pca_model[chan_ind].transform(w)
 
 
-def _init_work_all_pc_extractor(recording, all_pcs_args, spike_times, spike_labels, nbefore, nafter, unit_channels,
-                                pca_model):
+def _init_work_all_pc_extractor(
+    recording, all_pcs_args, spike_times, spike_labels, nbefore, nafter, unit_channels, pca_model
+):
     worker_ctx = {}
     if isinstance(recording, dict):
         from spikeinterface.core import load_extractor
+
         recording = load_extractor(recording)
-    worker_ctx['recording'] = recording
-    worker_ctx['all_pcs'] = np.lib.format.open_memmap(**all_pcs_args)
-    worker_ctx['spike_times'] = spike_times
-    worker_ctx['spike_labels'] = spike_labels
-    worker_ctx['nbefore'] = nbefore
-    worker_ctx['nafter'] = nafter
-    worker_ctx['unit_channels'] = unit_channels
-    worker_ctx['pca_model'] = pca_model
+    worker_ctx["recording"] = recording
+    worker_ctx["all_pcs"] = np.lib.format.open_memmap(**all_pcs_args)
+    worker_ctx["spike_times"] = spike_times
+    worker_ctx["spike_labels"] = spike_labels
+    worker_ctx["nbefore"] = nbefore
+    worker_ctx["nafter"] = nafter
+    worker_ctx["unit_channels"] = unit_channels
+    worker_ctx["pca_model"] = pca_model
 
     return worker_ctx
 
 
 WaveformPrincipalComponent.run_for_all_spikes.__doc__ = WaveformPrincipalComponent.run_for_all_spikes.__doc__.format(
-    _shared_job_kwargs_doc)
+    _shared_job_kwargs_doc
+)
 
 WaveformExtractor.register_extension(WaveformPrincipalComponent)
 
 
-def compute_principal_components(waveform_extractor, load_if_exists=False,
-                                 n_components=5, mode='by_channel_local', sparsity=None,
-                                 whiten=True, dtype='float32', **job_kwargs):
+def compute_principal_components(
+    waveform_extractor,
+    load_if_exists=False,
+    n_components=5,
+    mode="by_channel_local",
+    sparsity=None,
+    whiten=True,
+    dtype="float32",
+    tmp_folder=None,
+    **job_kwargs,
+):
     """
     Compute PC scores from waveform extractor. The PCA projections are pre-computed only
     on the sampled waveforms available from the WaveformExtractor.
 
     Parameters
     ----------
     waveform_extractor: WaveformExtractor
@@ -650,25 +685,30 @@
         If True and pc scores are already in the waveform extractor folders, pc scores are loaded and not recomputed.
     n_components: int
         Number of components fo PCA - default 5
     mode: str
         - 'by_channel_local': a local PCA is fitted for each channel (projection by channel)
         - 'by_channel_global': a global PCA is fitted for all channels (projection by channel)
         - 'concatenated': channels are concatenated and a global PCA is fitted
+        default 'by_channel_local'
     sparsity: ChannelSparsity or None
         The sparsity to apply to waveforms.
-        If waveform_extractor is already sparse, the default sparsity will be used.
+        If waveform_extractor is already sparse, the default sparsity will be used - default None
     whiten: bool
-        If True, waveforms are pre-whitened
+        If True, waveforms are pre-whitened - default True
     dtype: dtype
-        Dtype of the pc scores (default float32)
+        Dtype of the pc scores - default float32
     n_jobs: int
         Number of jobs used to fit the PCA model (if mode is 'by_channel_local') - default 1
     progress_bar: bool
         If True, a progress bar is shown - default False
+    tmp_folder: str
+        The temporary folder to use for parallel computation. If you run several `compute_principal_components`
+        functions in parallel with mode 'by_channel_local', you need to specify a different `tmp_folder` for each call,
+        to avoid overwriting to the same folder - default None
 
     Returns
     -------
     pc: WaveformPrincipalComponent
         The waveform principal component object
 
     Examples
@@ -686,20 +726,21 @@
     >>> # run for all spikes in the SortingExtractor
     >>> pc.run_for_all_spikes(file_path="all_pca_projections.npy")
     """
     if load_if_exists and waveform_extractor.is_extension(WaveformPrincipalComponent.extension_name):
         pc = waveform_extractor.load_extension(WaveformPrincipalComponent.extension_name)
     else:
         pc = WaveformPrincipalComponent.create(waveform_extractor)
-        pc.set_params(n_components=n_components, mode=mode, whiten=whiten, dtype=dtype,
-                      sparsity=sparsity)
+        pc.set_params(
+            n_components=n_components, mode=mode, whiten=whiten, dtype=dtype, sparsity=sparsity, tmp_folder=tmp_folder
+        )
         pc.run(**job_kwargs)
 
     return pc
 
 
 def partial_fit_one_channel(pca_file, wf_chan):
-    with open(pca_file, 'rb') as fid:
+    with open(pca_file, "rb") as fid:
         pca_model = pickle.load(fid)
     pca_model.partial_fit(wf_chan)
     with pca_file.open("wb") as f:
         pickle.dump(pca_model, f)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/postprocessing/spike_amplitudes.py` & `spikeinterface-0.98.0/src/spikeinterface/postprocessing/spike_amplitudes.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,100 +1,100 @@
 import numpy as np
 import shutil
 
-from spikeinterface.core.job_tools import (ChunkRecordingExecutor, _shared_job_kwargs_doc,
-                                           ensure_n_jobs, fix_job_kwargs)
+from spikeinterface.core.job_tools import ChunkRecordingExecutor, _shared_job_kwargs_doc, ensure_n_jobs, fix_job_kwargs
 
-from spikeinterface.core.template_tools import (get_template_extremum_channel,
-                                                get_template_extremum_channel_peak_shift)
+from spikeinterface.core.template_tools import get_template_extremum_channel, get_template_extremum_channel_peak_shift
 
 from spikeinterface.core.waveform_extractor import WaveformExtractor, BaseWaveformExtractorExtension
 
 
 class SpikeAmplitudesCalculator(BaseWaveformExtractorExtension):
     """
-    Computes spike amplitudes form WaveformExtractor.
-    """    
-    extension_name = 'spike_amplitudes'
-    
+    Computes spike amplitudes from WaveformExtractor.
+    """
+
+    extension_name = "spike_amplitudes"
+
     def __init__(self, waveform_extractor):
         BaseWaveformExtractorExtension.__init__(self, waveform_extractor)
 
         self._all_spikes = None
 
-    def _set_params(self, peak_sign='neg', return_scaled=True):
+    def _set_params(self, peak_sign="neg", return_scaled=True):
+        params = dict(peak_sign=str(peak_sign), return_scaled=bool(return_scaled))
+        return params
 
-        params = dict(peak_sign=str(peak_sign),
-                      return_scaled=bool(return_scaled))
-        return params        
-    
     def _select_extension_data(self, unit_ids):
         # load filter and save amplitude files
         new_extension_data = dict()
         for seg_index in range(self.waveform_extractor.recording.get_num_segments()):
             amp_data_name = f"amplitude_segment_{seg_index}"
             amps = self._extension_data[amp_data_name]
             _, all_labels = self.waveform_extractor.sorting.get_all_spike_trains()[seg_index]
             filtered_idxs = np.in1d(all_labels, np.array(unit_ids)).nonzero()
             new_extension_data[amp_data_name] = amps[filtered_idxs]
         return new_extension_data
-        
+
     def _run(self, **job_kwargs):
+        if not self.waveform_extractor.has_recording():
+            self.waveform_extractor.delete_extension(SpikeAmplitudesCalculator.extension_name)
+            raise ValueError("compute_spike_amplitudes() cannot run with a WaveformExtractor in recordless mode.")
+
         job_kwargs = fix_job_kwargs(job_kwargs)
         we = self.waveform_extractor
         recording = we.recording
         sorting = we.sorting
 
-        all_spikes = sorting.get_all_spike_trains(outputs='unit_index')
+        all_spikes = sorting.get_all_spike_trains(outputs="unit_index")
         self._all_spikes = all_spikes
-        
-        peak_sign = self._params['peak_sign']
-        return_scaled = self._params['return_scaled']
 
-        extremum_channels_index = get_template_extremum_channel(we, peak_sign=peak_sign, outputs='index')
+        peak_sign = self._params["peak_sign"]
+        return_scaled = self._params["return_scaled"]
+
+        extremum_channels_index = get_template_extremum_channel(we, peak_sign=peak_sign, outputs="index")
         peak_shifts = get_template_extremum_channel_peak_shift(we, peak_sign=peak_sign)
-        
+
         # put extremum_channels_index and peak_shifts in vector way
-        extremum_channels_index = np.array([extremum_channels_index[unit_id] for unit_id in sorting.unit_ids], 
-                                            dtype='int64')
-        peak_shifts = np.array([peak_shifts[unit_id] for unit_id in sorting.unit_ids], dtype='int64')
-        
+        extremum_channels_index = np.array(
+            [extremum_channels_index[unit_id] for unit_id in sorting.unit_ids], dtype="int64"
+        )
+        peak_shifts = np.array([peak_shifts[unit_id] for unit_id in sorting.unit_ids], dtype="int64")
+
         if return_scaled:
             # check if has scaled values:
             if not recording.has_scaled_traces():
                 print("Setting 'return_scaled' to False")
                 return_scaled = False
 
         # and run
         func = _spike_amplitudes_chunk
         init_func = _init_worker_spike_amplitudes
-        n_jobs = ensure_n_jobs(recording, job_kwargs.get('n_jobs', None))
-        if n_jobs == 1:
-            init_args = (recording, sorting)
-        else:
+        n_jobs = ensure_n_jobs(recording, job_kwargs.get("n_jobs", None))
+        if n_jobs != 1:
             # TODO: avoid dumping sorting and use spike vector and peak pipeline instead
             assert sorting.check_if_dumpable(), (
-                "The soring object is not dumpable and cannot be processed in parallel. You can use the "
+                "The sorting object is not dumpable and cannot be processed in parallel. You can use the "
                 "`sorting.save()` function to make it dumpable"
             )
-            init_args = (recording.to_dict(), sorting.to_dict())
-        init_args = init_args + (extremum_channels_index, peak_shifts, return_scaled)
-        processor = ChunkRecordingExecutor(recording, func, init_func, init_args,
-                                           handle_returns=True, job_name='extract amplitudes', **job_kwargs)
+        init_args = (recording, sorting, extremum_channels_index, peak_shifts, return_scaled)
+        processor = ChunkRecordingExecutor(
+            recording, func, init_func, init_args, handle_returns=True, job_name="extract amplitudes", **job_kwargs
+        )
         out = processor.run()
         amps, segments = zip(*out)
         amps = np.concatenate(amps)
         segments = np.concatenate(segments)
 
         for segment_index in range(recording.get_num_segments()):
             mask = segments == segment_index
             amps_seg = amps[mask]
-            self._extension_data[f'amplitude_segment_{segment_index}'] = amps_seg
+            self._extension_data[f"amplitude_segment_{segment_index}"] = amps_seg
 
-    def get_data(self, outputs='concatenated'):
+    def get_data(self, outputs="concatenated"):
         """
         Get computed spike amplitudes.
 
         Parameters
         ----------
         outputs : str, optional
             'concatenated' or 'by_unit', by default 'concatenated'
@@ -103,44 +103,43 @@
         -------
         spike_amplitudes : np.array or dict
             The spike amplitudes as an array (outputs='concatenated') or
             as a dict with units as key and spike amplitudes as values.
         """
         we = self.waveform_extractor
         sorting = we.sorting
-        all_spikes = sorting.get_all_spike_trains(outputs='unit_index')
+        all_spikes = sorting.get_all_spike_trains(outputs="unit_index")
 
-        if outputs == 'concatenated':
+        if outputs == "concatenated":
             amplitudes = []
             for segment_index in range(we.get_num_segments()):
-                amplitudes.append(self._extension_data[f'amplitude_segment_{segment_index}'])
+                amplitudes.append(self._extension_data[f"amplitude_segment_{segment_index}"])
             return amplitudes
-        elif outputs == 'by_unit':
+        elif outputs == "by_unit":
             amplitudes_by_unit = []
             for segment_index in range(we.get_num_segments()):
                 amplitudes_by_unit.append({})
                 for unit_index, unit_id in enumerate(sorting.unit_ids):
                     _, spike_labels = all_spikes[segment_index]
                     mask = spike_labels == unit_index
-                    amps = self._extension_data[f'amplitude_segment_{segment_index}'][mask]
+                    amps = self._extension_data[f"amplitude_segment_{segment_index}"][mask]
                     amplitudes_by_unit[segment_index][unit_id] = amps
             return amplitudes_by_unit
 
     @staticmethod
     def get_extension_function():
         return compute_spike_amplitudes
 
 
 WaveformExtractor.register_extension(SpikeAmplitudesCalculator)
 
 
-def compute_spike_amplitudes(waveform_extractor, load_if_exists=False, 
-                             peak_sign='neg', return_scaled=True,
-                             outputs='concatenated',
-                             **job_kwargs):
+def compute_spike_amplitudes(
+    waveform_extractor, load_if_exists=False, peak_sign="neg", return_scaled=True, outputs="concatenated", **job_kwargs
+):
     """
     Computes the spike amplitudes from a WaveformExtractor.
 
     1. The waveform extractor is used to determine the max channel per unit.
     2. Then a "peak_shift" is estimated because for some sorters the spike index is not always at the
        peak.
     3. Amplitudes are extracted in chunks (parallel or not)
@@ -173,92 +172,83 @@
     """
     if load_if_exists and waveform_extractor.is_extension(SpikeAmplitudesCalculator.extension_name):
         sac = waveform_extractor.load_extension(SpikeAmplitudesCalculator.extension_name)
     else:
         sac = SpikeAmplitudesCalculator(waveform_extractor)
         sac.set_params(peak_sign=peak_sign, return_scaled=return_scaled)
         sac.run(**job_kwargs)
-    
+
     amps = sac.get_data(outputs=outputs)
     return amps
 
 
 compute_spike_amplitudes.__doc__.format(_shared_job_kwargs_doc)
 
 
 def _init_worker_spike_amplitudes(recording, sorting, extremum_channels_index, peak_shifts, return_scaled):
-    # create a local dict per worker
     worker_ctx = {}
-    if isinstance(recording, dict):
-        from spikeinterface.core import load_extractor
-        recording = load_extractor(recording)
-    if isinstance(sorting, dict):
-        from spikeinterface.core import load_extractor
-        sorting = load_extractor(sorting)
-    
-    worker_ctx['recording'] = recording
-    worker_ctx['sorting'] = sorting
-    worker_ctx['return_scaled'] = return_scaled
-    worker_ctx['peak_shifts'] = peak_shifts
-    worker_ctx['min_shift'] = np.min(peak_shifts)
-    worker_ctx['max_shifts'] = np.max(peak_shifts)
-    
-    all_spikes = sorting.get_all_spike_trains(outputs='unit_index')
-
-    worker_ctx['all_spikes'] = all_spikes
-    worker_ctx['extremum_channels_index'] = extremum_channels_index
+    worker_ctx["recording"] = recording
+    worker_ctx["sorting"] = sorting
+    worker_ctx["return_scaled"] = return_scaled
+    worker_ctx["peak_shifts"] = peak_shifts
+    worker_ctx["min_shift"] = np.min(peak_shifts)
+    worker_ctx["max_shifts"] = np.max(peak_shifts)
+
+    all_spikes = sorting.get_all_spike_trains(outputs="unit_index")
+    worker_ctx["all_spikes"] = all_spikes
+    worker_ctx["extremum_channels_index"] = extremum_channels_index
 
     return worker_ctx
 
 
 def _spike_amplitudes_chunk(segment_index, start_frame, end_frame, worker_ctx):
     # recover variables of the worker
-    all_spikes = worker_ctx['all_spikes']
-    recording = worker_ctx['recording']
-    return_scaled = worker_ctx['return_scaled']
-    peak_shifts = worker_ctx['peak_shifts']
-    
+    all_spikes = worker_ctx["all_spikes"]
+    recording = worker_ctx["recording"]
+    return_scaled = worker_ctx["return_scaled"]
+    peak_shifts = worker_ctx["peak_shifts"]
+
     seg_size = recording.get_num_samples(segment_index=segment_index)
 
     spike_times, spike_labels = all_spikes[segment_index]
     d = np.diff(spike_times)
     assert np.all(d >= 0)
 
     i0 = np.searchsorted(spike_times, start_frame)
     i1 = np.searchsorted(spike_times, end_frame)
-    
+
     n_spikes = i1 - i0
     amplitudes = np.zeros(n_spikes, dtype=recording.get_dtype())
-    
+
     if i0 != i1:
         # some spike in the chunk
 
-        extremum_channels_index = worker_ctx['extremum_channels_index']
+        extremum_channels_index = worker_ctx["extremum_channels_index"]
 
         sample_inds = spike_times[i0:i1].copy()
         labels = spike_labels[i0:i1]
-        
+
         # apply shifts per spike
         sample_inds += peak_shifts[labels]
-        
+
         # get channels per spike
         chan_inds = extremum_channels_index[labels]
-        
+
         # prevent border accident due to shift
         sample_inds[sample_inds < 0] = 0
         sample_inds[sample_inds >= seg_size] = seg_size - 1
-        
+
         first = np.min(sample_inds)
         last = np.max(sample_inds)
         sample_inds -= first
-        
+
         # load trace in memory
-        traces = recording.get_traces(start_frame=first, end_frame=last+1, 
-                                      segment_index=segment_index,
-                                      return_scaled=return_scaled)
-        
+        traces = recording.get_traces(
+            start_frame=first, end_frame=last + 1, segment_index=segment_index, return_scaled=return_scaled
+        )
+
         # and get amplitudes
         amplitudes = traces[sample_inds, chan_inds]
-    
-    segments = np.zeros(amplitudes.size, dtype='int64') + segment_index
-    
+
+    segments = np.zeros(amplitudes.size, dtype="int64") + segment_index
+
     return amplitudes, segments
```

### Comparing `spikeinterface-0.97.1/spikeinterface/postprocessing/spike_locations.py` & `spikeinterface-0.98.0/src/spikeinterface/postprocessing/spike_locations.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,69 +1,66 @@
 import numpy as np
 
 from spikeinterface.core.job_tools import _shared_job_kwargs_doc, fix_job_kwargs
 
-from spikeinterface.core.template_tools import (get_template_extremum_channel,
-                                                get_template_extremum_channel_peak_shift)
+from spikeinterface.core.template_tools import get_template_extremum_channel, get_template_extremum_channel_peak_shift
 
 from spikeinterface.core.waveform_extractor import WaveformExtractor, BaseWaveformExtractorExtension
 
 
 class SpikeLocationsCalculator(BaseWaveformExtractorExtension):
     """
     Computes spike locations from WaveformExtractor.
-    
+
     Parameters
     ----------
     waveform_extractor: WaveformExtractor
         A waveform extractor object
-    """    
-    extension_name = 'spike_locations'
-    
+    """
+
+    extension_name = "spike_locations"
+
     def __init__(self, waveform_extractor):
         BaseWaveformExtractorExtension.__init__(self, waveform_extractor)
 
         extremum_channel_inds = get_template_extremum_channel(self.waveform_extractor, outputs="index")
         self.spikes = self.waveform_extractor.sorting.to_spike_vector(extremum_channel_inds=extremum_channel_inds)
 
-    def _set_params(self, ms_before=1., ms_after=1.5, method='center_of_mass',
-                    method_kwargs={}):
-
-        params = dict(ms_before=ms_before,
-                      ms_after=ms_after,
-                      method=method)
+    def _set_params(self, ms_before=0.5, ms_after=0.5, method="center_of_mass", method_kwargs={}):
+        params = dict(ms_before=ms_before, ms_after=ms_after, method=method)
         params.update(**method_kwargs)
-        return params        
-    
+        return params
+
     def _select_extension_data(self, unit_ids):
         old_unit_ids = self.waveform_extractor.sorting.unit_ids
         unit_inds = np.flatnonzero(np.in1d(old_unit_ids, unit_ids))
 
-        spike_mask = np.in1d(self.spikes['unit_ind'], unit_inds)
-        new_spike_locations = self._extension_data['spike_locations'][spike_mask]
+        spike_mask = np.in1d(self.spikes["unit_index"], unit_inds)
+        new_spike_locations = self._extension_data["spike_locations"][spike_mask]
         return dict(spike_locations=new_spike_locations)
-        
+
     def _run(self, **job_kwargs):
         """
         This function first transforms the sorting object into a `peaks` numpy array and then
         uses the`sortingcomponents.peak_localization.localize_peaks()` function to triangulate
         spike locations.
         """
         from spikeinterface.sortingcomponents.peak_localization import localize_peaks
+
         job_kwargs = fix_job_kwargs(job_kwargs)
 
         we = self.waveform_extractor
-        
+
         extremum_channel_inds = get_template_extremum_channel(we, outputs="index")
         self.spikes = we.sorting.to_spike_vector(extremum_channel_inds=extremum_channel_inds)
-        
+
         spike_locations = localize_peaks(we.recording, self.spikes, **self._params, **job_kwargs)
-        self._extension_data['spike_locations'] = spike_locations
-    
-    def get_data(self, outputs='concatenated'):
+        self._extension_data["spike_locations"] = spike_locations
+
+    def get_data(self, outputs="concatenated"):
         """
         Get computed spike locations
 
         Parameters
         ----------
         outputs : str, optional
             'concatenated' or 'by_unit', by default 'concatenated'
@@ -73,64 +70,68 @@
         spike_locations : np.array or dict
             The spike locations as a structured array (outputs='concatenated') or
             as a dict with units as key and spike locations as values.
         """
         we = self.waveform_extractor
         sorting = we.sorting
 
-        if outputs == 'concatenated':
-            return self._extension_data['spike_locations']
+        if outputs == "concatenated":
+            return self._extension_data["spike_locations"]
 
-        elif outputs == 'by_unit':
+        elif outputs == "by_unit":
             locations_by_unit = []
             for segment_index in range(self.waveform_extractor.get_num_segments()):
-                i0 =np.searchsorted(self.spikes['segment_ind'], segment_index, side="left")
-                i1 =np.searchsorted(self.spikes['segment_ind'], segment_index, side="right")
-                spikes = self.spikes[i0: i1]
-                locations = self._extension_data['spike_locations'][i0: i1]
-                
+                i0 = np.searchsorted(self.spikes["segment_index"], segment_index, side="left")
+                i1 = np.searchsorted(self.spikes["segment_index"], segment_index, side="right")
+                spikes = self.spikes[i0:i1]
+                locations = self._extension_data["spike_locations"][i0:i1]
+
                 locations_by_unit.append({})
                 for unit_ind, unit_id in enumerate(sorting.unit_ids):
-                    mask = spikes['unit_ind'] == unit_ind
+                    mask = spikes["unit_index"] == unit_ind
                     locations_by_unit[segment_index][unit_id] = locations[mask]
             return locations_by_unit
 
     @staticmethod
     def get_extension_function():
         return compute_spike_locations
 
 
 WaveformExtractor.register_extension(SpikeLocationsCalculator)
 
 
-def compute_spike_locations(waveform_extractor, load_if_exists=False, 
-                            ms_before=1., ms_after=1.5, 
-                            method='center_of_mass',
-                            method_kwargs={},
-                            outputs='concatenated',
-                            **job_kwargs):
+def compute_spike_locations(
+    waveform_extractor,
+    load_if_exists=False,
+    ms_before=0.5,
+    ms_after=0.5,
+    method="center_of_mass",
+    method_kwargs={},
+    outputs="concatenated",
+    **job_kwargs,
+):
     """
     Localize spikes in 2D or 3D with several methods given the template.
 
     Parameters
     ----------
     waveform_extractor : WaveformExtractor
         A waveform extractor object.
     load_if_exists : bool, default: False
         Whether to load precomputed spike locations, if they already exist.
     ms_before : float
         The left window, before a peak, in milliseconds.
     ms_after : float
         The right window, after a peak, in milliseconds.
     method : str
-        'center_of_mass' / 'monopolar_triangulation'
-    method_kwargs : dict 
+        'center_of_mass' / 'monopolar_triangulation' / 'grid_convolution'
+    method_kwargs : dict
         Other kwargs depending on the method.
-    outputs : str 
-        'numpy' (default) / 'numpy_dtype' / 'dict'
+    outputs : str
+        'concatenated' (default) / 'by_unit'
     {}
 
     Returns
     -------
     spike_locations: np.array or list of dict
         The spike locations.
             - If 'concatenated' all locations for all spikes and all units are concatenated
@@ -138,13 +139,13 @@
     """
     if load_if_exists and waveform_extractor.is_extension(SpikeLocationsCalculator.extension_name):
         slc = waveform_extractor.load_extension(SpikeLocationsCalculator.extension_name)
     else:
         slc = SpikeLocationsCalculator(waveform_extractor)
         slc.set_params(ms_before=ms_before, ms_after=ms_after, method=method, method_kwargs=method_kwargs)
         slc.run(**job_kwargs)
-    
+
     locs = slc.get_data(outputs=outputs)
     return locs
 
 
 compute_spike_locations.__doc__.format(_shared_job_kwargs_doc)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/postprocessing/template_metrics.py` & `spikeinterface-0.98.0/src/spikeinterface/postprocessing/template_metrics.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,19 +1,15 @@
 """
 Functions based on
 https://github.com/AllenInstitute/ecephys_spike_sorting/blob/master/ecephys_spike_sorting/modules/mean_waveforms/waveform_metrics.py
 22/04/2020
 """
 import numpy as np
-import pandas as pd
 from copy import deepcopy
 
-import scipy.stats
-from scipy.signal import resample_poly
-
 from ..core import WaveformExtractor
 from ..core.template_tools import get_template_extremum_channel
 from ..core.waveform_extractor import BaseWaveformExtractorExtension
 import warnings
 
 
 def get_template_metric_names():
@@ -24,65 +20,67 @@
     """Class to compute template metrics of waveform shapes.
 
     Parameters
     ----------
     waveform_extractor: WaveformExtractor
         The waveform extractor object
     """
-    extension_name = 'template_metrics'
+
+    extension_name = "template_metrics"
 
     def __init__(self, waveform_extractor):
         BaseWaveformExtractorExtension.__init__(self, waveform_extractor)
 
-    def _set_params(self, metric_names=None, peak_sign='neg', 
-                    upsampling_factor=10, sparsity=None,
-                    window_slope_ms=0.7):
-
+    def _set_params(self, metric_names=None, peak_sign="neg", upsampling_factor=10, sparsity=None, window_slope_ms=0.7):
         if metric_names is None:
             metric_names = get_template_metric_names()
 
-        params = dict(metric_names=[str(name) for name in metric_names],
-                      sparsity=sparsity,
-                      peak_sign=peak_sign,
-                      upsampling_factor=int(upsampling_factor),
-                      window_slope_ms=float(window_slope_ms))
+        params = dict(
+            metric_names=[str(name) for name in metric_names],
+            sparsity=sparsity,
+            peak_sign=peak_sign,
+            upsampling_factor=int(upsampling_factor),
+            window_slope_ms=float(window_slope_ms),
+        )
 
         return params
 
     def _select_extension_data(self, unit_ids):
         # filter metrics dataframe
-        new_metrics = self._extension_data['metrics'].loc[np.array(unit_ids)]
+        new_metrics = self._extension_data["metrics"].loc[np.array(unit_ids)]
         return dict(metrics=new_metrics)
-        
+
     def _run(self):
-        metric_names = self._params['metric_names']
-        sparsity = self._params['sparsity']
-        peak_sign = self._params['peak_sign']
-        upsampling_factor = self._params['upsampling_factor']
+        import pandas as pd
+        from scipy.signal import resample_poly
+
+        metric_names = self._params["metric_names"]
+        sparsity = self._params["sparsity"]
+        peak_sign = self._params["peak_sign"]
+        upsampling_factor = self._params["upsampling_factor"]
         unit_ids = self.waveform_extractor.sorting.unit_ids
         sampling_frequency = self.waveform_extractor.sampling_frequency
 
         if sparsity is None:
-            extremum_channels_ids = get_template_extremum_channel(self.waveform_extractor, 
-                                                                  peak_sign=peak_sign,
-                                                                  outputs='id')
+            extremum_channels_ids = get_template_extremum_channel(
+                self.waveform_extractor, peak_sign=peak_sign, outputs="id"
+            )
 
-            template_metrics = pd.DataFrame(
-                index=unit_ids, columns=metric_names)
+            template_metrics = pd.DataFrame(index=unit_ids, columns=metric_names)
         else:
             extremum_channels_ids = sparsity.unit_id_to_channel_ids
             index_unit_ids = []
             index_channel_ids = []
             for unit_id, sparse_channels in extremum_channels_ids.items():
                 index_unit_ids += [unit_id] * len(sparse_channels)
                 index_channel_ids += list(sparse_channels)
-            multi_index = pd.MultiIndex.from_tuples(list(zip(index_unit_ids, index_channel_ids)),
-                                                    names=["unit_id", "channel_id"])
-            template_metrics = pd.DataFrame(
-                index=multi_index, columns=metric_names)
+            multi_index = pd.MultiIndex.from_tuples(
+                list(zip(index_unit_ids, index_channel_ids)), names=["unit_id", "channel_id"]
+            )
+            template_metrics = pd.DataFrame(index=multi_index, columns=metric_names)
 
         all_templates = self.waveform_extractor.get_all_templates()
         for unit_index, unit_id in enumerate(unit_ids):
             template_all_chans = all_templates[unit_index]
             chan_ids = np.array(extremum_channels_ids[unit_id])
             if chan_ids.ndim == 0:
                 chan_ids = [chan_ids]
@@ -91,57 +89,62 @@
 
             for i, template_single in enumerate(template.T):
                 if sparsity is None:
                     index = unit_id
                 else:
                     index = (unit_id, chan_ids[i])
                 if upsampling_factor > 1:
-                    assert isinstance(
-                        upsampling_factor, (int, np.integer)), "'upsample' must be an integer"
-                    template_upsampled = resample_poly(
-                        template_single, up=upsampling_factor, down=1)
+                    assert isinstance(upsampling_factor, (int, np.integer)), "'upsample' must be an integer"
+                    template_upsampled = resample_poly(template_single, up=upsampling_factor, down=1)
                     sampling_frequency_up = upsampling_factor * sampling_frequency
                 else:
                     template_upsampled = template_single
                     sampling_frequency_up = sampling_frequency
 
                 for metric_name in metric_names:
                     func = _metric_name_to_func[metric_name]
-                    value = func(template_upsampled,
-                                 sampling_frequency=sampling_frequency_up,
-                                 window_ms=self._params['window_slope_ms'])
+                    value = func(
+                        template_upsampled,
+                        sampling_frequency=sampling_frequency_up,
+                        window_ms=self._params["window_slope_ms"],
+                    )
                     template_metrics.at[index, metric_name] = value
 
-        self._extension_data['metrics'] = template_metrics
+        self._extension_data["metrics"] = template_metrics
 
     def get_data(self):
         """
         Get the computed metrics.
-        
+
         Returns
         -------
         metrics : pd.DataFrame
             Dataframe with template metrics
         """
         msg = "Template metrics are not computed. Use the 'run()' function."
-        assert self._extension_data['metrics'] is not None, msg
-        return self._extension_data['metrics']
+        assert self._extension_data["metrics"] is not None, msg
+        return self._extension_data["metrics"]
 
     @staticmethod
     def get_extension_function():
         return compute_template_metrics
 
 
 WaveformExtractor.register_extension(TemplateMetricsCalculator)
 
 
-def compute_template_metrics(waveform_extractor, load_if_exists=False,
-                             metric_names=None, peak_sign='neg',
-                             upsampling_factor=10, sparsity=None,
-                             window_slope_ms=0.7):
+def compute_template_metrics(
+    waveform_extractor,
+    load_if_exists=False,
+    metric_names=None,
+    peak_sign="neg",
+    upsampling_factor=10,
+    sparsity=None,
+    window_slope_ms=0.7,
+):
     """
     Compute template metrics including:
         * peak_to_valley
         * peak_trough_ratio
         * halfwidth
         * repolarization_slope
         * recovery_slope
@@ -172,17 +175,21 @@
         If 'sparsity' is None, the index is the unit_id.
         If 'sparsity' is given, the index is a multi-index (unit_id, channel_id)
     """
     if load_if_exists and waveform_extractor.is_extension(TemplateMetricsCalculator.extension_name):
         tmc = waveform_extractor.load_extension(TemplateMetricsCalculator.extension_name)
     else:
         tmc = TemplateMetricsCalculator(waveform_extractor)
-        tmc.set_params(metric_names=metric_names, peak_sign=peak_sign,
-                       upsampling_factor=upsampling_factor, sparsity=sparsity,
-                       window_slope_ms=window_slope_ms)
+        tmc.set_params(
+            metric_names=metric_names,
+            peak_sign=peak_sign,
+            upsampling_factor=upsampling_factor,
+            sparsity=sparsity,
+            window_slope_ms=window_slope_ms,
+        )
         tmc.run()
 
     metrics = tmc.get_data()
 
     return metrics
 
 
@@ -227,16 +234,16 @@
     if peak_idx == 0:
         return np.nan
 
     trough_val = template[trough_idx]
     # threshold is half of peak heigth (assuming baseline is 0)
     threshold = 0.5 * trough_val
 
-    cpre_idx, = np.where(template[:trough_idx] < threshold)
-    cpost_idx, = np.where(template[trough_idx:] < threshold)
+    (cpre_idx,) = np.where(template[:trough_idx] < threshold)
+    (cpost_idx,) = np.where(template[trough_idx:] < threshold)
 
     if len(cpre_idx) == 0 or len(cpost_idx) == 0:
         hw = np.nan
 
     else:
         # last occurence of template lower than thr, before peak
         cross_pre_pk = cpre_idx[0] - 1
@@ -263,25 +270,26 @@
     sampling_frequency = kwargs["sampling_frequency"]
 
     times = np.arange(template.shape[0]) / sampling_frequency
 
     if trough_idx == 0:
         return np.nan
 
-    rtrn_idx, = np.nonzero(template[trough_idx:] >= 0)
+    (rtrn_idx,) = np.nonzero(template[trough_idx:] >= 0)
     if len(rtrn_idx) == 0:
         return np.nan
     # first time after  trough, where template is at baseline
     return_to_base_idx = rtrn_idx[0] + trough_idx
 
     if return_to_base_idx - trough_idx < 3:
         return np.nan
 
-    res = scipy.stats.linregress(
-        times[trough_idx:return_to_base_idx], template[trough_idx:return_to_base_idx])
+    import scipy.stats
+
+    res = scipy.stats.linregress(times[trough_idx:return_to_base_idx], template[trough_idx:return_to_base_idx])
     return res.slope
 
 
 def get_recovery_slope(template, window_ms=0.7, **kwargs):
     """
     Return the recovery slope of input waveforms. After repolarization,
     the neuron hyperpolarizes untill it peaks. The recovery slope is the
@@ -298,31 +306,34 @@
 
     times = np.arange(template.shape[0]) / sampling_frequency
 
     if peak_idx == 0:
         return np.nan
     max_idx = int(peak_idx + ((window_ms / 1000) * sampling_frequency))
     max_idx = np.min([max_idx, template.shape[0]])
-    res = scipy.stats.linregress(
-        times[peak_idx:max_idx], template[peak_idx:max_idx])
+
+    import scipy.stats
+
+    res = scipy.stats.linregress(times[peak_idx:max_idx], template[peak_idx:max_idx])
     return res.slope
 
 
 _metric_name_to_func = {
-    'peak_to_valley': get_peak_to_valley,
-    'peak_trough_ratio': get_peak_trough_ratio,
-    'half_width': get_half_width,
-    'repolarization_slope': get_repolarization_slope,
-    'recovery_slope': get_recovery_slope,
+    "peak_to_valley": get_peak_to_valley,
+    "peak_trough_ratio": get_peak_trough_ratio,
+    "half_width": get_half_width,
+    "repolarization_slope": get_repolarization_slope,
+    "recovery_slope": get_recovery_slope,
 }
 
 
-
 # back-compatibility
 def calculate_template_metrics(*args, **kwargs):
-    warnings.warn("The 'calculate_template_metrics' function is deprecated. "
-                  "Use 'compute_template_metrics' instead",
-                   DeprecationWarning, stacklevel=2)
+    warnings.warn(
+        "The 'calculate_template_metrics' function is deprecated. " "Use 'compute_template_metrics' instead",
+        DeprecationWarning,
+        stacklevel=2,
+    )
     return compute_template_metrics(*args, **kwargs)
 
 
 calculate_template_metrics.__doc__ = compute_template_metrics.__doc__
```

### Comparing `spikeinterface-0.97.1/spikeinterface/postprocessing/template_similarity.py` & `spikeinterface-0.98.0/src/spikeinterface/postprocessing/template_similarity.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,94 +1,93 @@
 import numpy as np
 from ..core import WaveformExtractor
 from ..core.waveform_extractor import BaseWaveformExtractorExtension
 
 
 class TemplateSimilarityCalculator(BaseWaveformExtractorExtension):
     """Compute similarity between templates with several methods.
-    
+
     Parameters
     ----------
     waveform_extractor: WaveformExtractor
         A waveform extractor object
     """
-    extension_name = 'similarity'
+
+    extension_name = "similarity"
 
     def __init__(self, waveform_extractor):
         BaseWaveformExtractorExtension.__init__(self, waveform_extractor)
 
-    def _set_params(self, method='cosine_similarity'):
-
+    def _set_params(self, method="cosine_similarity"):
         params = dict(method=method)
 
         return params
-    
+
     def _select_extension_data(self, unit_ids):
         # filter metrics dataframe
         unit_indices = self.waveform_extractor.sorting.ids_to_indices(unit_ids)
-        new_similarity = self._extension_data['similarity'][unit_indices][:, unit_indices]
+        new_similarity = self._extension_data["similarity"][unit_indices][:, unit_indices]
         return dict(similarity=new_similarity)
 
     def _run(self):
-        similarity = _compute_template_similarity(self.waveform_extractor, method=self._params['method'])
-        self._extension_data['similarity'] = similarity
+        similarity = _compute_template_similarity(self.waveform_extractor, method=self._params["method"])
+        self._extension_data["similarity"] = similarity
 
     def get_data(self):
         """
         Get the computed similarity.
-        
+
         Returns
         -------
         similarity : 2d np.array
             2d matrix with computed similarity values.
         """
         msg = "Template similarity is not computed. Use the 'run()' function."
-        assert self._extension_data['similarity'] is not None, msg
-        return self._extension_data['similarity']
+        assert self._extension_data["similarity"] is not None, msg
+        return self._extension_data["similarity"]
 
     @staticmethod
     def get_extension_function():
         return compute_template_similarity
 
 
 WaveformExtractor.register_extension(TemplateSimilarityCalculator)
 
 
-def _compute_template_similarity(waveform_extractor, 
-                                 load_if_exists=False,
-                                 method='cosine_similarity',
-                                 waveform_extractor_other=None):
+def _compute_template_similarity(
+    waveform_extractor, load_if_exists=False, method="cosine_similarity", waveform_extractor_other=None
+):
     import sklearn.metrics.pairwise
 
     templates = waveform_extractor.get_all_templates()
     s = templates.shape
-    if method == 'cosine_similarity':
+    if method == "cosine_similarity":
         templates_flat = templates.reshape(s[0], -1)
         if waveform_extractor_other is not None:
             templates_other = waveform_extractor_other.get_all_templates()
             s_other = templates_other.shape
             templates_other_flat = templates_other.reshape(s_other[0], -1)
-            assert len(templates_flat[0]) == len(templates_other_flat[0]), ("Templates from second WaveformExtractor "
-                                                                            "don't have the correct shape!")
+            assert len(templates_flat[0]) == len(templates_other_flat[0]), (
+                "Templates from second WaveformExtractor " "don't have the correct shape!"
+            )
         else:
             templates_other_flat = None
         similarity = sklearn.metrics.pairwise.cosine_similarity(templates_flat, templates_other_flat)
     # elif method == '':
     else:
-        raise ValueError(f'compute_template_similarity(method {method}) not exists')
+        raise ValueError(f"compute_template_similarity(method {method}) not exists")
 
     return similarity
 
 
-def compute_template_similarity(waveform_extractor, 
-                                load_if_exists=False,
-                                method='cosine_similarity',
-                                waveform_extractor_other=None):
+def compute_template_similarity(
+    waveform_extractor, load_if_exists=False, method="cosine_similarity", waveform_extractor_other=None
+):
     """Compute similarity between templates with several methods.
-    
+
     Parameters
     ----------
     waveform_extractor: WaveformExtractor
         A waveform extractor object
     load_if_exists : bool, default: False
         Whether to load precomputed similarity, if is already exists.
     method: str
@@ -110,35 +109,33 @@
             tmc.run()
         similarity = tmc.get_data()
         return similarity
     else:
         return _compute_template_similarity(waveform_extractor, waveform_extractor_other, method)
 
 
-
-def check_equal_template_with_distribution_overlap(waveforms0, waveforms1,
-                                                   template0=None, template1=None,
-                                                   num_shift = 2, quantile_limit=0.8, 
-                                                   return_shift=False):
+def check_equal_template_with_distribution_overlap(
+    waveforms0, waveforms1, template0=None, template1=None, num_shift=2, quantile_limit=0.8, return_shift=False
+):
     """
     Given 2 waveforms sets, check if they come from the same distribution.
-    
+
     This is computed with a simple trick:
     It project all waveforms from each cluster on the normed vector going from
     one template to another, if the cluster are well separate enought we should
     have one distribution around 0 and one distribution around .
     If the distribution overlap too much then then come from the same distribution.
-    
+
     Done by samuel Garcia with an idea of Crhistophe Pouzat.
     This is used internally by tridesclous for auto merge step.
-    
+
     Can be also used as a distance metrics between 2 clusters.
 
     waveforms0 and waveforms1 have to be spasifyed outside this function.
-    
+
     This is done with a combinaison of shift bewteen the 2 cluster to also check
     if cluster are similar with a sample shift.
 
     Parameters
     ----------
     waveforms0, waveforms1: numpy array
         Shape (num_spikes, num_samples, num_chans)
@@ -152,48 +149,47 @@
         The quantile overlap limit.
 
     Returns
     -------
     equal: bool
         equal or not
     """
-    
+
     assert waveforms0.shape[1] == waveforms1.shape[1]
     assert waveforms0.shape[2] == waveforms1.shape[2]
-    
+
     if template0 is None:
         template0 = np.mean(waveforms0, axis=0)
 
     if template1 is None:
         template1 = np.mean(waveforms1, axis=0)
-    
+
     template0_ = template0[num_shift:-num_shift, :]
     width = template0_.shape[0]
 
     wfs0 = waveforms0[:, num_shift:-num_shift, :].copy()
 
     equal = False
     final_shift = None
-    for shift in range(num_shift*2+1):
-
-        template1_ = template1[shift:width+shift, :]
-        vector_0_1 = (template1_ - template0_)
+    for shift in range(num_shift * 2 + 1):
+        template1_ = template1[shift : width + shift, :]
+        vector_0_1 = template1_ - template0_
         vector_0_1 /= np.sum(vector_0_1**2)
 
-        wfs1 = waveforms1[:, shift:width+shift, :].copy()
-        
-        scalar_product0 = np.sum((wfs0 - template0_[np.newaxis,:,:]) * vector_0_1[np.newaxis,:,:], axis=(1,2))
-        scalar_product1 = np.sum((wfs1 - template0_[np.newaxis,:,:]) * vector_0_1[np.newaxis,:,:], axis=(1,2))
-        
+        wfs1 = waveforms1[:, shift : width + shift, :].copy()
+
+        scalar_product0 = np.sum((wfs0 - template0_[np.newaxis, :, :]) * vector_0_1[np.newaxis, :, :], axis=(1, 2))
+        scalar_product1 = np.sum((wfs1 - template0_[np.newaxis, :, :]) * vector_0_1[np.newaxis, :, :], axis=(1, 2))
+
         l0 = np.quantile(scalar_product0, quantile_limit)
         l1 = np.quantile(scalar_product1, 1 - quantile_limit)
-        
+
         equal = l0 >= l1
-        
+
         if equal:
             final_shift = shift - num_shift
             break
-    
+
     if return_shift:
-            return equal, final_shift
+        return equal, final_shift
     else:
         return equal
```

### Comparing `spikeinterface-0.97.1/spikeinterface/postprocessing/template_tools.py` & `spikeinterface-0.98.0/src/spikeinterface/postprocessing/template_tools.py`

 * *Files 7% similar despite different names*

```diff
@@ -2,32 +2,38 @@
 
 import warnings
 
 import spikeinterface.core.template_tools as tt
 
 
 def _warn():
-    warnings.warn("The spikeinterface.postprocessing.template_tools is submodule is deprecated."
-                  "Use spikeinterface.core.template_tools instead",
-                  DeprecationWarning, stacklevel=2)
+    warnings.warn(
+        "The spikeinterface.postprocessing.template_tools is submodule is deprecated."
+        "Use spikeinterface.core.template_tools instead",
+        DeprecationWarning,
+        stacklevel=2,
+    )
 
 
 def get_template_amplitudes(*args, **kwargs):
     _warn()
     return tt.get_template_amplitudes(*args, **kwargs)
 
+
 def get_template_extremum_channel(*args, **kwargs):
     _warn()
     return tt.get_template_extremum_channel(*args, **kwargs)
 
 
 def get_template_channel_sparsity(*args, **kwargs):
     _warn()
     return tt.get_template_channel_sparsity(*args, **kwargs)
 
+
 def get_template_extremum_channel_peak_shift(*args, **kwargs):
     _warn()
     return tt.get_template_extremum_channel_peak_shift(*args, **kwargs)
 
+
 def get_template_extremum_amplitude(*args, **kwargs):
     _warn()
     return tt.get_template_extremum_amplitude(*args, **kwargs)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/postprocessing/unit_localization.py` & `spikeinterface-0.98.0/src/spikeinterface/postprocessing/unit_localization.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,121 +1,121 @@
 import warnings
 
 import numpy as np
-import scipy.optimize
 
-from scipy.spatial.distance import cdist
 
 try:
     import numba
+
     HAVE_NUMBA = True
 except ImportError:
     HAVE_NUMBA = False
 
 from ..core import compute_sparsity
 from ..core.waveform_extractor import WaveformExtractor, BaseWaveformExtractorExtension
-
+from ..core.template_tools import get_template_extremum_channel
 
 
 dtype_localize_by_method = {
-    'center_of_mass': [('x', 'float64'), ('y', 'float64')],
-    'peak_channel': [('x', 'float64'), ('y', 'float64')],
-    'monopolar_triangulation': [('x', 'float64'), ('y', 'float64'), ('z', 'float64'), ('alpha', 'float64')],
+    "center_of_mass": [("x", "float64"), ("y", "float64")],
+    "grid_convolution": [("x", "float64"), ("y", "float64")],
+    "peak_channel": [("x", "float64"), ("y", "float64")],
+    "monopolar_triangulation": [("x", "float64"), ("y", "float64"), ("z", "float64"), ("alpha", "float64")],
 }
 
 possible_localization_methods = list(dtype_localize_by_method.keys())
 
 
 class UnitLocationsCalculator(BaseWaveformExtractorExtension):
     """
     Comput unit locations from WaveformExtractor.
-    
+
     Parameters
     ----------
     waveform_extractor: WaveformExtractor
         A waveform extractor object
     """
-    extension_name = 'unit_locations'
+
+    extension_name = "unit_locations"
 
     def __init__(self, waveform_extractor):
         BaseWaveformExtractorExtension.__init__(self, waveform_extractor)
 
-    def _set_params(self, method='center_of_mass', method_kwargs={}):
-
-        params = dict(method=method,
-                      method_kwargs=method_kwargs)
+    def _set_params(self, method="center_of_mass", method_kwargs={}):
+        params = dict(method=method, method_kwargs=method_kwargs)
         return params
 
     def _select_extension_data(self, unit_ids):
         unit_inds = self.waveform_extractor.sorting.ids_to_indices(unit_ids)
-        new_unit_location = self._extension_data['unit_locations'][unit_inds]
+        new_unit_location = self._extension_data["unit_locations"][unit_inds]
         return dict(unit_locations=new_unit_location)
 
     def _run(self, **job_kwargs):
-        method = self._params['method']
-        method_kwargs = self._params['method_kwargs']
-        
+        method = self._params["method"]
+        method_kwargs = self._params["method_kwargs"]
+
         assert method in possible_localization_methods
 
-        if method == 'center_of_mass':
-            unit_location = compute_center_of_mass(self.waveform_extractor,  **method_kwargs)
-        elif method == 'monopolar_triangulation':
-            unit_location = compute_monopolar_triangulation(self.waveform_extractor,  **method_kwargs)
-        self._extension_data['unit_locations'] = unit_location
+        if method == "center_of_mass":
+            unit_location = compute_center_of_mass(self.waveform_extractor, **method_kwargs)
+        elif method == "grid_convolution":
+            unit_location = compute_grid_convolution(self.waveform_extractor, **method_kwargs)
+        elif method == "monopolar_triangulation":
+            unit_location = compute_monopolar_triangulation(self.waveform_extractor, **method_kwargs)
+        self._extension_data["unit_locations"] = unit_location
 
-    def get_data(self, outputs='numpy'):
+    def get_data(self, outputs="numpy"):
         """
         Get the computed unit locations.
 
         Parameters
         ----------
         outputs : str, optional
             'numpy' or 'by_unit', by default 'numpy'
 
         Returns
         -------
         unit_locations : np.array or dict
             The unit locations as a Nd array (outputs='numpy') or
             as a dict with units as key and locations as values.
         """
-        if outputs == 'numpy':
-            return self._extension_data['unit_locations']
+        if outputs == "numpy":
+            return self._extension_data["unit_locations"]
 
-        elif outputs == 'by_unit':
+        elif outputs == "by_unit":
             locations_by_unit = {}
             for unit_ind, unit_id in enumerate(self.waveform_extractor.sorting.unit_ids):
-                locations_by_unit[unit_id] = self._extension_data['unit_locations'][unit_ind]
+                locations_by_unit[unit_id] = self._extension_data["unit_locations"][unit_ind]
             return locations_by_unit
 
     @staticmethod
     def get_extension_function():
         return compute_unit_locations
 
 
 WaveformExtractor.register_extension(UnitLocationsCalculator)
 
 
-def compute_unit_locations(waveform_extractor, 
-                           load_if_exists=False,
-                           method='center_of_mass', 
-                           outputs='numpy', **method_kwargs):
+def compute_unit_locations(
+    waveform_extractor, load_if_exists=False, method="center_of_mass", outputs="numpy", **method_kwargs
+):
     """
     Localize units in 2D or 3D with several methods given the template.
 
     Parameters
     ----------
     waveform_extractor: WaveformExtractor
         A waveform extractor object.
     load_if_exists : bool, default: False
         Whether to load precomputed unit locations, if they already exist.
     method: str
-        'center_of_mass' / 'monopolar_triangulation'
-    outputs: str 
+        'center_of_mass' / 'monopolar_triangulation' / 'grid_convolution'
+    outputs: str
         'numpy' (default) / 'by_unit'
-    method_kwargs: 
+    method_kwargs:
         Other kwargs depending on the method.
 
     Returns
     -------
     unit_locations: np.array
         unit location with shape (num_unit, 2) or (num_unit, 3) or (num_unit, 3) (with alpha)
     """
@@ -126,100 +126,111 @@
         ulc.set_params(method=method, method_kwargs=method_kwargs)
         ulc.run()
 
     unit_locations = ulc.get_data(outputs=outputs)
     return unit_locations
 
 
-def make_initial_guess_and_bounds(wf_ptp, local_contact_locations, max_distance_um, initial_z=20):
-
+def make_initial_guess_and_bounds(wf_data, local_contact_locations, max_distance_um, initial_z=20):
     # constant for initial guess and bounds
-    ind_max = np.argmax(wf_ptp)
-    max_ptp = wf_ptp[ind_max]
+    ind_max = np.argmax(wf_data)
+    max_ptp = wf_data[ind_max]
     max_alpha = max_ptp * max_distance_um
 
     # initial guess is the center of mass
-    com = np.sum(wf_ptp[:, np.newaxis] * local_contact_locations, axis=0) / np.sum(wf_ptp)
-    x0 = np.zeros(4, dtype='float32')
+    com = np.sum(wf_data[:, np.newaxis] * local_contact_locations, axis=0) / np.sum(wf_data)
+    x0 = np.zeros(4, dtype="float32")
     x0[:2] = com
     x0[2] = initial_z
-    initial_alpha = np.sqrt(np.sum((com - local_contact_locations[ind_max, :])**2) + initial_z**2) * max_ptp
+    initial_alpha = np.sqrt(np.sum((com - local_contact_locations[ind_max, :]) ** 2) + initial_z**2) * max_ptp
     x0[3] = initial_alpha
 
     # bounds depend on initial guess
-    bounds = ([x0[0] - max_distance_um, x0[1] - max_distance_um, 1, 0],
-              [x0[0] + max_distance_um, x0[1] + max_distance_um, max_distance_um * 10, max_alpha])
+    bounds = (
+        [x0[0] - max_distance_um, x0[1] - max_distance_um, 1, 0],
+        [x0[0] + max_distance_um, x0[1] + max_distance_um, max_distance_um * 10, max_alpha],
+    )
 
     return x0, bounds
 
 
-def solve_monopolar_triangulation(wf_ptp, local_contact_locations, max_distance_um, optimizer):
-    x0, bounds = make_initial_guess_and_bounds(wf_ptp, local_contact_locations, max_distance_um)
+def solve_monopolar_triangulation(wf_data, local_contact_locations, max_distance_um, optimizer):
+    import scipy.optimize
 
-    if optimizer == 'least_square':
-        args = (wf_ptp, local_contact_locations)
+    x0, bounds = make_initial_guess_and_bounds(wf_data, local_contact_locations, max_distance_um)
+
+    if optimizer == "least_square":
+        args = (wf_data, local_contact_locations)
         try:
             output = scipy.optimize.least_squares(estimate_distance_error, x0=x0, bounds=bounds, args=args)
-            return tuple(output['x'])
+            return tuple(output["x"])
         except Exception as e:
             print(f"scipy.optimize.least_squares error: {e}")
             return (np.nan, np.nan, np.nan, np.nan)
 
-    if optimizer == 'minimize_with_log_penality':
+    if optimizer == "minimize_with_log_penality":
         x0 = x0[:3]
         bounds = [(bounds[0][0], bounds[1][0]), (bounds[0][1], bounds[1][1]), (bounds[0][2], bounds[1][2])]
-        maxptp = wf_ptp.max()
-        args = (wf_ptp, local_contact_locations, maxptp)
+        max_data = wf_data.max()
+        args = (wf_data, local_contact_locations, max_data)
         try:
             output = scipy.optimize.minimize(estimate_distance_error_with_log, x0=x0, bounds=bounds, args=args)
             # final alpha
-            q = ptp_at(*output['x'], 1.0, local_contact_locations)
-            alpha = (wf_ptp * q).sum() / np.square(q).sum()
-            return (*output['x'], alpha)
+            q = data_at(*output["x"], 1.0, local_contact_locations)
+            alpha = (wf_data * q).sum() / np.square(q).sum()
+            return (*output["x"], alpha)
         except Exception as e:
             print(f"scipy.optimize.minimize error: {e}")
             return (np.nan, np.nan, np.nan, np.nan)
 
 
 # ----
 # optimizer "least_square"
 
 
-def estimate_distance_error(vec, wf_ptp, local_contact_locations):
+def estimate_distance_error(vec, wf_data, local_contact_locations):
     # vec dims ar (x, y, z amplitude_factor)
     # given that for contact_location x=dim0 + z=dim1 and y is orthogonal to probe
-    dist = np.sqrt(((local_contact_locations - vec[np.newaxis, :2])**2).sum(axis=1) + vec[2]**2)
-    ptp_estimated = vec[3] / dist
-    err = wf_ptp - ptp_estimated
+    dist = np.sqrt(((local_contact_locations - vec[np.newaxis, :2]) ** 2).sum(axis=1) + vec[2] ** 2)
+    data_estimated = vec[3] / dist
+    err = wf_data - data_estimated
     return err
 
 
 # ----
 # optimizer "minimize_with_log_penality"
 
 
-def ptp_at(x, y, z, alpha, local_contact_locations):
+def data_at(x, y, z, alpha, local_contact_locations):
     return alpha / np.sqrt(
-        np.square(x - local_contact_locations[:, 0])
-        + np.square(y - local_contact_locations[:, 1])
-        + np.square(z)
+        np.square(x - local_contact_locations[:, 0]) + np.square(y - local_contact_locations[:, 1]) + np.square(z)
     )
 
 
-def estimate_distance_error_with_log(vec, wf_ptp, local_contact_locations, maxptp):
+def estimate_distance_error_with_log(vec, wf_data, local_contact_locations, max_data):
     x, y, z = vec
-    q = ptp_at(x, y, z, 1.0, local_contact_locations)
-    alpha = (q * wf_ptp / maxptp).sum() / (q * q).sum()
-    err = np.square(wf_ptp / maxptp - ptp_at(x, y, z, alpha, local_contact_locations)).mean() - np.log1p(10.0 * z) / 10000.0
+    q = data_at(x, y, z, 1.0, local_contact_locations)
+    alpha = (q * wf_data / max_data).sum() / (q * q).sum()
+    err = (
+        np.square(wf_data / max_data - data_at(x, y, z, alpha, local_contact_locations)).mean()
+        - np.log1p(10.0 * z) / 10000.0
+    )
     return err
 
 
-def compute_monopolar_triangulation(waveform_extractor, optimizer='minimize_with_log_penality',
-                                    radius_um=50, max_distance_um=1000, return_alpha=False):
-    '''
+def compute_monopolar_triangulation(
+    waveform_extractor,
+    optimizer="minimize_with_log_penality",
+    radius_um=75,
+    max_distance_um=1000,
+    return_alpha=False,
+    enforce_decrease=False,
+    feature="ptp",
+):
+    """
     Localize unit with monopolar triangulation.
     This method is from Julien Boussard, Erdem Varol and Charlie Windolf
     https://www.biorxiv.org/content/10.1101/2021.11.05.467503v1
 
     There are 2 implementations of the 2 optimizer variants:
       * https://github.com/int-brain-lab/spikes_localization_registration/blob/main/localization_pipeline/localizer.py
       * https://github.com/cwindolf/spike-psvae/blob/main/spike_psvae/localization.py
@@ -240,94 +251,238 @@
        2 variants of the method
     radius_um: float
         For channel sparsity
     max_distance_um: float
         to make bounddary in x, y, z and also for alpha
     return_alpha: bool default False
         Return or not the alpha value
+    enforce_decrease : bool (default False)
+        Enforce spatial decreasingness for PTP vectors
+    feature: string in ['ptp', 'energy', 'peak_voltage']
+        The available features to consider for estimating the position via
+        monopolar triangulation are peak-to-peak amplitudes ('ptp', default),
+        energy ('energy', as L2 norm) or voltages at the center of the waveform
+        ('peak_voltage')
 
     Returns
     -------
     unit_location: np.array
         3d or 4d, x, y, z, alpha
         alpha is the amplitude at source estimation
-    '''
-    assert optimizer in ('least_square', 'minimize_with_log_penality')
+    """
+    assert optimizer in ("least_square", "minimize_with_log_penality")
 
+    assert feature in ["ptp", "energy", "peak_voltage"], f"{feature} is not a valid feature"
     unit_ids = waveform_extractor.sorting.unit_ids
 
     contact_locations = waveform_extractor.get_channel_locations()
+    nbefore = waveform_extractor.nbefore
 
-    sparsity = compute_sparsity(waveform_extractor, method='radius', radius_um=radius_um)
-    templates = waveform_extractor.get_all_templates(mode='average')
+    sparsity = compute_sparsity(waveform_extractor, method="radius", radius_um=radius_um)
+    templates = waveform_extractor.get_all_templates(mode="average")
 
-    unit_location = np.zeros((unit_ids.size, 4), dtype='float64')
+    if enforce_decrease:
+        neighbours_mask = np.zeros((templates.shape[0], templates.shape[2]), dtype=bool)
+        for i, unit_id in enumerate(unit_ids):
+            chan_inds = sparsity.unit_id_to_channel_indices[unit_id]
+            neighbours_mask[i, chan_inds] = True
+        enforce_decrease_radial_parents = make_radial_order_parents(contact_locations, neighbours_mask)
+        best_channels = get_template_extremum_channel(waveform_extractor, outputs="index")
+
+    unit_location = np.zeros((unit_ids.size, 4), dtype="float64")
     for i, unit_id in enumerate(unit_ids):
         chan_inds = sparsity.unit_id_to_channel_indices[unit_id]
         local_contact_locations = contact_locations[chan_inds, :]
 
         # wf is (nsample, nchan) - chann is only nieghboor
-        wf = templates[i, :, :]
-        wf_ptp = wf[:, chan_inds].ptp(axis=0)
-        unit_location[i] = solve_monopolar_triangulation(wf_ptp, local_contact_locations, max_distance_um, optimizer)
+        wf = templates[i, :, :][:, chan_inds]
+        if feature == "ptp":
+            wf_data = wf.ptp(axis=0)
+        elif feature == "energy":
+            wf_data = np.linalg.norm(wf, axis=0)
+        elif feature == "peak_voltage":
+            wf_data = np.abs(wf[nbefore])
+
+        # if enforce_decrease:
+        #    enforce_decrease_shells_data(
+        #        wf_data, best_channels[unit_id], enforce_decrease_radial_parents, in_place=True
+        #    )
+
+        unit_location[i] = solve_monopolar_triangulation(wf_data, local_contact_locations, max_distance_um, optimizer)
 
     if not return_alpha:
         unit_location = unit_location[:, :3]
 
     return unit_location
 
 
-def compute_center_of_mass(waveform_extractor, peak_sign='neg', radius_um=50):
-    '''
+def compute_center_of_mass(waveform_extractor, peak_sign="neg", radius_um=75, feature="ptp"):
+    """
     Computes the center of mass (COM) of a unit based on the template amplitudes.
 
     Parameters
     ----------
     waveform_extractor: WaveformExtractor
         The waveform extractor
     peak_sign: str
         Sign of the template to compute best channels ('neg', 'pos', 'both')
-    num_channels: int
-        Number of channels used to compute COM
+    radius_um: float
+        Radius to consider in order to estimate the COM
+    feature: str ['ptp', 'mean', 'energy', 'peak_voltage']
+        Feature to consider for computation. Default is 'ptp'
 
     Returns
     -------
     unit_location: np.array
-    '''
+    """
     unit_ids = waveform_extractor.sorting.unit_ids
 
     recording = waveform_extractor.recording
     contact_locations = recording.get_channel_locations()
 
-    # TODO
-    sparsity = compute_sparsity(waveform_extractor, peak_sign=peak_sign, method='radius', radius_um=radius_um)
-    templates = waveform_extractor.get_all_templates(mode='average')
+    assert feature in ["ptp", "mean", "energy", "peak_voltage"], f"{feature} is not a valid feature"
+
+    sparsity = compute_sparsity(waveform_extractor, peak_sign=peak_sign, method="radius", radius_um=radius_um)
+    templates = waveform_extractor.get_all_templates(mode="average")
 
-    unit_location = np.zeros((unit_ids.size, 2), dtype='float64')
+    unit_location = np.zeros((unit_ids.size, 2), dtype="float64")
     for i, unit_id in enumerate(unit_ids):
         chan_inds = sparsity.unit_id_to_channel_indices[unit_id]
         local_contact_locations = contact_locations[chan_inds, :]
 
         wf = templates[i, :, :]
 
-        wf_ptp = wf[:, chan_inds].ptp(axis=0)
+        if feature == "ptp":
+            wf_data = (wf[:, chan_inds]).ptp(axis=0)
+        elif feature == "mean":
+            wf_data = (wf[:, chan_inds]).mean(axis=0)
+        elif feature == "energy":
+            wf_data = np.linalg.norm(wf[:, chan_inds], axis=0)
+        elif feature == "peak_voltage":
+            wf_data = wf[waveform_extractor.nbefore, chan_inds]
 
         # center of mass
-        com = np.sum(wf_ptp[:, np.newaxis] * local_contact_locations, axis=0) / np.sum(wf_ptp)
+        com = np.sum(wf_data[:, np.newaxis] * local_contact_locations, axis=0) / np.sum(wf_data)
         unit_location[i, :] = com
 
     return unit_location
 
 
+@np.errstate(divide="ignore", invalid="ignore")
+def compute_grid_convolution(
+    waveform_extractor,
+    peak_sign="neg",
+    radius_um=40.0,
+    upsampling_um=5,
+    sigma_um=np.linspace(5.0, 25.0, 5),
+    sigma_ms=0.25,
+    margin_um=50,
+    prototype=None,
+    percentile=10,
+    sparsity_threshold=0.01,
+):
+    """
+    Estimate the positions of the templates from a large grid of fake templates
+
+    Parameters
+    ----------
+    waveform_extractor: WaveformExtractor
+        The waveform extractor
+    peak_sign: str
+        Sign of the template to compute best channels ('neg', 'pos', 'both')
+    radius_um: float
+        Radius to consider for the fake templates
+    upsampling_um: float
+        Upsampling resolution for the grid of templates
+    sigma_um: np.array
+        Spatial decays of the fake templates
+    sigma_ms: float
+        The temporal decay of the fake templates
+    margin_um: float
+        The margin for the grid of fake templates
+    prototype: np.array
+        Fake waveforms for the templates. If None, generated as Gaussian
+    percentile: float (default 10)
+        The percentage  in [0, 100] of the best scalar products kept to
+        estimate the position
+    sparsity_threshold: float (default 0.01)
+        The sparsity threshold (in 0-1) below which weights should be considered as 0.
+    Returns
+    -------
+    unit_location: np.array
+    """
+
+    contact_locations = waveform_extractor.get_channel_locations()
+
+    nbefore = waveform_extractor.nbefore
+    nafter = waveform_extractor.nafter
+    fs = waveform_extractor.sampling_frequency
+    percentile = 100 - percentile
+    assert 0 <= percentile <= 100, "Percentile should be in [0, 100]"
+    assert 0 <= sparsity_threshold <= 1, "sparsity_threshold should be in [0, 1]"
+
+    time_axis = np.arange(-nbefore, nafter) * 1000 / fs
+    if prototype is None:
+        prototype = np.exp(-(time_axis**2) / (2 * (sigma_ms**2)))
+
+    prototype = prototype[:, np.newaxis]
+
+    template_positions, weights, nearest_template_mask = get_grid_convolution_templates_and_weights(
+        contact_locations, radius_um, upsampling_um, sigma_um, margin_um
+    )
+
+    templates = waveform_extractor.get_all_templates(mode="average")
+
+    peak_channels = get_template_extremum_channel(waveform_extractor, peak_sign, outputs="index")
+    unit_ids = waveform_extractor.sorting.unit_ids
+
+    weights_sparsity_mask = weights > sparsity_threshold
+
+    unit_location = np.zeros((unit_ids.size, 2), dtype="float64")
+    for i, unit_id in enumerate(unit_ids):
+        main_chan = peak_channels[unit_id]
+        wf = templates[i, :, :]
+        amplitude = wf[nbefore, main_chan]
+        nearest_templates = nearest_template_mask[main_chan, :]
+
+        channel_mask = np.sum(weights_sparsity_mask[:, :, nearest_templates], axis=(0, 2)) > 0
+        num_templates = np.sum(nearest_templates)
+        global_products = ((wf[:, channel_mask] / amplitude) * prototype).sum(axis=0)
+
+        dot_products = np.zeros((weights.shape[0], num_templates), dtype=np.float32)
+        for count in range(weights.shape[0]):
+            w = weights[count, :, :][channel_mask, :][:, nearest_templates]
+            # w = w / np.sum(w, axis=0)[np.newaxis, None]
+            # w[np.isnan(w)] = 0.
+            dot_products[count, :] = np.dot(global_products, w)
+
+        dot_products = np.maximum(0, dot_products)
+        if percentile < 100:
+            thresholds = np.percentile(dot_products, percentile, axis=0)
+            dot_products[dot_products < thresholds[np.newaxis, :]] = 0
+
+        found_positions = np.zeros(2, dtype=np.float32)
+        scalar_products = np.zeros(num_templates, dtype=np.float32)
+        for count in range(weights.shape[0]):
+            scalar_products += dot_products[count]
+            found_positions += np.dot(dot_products[count], template_positions[nearest_templates])
+
+        unit_location[i, :] = found_positions / scalar_products.sum()
+
+    return unit_location
+
+
 # ---
 # waveform cleaning for localization. could be moved to another file
 
 
 def make_shell(channel, geom, n_jumps=1):
     """See make_shells"""
+    from scipy.spatial.distance import cdist
+
     pt = geom[channel]
     dists = cdist([pt], geom).ravel()
     radius = np.unique(dists)[1 : n_jumps + 1][-1]
     return np.setdiff1d(np.flatnonzero(dists <= radius + 1e-8), [channel])
 
 
 def make_shells(geom, n_jumps=1):
@@ -348,17 +503,15 @@
         The ith entry in the list is an array with the indices of the neighbors
         of the ith channel.
         i is not included in these arrays (a channel is not in its own shell).
     """
     return [make_shell(c, geom, n_jumps=n_jumps) for c in range(geom.shape[0])]
 
 
-def make_radial_order_parents(
-    geom, neighbours_mask, n_jumps_per_growth=1, n_jumps_parent=3
-):
+def make_radial_order_parents(geom, neighbours_mask, n_jumps_per_growth=1, n_jumps_parent=3):
     """Pre-computes a helper data structure for enforce_decrease_shells"""
     n_channels = len(geom)
 
     # which channels should we consider as possible parents for each channel?
     shells = make_shells(geom, n_jumps=n_jumps_parent)
 
     radial_parents = []
@@ -373,58 +526,91 @@
         shell0 = make_shell(channel, geom, n_jumps=n_jumps_per_growth)
         already_seen += sorted(c for c in shell0 if c not in already_seen)
 
         # so we start at the second jump
         jumps = 2
         while len(already_seen) < (neighbors < n_channels).sum():
             # grow our search -- what are the next-closest channels?
-            new_shell = make_shell(
-                channel, geom, n_jumps=jumps * n_jumps_per_growth
-            )
-            new_shell = list(
-                sorted(
-                    c
-                    for c in new_shell
-                    if (c not in already_seen) and (c in neighbors)
-                )
-            )
+            new_shell = make_shell(channel, geom, n_jumps=jumps * n_jumps_per_growth)
+            new_shell = list(sorted(c for c in new_shell if (c not in already_seen) and (c in neighbors)))
 
             # for each new channel, find the intersection of the channels
             # from previous shells and that channel's shell in `shells`
             for new_chan in new_shell:
                 parents = np.intersect1d(shells[new_chan], already_seen)
                 parents_rel = np.flatnonzero(np.isin(neighbors, parents))
                 if not len(parents_rel):
                     # this can happen for some strange geometries. in that case, bail.
                     continue
-                channel_parents.append(
-                    (np.flatnonzero(neighbors == new_chan).item(), parents_rel)
-                )
+                channel_parents.append((np.flatnonzero(neighbors == new_chan).item(), parents_rel))
 
             # add this shell to what we have seen
             already_seen += new_shell
             jumps += 1
 
         radial_parents.append(channel_parents)
 
     return radial_parents
 
 
-def enforce_decrease_shells_ptp(
-    wf_ptp, maxchan, radial_parents, in_place=False
-):
+def enforce_decrease_shells_data(wf_data, maxchan, radial_parents, in_place=False):
     """Radial enforce decrease"""
-    (C,) = wf_ptp.shape
+    (C,) = wf_data.shape
 
-    # allocate storage for decreasing version of PTP
-    decreasing_ptp = wf_ptp if in_place else wf_ptp.copy()
+    # allocate storage for decreasing version of data
+    decreasing_data = wf_data if in_place else wf_data.copy()
 
-    # loop to enforce ptp decrease from parent shells
+    # loop to enforce data decrease from parent shells
     for c, parents_rel in radial_parents[maxchan]:
-        if decreasing_ptp[c] > decreasing_ptp[parents_rel].max():
-            decreasing_ptp[c] *= decreasing_ptp[parents_rel].max() / decreasing_ptp[c]
+        if decreasing_data[c] > decreasing_data[parents_rel].max():
+            decreasing_data[c] *= decreasing_data[parents_rel].max() / decreasing_data[c]
+
+    return decreasing_data
+
+
+def get_grid_convolution_templates_and_weights(
+    contact_locations, local_radius_um=50, upsampling_um=5, sigma_um=np.linspace(10, 50.0, 5), margin_um=50
+):
+    x_min, x_max = contact_locations[:, 0].min(), contact_locations[:, 0].max()
+    y_min, y_max = contact_locations[:, 1].min(), contact_locations[:, 1].max()
+
+    x_min -= margin_um
+    x_max += margin_um
+    y_min -= margin_um
+    y_max += margin_um
+
+    dx = np.abs(x_max - x_min)
+    dy = np.abs(y_max - y_min)
+
+    eps = upsampling_um / 10
+
+    all_x, all_y = np.meshgrid(
+        np.arange(x_min, x_max + eps, upsampling_um), np.arange(y_min, y_max + eps, upsampling_um)
+    )
+
+    nb_templates = all_x.size
+
+    template_positions = np.zeros((nb_templates, 2))
+    template_positions[:, 0] = all_x.flatten()
+    template_positions[:, 1] = all_y.flatten()
+
+    import sklearn
+
+    # mask to get nearest template given a channel
+    dist = sklearn.metrics.pairwise_distances(contact_locations, template_positions)
+    nearest_template_mask = dist < local_radius_um
+
+    weights = np.zeros((len(sigma_um), len(contact_locations), nb_templates), dtype=np.float32)
+    for count, sigma in enumerate(sigma_um):
+        weights[count] = np.exp(-(dist**2) / (2 * (sigma**2)))
+
+    # normalize
+    with np.errstate(divide="ignore", invalid="ignore"):
+        norm = np.sqrt(np.sum(weights**2, axis=1))[:, np.newaxis, :]
+        weights /= norm
+        weights[np.isnan(weights)] = 0.0
 
-    return decreasing_ptp
+    return template_positions, weights, nearest_template_mask
 
 
 if HAVE_NUMBA:
-    enforce_decrease_shells = numba.jit(enforce_decrease_shells_ptp, nopython=True)
+    enforce_decrease_shells = numba.jit(enforce_decrease_shells_data, nopython=True)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/preprocessing/align_snippets.py` & `spikeinterface-0.98.0/src/spikeinterface/preprocessing/align_snippets.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,90 +1,116 @@
-from spikeinterface.core import BaseSnippets, BaseSnippetsSegment
 from typing import List, Union
+
 import numpy as np
-from scipy.interpolate import CubicSpline
+
+from spikeinterface.core import BaseSnippets, BaseSnippetsSegment
 
 
 class AlignSnippets(BaseSnippets):
     installed = True  # check at class level if installed or not
     installation_mesg = ""  # err
-    name = 'align_snippets'
+    name = "align_snippets"
 
-    def __init__(self, snippets, new_nbefore, new_nafter, mode='main_peak', interpolate=1, det_sign=0):
+    def __init__(self, snippets, new_nbefore, new_nafter, mode="main_peak", interpolate=1, det_sign=0):
         assert isinstance(snippets, BaseSnippets), "'snippets' must be a SnippetsExtractor"
-        assert mode in ('ch_peak', 'main_peak'), 'mode must be ''ch_peak'' or ''main_peak'''
+        assert mode in ("ch_peak", "main_peak"), "mode must be " "ch_peak" " or " "main_peak" ""
 
-        BaseSnippets.__init__(self, sampling_frequency=snippets.get_sampling_frequency(),
-                              nbefore=new_nbefore, snippet_len=new_nbefore+new_nafter,
-                              channel_ids=snippets.channel_ids, dtype=snippets.get_dtype())
-        assert self.snippet_len >= new_nbefore+new_nafter, 'snippet_len smaller than new_nbefore+new_nafter'
+        BaseSnippets.__init__(
+            self,
+            sampling_frequency=snippets.get_sampling_frequency(),
+            nbefore=new_nbefore,
+            snippet_len=new_nbefore + new_nafter,
+            channel_ids=snippets.channel_ids,
+            dtype=snippets.get_dtype(),
+        )
+        assert self.snippet_len >= new_nbefore + new_nafter, "snippet_len smaller than new_nbefore+new_nafter"
 
         snippets.copy_metadata(self, only_main=False, ids=None)
 
         for i in range(snippets.get_num_segments()):
-            self.add_snippets_segment(AlignSnippetsSegment(snippets._snippets_segments[i], snippets.snippet_len,
-                                                           new_nbefore, new_nafter, mode, interpolate, det_sign))
-
-        self._kwargs = dict(snippets=snippets, new_nbefore=new_nbefore, new_nafter=new_nafter,
-                            mode=mode, interpolate=interpolate, det_sign=det_sign)
+            self.add_snippets_segment(
+                AlignSnippetsSegment(
+                    snippets._snippets_segments[i],
+                    snippets.snippet_len,
+                    new_nbefore,
+                    new_nafter,
+                    mode,
+                    interpolate,
+                    det_sign,
+                )
+            )
+
+        self._kwargs = dict(
+            snippets=snippets,
+            new_nbefore=new_nbefore,
+            new_nafter=new_nafter,
+            mode=mode,
+            interpolate=interpolate,
+            det_sign=det_sign,
+        )
 
 
 class AlignSnippetsSegment(BaseSnippetsSegment):
     def __init__(self, parent_snippets_segment, org_splen, new_nbefore, new_nafter, mode, interpolate, det_sign):
         BaseSnippetsSegment.__init__(self)
         self.parent_snippets_segment = parent_snippets_segment
         self._interpolate = interpolate
         self._new_nbefore = new_nbefore
         self._new_nafter = new_nafter
         self._org_splen = org_splen
         self._det_sign = det_sign
-        start_search = int(new_nbefore*interpolate)
-        end_search = int((org_splen-new_nafter+1)*interpolate)
+        start_search = int(new_nbefore * interpolate)
+        end_search = int((org_splen - new_nafter + 1) * interpolate)
         if det_sign == 0:
-            self._find_peak = lambda x: start_search+np.argmax(np.abs(x[:, start_search:end_search, :]), axis=1)
+            self._find_peak = lambda x: start_search + np.argmax(np.abs(x[:, start_search:end_search, :]), axis=1)
         elif det_sign > 0:
-            self._find_peak = lambda x: start_search+np.argmax(x[:, start_search:end_search, :], axis=1)
+            self._find_peak = lambda x: start_search + np.argmax(x[:, start_search:end_search, :], axis=1)
         else:
             self._find_peak = lambda x: start_search + np.argmin(x[:, start_search:end_search, :], axis=1)
         self._mode = mode
-        if mode == 'main_peak':
+        if mode == "main_peak":
             if det_sign == 0:
                 self._find_main_ch = lambda x: np.argmax(np.abs(x))
             elif det_sign > 0:
                 self._find_main_ch = np.argmax
             else:
                 self._find_main_ch = np.argmin
 
-    def get_snippets(self, indices=None,
-                     end_frame: Union[int, None] = None,
-                     channel_indices: Union[List, None] = None,
-                     ) -> np.ndarray:
+    def get_snippets(
+        self,
+        indices=None,
+        end_frame: Union[int, None] = None,
+        channel_indices: Union[List, None] = None,
+    ) -> np.ndarray:
         snippets = self.parent_snippets_segment.get_snippets(indices=indices, channel_indices=channel_indices)
         if self._interpolate > 1:
-            xs = np.arange(0, self._org_splen, 1/self._interpolate)
-            snippets = CubicSpline(xs[::self._interpolate], snippets, axis=1, bc_type='natural')(xs)
+            xs = np.arange(0, self._org_splen, 1 / self._interpolate)
+            from scipy.interpolate import CubicSpline
+
+            snippets = CubicSpline(xs[:: self._interpolate], snippets, axis=1, bc_type="natural")(xs)
 
         peaks_pos = self._find_peak(snippets)
 
-        aligned_snippets = np.empty([snippets.shape[0], self._new_nbefore + self._new_nafter, snippets.shape[2]], )
+        aligned_snippets = np.empty(
+            [snippets.shape[0], self._new_nbefore + self._new_nafter, snippets.shape[2]],
+        )
         inp = self._interpolate
         pres = self._new_nbefore * self._interpolate
         posts = self._new_nafter * self._interpolate
-        if self._mode == 'main_peak':
+        if self._mode == "main_peak":
             for i, pos in enumerate(peaks_pos):
                 jpeak = pos[self._find_main_ch([snippets[i, p, ch] for ch, p in enumerate(pos)])]
-                aligned_snippets[i, :, :] = snippets[i, jpeak-pres:jpeak+posts:inp, :]
+                aligned_snippets[i, :, :] = snippets[i, jpeak - pres : jpeak + posts : inp, :]
         else:
             for i, pos in enumerate(peaks_pos):
                 for chi, chpos in enumerate(pos):
-                    aligned_snippets[i, :, chi] = snippets[i, chpos-pres:chpos+posts:inp, chi]
+                    aligned_snippets[i, :, chi] = snippets[i, chpos - pres : chpos + posts : inp, chi]
         return aligned_snippets
 
     def get_num_snippets(self):
         return self.parent_snippets_segment.get_num_snippets()
 
     def get_frames(self, indices):
         return self.parent_snippets_segment.get_num_snippets(indices)
 
-    def frames_to_indices(self, start_frame: Union[int, None] = None,
-                          end_frame: Union[int, None] = None):
+    def frames_to_indices(self, start_frame: Union[int, None] = None, end_frame: Union[int, None] = None):
         return self.parent_snippets_segment.frames_to_indices(start_frame, end_frame)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/preprocessing/basepreprocessor.py` & `spikeinterface-0.98.0/src/spikeinterface/preprocessing/basepreprocessor.py`

 * *Files identical despite different names*

### Comparing `spikeinterface-0.97.1/spikeinterface/preprocessing/clip.py` & `spikeinterface-0.98.0/src/spikeinterface/preprocessing/clip.py`

 * *Files 10% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 from spikeinterface.core.core_tools import define_function_from_class
 from .basepreprocessor import BasePreprocessor, BasePreprocessorSegment
 
 from ..core import get_random_data_chunks
 
 
 class ClipRecording(BasePreprocessor):
-    '''
+    """
     Limit the values of the data between a_min and a_max. Values exceeding the
     range will be set to the minimum or maximum, respectively.
 
     Parameters
     ----------
     recording: RecordingExtractor
         The recording extractor to be transformed
@@ -22,38 +22,37 @@
         Maximum value. If `None`, clipping is not performed on upper
         interval edge.
 
     Returns
     -------
     rescaled_traces: ClipTracesRecording
         The clipped traces recording extractor object
-    '''
-    name = 'clip'
+    """
+
+    name = "clip"
 
     def __init__(self, recording, a_min=None, a_max=None):
         value_min = a_min
         value_max = a_max
 
         BasePreprocessor.__init__(self, recording)
         for parent_segment in recording._recording_segments:
-            rec_segment = ClipRecordingSegment(
-                parent_segment, a_min, value_min, a_max, value_max)
+            rec_segment = ClipRecordingSegment(parent_segment, a_min, value_min, a_max, value_max)
             self.add_recording_segment(rec_segment)
 
-        self._kwargs = dict(recording=recording.to_dict(),
-                            a_min=a_max, a_max=a_max)
+        self._kwargs = dict(recording=recording, a_min=a_min, a_max=a_max)
 
 
 class BlankSaturationRecording(BasePreprocessor):
     """
     Find and remove parts of the signal with extereme values. Some arrays
     may produce these when amplifiers enter saturation, typically for
-    short periods of time. To remove these artefacts, values below or above 
+    short periods of time. To remove these artefacts, values below or above
     a threshold are set to the median signal value.
-    The threshold is either be estimated automatically, using the lower and upper 
+    The threshold is either be estimated automatically, using the lower and upper
     0.1 signal percentile with the largest deviation from the median, or specificed.
     Use this function with caution, as it may clip uncontaminated signals. A warning is
     printed if the data range suggests no artefacts.
 
     Parameters
     ----------
     recording: RecordingExtractor
@@ -80,81 +79,92 @@
 
     Returns
     -------
     rescaled_traces: BlankSaturationRecording
         The filtered traces recording extractor object
 
     """
-    name = 'blank_staturation'
 
-    def __init__(self, recording, abs_threshold=None, quantile_threshold=None,
-                 direction='upper', fill_value=None,
-                 num_chunks_per_segment=50, chunk_size=500, seed=0):
+    name = "blank_staturation"
 
-        assert direction in ('upper', 'lower', 'both')
+    def __init__(
+        self,
+        recording,
+        abs_threshold=None,
+        quantile_threshold=None,
+        direction="upper",
+        fill_value=None,
+        num_chunks_per_segment=50,
+        chunk_size=500,
+        seed=0,
+    ):
+        assert direction in ("upper", "lower", "both")
 
         if fill_value is None or quantile_threshold is not None:
-            random_data = get_random_data_chunks(recording,
-                                                 num_chunks_per_segment=num_chunks_per_segment,
-                                                 chunk_size=chunk_size, seed=seed)
+            random_data = get_random_data_chunks(
+                recording, num_chunks_per_segment=num_chunks_per_segment, chunk_size=chunk_size, seed=seed
+            )
 
         if fill_value is None:
             fill_value = np.median(random_data)
 
         a_min, value_min, a_max, value_max = None, None, None, None
 
         if abs_threshold is None:
             assert quantile_threshold is not None
             assert 0 <= quantile_threshold <= 1
-            q = np.quantile(
-                random_data, [quantile_threshold, 1 - quantile_threshold])
-            if direction in ('lower', 'both'):
+            q = np.quantile(random_data, [quantile_threshold, 1 - quantile_threshold])
+            if direction in ("lower", "both"):
                 a_min = q[0]
                 value_min = fill_value
-            if direction in ('upper', 'both'):
+            if direction in ("upper", "both"):
                 a_max = q[1]
                 value_max = fill_value
         else:
             assert abs_threshold is not None
-            if direction == 'lower':
+            if direction == "lower":
                 a_min = abs_threshold
                 value_min = fill_value
-            if direction == 'upper':
+            if direction == "upper":
                 a_max = abs_threshold
                 value_max = fill_value
-            if direction == 'both':
+            if direction == "both":
                 a_min = -abs_threshold
                 value_min = fill_value
                 a_max = abs_threshold
                 value_max = fill_value
 
         BasePreprocessor.__init__(self, recording)
         for parent_segment in recording._recording_segments:
-            rec_segment = ClipRecordingSegment(
-                parent_segment, a_min, value_min, a_max, value_max)
+            rec_segment = ClipRecordingSegment(parent_segment, a_min, value_min, a_max, value_max)
             self.add_recording_segment(rec_segment)
 
-        self._kwargs = dict(recording=recording.to_dict(), abs_threshold=abs_threshold,
-                            quantile_threshold=quantile_threshold, direction=direction, fill_value=fill_value,
-                            num_chunks_per_segment=num_chunks_per_segment, chunk_size=chunk_size,
-                            seed=seed)
+        self._kwargs = dict(
+            recording=recording,
+            abs_threshold=abs_threshold,
+            quantile_threshold=quantile_threshold,
+            direction=direction,
+            fill_value=fill_value,
+            num_chunks_per_segment=num_chunks_per_segment,
+            chunk_size=chunk_size,
+            seed=seed,
+        )
 
 
 class ClipRecordingSegment(BasePreprocessorSegment):
     def __init__(self, parent_recording_segment, a_min, value_min, a_max, value_max):
         BasePreprocessorSegment.__init__(self, parent_recording_segment)
 
         self.a_min = a_min
         self.value_min = value_min
         self.a_max = a_max
         self.value_max = value_max
 
     def get_traces(self, start_frame, end_frame, channel_indices):
-        traces = self.parent_recording_segment.get_traces(
-            start_frame, end_frame, channel_indices)
+        traces = self.parent_recording_segment.get_traces(start_frame, end_frame, channel_indices)
         traces = traces.copy()
 
         if self.a_min is not None:
             traces[traces <= self.a_min] = self.value_min
         if self.a_max is not None:
             traces[traces >= self.a_max] = self.value_max
```

### Comparing `spikeinterface-0.97.1/spikeinterface/preprocessing/common_reference.py` & `spikeinterface-0.98.0/src/spikeinterface/preprocessing/common_reference.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,14 +1,16 @@
 import numpy as np
 
 from spikeinterface.core.core_tools import define_function_from_class
 
 from .basepreprocessor import BasePreprocessor, BasePreprocessorSegment
 from ..core import get_closest_channels
 
+from .filter import fix_dtype
+
 
 class CommonReferenceRecording(BasePreprocessor):
     """
     Re-references the recording extractor traces.
 
     Parameters
     ----------
@@ -31,123 +33,147 @@
         If no 'groups' are specified, all channels are referenced to 'ref_channel_ids'. If 'groups' is provided, then a
         list of channels to be applied to each group is expected. If 'single' reference, a list of one channel  or an
         int is expected.
     local_radius: tuple(int, int)
         Use in the local CAR implementation as the selecting annulus (exclude radius, include radius)
     verbose: bool
         If True, output is verbose
+    dtype: None or dtype
+        If None the parent dtype is kept.
 
     Returns
     -------
     referenced_recording: CommonReferenceRecording
         The re-referenced recording extractor object
     """
 
-    name = 'common_reference'
-
-    def __init__(self, recording, reference='global', operator='median', groups=None, ref_channel_ids=None,
-                 local_radius=(30, 55), verbose=False):
+    name = "common_reference"
 
+    def __init__(
+        self,
+        recording,
+        reference="global",
+        operator="median",
+        groups=None,
+        ref_channel_ids=None,
+        local_radius=(30, 55),
+        verbose=False,
+        dtype=None,
+    ):
         num_chans = recording.get_num_channels()
         neighbors = None
         # some checks
-        if reference not in ('global', 'single', 'local'):
+        if reference not in ("global", "single", "local"):
             raise ValueError("'reference' must be either 'global', 'single' or 'local'")
-        if operator not in ('median', 'average'):
+        if operator not in ("median", "average"):
             raise ValueError("'operator' must be either 'median', 'average'")
 
-        if reference == 'global':
+        if reference == "global":
             pass
-        elif reference == 'single':
+        elif reference == "single":
             assert ref_channel_ids is not None, "With 'single' reference, provide 'ref_channel_ids'"
             if groups is not None:
-                assert len(ref_channel_ids) == len(groups), \
-                    "'ref_channel_ids' and 'groups' must have the same length"
+                assert len(ref_channel_ids) == len(groups), "'ref_channel_ids' and 'groups' must have the same length"
             else:
                 if np.isscalar(ref_channel_ids):
                     ref_channel_ids = [ref_channel_ids]
                 else:
-                    assert len(ref_channel_ids) == 1, \
-                        "'ref_channel_ids' with no 'groups' must be int or a list of one element"
+                    assert (
+                        len(ref_channel_ids) == 1
+                    ), "'ref_channel_ids' with no 'groups' must be int or a list of one element"
                 ref_channel_ids = np.asarray(ref_channel_ids)
-                assert np.all([ch in recording.get_channel_ids() for ch in ref_channel_ids]), "Some wrong 'ref_channel_ids'!"
-        elif reference == 'local':
+                assert np.all(
+                    [ch in recording.get_channel_ids() for ch in ref_channel_ids]
+                ), "Some wrong 'ref_channel_ids'!"
+        elif reference == "local":
             assert groups is None, "With 'local' CAR, the group option should not be used."
             closest_inds, dist = get_closest_channels(recording)
             neighbors = {}
             for i in range(num_chans):
                 mask = (dist[i, :] > local_radius[0]) & (dist[i, :] <= local_radius[1])
                 neighbors[i] = closest_inds[i, mask]
                 assert len(neighbors[i]) > 0, "No reference channels available in the local annulus for selection."
 
-        BasePreprocessor.__init__(self, recording)
+        dtype_ = fix_dtype(recording, dtype)
+        BasePreprocessor.__init__(self, recording, dtype=dtype_)
 
         # tranforms groups (ids) to groups (indices)
         if groups is not None:
             groups = [self.ids_to_indices(g) for g in groups]
         if ref_channel_ids is not None:
             ref_channel_inds = self.ids_to_indices(ref_channel_ids)
         else:
             ref_channel_inds = None
 
         for parent_segment in recording._recording_segments:
-            rec_segment = CommonReferenceRecordingSegment(parent_segment,
-                                                          reference, operator, groups, ref_channel_inds, local_radius,
-                                                          neighbors)
+            rec_segment = CommonReferenceRecordingSegment(
+                parent_segment, reference, operator, groups, ref_channel_inds, local_radius, neighbors, dtype_
+            )
             self.add_recording_segment(rec_segment)
 
-        self._kwargs = dict(recording=recording.to_dict(), reference=reference, groups=groups, operator=operator,
-                            ref_channel_ids=ref_channel_ids, local_radius=local_radius)
+        self._kwargs = dict(
+            recording=recording,
+            reference=reference,
+            groups=groups,
+            operator=operator,
+            ref_channel_ids=ref_channel_ids,
+            local_radius=local_radius,
+            dtype=dtype_.str,
+        )
 
 
 class CommonReferenceRecordingSegment(BasePreprocessorSegment):
-    def __init__(self, parent_recording_segment, reference, operator, groups, ref_channel_inds, local_radius, neighbors):
+    def __init__(
+        self, parent_recording_segment, reference, operator, groups, ref_channel_inds, local_radius, neighbors, dtype
+    ):
         BasePreprocessorSegment.__init__(self, parent_recording_segment)
 
         self.reference = reference
         self.operator = operator
         self.groups = groups
         self.ref_channel_inds = ref_channel_inds
         self.local_radius = local_radius
         self.neighbors = neighbors
         self.temp = None
+        self.dtype = dtype
 
-        if self.operator == 'median':
+        if self.operator == "median":
             self.operator_func = lambda x: np.median(x, axis=1, out=self.temp)[:, None]
-        elif self.operator == 'average':
+        elif self.operator == "average":
             self.operator_func = lambda x: np.mean(x, axis=1, out=self.temp)[:, None]
 
     def get_traces(self, start_frame, end_frame, channel_indices):
         # need input trace
         all_traces = self.parent_recording_segment.get_traces(start_frame, end_frame, slice(None))
-        self.temp = np.zeros((all_traces.shape[0],),dtype=all_traces.dtype)
+        all_traces = all_traces.astype(self.dtype)
+        self.temp = np.zeros((all_traces.shape[0],), dtype=all_traces.dtype)
         _channel_indices = np.arange(all_traces.shape[1])
         if channel_indices is not None:
             _channel_indices = _channel_indices[channel_indices]
 
-        
-        if self.reference == 'global':
-            out_traces = np.zeros((all_traces.shape[0], _channel_indices.size), dtype=all_traces.dtype)
+        if self.reference == "global":
+            out_traces = np.zeros((all_traces.shape[0], _channel_indices.size), dtype=self.dtype)
             for chan_inds, chan_group_inds in self._groups(_channel_indices):
                 out_inds = np.array([np.where(_channel_indices == i)[0][0] for i in chan_inds])
-                out_traces[:, out_inds] = all_traces[:, chan_inds] \
-                    - self.operator_func(all_traces[:, chan_group_inds])
+                out_traces[:, out_inds] = all_traces[:, chan_inds] - self.operator_func(all_traces[:, chan_group_inds])
 
-        elif self.reference == 'single':
-            out_traces = np.zeros((all_traces.shape[0], _channel_indices.size), dtype=all_traces.dtype)
+        elif self.reference == "single":
+            out_traces = np.zeros((all_traces.shape[0], _channel_indices.size), dtype=self.dtype)
             for i, (chan_inds, _) in enumerate(self._groups(_channel_indices)):
                 out_inds = np.array([np.where(_channel_indices == i)[0][0] for i in chan_inds])
-                out_traces[:, out_inds] = all_traces[:, chan_inds] \
-                    - self.operator_func(all_traces[:, [self.ref_channel_inds[i]]])
-        
-        elif self.reference == 'local':
-            out_traces = np.hstack([
-                all_traces[:, [chan_ind]] - self.operator_func(all_traces[:, self.neighbors[chan_ind]])
-                for chan_ind in _channel_indices])
-
+                out_traces[:, out_inds] = all_traces[:, chan_inds] - self.operator_func(
+                    all_traces[:, [self.ref_channel_inds[i]]]
+                )
+
+        elif self.reference == "local":
+            out_traces = np.zeros((all_traces.shape[0], _channel_indices.size), dtype=self.dtype)
+            for i, chan_ind in enumerate(_channel_indices):
+                out_traces[:, [i]] = all_traces[:, [chan_ind]] - self.operator_func(
+                    all_traces[:, self.neighbors[chan_ind]]
+                )
         return out_traces
 
     def _groups(self, channel_indices):
         selected_groups = []
         selected_channels = []
         if self.groups:
             for chan_inds in self.groups:
```

### Comparing `spikeinterface-0.97.1/spikeinterface/preprocessing/correct_lsb.py` & `spikeinterface-0.98.0/src/spikeinterface/preprocessing/correct_lsb.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,16 +1,15 @@
 import warnings
 import numpy as np
 
 from .normalize_scale import scale
 from ..core import get_random_data_chunks
 
 
-def correct_lsb(recording, num_chunks_per_segment=20, chunk_size=10000, seed=None,
-                verbose=False):
+def correct_lsb(recording, num_chunks_per_segment=20, chunk_size=10000, seed=None, verbose=False):
     """
     Estimates the LSB of the recording and divide traces by LSB
     to ensure LSB = 1. Medians are also subtracted to avoid rounding errors.
 
     Parameters
     ----------
     recording : RecordingExtractor
@@ -25,17 +24,22 @@
         If True, estimate LSB value is printed, by default False
 
     Returns
     -------
     correct_lsb_recording: ScaleRecording
         The recording extractor with corrected LSB
     """
-    random_data = get_random_data_chunks(recording, num_chunks_per_segment=num_chunks_per_segment,
-                                         chunk_size=chunk_size, concatenated=True, seed=seed,
-                                         return_scaled=False)
+    random_data = get_random_data_chunks(
+        recording,
+        num_chunks_per_segment=num_chunks_per_segment,
+        chunk_size=chunk_size,
+        concatenated=True,
+        seed=seed,
+        return_scaled=False,
+    )
     # compute medians and lsb
     medians = np.median(random_data, axis=0)
     lsb = _estimate_lsb_from_data(random_data)
 
     if verbose:
         print(f"Estimated LSB value: {lsb}")
 
@@ -45,17 +49,17 @@
         recording_lsb = recording
     elif lsb == 1:
         warnings.warn("Estimated LSB=1. No operation is applied")
         recording_lsb = recording
     else:
         dtype = recording.get_dtype()
         # first remove medians
-        recording_lsb = scale(recording, gain=1., offset=-medians, dtype=dtype)
+        recording_lsb = scale(recording, gain=1.0, offset=-medians, dtype=dtype)
         # apply LSB division and instantiate parent
-        recording_lsb = scale(recording_lsb, gain=1. / lsb, dtype=dtype)
+        recording_lsb = scale(recording_lsb, gain=1.0 / lsb, dtype=dtype)
         # if recording has scaled traces, correct gains
         if recording.has_scaled():
             recording_lsb.set_channel_gains(recording_lsb.get_channel_gains() * lsb)
     return recording_lsb
 
 
 def _estimate_lsb_from_data(data):
```

### Comparing `spikeinterface-0.97.1/spikeinterface/preprocessing/deepinterpolation/deepinterpolation.py` & `spikeinterface-0.98.0/src/spikeinterface/preprocessing/deepinterpolation/deepinterpolation.py`

 * *Files 4% similar despite different names*

```diff
@@ -8,22 +8,22 @@
 from spikeinterface.core import get_random_data_chunks
 
 
 def import_tf(use_gpu=True, disable_tf_logger=True):
     import tensorflow as tf
 
     if not use_gpu:
-        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
+        os.environ["CUDA_VISIBLE_DEVICES"] = "-1"
 
     if disable_tf_logger:
-        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
-        tf.get_logger().setLevel('ERROR')
+        os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
+        tf.get_logger().setLevel("ERROR")
 
     tf.compat.v1.disable_eager_execution()
-    gpus = tf.config.list_physical_devices('GPU')
+    gpus = tf.config.list_physical_devices("GPU")
     if gpus:
         try:
             # Currently, memory growth needs to be the same across GPUs
             for gpu in gpus:
                 tf.config.experimental.set_memory_growth(gpu, True)
         except RuntimeError as e:
             # Memory growth must be set before GPUs have been initialized
@@ -53,68 +53,74 @@
     -------
     class
         The defined DeepInterpolationInputGenerator class
     """
     tf = import_tf(use_gpu, disable_tf_logger)
 
     class DeepInterpolationInputGenerator(tf.keras.utils.Sequence):
-
-        def __init__(self, recording, start_frame, end_frame, batch_size,
-                     pre_frames, post_frames, pre_post_omission,
-                     local_mean, local_std):
+        def __init__(
+            self,
+            recording,
+            start_frame,
+            end_frame,
+            batch_size,
+            pre_frames,
+            post_frames,
+            pre_post_omission,
+            local_mean,
+            local_std,
+        ):
             self.recording = recording
             self.start_frame = start_frame
             self.end_frame = end_frame
 
             self.batch_size = batch_size
-            self.last_batch_size = (end_frame-start_frame) - \
-                (self.__len__()-1)*batch_size
+            self.last_batch_size = (end_frame - start_frame) - (self.__len__() - 1) * batch_size
 
             self.pre_frames = pre_frames
             self.post_frames = post_frames
             self.pre_post_omission = pre_post_omission
 
             self.local_mean = local_mean
             self.local_std = local_std
 
         def __len__(self):
-            return -((self.end_frame-self.start_frame) // -self.batch_size)
+            return -((self.end_frame - self.start_frame) // -self.batch_size)
 
         def __getitem__(self, idx):
             n_batches = self.__len__()
             if idx < n_batches - 1:
-                start_frame = self.start_frame + self.batch_size * \
-                    idx-self.pre_frames-self.pre_post_omission
-                end_frame = self.start_frame + self.batch_size * \
-                    (idx + 1) + self.post_frames + self.pre_post_omission
-                traces = self.recording.get_traces(start_frame=start_frame,
-                                                   end_frame=end_frame,
-                                                   channel_indices=slice(None))
+                start_frame = self.start_frame + self.batch_size * idx - self.pre_frames - self.pre_post_omission
+                end_frame = self.start_frame + self.batch_size * (idx + 1) + self.post_frames + self.pre_post_omission
+                traces = self.recording.get_traces(
+                    start_frame=start_frame, end_frame=end_frame, channel_indices=slice(None)
+                )
                 batch_size = self.batch_size
             else:
-                start_frame = self.end_frame-self.last_batch_size - \
-                    self.pre_frames-self.pre_post_omission
-                end_frame = self.end_frame+self.post_frames+self.pre_post_omission
-                traces = self.recording.get_traces(start_frame=start_frame,
-                                                   end_frame=end_frame,
-                                                   channel_indices=slice(None))
+                start_frame = self.end_frame - self.last_batch_size - self.pre_frames - self.pre_post_omission
+                end_frame = self.end_frame + self.post_frames + self.pre_post_omission
+                traces = self.recording.get_traces(
+                    start_frame=start_frame, end_frame=end_frame, channel_indices=slice(None)
+                )
                 batch_size = self.last_batch_size
 
             shape = (traces.shape[0], int(384 / 2), 2)
             traces = np.reshape(traces, newshape=shape)
 
-            di_input = np.zeros(
-                (batch_size, 384, 2, self.pre_frames+self.post_frames))
+            di_input = np.zeros((batch_size, 384, 2, self.pre_frames + self.post_frames))
             di_label = np.zeros((batch_size, 384, 2, 1))
-            for index_frame in range(self.pre_frames+self.pre_post_omission,
-                                     batch_size+self.pre_frames+self.pre_post_omission):
-                di_input[index_frame-self.pre_frames -
-                         self.pre_post_omission] = self.reshape_input_forward(index_frame, traces)
-                di_label[index_frame-self.pre_frames -
-                         self.pre_post_omission] = self.reshape_label_forward(traces[index_frame])
+            for index_frame in range(
+                self.pre_frames + self.pre_post_omission, batch_size + self.pre_frames + self.pre_post_omission
+            ):
+                di_input[index_frame - self.pre_frames - self.pre_post_omission] = self.reshape_input_forward(
+                    index_frame, traces
+                )
+                di_label[index_frame - self.pre_frames - self.pre_post_omission] = self.reshape_label_forward(
+                    traces[index_frame]
+                )
             return (di_input, di_label)
 
         def reshape_input_forward(self, index_frame, raw_data):
             """Reshapes the frames surrounding the target frame to the form expected by model;
             also subtracts mean and divides by std.
 
             Parameters
@@ -129,42 +135,35 @@
             input_full : ndarray; (1, 384, 2, pre_frames+post_frames)
                 input to trained network to predict the center frame
             """
             # currently only works for recordings with 384 channels
             nb_probes = 384
 
             # We reorganize to follow true geometry of probe for convolution
-            input_full = np.zeros(
-                [1, nb_probes, 2,
-                 self.pre_frames + self.post_frames], dtype="float32"
-            )
+            input_full = np.zeros([1, nb_probes, 2, self.pre_frames + self.post_frames], dtype="float32")
 
             input_index = np.arange(
                 index_frame - self.pre_frames - self.pre_post_omission,
                 index_frame + self.post_frames + self.pre_post_omission + 1,
             )
             input_index = input_index[input_index != index_frame]
 
             for index_padding in np.arange(self.pre_post_omission + 1):
-                input_index = input_index[input_index !=
-                                          index_frame - index_padding]
-                input_index = input_index[input_index !=
-                                          index_frame + index_padding]
+                input_index = input_index[input_index != index_frame - index_padding]
+                input_index = input_index[input_index != index_frame + index_padding]
 
             data_img_input = raw_data[input_index, :, :]
 
             data_img_input = np.swapaxes(data_img_input, 1, 2)
             data_img_input = np.swapaxes(data_img_input, 0, 2)
 
             even = np.arange(0, nb_probes, 2)
             odd = even + 1
 
-            data_img_input = (
-                data_img_input.astype("float32") - self.local_mean
-            ) / self.local_std
+            data_img_input = (data_img_input.astype("float32") - self.local_mean) / self.local_std
 
             input_full[0, even, 0, :] = data_img_input[:, 0, :]
             input_full[0, odd, 1, :] = data_img_input[:, 1, :]
             return input_full
 
         def reshape_label_forward(self, label):
             """Reshapes the target frame to the form expected by model.
@@ -177,43 +176,47 @@
             -------
             reshaped_label : ndarray, (1, 384, 2, 1)
                 target frame after reshaping
             """
             # currently only works for recordings with 384 channels
             nb_probes = 384
 
-            input_full = np.zeros(
-                [1, nb_probes, 2, 1], dtype="float32"
-            )
+            input_full = np.zeros([1, nb_probes, 2, 1], dtype="float32")
 
             data_img_input = np.expand_dims(label, axis=0)
             data_img_input = np.swapaxes(data_img_input, 1, 2)
             data_img_input = np.swapaxes(data_img_input, 0, 2)
 
             even = np.arange(0, nb_probes, 2)
             odd = even + 1
 
-            data_img_input = (
-                data_img_input.astype("float32") - self.local_mean
-            ) / self.local_std
+            data_img_input = (data_img_input.astype("float32") - self.local_mean) / self.local_std
 
             input_full[0, even, 0, :] = data_img_input[:, 0, :]
             input_full[0, odd, 1, :] = data_img_input[:, 1, :]
             return input_full
 
     return DeepInterpolationInputGenerator
 
 
 class DeepInterpolatedRecording(BasePreprocessor):
-    name = 'deepinterpolate'
+    name = "deepinterpolate"
 
-    def __init__(self, recording: BaseRecording, model_path: str,
-                 pre_frames: int = 30, post_frames: int = 30, pre_post_omission: int = 1,
-                 batch_size=128, use_gpu: bool = True, disable_tf_logger: bool = True,
-                 **random_chunk_kwargs):
+    def __init__(
+        self,
+        recording: BaseRecording,
+        model_path: str,
+        pre_frames: int = 30,
+        post_frames: int = 30,
+        pre_post_omission: int = 1,
+        batch_size=128,
+        use_gpu: bool = True,
+        disable_tf_logger: bool = True,
+        **random_chunk_kwargs,
+    ):
         """Applies DeepInterpolation, a neural network based denoising method, to the recording.
 
         Notes
         -----
         * Currently this only works on Neuropixels 1.0-like recordings with 384 channels.
         If the recording has fewer number of channels, consider matching the channel count with
         `ZeroChannelPaddedRecording`.
@@ -241,130 +244,153 @@
         batch_size : int, optional
             Number of frames per batch to infer (adjust based on hardware); by default 128
         disable_tf_logger : bool, optional
             If True, tensorflow logging is disabled, by default True
         random_chunk_kwargs: keyword arguments for get_random_data_chunks
         """
 
-        assert has_tf(
-            use_gpu, disable_tf_logger), "To use DeepInterpolation, you first need to install `tensorflow`."
-        assert recording.get_num_channels() <= 384, ("DeepInterpolation only works on Neuropixels 1.0-like "
-                                                     "recordings with 384 channels. This recording has too many "
-                                                     "channels.")
-        assert recording.get_num_channels() == 384, ("DeepInterpolation only works on Neuropixels 1.0-like "
-                                                     "recordings with 384 channels. "
-                                                     "This recording has too few channels. Try matching the channel "
-                                                     "count with `ZeroChannelPaddedRecording`.")
+        assert has_tf(use_gpu, disable_tf_logger), "To use DeepInterpolation, you first need to install `tensorflow`."
+        assert recording.get_num_channels() <= 384, (
+            "DeepInterpolation only works on Neuropixels 1.0-like "
+            "recordings with 384 channels. This recording has too many "
+            "channels."
+        )
+        assert recording.get_num_channels() == 384, (
+            "DeepInterpolation only works on Neuropixels 1.0-like "
+            "recordings with 384 channels. "
+            "This recording has too few channels. Try matching the channel "
+            "count with `ZeroChannelPaddedRecording`."
+        )
         self.tf = import_tf(use_gpu, disable_tf_logger)
 
         # try move model load here with spawn
         BasePreprocessor.__init__(self, recording)
 
         # first time retrieving traces check that dimensions are ok
         self.tf.keras.backend.clear_session()
         self.model = self.tf.keras.models.load_model(filepath=model_path)
         # check input shape for the last dimension
         config = self.model.get_config()
         input_shape = config["layers"][0]["config"]["batch_input_shape"]
-        assert input_shape[-1] == pre_frames + \
-            post_frames, ("The sum of `pre_frames` and `post_frames` must match "
-                          "the last dimension of the model.")
+        assert input_shape[-1] == pre_frames + post_frames, (
+            "The sum of `pre_frames` and `post_frames` must match " "the last dimension of the model."
+        )
 
-        local_data = get_random_data_chunks(
-            recording, **random_chunk_kwargs)
+        local_data = get_random_data_chunks(recording, **random_chunk_kwargs)
         if isinstance(recording, ZeroChannelPaddedRecording):
             local_data = local_data[:, recording.channel_mapping]
 
         local_mean = np.mean(local_data.flatten())
         local_std = np.std(local_data.flatten())
 
         # add segment
         for segment in recording._recording_segments:
-            recording_segment = DeepInterpolatedRecordingSegment(segment, self.model,
-                                                                 pre_frames, post_frames, pre_post_omission,
-                                                                 local_mean, local_std, batch_size, use_gpu,
-                                                                 disable_tf_logger)
+            recording_segment = DeepInterpolatedRecordingSegment(
+                segment,
+                self.model,
+                pre_frames,
+                post_frames,
+                pre_post_omission,
+                local_mean,
+                local_std,
+                batch_size,
+                use_gpu,
+                disable_tf_logger,
+            )
             self.add_recording_segment(recording_segment)
 
         self._preferred_mp_context = "spawn"
-        self._kwargs = dict(recording=recording.to_dict(), model_path=model_path,
-                            pre_frames=pre_frames, post_frames=post_frames, pre_post_omission=pre_post_omission,
-                            batch_size=batch_size, **random_chunk_kwargs)
-        self.extra_requirements.extend(['tensorflow'])
+        self._kwargs = dict(
+            recording=recording,
+            model_path=model_path,
+            pre_frames=pre_frames,
+            post_frames=post_frames,
+            pre_post_omission=pre_post_omission,
+            batch_size=batch_size,
+            **random_chunk_kwargs,
+        )
+        self.extra_requirements.extend(["tensorflow"])
 
 
 class DeepInterpolatedRecordingSegment(BasePreprocessorSegment):
-
-    def __init__(self, recording_segment, model,
-                 pre_frames, post_frames, pre_post_omission,
-                 local_mean, local_std, batch_size, use_gpu,
-                 disable_tf_logger):
+    def __init__(
+        self,
+        recording_segment,
+        model,
+        pre_frames,
+        post_frames,
+        pre_post_omission,
+        local_mean,
+        local_std,
+        batch_size,
+        use_gpu,
+        disable_tf_logger,
+    ):
         BasePreprocessorSegment.__init__(self, recording_segment)
 
         self.model = model
         self.pre_frames = pre_frames
         self.post_frames = post_frames
         self.pre_post_omission = pre_post_omission
         self.local_mean = local_mean
         self.local_std = local_std
         self.batch_size = batch_size
         self.use_gpu = use_gpu
 
         # creating class dynamically to use the imported TF with GPU enabled/disabled based on the use_gpu flag
-        self.DeepInterpolationInputGenerator = define_input_generator_class(
-            use_gpu, disable_tf_logger)
+        self.DeepInterpolationInputGenerator = define_input_generator_class(use_gpu, disable_tf_logger)
 
     def get_traces(self, start_frame, end_frame, channel_indices):
         n_frames = self.parent_recording_segment.get_num_samples()
 
         if start_frame == None:
             start_frame = 0
 
         if end_frame == None:
             end_frame = n_frames
 
         # for frames that lack full training data (i.e. pre and post frames including omissinos),
         # just return uninterpolated
-        if start_frame < self.pre_frames+self.pre_post_omission:
-            true_start_frame = self.pre_frames+self.pre_post_omission
-            array_to_append_front = self.parent_recording_segment.get_traces(start_frame=0,
-                                                                             end_frame=true_start_frame,
-                                                                             channel_indices=channel_indices)
+        if start_frame < self.pre_frames + self.pre_post_omission:
+            true_start_frame = self.pre_frames + self.pre_post_omission
+            array_to_append_front = self.parent_recording_segment.get_traces(
+                start_frame=0, end_frame=true_start_frame, channel_indices=channel_indices
+            )
         else:
             true_start_frame = start_frame
 
-        if end_frame > n_frames-self.post_frames-self.pre_post_omission:
-            true_end_frame = n_frames-self.post_frames-self.pre_post_omission
-            array_to_append_back = self.parent_recording_segment.get_traces(start_frame=true_end_frame,
-                                                                            end_frame=n_frames,
-                                                                            channel_indices=channel_indices)
+        if end_frame > n_frames - self.post_frames - self.pre_post_omission:
+            true_end_frame = n_frames - self.post_frames - self.pre_post_omission
+            array_to_append_back = self.parent_recording_segment.get_traces(
+                start_frame=true_end_frame, end_frame=n_frames, channel_indices=channel_indices
+            )
         else:
             true_end_frame = end_frame
 
         # instantiate an input generator that can be passed directly to model.predict
-        input_generator = self.DeepInterpolationInputGenerator(recording=self.parent_recording_segment,
-                                                               start_frame=true_start_frame,
-                                                               end_frame=true_end_frame,
-                                                               pre_frames=self.pre_frames,
-                                                               post_frames=self.post_frames,
-                                                               pre_post_omission=self.pre_post_omission,
-                                                               local_mean=self.local_mean,
-                                                               local_std=self.local_std,
-                                                               batch_size=self.batch_size)
+        input_generator = self.DeepInterpolationInputGenerator(
+            recording=self.parent_recording_segment,
+            start_frame=true_start_frame,
+            end_frame=true_end_frame,
+            pre_frames=self.pre_frames,
+            post_frames=self.post_frames,
+            pre_post_omission=self.pre_post_omission,
+            local_mean=self.local_mean,
+            local_std=self.local_std,
+            batch_size=self.batch_size,
+        )
         di_output = self.model.predict(input_generator, verbose=2)
 
         out_traces = self.reshape_backward(di_output)
 
         if true_start_frame != start_frame:
-            out_traces = np.concatenate(
-                (array_to_append_front, out_traces), axis=0)
+            out_traces = np.concatenate((array_to_append_front, out_traces), axis=0)
 
         if true_end_frame != end_frame:
-            out_traces = np.concatenate(
-                (out_traces, array_to_append_back), axis=0)
+            out_traces = np.concatenate((out_traces, array_to_append_back), axis=0)
 
         return out_traces[:, channel_indices]
 
     def reshape_backward(self, di_frames):
         """reshapes the prediction from model back to frames
 
         Parameters
@@ -382,14 +408,13 @@
         n_frames = di_frames.shape[0]
         even = np.arange(0, nb_probes, 2)
         odd = even + 1
         reshaped_frames = np.zeros((n_frames, 384))
         for frame in range(n_frames):
             reshaped_frames[frame, 0::2] = di_frames[frame, even, 0, 0]
             reshaped_frames[frame, 1::2] = di_frames[frame, odd, 1, 0]
-        reshaped_frames = reshaped_frames*self.local_std+self.local_mean
+        reshaped_frames = reshaped_frames * self.local_std + self.local_mean
         return reshaped_frames
 
 
 # function for API
-deepinterpolate = define_function_from_class(
-    source_class=DeepInterpolatedRecording, name="deepinterpolate")
+deepinterpolate = define_function_from_class(source_class=DeepInterpolatedRecording, name="deepinterpolate")
```

### Comparing `spikeinterface-0.97.1/spikeinterface/preprocessing/detect_bad_channels.py` & `spikeinterface-0.98.0/src/spikeinterface/preprocessing/detect_bad_channels.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,48 +1,54 @@
 import warnings
+
 import numpy as np
-import scipy.stats
 
 from .filter import highpass_filter
 from ..core import get_random_data_chunks, order_channels_by_depth
 
 
-
-def detect_bad_channels(recording,
-                        method="coherence+psd",
-                        std_mad_threshold=5,
-                        psd_hf_threshold=0.02,
-                        dead_channel_threshold=-0.5,
-                        noisy_channel_threshold=1.,
-                        outside_channel_threshold=-0.75,
-                        n_neighbors=11,
-                        nyquist_threshold=0.8,
-                        direction='y',
-                        chunk_duration_s=0.3,
-                        num_random_chunks=10,
-                        welch_window_ms=10.,
-                        highpass_filter_cutoff=300,
-                        seed=None):
+def detect_bad_channels(
+    recording,
+    method="coherence+psd",
+    std_mad_threshold=5,
+    psd_hf_threshold=0.02,
+    dead_channel_threshold=-0.5,
+    noisy_channel_threshold=1.0,
+    outside_channel_threshold=-0.75,
+    n_neighbors=11,
+    nyquist_threshold=0.8,
+    direction="y",
+    chunk_duration_s=0.3,
+    num_random_chunks=10,
+    welch_window_ms=10.0,
+    highpass_filter_cutoff=300,
+    neighborhood_r2_threshold=0.9,
+    neighborhood_r2_radius_um=30.0,
+    seed=None,
+):
     """
     Perform bad channel detection.
     The recording is assumed to be filtered. If not, a highpass filter is applied on the fly.
 
     Different methods are implemented:
 
     * std : threhshold on channel standard deviations
         If the standard deviation of a channel is greater than `std_mad_threshold` times the median of all
         channels standard deviations, the channel is flagged as noisy
     * mad : same as std, but using median absolute deviations instead
     * coeherence+psd : method developed by the International Brain Laboratory that detects bad channels of three types:
-
         * Dead channels are those with low similarity to the surrounding channels (n=`n_neighbors` median)
-        * Noise channels are those with power at >80% Nyquist above the psd_hf_threshold (default 0.02 uV^2 / Hz) 
+        * Noise channels are those with power at >80% Nyquist above the psd_hf_threshold (default 0.02 uV^2 / Hz)
           and a high coherence with "far away" channels"
-        * Out of brain channels are contigious regions of channels dissimilar to the median of all channels 
+        * Out of brain channels are contigious regions of channels dissimilar to the median of all channels
           at the top end of the probe (i.e. large channel number)
+    * neighborhood_r2
+        A method tuned for LFP use-cases, where channels should be highly correlated with their spatial
+        neighbors. This method estimates the correlation of each channel with the median of its spatial
+        neighbors, and considers channels bad when this correlation is too small.
 
     Parameters
     ----------
     recording : BaseRecording
         The recording for which bad channels are detected
     method : str
         The method to be used:
@@ -51,15 +57,15 @@
         * mad
         * std
     std_mad_threshold (mstd) : float
         (method std, mad)
         The standard deviation/mad multiplier threshold
     psd_hf_threshold (coeherence+psd) : float
         An absolute threshold (uV^2/Hz) used as a cutoff for noise channels.
-        Channels with average power at >80% Nyquist larger than this threshold 
+        Channels with average power at >80% Nyquist larger than this threshold
         will be labeled as noise, by default 0.02
     dead_channel_threshold (coeherence+psd) : float, optional
         Threshold for channel coherence below which channels are labeled as dead, by default -0.5
     noisy_channel_threshold (coeherence+psd) : float
         Threshold for channel coherence above which channels are labeled as noisy (together with psd condition),
         by default 1
     outside_channel_threshold (coeherence+psd) : float
@@ -76,14 +82,18 @@
         If the recording is not filtered, the cutoff frequency of the highpass filter, by default 300
     chunk_duration_s : float
         Duration of each chunk, by default 0.3
     num_random_chunks : int
         Number of random chunks, by default 10
     welch_window_ms : float
         Window size for the scipy.signal.welch that will be converted to nperseg, by default 10ms
+    neighborhood_r2_threshold : float, default 0.95
+        R^2 threshold for the neighborhood_r2 method.
+    neighborhood_r2_radius_um : float, default 30
+        Spatial radius below which two channels are considered neighbors in the neighborhood_r2 method.
     seed : int or None
         The random seed to extract chunks, by default None
 
     Returns
     -------
     bad_channel_ids : np.array
         The identified bad channel ids
@@ -102,131 +112,193 @@
 
     Notes
     -----
     For details refer to:
     International Brain Laboratory et al. (2022). Spike sorting pipeline for the International Brain Laboratory.
     https://www.internationalbrainlab.com/repro-ephys
     """
-    method_list = ("std", "mad", "coherence+psd")
+    import scipy.stats
+
+    method_list = ("std", "mad", "coherence+psd", "neighborhood_r2")
     assert method in method_list, f"{method} is not a valid method. Available methods are {method_list}"
 
     # Get random subset of data to estimate from
     random_chunk_kwargs = dict(
         num_chunks_per_segment=num_random_chunks,
         chunk_size=int(chunk_duration_s * recording.sampling_frequency),
-        seed=seed
+        seed=seed,
     )
 
     # If recording is not filtered, apply a highpass filter
     if not recording.is_filtered():
         recording_hp = highpass_filter(recording, freq_min=highpass_filter_cutoff)
     else:
         recording_hp = recording
 
     # Adjust random chunk kwargs based on method
     if method in ("std", "mad"):
         random_chunk_kwargs["return_scaled"] = False
         random_chunk_kwargs["concatenated"] = True
-    else:
+    elif method == "coherence+psd":
         random_chunk_kwargs["return_scaled"] = True
         random_chunk_kwargs["concatenated"] = False
+    elif method == "neighborhood_r2":
+        random_chunk_kwargs["return_scaled"] = False
+        random_chunk_kwargs["concatenated"] = False
 
     random_data = get_random_data_chunks(recording_hp, **random_chunk_kwargs)
 
-    channel_labels = np.zeros(recording.get_num_channels(), dtype='U5')
-    channel_labels[:] = 'good'
+    channel_labels = np.zeros(recording.get_num_channels(), dtype="U5")
+    channel_labels[:] = "good"
 
     if method in ("std", "mad"):
         if method == "std":
             deviations = np.std(random_data, axis=0)
         else:
             deviations = scipy.stats.median_abs_deviation(random_data, axis=0)
         thresh = std_mad_threshold * np.median(deviations)
         mask = deviations > thresh
         bad_channel_ids = recording.channel_ids[mask]
-        channel_labels[mask] = 'noise'
+        channel_labels[mask] = "noise"
 
     elif method == "coherence+psd":
         # some checks
-        assert recording.has_scaled(), \
-            ("The 'coherence+psd' method uses thresholds assuming the traces are in uV, "
-             "but the recording does not have scaled traces. If the recording is already scaled, "
-             "you need to set gains and offsets: "
-             ">>> recording.set_channel_gains(1); recording.set_channel_offsets(0)")
+        assert recording.has_scaled(), (
+            "The 'coherence+psd' method uses thresholds assuming the traces are in uV, "
+            "but the recording does not have scaled traces. If the recording is already scaled, "
+            "you need to set gains and offsets: "
+            ">>> recording.set_channel_gains(1); recording.set_channel_offsets(0)"
+        )
         assert 0 < nyquist_threshold < 1, "nyquist_threshold must be between 0 and 1"
 
         # If location are not sorted, estimate forward and reverse sorting
         channel_locations = recording.get_channel_locations()
         dim = ["x", "y", "z"].index(direction)
-        assert dim < channel_locations.ndim, f"Direction {direction} is wrong"
+        assert dim < channel_locations.shape[1], f"Direction {direction} is wrong"
         locs_depth = channel_locations[:, dim]
         if np.array_equal(np.sort(locs_depth), locs_depth):
             order_f = None
             order_r = None
         else:
             # sort by x, y to avoid ambiguity
-            order_f, order_r = order_channels_by_depth(recording=recording, dimensions=('x', 'y'))
+            order_f, order_r = order_channels_by_depth(recording=recording, dimensions=("x", "y"))
 
         # Create empty channel labels and fill with bad-channel detection estimate for each chunk
         chunk_channel_labels = np.zeros((recording.get_num_channels(), len(random_data)), dtype=np.int8)
 
         for i, random_chunk in enumerate(random_data):
             random_chunk_sorted = random_chunk[order_f] if order_f is not None else random_chunk
-            chunk_channel_labels[:, i] = detect_bad_channels_ibl(raw=random_chunk_sorted,
-                                                                 fs=recording.sampling_frequency,
-                                                                 psd_hf_threshold=psd_hf_threshold,
-                                                                 dead_channel_thr=dead_channel_threshold,
-                                                                 noisy_channel_thr=noisy_channel_threshold,
-                                                                 outside_channel_thr=outside_channel_threshold,
-                                                                 n_neighbors=n_neighbors,
-                                                                 nyquist_threshold=nyquist_threshold,
-                                                                 welch_window_ms=welch_window_ms)
+            chunk_channel_labels[:, i] = detect_bad_channels_ibl(
+                raw=random_chunk_sorted,
+                fs=recording.sampling_frequency,
+                psd_hf_threshold=psd_hf_threshold,
+                dead_channel_thr=dead_channel_threshold,
+                noisy_channel_thr=noisy_channel_threshold,
+                outside_channel_thr=outside_channel_threshold,
+                n_neighbors=n_neighbors,
+                nyquist_threshold=nyquist_threshold,
+                welch_window_ms=welch_window_ms,
+            )
 
         # Take the mode of the chunk estimates as final result. Convert to binary good / bad channel output.
         mode_channel_labels, _ = scipy.stats.mode(chunk_channel_labels, axis=1, keepdims=False)
         if order_r is not None:
             mode_channel_labels = mode_channel_labels[order_r]
 
-        bad_inds, = np.where(mode_channel_labels != 0)
+        (bad_inds,) = np.where(mode_channel_labels != 0)
         bad_channel_ids = recording.channel_ids[bad_inds]
 
-        channel_labels[mode_channel_labels == 1] = 'dead'
-        channel_labels[mode_channel_labels == 2] = 'noise'
-        channel_labels[mode_channel_labels == 3] = 'out'
+        channel_labels[mode_channel_labels == 1] = "dead"
+        channel_labels[mode_channel_labels == 2] = "noise"
+        channel_labels[mode_channel_labels == 3] = "out"
 
         if bad_channel_ids.size > recording.get_num_channels() / 3:
-            warnings.warn("Over 1/3 of channels are detected as bad. In the precense of a high"
-                          "number of dead / noisy channels, bad channel detection may fail "
-                          "(erroneously label good channels as dead).")
+            warnings.warn(
+                "Over 1/3 of channels are detected as bad. In the precense of a high"
+                "number of dead / noisy channels, bad channel detection may fail "
+                "(erroneously label good channels as dead)."
+            )
+
+    elif method == "neighborhood_r2":
+        # make neighboring channels structure. this should probably be a function in core.
+        geom = recording.get_channel_locations()
+        num_channels = recording.get_num_channels()
+        chan_distances = np.linalg.norm(geom[:, None, :] - geom[None, :, :], axis=2)
+        np.fill_diagonal(chan_distances, neighborhood_r2_radius_um + 1)
+        neighbors_mask = chan_distances < neighborhood_r2_radius_um
+        if neighbors_mask.sum(axis=1).min() < 1:
+            warnings.warn(
+                f"neighborhood_r2_radius_um={neighborhood_r2_radius_um} led "
+                "to channels with no neighbors for this geometry, which has "
+                f"minimal channel distance {chan_distances.min()}um. These "
+                "channels will not be marked as bad, but you might want to "
+                "check them."
+            )
+        max_neighbors = neighbors_mask.sum(axis=1).max()
+        channel_index = np.full((num_channels, max_neighbors), num_channels)
+        for c in range(num_channels):
+            my_neighbors = np.flatnonzero(neighbors_mask[c])
+            channel_index[c, : my_neighbors.size] = my_neighbors
+
+        # get the correlation of each channel with its neighbors' median inside each chunk
+        # note that we did not concatenate the chunks here
+        correlations = []
+        for chunk in random_data:
+            chunk = chunk.astype(np.float32, copy=False)
+            chunk = chunk - np.median(chunk, axis=0, keepdims=True)
+            padded_chunk = np.pad(chunk, [(0, 0), (0, 1)], constant_values=np.nan)
+            # channels with no neighbors will get a pure-nan median trace here
+            neighbmeans = np.nanmedian(
+                padded_chunk[:, channel_index],
+                axis=2,
+            )
+            denom = np.sqrt(np.nanmean(np.square(chunk), axis=0) * np.nanmean(np.square(neighbmeans), axis=0))
+            denom[denom == 0] = 1
+            # channels with no neighbors will get a nan here
+            chunk_correlations = np.nanmean(chunk * neighbmeans, axis=0) / denom
+            correlations.append(chunk_correlations)
+
+        # now take the median over chunks and threshold to finish
+        median_correlations = np.nanmedian(correlations, 0)
+        r2s = median_correlations**2
+        # channels with no neighbors will have r2==nan, and nan<x==False always
+        bad_channel_mask = r2s < neighborhood_r2_threshold
+        bad_channel_ids = recording.channel_ids[bad_channel_mask]
+        channel_labels[bad_channel_mask] = "noise"
 
     return bad_channel_ids, channel_labels
 
 
 # ----------------------------------------------------------------------------------------------
 # IBL Detect Bad Channels
 # ----------------------------------------------------------------------------------------------
 
-def detect_bad_channels_ibl(raw, fs, psd_hf_threshold,
-                            dead_channel_thr=-0.5,
-                            noisy_channel_thr=1.,
-                            outside_channel_thr=-0.75,
-                            n_neighbors=11,
-                            nyquist_threshold=0.8, 
-                            welch_window_ms=0.3):
+
+def detect_bad_channels_ibl(
+    raw,
+    fs,
+    psd_hf_threshold,
+    dead_channel_thr=-0.5,
+    noisy_channel_thr=1.0,
+    outside_channel_thr=-0.75,
+    n_neighbors=11,
+    nyquist_threshold=0.8,
+    welch_window_ms=0.3,
+):
     """
     Bad channels detection for Neuropixel probes developed by IBL
 
     Parameters
     ----------
     raw : traces
         (num_samples, n_channels) raw traces
     fs : float
         sampling frequency
     psd_hf_threshold : float
-        Threshold for high frequency PSD. If mean PSD above `nyquist_threshold` * fn is greater than this 
+        Threshold for high frequency PSD. If mean PSD above `nyquist_threshold` * fn is greater than this
         value, channels are flagged as noisy (together with channel coherence condition).
     dead_channel_thr : float, optional
         Threshold for channel coherence below which channels are labeled as dead, by default -0.5
     noisy_channel_thr : float
         Threshold for channel coherence above which channels are labeled as noisy (together with psd condition),
         by default -0.5
     outside_channel_thr : float
@@ -241,31 +313,32 @@
     -------
     1d array
         Channels labels: 0: good,  1: dead low coherence / amplitude, 2: noisy, 3: outside of the brain
     """
     _, nc = raw.shape
     raw = raw - np.mean(raw, axis=0)[np.newaxis, :]
     nperseg = int(welch_window_ms * fs / 1000)
-    fscale, psd = scipy.signal.welch(raw, fs=fs, axis=0, window='hann', nperseg=nperseg)
+    import scipy.signal
+
+    fscale, psd = scipy.signal.welch(raw, fs=fs, axis=0, window="hann", nperseg=nperseg)
 
     # compute similarities
     ref = np.median(raw, axis=1)
     xcorr = np.sum(raw * ref[:, np.newaxis], axis=0) / np.sum(ref**2)
 
     # compute coherence
     xcorr_neighbors = detrend(xcorr, n_neighbors)
     xcorr_distant = xcorr - detrend(xcorr, n_neighbors) - 1
 
     # make recommendation
     psd_hf = np.mean(psd[fscale > (fs / 2 * nyquist_threshold), :], axis=0)
 
     ichannels = np.zeros(nc, dtype=int)
     idead = np.where(xcorr_neighbors < dead_channel_thr)[0]
-    inoisy = np.where(np.logical_or(psd_hf > psd_hf_threshold,
-                                    xcorr_neighbors > noisy_channel_thr))[0]
+    inoisy = np.where(np.logical_or(psd_hf > psd_hf_threshold, xcorr_neighbors > noisy_channel_thr))[0]
 
     ichannels[idead] = 1
     ichannels[inoisy] = 2
 
     # the channels outside of the brains are the contiguous channels below the threshold on the trend coherency
     # the chanels outide need to be at either extremes of the probe
     ioutside = np.where(xcorr_distant < outside_channel_thr)[0]
@@ -277,20 +350,23 @@
     return ichannels
 
 
 # ----------------------------------------------------------------------------------------------
 # IBL Helpers
 # ----------------------------------------------------------------------------------------------
 
+
 def detrend(x, nmed):
     """
     Subtract the trend from a vector
     The trend is a median filtered version of the said vector with tapering
     :param x: input vector
     :param nmed: number of points of the median filter
     :return: np.array
     """
     ntap = int(np.ceil(nmed / 2))
     xf = np.r_[np.zeros(ntap) + x[0], x, np.zeros(ntap) + x[-1]]
 
+    import scipy.signal
+
     xf = scipy.signal.medfilt(xf, nmed)[ntap:-ntap]
     return x - xf
```

### Comparing `spikeinterface-0.97.1/spikeinterface/preprocessing/filter.py` & `spikeinterface-0.98.0/src/spikeinterface/preprocessing/filter.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,18 +1,16 @@
 import numpy as np
-import scipy.signal
 
 from spikeinterface.core.core_tools import define_function_from_class
 from .basepreprocessor import BasePreprocessor, BasePreprocessorSegment
 
 from ..core import get_chunk_with_margin
 
 
-_common_filter_docs = \
-    """**filter_kwargs: keyword arguments for parallel processing:
+_common_filter_docs = """**filter_kwargs: keyword arguments for parallel processing:
 
             * filter_order: order
                 The order of the filter
             * filter_mode: 'sos or 'ba'
                 'sos' is bi quadratic and more stable than ab so thery are prefered.
             * ftype: str
                 Filter type for iirdesign ('butter' / 'cheby1' / ... all possible of scipy.signal.iirdesign)
@@ -40,91 +38,118 @@
     margin_ms: float
         Margin in ms on border to avoid border effect
     filter_mode: str 'sos' or 'ba'
         Filter form of the filter coefficients:
         - second-order sections (default): 'sos'
         - numerator/denominator: 'ba'
     coef: ndarray or None
-        Filter coefficients in the filter_mode form. 
+        Filter coefficients in the filter_mode form.
     dtype: dtype or None
         The dtype of the returned traces. If None, the dtype of the parent recording is used
     {}
 
     Returns
     -------
     filter_recording: FilterRecording
         The filtered recording extractor object
 
     """
-    name = 'filter'
 
-    def __init__(self, recording, band=[300., 6000.], btype='bandpass',
-                 filter_order=5, ftype='butter', filter_mode='sos', margin_ms=5.0,
-                 coeff=None, dtype=None):
+    name = "filter"
 
-        assert filter_mode in ('sos', 'ba')
-        sf = recording.get_sampling_frequency()
+    def __init__(
+        self,
+        recording,
+        band=[300.0, 6000.0],
+        btype="bandpass",
+        filter_order=5,
+        ftype="butter",
+        filter_mode="sos",
+        margin_ms=5.0,
+        add_reflect_padding=False,
+        coeff=None,
+        dtype=None,
+    ):
+        import scipy.signal
+
+        assert filter_mode in ("sos", "ba")
+        fs = recording.get_sampling_frequency()
         if coeff is None:
-            assert btype in ('bandpass', 'highpass')
+            assert btype in ("bandpass", "highpass")
             # coefficient
-            if btype in ('bandpass', 'bandstop'):
-                assert len(band) == 2
-                Wn = [e / sf * 2 for e in band]
-            else:
-                Wn = float(band) / sf * 2
-            N = filter_order
             # self.coeff is 'sos' or 'ab' style
-            filter_coeff = scipy.signal.iirfilter(N, Wn, analog=False, btype=btype, ftype=ftype, output=filter_mode)
+            filter_coeff = scipy.signal.iirfilter(
+                filter_order, band, fs=fs, analog=False, btype=btype, ftype=ftype, output=filter_mode
+            )
         else:
             filter_coeff = coeff
             if not isinstance(coeff, list):
-                if filter_mode == 'ba':
+                if filter_mode == "ba":
                     coeff = [c.tolist() for c in coeff]
                 else:
                     coeff = coeff.tolist()
         dtype = fix_dtype(recording, dtype)
 
         BasePreprocessor.__init__(self, recording, dtype=dtype)
         self.annotate(is_filtered=True)
 
         if "offset_to_uV" in self.get_property_keys():
             self.set_channel_offsets(0)
 
-        margin = int(margin_ms * sf / 1000.)
+        margin = int(margin_ms * fs / 1000.0)
         for parent_segment in recording._recording_segments:
-            self.add_recording_segment(FilterRecordingSegment(parent_segment, filter_coeff, filter_mode, margin,
-                                                              dtype))
-
-        self._kwargs = dict(recording=recording.to_dict(), band=band, btype=btype,
-                            filter_order=filter_order, ftype=ftype, filter_mode=filter_mode, coeff=coeff,
-                            margin_ms=margin_ms, dtype=dtype.str)
+            self.add_recording_segment(
+                FilterRecordingSegment(
+                    parent_segment, filter_coeff, filter_mode, margin, dtype, add_reflect_padding=add_reflect_padding
+                )
+            )
+
+        self._kwargs = dict(
+            recording=recording,
+            band=band,
+            btype=btype,
+            filter_order=filter_order,
+            ftype=ftype,
+            filter_mode=filter_mode,
+            coeff=coeff,
+            margin_ms=margin_ms,
+            add_reflect_padding=add_reflect_padding,
+            dtype=dtype.str,
+        )
 
 
 class FilterRecordingSegment(BasePreprocessorSegment):
-    def __init__(self, parent_recording_segment, coeff, filter_mode, margin, dtype):
+    def __init__(self, parent_recording_segment, coeff, filter_mode, margin, dtype, add_reflect_padding=False):
         BasePreprocessorSegment.__init__(self, parent_recording_segment)
-
         self.coeff = coeff
         self.filter_mode = filter_mode
         self.margin = margin
+        self.add_reflect_padding = add_reflect_padding
         self.dtype = dtype
 
     def get_traces(self, start_frame, end_frame, channel_indices):
-        traces_chunk, left_margin, right_margin = get_chunk_with_margin(self.parent_recording_segment,
-                                                                        start_frame, end_frame, channel_indices,
-                                                                        self.margin)
+        traces_chunk, left_margin, right_margin = get_chunk_with_margin(
+            self.parent_recording_segment,
+            start_frame,
+            end_frame,
+            channel_indices,
+            self.margin,
+            add_reflect_padding=self.add_reflect_padding,
+        )
 
         traces_dtype = traces_chunk.dtype
         # if uint --> force int
         if traces_dtype.kind == "u":
             traces_chunk = traces_chunk.astype("float32")
 
-        if self.filter_mode == 'sos':
+        import scipy.signal
+
+        if self.filter_mode == "sos":
             filtered_traces = scipy.signal.sosfiltfilt(self.coeff, traces_chunk, axis=0)
-        elif self.filter_mode == 'ba':
+        elif self.filter_mode == "ba":
             b, a = self.coeff
             filtered_traces = scipy.signal.filtfilt(b, a, traces_chunk, axis=0)
 
         if right_margin > 0:
             filtered_traces = filtered_traces[left_margin:-right_margin, :]
         else:
             filtered_traces = filtered_traces[left_margin:, :]
@@ -149,21 +174,25 @@
         The dtype of the returned traces. If None, the dtype of the parent recording is used
     {}
     Returns
     -------
     filter_recording: BandpassFilterRecording
         The bandpass-filtered recording extractor object
     """
-    name = 'bandpass_filter'
 
-    def __init__(self, recording, freq_min=300., freq_max=6000., margin_ms=5.0, dtype=None, **filter_kwargs):
-        FilterRecording.__init__(self, recording, band=[freq_min, freq_max], margin_ms=margin_ms, dtype=dtype,
-                                 **filter_kwargs)
+    name = "bandpass_filter"
+
+    def __init__(self, recording, freq_min=300.0, freq_max=6000.0, margin_ms=5.0, dtype=None, **filter_kwargs):
+        FilterRecording.__init__(
+            self, recording, band=[freq_min, freq_max], margin_ms=margin_ms, dtype=dtype, **filter_kwargs
+        )
         dtype = fix_dtype(recording, dtype)
-        self._kwargs = dict(recording=recording.to_dict(), freq_min=freq_min, freq_max=freq_max, margin_ms=margin_ms, dtype=dtype.str)
+        self._kwargs = dict(
+            recording=recording, freq_min=freq_min, freq_max=freq_max, margin_ms=margin_ms, dtype=dtype.str
+        )
         self._kwargs.update(filter_kwargs)
 
 
 class HighpassFilterRecording(FilterRecording):
     """
     Highpass filter of a recording
 
@@ -179,21 +208,23 @@
         The dtype of the returned traces. If None, the dtype of the parent recording is used
     {}
     Returns
     -------
     filter_recording: HighpassFilterRecording
         The highpass-filtered recording extractor object
     """
-    name = 'highpass_filter'
 
-    def __init__(self, recording, freq_min=300., margin_ms=5.0, dtype=None, **filter_kwargs):
-        FilterRecording.__init__(self, recording, band=freq_min, margin_ms=margin_ms, dtype=dtype,
-                                 btype='highpass', **filter_kwargs)
+    name = "highpass_filter"
+
+    def __init__(self, recording, freq_min=300.0, margin_ms=5.0, dtype=None, **filter_kwargs):
+        FilterRecording.__init__(
+            self, recording, band=freq_min, margin_ms=margin_ms, dtype=dtype, btype="highpass", **filter_kwargs
+        )
         dtype = fix_dtype(recording, dtype)
-        self._kwargs = dict(recording=recording.to_dict(), freq_min=freq_min, margin_ms=margin_ms, dtype=dtype.str)
+        self._kwargs = dict(recording=recording, freq_min=freq_min, margin_ms=margin_ms, dtype=dtype.str)
         self._kwargs.update(filter_kwargs)
 
 
 class NotchFilterRecording(BasePreprocessor):
     """
     Parameters
     ----------
@@ -205,39 +236,44 @@
         The quality factor of the notch filter
     {}
     Returns
     -------
     filter_recording: NotchFilterRecording
         The notch-filtered recording extractor object
     """
-    name = 'notch_filter'
+
+    name = "notch_filter"
 
     def __init__(self, recording, freq=3000, q=30, margin_ms=5.0, dtype=None):
         # coeef is 'ba' type
         fn = 0.5 * float(recording.get_sampling_frequency())
+        import scipy.signal
+
         coeff = scipy.signal.iirnotch(freq / fn, q)
 
         if dtype is None:
             dtype = recording.get_dtype()
         dtype = np.dtype(dtype)
 
         # if uint --> unsupported
         if dtype.kind == "u":
-            raise TypeError("The notch filter only supports signed types. Use the 'dtype' argument"
-                            "to specify a signed type (e.g. 'int16', 'float32'")
+            raise TypeError(
+                "The notch filter only supports signed types. Use the 'dtype' argument"
+                "to specify a signed type (e.g. 'int16', 'float32'"
+            )
 
         BasePreprocessor.__init__(self, recording, dtype=dtype)
         self.annotate(is_filtered=True)
 
         sf = recording.get_sampling_frequency()
-        margin = int(margin_ms * sf / 1000.)
+        margin = int(margin_ms * sf / 1000.0)
         for parent_segment in recording._recording_segments:
-            self.add_recording_segment(FilterRecordingSegment(parent_segment, coeff, 'ba', margin, dtype))
+            self.add_recording_segment(FilterRecordingSegment(parent_segment, coeff, "ba", margin, dtype))
 
-        self._kwargs = dict(recording=recording.to_dict(), freq=freq, q=q, margin_ms=margin_ms, dtype=dtype.str)
+        self._kwargs = dict(recording=recording, freq=freq, q=q, margin_ms=margin_ms, dtype=dtype.str)
 
 
 # functions for API
 filter = define_function_from_class(source_class=FilterRecording, name="filter")
 bandpass_filter = define_function_from_class(source_class=BandpassFilterRecording, name="bandpass_filter")
 notch_filter = define_function_from_class(source_class=NotchFilterRecording, name="notch_filter")
 highpass_filter = define_function_from_class(source_class=HighpassFilterRecording, name="highpass_filter")
```

### Comparing `spikeinterface-0.97.1/spikeinterface/preprocessing/filter_opencl.py` & `spikeinterface-0.98.0/src/spikeinterface/preprocessing/filter_opencl.py`

 * *Files 6% similar despite different names*

```diff
@@ -13,92 +13,107 @@
 except ImportError:
     HAVE_PYOPENCL = False
 
 
 class FilterOpenCLRecording(BasePreprocessor):
     """
     Simple implementation of FilterRecording in OpenCL.
-    
+
     Only filter_mode='sos' is supported.
-    
+
     Author : Samuel Garcia
     This kernel is ported from "tridesclous"
-    
+
     Parameters
     ----------
     recording: Recording
         The recording extractor to be re-referenced
-    
+
     N: order
     filter_mode: 'sos' only
 
     ftypestr: 'butter' / 'cheby1' / ... all possible of scipy.signal.iirdesign
-    
+
     margin: margin in second on border to avoid border effect
-    
+
     """
-    name = 'filter'
 
-    def __init__(self, recording, band=[300., 6000.], btype='bandpass',
-                 filter_order=5, ftype='butter', filter_mode='sos', margin_ms=5.0):
+    name = "filter"
 
-        assert HAVE_PYOPENCL, 'You need to install pyopencl (and GPU driver!!)'
+    def __init__(
+        self,
+        recording,
+        band=[300.0, 6000.0],
+        btype="bandpass",
+        filter_order=5,
+        ftype="butter",
+        filter_mode="sos",
+        margin_ms=5.0,
+    ):
+        assert HAVE_PYOPENCL, "You need to install pyopencl (and GPU driver!!)"
 
-        assert btype in ('bandpass', 'lowpass', 'highpass', 'bandstop')
-        assert filter_mode in ('sos',)
+        assert btype in ("bandpass", "lowpass", "highpass", "bandstop")
+        assert filter_mode in ("sos",)
 
         # coefficient
         sf = recording.get_sampling_frequency()
-        if btype in ('bandpass', 'bandstop'):
+        if btype in ("bandpass", "bandstop"):
             assert len(band) == 2
             Wn = [e / sf * 2 for e in band]
         else:
             Wn = float(band) / sf * 2
         N = filter_order
 
         coefficients = scipy.signal.iirfilter(N, Wn, analog=False, btype=btype, ftype=ftype, output=filter_mode)
 
         BasePreprocessor.__init__(self, recording)
 
-        margin = int(margin_ms * sf / 1000.)
+        margin = int(margin_ms * sf / 1000.0)
         num_channels = recording.get_num_channels()
         dtype = recording.get_dtype()
         # DEBUG force float32 at the moement
-        dtype = 'float32'
+        dtype = "float32"
         executor = OpenCLFilterExecutor(coefficients, num_channels, dtype, margin)
 
         for parent_segment in recording._recording_segments:
             self.add_recording_segment(FilterOpenCLRecordingSegment(parent_segment, executor, margin))
 
-        self._kwargs = dict(recording=recording.to_dict(), band=band, btype=btype,
-                            filter_order=filter_order, ftype=ftype, filter_mode=filter_mode, margin_ms=margin_ms)
+        self._kwargs = dict(
+            recording=recording,
+            band=band,
+            btype=btype,
+            filter_order=filter_order,
+            ftype=ftype,
+            filter_mode=filter_mode,
+            margin_ms=margin_ms,
+        )
 
 
 class FilterOpenCLRecordingSegment(BasePreprocessorSegment):
     def __init__(self, parent_recording_segment, executor, margin):
         BasePreprocessorSegment.__init__(self, parent_recording_segment)
 
         self.executor = executor
         self.margin = margin
 
     def get_traces(self, start_frame, end_frame, channel_indices):
-        assert start_frame is not None, 'FilterOpenCLRecording work with fixed chunk_size'
-        assert end_frame is not None, 'FilterOpenCLRecording work with fixed chunk_size'
+        assert start_frame is not None, "FilterOpenCLRecording work with fixed chunk_size"
+        assert end_frame is not None, "FilterOpenCLRecording work with fixed chunk_size"
 
         chunk_size = end_frame - start_frame
         if chunk_size != self.executor.chunk_size:
             self.executor.create_buffers_and_compile(chunk_size)
 
         #  get with margin and force zeros!!
-        traces_chunk, left_margin, right_margin = get_chunk_with_margin(self.parent_recording_segment,
-                                                                        start_frame, end_frame, channel_indices,
-                                                                        self.margin, add_zeros=True)
+        traces_chunk, left_margin, right_margin = get_chunk_with_margin(
+            self.parent_recording_segment, start_frame, end_frame, channel_indices, self.margin, add_zeros=True
+        )
 
         # DEBUG
-        traces_chunk_float32 = traces_chunk.astype('float32')
+        traces_chunk_float32 = traces_chunk.astype("float32")
 
         filtered_traces = self.executor.process(traces_chunk_float32)
 
         # DEBUG
         filtered_traces = filtered_traces.astype(traces_chunk.dtype)
 
         if right_margin > 0:
@@ -108,23 +123,23 @@
 
         return filtered_traces
 
 
 class OpenCLFilterExecutor:
     """
     Executor function shared across FilterOpenCLRecordingSegment.
-    
+
     The input/ouput can be  float32 only (int16 will be implemented soon).
-    
+
     Internally it is computed as float32 with coeff
-    
+
     """
 
     def __init__(self, coefficients, num_channels, dtype, margin):
-        self.coefficients = np.ascontiguousarray(coefficients, dtype='float32')
+        self.coefficients = np.ascontiguousarray(coefficients, dtype="float32")
         self.num_channels = num_channels
         self.dtype = np.dtype(dtype)
         self.margin = margin
 
         #  plat = pyopencl.get_platforms()
         #  dev = plat[0].get_devices()
         #  ctx = pyopencl.Context(dev)
@@ -138,35 +153,43 @@
         self.full_size = None
 
     def process(self, traces):
         assert traces.dtype == self.dtype
 
         if traces.shape[0] != self.full_size:
             if self.full_size is not None:
-                print(f'Warning : chunk_size have change {self.chunk_size} {traces.shape[0]}, need recompile CL!!!')
+                print(f"Warning : chunk_size have change {self.chunk_size} {traces.shape[0]}, need recompile CL!!!")
             self.create_buffers_and_compile()
 
         event = pyopencl.enqueue_copy(self.queue, self.input_cl, traces)
-        event = self.kern_sosfiltfilt(self.queue, (self.num_channels,), (self.num_channels,),
-                                      self.input_cl, self.output_cl, self.coefficients_cl, self.zi1_cl, self.zi2_cl)
+        event = self.kern_sosfiltfilt(
+            self.queue,
+            (self.num_channels,),
+            (self.num_channels,),
+            self.input_cl,
+            self.output_cl,
+            self.coefficients_cl,
+            self.zi1_cl,
+            self.zi2_cl,
+        )
         event.wait()
         event = pyopencl.enqueue_copy(self.queue, self.output, self.output_cl)
 
         return self.output
 
     def create_buffers_and_compile(self, chunk_size):
         self.chunk_size = chunk_size
         self.full_size = chunk_size + self.margin * 2
         n_section = self.coefficients.shape[0]
 
         buffer_nbytes = self.full_size * self.num_channels * self.dtype.itemsize
 
         # this is for stream processing
-        self.zi1 = np.zeros((self.num_channels, n_section, 2), dtype='float32')
-        self.zi2 = np.zeros((self.num_channels, n_section, 2), dtype='float32')
+        self.zi1 = np.zeros((self.num_channels, n_section, 2), dtype="float32")
+        self.zi2 = np.zeros((self.num_channels, n_section, 2), dtype="float32")
         self.output = np.zeros((self.full_size, self.num_channels), dtype=self.dtype)
 
         # GPU buffers
         self.coefficients_cl = pyopencl.Buffer(self.ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=self.coefficients)
         self.zi1_cl = pyopencl.Buffer(self.ctx, mf.READ_WRITE | mf.COPY_HOST_PTR, hostbuf=self.zi1)
         self.zi2_cl = pyopencl.Buffer(self.ctx, mf.READ_WRITE | mf.COPY_HOST_PTR, hostbuf=self.zi2)
         self.input_cl = pyopencl.Buffer(self.ctx, mf.READ_WRITE, size=buffer_nbytes)
@@ -179,85 +202,85 @@
         )
         kernel_formated = processor_kernel % variables
         # print(kernel_formated)
 
         prg = pyopencl.Program(self.ctx, kernel_formated)
         self.opencl_prg = prg.build()  # options='-cl-mad-enable'
 
-        self.kern_sosfiltfilt = getattr(self.opencl_prg, 'sosfiltfilt')
+        self.kern_sosfiltfilt = getattr(self.opencl_prg, "sosfiltfilt")
 
 
 processor_kernel = """
 #define full_size %(full_size)d
 #define n_section %(n_section)d
 #define num_channels %(num_channels)d
 
 
 __kernel void sos_filter(__global  float *input,
                                     __global  float *output,
-                                    __constant  float *coefficients, 
+                                    __constant  float *coefficients,
                                     __global float *zi,
                                     int local_chunksize,
                                     int direction) {
 
     int chan = get_global_id(0); //channel indice
-    
+
     int offset_filt2;  //offset channel within section
     int offset_zi = chan*n_section*2;
-    
+
     int idx;
 
     float w0, w1,w2;
     float res;
-    
+
     for (int section=0; section<n_section; section++){
-    
+
         //offset_filt2 = chan*n_section*6+section*6;
         offset_filt2 = section*6;
-        
+
         w1 = zi[offset_zi+section*2+0];
         w2 = zi[offset_zi+section*2+1];
-        
+
         for (int s=0; s<local_chunksize;s++){
-            
+
             if (direction==1) {idx = s*num_channels+chan;}
             else if (direction==-1) {idx = (local_chunksize-s-1)*num_channels+chan;}
-            
+
             if (section==0)  {w0 = input[idx];}
             else {w0 = output[idx];}
-            
+
             w0 -= coefficients[offset_filt2+4] * w1;
             w0 -= coefficients[offset_filt2+5] * w2;
             res = coefficients[offset_filt2+0] * w0 + coefficients[offset_filt2+1] * w1 +  coefficients[offset_filt2+2] * w2;
             w2 = w1; w1 =w0;
-            
+
             output[idx] = res;
         }
-        
+
         zi[offset_zi+section*2+0] = w1;
         zi[offset_zi+section*2+1] = w2;
 
     }
-   
+
 }
 
 
 __kernel void sosfiltfilt(__global  float *input,
                                         __global  float *output,
                                         __constant  float *coefficients,
                                         __global float * zi1,
                                         __global float * zi2
                                         ){
 
     int chan = get_global_id(0); //channel indice
 
     // filter forward
     sos_filter(input, input, coefficients, zi1, full_size, 1);
-    
+
     // filter backward
     sos_filter(input, output, coefficients, zi2, full_size, -1);
-    
+
 }
 
 
 
 """
```

### Comparing `spikeinterface-0.97.1/spikeinterface/preprocessing/highpass_spatial_filter.py` & `spikeinterface-0.98.0/src/spikeinterface/preprocessing/highpass_spatial_filter.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 import numpy as np
-import scipy.signal
+
 from .basepreprocessor import BasePreprocessor, BasePreprocessorSegment
 from ..core import order_channels_by_depth, get_chunk_with_margin
 from ..core.core_tools import define_function_from_class
 
 
 class HighpassSpatialFilterRecording(BasePreprocessor):
     """
@@ -23,17 +23,17 @@
     Applies a butterworth filter on the 0-axis with tapering / padding.
 
     Parameters
     ----------
     recording : BaseRecording
         The parent recording
     n_channel_pad : int
-        Number of channels to pad prior to filtering. 
-        Channels are padded with mirroring. 
-        If None, no padding is applied, by default 5
+        Number of channels to pad prior to filtering.
+        Channels are padded with mirroring.
+        If None, no padding is applied, by default 60
     n_channel_taper : int
         Number of channels to perform cosine tapering on
         prior to filtering. If None and n_channel_pad is set,
         n_channel_taper will be set to the number of padded channels.
         Otherwise, the passed value will be used, by default None
     direction : str
         The direction in which the spatial filter is applied, by default "y"
@@ -54,93 +54,119 @@
     References
     ----------
     Details of the high-pass spatial filter function (written by Olivier Winter)
     used in the IBL pipeline can be found at:
     International Brain Laboratory et al. (2022). Spike sorting pipeline for the International Brain Laboratory.
     https://www.internationalbrainlab.com/repro-ephys
     """
-    name = 'highpass_spatial_filter'
 
-    def __init__(self, recording, n_channel_pad=None, n_channel_taper=5, direction="y",
-                 apply_agc=True, agc_window_length_s=0.01, highpass_butter_order=3,
-                 highpass_butter_wn=0.01):
+    name = "highpass_spatial_filter"
+
+    def __init__(
+        self,
+        recording,
+        n_channel_pad=60,
+        n_channel_taper=0,
+        direction="y",
+        apply_agc=True,
+        agc_window_length_s=0.1,
+        highpass_butter_order=3,
+        highpass_butter_wn=0.01,
+    ):
         BasePreprocessor.__init__(self, recording)
 
+        import scipy.signal
+
         # Check single group
         channel_groups = recording.get_channel_groups()
-        assert len(np.unique(channel_groups)) == 1, \
-            ("The recording contains multiple groups! It is recommended to apply spatial filtering "
-             "separately for each group. You can split by group with: "
-             ">>> recording_split = recording.split_by(property='group')")
+        assert len(np.unique(channel_groups)) == 1, (
+            "The recording contains multiple groups! It is recommended to apply spatial filtering "
+            "separately for each group. You can split by group with: "
+            ">>> recording_split = recording.split_by(property='group')"
+        )
         # If location are not sorted, estimate forward and reverse sorting
         channel_locations = recording.get_channel_locations()
         dim = ["x", "y", "z"].index(direction)
         assert dim < channel_locations.ndim, f"Direction {direction} is wrong"
         locs_depth = channel_locations[:, dim]
         if np.array_equal(np.sort(locs_depth), locs_depth):
             order_f = None
             order_r = None
         else:
             # sort by x, y to avoid ambiguity
-            order_f, order_r = order_channels_by_depth(recording=recording, dimensions=('x', 'y'))
+            order_f, order_r = order_channels_by_depth(recording=recording, dimensions=("x", "y"))
 
         # Fix channel padding and tapering
         n_channels = recording.get_num_channels()
         n_channel_pad = 0 if n_channel_pad is None else int(n_channel_pad)
-        assert n_channel_pad <= recording.get_num_channels(), \
-            "'n_channel_pad' must be less than the number of channels in recording."
+        assert (
+            n_channel_pad <= recording.get_num_channels()
+        ), "'n_channel_pad' must be less than the number of channels in recording."
         n_channel_taper = n_channel_pad if n_channel_taper is None else int(n_channel_taper)
-        assert n_channel_taper <= recording.get_num_channels(), \
-            "'n_channel_taper' must be less than the number of channels in recording."
+        assert (
+            n_channel_taper <= recording.get_num_channels()
+        ), "'n_channel_taper' must be less than the number of channels in recording."
 
         # Fix Automatic Gain Control options
         sampling_interval = 1 / recording.sampling_frequency
         if not apply_agc:
             agc_window_length_s = None
 
         # Pre-compute spatial filtering parameters
-        butter_kwargs = dict(
-            btype='highpass',
-            N=highpass_butter_order,
-            Wn=highpass_butter_wn
-        )
-        sos_filter = scipy.signal.butter(**butter_kwargs, output='sos')
+        butter_kwargs = dict(btype="highpass", N=highpass_butter_order, Wn=highpass_butter_wn)
+        sos_filter = scipy.signal.butter(**butter_kwargs, output="sos")
 
         for parent_segment in recording._recording_segments:
-            rec_segment = HighPassSpatialFilterSegment(parent_segment,
-                                                       n_channel_pad,
-                                                       n_channel_taper,
-                                                       n_channels,
-                                                       agc_window_length_s,
-                                                       sampling_interval,
-                                                       sos_filter,
-                                                       order_f,
-                                                       order_r)
+            rec_segment = HighPassSpatialFilterSegment(
+                parent_segment,
+                n_channel_pad,
+                n_channel_taper,
+                n_channels,
+                agc_window_length_s,
+                sampling_interval,
+                sos_filter,
+                order_f,
+                order_r,
+            )
             self.add_recording_segment(rec_segment)
 
-        self._kwargs = dict(recording=recording.to_dict(),
-                            n_channel_pad=n_channel_pad,
-                            n_channel_taper=n_channel_taper,
-                            direction=direction,
-                            apply_agc=apply_agc,
-                            agc_window_length_s=agc_window_length_s,
-                            highpass_butter_order=highpass_butter_order,
-                            highpass_butter_wn=highpass_butter_wn)
+        self._kwargs = dict(
+            recording=recording,
+            n_channel_pad=n_channel_pad,
+            n_channel_taper=n_channel_taper,
+            direction=direction,
+            apply_agc=apply_agc,
+            agc_window_length_s=agc_window_length_s,
+            highpass_butter_order=highpass_butter_order,
+            highpass_butter_wn=highpass_butter_wn,
+        )
 
 
 class HighPassSpatialFilterSegment(BasePreprocessorSegment):
-    def __init__(self, parent_recording_segment, n_channel_pad, n_channel_taper,
-                 n_channels, agc_window_length_s, sampling_interval, sos_filter, order_f, order_r):
+    def __init__(
+        self,
+        parent_recording_segment,
+        n_channel_pad,
+        n_channel_taper,
+        n_channels,
+        agc_window_length_s,
+        sampling_interval,
+        sos_filter,
+        order_f,
+        order_r,
+    ):
         BasePreprocessorSegment.__init__(self, parent_recording_segment)
         self.parent_recording_segment = parent_recording_segment
         self.n_channel_pad = n_channel_pad
         if n_channel_taper > 0:
             num_channels_padded = n_channels + n_channel_pad * 2
             self.taper = fcn_cosine([0, n_channel_taper])(np.arange(num_channels_padded))  # taper up
-            self.taper *= 1 - fcn_cosine([num_channels_padded - n_channel_taper, num_channels_padded])(np.arange(num_channels_padded))   # taper down
+            self.taper *= 1 - fcn_cosine([num_channels_padded - n_channel_taper, num_channels_padded])(
+                np.arange(num_channels_padded)
+            )  # taper down
         else:
             self.taper = None
         if agc_window_length_s is not None:
             num_samples_window = int(np.round(agc_window_length_s / sampling_interval / 2) * 2 + 1)
             window = np.hanning(num_samples_window)
             window /= np.sum(window)
             self.window = window
@@ -154,43 +180,49 @@
     def get_traces(self, start_frame, end_frame, channel_indices):
         if channel_indices is None:
             channel_indices = slice(None)
         if self.window is not None:
             margin = len(self.window) // 2
         else:
             margin = 0
-        traces, left_margin, right_margin = get_chunk_with_margin(self.parent_recording_segment,
-                                                                  start_frame=start_frame, end_frame=end_frame,
-                                                                  channel_indices=slice(None), margin=margin)
+        traces, left_margin, right_margin = get_chunk_with_margin(
+            self.parent_recording_segment,
+            start_frame=start_frame,
+            end_frame=end_frame,
+            channel_indices=slice(None),
+            margin=margin,
+        )
         # apply sorting by depth
         if self.order_f is not None:
             traces = traces[:, self.order_f]
         else:
             traces = traces.copy()
 
         # apply AGC and keep the gains
         if self.window is not None:
             traces, agc_gains = agc(traces, window=self.window)
         else:
             agc_gains = None
         # pad the array with a mirrored version of itself and apply a cosine taper
         if self.n_channel_pad > 0:
-            traces = np.c_[np.fliplr(traces[:, :self.n_channel_pad]),
-                           traces,
-                           np.fliplr(traces[:, -self.n_channel_pad:])]
+            traces = np.c_[
+                np.fliplr(traces[:, : self.n_channel_pad]), traces, np.fliplr(traces[:, -self.n_channel_pad :])
+            ]
         # apply tapering
         if self.taper is not None:
             traces = traces * self.taper[np.newaxis, :]
 
         # apply actual HP filter
+        import scipy
+
         traces = scipy.signal.sosfiltfilt(self.sos_filter, traces, axis=1)
 
         # remove padding
         if self.n_channel_pad > 0:
-            traces = traces[:, self.n_channel_pad:-self.n_channel_pad]
+            traces = traces[:, self.n_channel_pad : -self.n_channel_pad]
 
         # remove AGC gains
         if agc_gains is not None:
             traces = traces * agc_gains
 
         # reverse sorting by depth
         if self.order_r is not None:
@@ -198,44 +230,50 @@
         # remove margin and slice channels
         if right_margin > 0:
             traces = traces[left_margin:-right_margin, channel_indices]
         else:
             traces = traces[left_margin:, channel_indices]
         return traces
 
+
 # function for API
-highpass_spatial_filter = define_function_from_class(source_class=HighpassSpatialFilterRecording,
-                                                     name="highpass_spatial_filter")
+highpass_spatial_filter = define_function_from_class(
+    source_class=HighpassSpatialFilterRecording, name="highpass_spatial_filter"
+)
 
 
 # -----------------------------------------------------------------------------------------------
 # IBL Helper Functions
 # -----------------------------------------------------------------------------------------------
 
+
 def agc(traces, window, epsilon=1e-8):
     """
     Automatic gain control
     w_agc, gain = agc(w, window_length=.5, si=.002, epsilon=1e-8)
     such as w_agc * gain = w
     :param traces: seismic array (sample last dimension)
     :param window_length: window length (secs) (original default 0.5)
     :param si: sampling interval (secs) (original default 0.002)
     :param epsilon: whitening (useful mainly for synthetic data)
     :return: AGC data array, gain applied to data
     """
-    gain = scipy.signal.fftconvolve(np.abs(traces), window[:, None], mode='same', axes=0)
+    import scipy.signal
+
+    gain = scipy.signal.fftconvolve(np.abs(traces), window[:, None], mode="same", axes=0)
 
     gain += (np.sum(gain, axis=0) * epsilon / traces.shape[0])[np.newaxis, :]
 
     dead_channels = np.sum(gain, axis=0) == 0
 
     traces[:, ~dead_channels] = traces[:, ~dead_channels] / gain[:, ~dead_channels]
 
     return traces, gain
 
+
 def fcn_extrap(x, f, bounds):
     """
     Extrapolates a flat value before and after bounds
     x: array to be filtered
     f: function to be applied between bounds (cf. fcn_cosine below)
     bounds: 2 elements list or np.array
     """
@@ -250,11 +288,13 @@
     Returns a soft thresholding function with a cosine taper:
     values <= bounds[0]: values
     values < bounds[0] < bounds[1] : cosine taper
     values < bounds[1]: bounds[1]
     :param bounds:
     :return: lambda function
     """
+
     def _cos(x):
         return (1 - np.cos((x - bounds[0]) / (bounds[1] - bounds[0]) * np.pi)) / 2
+
     func = lambda x: fcn_extrap(x, _cos, bounds)  # noqa
     return func
```

### Comparing `spikeinterface-0.97.1/spikeinterface/preprocessing/interpolate_bad_channels.py` & `spikeinterface-0.98.0/src/spikeinterface/preprocessing/interpolate_bad_channels.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,13 +1,12 @@
 import numpy as np
 
 from .basepreprocessor import BasePreprocessor, BasePreprocessorSegment
 from spikeinterface.core.core_tools import define_function_from_class
 from spikeinterface.preprocessing import preprocessing_tools
-import scipy.stats
 
 
 class InterpolateBadChannelsRecording(BasePreprocessor):
     """
     Interpolate the channel labeled as bad channels using linear interpolation.
     This is based on the distance (Gaussian kernel) from the bad channel,
     as determined from x,y channel coordinates.
@@ -35,15 +34,16 @@
         If None, weights are automatically computed, by default None
 
     Returns
     -------
     interpolated_recording: InterpolateBadChannelsRecording
         The recording object with interpolated bad channels
     """
-    name = 'interpolate_bad_channels'
+
+    name = "interpolate_bad_channels"
 
     def __init__(self, recording, bad_channel_ids, sigma_um=None, p=1.3, weights=None):
         BasePreprocessor.__init__(self, recording)
 
         bad_channel_ids = np.array(bad_channel_ids)
         self.check_inputs(recording, bad_channel_ids)
 
@@ -55,72 +55,64 @@
         if sigma_um is None:
             sigma_um = estimate_recommended_sigma_um(recording)
 
         if weights is None:
             locations = recording.get_channel_locations()
             locations_good = locations[self._good_channel_idxs]
             locations_bad = locations[self._bad_channel_idxs]
-            weights = preprocessing_tools.get_kriging_channel_weights(locations_good,
-                                                                      locations_bad,
-                                                                      sigma_um,
-                                                                      p)
+            weights = preprocessing_tools.get_kriging_channel_weights(locations_good, locations_bad, sigma_um, p)
 
         for parent_segment in recording._recording_segments:
-            rec_segment = InterpolateBadChannelsSegment(parent_segment,
-                                                        self._good_channel_idxs,
-                                                        self._bad_channel_idxs,
-                                                        weights)
+            rec_segment = InterpolateBadChannelsSegment(
+                parent_segment, self._good_channel_idxs, self._bad_channel_idxs, weights
+            )
             self.add_recording_segment(rec_segment)
 
-        self._kwargs = dict(recording=recording.to_dict(),
-                            bad_channel_ids=bad_channel_ids,
-                            p=p,
-                            sigma_um=sigma_um,
-                            weights=weights)
+        self._kwargs = dict(
+            recording=recording, bad_channel_ids=bad_channel_ids, p=p, sigma_um=sigma_um, weights=weights
+        )
 
     def check_inputs(self, recording, bad_channel_ids):
-
         if bad_channel_ids.ndim != 1:
             raise TypeError("'bad_channel_ids' must be a 1d array or list.")
 
-        if recording.get_property('contact_vector') is None:
-            raise ValueError('A probe must be attached to use bad channel interpolation. Use set_probe(...)')
+        if recording.get_property("contact_vector") is None:
+            raise ValueError("A probe must be attached to use bad channel interpolation. Use set_probe(...)")
 
         if recording.get_probe().si_units != "um":
             raise NotImplementedError("Channel spacing units must be um")
 
 
 class InterpolateBadChannelsSegment(BasePreprocessorSegment):
-
     def __init__(self, parent_recording_segment, good_channel_indices, bad_channel_indices, weights):
         BasePreprocessorSegment.__init__(self, parent_recording_segment)
 
         self._good_channel_indices = good_channel_indices
         self._bad_channel_indices = bad_channel_indices
         self._weights = weights
 
     def get_traces(self, start_frame, end_frame, channel_indices):
         if channel_indices is None:
             channel_indices = slice(None)
 
-        traces = self.parent_recording_segment.get_traces(start_frame,
-                                                          end_frame,
-                                                          slice(None))
+        traces = self.parent_recording_segment.get_traces(start_frame, end_frame, slice(None))
 
         traces = traces.copy()
 
         traces[:, self._bad_channel_indices] = traces[:, self._good_channel_indices] @ self._weights
 
         return traces[:, channel_indices]
 
 
 def estimate_recommended_sigma_um(recording):
     """
     Get the most common distance between channels on the y-axis
     """
     y_sorted = np.sort(recording.get_channel_locations()[:, 1])
+    import scipy.stats
 
     return scipy.stats.mode(np.diff(np.unique(y_sorted)), keepdims=False)[0]
 
 
-interpolate_bad_channels = define_function_from_class(source_class=InterpolateBadChannelsRecording,
-                                                      name='interpolate_bad_channels')
+interpolate_bad_channels = define_function_from_class(
+    source_class=InterpolateBadChannelsRecording, name="interpolate_bad_channels"
+)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/preprocessing/normalize_scale.py` & `spikeinterface-0.98.0/src/spikeinterface/preprocessing/normalize_scale.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,27 +1,29 @@
 import numpy as np
 
 from spikeinterface.core.core_tools import define_function_from_class
 
 from .basepreprocessor import BasePreprocessor, BasePreprocessorSegment
 
+from .filter import fix_dtype
+
 from ..core import get_random_data_chunks
 
 
 class ScaleRecordingSegment(BasePreprocessorSegment):
     # use by NormalizeByQuantileRecording/ScaleRecording/CenterRecording
     def __init__(self, parent_recording_segment, gain, offset, dtype):
         BasePreprocessorSegment.__init__(self, parent_recording_segment)
         self.gain = gain
         self.offset = offset
         self._dtype = dtype
 
     def get_traces(self, start_frame, end_frame, channel_indices):
         traces = self.parent_recording_segment.get_traces(start_frame, end_frame, channel_indices)
-        scaled_traces = traces * self.gain[:, channel_indices] + self.offset[:, channel_indices] 
+        scaled_traces = traces * self.gain[:, channel_indices] + self.offset[:, channel_indices]
         return scaled_traces.astype(self._dtype)
 
 
 class NormalizeByQuantileRecording(BasePreprocessor):
     """
     Rescale the traces from the given recording extractor with a scalar
     and offset. First, the median and quantiles of the distribution are estimated.
@@ -36,46 +38,55 @@
     scalar: float
         Scale for the output distribution
     median: float
         Median for the output distribution
     q1: float (default 0.01)
         Lower quantile used for measuring the scale
     q1: float (default 0.99)
-        Upper quantile used for measuring the 
+        Upper quantile used for measuring the
     seed: int
         Random seed for reproducibility
     dtype: str or np.dtype
         The dtype of the output traces. Default "float32"
-    **random_chunk_kwargs: keyword arguments for `get_random_data_chunks()` function
-    
+    **random_chunk_kwargs: Keyword arguments for `spikeinterface.core.get_random_data_chunk()` function
+
     Returns
     -------
     rescaled_traces: NormalizeByQuantileRecording
         The rescaled traces recording extractor object
     """
-    name = 'normalize_by_quantile'
 
-    def __init__(self, recording, scale=1.0, median=0.0, q1=0.01, q2=0.99,
-                 mode='by_channel', dtype="float32", **random_chunk_kwargs):
+    name = "normalize_by_quantile"
 
-        assert mode in ('pool_channel', 'by_channel')
+    def __init__(
+        self,
+        recording,
+        scale=1.0,
+        median=0.0,
+        q1=0.01,
+        q2=0.99,
+        mode="by_channel",
+        dtype="float32",
+        **random_chunk_kwargs,
+    ):
+        assert mode in ("pool_channel", "by_channel")
 
         random_data = get_random_data_chunks(recording, **random_chunk_kwargs)
 
-        if mode == 'pool_channel':
+        if mode == "pool_channel":
             num_chans = recording.get_num_channels()
             # old behavior
             loc_q1, pre_median, loc_q2 = np.quantile(random_data, q=[q1, 0.5, q2])
             pre_scale = abs(loc_q2 - loc_q1)
             gain = scale / pre_scale
             offset = median - pre_median * gain
             gain = np.ones((1, num_chans)) * gain
             offset = np.ones((1, num_chans)) * offset
 
-        elif mode == 'by_channel':
+        elif mode == "by_channel":
             # new behavior gain.offset indepenant by chans
             loc_q1, pre_median, loc_q2 = np.quantile(random_data, q=[q1, 0.5, q2], axis=0)
             pre_scale = abs(loc_q2 - loc_q1)
             gain = scale / pre_scale
             offset = median - pre_median * gain
 
             gain = gain[None, :]
@@ -83,16 +94,23 @@
 
         BasePreprocessor.__init__(self, recording, dtype=dtype)
 
         for parent_segment in recording._recording_segments:
             rec_segment = ScaleRecordingSegment(parent_segment, gain, offset, dtype=self._dtype)
             self.add_recording_segment(rec_segment)
 
-        self._kwargs = dict(recording=recording.to_dict(), scale=scale, median=median,
-                            q1=q1, q2=q2, mode=mode, dtype=np.dtype(self._dtype).str)
+        self._kwargs = dict(
+            recording=recording,
+            scale=scale,
+            median=median,
+            q1=q1,
+            q2=q2,
+            mode=mode,
+            dtype=np.dtype(self._dtype).str,
+        )
         self._kwargs.update(random_chunk_kwargs)
 
 
 class ScaleRecording(BasePreprocessor):
     """
     Scale traces from the given recording extractor with a scalar
     and offset. New traces = traces*scalar + offset.
@@ -109,18 +127,18 @@
         The dtype of the output traces. Default "float32"
 
     Returns
     -------
     transform_traces: ScaleRecording
         The transformed traces recording extractor object
     """
-    name = 'scale'
 
-    def __init__(self, recording, gain=1.0, offset=0., dtype="float32"):
+    name = "scale"
 
+    def __init__(self, recording, gain=1.0, offset=0.0, dtype="float32"):
         if dtype is None:
             dtype = recording.get_dtype()
 
         num_chans = recording.get_num_channels()
 
         if np.isscalar(gain):
             gain = np.ones((1, num_chans)) * gain
@@ -141,114 +159,167 @@
 
         BasePreprocessor.__init__(self, recording, dtype=dtype)
 
         for parent_segment in recording._recording_segments:
             rec_segment = ScaleRecordingSegment(parent_segment, gain, offset, self._dtype)
             self.add_recording_segment(rec_segment)
 
-        self._kwargs = dict(recording=recording.to_dict(), gain=gain, 
-                            offset=offset, dtype=np.dtype(self._dtype).str)
+        self._kwargs = dict(
+            recording=recording,
+            gain=gain,
+            offset=offset,
+            dtype=np.dtype(self._dtype).str,
+        )
 
 
 class CenterRecording(BasePreprocessor):
     """
     Centers traces from the given recording extractor by removing the median/mean of each channel.
 
     Parameters
     ----------
     recording: RecordingExtractor
         The recording extractor to be centered
     mode: str
         'median' (default) | 'mean'
     dtype: str or np.dtype
         The dtype of the output traces. Default "float32"
-    **random_chunk_kwargs: keyword arguments for `get_random_data_chunks()` function
-    
+    **random_chunk_kwargs: Keyword arguments for `spikeinterface.core.get_random_data_chunk()` function
+
     Returns
     -------
     centered_traces: ScaleRecording
         The centered traces recording extractor object
     """
-    name = 'center'
 
-    def __init__(self, recording, mode='median',
-                 dtype="float32", **random_chunk_kwargs):
+    name = "center"
 
-        assert mode in ('median', 'mean')
+    def __init__(self, recording, mode="median", dtype="float32", **random_chunk_kwargs):
+        assert mode in ("median", "mean")
         random_data = get_random_data_chunks(recording, **random_chunk_kwargs)
 
-        if mode == 'mean':
+        if mode == "mean":
             offset = -np.mean(random_data, axis=0)
-        elif mode == 'median':
+        elif mode == "median":
             offset = -np.median(random_data, axis=0)
         offset = offset[None, :]
         gain = np.ones(offset.shape)
 
         BasePreprocessor.__init__(self, recording, dtype=dtype)
 
         for parent_segment in recording._recording_segments:
             rec_segment = ScaleRecordingSegment(parent_segment, gain, offset, dtype=self._dtype)
             self.add_recording_segment(rec_segment)
 
-        self._kwargs = dict(recording=recording.to_dict(), mode=mode, dtype=np.dtype(self._dtype).str)
+        self._kwargs = dict(
+            recording=recording,
+            mode=mode,
+            dtype=np.dtype(self._dtype).str,
+        )
         self._kwargs.update(random_chunk_kwargs)
 
 
 class ZScoreRecording(BasePreprocessor):
     """
     Centers traces from the given recording extractor by removing the median of each channel
     and dividing by the MAD.
 
     Parameters
     ----------
     recording: RecordingExtractor
         The recording extractor to be centered
     mode: str
         "median+mad" (default) or "mean+std"
-    dtype: str or np.dtype
-        The dtype of the output traces. Default "float32"
-    **random_chunk_kwargs: keyword arguments for `get_random_data_chunks()` function
-    
+    dtype: None or dtype
+        If None the the parent dtype is kept.
+        For integer dtype a int_scale must be also given.
+    gain : None or np.array
+        Pre-computed gain.
+    offset : None or np.array
+        Pre-computed offset
+    int_scale : None or float
+        Apply a scaling factor to fit the integer range.
+        This is used when the dtype is an integer, so that the output is scaled.
+        For example, a value of `int_scale=200` will scale the zscore value to a standard deviation of 200.
+    **random_chunk_kwargs: Keyword arguments for `spikeinterface.core.get_random_data_chunk()` function
+
     Returns
     -------
     centered_traces: ScaleRecording
         The centered traces recording extractor object
     """
-    name = 'center'
 
-    def __init__(self, recording, mode="median+mad",
-                 dtype="float32", **random_chunk_kwargs):
-        
+    name = "zscore"
+
+    def __init__(
+        self,
+        recording,
+        mode="median+mad",
+        gain=None,
+        offset=None,
+        int_scale=None,
+        dtype="float32",
+        **random_chunk_kwargs,
+    ):
         assert mode in ("median+mad", "mean+std")
 
+        # fix dtype
+        dtype_ = fix_dtype(recording, dtype)
+
+        if dtype_.kind == "i":
+            assert int_scale is not None, "For recording with dtype=int you must set dtype=float32 OR set a scale"
+
         random_data = get_random_data_chunks(recording, **random_chunk_kwargs)
 
-        if mode == "median+mad":
+        if gain is not None:
+            assert offset is not None
+            gain = np.asarray(gain)
+            offset = np.asarray(offset)
+            n = recording.get_num_channels()
+            if gain.ndim == 1:
+                gain = gain[None, :]
+            assert gain.shape[1] == n
+            if offset.ndim == 1:
+                offset = offset[None, :]
+            assert offset.shape[1] == n
+        elif mode == "median+mad":
             medians = np.median(random_data, axis=0)
             medians = medians[None, :]
-            mads = np.median(np.abs(random_data - medians), axis=0) / 0.6745
-            mads = mads[None, :] 
+            mads = np.median(np.abs(random_data - medians), axis=0) / 0.6744897501960817
+            mads = mads[None, :]
             gain = 1 / mads
             offset = -medians / mads
         else:
             means = np.mean(random_data, axis=0)
             means = means[None, :]
             stds = np.std(random_data, axis=0)
-            stds = stds[None, :] 
+            stds = stds[None, :]
             gain = 1 / stds
             offset = -means / stds
-        
+
+        if int_scale is not None:
+            gain *= int_scale
+            offset *= int_scale
+
+        # convenient to have them here
+        self.gain = gain
+        self.offset = offset
+
         BasePreprocessor.__init__(self, recording, dtype=dtype)
 
         for parent_segment in recording._recording_segments:
             rec_segment = ScaleRecordingSegment(parent_segment, gain, offset, dtype=self._dtype)
             self.add_recording_segment(rec_segment)
 
-        self._kwargs = dict(recording=recording.to_dict(), dtype=np.dtype(self._dtype).str)
+        self._kwargs = dict(
+            recording=recording, dtype=np.dtype(self._dtype).str, mode=mode, gain=gain.tolist(), offset=offset.tolist()
+        )
         self._kwargs.update(random_chunk_kwargs)
 
 
 # functions for API
-normalize_by_quantile = define_function_from_class(source_class=NormalizeByQuantileRecording, name="normalize_by_quantile")
+normalize_by_quantile = define_function_from_class(
+    source_class=NormalizeByQuantileRecording, name="normalize_by_quantile"
+)
 scale = define_function_from_class(source_class=ScaleRecording, name="scale")
 center = define_function_from_class(source_class=CenterRecording, name="center")
 zscore = define_function_from_class(source_class=ZScoreRecording, name="zscore")
```

### Comparing `spikeinterface-0.97.1/spikeinterface/preprocessing/phase_shift.py` & `spikeinterface-0.98.0/src/spikeinterface/preprocessing/phase_shift.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,30 +1,28 @@
 import numpy as np
-from scipy.fft import rfft, irfft
 
 from spikeinterface.core.core_tools import define_function_from_class
 
 from ..core import get_chunk_with_margin
 
 from .basepreprocessor import BasePreprocessor, BasePreprocessorSegment
 
 
-
 class PhaseShiftRecording(BasePreprocessor):
     """
     This apply a phase shift to a recording to cancel the small sampling
     delay across for some recording system.
-    
+
     This is particularly relevant for neuropixel recording.
-    
+
     This code is inspired from from  IBL lib.
     https://github.com/int-brain-lab/ibllib/blob/master/ibllib/dsp/fourier.py
     and also the one from spikeglx
     https://billkarsh.github.io/SpikeGLX/help/dmx_vs_gbl/dmx_vs_gbl/
-    
+
     Parameters
     ----------
     recording: Recording
         The recording. It need to have  "inter_sample_shift" in properties.
     margin_ms: float (default 40)
         margin in ms for computation
         40ms ensure a very small error when doing chunk processing
@@ -32,45 +30,45 @@
         If "inter_sample_shift" is not in recording.properties
         we can externaly provide one.
     Returns
     -------
     filter_recording: PhaseShiftRecording
         The phase shifted recording object
     """
-    name = 'phase_shift'
-    def __init__(self, recording, margin_ms=40.,  inter_sample_shift=None, dtype=None):
+
+    name = "phase_shift"
+
+    def __init__(self, recording, margin_ms=40.0, inter_sample_shift=None, dtype=None):
         if inter_sample_shift is None:
             assert "inter_sample_shift" in recording.get_property_keys(), "'inter_sample_shift' is not a property!"
             sample_shifts = recording.get_property("inter_sample_shift")
         else:
             assert len(inter_sample_shift) == recording.get_num_channels(), "sample "
             sample_shifts = np.asarray(inter_sample_shift)
-        
-        margin = int(margin_ms * recording.get_sampling_frequency() / 1000.)
-        
+
+        margin = int(margin_ms * recording.get_sampling_frequency() / 1000.0)
+
         if dtype is None:
             dtype = recording.get_dtype()
         # the "apply_shift" function returns a float64 buffer. In case the dtype is different
         # than float64, we need a temporary casting and to force the buffer back to the original dtype
         if str(dtype) != "float64":
-            tmp_dtype = np.dtype('float64')
+            tmp_dtype = np.dtype("float64")
         else:
             tmp_dtype = None
 
-
         BasePreprocessor.__init__(self, recording, dtype=dtype)
         for parent_segment in recording._recording_segments:
             rec_segment = PhaseShiftRecordingSegment(parent_segment, sample_shifts, margin, dtype, tmp_dtype)
             self.add_recording_segment(rec_segment)
-        
+
         # for dumpability
         if inter_sample_shift is not None:
             inter_sample_shift = list(inter_sample_shift)
-        self._kwargs = dict(recording=recording.to_dict(), margin_ms=float(margin_ms),
-                             inter_sample_shift=inter_sample_shift)
+        self._kwargs = dict(recording=recording, margin_ms=float(margin_ms), inter_sample_shift=inter_sample_shift)
 
 
 class PhaseShiftRecordingSegment(BasePreprocessorSegment):
     def __init__(self, parent_recording_segment, sample_shifts, margin, dtype, tmp_dtype):
         BasePreprocessorSegment.__init__(self, parent_recording_segment)
         self.sample_shifts = sample_shifts
         self.margin = margin
@@ -82,34 +80,39 @@
             start_frame = 0
         if end_frame is None:
             end_frame = self.get_num_samples()
         if channel_indices is None:
             channel_indices = slice(None)
 
         # this return a copy with margin  + taper on border always
-        traces_chunk, left_margin, right_margin = get_chunk_with_margin(self.parent_recording_segment,
-                                                                        start_frame, end_frame, channel_indices,
-                                                                        self.margin, dtype=self.tmp_dtype,
-                                                                        add_zeros=True, window_on_margin=True)
-        
+        traces_chunk, left_margin, right_margin = get_chunk_with_margin(
+            self.parent_recording_segment,
+            start_frame,
+            end_frame,
+            channel_indices,
+            self.margin,
+            dtype=self.tmp_dtype,
+            add_zeros=True,
+            window_on_margin=True,
+        )
+
         traces_shift = apply_fshift_sam(traces_chunk, self.sample_shifts[channel_indices], axis=0)
-        # traces_shift = apply_fshift_ibl(traces_chunk, self.sample_shifts, axis=0)
+        # traces_shift = apply_fshift_ibl(traces_chunk, self.sample_shifts, axis=0)
 
         traces_shift = traces_shift[left_margin:-right_margin, :]
         if self.tmp_dtype is not None:
             traces_shift = traces_shift.astype(self.dtype)
-        
+
         return traces_shift
 
 
 # function for API
 phase_shift = define_function_from_class(source_class=PhaseShiftRecording, name="phase_shift")
 
 
-
 def apply_fshift_sam(sig, sample_shifts, axis=0):
     """
     Apply the shift on a traces buffer.
     """
     n = sig.shape[axis]
     sig_f = np.fft.rfft(sig, axis=axis)
     if n % 2 == 0:
@@ -119,39 +122,42 @@
         # n is odd sig_f[-1] is exactly nyquist!! we need (n-1) / n factor!!
         omega = np.linspace(0, np.pi * (n - 1) / n, sig_f.shape[axis])
     # broadcast omega and sample_shifts depend the axis
     if axis == 0:
         shifts = omega[:, np.newaxis] * sample_shifts[np.newaxis, :]
     else:
         shifts = omega[np.newaxis, :] * sample_shifts[:, np.newaxis]
-    sig_shift = np.fft.irfft(sig_f * np.exp(- 1j  * shifts), n=n, axis=axis)
+    sig_shift = np.fft.irfft(sig_f * np.exp(-1j * shifts), n=n, axis=axis)
     return sig_shift
 
+
 apply_fshift = apply_fshift_sam
-    
+
 
 def apply_fshift_ibl(w, s, axis=0, ns=None):
     """
     Function from IBLIB: https://github.com/int-brain-lab/ibllib/blob/master/ibllib/dsp/fourier.py
 
     Shifts a 1D or 2D signal in frequency domain, to allow for accurate non-integer shifts
     :param w: input signal (if complex, need to provide ns too)
     :param s: shift in samples, positive shifts forward
     :param axis: axis along which to shift (last axis by default)
     :param axis: axis along which to shift (last axis by default)
     :param ns: if a rfft frequency domain array is provided, give a number of samples as there
      is an ambiguity
     :return: w
     """
+    from scipy.fft import rfft, irfft
+
     # create a vector that contains a 1 sample shift on the axis
     ns = ns or w.shape[axis]
     shape = np.array(w.shape) * 0 + 1
     shape[axis] = ns
     dephas = np.zeros(shape)
-    # np.put(dephas, 1, 1)
+    # np.put(dephas, 1, 1)
     dephas[1] = 1
     dephas = rfft(dephas, axis=axis)
     # fft the data along the axis and the dephas
     do_fft = np.invert(np.iscomplexobj(w))
     if do_fft:
         W = rfft(w, axis=axis)
     else:
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `spikeinterface-0.97.1/spikeinterface/preprocessing/preprocessing_tools.py` & `spikeinterface-0.98.0/src/spikeinterface/preprocessing/preprocessing_tools.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,17 +1,24 @@
 import numpy as np
-import scipy.spatial
 
 
-def get_spatial_interpolation_kernel(source_location, target_location, method='kriging',
-                                     sigma_um=20., p=1, num_closest=3, dtype='float32',
-                                     force_extrapolate=False):
+def get_spatial_interpolation_kernel(
+    source_location,
+    target_location,
+    method="kriging",
+    sigma_um=20.0,
+    p=1,
+    num_closest=3,
+    sparse_thresh=None,
+    dtype="float32",
+    force_extrapolate=False,
+):
     """
     Compute the spatial kernel for linear spatial interpolation.
-    
+
     This is used for interpolation of bad channels or to correct the drift
     by interpolating between contacts.
 
     For reference, here is a simple overview on spatial interpolation:
     https://www.aspexit.com/spatial-data-interpolation-tin-idw-kriging-block-kriging-co-kriging-what-are-the-differences/
 
     Parameters
@@ -21,132 +28,143 @@
     target_location: array shape (n, 2)
         Scale for the output distribution
     method: 'kriging' or 'idw' or 'nearest'
         Choice of the method
             'kriging' : the same one used in kilosort
             'idw' : inverse  distance weithed
             'nearest' : use nereast channel
-    sigma_um : float (default 20.)
-        Used in the 'kriging' formula
+    sigma_um : float or list (default 20.)
+        Used in the 'kriging' formula. When list, it needs to have 2 elements (for the x and y directions).
     p: int (default 1)
         Used in the 'kriging' formula
+    sparse_thresh: None or float (default None)
+        If not None for 'kriging' force small value to be zeros to get a sparse matrix.
     num_closest: int (default 3)
         Used for 'idw'
     force_extrapolate: bool (false by default)
         How to handle when target location are outside source location.
         When False :  no extrapolation all target location outside are set to zero.
         When True : extrapolation done with the formula of the method.
                     In that case the sum of the kernel is not force to be 1.
 
     Returns
     -------
     interpolation_kernel: array (m, n)
     """
+    import scipy.spatial
 
     target_is_inside = np.ones(target_location.shape[0], dtype=bool)
     for dim in range(source_location.shape[1]):
         l0, l1 = np.min(source_location[:, dim]), np.max(source_location[:, dim])
         target_is_inside &= (target_location[:, dim] >= l0) & (target_location[:, dim] <= l1)
-    
-    
-    if method == 'kriging':
+
+    if method == "kriging":
         # this is an adaptation of the pykilosort implementation by Kush Benga
         # https://github.com/int-brain-lab/pykilosort/blob/ibl_prod/pykilosort/datashift2.py#L352
 
         Kxx = get_kriging_kernel_distance(source_location, source_location, sigma_um, p)
         Kyx = get_kriging_kernel_distance(target_location, source_location, sigma_um, p)
 
-        interpolation_kernel = Kyx @ np.linalg.pinv(Kxx + 0.01 * np.eye(Kxx.shape[0]))
+        interpolation_kernel = Kyx @ np.linalg.pinv(Kxx + 1e-6 * np.eye(Kxx.shape[0]))
         interpolation_kernel = interpolation_kernel.T.copy()
 
         # sparsify
-        interpolation_kernel[interpolation_kernel < 0.001] = 0.
+
+        if sparse_thresh is not None:
+            interpolation_kernel[interpolation_kernel < sparse_thresh] = 0.0
 
         # ensure sum = 1 for target inside
         s = np.sum(interpolation_kernel, axis=0)
         interpolation_kernel[:, target_is_inside] /= s[target_is_inside].reshape(1, -1)
 
-    elif method == 'idw':
-        distances = scipy.spatial.distance.cdist(source_location, target_location, metric='euclidean')
-        interpolation_kernel = np.zeros((source_location.shape[0], target_location.shape[0]), dtype='float64')
+    elif method == "idw":
+        distances = scipy.spatial.distance.cdist(source_location, target_location, metric="euclidean")
+        interpolation_kernel = np.zeros((source_location.shape[0], target_location.shape[0]), dtype="float64")
         for c in range(target_location.shape[0]):
             ind_sorted = np.argsort(distances[:, c])
             chan_closest = ind_sorted[:num_closest]
             dists = distances[chan_closest, c]
-            if dists[0] == 0.:
+            if dists[0] == 0.0:
                 # no interpolation the first have zeros distance
-                interpolation_kernel[chan_closest[0], c] = 1.
+                interpolation_kernel[chan_closest[0], c] = 1.0
             else:
                 interpolation_kernel[chan_closest, c] = 1 / dists
 
         # ensure sum = 1 for target inside
         s = np.sum(interpolation_kernel, axis=0)
         interpolation_kernel[:, target_is_inside] /= s[target_is_inside].reshape(1, -1)
 
-
-    elif method == 'nearest':
-        distances = scipy.spatial.distance.cdist(source_location, target_location, metric='euclidean')
-        interpolation_kernel = np.zeros((source_location.shape[0], target_location.shape[0]), dtype='float64')
+    elif method == "nearest":
+        distances = scipy.spatial.distance.cdist(source_location, target_location, metric="euclidean")
+        interpolation_kernel = np.zeros((source_location.shape[0], target_location.shape[0]), dtype="float64")
         for c in range(target_location.shape[0]):
             ind_closest = np.argmin(distances[:, c])
-            interpolation_kernel[ind_closest, c] = 1.
+            interpolation_kernel[ind_closest, c] = 1.0
 
     else:
-        raise ValueError('get_interpolation_kernel wrong method')
-    
+        raise ValueError("get_interpolation_kernel wrong method")
+
     if not force_extrapolate:
         interpolation_kernel[:, ~target_is_inside] = 0
 
     return interpolation_kernel.astype(dtype)
 
 
-def get_kriging_kernel_distance(locations_1, locations_2, sigma_um, p):
+def get_kriging_kernel_distance(locations_1, locations_2, sigma_um, p, distance_metric="euclidean"):
     """
     Get the kriging kernel between two sets of locations.
 
     Parameters
     ----------
 
     locations_1 / locations_2 : 2D np.array
         Locations of shape (N, D) where N is number of
         channels and d is spatial dimension (e.g. 2 for [x, y])
-    sigma_um : float
+    sigma_um : float or list
         Scale paremter on  the Gaussian kernel,
         typically distance between contacts in micrometers.
+        In case sigma_um is list then this mimics the Kilosort2.5 behavior, which uses two separate sigmas for each dimension.
+        In the later case the metric is always a 'cityblock'
     p : float
         Weight parameter on the exponential function. Default
         in IBL kriging interpolation is 1.3.
 
     Results
     ----------
     kernal_dist : n x m array (i.e. locations 1 x locations 2) of
                   distances (gaussian kernel) between locations 1 and 2.
 
     """
-    dist = scipy.spatial.distance.cdist(locations_1, locations_2, metric='euclidean')
-    kernal_dist = np.exp(-(dist / sigma_um) ** p)
+
+    if np.isscalar(sigma_um):
+        import scipy
+
+        dist = scipy.spatial.distance.cdist(locations_1, locations_2, metric=distance_metric)
+        kernal_dist = np.exp(-((dist / sigma_um) ** p))
+    else:
+        # this mimic the kilosort case where a sigma on x and y are diffrents.
+        # note that in that case the distance metric become a cityblock
+        sigma_x, sigma_y = sigma_um
+        distx = np.abs(locations_1[:, 0][:, np.newaxis] - locations_2[:, 0][np.newaxis, :])
+        disty = np.abs(locations_1[:, 1][:, np.newaxis] - locations_2[:, 1][np.newaxis, :])
+        kernal_dist = np.exp(-((distx / sigma_x) ** p) - (disty / sigma_y) ** p)
     return kernal_dist
 
 
-def get_kriging_channel_weights(contact_positions1, contact_positions2, sigma_um, p,
-                                weight_threshold=0.005):
+def get_kriging_channel_weights(contact_positions1, contact_positions2, sigma_um, p, weight_threshold=0.005):
     """
     Calculate weights for kriging interpolation. Weights below weight_threshold are set to 0.
 
     Based on the interpolate_bad_channels() function of the International Brain Laboratory.
 
     International Brain Laboratory et al. (2022). Spike sorting pipeline for the
     International Brain Laboratory. https://www.internationalbrainlab.com/repro-ephys
     """
-    weights = get_kriging_kernel_distance(contact_positions1,
-                                          contact_positions2,
-                                          sigma_um,
-                                          p)
+    weights = get_kriging_kernel_distance(contact_positions1, contact_positions2, sigma_um, p)
     weights[weights < weight_threshold] = 0
 
-    with np.errstate(divide='ignore', invalid='ignore'):
+    with np.errstate(divide="ignore", invalid="ignore"):
         weights /= np.sum(weights, axis=0)[None, :]
 
-    weights[np.logical_or(weights < weight_threshold,
-                          np.isnan(weights))] = 0
+    weights[np.logical_or(weights < weight_threshold, np.isnan(weights))] = 0
 
     return weights
```

### Comparing `spikeinterface-0.97.1/spikeinterface/preprocessing/preprocessinglist.py` & `spikeinterface-0.98.0/src/spikeinterface/preprocessing/preprocessinglist.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,62 +1,78 @@
 ### PREPROCESSORS ###
-from .resample import ResampleRecording
-from .filter import (FilterRecording, filter,
-                     BandpassFilterRecording, bandpass_filter,
-                     NotchFilterRecording, notch_filter,
-                     HighpassFilterRecording, highpass_filter,
-                     )
+from .resample import ResampleRecording, resample
+from .filter import (
+    FilterRecording,
+    filter,
+    BandpassFilterRecording,
+    bandpass_filter,
+    NotchFilterRecording,
+    notch_filter,
+    HighpassFilterRecording,
+    highpass_filter,
+)
+from .filter_gaussian import GaussianBandpassFilterRecording, gaussian_bandpass_filter
 from .normalize_scale import (
-    NormalizeByQuantileRecording, normalize_by_quantile,
-    ScaleRecording, scale,
-    ZScoreRecording, zscore,
-    CenterRecording, center)
-from .whiten import WhitenRecording, whiten
+    NormalizeByQuantileRecording,
+    normalize_by_quantile,
+    ScaleRecording,
+    scale,
+    ZScoreRecording,
+    zscore,
+    CenterRecording,
+    center,
+)
+from .whiten import WhitenRecording, whiten, compute_whitening_matrix
 from .rectify import RectifyRecording, rectify
-from .clip import (
-    BlankSaturationRecording, blank_staturation,
-    ClipRecording, clip)
+from .clip import BlankSaturationRecording, blank_staturation, ClipRecording, clip
 from .common_reference import CommonReferenceRecording, common_reference
 from .remove_artifacts import RemoveArtifactsRecording, remove_artifacts
-from .resample import ResampleRecording, resample
+from .silence_periods import SilencedPeriodsRecording, silence_periods
 from .phase_shift import PhaseShiftRecording, phase_shift
 from .zero_channel_pad import ZeroChannelPaddedRecording, zero_channel_pad
 from .deepinterpolation import DeepInterpolatedRecording, deepinterpolate
 from .highpass_spatial_filter import HighpassSpatialFilterRecording, highpass_spatial_filter
 from .interpolate_bad_channels import InterpolateBadChannelsRecording, interpolate_bad_channels
+from .average_across_direction import AverageAcrossDirectionRecording, average_across_direction
+from .directional_derivative import DirectionalDerivativeRecording, directional_derivative
+from .depth_order import DepthOrderRecording, depth_order
+from .astype import AstypeRecording, astype
+from .unsigned_to_signed import UnsignedToSignedRecording, unsigned_to_signed
 
 
 preprocessers_full_list = [
     # filter stuff
     FilterRecording,
     BandpassFilterRecording,
     HighpassFilterRecording,
     NotchFilterRecording,
-
+    GaussianBandpassFilterRecording,
     # gain offset stuff
     NormalizeByQuantileRecording,
     ScaleRecording,
     CenterRecording,
     ZScoreRecording,
-
     # decorrelation stuff
     WhitenRecording,
-
     # re-reference
     CommonReferenceRecording,
-    
     PhaseShiftRecording,
-
     # misc
     RectifyRecording,
     ClipRecording,
     BlankSaturationRecording,
+    SilencedPeriodsRecording,
     RemoveArtifactsRecording,
     ZeroChannelPaddedRecording,
     DeepInterpolatedRecording,
     ResampleRecording,
     HighpassSpatialFilterRecording,
-    InterpolateBadChannelsRecording
+    InterpolateBadChannelsRecording,
+    DepthOrderRecording,
+    AverageAcrossDirectionRecording,
+    DirectionalDerivativeRecording,
+    AstypeRecording,
+    UnsignedToSignedRecording,
 ]
 
 installed_preprocessers_list = [pp for pp in preprocessers_full_list if pp.installed]
 preprocesser_dict = {pp_class.name: pp_class for pp_class in preprocessers_full_list}
```

### Comparing `spikeinterface-0.97.1/spikeinterface/preprocessing/rectify.py` & `spikeinterface-0.98.0/src/spikeinterface/preprocessing/rectify.py`

 * *Files 2% similar despite different names*

```diff
@@ -2,22 +2,22 @@
 
 from spikeinterface.core.core_tools import define_function_from_class
 
 from .basepreprocessor import BasePreprocessor, BasePreprocessorSegment
 
 
 class RectifyRecording(BasePreprocessor):
-    name = 'rectify'
+    name = "rectify"
 
     def __init__(self, recording):
         BasePreprocessor.__init__(self, recording)
         for parent_segment in recording._recording_segments:
             rec_segment = RectifyRecordingSegment(parent_segment)
             self.add_recording_segment(rec_segment)
-        self._kwargs = dict(recording=recording.to_dict())
+        self._kwargs = dict(recording=recording)
 
 
 class RectifyRecordingSegment(BasePreprocessorSegment):
     def __init__(self, parent_recording_segment):
         BasePreprocessorSegment.__init__(self, parent_recording_segment)
 
     def get_traces(self, start_frame, end_frame, channel_indices):
```

### Comparing `spikeinterface-0.97.1/spikeinterface/preprocessing/remove_artifacts.py` & `spikeinterface-0.98.0/src/spikeinterface/preprocessing/remove_artifacts.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,165 +1,178 @@
 import numpy as np
-import scipy.interpolate
 
 from spikeinterface.core.core_tools import define_function_from_class
 
 from .basepreprocessor import BasePreprocessor, BasePreprocessorSegment
 from spikeinterface.core import NumpySorting, extract_waveforms
 
+
 class RemoveArtifactsRecording(BasePreprocessor):
     """
-    Removes stimulation artifacts from recording extractor traces. By default, 
-    artifact periods are zeroed-out (mode = 'zeros'). This is only recommended 
+    Removes stimulation artifacts from recording extractor traces. By default,
+    artifact periods are zeroed-out (mode = 'zeros'). This is only recommended
     for traces that are centered around zero (e.g. through a prior highpass
     filter); if this is not the case, linear and cubic interpolation modes are
     also available, controlled by the 'mode' input argument.
-    Note that several artifacts can be removed at once (potentially with 
+    Note that several artifacts can be removed at once (potentially with
     distinct duration each), if labels are specified
 
     Parameters
     ----------
     recording: RecordingExtractor
         The recording extractor to remove artifacts from
     list_triggers: list of lists/arrays
         One list per segment of int with the stimulation trigger frames
     ms_before: float or None
-        Time interval in ms to remove before the trigger events. 
+        Time interval in ms to remove before the trigger events.
         If None, then also ms_after must be None and a single sample is removed
     ms_after: float or None
         Time interval in ms to remove after the trigger events.
         If None, then also ms_before must be None and a single sample is removed
     list_labels: list of lists/arrays or None
         One list per segment of labels with the stimulation labels for the given
         artefacs. labels should be strings, for JSON serialization.
         Required for 'median' and 'average' modes.
     mode: str
         Determines what artifacts are replaced by. Can be one of the following:
-            
+
         - 'zeros' (default): Artifacts are replaced by zeros.
 
-        - 'median': The median over all artifacts is computed and subtracted for 
+        - 'median': The median over all artifacts is computed and subtracted for
             each occurence of an artifact
 
-        - 'average': The mean over all artifacts is computed and subtracted for each 
+        - 'average': The mean over all artifacts is computed and subtracted for each
             occurence of an artifact
-        
+
         - 'linear': Replacement are obtained through Linear interpolation between
            the trace before and after the artifact.
            If the trace starts or ends with an artifact period, the gap is filled
            with the closest available value before or after the artifact.
-        
+
         - 'cubic': Cubic spline interpolation between the trace before and after
            the artifact, referenced to evenly spaced fit points before and after
            the artifact. This is an option thatcan be helpful if there are
            significant LFP effects around the time of the artifact, but visual
            inspection of fit behaviour with your chosen settings is recommended.
            The spacing of fit points is controlled by 'fit_sample_spacing', with
            greater spacing between points leading to a fit that is less sensitive
            to high frequency fluctuations but at the cost of a less smooth
            continuation of the trace.
            If the trace starts or ends with an artifact, the gap is filled with
            the closest available value before or after the artifact.
     fit_sample_spacing: float
         Determines the spacing (in ms) of reference points for the cubic spline
-        fit if mode = 'cubic'. Default = 1ms. Note: The actual fit samples are 
+        fit if mode = 'cubic'. Default = 1ms. Note: The actual fit samples are
         the median of the 5 data points around the time of each sample point to
         avoid excessive influence from hyper-local fluctuations.
     artifacts: dict
         If provided (when mode is 'median' or 'average') then it must be a dict with
         keys that are the labels of the artifacts, and values the artifacts themselves,
         on all channels (and thus bypassing ms_before and ms_after)
     sparsity: dict
         If provided (when mode is 'median' or 'average') then it must be a dict with
         keys that are the labels of the artifacts, and values that are boolean mask of
         the channels where the artifacts should be considered (for subtraction/scaling)
     scale_amplitude: False
         If true, then for mode 'median' or 'average' the amplitude of the template
         will be scaled in amplitude at each time occurence to minimize residuals
     time_jitter: float (default 0)
-        If non 0, then for mode 'median' or 'average', a time jitter in ms 
+        If non 0, then for mode 'median' or 'average', a time jitter in ms
         can be allowed to minimize the residuals
     waveforms_kwargs: dict or None
         The arguments passed to the WaveformExtractor object when extracting the
         artifacts, for mode 'median' or 'average'.
         By default, the global job kwargs are used, in addition to {'allow_unfiltered' : True, 'mode':'memory'}.
         To estimate sparse artifact
 
     Returns
     -------
     removed_recording: RemoveArtifactsRecording
-        The recording extractor after artifact removal    
+        The recording extractor after artifact removal
     """
-    name = 'remove_artifacts'
 
-    def __init__(self, recording, list_triggers, ms_before=0.5, ms_after=3.0, mode='zeros', fit_sample_spacing=1.,
-                 list_labels=None, artifacts=None, sparsity=None, scale_amplitude=False, time_jitter=0,
-                 waveforms_kwargs={'allow_unfiltered' : True, 'mode':'memory'}):
+    name = "remove_artifacts"
+
+    def __init__(
+        self,
+        recording,
+        list_triggers,
+        ms_before=0.5,
+        ms_after=3.0,
+        mode="zeros",
+        fit_sample_spacing=1.0,
+        list_labels=None,
+        artifacts=None,
+        sparsity=None,
+        scale_amplitude=False,
+        time_jitter=0,
+        waveforms_kwargs={"allow_unfiltered": True, "mode": "memory"},
+    ):
+        import scipy.interpolate
 
-        available_modes = ('zeros', 'linear', 'cubic', 'average', 'median')
+        available_modes = ("zeros", "linear", "cubic", "average", "median")
         num_seg = recording.get_num_segments()
 
         if num_seg == 1:
             if isinstance(list_triggers, (list, np.ndarray)) and np.isscalar(list_triggers[0]):
                 # when unique segment accept list instead of of list of list/arrays
                 list_triggers = [list_triggers]
             if isinstance(list_labels, (list, np.ndarray)) and np.isscalar(list_labels[0]):
                 # when unique segment accept list instead of of list of list/arrays
                 list_labels = [list_labels]
 
         # if no labels are given, assume single label
         if list_labels is None:
-            list_labels = [[0]*len(i) for i in list_triggers]
+            list_labels = [[0] * len(i) for i in list_triggers]
 
         # some checks
         assert isinstance(list_triggers, list), "'list_triggers' must be a list (one per segment)"
         assert len(list_triggers) == num_seg, "'list_triggers' must have the same length as the number of segments"
-        assert all(isinstance(list_triggers[i], (list, np.ndarray)) for i in range(num_seg)), \
-            "Each element of 'list_triggers' must be array-like"
+        assert all(
+            isinstance(list_triggers[i], (list, np.ndarray)) for i in range(num_seg)
+        ), "Each element of 'list_triggers' must be array-like"
 
         if list_labels is not None:
             assert isinstance(list_labels, list), "'list_labels' must be a list (one per segment)"
             assert len(list_labels) == num_seg
             assert all(isinstance(list_labels[i], (list, np.ndarray)) for i in range(num_seg))
 
         assert mode in available_modes, f"mode {mode} is not an available mode: {available_modes}"
 
         if ms_before is None:
             assert ms_after is None, "To remove a single sample, set both ms_before and ms_after to None"
         else:
             ms_before = float(ms_before)
             ms_after = float(ms_after)
-        
+
         fs = recording.get_sampling_frequency()
         if ms_before is not None:
             pad = [int(ms_before * fs / 1000), int(ms_after * fs / 1000)]
         else:
             pad = None
 
-        fit_sample_interval = int(fit_sample_spacing * fs / 1000.)
+        fit_sample_interval = int(fit_sample_spacing * fs / 1000.0)
         fit_sample_range = fit_sample_interval * 2 + 1
         fit_samples = np.arange(0, fit_sample_range, fit_sample_interval)
 
-        if mode in ['median', 'average']:
+        if mode in ["median", "average"]:
             assert time_jitter >= 0, "time jitter should be a positive value"
-            time_pad = int(time_jitter * fs / 1000.)
+            time_pad = int(time_jitter * fs / 1000.0)
 
             if artifacts is not None:
                 labels = []
                 for sub_list in list_labels:
                     labels += list(np.unique(sub_list))
                 for l in np.unique(labels):
                     assert l in artifacts.keys(), f"Artefacts are provided but label {l} has no value!"
             else:
-                assert 'ms_before' != None and 'ms_after' != None, \
-                    f"ms_before/after should not be None for mode {mode}"
-                sorting = NumpySorting.from_times_labels(list_triggers, list_labels,
-                                                         recording.get_sampling_frequency())
+                assert "ms_before" != None and "ms_after" != None, f"ms_before/after should not be None for mode {mode}"
+                sorting = NumpySorting.from_times_labels(list_triggers, list_labels, recording.get_sampling_frequency())
                 sorting = sorting.save()
-                waveforms_kwargs.update({'ms_before' : ms_before, 'ms_after' : ms_after})
+                waveforms_kwargs.update({"ms_before": ms_before, "ms_after": ms_after})
                 w = extract_waveforms(recording, sorting, None, **waveforms_kwargs)
 
                 artifacts = {}
                 sparsity = {}
                 for label in w.sorting.unit_ids:
                     artifacts[label] = w.get_template(label, mode=mode).astype(recording.dtype)
                     if w.is_sparse():
@@ -178,51 +191,72 @@
             artifacts = None
             time_pad = None
 
         BasePreprocessor.__init__(self, recording)
         for seg_index, parent_segment in enumerate(recording._recording_segments):
             triggers = list_triggers[seg_index]
             labels = list_labels[seg_index]
-            rec_segment = RemoveArtifactsRecordingSegment(parent_segment, triggers, pad, mode, fit_samples,
-                                                          artifacts, labels, scale_amplitude, time_pad, sparsity)
+            rec_segment = RemoveArtifactsRecordingSegment(
+                parent_segment, triggers, pad, mode, fit_samples, artifacts, labels, scale_amplitude, time_pad, sparsity
+            )
             self.add_recording_segment(rec_segment)
 
         list_triggers_ = [[int(trig) for trig in trig_seg] for trig_seg in list_triggers]
         if list_labels is not None:
-            list_labels_= [list(lab_seg) for lab_seg in list_labels]
+            list_labels_ = [list(lab_seg) for lab_seg in list_labels]
         else:
             list_labels_ = None
-        self._kwargs = dict(recording=recording.to_dict(), list_triggers=list_triggers_,
-                            ms_before=ms_before, ms_after=ms_after, mode=mode,
-                            fit_sample_spacing=fit_sample_spacing, artifacts=artifacts,
-                            list_labels=list_labels_, scale_amplitude=scale_amplitude,
-                            time_jitter=time_jitter, sparsity=sparsity)
+        self._kwargs = dict(
+            recording=recording,
+            list_triggers=list_triggers_,
+            ms_before=ms_before,
+            ms_after=ms_after,
+            mode=mode,
+            fit_sample_spacing=fit_sample_spacing,
+            artifacts=artifacts,
+            list_labels=list_labels_,
+            scale_amplitude=scale_amplitude,
+            time_jitter=time_jitter,
+            sparsity=sparsity,
+        )
 
 
 class RemoveArtifactsRecordingSegment(BasePreprocessorSegment):
-    def __init__(self, parent_recording_segment, triggers, pad, mode, fit_samples, artifacts,
-                 labels, scale_amplitude, time_pad, sparsity):
+    def __init__(
+        self,
+        parent_recording_segment,
+        triggers,
+        pad,
+        mode,
+        fit_samples,
+        artifacts,
+        labels,
+        scale_amplitude,
+        time_pad,
+        sparsity,
+    ):
+        import scipy.interpolate
+
         BasePreprocessorSegment.__init__(self, parent_recording_segment)
 
-        self.triggers = np.asarray(triggers, dtype='int64')
+        self.triggers = np.asarray(triggers, dtype="int64")
         self.pad = pad
         self.mode = mode
         self.artifacts = artifacts
         if self.artifacts is not None:
             for key, value in self.artifacts.items():
                 self.artifacts[key] = np.array(value)
         self.labels = np.asarray(labels)
         self.fit_samples = fit_samples
         self.scale_amplitude = scale_amplitude
         self.time_pad = time_pad
         self.sparsity = sparsity
 
     def get_traces(self, start_frame, end_frame, channel_indices):
-
-        if self.mode in ['average', 'median']:
+        if self.mode in ["average", "median"]:
             traces = self.parent_recording_segment.get_traces(start_frame, end_frame, slice(None))
         else:
             traces = self.parent_recording_segment.get_traces(start_frame, end_frame, channel_indices)
         traces = traces.copy()
 
         if start_frame is None:
             start_frame = 0
@@ -231,28 +265,28 @@
 
         mask = (self.triggers >= start_frame) & (self.triggers < end_frame)
         triggers = self.triggers[mask] - start_frame
         labels = self.labels[mask]
 
         pad = self.pad
 
-        if self.mode == 'zeros':
+        if self.mode == "zeros":
             for trig in triggers:
                 if pad is None:
                     traces[trig, :] = 0
                 else:
                     if trig - pad[0] > 0 and trig + pad[1] < end_frame - start_frame:
-                        traces[trig - pad[0]:trig + pad[1] + 1, :] = 0
+                        traces[trig - pad[0] : trig + pad[1] + 1, :] = 0
                     elif trig - pad[0] <= 0 and trig + pad[1] >= end_frame - start_frame:
                         traces[:] = 0
                     elif trig - pad[0] <= 0:
-                        traces[:trig + pad[1], :] = 0
+                        traces[: trig + pad[1], :] = 0
                     elif trig + pad[1] >= end_frame - start_frame:
-                        traces[trig - pad[0]:, :] = 0
-        elif self.mode in ['linear', 'cubic']:
+                        traces[trig - pad[0] :, :] = 0
+        elif self.mode in ["linear", "cubic"]:
             for trig in triggers:
                 if pad is None:
                     pre_data_end_idx = trig - 1
                     post_data_start_idx = trig + 1
                 else:
                     pre_data_end_idx = trig - pad[0] - 1
                     post_data_start_idx = trig + pad[1] + 1
@@ -272,57 +306,57 @@
                 # correct for out of bounds indices on both sides:
                 if np.max(post_idx) >= traces.shape[0]:
                     post_idx = post_idx[post_idx < traces.shape[0]]
 
                 if np.min(pre_idx) < 0:
                     pre_idx = pre_idx[pre_idx >= 0]
 
-                # fit x values                
+                # fit x values
                 all_idx = np.hstack((pre_idx, post_idx))
 
                 # fit y values
                 interp_traces = traces[all_idx, :]
 
                 # Get the median value from 5 samples around each fit point
                 # for robustness to noise / small fluctuations
                 pre_vals = []  #  np.zeros((0, traces.shape[1]), dtype=traces.dtype)1
                 for idx in iter(pre_idx):
                     if idx == pre_idx[-1]:
                         idxs = np.arange(idx - 3, idx + 1)
                     else:
                         idxs = np.arange(idx - 2, idx + 3)
-                    if np.min(idx) < 0:
-                        idx = idx[idx >= 0]
+                    if np.min(idxs) < 0:
+                        idxs = idxs[idxs >= 0]
                     median_vals = np.median(traces[idxs, :], axis=0, keepdims=True)
                     pre_vals.append(median_vals)
                 post_vals = []
                 for idx in iter(post_idx):
                     if idx == post_idx[0]:
                         idxs = np.arange(idx, idx + 4)
                     else:
                         idxs = np.arange(idx - 2, idx + 3)
-                    if np.max(idx) >= traces.shape[0]:
-                        idx = idx[idx < traces.shape[0]]
+                    if np.max(idxs) >= traces.shape[0]:
+                        idxs = idxs[idxs < traces.shape[0]]
                     median_vals = np.median(traces[idxs, :], axis=0, keepdims=True)
                     post_vals.append(median_vals)
 
                 if len(all_idx) > 0:
                     interp_traces = np.concatenate(pre_vals + post_vals, axis=0)
 
-                if self.mode == 'cubic' and len(all_idx) >= 5:
+                if self.mode == "cubic" and len(all_idx) >= 5:
                     # Enough fit points present on either side to do cubic spline fit:
-                    interp_function = scipy.interpolate.interp1d(all_idx, interp_traces,
-                                                                 kind='cubic', axis=0, bounds_error=False,
-                                                                 fill_value='extrapolate')
+                    interp_function = scipy.interpolate.interp1d(
+                        all_idx, interp_traces, kind="cubic", axis=0, bounds_error=False, fill_value="extrapolate"
+                    )
                     traces[gap_idx, :] = interp_function(gap_idx)
-                elif self.mode == 'linear' and len(all_idx) >= 2:
+                elif self.mode == "linear" and len(all_idx) >= 2:
                     # Enough fit points present for a linear fit
-                    interp_function = scipy.interpolate.interp1d(all_idx, interp_traces,
-                                                                 kind='linear', axis=0, bounds_error=False,
-                                                                 fill_value='extrapolate')
+                    interp_function = scipy.interpolate.interp1d(
+                        all_idx, interp_traces, kind="linear", axis=0, bounds_error=False, fill_value="extrapolate"
+                    )
                     traces[gap_idx, :] = interp_function(gap_idx)
                 elif len(pre_idx) > len(post_idx):
                     # not enough fit points, fill with nearest neighbour on side with the most data points
                     traces[gap_idx, :] = np.repeat(traces[[pre_idx[-1]], :], len(gap_idx), axis=0)
                 elif len(post_idx) > len(pre_idx):
                     # not enough fit points, fill with nearest neighbour on side with the most data points
                     traces[gap_idx, :] = np.repeat(traces[[post_idx[0]], :], len(gap_idx), axis=0)
@@ -330,15 +364,15 @@
                     # not enough fit points, both sides tied for most data points, fill with last pre value
                     traces[gap_idx, :] = np.repeat(traces[[pre_idx[-1]], :], len(gap_idx), axis=0)
                 else:
                     # No data to interpolate from on either side of gap;
                     # Fill with zeros
                     traces[gap_idx, :] = 0
 
-        elif self.mode in ['average', 'median']:
+        elif self.mode in ["average", "median"]:
             for label, trig in zip(labels, triggers):
                 if self.sparsity is not None:
                     mask = self.sparsity[label]
                 else:
                     mask = None
                 artifact_duration = len(self.artifacts[label])
                 if self.time_pad > 0:
@@ -346,15 +380,14 @@
                 else:
                     jitters = np.array([0])
 
                 nb_jitters = len(jitters)
                 best_amplitudes = np.zeros(nb_jitters, dtype=np.float32)
 
                 for count, padding in enumerate(jitters):
-
                     t_trig = trig + padding
 
                     if t_trig - pad[0] >= 0 and t_trig + pad[1] < end_frame - start_frame:
                         trace_slice = slice(t_trig - pad[0], t_trig + pad[1])
                         artifact_slice = slice(0, artifact_duration)
                     elif t_trig - pad[0] < 0:
                         trace_slice = slice(0, t_trig + pad[1])
@@ -364,19 +397,21 @@
                         trace_slice = slice(t_trig - pad[0], end_frame - start_frame)
                         duration = (end_frame - start_frame) - (t_trig - pad[0])
                         artifact_slice = slice(0, duration)
 
                     trace_slice_values = traces[trace_slice]
                     if mask is not None:
                         trace_slice_values = trace_slice_values[:, mask]
-                    
+
                     artifact_slice_values = self.artifacts[label][artifact_slice]
 
-                    norm = np.linalg.norm(trace_slice_values)*np.linalg.norm(artifact_slice_values)
-                    best_amplitudes[count] = np.dot(trace_slice_values.flatten(), artifact_slice_values.flatten())/norm
+                    norm = np.linalg.norm(trace_slice_values) * np.linalg.norm(artifact_slice_values)
+                    best_amplitudes[count] = (
+                        np.dot(trace_slice_values.flatten(), artifact_slice_values.flatten()) / norm
+                    )
 
                 if nb_jitters > 0:
                     idx_best_jitter = np.argmax(best_amplitudes)
                     t_trig = trig + jitters[idx_best_jitter]
 
                     if t_trig - pad[0] >= 0 and t_trig + pad[1] < end_frame - start_frame:
                         trace_slice = slice(t_trig - pad[0], t_trig + pad[1])
@@ -387,26 +422,26 @@
                         artifact_slice = slice(artifact_duration - duration, artifact_duration)
                     elif t_trig + pad[1] >= end_frame - start_frame:
                         trace_slice = slice(t_trig - pad[0], end_frame - start_frame)
                         duration = (end_frame - start_frame) - (t_trig - pad[0])
                         artifact_slice = slice(0, duration)
                 else:
                     idx_best_jitter = 0
-                
+
                 if self.scale_amplitude:
                     best_amp = best_amplitudes[idx_best_jitter]
                 else:
                     best_amp = 1
 
                 if mask is not None:
-                    traces[trace_slice][:, mask] -= \
-                        (best_amp * self.artifacts[label][artifact_slice]).astype(traces.dtype)
+                    traces[trace_slice][:, mask] -= (best_amp * self.artifacts[label][artifact_slice]).astype(
+                        traces.dtype
+                    )
                 else:
-                    traces[trace_slice] -= \
-                        (best_amp * self.artifacts[label][artifact_slice]).astype(traces.dtype)
+                    traces[trace_slice] -= (best_amp * self.artifacts[label][artifact_slice]).astype(traces.dtype)
             traces = traces[:, channel_indices]
 
         return traces
 
 
 # function for API
 remove_artifacts = define_function_from_class(source_class=RemoveArtifactsRecording, name="remove_artifacts")
```

### Comparing `spikeinterface-0.97.1/spikeinterface/preprocessing/resample.py` & `spikeinterface-0.98.0/src/spikeinterface/preprocessing/resample.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,24 +1,26 @@
 import numpy as np
-from scipy import signal
 import warnings
 
-from spikeinterface.core.core_tools import define_function_from_class, recursive_key_finder
+from spikeinterface.core.core_tools import (
+    define_function_from_class,
+    recursive_key_finder,
+)
 
 from .basepreprocessor import BasePreprocessor
 from .filter import fix_dtype
 from ..core import get_chunk_with_margin, BaseRecordingSegment
 
 
 class ResampleRecording(BasePreprocessor):
     """
     Resample the recording extractor traces.
 
     If the original sampling rate is multiple of the resample_rate, it will use
-    the signal.decimate method. In other cases, it uses signal.resample. In the
+    the signal.decimate method from scipy. In other cases, it uses signal.resample. In the
     later case, the resulting signal can have issues on the edges, mainly on the
     rightmost.
 
     Parameters
     ----------
     recording : Recording
         The recording extractor to be re-referenced
@@ -26,152 +28,161 @@
         The resampling frequency
     margin : float (default 100)
         Margin in ms for computations, will be used to decrease edge effects.
     dtype : dtype or None
         The dtype of the returned traces. If None, the dtype of the parent recording is used.
     skip_checks : bool
         If True, checks on sampling frequencies and cutoff filter frequencies are skipped, by default False
-    
+
     Returns
     -------
     resample_recording: ResampleRecording
         The resampled recording extractor object.
 
     """
 
     name = "resample"
 
     def __init__(
         self,
         recording,
         resample_rate,
-        margin_ms=100.,
+        margin_ms=100.0,
         dtype=None,
-        skip_checks=False
+        skip_checks=False,
     ):
         # Floating point resampling rates can lead to unexpected results, avoid actively
         msg = "Non integer resampling rates can lead to unexpected results."
         assert isinstance(resample_rate, (int, np.integer)), msg
         # Original sampling frequency
         self._orig_samp_freq = recording.get_sampling_frequency()
         self._resample_rate = resample_rate
         self._sampling_frequency = resample_rate
         # fix_dtype not always returns the str, make sure it does
         dtype = fix_dtype(recording, dtype).str
         # Ensure that the requested resample rate is doable:
         if skip_checks:
-            assert check_nyquist(recording, resample_rate), \
-                "The requested resample rate would induce errors!"
+            assert check_nyquist(recording, resample_rate), "The requested resample rate would induce errors!"
 
         # Get a margin to avoid issues later
         margin = int(margin_ms * recording.get_sampling_frequency() / 1000)
 
-        BasePreprocessor.__init__(
-            self, recording, sampling_frequency=resample_rate, dtype=dtype
-        )
+        BasePreprocessor.__init__(self, recording, sampling_frequency=resample_rate, dtype=dtype)
         # in case there was a time_vector, it will be dropped for sanity.
         for parent_segment in recording._recording_segments:
             parent_segment.time_vector = None
             self.add_recording_segment(
                 ResampleRecordingSegment(
                     parent_segment,
                     resample_rate,
                     recording.get_sampling_frequency(),
                     margin,
                     dtype,
                 )
             )
 
         self._kwargs = dict(
-            recording=recording.to_dict(),
+            recording=recording,
             resample_rate=resample_rate,
             margin_ms=margin_ms,
             dtype=dtype,
-            skip_checks=skip_checks
+            skip_checks=skip_checks,
         )
 
 
 class ResampleRecordingSegment(BaseRecordingSegment):
-    def __init__(self, parent_recording_segment, resample_rate, parent_rate, margin, dtype):
+    def __init__(
+        self,
+        parent_recording_segment,
+        resample_rate,
+        parent_rate,
+        margin,
+        dtype,
+    ):
         # Do not use BasePreprocessorSegment bcause we have to reset the sampling rate!
-        BaseRecordingSegment.__init__(self, sampling_frequency=resample_rate,
-                                      t_start=parent_recording_segment.t_start)
+        BaseRecordingSegment.__init__(
+            self,
+            sampling_frequency=resample_rate,
+            t_start=parent_recording_segment.t_start,
+        )
         self._parent_segment = parent_recording_segment
         self._parent_rate = parent_rate
         self._margin = margin
         self._dtype = dtype
 
     def get_num_samples(self):
         return int(self._parent_segment.get_num_samples() / self._parent_rate * self.sampling_frequency)
 
     def get_traces(self, start_frame, end_frame, channel_indices):
-        if start_frame == None:
+        if start_frame is None:
             start_frame = 0
-        if end_frame == None:
+        if end_frame is None:
             end_frame = self.get_num_samples()
 
-        # get parent traces with margin        
+        # get parent traces with margin
         parent_start_frame, parent_end_frame = [
-            int((frame / self.sampling_frequency) * self._parent_rate)
-            for frame in [start_frame, end_frame]
+            int((frame / self.sampling_frequency) * self._parent_rate) for frame in [start_frame, end_frame]
         ]
         parent_traces, left_margin, right_margin = get_chunk_with_margin(
             self._parent_segment,
-            parent_start_frame, parent_end_frame, channel_indices, self._margin,
-            window_on_margin=False, add_zeros=False, dtype=np.float32,
+            parent_start_frame,
+            parent_end_frame,
+            channel_indices,
+            self._margin,
+            add_reflect_padding=True,
+            dtype=np.float32,
         )
         # get left and right margins for the resampled case
         left_margin_rs, right_margin_rs = [
-            int((margin / self._parent_rate) * self.sampling_frequency)
-            for margin in [left_margin, right_margin]
+            int((margin / self._parent_rate) * self.sampling_frequency) for margin in [left_margin, right_margin]
         ]
-        
+
         # get the size for the resampled traces in case of resample:
         num = int((end_frame + right_margin_rs) - (start_frame - left_margin_rs))
-        
-        # Decimate can misbehave on some cases, while resample allways looks nice enough.
+
+        # Decimate can misbehave on some cases, while resample always looks nice enough.
         # Check which method to use:
+        from scipy import signal
+
         if np.mod(self._parent_rate, self.sampling_frequency) == 0:
             # Ratio between sampling frequencies
             q = int(self._parent_rate / self.sampling_frequency)
             # Decimate can have issues for some cases, returning NaNs
             resampled_traces = signal.decimate(parent_traces, q=q, axis=0)
             # If that's the case, use signal.resample
             if np.any(np.isnan(resampled_traces)):
                 resampled_traces = signal.resample(parent_traces, num, axis=0)
         else:
             resampled_traces = signal.resample(parent_traces, num, axis=0)
 
-        # now take care of the edges:
-        if right_margin > 0:
-            resampled_traces = resampled_traces[left_margin_rs:-right_margin_rs, :]
-        else:
-            resampled_traces = resampled_traces[left_margin_rs:, :]
+        # now take care of the edges
+        resampled_traces = resampled_traces[left_margin_rs : num - right_margin_rs]
         return resampled_traces.astype(self._dtype)
 
 
 resample = define_function_from_class(source_class=ResampleRecording, name="resample")
 
 
 # Some helpers to do checks
 def check_nyquist(recording, resample_rate):
     # Check that the original and requested sampling rates will not induce aliasing
     # Basic test, compare the sampling frequency with the resample rate
     sampling_frequency_check = recording.get_sampling_frequency() / 2 > resample_rate
     # Check that the signal, if it has been filtered, is still not violating
     if recording.is_filtered():
         # Check if we have access to the highcut frequency
-        freq_max = list(recursive_key_finder(recording.to_dict(), "freq_max"))
+        freq_max = list(recursive_key_finder(recording, "freq_max"))
         if freq_max:
             # Given that there might be more than one filter applied, keep the lowest
             freq_max = min(freq_max)
             lowpass_cutoff_check = freq_max / 2 > resample_rate
         else:
             # If has been filterd but unknown high cutoff, give warning and asume the best
             warnings.warn("The recording is filtered, but we can't ensure that it complies with the Nyquist limit.")
             lowpass_cutoff_check = True
     else:
         # If it hasn't been filtered, we only depend on the previous test
-        warnings.warn("The recording is not filtered, so cutoff frequencies cannot be checked. "
-                      "Use resampling with caution")
+        warnings.warn(
+            "The recording is not filtered, so cutoff frequencies cannot be checked. " "Use resampling with caution"
+        )
         lowpass_cutoff_check = True
     return all([sampling_frequency_check, lowpass_cutoff_check])
```

### Comparing `spikeinterface-0.97.1/spikeinterface/qualitymetrics/misc_metrics.py` & `spikeinterface-0.98.0/src/spikeinterface/qualitymetrics/misc_metrics.py`

 * *Files 6% similar despite different names*

```diff
@@ -8,145 +8,180 @@
 """
 
 from collections import namedtuple
 
 import math
 import numpy as np
 import warnings
-from scipy.ndimage import gaussian_filter1d
-from scipy.stats import poisson
 
 from ..postprocessing import correlogram_for_one_segment
 from ..core import get_noise_levels
 from ..core.template_tools import (
     get_template_extremum_channel,
     get_template_extremum_amplitude,
 )
 
 try:
     import numba
+
     HAVE_NUMBA = True
 except ModuleNotFoundError as err:
     HAVE_NUMBA = False
 
 
 _default_params = dict()
 
 
-def compute_num_spikes(waveform_extractor, **kwargs):
+def compute_num_spikes(waveform_extractor, unit_ids=None, **kwargs):
     """Compute the number of spike across segments.
 
     Parameters
     ----------
     waveform_extractor : WaveformExtractor
         The waveform extractor object.
+    unit_ids : list or None
+        The list of unit ids to compute the number of spikes. If None, all units are used.
 
     Returns
     -------
     num_spikes : dict
         The number of spikes, across all segments, for each unit ID.
     """
 
     sorting = waveform_extractor.sorting
-    unit_ids = sorting.unit_ids
+    if unit_ids is None:
+        unit_ids = sorting.unit_ids
     num_segs = sorting.get_num_segments()
 
     num_spikes = {}
     for unit_id in unit_ids:
         n = 0
         for segment_index in range(num_segs):
             st = sorting.get_unit_spike_train(unit_id=unit_id, segment_index=segment_index)
             n += st.size
         num_spikes[unit_id] = n
 
     return num_spikes
 
 
-def compute_firing_rates(waveform_extractor):
+def compute_firing_rates(waveform_extractor, unit_ids=None, **kwargs):
     """Compute the firing rate across segments.
 
     Parameters
     ----------
     waveform_extractor : WaveformExtractor
         The waveform extractor object.
+    unit_ids : list or None
+        The list of unit ids to compute the firing rate. If None, all units are used.
 
     Returns
     -------
     firing_rates : dict of floats
         The firing rate, across all segments, for each unit ID.
     """
 
     sorting = waveform_extractor.sorting
-    unit_ids = sorting.unit_ids
+    if unit_ids is None:
+        unit_ids = sorting.unit_ids
     total_duration = waveform_extractor.get_total_duration()
 
     firing_rates = {}
     num_spikes = compute_num_spikes(waveform_extractor)
     for unit_id in unit_ids:
-        firing_rates[unit_id] = num_spikes[unit_id]/total_duration
+        firing_rates[unit_id] = num_spikes[unit_id] / total_duration
     return firing_rates
 
 
-def compute_presence_ratios(waveform_extractor, bin_duration_s=60):
-    """Calculate the presence ratio, representing the fraction of time the unit is firing.
+def compute_presence_ratios(waveform_extractor, bin_duration_s=60.0, mean_fr_ratio_thresh=0.0, unit_ids=None, **kwargs):
+    """Calculate the presence ratio, the fraction of time the unit is firing above a certain threshold.
 
     Parameters
     ----------
     waveform_extractor : WaveformExtractor
         The waveform extractor object.
     bin_duration_s : float, default: 60
-        The duration of each bin in seconds. If the duration is less than this value, 
+        The duration of each bin in seconds. If the duration is less than this value,
         presence_ratio is set to NaN
+    mean_fr_ratio_thresh: float, default: 0
+        The unit is considered active in a bin if its firing rate during that bin
+        is strictly above `mean_fr_ratio_thresh` times its mean firing rate throughout the recording.
+    unit_ids : list or None
+        The list of unit ids to compute the presence ratio. If None, all units are used.
 
     Returns
     -------
     presence_ratio : dict of flaots
         The presence ratio for each unit ID.
 
     Notes
     -----
     The total duration, across all segments, is divided into "num_bins".
     To do so, spike trains across segments are concatenated to mimic a continuous segment.
     """
     sorting = waveform_extractor.sorting
-    unit_ids = sorting.unit_ids
+    if unit_ids is None:
+        unit_ids = sorting.unit_ids
     num_segs = sorting.get_num_segments()
 
     seg_lengths = [waveform_extractor.get_num_samples(i) for i in range(num_segs)]
     total_length = waveform_extractor.get_total_samples()
+    total_duration = waveform_extractor.get_total_duration()
     bin_duration_samples = int((bin_duration_s * waveform_extractor.sampling_frequency))
     num_bin_edges = total_length // bin_duration_samples + 1
     bin_edges = np.arange(num_bin_edges) * bin_duration_samples
 
+    mean_fr_ratio_thresh = float(mean_fr_ratio_thresh)
+    if mean_fr_ratio_thresh < 0:
+        raise ValueError(
+            f"Expected positive float for `mean_fr_ratio_thresh` param." f"Provided value: {mean_fr_ratio_thresh}"
+        )
+    if mean_fr_ratio_thresh > 1:
+        warnings.warn("`mean_fr_ratio_thres` parameter above 1 might lead to low presence ratios.")
+
     presence_ratios = {}
     if total_length < bin_duration_samples:
-        warnings.warn(f"Bin duration of {bin_duration_s}s is larger than recording duration. "
-                      f"Presence ratios are set to NaN.")
-        presence_ratios = {unit_id: np.nan for unit_id in sorting.unit_ids}
+        warnings.warn(
+            f"Bin duration of {bin_duration_s}s is larger than recording duration. " f"Presence ratios are set to NaN."
+        )
+        presence_ratios = {unit_id: np.nan for unit_id in unit_ids}
     else:
         for unit_id in unit_ids:
             spike_train = []
             for segment_index in range(num_segs):
                 st = sorting.get_unit_spike_train(unit_id=unit_id, segment_index=segment_index)
                 st = st + np.sum(seg_lengths[:segment_index])
                 spike_train.append(st)
             spike_train = np.concatenate(spike_train)
 
-            presence_ratios[unit_id] = presence_ratio(spike_train, total_length, bin_edges=bin_edges)
+            unit_fr = spike_train.size / total_duration
+            bin_n_spikes_thres = math.floor(unit_fr * bin_duration_s * mean_fr_ratio_thresh)
+
+            presence_ratios[unit_id] = presence_ratio(
+                spike_train,
+                total_length,
+                bin_edges=bin_edges,
+                bin_n_spikes_thres=bin_n_spikes_thres,
+            )
 
     return presence_ratios
 
 
 _default_params["presence_ratio"] = dict(
-    bin_duration_s=60
+    bin_duration_s=60,
+    mean_fr_ratio_thresh=0.0,
 )
 
 
-def compute_snrs(waveform_extractor, peak_sign: str = 'neg', peak_mode: str = "extremum",
-                 random_chunk_kwargs_dict=None):
+def compute_snrs(
+    waveform_extractor,
+    peak_sign: str = "neg",
+    peak_mode: str = "extremum",
+    random_chunk_kwargs_dict=None,
+    unit_ids=None,
+):
     """Compute signal to noise ratio.
 
     Parameters
     ----------
     waveform_extractor : WaveformExtractor
         The waveform extractor object.
     peak_sign : {'neg', 'pos', 'both'}
@@ -154,34 +189,37 @@
     peak_mode: {'extremum', 'at_index'}
         How to compute the amplitude.
         Extremum takes the maxima/minima
         At_index takes the value at t=waveform_extractor.nbefore
     random_chunk_kwarg_dict: dict or None
         Dictionary to control the get_random_data_chunks() function.
         If None, default values are used
+    unit_ids : list or None
+        The list of unit ids to compute the SNR. If None, all units are used.
 
     Returns
     -------
     snrs : dict
         Computed signal to noise ratio for each unit.
     """
     if waveform_extractor.is_extension("noise_levels"):
         noise_levels = waveform_extractor.load_extension("noise_levels").get_data()
     else:
         if random_chunk_kwargs_dict is None:
             random_chunk_kwargs_dict = {}
-        noise_levels = get_noise_levels(waveform_extractor.recording,
-                                        return_scaled=waveform_extractor.return_scaled,
-                                        **random_chunk_kwargs_dict)
+        noise_levels = get_noise_levels(
+            waveform_extractor.recording, return_scaled=waveform_extractor.return_scaled, **random_chunk_kwargs_dict
+        )
 
     assert peak_sign in ("neg", "pos", "both")
     assert peak_mode in ("extremum", "at_index")
 
     sorting = waveform_extractor.sorting
-    unit_ids = sorting.unit_ids
+    if unit_ids is None:
+        unit_ids = sorting.unit_ids
     channel_ids = waveform_extractor.channel_ids
 
     extremum_channels_ids = get_template_extremum_channel(waveform_extractor, peak_sign=peak_sign, mode=peak_mode)
     unit_amplitudes = get_template_extremum_amplitude(waveform_extractor, peak_sign=peak_sign, mode=peak_mode)
 
     # make a dict to access by chan_id
     noise_levels = dict(zip(channel_ids, noise_levels))
@@ -192,22 +230,18 @@
         noise = noise_levels[chan_id]
         amplitude = unit_amplitudes[unit_id]
         snrs[unit_id] = np.abs(amplitude) / noise
 
     return snrs
 
 
-_default_params["snr"] = dict(
-    peak_sign="neg",
-    peak_mode="extremum",
-    random_chunk_kwargs_dict=None
-)
+_default_params["snr"] = dict(peak_sign="neg", peak_mode="extremum", random_chunk_kwargs_dict=None)
 
 
-def compute_isi_violations(waveform_extractor, isi_threshold_ms=1.5, min_isi_ms=0):
+def compute_isi_violations(waveform_extractor, isi_threshold_ms=1.5, min_isi_ms=0, unit_ids=None):
     """Calculate Inter-Spike Interval (ISI) violations.
 
     It computes several metrics related to isi violations:
         * isi_violations_ratio: the relative firing rate of the hypothetical neurons that are
                                 generating the ISI violations. Described in [1]. See Notes.
         * isi_violation_count: number of ISI violations
 
@@ -218,14 +252,16 @@
     isi_threshold_ms : float, default: 1.5
         Threshold for classifying adjacent spikes as an ISI violation, in ms.
         This is the biophysical refractory period (default=1.5).
     min_isi_ms : float, default: 0
         Minimum possible inter-spike interval, in ms.
         This is the artificial refractory period enforced
         by the data acquisition system or post-processing algorithms.
+    unit_ids : list or None
+        List of unit ids to compute the ISI violations. If None, all units are used.
 
     Returns
     -------
     isi_violations_ratio : dict
         The isi violation ratio described in [1].
     isi_violation_count : dict
         Number of violations.
@@ -239,56 +275,56 @@
     References
     ----------
     Based on metrics described in [Hill]_
 
     Originally written in Matlab by Nick Steinmetz (https://github.com/cortex-lab/sortingQuality)
     and converted to Python by Daniel Denman.
     """
-    res = namedtuple('isi_violation',
-                     ['isi_violations_ratio', 'isi_violations_count'])
+    res = namedtuple("isi_violation", ["isi_violations_ratio", "isi_violations_count"])
 
     sorting = waveform_extractor.sorting
-    unit_ids = sorting.unit_ids
+    if unit_ids is None:
+        unit_ids = sorting.unit_ids
     num_segs = sorting.get_num_segments()
 
     total_duration_s = waveform_extractor.get_total_duration()
     fs = waveform_extractor.sampling_frequency
 
     isi_threshold_s = isi_threshold_ms / 1000
     min_isi_s = min_isi_ms / 1000
 
     isi_violations_count = {}
     isi_violations_ratio = {}
 
     # all units converted to seconds
     for unit_id in unit_ids:
-
-        spike_trains = []
+        spike_train_list = []
 
         for segment_index in range(num_segs):
             spike_train = sorting.get_unit_spike_train(unit_id=unit_id, segment_index=segment_index)
-            spike_trains.append(spike_train / fs)
+            if len(spike_train) > 0:
+                spike_train_list.append(spike_train / fs)
+
+        if not any([len(train) > 0 for train in spike_train_list]):
+            continue
 
-        ratio, _, count = isi_violations(spike_trains, total_duration_s,
-                                         isi_threshold_s, min_isi_s)
+        ratio, _, count = isi_violations(spike_train_list, total_duration_s, isi_threshold_s, min_isi_s)
 
         isi_violations_ratio[unit_id] = ratio
         isi_violations_count[unit_id] = count
 
     return res(isi_violations_ratio, isi_violations_count)
 
 
-_default_params["isi_violation"] = dict(
-    isi_threshold_ms=1.5, 
-    min_isi_ms=0
-)
+_default_params["isi_violation"] = dict(isi_threshold_ms=1.5, min_isi_ms=0)
 
 
-def compute_refrac_period_violations(waveform_extractor, refractory_period_ms: float = 1.0,
-                                     censored_period_ms: float=0.0):
+def compute_refrac_period_violations(
+    waveform_extractor, refractory_period_ms: float = 1.0, censored_period_ms: float = 0.0, unit_ids=None
+):
     """Calculates the number of refractory period violations.
 
     This is similar (but slightly different) to the ISI violations.
     The key difference being that the violations are not only computed on consecutive spikes.
 
     This is required for some formulas (e.g. the ones from Llobet & Wyngaard 2022).
 
@@ -297,180 +333,222 @@
     waveform_extractor : WaveformExtractor
         The waveform extractor object
     refractory_period_ms : float, default: 1.0
         The period (in ms) where no 2 good spikes can occur.
     censored_period_ùs : float, default: 0.0
         The period (in ms) where no 2 spikes can occur (because they are not detected, or
         because they were removed by another mean).
+    unit_ids : list or None
+        List of unit ids to compute the refractory period violations. If None, all units are used.
 
     Returns
     -------
     rp_contamination : dict
         The refactory period contamination described in [1].
     rp_violations : dict
         Number of refractory period violations.
-    
+
     Notes
     -----
     Requires "numba" package
 
     References
     ----------
     Based on metrics described in [Llobet]_
 
     """
-    res = namedtuple("rp_violations", ['rp_contamination', 'rp_violations'])
+    res = namedtuple("rp_violations", ["rp_contamination", "rp_violations"])
 
     if not HAVE_NUMBA:
         print("Error: numba is not installed.")
         print("compute_refrac_period_violations cannot run without numba.")
         return None
 
     sorting = waveform_extractor.sorting
     fs = sorting.get_sampling_frequency()
     num_units = len(sorting.unit_ids)
     num_segments = sorting.get_num_segments()
     spikes = sorting.get_all_spike_trains(outputs="unit_index")
+    if unit_ids is None:
+        unit_ids = sorting.unit_ids
     num_spikes = compute_num_spikes(waveform_extractor)
 
     t_c = int(round(censored_period_ms * fs * 1e-3))
     t_r = int(round(refractory_period_ms * fs * 1e-3))
-    nb_rp_violations = np.zeros((num_units), dtype=np.int32)
+    nb_rp_violations = np.zeros((num_units), dtype=np.int64)
 
     for seg_index in range(num_segments):
-        _compute_rp_violations_numba(nb_rp_violations, spikes[seg_index][0].astype(np.int64),
-                                     spikes[seg_index][1].astype(np.int32), t_c, t_r)
+        _compute_rp_violations_numba(
+            nb_rp_violations, spikes[seg_index][0].astype(np.int64), spikes[seg_index][1].astype(np.int32), t_c, t_r
+        )
 
     T = waveform_extractor.get_total_samples()
 
     nb_violations = {}
     rp_contamination = {}
 
-    for i, unit_id in enumerate(sorting.unit_ids):
+    for i, unit_id in enumerate(unit_ids):
         nb_violations[unit_id] = n_v = nb_rp_violations[i]
         N = num_spikes[unit_id]
-        D = 1 - n_v * (T - 2*N*t_c) / (N**2 * (t_r - t_c))
-        rp_contamination[unit_id] = 1 - math.sqrt(D) if D >= 0 else 1.0
+        if N == 0:
+            rp_contamination[unit_id] = np.nan
+        else:
+            D = 1 - n_v * (T - 2 * N * t_c) / (N**2 * (t_r - t_c))
+            rp_contamination[unit_id] = 1 - math.sqrt(D) if D >= 0 else 1.0
 
     return res(rp_contamination, nb_violations)
 
 
-_default_params["rp_violation"] = dict(
-    refractory_period_ms=1.0,
-    censored_period_ms=0.0
-)
+_default_params["rp_violation"] = dict(refractory_period_ms=1.0, censored_period_ms=0.0)
 
 
-def compute_sliding_rp_violations(waveform_extractor, bin_size_ms=0.25, window_size_s=1,
-                                  exclude_ref_period_below_ms=0.5, max_ref_period_ms=10,
-                                  contamination_values=None):
-    """Compute sliding refractory period violations, a metric developed by IBL which computes 
+def compute_sliding_rp_violations(
+    waveform_extractor,
+    min_spikes=0,
+    bin_size_ms=0.25,
+    window_size_s=1,
+    exclude_ref_period_below_ms=0.5,
+    max_ref_period_ms=10,
+    contamination_values=None,
+    unit_ids=None,
+):
+    """Compute sliding refractory period violations, a metric developed by IBL which computes
     contamination by using a sliding refractory period.
     This metric computes the minimum contamination with at least 90% confidence.
 
     Parameters
     ----------
     waveform_extractor : WaveformExtractor
         The waveform extractor object.
+    min_spikes : int, default 0
+        Contamination  is set to np.nan if the unit has less than this many
+        spikes across all segments.
     bin_size_ms : float
         The size of binning for the autocorrelogram in ms, by default 0.25
     window_size_s : float
         Window in seconds to compute correlogram, by default 1
     exclude_ref_period_below_ms : float
         Refractory periods below this value are excluded, by default 0.5
     max_ref_period_ms : float
         Maximum refractory period to test in ms, by default 10 ms
     contamination_values : 1d array or None
         The contamination values to test, by default np.arange(0.5, 35, 0.5) %
+    unit_ids : list or None
+        List of unit ids to compute the sliding RP violations. If None, all units are used.
 
     Returns
     -------
     contamination : dict of floats
         The minimum contamination at 90% confidence
 
     References
     ----------
     Based on metrics described in [IBL]_
     This code was adapted from https://github.com/SteinmetzLab/slidingRefractory/blob/1.0.0/python/slidingRP/metrics.py
     """
     duration = waveform_extractor.get_total_duration()
     sorting = waveform_extractor.sorting
-    unit_ids = sorting.unit_ids
+    if unit_ids is None:
+        unit_ids = sorting.unit_ids
     num_segs = sorting.get_num_segments()
     fs = waveform_extractor.sampling_frequency
 
     contamination = {}
 
     # all units converted to seconds
     for unit_id in unit_ids:
-
         spike_train_list = []
 
         for segment_index in range(num_segs):
             spike_train = sorting.get_unit_spike_train(unit_id=unit_id, segment_index=segment_index)
-            spike_train_list.append(spike_train)
+            if np.any(spike_train):
+                spike_train_list.append(spike_train)
+
+        if not any([np.any(train) for train in spike_train_list]):
+            continue
 
-        contamination[unit_id] = slidingRP_violations(spike_train_list, fs, duration, bin_size_ms, window_size_s,
-                                                      exclude_ref_period_below_ms, max_ref_period_ms,
-                                                      contamination_values)
+        unit_n_spikes = np.sum([len(train) for train in spike_train_list])
+        if unit_n_spikes <= min_spikes:
+            contamination[unit_id] = np.nan
+            continue
+
+        contamination[unit_id] = slidingRP_violations(
+            spike_train_list,
+            fs,
+            duration,
+            bin_size_ms,
+            window_size_s,
+            exclude_ref_period_below_ms,
+            max_ref_period_ms,
+            contamination_values,
+        )
 
     return contamination
 
 
 _default_params["sliding_rp_violation"] = dict(
+    min_spikes=0,
     bin_size_ms=0.25,
     window_size_s=1,
     exclude_ref_period_below_ms=0.5,
     max_ref_period_ms=10,
-    contamination_values=None
+    contamination_values=None,
 )
 
 
-def compute_amplitude_cutoffs(waveform_extractor, peak_sign='neg',
-                              num_histogram_bins=500, histogram_smoothing_value=3,
-                              amplitudes_bins_min_ratio=5):
+def compute_amplitude_cutoffs(
+    waveform_extractor,
+    peak_sign="neg",
+    num_histogram_bins=500,
+    histogram_smoothing_value=3,
+    amplitudes_bins_min_ratio=5,
+    unit_ids=None,
+):
     """Calculate approximate fraction of spikes missing from a distribution of amplitudes.
 
     Parameters
     ----------
     waveform_extractor : WaveformExtractor
         The waveform extractor object.
     peak_sign : {'neg', 'pos', 'both'}
         The sign of the peaks.
     num_histogram_bins : int, default: 100
         The number of bins to use to compute the amplitude histogram.
     histogram_smoothing_value : int, default: 3
         Controls the smoothing applied to the amplitude histogram.
     amplitudes_bins_min_ratio : int, default: 5
         The minimum ratio between number of amplitudes for a unit and the number of bins.
-        If the ratio is less than this threshold, the amplitude_cutoff for the unit is set 
+        If the ratio is less than this threshold, the amplitude_cutoff for the unit is set
         to NaN.
+    unit_ids : list or None
+        List of unit ids to compute the amplitude cutoffs. If None, all units are used.
 
     Returns
     -------
     all_fraction_missing : dict of floats
         Estimated fraction of missing spikes, based on the amplitude distribution, for each unit ID.
 
 
     Notes
     -----
     This approach assumes the amplitude histogram is symmetric (not valid in the presence of drift).
-    If available, amplitudes are extracted from the "spike_amplitude" extension (recommended). 
+    If available, amplitudes are extracted from the "spike_amplitude" extension (recommended).
     If the "spike_amplitude" extension is not available, the amplitudes are extracted from the waveform extractor,
     which usually has waveforms for a small subset of spikes (500 by default).
 
     References
     ----------
     Inspired by metric described in [Hill]_
-    
+
     This code was adapted from https://github.com/AllenInstitute/ecephys_spike_sorting/tree/master/ecephys_spike_sorting/modules/quality_metrics
 
     """
     sorting = waveform_extractor.sorting
-    unit_ids = sorting.unit_ids
+    if unit_ids is None:
+        unit_ids = sorting.unit_ids
 
     before = waveform_extractor.nbefore
     extremum_channels_ids = get_template_extremum_channel(waveform_extractor, peak_sign=peak_sign)
 
     spike_amplitudes = None
     invert_amplitudes = False
     if waveform_extractor.is_extension("spike_amplitudes"):
@@ -496,60 +574,59 @@
         else:
             amplitudes = np.concatenate([spike_amps[unit_id] for spike_amps in spike_amplitudes])
 
         # change amplitudes signs in case peak_sign is pos
         if invert_amplitudes:
             amplitudes = -amplitudes
 
-        fraction_missing = amplitude_cutoff(amplitudes, num_histogram_bins, histogram_smoothing_value,
-                                            amplitudes_bins_min_ratio)
-        if np.isnan(fraction_missing) :
+        fraction_missing = amplitude_cutoff(
+            amplitudes, num_histogram_bins, histogram_smoothing_value, amplitudes_bins_min_ratio
+        )
+        if np.isnan(fraction_missing):
             nan_units.append(unit_id)
 
         all_fraction_missing[unit_id] = fraction_missing
 
     if len(nan_units) > 0:
-        warnings.warn(f"Units {nan_units} have too few spikes and "
-                       "amplitude_cutoff is set to NaN")
+        warnings.warn(f"Units {nan_units} have too few spikes and " "amplitude_cutoff is set to NaN")
 
     return all_fraction_missing
 
 
 _default_params["amplitude_cutoff"] = dict(
-    peak_sign='neg',
-    num_histogram_bins=100,
-    histogram_smoothing_value=3,
-    amplitudes_bins_min_ratio=5
+    peak_sign="neg", num_histogram_bins=100, histogram_smoothing_value=3, amplitudes_bins_min_ratio=5
 )
 
 
-
-def compute_amplitude_medians(waveform_extractor, peak_sign='neg'):
+def compute_amplitude_medians(waveform_extractor, peak_sign="neg", unit_ids=None):
     """Compute median of the amplitude distributions (in absolute value).
 
     Parameters
     ----------
     waveform_extractor : WaveformExtractor
         The waveform extractor object.
     peak_sign : {'neg', 'pos', 'both'}
         The sign of the peaks.
+    unit_ids : list or None
+        List of unit ids to compute the amplitude medians. If None, all units are used.
 
     Returns
     -------
     all_amplitude_medians : dict
         Estimated amplitude median for each unit ID.
 
     References
     ----------
     Inspired by metric described in [IBL]_
     This code is ported from:
     https://github.com/int-brain-lab/ibllib/blob/master/brainbox/metrics/single_units.py
     """
     sorting = waveform_extractor.sorting
-    unit_ids = sorting.unit_ids
+    if unit_ids is None:
+        unit_ids = sorting.unit_ids
 
     before = waveform_extractor.nbefore
 
     extremum_channels_ids = get_template_extremum_channel(waveform_extractor, peak_sign=peak_sign)
 
     spike_amplitudes = None
     if waveform_extractor.is_extension("spike_amplitudes"):
@@ -572,26 +649,30 @@
         # change amplitudes signs in case peak_sign is pos
         abs_amplitudes = np.abs(amplitudes)
         all_amplitude_medians[unit_id] = np.median(abs_amplitudes)
 
     return all_amplitude_medians
 
 
-_default_params["amplitude_median"] = dict(
-    peak_sign='neg'
-)
+_default_params["amplitude_median"] = dict(peak_sign="neg")
 
 
-def compute_drift_metrics(waveform_extractor, interval_s=60,
-                          min_spikes_per_interval=100, direction="y",
-                          min_fraction_valid_intervals=0.5, min_num_bins=2,
-                          return_positions=False):
+def compute_drift_metrics(
+    waveform_extractor,
+    interval_s=60,
+    min_spikes_per_interval=100,
+    direction="y",
+    min_fraction_valid_intervals=0.5,
+    min_num_bins=2,
+    return_positions=False,
+    unit_ids=None,
+):
     """Compute drifts metrics using estimated spike locations.
-    Over the duration of the recording, the drift signal for each unit is calculated as the median 
-    position in an interval with respect to the overall median positions over the entire duration 
+    Over the duration of the recording, the drift signal for each unit is calculated as the median
+    position in an interval with respect to the overall median positions over the entire duration
     (reference position).
 
     The following metrics are computed for each unit (in um):
 
     * drift_ptp: peak-to-peak of the drift signal
     * drift_std: standard deviation of the drift signal
     * drift_mad: median absolute deviation of the drift signal
@@ -609,69 +690,75 @@
     direction : str, optional
         The direction along which drift metrics are estimated, by default 'y'
     min_fraction_valid_intervals : float, optional
         The fraction of valid (not NaN) position estimates to estimate drifts.
         E.g., if 0.5 at least 50% of estimated positions in the intervals need to be valid,
         otherwise drift metrics are set to None, by default 0.5
     min_num_bins : int, optional
-        Minimum number of bins required to return a valid metric value. In case there are 
+        Minimum number of bins required to return a valid metric value. In case there are
         less bins, the metric values are set to NaN.
     return_positions : bool, optional
         If True, median positions are returned (for debugging), by default False
+    unit_ids : list or None
+        List of unit ids to compute the drift metrics. If None, all units are used.
 
     Returns
     -------
     drift_ptp : dict
         The drift signal peak-to-peak in um
     drift_std : dict
         The drift signal standard deviation in um
     drift_mad : dict
         The drift signal median absolute deviation in um
     median_positions : np.array (optional)
         The median positions of each unit over time (only returned if return_positions=True)
 
     Notes
     -----
-    For multi-segment object, segments are concatenated before the computation. This means that if 
+    For multi-segment object, segments are concatenated before the computation. This means that if
     there are large displacements in between segments, the resulting metric values will be very high.
     """
-    res = namedtuple("drift_metrics", ['drift_ptp', 'drift_std', 'drift_mad'])
+    res = namedtuple("drift_metrics", ["drift_ptp", "drift_std", "drift_mad"])
+    sorting = waveform_extractor.sorting
+    if unit_ids is None:
+        unit_ids = sorting.unit_ids
 
     if waveform_extractor.is_extension("spike_locations"):
         locs_calculator = waveform_extractor.load_extension("spike_locations")
         spike_locations = locs_calculator.get_data(outputs="concatenated")
         spike_locations_by_unit = locs_calculator.get_data(outputs="by_unit")
     else:
-        warnings.warn("The drift metrics require the `spike_locations` waveform extension. "
-                      "Use the `postprocessing.compute_spike_locations()` function. "
-                      "Drift metrics will be set to NaN")
-        empty_dict = {unit_id: np.nan for unit_id in waveform_extractor.unit_ids}
+        warnings.warn(
+            "The drift metrics require the `spike_locations` waveform extension. "
+            "Use the `postprocessing.compute_spike_locations()` function. "
+            "Drift metrics will be set to NaN"
+        )
+        empty_dict = {unit_id: np.nan for unit_id in unit_ids}
         if return_positions:
             return res(empty_dict, empty_dict, empty_dict), np.nan
         else:
             return res(empty_dict, empty_dict, empty_dict)
 
-    sorting = waveform_extractor.sorting
-    unit_ids = waveform_extractor.unit_ids
     interval_samples = int(interval_s * waveform_extractor.sampling_frequency)
     assert direction in spike_locations.dtype.names, (
-        f"Direction {direction} is invalid. Available directions: "
-        f"{spike_locations.dtype.names}"
+        f"Direction {direction} is invalid. Available directions: " f"{spike_locations.dtype.names}"
     )
     total_duration = waveform_extractor.get_total_duration()
     if total_duration < min_num_bins * interval_s:
-        warnings.warn("The recording is too short given the specified 'interval_s' and "
-                      "'min_num_bins'. Drift metrics will be set to NaN")
-        empty_dict = {unit_id: np.nan for unit_id in waveform_extractor.unit_ids}
+        warnings.warn(
+            "The recording is too short given the specified 'interval_s' and "
+            "'min_num_bins'. Drift metrics will be set to NaN"
+        )
+        empty_dict = {unit_id: np.nan for unit_id in unit_ids}
         if return_positions:
             return res(empty_dict, empty_dict, empty_dict), np.nan
         else:
             return res(empty_dict, empty_dict, empty_dict)
 
-    # we need 
+    # we need
     drift_ptps = {}
     drift_stds = {}
     drift_mads = {}
 
     # reference positions are the medians across segments
     reference_positions = np.zeros(len(unit_ids))
     for unit_ind, unit_id in enumerate(unit_ids):
@@ -685,37 +772,37 @@
     for segment_index in range(waveform_extractor.get_num_segments()):
         seg_length = waveform_extractor.get_num_samples(segment_index)
         num_bin_edges = seg_length // interval_samples + 1
         bins = np.arange(num_bin_edges) * interval_samples
         spike_vector = sorting.to_spike_vector()
 
         # retrieve spikes in segment
-        i0 = np.searchsorted(spike_vector['segment_ind'], segment_index)
-        i1 = np.searchsorted(spike_vector['segment_ind'], segment_index + 1)
+        i0 = np.searchsorted(spike_vector["segment_index"], segment_index)
+        i1 = np.searchsorted(spike_vector["segment_index"], segment_index + 1)
         spikes_in_segment = spike_vector[i0:i1]
         spike_locations_in_segment = spike_locations[i0:i1]
 
         # compute median positions (if less than min_spikes_per_interval, median position is 0)
         median_positions = np.nan * np.zeros((len(unit_ids), num_bin_edges - 1))
         for bin_index, (start_frame, end_frame) in enumerate(zip(bins[:-1], bins[1:])):
-            i0 = np.searchsorted(spikes_in_segment['sample_ind'], start_frame)
-            i1 = np.searchsorted(spikes_in_segment['sample_ind'], end_frame)
+            i0 = np.searchsorted(spikes_in_segment["sample_index"], start_frame)
+            i1 = np.searchsorted(spikes_in_segment["sample_index"], end_frame)
             spikes_in_bin = spikes_in_segment[i0:i1]
             spike_locations_in_bin = spike_locations_in_segment[i0:i1][direction]
 
             for unit_ind in np.arange(len(unit_ids)):
-                mask = spikes_in_bin['unit_ind'] == unit_ind
+                mask = spikes_in_bin["unit_index"] == unit_ind
                 if np.sum(mask) >= min_spikes_per_interval:
                     median_positions[unit_ind, bin_index] = np.median(spike_locations_in_bin[mask])
         if median_position_segments is None:
             median_position_segments = median_positions
         else:
             median_position_segments = np.hstack((median_position_segments, median_positions))
 
-    # finally, compute deviations and drifts    
+    # finally, compute deviations and drifts
     position_diffs = median_position_segments - reference_positions[:, None]
     for unit_ind, unit_id in enumerate(unit_ids):
         position_diff = position_diffs[unit_ind]
         if np.any(np.isnan(position_diff)):
             # deal with nans: if more than 50% nans --> set to nan
             if np.sum(np.isnan(position_diff)) > min_fraction_valid_intervals * len(position_diff):
                 ptp_drift = np.nan
@@ -735,59 +822,54 @@
     if return_positions:
         outs = res(drift_ptps, drift_stds, drift_mads), median_positions
     else:
         outs = res(drift_ptps, drift_stds, drift_mads)
     return outs
 
 
-_default_params["drift"] = dict(
-    interval_s=60,
-    min_spikes_per_interval=100,
-    direction="y",
-    min_num_bins=2
-)
-
+_default_params["drift"] = dict(interval_s=60, min_spikes_per_interval=100, direction="y", min_num_bins=2)
 
 
 ### LOW-LEVEL FUNCTIONS ###
-def presence_ratio(spike_train, total_length, bin_edges=None, num_bin_edges=None):
+def presence_ratio(spike_train, total_length, bin_edges=None, num_bin_edges=None, bin_n_spikes_thres=0):
     """Calculate the presence ratio for a single unit
 
     Parameters
     ----------
     spike_train : np.ndarray
         Spike times for this unit, in samples
     total_length : int
         Total length of the recording in samples
     bin_edges : np.array
         Pre-computed bin edges (mutually exclusive with num_bin_edges).
     num_bin_edges : int, default: 101
         The number of bins edges to use to compute the presence ratio.
         (mutually exclusive with bin_edges).
+    bin_n_spikes_thres: int, default 0
+        Minimum number of spikes within a bin to consider the unit active
 
     Returns
     -------
     presence_ratio : float
         The presence ratio for one unit
 
     """
     assert bin_edges is not None or num_bin_edges is not None, "Use either bin_edges or num_bin_edges"
+    assert bin_n_spikes_thres >= 0
     if bin_edges is not None:
         bins = bin_edges
         num_bin_edges = len(bin_edges)
     else:
         bins = num_bin_edges
     h, _ = np.histogram(spike_train, bins=bins)
-    
-    return np.sum(h > 0) / (num_bin_edges - 1)
+
+    return np.sum(h > bin_n_spikes_thres) / (num_bin_edges - 1)
 
 
-def isi_violations(spike_trains, total_duration_s,
-                   isi_threshold_s=0.0015, 
-                   min_isi_s=0):
+def isi_violations(spike_trains, total_duration_s, isi_threshold_s=0.0015, min_isi_s=0):
     """Calculate Inter-Spike Interval (ISI) violations.
 
     See compute_isi_violations for additional documentation
 
     Parameters
     ----------
     spike_trains : list of np.ndarrays
@@ -822,27 +904,26 @@
 
     for spike_train in spike_trains:
         isis = np.diff(spike_train)
         num_spikes += len(spike_train)
         num_violations += np.sum(isis < isi_threshold_s)
 
     violation_time = 2 * num_spikes * (isi_threshold_s - min_isi_s)
-    
+
     if num_spikes > 0:
         total_rate = num_spikes / total_duration_s
         violation_rate = num_violations / violation_time
         isi_violations_ratio = violation_rate / total_rate
         isi_violations_rate = num_violations / total_duration_s
-        isi_violations_count = num_violations      
-    
+        isi_violations_count = num_violations
+
     return isi_violations_ratio, isi_violations_rate, isi_violations_count
 
 
-def amplitude_cutoff(amplitudes, num_histogram_bins=500, histogram_smoothing_value=3,
-                     amplitudes_bins_min_ratio=5):
+def amplitude_cutoff(amplitudes, num_histogram_bins=500, histogram_smoothing_value=3, amplitudes_bins_min_ratio=5):
     """Calculate approximate fraction of spikes missing from a distribution of amplitudes.
 
 
     See compute_amplitude_cutoffs for additional documentation
 
     Parameters
     ----------
@@ -852,52 +933,64 @@
         The sign of the template to compute best channels.
     num_histogram_bins : int, default: 500
         The number of bins to use to compute the amplitude histogram.
     histogram_smoothing_value : int, default: 3
         Controls the smoothing applied to the amplitude histogram.
     amplitudes_bins_min_ratio : int, default: 5
         The minimum ratio between number of amplitudes for a unit and the number of bins.
-        If the ratio is less than this threshold, the amplitude_cutoff for the unit is set 
+        If the ratio is less than this threshold, the amplitude_cutoff for the unit is set
         to NaN.
 
     Returns
     -------
     fraction_missing : float
         Estimated fraction of missing spikes, based on the amplitude distribution.
 
     """
     if len(amplitudes) / num_histogram_bins < amplitudes_bins_min_ratio:
         return np.nan
     else:
         h, b = np.histogram(amplitudes, num_histogram_bins, density=True)
 
         # TODO : use something better than scipy.ndimage.gaussian_filter1d
+        from scipy.ndimage import gaussian_filter1d
+
         pdf = gaussian_filter1d(h, histogram_smoothing_value)
         support = b[:-1]
         bin_size = np.mean(np.diff(support))
         peak_index = np.argmax(pdf)
-        
+
         pdf_above = np.abs(pdf[peak_index:] - pdf[0])
 
         if len(np.where(pdf_above == pdf_above.min())[0]) > 1:
-            warnings.warn("Amplitude PDF does not have a unique minimum! More spikes might be required for a correct "
-                          "amplitude_cutoff computation!")
-        
+            warnings.warn(
+                "Amplitude PDF does not have a unique minimum! More spikes might be required for a correct "
+                "amplitude_cutoff computation!"
+            )
+
         G = np.argmin(pdf_above) + peak_index
         fraction_missing = np.sum(pdf[G:]) * bin_size
         fraction_missing = np.min([fraction_missing, 0.5])
 
         return fraction_missing
 
 
-def slidingRP_violations(spike_samples, sample_rate, duration, bin_size_ms=0.25, window_size_s=1,
-                         exclude_ref_period_below_ms=0.5, max_ref_period_ms=10,
-                         contamination_values=None, return_conf_matrix=False):
+def slidingRP_violations(
+    spike_samples,
+    sample_rate,
+    duration,
+    bin_size_ms=0.25,
+    window_size_s=1,
+    exclude_ref_period_below_ms=0.5,
+    max_ref_period_ms=10,
+    contamination_values=None,
+    return_conf_matrix=False,
+):
     """
-    A metric developed by IBL which determines whether the refractory period violations 
+    A metric developed by IBL which determines whether the refractory period violations
     by using sliding refractory periods.
 
     See compute_slidingRP_viol for additional documentation
 
     Parameters
     ----------
     spike_samples : ndarray_like or list (for multi-segment)
@@ -921,42 +1014,49 @@
 
     Returns
     -------
     min_cont_with_90_confidence : dict of floats
         The minimum contamination with confidence > 90%
     """
     if contamination_values is None:
-        contamination_values = np.arange(0.5, 35, 0.5) / 100 # vector of contamination values to test
+        contamination_values = np.arange(0.5, 35, 0.5) / 100  # vector of contamination values to test
     rp_bin_size = bin_size_ms / 1000
     rp_edges = np.arange(0, max_ref_period_ms / 1000, rp_bin_size)  # in s
-    rp_centers = rp_edges + ((rp_edges[1] - rp_edges[0]) / 2) # vector of refractory period durations to test
-    
+    rp_centers = rp_edges + ((rp_edges[1] - rp_edges[0]) / 2)  # vector of refractory period durations to test
+
     # compute firing rate and spike count (concatenate for multi-segments)
     n_spikes = len(np.concatenate(spike_samples))
     firing_rate = n_spikes / duration
     if np.isscalar(spike_samples[0]):
         spike_samples_list = [spike_samples]
     else:
         spike_samples_list = spike_samples
     # compute correlograms
     correlogram = None
     for spike_samples in spike_samples_list:
-        c0 = correlogram_for_one_segment(spike_samples, np.zeros(len(spike_samples), dtype='int8'),
-                                         bin_size=max(int(bin_size_ms / 1000 * sample_rate), 1), # convert to sample counts
-                                         window_size=int(window_size_s * sample_rate))[0, 0]
+        c0 = correlogram_for_one_segment(
+            spike_samples,
+            np.zeros(len(spike_samples), dtype="int8"),
+            bin_size=max(int(bin_size_ms / 1000 * sample_rate), 1),  # convert to sample counts
+            window_size=int(window_size_s * sample_rate),
+        )[0, 0]
         if correlogram is None:
             correlogram = c0
         else:
             correlogram += c0
-    correlogram_positive = correlogram[len(correlogram)//2:]
+    correlogram_positive = correlogram[len(correlogram) // 2 :]
 
-    conf_matrix = _compute_violations(np.cumsum(correlogram_positive[0:rp_centers.size])[np.newaxis, :],
-                                      firing_rate, n_spikes, rp_centers[np.newaxis, :] + rp_bin_size / 2,
-                                      contamination_values[:, np.newaxis])
-    test_rp_centers_mask = rp_centers > exclude_ref_period_below_ms / 1000. # (in seconds)
+    conf_matrix = _compute_violations(
+        np.cumsum(correlogram_positive[0 : rp_centers.size])[np.newaxis, :],
+        firing_rate,
+        n_spikes,
+        rp_centers[np.newaxis, :] + rp_bin_size / 2,
+        contamination_values[:, np.newaxis],
+    )
+    test_rp_centers_mask = rp_centers > exclude_ref_period_below_ms / 1000.0  # (in seconds)
 
     # only test for refractory period durations greater than 'exclude_ref_period_below_ms'
     inds_confidence90 = np.row_stack(np.where(conf_matrix[:, test_rp_centers_mask] > 0.9))
 
     if len(inds_confidence90[0]) > 0:
         minI = np.min(inds_confidence90[0][0])
         min_cont_with_90_confidence = contamination_values[minI]
@@ -965,43 +1065,52 @@
     if return_conf_matrix:
         return min_cont_with_90_confidence, conf_matrix
     else:
         return min_cont_with_90_confidence
 
 
 def _compute_violations(obs_viol, firing_rate, spike_count, ref_period_dur, contamination_prop):
-    contamination_rate = firing_rate * contamination_prop 
+    contamination_rate = firing_rate * contamination_prop
     expected_viol = contamination_rate * ref_period_dur * 2 * spike_count
+
+    from scipy.stats import poisson
+
     confidence_score = 1 - poisson.cdf(obs_viol, expected_viol)
 
     return confidence_score
 
 
 if HAVE_NUMBA:
+
     @numba.jit((numba.int64[::1], numba.int32), nopython=True, nogil=True, cache=True)
     def _compute_nb_violations_numba(spike_train, t_r):
         n_v = 0
         N = len(spike_train)
 
         for i in range(N):
-            for j in range(i+1, N):
+            for j in range(i + 1, N):
                 diff = spike_train[j] - spike_train[i]
 
                 if diff > t_r:
                     break
 
                 # if diff < t_c:
                 #     continue
 
                 n_v += 1
 
         return n_v
 
-    @numba.jit((numba.int32[::1], numba.int64[::1], numba.int32[::1], numba.int32, numba.int32),
-               nopython=True, nogil=True, cache=True, parallel=True)
+    @numba.jit(
+        (numba.int64[::1], numba.int64[::1], numba.int32[::1], numba.int32, numba.int32),
+        nopython=True,
+        nogil=True,
+        cache=True,
+        parallel=True,
+    )
     def _compute_rp_violations_numba(nb_rp_violations, spike_trains, spike_clusters, t_c, t_r):
         n_units = len(nb_rp_violations)
 
         for i in numba.prange(n_units):
             spike_train = spike_trains[spike_clusters == i]
             n_v = _compute_nb_violations_numba(spike_train, t_r)
             nb_rp_violations[i] += n_v
```

### Comparing `spikeinterface-0.97.1/spikeinterface/qualitymetrics/pca_metrics.py` & `spikeinterface-0.98.0/src/spikeinterface/qualitymetrics/pca_metrics.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,71 +1,75 @@
 """Cluster quality metrics computed from principal components."""
 
 from cmath import nan
+from copy import deepcopy
+
 import numpy as np
 from tqdm.auto import tqdm
-import scipy.stats
-import scipy.spatial.distance
-from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
-from sklearn.neighbors import NearestNeighbors
-from sklearn.decomposition import IncrementalPCA
-from joblib import delayed, Parallel
-from copy import deepcopy
+
+try:
+    import scipy.stats
+    import scipy.spatial.distance
+    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
+    from sklearn.neighbors import NearestNeighbors
+    from sklearn.decomposition import IncrementalPCA
+    from joblib import delayed, Parallel
+except:
+    pass
 
 import spikeinterface as si
 from ..core import get_random_data_chunks, compute_sparsity, WaveformExtractor
 from ..core.job_tools import tqdm_joblib
 from ..core.template_tools import get_template_extremum_channel
 
 from ..postprocessing import WaveformPrincipalComponent
 import warnings
 
-from .misc_metrics import compute_num_spikes
+from .misc_metrics import compute_num_spikes, compute_firing_rates
 
 from ..core import get_random_data_chunks, load_waveforms, compute_sparsity, WaveformExtractor
 from ..core.job_tools import tqdm_joblib
 from ..core.template_tools import get_template_extremum_channel
 from ..postprocessing import WaveformPrincipalComponent
 
 
-_possible_pc_metric_names = ['isolation_distance', 'l_ratio', 'd_prime',
-                             'nearest_neighbor', 'nn_isolation', 'nn_noise_overlap']
+_possible_pc_metric_names = [
+    "isolation_distance",
+    "l_ratio",
+    "d_prime",
+    "nearest_neighbor",
+    "nn_isolation",
+    "nn_noise_overlap",
+    "silhouette",
+]
 
 
 _default_params = dict(
     nearest_neighbor=dict(
         max_spikes=10000,
         n_neighbors=5,
     ),
     nn_isolation=dict(
-        max_spikes=10000,
-        min_spikes=10,
-        n_neighbors=4,
-        n_components=10,
-        radius_um=100,
-        peak_sign='neg'
+        max_spikes=10000, min_spikes=10, min_fr=0.0, n_neighbors=4, n_components=10, radius_um=100, peak_sign="neg"
     ),
     nn_noise_overlap=dict(
-        max_spikes=10000,
-        min_spikes=10,
-        n_neighbors=4,
-        n_components=10,
-        radius_um=100,
-        peak_sign='neg'
-    )
+        max_spikes=10000, min_spikes=10, min_fr=0.0, n_neighbors=4, n_components=10, radius_um=100, peak_sign="neg"
+    ),
+    silhouette=dict(method=("simplified",)),
 )
 
 
 def get_quality_pca_metric_list():
     """Get a list of the available PCA-based quality metrics."""
     return deepcopy(_possible_pc_metric_names)
 
 
-def calculate_pc_metrics(pca, metric_names=None, sparsity=None, qm_params=None,
-                         seed=None, n_jobs=1, progress_bar=False):
+def calculate_pc_metrics(
+    pca, metric_names=None, sparsity=None, qm_params=None, unit_ids=None, seed=None, n_jobs=1, progress_bar=False
+):
     """Calculate principal component derived metrics.
 
     Parameters
     ----------
     pca : WaveformPrincipalComponent
         Waveform object with principal components computed.
     metric_names : list of str, optional
@@ -73,14 +77,16 @@
         If not provided, defaults to all PC metrics.
     sparsity: ChannelSparsity or None
         The sparsity object. This is used also to identify neighbor
         units and speed up computations. If None (default) all channels and all units are used
         for each unit.
     qm_params : dict or None
         Dictionary with parameters for each PC metric function.
+    unit_ids : list of int or None
+        List of unit ids to compute metrics for.
     seed : int, default: None
         Random seed value.
     n_jobs : int
         Number of jobs to parallelize metric computations.
     progress_bar : bool
         If True, progress bar is shown.
 
@@ -94,53 +100,78 @@
     if qm_params is None:
         qm_params = _default_params
 
     assert isinstance(pca, WaveformPrincipalComponent)
     we = pca.waveform_extractor
     extremum_channels = get_template_extremum_channel(we)
 
-    unit_ids = we.unit_ids
+    if unit_ids is None:
+        unit_ids = we.unit_ids
     channel_ids = we.channel_ids
 
     # create output dict of dict  pc_metrics['metric_name'][unit_id]
     pc_metrics = {k: {} for k in metric_names}
-    if 'nearest_neighbor' in metric_names:
-        pc_metrics.pop('nearest_neighbor')
-        pc_metrics['nn_hit_rate'] = {}
-        pc_metrics['nn_miss_rate'] = {}
+    if "nearest_neighbor" in metric_names:
+        pc_metrics.pop("nearest_neighbor")
+        pc_metrics["nn_hit_rate"] = {}
+        pc_metrics["nn_miss_rate"] = {}
+
+    if "nn_isolation" in metric_names:
+        pc_metrics["nn_unit_id"] = {}
+
+    # Compute nspikes and firing rate outside of main loop for speed
+    if any([n in metric_names for n in ["nn_isolation", "nn_noise_overlap"]]):
+        n_spikes_all_units = compute_num_spikes(we, unit_ids=unit_ids)
+        fr_all_units = compute_firing_rates(we, unit_ids=unit_ids)
+    else:
+        n_spikes_all_units = None
+        fr_all_units = None
 
     run_in_parallel = n_jobs > 1
 
     units_loop = enumerate(unit_ids)
     if progress_bar and not run_in_parallel:
         units_loop = tqdm(units_loop, desc="Computing PCA metrics", total=len(unit_ids))
 
     if run_in_parallel:
         parallel_functions = []
 
     all_labels, all_pcs = pca.get_all_projections()
-    for unit_ind, unit_id in units_loop:        
+    for unit_ind, unit_id in units_loop:
         if we.is_sparse():
             neighbor_channel_ids = we.sparsity.unit_id_to_channel_ids[unit_id]
-            neighbor_unit_ids = [other_unit for other_unit in unit_ids
-                                 if extremum_channels[other_unit] in neighbor_channel_ids]
+            neighbor_unit_ids = [
+                other_unit for other_unit in unit_ids if extremum_channels[other_unit] in neighbor_channel_ids
+            ]
         elif sparsity is not None:
             neighbor_channel_ids = sparsity.unit_id_to_channel_ids[unit_id]
-            neighbor_unit_ids = [other_unit for other_unit in unit_ids 
-                                 if extremum_channels[other_unit] in neighbor_channel_ids]
+            neighbor_unit_ids = [
+                other_unit for other_unit in unit_ids if extremum_channels[other_unit] in neighbor_channel_ids
+            ]
         else:
             neighbor_channel_ids = channel_ids
             neighbor_unit_ids = unit_ids
         neighbor_channel_indices = we.channel_ids_to_indices(neighbor_channel_ids)
 
         labels = all_labels[np.in1d(all_labels, neighbor_unit_ids)]
         pcs = all_pcs[np.in1d(all_labels, neighbor_unit_ids)][:, :, neighbor_channel_indices]
         pcs_flat = pcs.reshape(pcs.shape[0], -1)
 
-        func_args = (pcs_flat, labels, metric_names, unit_id, unit_ids, qm_params, seed, we.folder)
+        func_args = (
+            pcs_flat,
+            labels,
+            metric_names,
+            unit_id,
+            unit_ids,
+            qm_params,
+            seed,
+            we.folder,
+            n_spikes_all_units,
+            fr_all_units,
+        )
 
         if not run_in_parallel:
             pca_metrics_unit = pca_metrics_one_unit(*func_args)
             for metric_name, metric in pca_metrics_unit.items():
                 pc_metrics[metric_name][unit_id] = metric
         else:
             parallel_functions.append(delayed(pca_metrics_one_unit)(*func_args))
@@ -160,14 +191,15 @@
 
     return pc_metrics
 
 
 #################################################################
 # Code from spikemetrics
 
+
 def mahalanobis_metrics(all_pcs, all_labels, this_unit_id):
     """Calculates isolation distance and L-ratio (metrics computed from Mahalanobis distance)
 
     Parameters
     ----------
     all_pcs : 2d array
         The PCs for all spikes, organized as [num_spikes, PCs].
@@ -195,29 +227,24 @@
 
     try:
         VI = np.linalg.inv(np.cov(pcs_for_this_unit.T))
     except np.linalg.linalg.LinAlgError:
         # case of singular matrix
         return np.nan, np.nan
 
-    mahalanobis_other = np.sort(scipy.spatial.distance.cdist(mean_value,
-                                                             pcs_for_other_units,
-                                                             'mahalanobis', VI=VI)[0])
-
-    mahalanobis_self = np.sort(scipy.spatial.distance.cdist(mean_value,
-                                                            pcs_for_this_unit,
-                                                            'mahalanobis', VI=VI)[0])
+    mahalanobis_other = np.sort(scipy.spatial.distance.cdist(mean_value, pcs_for_other_units, "mahalanobis", VI=VI)[0])
+
+    mahalanobis_self = np.sort(scipy.spatial.distance.cdist(mean_value, pcs_for_this_unit, "mahalanobis", VI=VI)[0])
 
     # number of spikes
     n = np.min([pcs_for_this_unit.shape[0], pcs_for_other_units.shape[0]])
 
     if n >= 2:
         dof = pcs_for_this_unit.shape[1]  # number of features
-        l_ratio = np.sum(1 - scipy.stats.chi2.cdf(pow(mahalanobis_other, 2), dof)) \
-            / mahalanobis_self.shape[0]
+        l_ratio = np.sum(1 - scipy.stats.chi2.cdf(pow(mahalanobis_other, 2), dof)) / mahalanobis_self.shape[0]
         isolation_distance = pow(mahalanobis_other[n - 1], 2)
         # if math.isnan(l_ratio):
         #     print("NaN detected", mahalanobis_other, VI)
     else:
         l_ratio = np.nan
         isolation_distance = np.nan
 
@@ -244,26 +271,27 @@
     References
     ----------
     Based on metric described in [Hill]_
     """
 
     X = all_pcs
 
-    y = np.zeros((X.shape[0],), dtype='bool')
+    y = np.zeros((X.shape[0],), dtype="bool")
     y[all_labels == this_unit_id] = True
 
     lda = LinearDiscriminantAnalysis(n_components=1)
 
     X_flda = lda.fit_transform(X, y)
 
     flda_this_cluster = X_flda[np.where(y)[0]]
     flda_other_cluster = X_flda[np.where(np.invert(y))[0]]
 
     d_prime = (np.mean(flda_this_cluster) - np.mean(flda_other_cluster)) / np.sqrt(
-        0.5 * (np.std(flda_this_cluster) ** 2 + np.std(flda_other_cluster) ** 2))
+        0.5 * (np.std(flda_this_cluster) ** 2 + np.std(flda_other_cluster) ** 2)
+    )
 
     return d_prime
 
 
 def nearest_neighbors_metrics(all_pcs, all_labels, this_unit_id, max_spikes, n_neighbors):
     """
     Calculates unit contamination based on NearestNeighbors search in PCA space.
@@ -314,68 +342,91 @@
     this_unit_pcs = all_pcs[this_unit, :]
     other_units_pcs = all_pcs[np.invert(this_unit), :]
     X = np.concatenate((this_unit_pcs, other_units_pcs), 0)
 
     num_obs_this_unit = np.sum(this_unit)
 
     if ratio < 1:
-        inds = np.arange(0, X.shape[0] - 1, 1 / ratio).astype('int')
+        inds = np.arange(0, X.shape[0] - 1, 1 / ratio).astype("int")
         X = X[inds, :]
         num_obs_this_unit = int(num_obs_this_unit * ratio)
 
-    nbrs = NearestNeighbors(n_neighbors=n_neighbors,
-                            algorithm='ball_tree').fit(X)
+    nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm="ball_tree").fit(X)
     distances, indices = nbrs.kneighbors(X)
 
     this_cluster_nearest = indices[:num_obs_this_unit, 1:].flatten()
     other_cluster_nearest = indices[num_obs_this_unit:, 1:].flatten()
 
     hit_rate = np.mean(this_cluster_nearest < num_obs_this_unit)
     miss_rate = np.mean(other_cluster_nearest < num_obs_this_unit)
 
     return hit_rate, miss_rate
 
 
-def nearest_neighbors_isolation(waveform_extractor: WaveformExtractor, this_unit_id: int,
-                                max_spikes: int = 1000, min_spikes: int = 10, n_neighbors: int = 5,
-                                n_components: int = 10, radius_um: float = 100, peak_sign: str = 'neg',
-                                min_spatial_overlap: float = 0.5, seed=None):
+def nearest_neighbors_isolation(
+    waveform_extractor: WaveformExtractor,
+    this_unit_id: int,
+    n_spikes_all_units: dict = None,
+    fr_all_units: dict = None,
+    max_spikes: int = 1000,
+    min_spikes: int = 10,
+    min_fr: float = 0.0,
+    n_neighbors: int = 5,
+    n_components: int = 10,
+    radius_um: float = 100,
+    peak_sign: str = "neg",
+    min_spatial_overlap: float = 0.5,
+    seed=None,
+):
     """Calculates unit isolation based on NearestNeighbors search in PCA space.
 
     Parameters
     ----------
     waveform_extractor : WaveformExtractor
         The waveform extractor object.
     this_unit_id : int
         The ID for the unit to calculate these metrics for.
+    n_spikes_all_units: dict, default: None
+        Dictionary of the form ``{<unit_id>: <n_spikes>}`` for the waveform extractor.
+        Recomputed if None.
+    fr_all_units: dict, default: None
+        Dictionary of the form ``{<unit_id>: <firing_rate>}`` for the waveform extractor.
+        Recomputed if None.
     max_spikes : int, default: 1000
         Max number of spikes to use per unit.
-    min_spikes : int, optional, defalt: 10
+    min_spikes : int, optional, default: 10
         Min number of spikes a unit must have to go through with metric computation.
-        Units with spikes < min_spikes gets numpy.NaN as the quality metric.
+        Units with spikes < min_spikes gets numpy.NaN as the quality metric,
+        and are ignored when selecting other units' neighbors.
+    min_fr : float, optional, default: 0.0
+        Min firing rate a unit must have to go through with metric computation.
+        Units with firing rate < min_fr gets numpy.NaN as the quality metric,
+        and are ignored when selecting other units' neighbors.
     n_neighbors : int, default: 5
         Number of neighbors to check membership of.
     n_components : int, default: 10
         The number of PC components to use to project the snippets to.
     radius_um : float, default: 100
         The radius, in um, that channels need to be within the peak channel to be included.
     peak_sign: str, default: 'neg'
-        The peak_sign used to compute sparsity and neighbor units. Used if waveform_extractor 
+        The peak_sign used to compute sparsity and neighbor units. Used if waveform_extractor
         is not sparse already.
     min_spatial_overlap : float, default: 100
-        In case waveform_extractor is sparse, other units are selected if they share at least 
+        In case waveform_extractor is sparse, other units are selected if they share at least
         `min_spatial_overlap` times `n_target_unit_channels` with the target unit
     seed : int, default: None
         Seed for random subsampling of spikes.
 
     Returns
     -------
     nn_isolation : float
         The calculation nearest neighbor isolation metric for `this_unit_id`.
         If the unit has fewer than `min_spikes`, returns numpy.NaN instead.
+    nn_unit_id : np.int16
+        Id of the "nearest neighbor" unit (unit with lowest isolation score from `this_unit_id`)
 
     Notes
     -----
     The overall logic of this approach is:
 
     #. Choose a cluster
     #. Compute the isolation score with every other cluster
@@ -403,55 +454,76 @@
     ----------
     Based on isolation metric described in [Chung]_
     """
     rng = np.random.default_rng(seed=seed)
 
     sorting = waveform_extractor.sorting
     all_units_ids = sorting.get_unit_ids()
-    n_spikes_all_units = compute_num_spikes(waveform_extractor)
+    if n_spikes_all_units is None:
+        n_spikes_all_units = compute_num_spikes(waveform_extractor)
+    if fr_all_units is None:
+        fr_all_units = compute_firing_rates(waveform_extractor)
 
     # if target unit has fewer than `min_spikes` spikes, print out a warning and return NaN
     if n_spikes_all_units[this_unit_id] < min_spikes:
-        warnings.warn(f'Warning: unit {this_unit_id} has fewer spikes than ',
-                      f'specified by `min_spikes` ({min_spikes}); ',
-                      f'returning NaN as the quality metric...')
-        return np.nan
+        warnings.warn(
+            f"Warning: unit {this_unit_id} has fewer spikes than ",
+            f"specified by `min_spikes` ({min_spikes}); ",
+            f"returning NaN as the quality metric...",
+        )
+        return np.nan, np.nan
+    elif fr_all_units[this_unit_id] < min_fr:
+        warnings.warn(
+            f"Warning: unit {this_unit_id} has a firing rate ",
+            f"below the specified `min_fr` ({min_fr}Hz); " f"returning NaN as the quality metric...",
+        )
+        return np.nan, np.nan
     else:
         # first remove the units with too few spikes
-        unit_ids_to_keep = np.array([unit for unit, num_spikes in n_spikes_all_units.items()
-                                     if num_spikes >= min_spikes])
+        unit_ids_to_keep = np.array(
+            [
+                unit
+                for unit in all_units_ids
+                if (n_spikes_all_units[unit] >= min_spikes and fr_all_units[unit] >= min_fr)
+            ]
+        )
         sorting = sorting.select_units(unit_ids=unit_ids_to_keep)
 
         all_units_ids = sorting.get_unit_ids()
         other_units_ids = np.setdiff1d(all_units_ids, this_unit_id)
 
         # get waveforms of target unit
         waveforms_target_unit = waveform_extractor.get_waveforms(unit_id=this_unit_id)
         n_spikes_target_unit = waveforms_target_unit.shape[0]
 
         # find units whose signal channels (i.e. channels inside some radius around
         # the channel with largest amplitude) overlap with signal channels of the target unit
         if waveform_extractor.is_sparse():
             sparsity = waveform_extractor.sparsity
         else:
-            sparsity = compute_sparsity(waveform_extractor, method='radius', peak_sign=peak_sign,
-                                        radius_um=radius_um)
+            sparsity = compute_sparsity(waveform_extractor, method="radius", peak_sign=peak_sign, radius_um=radius_um)
         closest_chans_target_unit = sparsity.unit_id_to_channel_indices[this_unit_id]
         n_channels_target_unit = len(closest_chans_target_unit)
         # select other units that have a minimum spatial overlap with target unit
-        other_units_ids = [unit_id for unit_id in other_units_ids if
-                           np.sum(np.in1d(sparsity.unit_id_to_channel_indices[unit_id], closest_chans_target_unit))
-                           >= (n_channels_target_unit * min_spatial_overlap)]
+        other_units_ids = [
+            unit_id
+            for unit_id in other_units_ids
+            if np.sum(np.in1d(sparsity.unit_id_to_channel_indices[unit_id], closest_chans_target_unit))
+            >= (n_channels_target_unit * min_spatial_overlap)
+        ]
 
         # if no unit is within neighborhood of target unit, then just say isolation is 1 (best possible)
         if not other_units_ids:
             nn_isolation = 1
+            nn_unit_id = np.nan
         # if there are units to compare, then compute isolation with each
         else:
-            isolation = np.zeros(len(other_units_ids),)
+            isolation = np.zeros(
+                len(other_units_ids),
+            )
             for other_unit_id in other_units_ids:
                 waveforms_other_unit = waveform_extractor.get_waveforms(unit_id=other_unit_id)
                 n_spikes_other_unit = waveforms_other_unit.shape[0]
                 closest_chans_other_unit = sparsity.unit_id_to_channel_indices[other_unit_id]
                 n_snippets = np.min([n_spikes_target_unit, n_spikes_other_unit, max_spikes])
 
                 # make the two clusters equal in terms of: number of spikes & channels with signal
@@ -460,65 +532,92 @@
                 waveforms_other_unit_idx = rng.choice(n_spikes_other_unit, size=n_snippets, replace=False)
                 waveforms_other_unit_sampled = waveforms_other_unit[waveforms_other_unit_idx]
 
                 # project this unit and other unit waveforms on common subspace
                 common_channel_idxs = np.intersect1d(closest_chans_target_unit, closest_chans_other_unit)
                 if waveform_extractor.is_sparse():
                     # in this case, waveforms are sparse so we need to do some smart indexing
-                    waveforms_target_unit_sampled = \
-                        waveforms_target_unit_sampled[:, :, np.in1d(closest_chans_target_unit, common_channel_idxs)]
-                    waveforms_other_unit_sampled = \
-                        waveforms_other_unit_sampled[:, :, np.in1d(closest_chans_other_unit, common_channel_idxs)]
+                    waveforms_target_unit_sampled = waveforms_target_unit_sampled[
+                        :, :, np.in1d(closest_chans_target_unit, common_channel_idxs)
+                    ]
+                    waveforms_other_unit_sampled = waveforms_other_unit_sampled[
+                        :, :, np.in1d(closest_chans_other_unit, common_channel_idxs)
+                    ]
                 else:
                     waveforms_target_unit_sampled = waveforms_target_unit_sampled[:, :, common_channel_idxs]
                     waveforms_other_unit_sampled = waveforms_other_unit_sampled[:, :, common_channel_idxs]
 
                 # compute principal components after concatenation
-                all_snippets = np.concatenate([waveforms_target_unit_sampled.reshape((n_snippets, -1)),
-                                               waveforms_other_unit_sampled.reshape((n_snippets, -1))], axis=0)
+                all_snippets = np.concatenate(
+                    [
+                        waveforms_target_unit_sampled.reshape((n_snippets, -1)),
+                        waveforms_other_unit_sampled.reshape((n_snippets, -1)),
+                    ],
+                    axis=0,
+                )
                 pca = IncrementalPCA(n_components=n_components)
                 pca.partial_fit(all_snippets)
                 projected_snippets = pca.transform(all_snippets)
 
                 # compute isolation
-                isolation[other_unit_id == other_units_ids] = _compute_isolation(projected_snippets[:n_snippets, :],
-                                                                                 projected_snippets[n_snippets:, :],
-                                                                                 n_neighbors)
+                isolation[other_unit_id == other_units_ids] = _compute_isolation(
+                    projected_snippets[:n_snippets, :], projected_snippets[n_snippets:, :], n_neighbors
+                )
             # isolation metric is the minimum of the pairwise isolations
+            # nn_unit_id is the unit with lowest isolation score
             nn_isolation = np.min(isolation)
+            nn_unit_id = other_units_ids[np.argmin(isolation)]
 
-        return nn_isolation
+        return nn_isolation, nn_unit_id
 
 
-def nearest_neighbors_noise_overlap(waveform_extractor: WaveformExtractor,
-                                    this_unit_id: int, max_spikes: int = 1000,
-                                    min_spikes: int = 10, n_neighbors: int = 5,
-                                    n_components: int = 10, radius_um: float = 100,
-                                    peak_sign: str = 'neg', seed=None):
+def nearest_neighbors_noise_overlap(
+    waveform_extractor: WaveformExtractor,
+    this_unit_id: int,
+    n_spikes_all_units: dict = None,
+    fr_all_units: dict = None,
+    max_spikes: int = 1000,
+    min_spikes: int = 10,
+    min_fr: float = 0.0,
+    n_neighbors: int = 5,
+    n_components: int = 10,
+    radius_um: float = 100,
+    peak_sign: str = "neg",
+    seed=None,
+):
     """Calculates unit noise overlap based on NearestNeighbors search in PCA space.
 
     Parameters
     ----------
     waveform_extractor : WaveformExtractor
         The waveform extractor object.
     this_unit_id : int
         The ID of the unit to calculate this metric on.
+    n_spikes_all_units: dict, default: None
+        Dictionary of the form ``{<unit_id>: <n_spikes>}`` for the waveform extractor.
+        Recomputed if None.
+    fr_all_units: dict, default: None
+        Dictionary of the form ``{<unit_id>: <firing_rate>}`` for the waveform extractor.
+        Recomputed if None.
     max_spikes : int, default: 1000
         The max number of spikes to use per cluster.
-    min_spikes : int, optional, defalt: 10
+    min_spikes : int, optional, default: 10
         Min number of spikes a unit must have to go through with metric computation.
         Units with spikes < min_spikes gets numpy.NaN as the quality metric.
+    min_fr : float, optional, default: 0.0
+        Min firing rate a unit must have to go through with metric computation.
+        Units with firing rate < min_fr gets numpy.NaN as the quality metric.
     n_neighbors : int, default: 5
         The number of neighbors to check membership.
     n_components : int, default: 10
         The number of PC components to use to project the snippets to.
     radius_um : float, default: 100
         The radius, in um, that channels need to be within the peak channel to be included.
     peak_sign: str, default: 'neg'
-        The peak_sign used to compute sparsity and neighbor units. Used if waveform_extractor 
+        The peak_sign used to compute sparsity and neighbor units. Used if waveform_extractor
         is not sparse already.
     seed : int, default: 0
         Random seed for subsampling spikes.
 
     Returns
     -------
     nn_noise_overlap : float
@@ -541,28 +640,43 @@
 
     References
     ----------
     Based on noise overlap metric described in [Chung]_
     """
     rng = np.random.default_rng(seed=seed)
 
-    n_spikes_all_units = compute_num_spikes(waveform_extractor)
+    if n_spikes_all_units is None:
+        n_spikes_all_units = compute_num_spikes(waveform_extractor)
+    if fr_all_units is None:
+        fr_all_units = compute_firing_rates(waveform_extractor)
 
     # if target unit has fewer than `min_spikes` spikes, print out a warning and return NaN
     if n_spikes_all_units[this_unit_id] < min_spikes:
-        warnings.warn(f'Warning: unit {this_unit_id} has fewer spikes than ',
-                      f'specified by `min_spikes` ({min_spikes}); ',
-                      f'returning NaN as the quality metric...')
+        warnings.warn(
+            f"Warning: unit {this_unit_id} has fewer spikes than ",
+            f"specified by `min_spikes` ({min_spikes}); ",
+            f"returning NaN as the quality metric...",
+        )
+        return np.nan
+    elif fr_all_units[this_unit_id] < min_fr:
+        warnings.warn(
+            f"Warning: unit {this_unit_id} has a firing rate ",
+            f"below the specified `min_fr` ({min_fr}Hz); " f"returning NaN as the quality metric...",
+        )
         return np.nan
     else:
         # get random snippets from the recording to create a noise cluster
         recording = waveform_extractor.recording
-        noise_cluster = get_random_data_chunks(recording, return_scaled=waveform_extractor.return_scaled,
-                                               num_chunks_per_segment=max_spikes,
-                                               chunk_size=waveform_extractor.nsamples, seed=seed)
+        noise_cluster = get_random_data_chunks(
+            recording,
+            return_scaled=waveform_extractor.return_scaled,
+            num_chunks_per_segment=max_spikes,
+            chunk_size=waveform_extractor.nsamples,
+            seed=seed,
+        )
         noise_cluster = np.reshape(noise_cluster, (max_spikes, waveform_extractor.nsamples, -1))
 
         # get waveforms for target cluster
         waveforms = waveform_extractor.get_waveforms(unit_id=this_unit_id).copy()
 
         # adjust the size of the target and noise clusters to be equal
         if waveforms.shape[0] > max_spikes:
@@ -576,56 +690,145 @@
         else:
             n_snippets = max_spikes
 
         # restrict to channels with significant signal
         if waveform_extractor.is_sparse():
             sparsity = waveform_extractor.sparsity
         else:
-            sparsity = compute_sparsity(waveform_extractor, method='radius', peak_sign=peak_sign,
-                                        radius_um=radius_um)
+            sparsity = compute_sparsity(waveform_extractor, method="radius", peak_sign=peak_sign, radius_um=radius_um)
         noise_cluster = noise_cluster[:, :, sparsity.unit_id_to_channel_indices[this_unit_id]]
 
         # compute weighted noise snippet (Z)
-        median_waveform = waveform_extractor.get_template(unit_id=this_unit_id, mode='median')
+        median_waveform = waveform_extractor.get_template(unit_id=this_unit_id, mode="median")
 
         # in case waveform_extractor is sparse, waveforms and templates are already sparse
         if not waveform_extractor.is_sparse():
             waveforms = waveforms[:, :, sparsity.unit_id_to_channel_indices[this_unit_id]]
             median_waveform = median_waveform[:, sparsity.unit_id_to_channel_indices[this_unit_id]]
 
         tmax, chmax = np.unravel_index(np.argmax(np.abs(median_waveform)), median_waveform.shape)
         weights = [noise_clip[tmax, chmax] for noise_clip in noise_cluster]
         weights = np.asarray(weights)
         weights = weights / np.sum(weights)
         weighted_noise_snippet = np.sum(weights * noise_cluster.swapaxes(0, 2), axis=2).swapaxes(0, 1)
 
         # subtract projection onto weighted noise snippet
         for snippet in range(n_snippets):
-            waveforms[snippet, :, :] = \
-                _subtract_clip_component(waveforms[snippet, :, :], weighted_noise_snippet)
-            noise_cluster[snippet, :, :] = \
-                _subtract_clip_component(noise_cluster[snippet, :, :], weighted_noise_snippet)
+            waveforms[snippet, :, :] = _subtract_clip_component(waveforms[snippet, :, :], weighted_noise_snippet)
+            noise_cluster[snippet, :, :] = _subtract_clip_component(
+                noise_cluster[snippet, :, :], weighted_noise_snippet
+            )
 
         # compute principal components after concatenation
-        all_snippets = np.concatenate([waveforms.reshape((n_snippets, -1)),
-                                       noise_cluster.reshape((n_snippets, -1))], axis=0)
+        all_snippets = np.concatenate(
+            [waveforms.reshape((n_snippets, -1)), noise_cluster.reshape((n_snippets, -1))], axis=0
+        )
         pca = IncrementalPCA(n_components=n_components)
         pca.partial_fit(all_snippets)
         projected_snippets = pca.transform(all_snippets)
 
         # compute overlap
-        nn_noise_overlap = 1 - _compute_isolation(projected_snippets[:n_snippets, :],
-                                                  projected_snippets[n_snippets:, :],
-                                                  n_neighbors)
+        nn_noise_overlap = 1 - _compute_isolation(
+            projected_snippets[:n_snippets, :], projected_snippets[n_snippets:, :], n_neighbors
+        )
 
         return nn_noise_overlap
 
 
-def _subtract_clip_component(clip1, component):
+def simplified_silhouette_score(all_pcs, all_labels, this_unit_id):
+    """Calculates the simplified silhouette score for each cluster. The value ranges
+    from -1 (bad clustering) to 1 (good clustering). The simplified silhoutte score
+    utilizes the centroids for distance calculations rather than pairwise calculations.
+    Parameters
+    ----------
+    all_pcs : 2d array
+        The PCs for all spikes, organized as [num_spikes, PCs].
+    all_labels : 1d array
+        The cluster labels for all spikes. Must have length of number of spikes.
+    this_unit_id : int
+        The ID for the unit to calculate this metric for.
+    Returns
+    -------
+    unit_silhouette_score : float
+        Simplified Silhouette Score for this unit
+    References
+    ------------
+    Based on simplified silhouette score suggested by [Hruschka]_
+    """
+
+    pcs_for_this_unit = all_pcs[all_labels == this_unit_id, :]
+    centroid_for_this_unit = np.expand_dims(np.mean(pcs_for_this_unit, 0), 0)
+    distances_for_this_unit = scipy.spatial.distance.cdist(centroid_for_this_unit, pcs_for_this_unit)
+    distance = np.inf
+
+    # find centroid of other cluster and measure distances to that rather than pairwise
+    # if less than current minimum distance update
+    for label in np.unique(all_labels):
+        if label != this_unit_id:
+            pcs_for_other_cluster = all_pcs[all_labels == label, :]
+            centroid_for_other_cluster = np.expand_dims(np.mean(pcs_for_other_cluster, 0), 0)
+            distances_for_other_cluster = scipy.spatial.distance.cdist(centroid_for_other_cluster, pcs_for_this_unit)
+            mean_distance_for_other_cluster = np.mean(distances_for_other_cluster)
+            if mean_distance_for_other_cluster < distance:
+                distance = mean_distance_for_other_cluster
+                distances_for_minimum_cluster = distances_for_other_cluster
 
+    sil_distances = (distances_for_minimum_cluster - distances_for_this_unit) / np.maximum(
+        distances_for_minimum_cluster, distances_for_this_unit
+    )
+
+    unit_silhouette_score = np.mean(sil_distances)
+    return unit_silhouette_score
+
+
+def silhouette_score(all_pcs, all_labels, this_unit_id):
+    """Calculates the silhouette score which is a marker of cluster quality ranging from
+    -1 (bad clustering) to 1 (good clustering). Distances are all calculated as pairwise
+    comparisons of all data points.
+    Parameters
+    ----------
+    all_pcs : 2d array
+        The PCs for all spikes, organized as [num_spikes, PCs].
+    all_labels : 1d array
+        The cluster labels for all spikes. Must have length of number of spikes.
+    this_unit_id : int
+        The ID for the unit to calculate this metric for.
+    Returns
+    -------
+    unit_silhouette_score : float
+        Silhouette Score for this unit
+    References
+    ------------
+    Based on [Rousseeuw]_
+    """
+
+    pcs_for_this_unit = all_pcs[all_labels == this_unit_id, :]
+    distances_for_this_unit = scipy.spatial.distance.cdist(pcs_for_this_unit, pcs_for_this_unit)
+    distance = np.inf
+
+    # iterate through all other clusters and do pairwise distance comparisons
+    # if current cluster distances < current mimimum update
+    for label in np.unique(all_labels):
+        if label != this_unit_id:
+            pcs_for_other_cluster = all_pcs[all_labels == label, :]
+            distances_for_other_cluster = scipy.spatial.distance.cdist(pcs_for_other_cluster, pcs_for_this_unit)
+            mean_distance_for_other_cluster = np.mean(distances_for_other_cluster)
+            if mean_distance_for_other_cluster < distance:
+                distance = mean_distance_for_other_cluster
+                distances_for_minimum_cluster = distances_for_other_cluster
+
+    sil_distances = (distances_for_minimum_cluster - distances_for_this_unit) / np.maximum(
+        distances_for_minimum_cluster, distances_for_this_unit
+    )
+
+    unit_silhouette_score = np.mean(sil_distances)
+    return unit_silhouette_score
+
+
+def _subtract_clip_component(clip1, component):
     V1 = clip1.flatten()
     V2 = component.flatten()
     V1 = V1 - V2 * np.dot(V1, V2) / np.dot(V2, V2)
 
     return V1.reshape(clip1.shape)
 
 
@@ -663,75 +866,109 @@
 
     # concatenate
     pcs_concat = np.concatenate((pcs_target_unit, pcs_other_unit), axis=0)
     label_concat = np.concatenate((np.zeros(n_spikes_target), np.ones(n_spikes_other)))
 
     # if n_neighbors is greater than the number of spikes in both clusters, set it to max possible
     if n_neighbors > len(label_concat):
-        n_neighbors_adjusted = len(label_concat)-1
+        n_neighbors_adjusted = len(label_concat) - 1
     else:
         n_neighbors_adjusted = n_neighbors
 
-    _, membership_ind = NearestNeighbors(
-        n_neighbors=n_neighbors_adjusted, algorithm='auto').fit(pcs_concat).kneighbors()
+    _, membership_ind = (
+        NearestNeighbors(n_neighbors=n_neighbors_adjusted, algorithm="auto").fit(pcs_concat).kneighbors()
+    )
 
     target_nn_in_target = np.sum(label_concat[membership_ind[:n_spikes_target]] == 0)
     other_nn_in_other = np.sum(label_concat[membership_ind[n_spikes_target:]] == 1)
 
-    isolation = (target_nn_in_target + other_nn_in_other) / (n_spikes_target+n_spikes_other) / n_neighbors_adjusted
+    isolation = (target_nn_in_target + other_nn_in_other) / (n_spikes_target + n_spikes_other) / n_neighbors_adjusted
 
     return isolation
 
 
-def pca_metrics_one_unit(pcs_flat, labels, metric_names, unit_id,
-                         unit_ids, qm_params, seed, we_folder):
-    if 'nn_isolation' in metric_names or 'nn_noise_overlap' in metric_names:
+def pca_metrics_one_unit(
+    pcs_flat, labels, metric_names, unit_id, unit_ids, qm_params, seed, we_folder, n_spikes_all_units, fr_all_units
+):
+    if "nn_isolation" in metric_names or "nn_noise_overlap" in metric_names:
         we = load_waveforms(we_folder)
 
     pc_metrics = {}
     # metrics
-    if 'isolation_distance' in metric_names or 'l_ratio' in metric_names:
+    if "isolation_distance" in metric_names or "l_ratio" in metric_names:
         try:
             isolation_distance, l_ratio = mahalanobis_metrics(pcs_flat, labels, unit_id)
         except:
             isolation_distance = np.nan
             l_ratio = np.nan
-        if 'isolation_distance' in metric_names:
-            pc_metrics['isolation_distance'] = isolation_distance
-        if 'l_ratio' in metric_names:
-            pc_metrics['l_ratio'] = l_ratio
+        if "isolation_distance" in metric_names:
+            pc_metrics["isolation_distance"] = isolation_distance
+        if "l_ratio" in metric_names:
+            pc_metrics["l_ratio"] = l_ratio
 
-    if 'd_prime' in metric_names:
+    if "d_prime" in metric_names:
         if len(unit_ids) == 1:
             d_prime = np.nan
         else:
             try:
                 d_prime = lda_metrics(pcs_flat, labels, unit_id)
             except:
                 d_prime = np.nan
-        pc_metrics['d_prime'] = d_prime
+        pc_metrics["d_prime"] = d_prime
 
-    if 'nearest_neighbor' in metric_names:
+    if "nearest_neighbor" in metric_names:
         try:
-            nn_hit_rate, nn_miss_rate = nearest_neighbors_metrics(pcs_flat, labels, unit_id, 
-                                                                  **qm_params['nearest_neighbor'])
+            nn_hit_rate, nn_miss_rate = nearest_neighbors_metrics(
+                pcs_flat, labels, unit_id, **qm_params["nearest_neighbor"]
+            )
         except:
             nn_hit_rate = np.nan
             nn_miss_rate = np.nan
-        pc_metrics['nn_hit_rate'] = nn_hit_rate
-        pc_metrics['nn_miss_rate'] = nn_miss_rate
+        pc_metrics["nn_hit_rate"] = nn_hit_rate
+        pc_metrics["nn_miss_rate"] = nn_miss_rate
 
-    if 'nn_isolation' in metric_names:
+    if "nn_isolation" in metric_names:
         try:
-            nn_isolation = nearest_neighbors_isolation(we, unit_id, seed=seed, **qm_params['nn_isolation'])
+            nn_isolation, nn_unit_id = nearest_neighbors_isolation(
+                we,
+                unit_id,
+                seed=seed,
+                n_spikes_all_units=n_spikes_all_units,
+                fr_all_units=fr_all_units,
+                **qm_params["nn_isolation"],
+            )
         except:
             nn_isolation = np.nan
-        pc_metrics['nn_isolation'] = nn_isolation
+            nn_unit_id = np.nan
+        pc_metrics["nn_isolation"] = nn_isolation
+        pc_metrics["nn_unit_id"] = nn_unit_id
 
-    if 'nn_noise_overlap' in metric_names:
+    if "nn_noise_overlap" in metric_names:
         try:
-            nn_noise_overlap = nearest_neighbors_noise_overlap(we, unit_id, seed=seed, **qm_params['nn_noise_overlap'])
+            nn_noise_overlap = nearest_neighbors_noise_overlap(
+                we,
+                unit_id,
+                n_spikes_all_units=n_spikes_all_units,
+                fr_all_units=fr_all_units,
+                seed=seed,
+                **qm_params["nn_noise_overlap"],
+            )
         except:
             nn_noise_overlap = np.nan
-        pc_metrics['nn_noise_overlap'] = nn_noise_overlap
+        pc_metrics["nn_noise_overlap"] = nn_noise_overlap
+
+    if "silhouette" in metric_names:
+        silhouette_method = qm_params["silhouette"]["method"]
+        if "simplified" in silhouette_method:
+            try:
+                unit_silhouette_score = simplified_silhouette_score(pcs_flat, labels, unit_id)
+            except:
+                unit_silhouette_score = np.nan
+            pc_metrics["silhouette"] = unit_silhouette_score
+        if "full" in silhouette_method:
+            try:
+                unit_silhouette_score = silhouette_score(pcs_flat, labels, unit_id)
+            except:
+                unit_silhouette_score = np.nan
+            pc_metrics["silhouette_full"] = unit_silhouette_socre
 
     return pc_metrics
```

### Comparing `spikeinterface-0.97.1/spikeinterface/qualitymetrics/quality_metric_calculator.py` & `spikeinterface-0.98.0/src/spikeinterface/qualitymetrics/quality_metric_calculator.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,187 +1,210 @@
 """Classes and functions for computing multiple quality metrics."""
-
+import warnings
 from copy import deepcopy
 
 import numpy as np
-import pandas as pd
 
 from spikeinterface.core.job_tools import fix_job_kwargs
 from spikeinterface.core.waveform_extractor import WaveformExtractor, BaseWaveformExtractorExtension
 
-from .quality_metric_list import (calculate_pc_metrics,
-                                  _misc_metric_name_to_func,
-                                  _possible_pc_metric_names)
+from .quality_metric_list import calculate_pc_metrics, _misc_metric_name_to_func, _possible_pc_metric_names
 from .misc_metrics import _default_params as misc_metrics_params
 from .pca_metrics import _default_params as pca_metrics_params
 
 
-
 class QualityMetricCalculator(BaseWaveformExtractorExtension):
     """Class to compute quality metrics of spike sorting output.
 
     Parameters
     ----------
     waveform_extractor: WaveformExtractor
         The waveform extractor object
 
     Notes
     -----
     principal_components are loaded automatically if already computed.
     """
 
-    extension_name = 'quality_metrics'
+    extension_name = "quality_metrics"
 
     def __init__(self, waveform_extractor):
         BaseWaveformExtractorExtension.__init__(self, waveform_extractor)
 
         if waveform_extractor.has_recording():
             self.recording = waveform_extractor.recording
         else:
             self.recording = None
         self.sorting = waveform_extractor.sorting
 
-    def _set_params(self, metric_names=None, qm_params=None, peak_sign=None,
-                    seed=None, sparsity=None, skip_pc_metrics=False):
-
+    def _set_params(
+        self, metric_names=None, qm_params=None, peak_sign=None, seed=None, sparsity=None, skip_pc_metrics=False
+    ):
         if metric_names is None:
             metric_names = list(_misc_metric_name_to_func.keys())
             # if PC is available, PC metrics are automatically added to the list
-            if self.waveform_extractor.is_extension('principal_components'):
+            if self.waveform_extractor.is_extension("principal_components"):
                 # by default 'nearest_neightbor' is removed because too slow
                 pc_metrics = _possible_pc_metric_names.copy()
                 pc_metrics.remove("nn_isolation")
                 pc_metrics.remove("nn_noise_overlap")
                 metric_names += pc_metrics
             # if spike_locations are not available, drift is removed from the list
-            if not self.waveform_extractor.is_extension('spike_locations'):
+            if not self.waveform_extractor.is_extension("spike_locations"):
                 if "drift" in metric_names:
                     metric_names.remove("drift")
 
         qm_params_ = get_default_qm_params()
         for k in qm_params_:
             if qm_params is not None and k in qm_params:
                 qm_params_[k].update(qm_params[k])
-            if 'peak_sign' in qm_params_[k] and peak_sign is not None:
-                qm_params_[k]['peak_sign'] = peak_sign
+            if "peak_sign" in qm_params_[k] and peak_sign is not None:
+                qm_params_[k]["peak_sign"] = peak_sign
 
-        params = dict(metric_names=[str(name) for name in metric_names],
-                      sparsity=sparsity,
-                      peak_sign=peak_sign,
-                      seed=seed,
-                      qm_params=qm_params_,
-                      skip_pc_metrics=skip_pc_metrics)
+        params = dict(
+            metric_names=[str(name) for name in metric_names],
+            sparsity=sparsity,
+            peak_sign=peak_sign,
+            seed=seed,
+            qm_params=qm_params_,
+            skip_pc_metrics=skip_pc_metrics,
+        )
 
         return params
 
     def _select_extension_data(self, unit_ids):
         # filter metrics dataframe
-        new_metrics = self._extension_data['metrics'].loc[np.array(unit_ids)]
+        new_metrics = self._extension_data["metrics"].loc[np.array(unit_ids)]
         return dict(metrics=new_metrics)
 
     def _run(self, verbose, **job_kwargs):
         """
         Compute quality metrics.
         """
-        metric_names = self._params['metric_names']
-        qm_params = self._params['qm_params']
-        sparsity = self._params['sparsity']
-        seed = self._params['seed']
+        metric_names = self._params["metric_names"]
+        qm_params = self._params["qm_params"]
+        sparsity = self._params["sparsity"]
+        seed = self._params["seed"]
 
         # update job_kwargs with global ones
         job_kwargs = fix_job_kwargs(job_kwargs)
-        n_jobs = job_kwargs['n_jobs']
-        progress_bar = job_kwargs['progress_bar']
+        n_jobs = job_kwargs["n_jobs"]
+        progress_bar = job_kwargs["progress_bar"]
 
         unit_ids = self.sorting.unit_ids
+        non_empty_unit_ids = self.sorting.get_non_empty_unit_ids()
+        empty_unit_ids = unit_ids[~np.isin(unit_ids, non_empty_unit_ids)]
+        if len(empty_unit_ids) > 0:
+            warnings.warn(
+                f"Units {empty_unit_ids} are empty. Quality metrcs will be set to NaN "
+                f"for these units.\n To remove empty units, use `sorting.remove_empty_units()`."
+            )
+
+        import pandas as pd
+
         metrics = pd.DataFrame(index=unit_ids)
 
         # simple metrics not based on PCs
         for metric_name in metric_names:
             # keep PC metrics for later
             if metric_name in _possible_pc_metric_names:
                 continue
             if verbose:
                 if metric_name not in _possible_pc_metric_names:
                     print(f"Computing {metric_name}")
 
             func = _misc_metric_name_to_func[metric_name]
 
             params = qm_params[metric_name] if metric_name in qm_params else {}
-            res = func(self.waveform_extractor, **params)
+            res = func(self.waveform_extractor, unit_ids=non_empty_unit_ids, **params)
             # QM with uninstall dependencies might return None
             if res is not None:
                 if isinstance(res, dict):
                     # res is a dict convert to series
-                    metrics[metric_name] = pd.Series(res)
+                    metrics.loc[non_empty_unit_ids, metric_name] = pd.Series(res)
                 else:
                     # res is a namedtuple with several dict
                     # so several columns
                     for i, col in enumerate(res._fields):
-                        metrics[col] = pd.Series(res[i])
+                        metrics.loc[non_empty_unit_ids, col] = pd.Series(res[i])
 
         # metrics based on PCs
         pc_metric_names = [k for k in metric_names if k in _possible_pc_metric_names]
-        if len(pc_metric_names) > 0 and not self._params['skip_pc_metrics']:
-            if not self.waveform_extractor.is_extension('principal_components'):
-                raise ValueError('waveform_principal_component must be provied')
-            pc_extension = self.waveform_extractor.load_extension('principal_components')
-            pc_metrics = calculate_pc_metrics(pc_extension,
-                                              metric_names=pc_metric_names, 
-                                              sparsity=sparsity,
-                                              progress_bar=progress_bar, 
-                                              n_jobs=n_jobs, qm_params=qm_params,
-                                              seed=seed)
+        if len(pc_metric_names) > 0 and not self._params["skip_pc_metrics"]:
+            if not self.waveform_extractor.is_extension("principal_components"):
+                raise ValueError("waveform_principal_component must be provied")
+            pc_extension = self.waveform_extractor.load_extension("principal_components")
+            pc_metrics = calculate_pc_metrics(
+                pc_extension,
+                unit_ids=non_empty_unit_ids,
+                metric_names=pc_metric_names,
+                sparsity=sparsity,
+                progress_bar=progress_bar,
+                n_jobs=n_jobs,
+                qm_params=qm_params,
+                seed=seed,
+            )
             for col, values in pc_metrics.items():
-                metrics[col] = pd.Series(values)
+                metrics.loc[non_empty_unit_ids, col] = pd.Series(values)
 
-        self._extension_data['metrics'] = metrics
+        # add NaN for empty units
+        if len(empty_unit_ids) > 0:
+            metrics.loc[empty_unit_ids] = np.nan
 
+        self._extension_data["metrics"] = metrics
 
     def get_data(self):
         """
         Get the computed metrics.
-        
+
         Returns
         -------
         metrics : pd.DataFrame
             Dataframe with quality metrics
         """
         msg = "Quality metrics are not computed. Use the 'run()' function."
-        assert self._extension_data['metrics'] is not None, msg
-        return self._extension_data['metrics']
+        assert self._extension_data["metrics"] is not None, msg
+        return self._extension_data["metrics"]
 
     @staticmethod
     def get_extension_function():
         return compute_quality_metrics
 
 
 WaveformExtractor.register_extension(QualityMetricCalculator)
 
 
-def compute_quality_metrics(waveform_extractor, load_if_exists=False,
-                            metric_names=None, qm_params=None, peak_sign=None, seed=None,
-                            sparsity=None, skip_pc_metrics=False, 
-                            verbose=False, **job_kwargs):
+def compute_quality_metrics(
+    waveform_extractor,
+    load_if_exists=False,
+    metric_names=None,
+    qm_params=None,
+    peak_sign=None,
+    seed=None,
+    sparsity=None,
+    skip_pc_metrics=False,
+    verbose=False,
+    **job_kwargs,
+):
     """Compute quality metrics on waveform extractor.
 
     Parameters
     ----------
     waveform_extractor: WaveformExtractor
         The waveform extractor to compute metrics on.
     load_if_exists : bool, default: False
         Whether to load precomputed quality metrics, if they already exist.
     metric_names : list or None
         List of quality metrics to compute.
     qm_params : dict or None
         Dictionary with parameters for quality metrics calculation.
         Default parameters can be obtained with: `si.qualitymetrics.get_default_qm_params()`
     sparsity : dict or None
-        If given, the sparse channel_ids for each unit in PCA metrics computation. 
-        This is used also to identify neighbor units and speed up computations. 
+        If given, the sparse channel_ids for each unit in PCA metrics computation.
+        This is used also to identify neighbor units and speed up computations.
         If None (default) all channels and all units are used for each unit.
     skip_pc_metrics : bool
         If True, PC metrics computation is skipped.
     n_jobs : int
         Number of jobs (used for PCA metrics)
     verbose : bool
         If True, output is verbose.
@@ -193,16 +216,22 @@
     metrics: pandas.DataFrame
         Data frame with the computed metrics
     """
     if load_if_exists and waveform_extractor.is_extension(QualityMetricCalculator.extension_name):
         qmc = waveform_extractor.load_extension(QualityMetricCalculator.extension_name)
     else:
         qmc = QualityMetricCalculator(waveform_extractor)
-        qmc.set_params(metric_names=metric_names, qm_params=qm_params, peak_sign=peak_sign, seed=seed,
-                       sparsity=sparsity, skip_pc_metrics=skip_pc_metrics)
+        qmc.set_params(
+            metric_names=metric_names,
+            qm_params=qm_params,
+            peak_sign=peak_sign,
+            seed=seed,
+            sparsity=sparsity,
+            skip_pc_metrics=skip_pc_metrics,
+        )
         qmc.run(verbose=verbose, **job_kwargs)
 
     metrics = qmc.get_data()
 
     return metrics
```

### Comparing `spikeinterface-0.97.1/spikeinterface/qualitymetrics/quality_metric_list.py` & `spikeinterface-0.98.0/src/spikeinterface/qualitymetrics/quality_metric_list.py`

 * *Files 23% similar despite different names*

```diff
@@ -6,37 +6,38 @@
     compute_presence_ratios,
     compute_snrs,
     compute_isi_violations,
     compute_refrac_period_violations,
     compute_sliding_rp_violations,
     compute_amplitude_cutoffs,
     compute_amplitude_medians,
-    compute_drift_metrics
+    compute_drift_metrics,
 )
 
 from .pca_metrics import (
     calculate_pc_metrics,
     mahalanobis_metrics,
     lda_metrics,
     nearest_neighbors_metrics,
     nearest_neighbors_isolation,
-    nearest_neighbors_noise_overlap
+    nearest_neighbors_noise_overlap,
+    silhouette_score,
+    simplified_silhouette_score,
 )
 
 from .pca_metrics import _possible_pc_metric_names
 
 
 # list of all available metrics and mapping to function
 # this list MUST NOT contain pca metrics, which are handled separately
 _misc_metric_name_to_func = {
-    "num_spikes" : compute_num_spikes,
-    "firing_rate" : compute_firing_rates,
-    "presence_ratio" : compute_presence_ratios,
-    "snr" : compute_snrs,
-    "isi_violation" : compute_isi_violations,
-    "rp_violation" : compute_refrac_period_violations,
-    "sliding_rp_violation" : compute_sliding_rp_violations,
-    "amplitude_cutoff" : compute_amplitude_cutoffs,
-    "amplitude_median" : compute_amplitude_medians,
-    "drift" : compute_drift_metrics
+    "num_spikes": compute_num_spikes,
+    "firing_rate": compute_firing_rates,
+    "presence_ratio": compute_presence_ratios,
+    "snr": compute_snrs,
+    "isi_violation": compute_isi_violations,
+    "rp_violation": compute_refrac_period_violations,
+    "sliding_rp_violation": compute_sliding_rp_violations,
+    "amplitude_cutoff": compute_amplitude_cutoffs,
+    "amplitude_median": compute_amplitude_medians,
+    "drift": compute_drift_metrics,
 }
-
```

### Comparing `spikeinterface-0.97.1/spikeinterface/qualitymetrics/utils.py` & `spikeinterface-0.98.0/src/spikeinterface/qualitymetrics/utils.py`

 * *Files 12% similar despite different names*

```diff
@@ -18,26 +18,26 @@
     numpy.ndarray
         PC scores for each point
     numpy.array
         Labels for each point
     """
     np.random.seed(0)
 
-    if len(np.array(center_locations).shape)==1:
-        distributions = [multivariate_normal.rvs(mean=[center, 0.0, 0.0],
-                                                 cov=[1.0, 1.0, 1.0],
-                                                 size=size)
-                        for center, size in zip(center_locations, total_points)]
+    if len(np.array(center_locations).shape) == 1:
+        distributions = [
+            multivariate_normal.rvs(mean=[center, 0.0, 0.0], cov=[1.0, 1.0, 1.0], size=size)
+            for center, size in zip(center_locations, total_points)
+        ]
         all_pcs = np.concatenate(distributions, axis=0)
 
     else:
-        all_pcs = np.empty((np.sum(total_points),3,center_locations.shape[0]))
+        all_pcs = np.empty((np.sum(total_points), 3, center_locations.shape[0]))
         for channel in range(center_locations.shape[0]):
-            distributions = [multivariate_normal.rvs(mean=[center, 0.0, 0.0],
-                                                         cov=[1.0, 1.0, 1.0],
-                                                         size=size)
-                        for center, size in zip(center_locations[channel], total_points)]
-            all_pcs[:,:,channel] = np.concatenate(distributions, axis=0)
+            distributions = [
+                multivariate_normal.rvs(mean=[center, 0.0, 0.0], cov=[1.0, 1.0, 1.0], size=size)
+                for center, size in zip(center_locations[channel], total_points)
+            ]
+            all_pcs[:, :, channel] = np.concatenate(distributions, axis=0)
 
-    all_labels = np.concatenate([np.ones((total_points[i],),dtype='int')*i  for i in range(len(total_points))])
+    all_labels = np.concatenate([np.ones((total_points[i],), dtype="int") * i for i in range(len(total_points))])
 
     return all_pcs, all_labels
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sorters/basesorter.py` & `spikeinterface-0.98.0/src/spikeinterface/sorters/basesorter.py`

 * *Files 4% similar despite different names*

```diff
@@ -23,40 +23,47 @@
 default_job_kwargs = {"n_jobs": -1}
 
 default_job_kwargs_description = {
     "n_jobs": "Number of jobs (when saving ti binary) - default -1 (all cores)",
     "chunk_size": "Number of samples per chunk (when saving ti binary) - default global",
     "chunk_memory": "Memory usage for each job (e.g. '100M', '1G') (when saving to binary) - default global",
     "total_memory": "Total memory usage (e.g. '500M', '2G') (when saving to binary) - default global",
-    "chunk_duration": "Chunk duration in s if float or with units if str (e.g. '1s', '500ms') (when saving to binary)" \
-                      " - default global",
-    "progress_bar": "If True, progress bar is shown (when saving to binary) - default global"}
+    "chunk_duration": "Chunk duration in s if float or with units if str (e.g. '1s', '500ms') (when saving to binary)"
+    " - default global",
+    "progress_bar": "If True, progress bar is shown (when saving to binary) - default global",
+}
 
 
 class BaseSorter:
     """Base Sorter object."""
 
     sorter_name = ""  # convenience for reporting
     compiled_name = None
     SortingExtractor_Class = None  # convenience to get the extractor
     requires_locations = False
-    gpu_capability = 'not-supported'
+    gpu_capability = "not-supported"
     requires_binary_data = False
-    compatible_with_parallel = {'loky': True, 'multiprocessing': True, 'threading': True}
-    
+    compatible_with_parallel = {"loky": True, "multiprocessing": True, "threading": True}
+
     _default_params = {}
     _params_description = {}
     sorter_description = ""
     installation_mesg = ""  # error message when not installed
 
     # by default no sorters handle multi segment
     handle_multi_segment = False
 
-    def __init__(self, recording=None, output_folder=None, verbose=False,
-                 remove_existing_folder=False, delete_output_folder=False, ):
+    def __init__(
+        self,
+        recording=None,
+        output_folder=None,
+        verbose=False,
+        remove_existing_folder=False,
+        delete_output_folder=False,
+    ):
         output_folder = self.initialize_folder(recording, output_folder, verbose, remove_existing_folder)
 
         self.recording = recording
         self.verbose = verbose
         self.delete_output_folder = delete_output_folder
         self.output_folder = output_folder
         self.sorter_folder = self.output_folder / "sorter_output" if self.output_folder is not None else None
@@ -88,57 +95,60 @@
 
     #############################################
 
     # class method zone
 
     @classmethod
     def initialize_folder(cls, recording, output_folder, verbose, remove_existing_folder):
-
         # installed ?
         if not cls.is_installed():
-            raise Exception(f"The sorter {cls.sorter_name} is not installed."
-                            f"Please install it with:  \n{cls.installation_mesg} ")
+            raise Exception(
+                f"The sorter {cls.sorter_name} is not installed." f"Please install it with:  \n{cls.installation_mesg} "
+            )
 
         if not isinstance(recording, BaseRecordingSnippets):
-            raise ValueError('recording must be a Recording or Snippets!!')
+            raise ValueError("recording must be a Recording or Snippets!!")
 
         if cls.requires_locations:
             locations = recording.get_channel_locations()
             if locations is None:
-                raise RuntimeError("Channel locations are required for this spike sorter. "
-                                   "Locations can be added to the RecordingExtractor by loading a probe file "
-                                   "(.prb or .csv) or by setting them manually.")
+                raise RuntimeError(
+                    "Channel locations are required for this spike sorter. "
+                    "Locations can be added to the RecordingExtractor by loading a probe file "
+                    "(.prb or .csv) or by setting them manually."
+                )
 
         if output_folder is None:
-            output_folder = cls.sorter_name + '_output'
+            output_folder = cls.sorter_name + "_output"
 
         #  .absolute() not anymore
         output_folder = Path(output_folder)
         sorter_output_folder = output_folder / "sorter_output"
 
         if output_folder.is_dir():
             if remove_existing_folder:
                 shutil.rmtree(str(output_folder))
             else:
-                raise ValueError(f'Folder {output_folder} already exists')
+                raise ValueError(f"Folder {output_folder} already exists")
 
         output_folder.mkdir(parents=True, exist_ok=True)
         sorter_output_folder.mkdir()
 
         if recording.get_num_segments() > 1:
             if not cls.handle_multi_segment:
                 raise ValueError(
-                    f'This sorter {cls.sorter_name} do not handle multi segment, use si.concatenate_recordings(...)')
+                    f"This sorter {cls.sorter_name} do not handle multi segment, use si.concatenate_recordings(...)"
+                )
 
-        rec_file = output_folder / 'spikeinterface_recording.json'
-        if recording.is_dumpable:
-            recording.dump_to_json(rec_file)
+        rec_file = output_folder / "spikeinterface_recording.json"
+        if recording.check_if_json_serializable():
+            recording.dump_to_json(rec_file, relative_to=output_folder)
         else:
-            d = {'warning': 'The recording is not dumpable'}
-            rec_file.write_text(json.dumps(d, indent=4), encoding='utf8')
+            d = {"warning": "The recording is not rerializable to json"}
+            rec_file.write_text(json.dumps(d, indent=4), encoding="utf8")
 
         return output_folder
 
     @classmethod
     def default_params(cls):
         p = copy.deepcopy(cls._default_params)
         if cls.requires_binary_data:
@@ -159,15 +169,15 @@
 
         # verify params are in list
         bad_params = []
         for p in new_params.keys():
             if p not in params.keys():
                 bad_params.append(p)
         if len(bad_params) > 0:
-            raise AttributeError('Bad parameters: ' + str(bad_params))
+            raise AttributeError("Bad parameters: " + str(bad_params))
 
         params.update(new_params)
 
         # custom check params
         params = cls._check_params(recording, output_folder, params)
         # common check : filter warning
         if recording.is_filtered and cls._check_apply_filter_in_params(params) and verbose:
@@ -176,131 +186,132 @@
         # dump parameters inside the folder with json
         cls._dump_params(recording, output_folder, params, verbose)
 
         return params
 
     @classmethod
     def _dump_params(cls, recording, output_folder, sorter_params, verbose):
-        with (output_folder / 'spikeinterface_params.json').open(mode='w', encoding='utf8') as f:
+        with (output_folder / "spikeinterface_params.json").open(mode="w", encoding="utf8") as f:
             all_params = dict()
-            all_params['sorter_name'] = cls.sorter_name
-            all_params['sorter_params'] = sorter_params
+            all_params["sorter_name"] = cls.sorter_name
+            all_params["sorter_params"] = sorter_params
             json.dump(check_json(all_params), f, indent=4)
 
     @classmethod
     def setup_recording(cls, recording, output_folder, verbose):
         output_folder = Path(output_folder)
         sorter_output_folder = output_folder / "sorter_output"
-        with (output_folder / 'spikeinterface_params.json').open(mode='r', encoding='utf8') as f:
+        with (output_folder / "spikeinterface_params.json").open(mode="r", encoding="utf8") as f:
             all_params = json.load(f)
-            sorter_params = all_params['sorter_params']
+            sorter_params = all_params["sorter_params"]
         cls._setup_recording(recording, sorter_output_folder, sorter_params, verbose)
 
     @classmethod
     def run_from_folder(cls, output_folder, raise_error, verbose):
         # need setup_recording to be done.
         output_folder = Path(output_folder)
         sorter_output_folder = output_folder / "sorter_output"
 
         # retrieve sorter_name and params
-        with (output_folder / 'spikeinterface_params.json').open(mode='r') as f:
-              params = json.load(f)
-        sorter_params = params['sorter_params']
-        sorter_name = params['sorter_name']
+        with (output_folder / "spikeinterface_params.json").open(mode="r") as f:
+            params = json.load(f)
+        sorter_params = params["sorter_params"]
+        sorter_name = params["sorter_name"]
 
         from .sorterlist import sorter_dict
-        SorterClass = sorter_dict[sorter_name]
 
-        # not needed normally
-        #  recording = load_extractor(output_folder / 'spikeinterface_recording.json')
+        SorterClass = sorter_dict[sorter_name]
 
         now = datetime.datetime.now()
         log = {
-            'sorter_name': str(SorterClass.sorter_name),
-            'sorter_version': str(SorterClass.get_sorter_version()),
-            'datetime': now,
-            'runtime_trace': []
+            "sorter_name": str(SorterClass.sorter_name),
+            "sorter_version": str(SorterClass.get_sorter_version()),
+            "datetime": now,
+            "runtime_trace": [],
         }
         t0 = time.perf_counter()
 
         try:
             SorterClass._run_from_folder(sorter_output_folder, sorter_params, verbose)
             t1 = time.perf_counter()
             run_time = float(t1 - t0)
             has_error = False
         except Exception as err:
             has_error = True
             run_time = None
-            log['error'] = True
-            log['error_trace'] = traceback.format_exc()
+            log["error"] = True
+            log["error_trace"] = traceback.format_exc()
 
-        log['error'] = has_error
-        log['run_time'] = run_time
+        log["error"] = has_error
+        log["run_time"] = run_time
 
         # some sorter have a log file dur to shellscript launcher
-        runtime_trace_path = output_folder / f'{sorter_name}.log'
+        runtime_trace_path = sorter_output_folder / f"{sorter_name}.log"
         runtime_trace = []
         if runtime_trace_path.is_file():
-            with open(runtime_trace_path, 'r') as fp:
+            with open(runtime_trace_path, "r") as fp:
                 line = fp.readline()
                 while line:
                     runtime_trace.append(line.strip())
                     line = fp.readline()
-        log['runtime_trace'] = runtime_trace
+        log["runtime_trace"] = runtime_trace
 
         # dump to json
-        with (output_folder / 'spikeinterface_log.json').open('w', encoding='utf8') as f:
+        with (output_folder / "spikeinterface_log.json").open("w", encoding="utf8") as f:
             json.dump(check_json(log), f, indent=4)
 
         if verbose:
             if has_error:
-                print(f'Error running {sorter_name}')
+                print(f"Error running {sorter_name}")
             else:
-                print(f'{sorter_name} run time {run_time:0.2f}s')
+                print(f"{sorter_name} run time {run_time:0.2f}s")
 
         if has_error and raise_error:
-            print(log['error_trace'])
             raise SpikeSortingError(
-                f"Spike sorting failed. You can inspect the runtime trace in {output_folder}/spikeinterface_log.json")
+                f"Spike sorting error trace:\n{log['error_trace']}\n"
+                f"Spike sorting failed. You can inspect the runtime trace in {output_folder}/spikeinterface_log.json."
+            )
 
         return run_time
 
     @classmethod
     def get_result_from_folder(cls, output_folder):
         output_folder = Path(output_folder)
         sorter_output_folder = output_folder / "sorter_output"
         # check errors in log file
-        log_file = output_folder / 'spikeinterface_log.json'
+        log_file = output_folder / "spikeinterface_log.json"
         if not log_file.is_file():
-            raise SpikeSortingError('get result error: the folder do not contain spikeinterface_log.json')
+            raise SpikeSortingError("get result error: the folder does not contain the `spikeinterface_log.json` file")
 
-        with log_file.open('r', encoding='utf8') as f:
+        with log_file.open("r", encoding="utf8") as f:
             log = json.load(f)
 
-        if bool(log['error']):
+        if bool(log["error"]):
             raise SpikeSortingError(
-                "Spike sorting failed. You can inspect the runtime trace in spikeinterface_log.json")
+                f"Spike sorting error trace:\n{log['error_trace']}\n"
+                f"Spike sorting failed. You can inspect the runtime trace in {output_folder}/spikeinterface_log.json."
+            )
 
         if sorter_output_folder.is_dir():
             sorting = cls._get_result_from_folder(sorter_output_folder)
         else:
             # back-compatibility
             sorting = cls._get_result_from_folder(output_folder)
-        
+
         # register recording to Sorting object
-        recording = load_extractor(output_folder / 'spikeinterface_recording.json')
+        recording = load_extractor(output_folder / "spikeinterface_recording.json", base_folder=output_folder)
         if recording is not None:
             # can be None when not dumpable
             sorting.register_recording(recording)
         # set sorting info to Sorting object
-        with open(output_folder /'spikeinterface_recording.json', 'r') as f:
+        with open(output_folder / "spikeinterface_recording.json", "r") as f:
             rec_dict = json.load(f)
-        with open(output_folder /'spikeinterface_params.json', 'r') as f:
+        with open(output_folder / "spikeinterface_params.json", "r") as f:
             params_dict = json.load(f)
-        with open(output_folder /'spikeinterface_log.json', 'r') as f:
+        with open(output_folder / "spikeinterface_log.json", "r") as f:
             log_dict = json.load(f)
         sorting.set_sorting_info(rec_dict, params_dict, log_dict)
 
         return sorting
 
     @classmethod
     def check_compiled(cls):
@@ -311,32 +322,32 @@
         -------
         is_compiled: bool
             Boolean indicating if a bash command for cls.compiled_name exists or not
         """
         if cls.compiled_name is None:
             return False
 
-        shell_cmd = f'''
+        shell_cmd = f"""
         #!/bin/bash
         if ! [ -x "$(command -v {cls.compiled_name})" ]; then
             echo 'Error: {cls.compiled_name} is not installed.' >&2
             exit 1
         fi
-        '''
+        """
         shell_script = ShellScript(shell_cmd)
         shell_script.start()
         shell_script.wait()
         retcode = shell_script.wait()
         if retcode != 0:
             return False
         return True
-    
+
     @classmethod
     def use_gpu(cls, params):
-        return cls.gpu_capability != 'not-supported'
+        return cls.gpu_capability != "not-supported"
 
     #############################################
 
     # Zone to be implemented
     # by design all are implemented with class method.
     # No instance!!
     # So "self" is not available. Everything is folder based.
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sorters/external/combinato.py` & `spikeinterface-0.98.0/src/spikeinterface/sorters/external/combinato.py`

 * *Files 15% similar despite different names*

```diff
@@ -25,67 +25,66 @@
         return False
     assert isinstance(combinato_path, str)
 
     if combinato_path.startswith('"'):
         combinato_path = combinato_path[1:-1]
     combinato_path = str(Path(combinato_path).absolute())
 
-    if (Path(combinato_path) / 'css-extract').is_file():
+    if (Path(combinato_path) / "css-extract").is_file():
         return True
     else:
         return False
 
 
 class CombinatoSorter(BaseSorter):
     """Combinato Sorter object."""
 
-    sorter_name: str = 'combinato'
-    combinato_path: Union[str, None] = os.getenv('COMBINATO_PATH', None)
+    sorter_name: str = "combinato"
+    combinato_path: Union[str, None] = os.getenv("COMBINATO_PATH", None)
     requires_locations = False
     _default_params = {
-        'detect_sign': -1,  # -1 - 1 - 0
-        'MaxClustersPerTemp': 5,
-        'MinSpikesPerClusterMultiSelect': 15,
-        'RecursiveDepth': 1,
-        'ReclusterClusters': True,
-        'MinInputSizeRecluster': 2000,
-        'FirstMatchFactor': .75,
-        'SecondMatchFactor': 3,
-        'MaxDistMatchGrouping': 1.8,
-        'detect_threshold': 5,
-        'max_spike_duration': 0.0015,
-        'indices_per_spike': 64,
-        'index_maximum': 19,
-        'upsampling_factor': 3,
-        'denoise': True,
-        'do_filter': True,
-        'keep_good_only': True,
-        'chunk_memory': '500M'
+        "detect_sign": -1,  # -1 - 1 - 0
+        "MaxClustersPerTemp": 5,
+        "MinSpikesPerClusterMultiSelect": 15,
+        "RecursiveDepth": 1,
+        "ReclusterClusters": True,
+        "MinInputSizeRecluster": 2000,
+        "FirstMatchFactor": 0.75,
+        "SecondMatchFactor": 3,
+        "MaxDistMatchGrouping": 1.8,
+        "detect_threshold": 5,
+        "max_spike_duration": 0.0015,
+        "indices_per_spike": 64,
+        "index_maximum": 19,
+        "upsampling_factor": 3,
+        "denoise": True,
+        "do_filter": True,
+        "keep_good_only": True,
+        "chunk_memory": "500M",
     }
 
     _params_description = {
-        'detect_sign': "Use -1 (negative) or 1 (positive) depending "
-                       "on the sign of the spikes in the recording",
-        'MaxClustersPerTemp': 'How many clusters can be selected at one temperature',
-        'MinSpikesPerClusterMultiSelect': 'How many spikes does a cluster need to be selected',
-        'RecursiveDepth': 'How many clustering recursions should be run (1 do not recurse)',
-        'ReclusterClusters': 'Iteratively recluster big clusters?',
-        'MinInputSizeRecluster': 'How many spikes does a cluster need to be re-clustered',
-        'FirstMatchFactor': 'How close do spikes have to be in the first template matching step',
-        'SecondMatchFactor': 'How close do spikes have to be in the second template matching step',
-        'MaxDistMatchGrouping': 'At what cluster distance does grouping stop',
-        'detect_threshold': "Threshold for spike detection",
-        'max_spike_duration': 'max spike duration in seconds',
-        'indices_per_spike': 'samples per spikes',
-        'index_maximum': "Number of samples from the beginning of the spike waveform up to (not including) the peak",
-        'upsampling_factor': 'upsampling factor',
-        'denoise': 'Use denoise filter',
-        'do_filter': 'Use bandpass filter',
-        'keep_good_only': "If True only 'good' units are returned",
-        'chunk_memory': 'Chunk size in Mb to write h5 file (default 500Mb)'
+        "detect_sign": "Use -1 (negative) or 1 (positive) depending " "on the sign of the spikes in the recording",
+        "MaxClustersPerTemp": "How many clusters can be selected at one temperature",
+        "MinSpikesPerClusterMultiSelect": "How many spikes does a cluster need to be selected",
+        "RecursiveDepth": "How many clustering recursions should be run (1 do not recurse)",
+        "ReclusterClusters": "Iteratively recluster big clusters?",
+        "MinInputSizeRecluster": "How many spikes does a cluster need to be re-clustered",
+        "FirstMatchFactor": "How close do spikes have to be in the first template matching step",
+        "SecondMatchFactor": "How close do spikes have to be in the second template matching step",
+        "MaxDistMatchGrouping": "At what cluster distance does grouping stop",
+        "detect_threshold": "Threshold for spike detection",
+        "max_spike_duration": "max spike duration in seconds",
+        "indices_per_spike": "samples per spikes",
+        "index_maximum": "Number of samples from the beginning of the spike waveform up to (not including) the peak",
+        "upsampling_factor": "upsampling factor",
+        "denoise": "Use denoise filter",
+        "do_filter": "Use bandpass filter",
+        "keep_good_only": "If True only 'good' units are returned",
+        "chunk_memory": "Chunk size in Mb to write h5 file (default 500Mb)",
     }
 
     sorter_description = """Combinato is a complete data-analysis framework for spike sorting in noisy recordings
     lasting twelve hours or more. It combines a wavelet-based feature extraction and paramagnetic clustering with
     multiple stages of template-matching. includes software for artifact rejection, automatic spike sorting,
     manual optimization, and efficient visualization of results.
     For more information see https://doi:10.1371/journal.pone.0166598"""
@@ -105,93 +104,104 @@
 
     @classmethod
     def is_installed(cls):
         return check_if_installed(cls.combinato_path)
 
     @staticmethod
     def get_sorter_version():
-        return 'unknown'
+        return "unknown"
 
     @staticmethod
     def set_combinato_path(combinato_path: PathType):
         combinato_path = str(Path(combinato_path).absolute())
         CombinatoSorter.combinato_path = combinato_path
         try:
             print("Setting COMBINATO_PATH environment variable for subprocess calls to:", combinato_path)
             os.environ["COMBINATO_PATH"] = combinato_path
         except Exception as e:
             print("Could not set COMBINATO_PATH environment variable:", e)
 
     @classmethod
     def _check_apply_filter_in_params(cls, params):
-        return params['do_filter']
+        return params["do_filter"]
 
     @classmethod
     def _setup_recording(cls, recording, sorter_output_folder, params, verbose):
-        assert HAVE_H5PY, 'You must install h5py for combinato'
+        assert HAVE_H5PY, "You must install h5py for combinato"
         # Generate h5 files in the dataset directory
         chan_ids = recording.get_channel_ids()
-        assert len(chan_ids) == 1, 'combinato is a single-channel recording'
+        assert len(chan_ids) == 1, "combinato is a single-channel recording"
         chid = chan_ids[0]
-        vcFile_h5 = str(sorter_output_folder / ('recording.h5'))
-        with h5py.File(vcFile_h5, mode='w') as f:
-            f.create_dataset("sr", data=[recording.get_sampling_frequency()], dtype='float32')
-            write_to_h5_dataset_format(recording, dataset_path='/data', segment_index=0,
-                                       file_handle=f, time_axis=0, single_axis=True,
-                                       chunk_memory=params['chunk_memory'], return_scaled=True)
+        vcFile_h5 = str(sorter_output_folder / ("recording.h5"))
+        with h5py.File(vcFile_h5, mode="w") as f:
+            f.create_dataset("sr", data=[recording.get_sampling_frequency()], dtype="float32")
+            write_to_h5_dataset_format(
+                recording,
+                dataset_path="/data",
+                segment_index=0,
+                file_handle=f,
+                time_axis=0,
+                single_axis=True,
+                chunk_memory=params["chunk_memory"],
+                return_scaled=True,
+            )
 
     @classmethod
     def _run_from_folder(cls, sorter_output_folder, params, verbose):
-
         p = params.copy()
-        p['threshold_factor'] = p.pop('detect_threshold')
-        sign_thr = p.pop('detect_sign')
+        p["threshold_factor"] = p.pop("detect_threshold")
+        sign_thr = p.pop("detect_sign")
         if sign_thr == -1:
-            sign_thr = '--neg'
+            sign_thr = "--neg"
         elif sign_thr == 1:
-            sign_thr = ''
+            sign_thr = ""
 
         tmpdir = sorter_output_folder
 
         if verbose:
-            print(f'Running combinato in {tmpdir}...')
+            print(f"Running combinato in {tmpdir}...")
 
         with open(tmpdir / "local_options.py", "w") as outFile:
             outFile.writelines("options = {}".format(p))
 
-        shell_cmd = '''
+        shell_cmd = """
             {extra_cmd}
             cd "{tmpdir}"
             python {css_folder}/css-extract --h5 --files recording.h5
             python {css_folder}/css-simple-clustering {sign_thr} --datafile recording/data_recording.h5
-        '''
+        """
 
-        if 'win' in sys.platform and sys.platform != 'darwin':
+        if "win" in sys.platform and sys.platform != "darwin":
             extra_cmd = str(tmpdir)[:2]
-            shell_cmd = shell_cmd.replace('/', '\\')
+            shell_cmd = shell_cmd.replace("/", "\\")
         else:
-            extra_cmd = '#!/bin/bash'
+            extra_cmd = "#!/bin/bash"
 
         shell_cmd = shell_cmd.format(
             extra_cmd=extra_cmd,
             tmpdir=sorter_output_folder.absolute(),
             css_folder=CombinatoSorter.combinato_path,
-            sign_thr=sign_thr)
+            sign_thr=sign_thr,
+        )
 
-        shell_script = ShellScript(shell_cmd, script_path=sorter_output_folder / f'run_{cls.sorter_name}',
-                                   log_path=sorter_output_folder / f'{cls.sorter_name}.log', verbose=verbose)
+        shell_script = ShellScript(
+            shell_cmd,
+            script_path=sorter_output_folder / f"run_{cls.sorter_name}",
+            log_path=sorter_output_folder / f"{cls.sorter_name}.log",
+            verbose=verbose,
+        )
         shell_script.start()
         retcode = shell_script.wait()
 
         if retcode != 0:
-            raise Exception('combinato returned a non-zero exit code')
+            raise Exception("combinato returned a non-zero exit code")
 
     @classmethod
     def _get_result_from_folder(cls, sorter_output_folder):
         sorter_output_folder = Path(sorter_output_folder)
-        result_fname = str(sorter_output_folder / 'recording')
+        result_fname = str(sorter_output_folder / "recording")
 
-        with (sorter_output_folder.parent / 'spikeinterface_params.json').open('r') as f:
-            sorter_params = json.load(f)['sorter_params']
-        keep_good_only = sorter_params.get('keep_good_only', True)
+        with (sorter_output_folder.parent / "spikeinterface_params.json").open("r") as f:
+            sorter_params = json.load(f)["sorter_params"]
+        keep_good_only = sorter_params.get("keep_good_only", True)
         sorting = CombinatoSortingExtractor(folder_path=result_fname, keep_good_only=keep_good_only)
         return sorting
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sorters/external/hdsort.py` & `spikeinterface-0.98.0/src/spikeinterface/sorters/external/hdsort.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,15 +1,14 @@
 from pathlib import Path
 import os
 from typing import Union
 import shutil
 import sys
 
 import numpy as np
-import scipy.io
 
 from spikeinterface.core.core_tools import write_to_h5_dataset_format
 from ..basesorter import BaseSorter
 from ..utils import ShellScript
 
 # from spikeinterface.extractors import MaxOneRecordingExtractor
 from spikeinterface.extractors import HDSortSortingExtractor
@@ -21,60 +20,59 @@
     if hdsort_path is None:
         return False
     assert isinstance(hdsort_path, str)
 
     if hdsort_path.startswith('"'):
         hdsort_path = hdsort_path[1:-1]
     hdsort_path = str(Path(hdsort_path).absolute())
-    if (Path(hdsort_path) / '+hdsort').is_dir():
+    if (Path(hdsort_path) / "+hdsort").is_dir():
         return True
     else:
         return False
 
 
 class HDSortSorter(BaseSorter):
     """HDSort Sorter object."""
 
-    sorter_name: str = 'hdsort'
-    compiled_name: str = 'hdsort_compiled'
-    hdsort_path: Union[str, None] = os.getenv('HDSORT_PATH', None)
+    sorter_name: str = "hdsort"
+    compiled_name: str = "hdsort_compiled"
+    hdsort_path: Union[str, None] = os.getenv("HDSORT_PATH", None)
     requires_locations = False
     _default_params = {
-        'detect_threshold': 4.2,
-        'detect_sign': -1,  # -1 - 1
-        'filter': True,
-        'parfor': True,
-        'freq_min': 300,
-        'freq_max': 7000,
-        'max_el_per_group': 9,
-        'min_el_per_group': 1,
-        'add_if_nearer_than': 20,
-        'max_distance_within_group': 52,
-        'n_pc_dims': 6,
-        'chunk_size': 500000,
-        'loop_mode': 'local_parfor',
-        'chunk_memory': '500M'
+        "detect_threshold": 4.2,
+        "detect_sign": -1,  # -1 - 1
+        "filter": True,
+        "parfor": True,
+        "freq_min": 300,
+        "freq_max": 7000,
+        "max_el_per_group": 9,
+        "min_el_per_group": 1,
+        "add_if_nearer_than": 20,
+        "max_distance_within_group": 52,
+        "n_pc_dims": 6,
+        "chunk_size": 500000,
+        "loop_mode": "local_parfor",
+        "chunk_memory": "500M",
     }
 
     _params_description = {
-        'detect_threshold': "Threshold for spike detection",
-        'detect_sign': "Use -1 (negative) or 1 (positive) depending "
-                       "on the sign of the spikes in the recording",
-        'filter': "Enable or disable filter",
-        'parfor': "If True, the Matlab parfor is used",
-        'freq_min': "High-pass filter cutoff frequency",
-        'freq_max': "Low-pass filter cutoff frequency",
-        'max_el_per_group': "Maximum number of channels per electrode group",
-        'min_el_per_group': "Minimum number of channels per electrode group",
-        'add_if_nearer_than': "Minimum distance to add electrode to an electrode group",
-        'max_distance_within_group': "Maximum distance within an electrode group",
-        'n_pc_dims': "Number of principal components dimensions to perform initial clustering",
-        'chunk_size': "Chunk size in number of frames for template-matching",
-        'loop_mode': "Loop mode: 'loop', 'local_parfor', 'grid' (requires a grid architecture)",
-        'chunk_memory': "Chunk size in Mb for saving to binary format (default 500Mb)",
+        "detect_threshold": "Threshold for spike detection",
+        "detect_sign": "Use -1 (negative) or 1 (positive) depending " "on the sign of the spikes in the recording",
+        "filter": "Enable or disable filter",
+        "parfor": "If True, the Matlab parfor is used",
+        "freq_min": "High-pass filter cutoff frequency",
+        "freq_max": "Low-pass filter cutoff frequency",
+        "max_el_per_group": "Maximum number of channels per electrode group",
+        "min_el_per_group": "Minimum number of channels per electrode group",
+        "add_if_nearer_than": "Minimum distance to add electrode to an electrode group",
+        "max_distance_within_group": "Maximum distance within an electrode group",
+        "n_pc_dims": "Number of principal components dimensions to perform initial clustering",
+        "chunk_size": "Chunk size in number of frames for template-matching",
+        "loop_mode": "Loop mode: 'loop', 'local_parfor', 'grid' (requires a grid architecture)",
+        "chunk_memory": "Chunk size in Mb for saving to binary format (default 500Mb)",
     }
 
     sorter_description = """HDSort is a template-matching spike sorter designed for high density micro-electrode arrays.
     For more information see https://doi.org/10.1152/jn.00803.2017"""
 
     installation_mesg = """\nTo use HDSort run:\n
         >>> git clone https://git.bsse.ethz.ch/hima_public/HDsort.git
@@ -92,227 +90,224 @@
         if cls.check_compiled():
             return True
         return check_if_installed(cls.hdsort_path)
 
     @classmethod
     def get_sorter_version(cls):
         if cls.check_compiled():
-            return 'compiled'
-        p = os.getenv('HDSORT_PATH', None)
+            return "compiled"
+        p = os.getenv("HDSORT_PATH", None)
         if p is None:
-            return 'unknown'
+            return "unknown"
         else:
-            with open(str(Path(p) / 'version.txt'), mode='r', encoding='utf8') as f:
+            with open(str(Path(p) / "version.txt"), mode="r", encoding="utf8") as f:
                 version = f.readline()
         return version
 
     @staticmethod
     def set_hdsort_path(hdsort_path: PathType):
         HDSortSorter.hdsort_path = str(Path(hdsort_path).absolute())
         try:
             print("Setting HDSORT_PATH environment variable for subprocess calls to:", hdsort_path)
             os.environ["HDSORT_PATH"] = hdsort_path
         except Exception as e:
             print("Could not set HDSORT_PATH environment variable:", e)
 
     @classmethod
     def _check_apply_filter_in_params(cls, params):
-        return params['filter']
+        return params["filter"]
 
     @staticmethod
     def _generate_configs_file(sorter_output_folder, params, file_name, file_format):
         P = {}
 
         # preprocess
-        P['filter'] = 1.0 if params['filter'] else 0.0
-        P['parfor'] = True if params['parfor'] else False
-        P['hpf'] = float(params['freq_min'])
-        P['lpf'] = float(params['freq_max'])
+        P["filter"] = 1.0 if params["filter"] else 0.0
+        P["parfor"] = True if params["parfor"] else False
+        P["hpf"] = float(params["freq_min"])
+        P["lpf"] = float(params["freq_max"])
 
         # leg creationg
-        P['legs'] = {
-            'maxElPerGroup': float(params['max_el_per_group']),
-            'minElPerGroup': float(params['min_el_per_group']),
-            'addIfNearerThan': float(params['add_if_nearer_than']),  # always add direct neighbors
-            'maxDistanceWithinGroup': float(params['max_distance_within_group'])
+        P["legs"] = {
+            "maxElPerGroup": float(params["max_el_per_group"]),
+            "minElPerGroup": float(params["min_el_per_group"]),
+            "addIfNearerThan": float(params["add_if_nearer_than"]),  # always add direct neighbors
+            "maxDistanceWithinGroup": float(params["max_distance_within_group"]),
         }
 
         # spike detection
-        P['spikeDetection'] = {
-            'method': '-',
-            'thr': float(params['detect_threshold'])
-        }
-        P['artefactDetection'] = {'use': 0.0}
+        P["spikeDetection"] = {"method": "-", "thr": float(params["detect_threshold"])}
+        P["artefactDetection"] = {"use": 0.0}
 
         # pre-clustering
-        P['noiseEstimation'] = {'minDistFromSpikes': 80.0}
-        P['spikeAlignment'] = {
-            'initAlignment': '-',
-            'maxSpikes': 50000.0  # so many spikes will be clustered
-        }
-        P['featureExtraction'] = {'nDims': float(params['n_pc_dims'])}  # 6
-        P['clustering'] = {
-            'maxSpikes': 50000.0,  # dont align spikes you dont cluster..
-            'meanShiftBandWidthFactor': 1.8
+        P["noiseEstimation"] = {"minDistFromSpikes": 80.0}
+        P["spikeAlignment"] = {"initAlignment": "-", "maxSpikes": 50000.0}  # so many spikes will be clustered
+        P["featureExtraction"] = {"nDims": float(params["n_pc_dims"])}  # 6
+        P["clustering"] = {
+            "maxSpikes": 50000.0,  # dont align spikes you dont cluster..
+            "meanShiftBandWidthFactor": 1.8
             # 'meanShiftBandWidth': sqrt(1.8*6)  # todo: check this!
         }
 
         # template matching
-        P['botm'] = {
-            'run': 0.0,
-            'Tf': 75.0,
-            'cutLeft': 20.0
-        }
-        P['spikeCutting'] = {
-            'maxSpikes': 200000000000.0,  # Set this to basically inf
-            'blockwise': False
-        }
-        P['templateEstimation'] = {
-            'cutLeft': 10.0,
-            'Tf': 55.0,
-            'maxSpikes': 100.0
-        }
+        P["botm"] = {"run": 0.0, "Tf": 75.0, "cutLeft": 20.0}
+        P["spikeCutting"] = {"maxSpikes": 200000000000.0, "blockwise": False}  # Set this to basically inf
+        P["templateEstimation"] = {"cutLeft": 10.0, "Tf": 55.0, "maxSpikes": 100.0}
 
         # merging
-        P['mergeTemplates'] = {
-            'merge': 1.0,
-            'upsampleFactor': 3.0,
-            'atCorrelation': .93,  # DONT SET THIS TOO LOW! USE OTHER ELECTRODES ON FULL FOOTPRINT TO MERGE
-            'ifMaxRelDistSmallerPercent': 30.0
-
+        P["mergeTemplates"] = {
+            "merge": 1.0,
+            "upsampleFactor": 3.0,
+            "atCorrelation": 0.93,  # DONT SET THIS TOO LOW! USE OTHER ELECTRODES ON FULL FOOTPRINT TO MERGE
+            "ifMaxRelDistSmallerPercent": 30.0,
         }
 
         # configs
-        sort_name = 'hdsort_output'
+        sort_name = "hdsort_output"
         cfgs = {}
-        cfgs['rawFile'] = file_name
-        cfgs['sortingName'] = sort_name
-        cfgs['fileFormat'] = file_format
-        cfgs['chunkSize'] = float(params['chunk_size'])
-        cfgs['loopMode'] = params['loop_mode']
-
-        data = {
-            'P': P,
-            **cfgs
-        }
+        cfgs["rawFile"] = file_name
+        cfgs["sortingName"] = sort_name
+        cfgs["fileFormat"] = file_format
+        cfgs["chunkSize"] = float(params["chunk_size"])
+        cfgs["loopMode"] = params["loop_mode"]
+
+        data = {"P": P, **cfgs}
+        import scipy.io
 
-        scipy.io.savemat(str(sorter_output_folder / 'configsParams.mat'), data)
+        scipy.io.savemat(str(sorter_output_folder / "configsParams.mat"), data)
 
     @classmethod
     def _setup_recording(cls, recording, sorter_output_folder, params, verbose):
         #  if isinstance(recording, MaxOneRecordingExtractor):
         if False:  # TODO
             # ~ self.params['file_name'] = str(Path(recording._file_path).absolute())
             trace_file_name = str(Path(recording._file_path).absolute())
             # ~ self.params['file_format'] =  'maxone'
-            file_format = 'maxone'
+            file_format = "maxone"
             if verbose:
-                print('Using MaxOne format directly')
+                print("Using MaxOne format directly")
         else:
             # Generate three files dataset in Mea1k format
-            trace_file_name = cls.write_hdsort_input_format(recording,
-                                                            str(sorter_output_folder / 'recording.h5'),
-                                                            chunk_memory=params["chunk_memory"])
+            trace_file_name = cls.write_hdsort_input_format(
+                recording, str(sorter_output_folder / "recording.h5"), chunk_memory=params["chunk_memory"]
+            )
             # ~ self.params['file_format'] = 'mea1k'
-            file_format = 'mea1k'
+            file_format = "mea1k"
 
         cls._generate_configs_file(sorter_output_folder, params, trace_file_name, file_format)
 
         # store sample rate in a file
         samplerate = recording.get_sampling_frequency()
-        samplerate_fname = str(sorter_output_folder / 'samplerate.txt')
-        with open(samplerate_fname, 'w') as f:
-            f.write('{}'.format(samplerate))
+        samplerate_fname = str(sorter_output_folder / "samplerate.txt")
+        with open(samplerate_fname, "w") as f:
+            f.write("{}".format(samplerate))
 
         source_dir = Path(Path(__file__).parent)
-        shutil.copy(str(source_dir / 'hdsort_master.m'), str(sorter_output_folder))
+        shutil.copy(str(source_dir / "hdsort_master.m"), str(sorter_output_folder))
 
     @classmethod
     def _run_from_folder(cls, sorter_output_folder, params, verbose):
         if cls.check_compiled():
-            shell_cmd = f'''
+            shell_cmd = f"""
                 #!/bin/bash
                 {cls.compiled_name} {sorter_output_folder}
-            '''
+            """
         else:
             sorter_output_folder = sorter_output_folder.absolute()
             hdsort_path = Path(cls.hdsort_path).absolute()
 
-            if "win" in sys.platform and sys.platform != 'darwin':
+            if "win" in sys.platform and sys.platform != "darwin":
                 disk_move = str(sorter_output_folder)[:2]
-                shell_cmd = f'''
+                shell_cmd = f"""
                             {disk_move}
                             cd {sorter_output_folder}
                             matlab -nosplash -wait -r "{cls.sorter_name}_master('{sorter_output_folder}', '{hdsort_path}')"
-                        '''
+                        """
             else:
-                shell_cmd = f'''
+                shell_cmd = f"""
                             #!/bin/bash
                             cd "{sorter_output_folder}"
                             matlab -nosplash -nodisplay -r "{cls.sorter_name}_master('{sorter_output_folder}', '{hdsort_path}')"
-                        '''
-        shell_script = ShellScript(shell_cmd, script_path=sorter_output_folder / f'run_{cls.sorter_name}',
-                                   log_path=sorter_output_folder / f'{cls.sorter_name}.log', verbose=verbose)
+                        """
+        shell_script = ShellScript(
+            shell_cmd,
+            script_path=sorter_output_folder / f"run_{cls.sorter_name}",
+            log_path=sorter_output_folder / f"{cls.sorter_name}.log",
+            verbose=verbose,
+        )
         shell_script.start()
         retcode = shell_script.wait()
 
         if retcode != 0:
-            raise Exception('HDsort returned a non-zero exit code')
+            raise Exception("HDsort returned a non-zero exit code")
 
     @classmethod
     def _get_result_from_folder(cls, sorter_output_folder):
         sorter_output_folder = Path(sorter_output_folder)
-        sorting = HDSortSortingExtractor(file_path=str(sorter_output_folder / 'hdsort_output' /
-                                                       'hdsort_output_results.mat'))
+        sorting = HDSortSortingExtractor(
+            file_path=str(sorter_output_folder / "hdsort_output" / "hdsort_output_results.mat")
+        )
         return sorting
 
     @classmethod
-    def write_hdsort_input_format(cls, recording, save_path, chunk_memory='500M'):
+    def write_hdsort_input_format(cls, recording, save_path, chunk_memory="500M"):
         try:
             import h5py
         except:
             raise Exception("To use HDSort, install h5py: pip install h5py")
 
         # check if already in write format
         write_file = True
-        if hasattr(recording, '_file_path'):
-            if Path(recording._file_path).suffix in ['.h5', '.hdf5']:
-                with h5py.File(recording._file_path, 'r') as f:
+        if hasattr(recording, "_file_path"):
+            if Path(recording._file_path).suffix in [".h5", ".hdf5"]:
+                with h5py.File(recording._file_path, "r") as f:
                     keys = f.keys()
-                    if "version" in keys and "ephys" in keys and "mapping" in keys and "frame_rate" in keys \
-                            and "frame_numbers" in keys:
+                    if (
+                        "version" in keys
+                        and "ephys" in keys
+                        and "mapping" in keys
+                        and "frame_rate" in keys
+                        and "frame_numbers" in keys
+                    ):
                         if "sig" in f["ephys"].keys():
                             write_file = False
                             trace_file_name = str(Path(recording._file_path).absolute())
 
         if write_file:
             save_path = Path(save_path)
-            if save_path.suffix == '':
-                save_path = Path(str(save_path) + '.h5')
-            mapping_dtype = np.dtype([('electrode', np.int32), ('x', np.float64), ('y', np.float64),
-                                      ('channel', np.int32)])
+            if save_path.suffix == "":
+                save_path = Path(str(save_path) + ".h5")
+            mapping_dtype = np.dtype(
+                [("electrode", np.int32), ("x", np.float64), ("y", np.float64), ("channel", np.int32)]
+            )
 
-            locations = recording.get_property('location')
+            locations = recording.get_property("location")
             assert locations is not None, "'location' property is needed to run HDSort"
 
-            with h5py.File(save_path, 'w') as f:
-                f.create_group('ephys')
-                f.create_dataset('version', data=str(20161003))
-                ephys = f['ephys']
-                ephys.create_dataset('frame_rate', data=recording.get_sampling_frequency())
-                ephys.create_dataset('frame_numbers', data=np.arange(recording.get_num_frames(segment_index=0)))
+            with h5py.File(save_path, "w") as f:
+                f.create_group("ephys")
+                f.create_dataset("version", data=str(20161003))
+                ephys = f["ephys"]
+                ephys.create_dataset("frame_rate", data=recording.get_sampling_frequency())
+                ephys.create_dataset("frame_numbers", data=np.arange(recording.get_num_frames(segment_index=0)))
                 # save mapping
                 mapping = np.empty(recording.get_num_channels(), dtype=mapping_dtype)
                 x = locations[:, 0]
                 y = locations[:, 1]
                 # channel should be from 0 to num_channel - 1
                 for i, ch in enumerate(recording.get_channel_ids()):
                     mapping[i] = (ch, x[i], y[i], i)
-                ephys.create_dataset('mapping', data=mapping)
+                ephys.create_dataset("mapping", data=mapping)
                 # save traces
                 segment_index = 0
-                write_to_h5_dataset_format(recording, dataset_path='/ephys/signal', segment_index=0,
-                                           file_handle=f, time_axis=1, chunk_memory=chunk_memory)
+                write_to_h5_dataset_format(
+                    recording,
+                    dataset_path="/ephys/signal",
+                    segment_index=0,
+                    file_handle=f,
+                    time_axis=1,
+                    chunk_memory=chunk_memory,
+                )
 
             trace_file_name = str(save_path.absolute())
 
         return trace_file_name
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sorters/external/hdsort_master.m` & `spikeinterface-0.98.0/src/spikeinterface/sorters/external/hdsort_master.m`

 * *Files identical despite different names*

### Comparing `spikeinterface-0.97.1/spikeinterface/sorters/external/herdingspikes.py` & `spikeinterface-0.98.0/src/spikeinterface/sorters/external/herdingspikes.py`

 * *Files 18% similar despite different names*

```diff
@@ -8,110 +8,98 @@
 from spikeinterface.core import load_extractor
 from spikeinterface.extractors import HerdingspikesSortingExtractor
 
 
 class HerdingspikesSorter(BaseSorter):
     """HerdingSpikes Sorter object."""
 
-    sorter_name = 'herdingspikes'
+    sorter_name = "herdingspikes"
 
     requires_locations = True
-    compatible_with_parallel = {'loky': True, 'multiprocessing': True, 'threading': False}
+    compatible_with_parallel = {"loky": True, "multiprocessing": True, "threading": False}
     _default_params = {
         # core params
-        'clustering_bandwidth': 5.5,  # 5.0,
-        'clustering_alpha': 5.5,  # 5.0,
-        'clustering_n_jobs': -1,
-        'clustering_bin_seeding': True,
-        'clustering_min_bin_freq': 16,  # 10,
-        'clustering_subset': None,
-        'left_cutout_time': 0.3,  # 0.2,
-        'right_cutout_time': 1.8,  # 0.8,
-        'detect_threshold': 20,  # 24, #15,
-
+        "clustering_bandwidth": 5.5,  # 5.0,
+        "clustering_alpha": 5.5,  # 5.0,
+        "clustering_n_jobs": -1,
+        "clustering_bin_seeding": True,
+        "clustering_min_bin_freq": 16,  # 10,
+        "clustering_subset": None,
+        "left_cutout_time": 0.3,  # 0.2,
+        "right_cutout_time": 1.8,  # 0.8,
+        "detect_threshold": 20,  # 24, #15,
         # extra probe params
-        'probe_masked_channels': [],
-        'probe_inner_radius': 70,
-        'probe_neighbor_radius': 90,
-        'probe_event_length': 0.26,
-        'probe_peak_jitter': 0.2,
-
+        "probe_masked_channels": [],
+        "probe_inner_radius": 70,
+        "probe_neighbor_radius": 90,
+        "probe_event_length": 0.26,
+        "probe_peak_jitter": 0.2,
         # extra detection params
-        't_inc': 100000,
-        'num_com_centers': 1,
-        'maa': 12,
-        'ahpthr': 11,
-        'out_file_name': "HS2_detected",
-        'decay_filtering': False,
-        'save_all': False,
-        'amp_evaluation_time': 0.4,  # 0.14,
-        'spk_evaluation_time': 1.0,
-
+        "t_inc": 100000,
+        "num_com_centers": 1,
+        "maa": 12,
+        "ahpthr": 11,
+        "out_file_name": "HS2_detected",
+        "decay_filtering": False,
+        "save_all": False,
+        "amp_evaluation_time": 0.4,  # 0.14,
+        "spk_evaluation_time": 1.0,
         # extra pca params
-        'pca_ncomponents': 2,
-        'pca_whiten': True,
-
+        "pca_ncomponents": 2,
+        "pca_whiten": True,
         # bandpass filter
-        'freq_min': 300.0,
-        'freq_max': 6000.0,
-        'filter': True,
-
+        "freq_min": 300.0,
+        "freq_max": 6000.0,
+        "filter": True,
         # rescale traces
-        'pre_scale': True,
-        'pre_scale_value': 20.0,
-
+        "pre_scale": True,
+        "pre_scale_value": 20.0,
         # remove duplicates (based on spk_evaluation_time)
-        'filter_duplicates': True
+        "filter_duplicates": True,
     }
 
     _params_description = {
         # core params
-        'clustering_bandwidth': "Meanshift bandwidth, average spatial extent of spike clusters (um)",
-        'clustering_alpha': "Scalar for the waveform PC features when clustering.",
-        'clustering_n_jobs': "Number of cores to use for clustering.",
-        'clustering_bin_seeding': "Enable clustering bin seeding.",
-        'clustering_min_bin_freq': "Minimum spikes per bin for bin seeding.",
-        'clustering_subset': "Number of spikes used to build clusters. All by default.",
-        'left_cutout_time': "Cutout size before peak (ms).",
-        'right_cutout_time': "Cutout size after peak (ms).",
-        'detect_threshold': "Detection threshold",
-
+        "clustering_bandwidth": "Meanshift bandwidth, average spatial extent of spike clusters (um)",
+        "clustering_alpha": "Scalar for the waveform PC features when clustering.",
+        "clustering_n_jobs": "Number of cores to use for clustering.",
+        "clustering_bin_seeding": "Enable clustering bin seeding.",
+        "clustering_min_bin_freq": "Minimum spikes per bin for bin seeding.",
+        "clustering_subset": "Number of spikes used to build clusters. All by default.",
+        "left_cutout_time": "Cutout size before peak (ms).",
+        "right_cutout_time": "Cutout size after peak (ms).",
+        "detect_threshold": "Detection threshold",
         # extra probe params
-        'probe_masked_channels': "Masked channels",
-        'probe_inner_radius': "Radius of area around probe channel for localization",
-        'probe_neighbor_radius': "Radius of area around probe channel for neighbor classification.",
-        'probe_event_length': "Duration of a spike event (ms)",
-        'probe_peak_jitter': "Maximum peak misalignment for synchronous spike (ms)",
-
+        "probe_masked_channels": "Masked channels",
+        "probe_inner_radius": "Radius of area around probe channel for localization",
+        "probe_neighbor_radius": "Radius of area around probe channel for neighbor classification.",
+        "probe_event_length": "Duration of a spike event (ms)",
+        "probe_peak_jitter": "Maximum peak misalignment for synchronous spike (ms)",
         # extra detection params
-        't_inc': "Number of samples per chunk during detection.",
-        'num_com_centers': "Number of centroids to average when localizing.",
-        'maa': "Minimum summed spike amplitude for spike acceptance.",
-        'ahpthr': "Requires magnitude of spike rebound for acceptance",
-        'out_file_name': "File name for storage of unclustered detected spikes",
-        'decay_filtering': "Experimental: Set to True at your risk",
-        'save_all': "Save all working files after sorting (slow)",
-        'amp_evaluation_time': "Amplitude evaluation time (ms)",
-        'spk_evaluation_time': "Spike evaluation time (ms)",
-
+        "t_inc": "Number of samples per chunk during detection.",
+        "num_com_centers": "Number of centroids to average when localizing.",
+        "maa": "Minimum summed spike amplitude for spike acceptance.",
+        "ahpthr": "Requires magnitude of spike rebound for acceptance",
+        "out_file_name": "File name for storage of unclustered detected spikes",
+        "decay_filtering": "Experimental: Set to True at your risk",
+        "save_all": "Save all working files after sorting (slow)",
+        "amp_evaluation_time": "Amplitude evaluation time (ms)",
+        "spk_evaluation_time": "Spike evaluation time (ms)",
         # extra pca params
-        'pca_ncomponents': "Number of principal components to use when clustering",
-        'pca_whiten': "If true, whiten data for pca",
-
+        "pca_ncomponents": "Number of principal components to use when clustering",
+        "pca_whiten": "If true, whiten data for pca",
         # bandpass filter
-        'freq_min': "High-pass filter cutoff frequency",
-        'freq_max': "Low-pass filter cutoff frequency",
-        'filter': "Enable or disable filter",
-
+        "freq_min": "High-pass filter cutoff frequency",
+        "freq_max": "Low-pass filter cutoff frequency",
+        "filter": "Enable or disable filter",
         # rescale traces
-        'pre_scale': "Scales recording traces to optimize HerdingSpikes performance",
-        'pre_scale_value': "Scale to apply in case of pre-scaling of traces",
-
+        "pre_scale": "Scales recording traces to optimize HerdingSpikes performance",
+        "pre_scale_value": "Scale to apply in case of pre-scaling of traces",
         # remove duplicates (based on spk_evaluation_time)
-        'filter_duplicates': "Remove spike duplicates (based on spk_evaluation_time)"
+        "filter_duplicates": "Remove spike duplicates (based on spk_evaluation_time)",
     }
 
     sorter_description = """Herding Spikes is a density-based spike sorter designed for high-density retinal recordings.
     It uses both PCA features and an estimate of the spike location to cluster different units.
     For more information see https://doi.org/10.1016/j.jneumeth.2016.06.006"""
 
     installation_mesg = """\nTo use HerdingSpikes run:\n
@@ -122,27 +110,29 @@
 
     handle_multi_segment = False
 
     @classmethod
     def is_installed(cls):
         try:
             import herdingspikes as hs
+
             HAVE_HS = True
         except ImportError:
             HAVE_HS = False
         return HAVE_HS
 
     @classmethod
     def get_sorter_version(cls):
         import herdingspikes as hs
+
         return hs.__version__
 
     @classmethod
     def _check_apply_filter_in_params(cls, params):
-        return params['filter']
+        return params["filter"]
 
     @classmethod
     def _setup_recording(cls, recording, sorter_output_folder, params, verbose):
         # nothing to copy inside the folder : Herdingspikes used natively spikeinterface
         pass
 
     @classmethod
@@ -153,86 +143,90 @@
         hs_version = version.parse(hs.__version__)
 
         if hs_version >= version.parse("0.3.99"):
             new_api = True
         else:
             new_api = False
 
-        recording = load_extractor(sorter_output_folder.parent / 'spikeinterface_recording.json')
+        recording = load_extractor(
+            sorter_output_folder.parent / "spikeinterface_recording.json", base_folder=sorter_output_folder.parent
+        )
 
         p = params
 
         # Bandpass filter
-        if p['filter'] and p['freq_min'] is not None and p['freq_max'] is not None:
-            recording = bandpass_filter(
-                recording=recording, freq_min=p['freq_min'], freq_max=p['freq_max'])
+        if p["filter"] and p["freq_min"] is not None and p["freq_max"] is not None:
+            recording = bandpass_filter(recording=recording, freq_min=p["freq_min"], freq_max=p["freq_max"])
 
-        if p['pre_scale']:
+        if p["pre_scale"]:
             recording = normalize_by_quantile(
-                recording=recording, scale=p['pre_scale_value'],
-                median=0.0, q1=0.05, q2=0.95
+                recording=recording, scale=p["pre_scale_value"], median=0.0, q1=0.05, q2=0.95
             )
 
         if new_api:
             recording_to_hs = recording
         else:
-            print('herdingspikes version<0.3.99 uses the OLD spikeextractors with NewToOldRecording.\n'
-                  'Consider updating herdingspikes (pip install herdingspikes>=0.3.99)')
+            print(
+                "herdingspikes version<0.3.99 uses the OLD spikeextractors with NewToOldRecording.\n"
+                "Consider updating herdingspikes (pip install herdingspikes>=0.3.99)"
+            )
             recording_to_hs = NewToOldRecording(recording)
 
         # this should have its name changed
         Probe = hs.probe.RecordingExtractor(
             recording_to_hs,
-            masked_channels=p['probe_masked_channels'],
-            inner_radius=p['probe_inner_radius'],
-            neighbor_radius=p['probe_neighbor_radius'],
-            event_length=p['probe_event_length'],
-            peak_jitter=p['probe_peak_jitter'])
+            masked_channels=p["probe_masked_channels"],
+            inner_radius=p["probe_inner_radius"],
+            neighbor_radius=p["probe_neighbor_radius"],
+            event_length=p["probe_event_length"],
+            peak_jitter=p["probe_peak_jitter"],
+        )
 
         H = hs.HSDetection(
-            Probe, file_directory_name=str(sorter_output_folder),
-            left_cutout_time=p['left_cutout_time'],
-            right_cutout_time=p['right_cutout_time'],
-            threshold=p['detect_threshold'],
+            Probe,
+            file_directory_name=str(sorter_output_folder),
+            left_cutout_time=p["left_cutout_time"],
+            right_cutout_time=p["right_cutout_time"],
+            threshold=p["detect_threshold"],
             to_localize=True,
-            num_com_centers=p['num_com_centers'],
-            maa=p['maa'],
-            ahpthr=p['ahpthr'],
-            out_file_name=p['out_file_name'],
-            decay_filtering=p['decay_filtering'],
-            save_all=p['save_all'],
-            amp_evaluation_time=p['amp_evaluation_time'],
-            spk_evaluation_time=p['spk_evaluation_time']
+            num_com_centers=p["num_com_centers"],
+            maa=p["maa"],
+            ahpthr=p["ahpthr"],
+            out_file_name=p["out_file_name"],
+            decay_filtering=p["decay_filtering"],
+            save_all=p["save_all"],
+            amp_evaluation_time=p["amp_evaluation_time"],
+            spk_evaluation_time=p["spk_evaluation_time"],
         )
 
-        H.DetectFromRaw(load=True, tInc=int(p['t_inc']))
+        H.DetectFromRaw(load=True, tInc=int(p["t_inc"]))
 
-        sorted_file = str(sorter_output_folder / 'HS2_sorted.hdf5')
-        if (not H.spikes.empty):
+        sorted_file = str(sorter_output_folder / "HS2_sorted.hdf5")
+        if not H.spikes.empty:
             C = hs.HSClustering(H)
-            C.ShapePCA(pca_ncomponents=p['pca_ncomponents'],
-                       pca_whiten=p['pca_whiten'])
+            C.ShapePCA(pca_ncomponents=p["pca_ncomponents"], pca_whiten=p["pca_whiten"])
             C.CombinedClustering(
-                alpha=p['clustering_alpha'],
-                cluster_subset=p['clustering_subset'],
-                bandwidth=p['clustering_bandwidth'],
-                bin_seeding=p['clustering_bin_seeding'],
-                n_jobs=p['clustering_n_jobs'],
-                min_bin_freq=p['clustering_min_bin_freq']
+                alpha=p["clustering_alpha"],
+                cluster_subset=p["clustering_subset"],
+                bandwidth=p["clustering_bandwidth"],
+                bin_seeding=p["clustering_bin_seeding"],
+                n_jobs=p["clustering_n_jobs"],
+                min_bin_freq=p["clustering_min_bin_freq"],
             )
         else:
             C = hs.HSClustering(H)
 
-        if p['filter_duplicates']:
+        if p["filter_duplicates"]:
             uids = C.spikes.cl.unique()
             for u in uids:
-                s = C.spikes[C.spikes.cl == u].t.diff() < p['spk_evaluation_time'] / 1000 * Probe.fps
+                s = C.spikes[C.spikes.cl == u].t.diff() < p["spk_evaluation_time"] / 1000 * Probe.fps
                 C.spikes = C.spikes.drop(s.index[s])
 
         if verbose:
-            print('Saving to', sorted_file)
+            print("Saving to", sorted_file)
         C.SaveHDF5(sorted_file, sampling=Probe.fps)
 
     @classmethod
     def _get_result_from_folder(cls, sorter_output_folder):
-        return HerdingspikesSortingExtractor(file_path=Path(sorter_output_folder) / 'HS2_sorted.hdf5',
-                                             load_unit_info=True)
+        return HerdingspikesSortingExtractor(
+            file_path=Path(sorter_output_folder) / "HS2_sorted.hdf5", load_unit_info=True
+        )
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sorters/external/ironclust.py` & `spikeinterface-0.98.0/src/spikeinterface/sorters/external/ironclust.py`

 * *Files 24% similar despite different names*

```diff
@@ -16,100 +16,100 @@
         return False
     assert isinstance(ironclust_path, str)
 
     if ironclust_path.startswith('"'):
         ironclust_path = ironclust_path[1:-1]
     ironclust_path = str(Path(ironclust_path).absolute())
 
-    if (Path(ironclust_path) / 'matlab' / 'irc2.m').is_file():
+    if (Path(ironclust_path) / "matlab" / "irc2.m").is_file():
         return True
     else:
         return False
 
 
 class IronClustSorter(BaseSorter):
     """IronClust Sorter object."""
 
-    sorter_name: str = 'ironclust'
-    compiled_name: str = 'p_ironclust'
-    ironclust_path: Union[str, None] = os.getenv('IRONCLUST_PATH', None)
+    sorter_name: str = "ironclust"
+    compiled_name: str = "p_ironclust"
+    ironclust_path: Union[str, None] = os.getenv("IRONCLUST_PATH", None)
 
     requires_locations = True
-    gpu_capability = 'nvidia-optional'
+    gpu_capability = "nvidia-optional"
     requires_binary_data = True
 
     _default_params = {
-        'detect_sign': -1,  # Use -1, 0, or 1, depending on the sign of the spikes in the recording
-        'adjacency_radius': 50,  # Use -1 to include all channels in every neighborhood
-        'adjacency_radius_out': 100,  # Use -1 to include all channels in every neighborhood
-        'detect_threshold': 3.5,  # detection threshold
-        'prm_template_name': '',  # .prm template file name
-        'freq_min': 300,
-        'freq_max': 8000,
-        'merge_thresh': 0.985,  # Threshold for automated merging
-        'pc_per_chan': 9,  # Number of principal components per channel
-        'whiten': False,  # Whether to do channel whitening as part of preprocessing
-        'filter_type': 'bandpass',  # none, bandpass, wiener, fftdiff, ndiff
-        'filter_detect_type': 'none',  # none, bandpass, wiener, fftdiff, ndiff
-        'common_ref_type': 'trimmean',  # none, mean, median
-        'batch_sec_drift': 300,  # batch duration in seconds. clustering time duration
-        'step_sec_drift': 20,  # compute anatomical similarity every n sec
-        'knn': 30,  # K nearest neighbors
-        'min_count': 30,  # Minimum cluster size
-        'fGpu': True,  # Use GPU if available
-        'fft_thresh': 8,  # FFT-based noise peak threshold
-        'fft_thresh_low': 0,  # FFT-based noise peak lower threshold (set to 0 to disable dual thresholding scheme)
-        'nSites_whiten': 16,  # Number of adjacent channels to whiten
-        'feature_type': 'gpca',  # gpca, pca, vpp, vmin, vminmax, cov, energy, xcov
-        'delta_cut': 1,  # Cluster detection threshold (delta-cutoff)
-        'post_merge_mode': 1,  # post merge mode
-        'sort_mode': 1,  # sort mode
-        'fParfor': False,  # parfor loop
-        'filter': True,  # Enable or disable filter
-        'clip_pre': 0.25,  # pre-peak clip duration in ms
-        'clip_post': 0.75,  # post-peak clip duration in ms
-        'merge_thresh_cc': 1,  # cross-correlogram merging threshold, set to 1 to disable
-        'nRepeat_merge': 3,  # number of repeats for merge
-        'merge_overlap_thresh': 0.95  # knn-overlap merge threshold
+        "detect_sign": -1,  # Use -1, 0, or 1, depending on the sign of the spikes in the recording
+        "adjacency_radius": 50,  # Use -1 to include all channels in every neighborhood
+        "adjacency_radius_out": 100,  # Use -1 to include all channels in every neighborhood
+        "detect_threshold": 3.5,  # detection threshold
+        "prm_template_name": "",  # .prm template file name
+        "freq_min": 300,
+        "freq_max": 8000,
+        "merge_thresh": 0.985,  # Threshold for automated merging
+        "pc_per_chan": 9,  # Number of principal components per channel
+        "whiten": False,  # Whether to do channel whitening as part of preprocessing
+        "filter_type": "bandpass",  # none, bandpass, wiener, fftdiff, ndiff
+        "filter_detect_type": "none",  # none, bandpass, wiener, fftdiff, ndiff
+        "common_ref_type": "trimmean",  # none, mean, median
+        "batch_sec_drift": 300,  # batch duration in seconds. clustering time duration
+        "step_sec_drift": 20,  # compute anatomical similarity every n sec
+        "knn": 30,  # K nearest neighbors
+        "min_count": 30,  # Minimum cluster size
+        "fGpu": True,  # Use GPU if available
+        "fft_thresh": 8,  # FFT-based noise peak threshold
+        "fft_thresh_low": 0,  # FFT-based noise peak lower threshold (set to 0 to disable dual thresholding scheme)
+        "nSites_whiten": 16,  # Number of adjacent channels to whiten
+        "feature_type": "gpca",  # gpca, pca, vpp, vmin, vminmax, cov, energy, xcov
+        "delta_cut": 1,  # Cluster detection threshold (delta-cutoff)
+        "post_merge_mode": 1,  # post merge mode
+        "sort_mode": 1,  # sort mode
+        "fParfor": False,  # parfor loop
+        "filter": True,  # Enable or disable filter
+        "clip_pre": 0.25,  # pre-peak clip duration in ms
+        "clip_post": 0.75,  # post-peak clip duration in ms
+        "merge_thresh_cc": 1,  # cross-correlogram merging threshold, set to 1 to disable
+        "nRepeat_merge": 3,  # number of repeats for merge
+        "merge_overlap_thresh": 0.95,  # knn-overlap merge threshold
     }
 
     _params_description = {
-        'detect_sign': "Use -1 (negative), 1 (positive) or 0 (both) depending "
-                       "on the sign of the spikes in the recording",
-        'adjacency_radius': "Use -1 to include all channels in every neighborhood",
-        'adjacency_radius_out': "Use -1 to include all channels in every neighborhood",
-        'detect_threshold': "detection threshold",
-        'prm_template_name': ".prm template file name",
-        'freq_min': "High-pass filter cutoff frequency",
-        'freq_max': "Low-pass filter cutoff frequency",
-        'merge_thresh': "Threshold for automated merging",
-        'pc_per_chan': "Number of principal components per channel",
-        'whiten': "Whether to do channel whitening as part of preprocessing",
-        'filter_type': "Filter type: none, bandpass, wiener, fftdiff, ndiff",
-        'filter_detect_type': "Filter type for detection: none, bandpass, wiener, fftdiff, ndiff",
-        'common_ref_type': "Common reference type: none, mean, median, trimmean",
-        'batch_sec_drift': "Batch duration in seconds. clustering time duration",
-        'step_sec_drift': "Compute anatomical similarity every n sec",
-        'knn': "K nearest neighbors",
-        'min_count': "Minimum cluster size",
-        'fGpu': "Use GPU if True",
-        'fft_thresh': "FFT-based noise peak threshold",
-        'fft_thresh_low': "FFT-based noise peak lower threshold (set to 0 to disable dual thresholding scheme)",
-        'nSites_whiten': "Number of adjacent channels to whiten",
-        'feature_type': "gpca, pca, vpp, vmin, vminmax, cov, energy, xcov",
-        'delta_cut': "Cluster detection threshold (delta-cutoff)",
-        'post_merge_mode': "Post merge mode",
-        'sort_mode': "Sort mode",
-        'fParfor': "Parfor loop",
-        'filter': "Enable or disable filter",
-        'clip_pre': "Pre-peak clip duration in ms",
-        'clip_post': "Post-peak clip duration in ms",
-        'merge_thresh_cc': "Cross-correlogram merging threshold, set to 1 to disable",
-        'nRepeat_merge': "Number of repeats for merge",
-        'merge_overlap_thresh': "Knn-overlap merge threshold",
+        "detect_sign": "Use -1 (negative), 1 (positive) or 0 (both) depending "
+        "on the sign of the spikes in the recording",
+        "adjacency_radius": "Use -1 to include all channels in every neighborhood",
+        "adjacency_radius_out": "Use -1 to include all channels in every neighborhood",
+        "detect_threshold": "detection threshold",
+        "prm_template_name": ".prm template file name",
+        "freq_min": "High-pass filter cutoff frequency",
+        "freq_max": "Low-pass filter cutoff frequency",
+        "merge_thresh": "Threshold for automated merging",
+        "pc_per_chan": "Number of principal components per channel",
+        "whiten": "Whether to do channel whitening as part of preprocessing",
+        "filter_type": "Filter type: none, bandpass, wiener, fftdiff, ndiff",
+        "filter_detect_type": "Filter type for detection: none, bandpass, wiener, fftdiff, ndiff",
+        "common_ref_type": "Common reference type: none, mean, median, trimmean",
+        "batch_sec_drift": "Batch duration in seconds. clustering time duration",
+        "step_sec_drift": "Compute anatomical similarity every n sec",
+        "knn": "K nearest neighbors",
+        "min_count": "Minimum cluster size",
+        "fGpu": "Use GPU if True",
+        "fft_thresh": "FFT-based noise peak threshold",
+        "fft_thresh_low": "FFT-based noise peak lower threshold (set to 0 to disable dual thresholding scheme)",
+        "nSites_whiten": "Number of adjacent channels to whiten",
+        "feature_type": "gpca, pca, vpp, vmin, vminmax, cov, energy, xcov",
+        "delta_cut": "Cluster detection threshold (delta-cutoff)",
+        "post_merge_mode": "Post merge mode",
+        "sort_mode": "Sort mode",
+        "fParfor": "Parfor loop",
+        "filter": "Enable or disable filter",
+        "clip_pre": "Pre-peak clip duration in ms",
+        "clip_post": "Post-peak clip duration in ms",
+        "merge_thresh_cc": "Cross-correlogram merging threshold, set to 1 to disable",
+        "nRepeat_merge": "Number of repeats for merge",
+        "merge_overlap_thresh": "Knn-overlap merge threshold",
     }
 
     sorter_descrpition = """Ironclust is a density-based spike sorter designed for high-density probes
     (e.g. Neuropixels). It uses features and spike location estimates for clustering, and it performs a drift
     correction. For more information see https://doi.org/10.1101/101030"""
 
     installation_mesg = """\nTo use IronClust run:\n
@@ -125,143 +125,158 @@
         if cls.check_compiled():
             return True
         return check_if_installed(cls.ironclust_path)
 
     @classmethod
     def get_sorter_version(cls):
         if cls.check_compiled():
-            return 'compiled'
-        version_filename = Path(os.environ["IRONCLUST_PATH"]) / 'matlab' / 'version.txt'
+            return "compiled"
+        version_filename = Path(os.environ["IRONCLUST_PATH"]) / "matlab" / "version.txt"
         if version_filename.is_file():
-            with open(str(version_filename), mode='r', encoding='utf8') as f:
+            with open(str(version_filename), mode="r", encoding="utf8") as f:
                 line = f.readline()
                 d = {}
                 exec(line, None, d)
-                version = d['version']
+                version = d["version"]
                 return version
-        return 'unknown'
-    
+        return "unknown"
+
     @classmethod
     def use_gpu(cls, params):
-        if 'fGpu' in params:
-            return params['fGpu']
-        return cls.default_params()['fGpu']
+        if "fGpu" in params:
+            return params["fGpu"]
+        return cls.default_params()["fGpu"]
 
     @staticmethod
     def set_ironclust_path(ironclust_path: PathType):
         ironclust_path = str(Path(ironclust_path).absolute())
         IronClustSorter.ironclust_path = ironclust_path
         try:
             print("Setting IRONCLUST_PATH environment variable for subprocess calls to:", ironclust_path)
             os.environ["IRONCLUST_PATH"] = ironclust_path
         except Exception as e:
             print("Could not set IRONCLUST_PATH environment variable:", e)
 
     @classmethod
     def _check_apply_filter_in_params(cls, params):
-        return params['filter']
+        return params["filter"]
 
     @classmethod
     def _setup_recording(cls, recording, sorter_output_folder, params, verbose):
         p = params
 
-        dataset_dir = sorter_output_folder / 'ironclust_dataset'
+        dataset_dir = sorter_output_folder / "ironclust_dataset"
         # Generate three files in the dataset directory: raw.mda, geom.csv, params.json
-        MdaRecordingExtractor.write_recording(recording=recording, save_path=str(dataset_dir), verbose=False,
-                                              **get_job_kwargs(params, verbose))
+        MdaRecordingExtractor.write_recording(
+            recording=recording, save_path=str(dataset_dir), verbose=False, **get_job_kwargs(params, verbose)
+        )
 
         samplerate = recording.get_sampling_frequency()
         num_channels = recording.get_num_channels()
         num_timepoints = recording.get_num_frames(segment_index=0)
         duration_minutes = num_timepoints / samplerate / 60
         if verbose:
-            print(f'channels = {num_channels}, timepoints = {num_timepoints}, duration = {duration_minutes} minutes')
+            print(f"channels = {num_channels}, timepoints = {num_timepoints}, duration = {duration_minutes} minutes")
 
         if verbose:
-            print('Creating argfile.txt..')
-        txt = ''
+            print("Creating argfile.txt..")
+        txt = ""
         for key0, val0 in params.items():
-            txt += '{}={}\n'.format(key0, val0)
-        txt += 'samplerate={}\n'.format(samplerate)
-        with (dataset_dir / 'argfile.txt').open('w') as f:
+            txt += "{}={}\n".format(key0, val0)
+        txt += "samplerate={}\n".format(samplerate)
+        with (dataset_dir / "argfile.txt").open("w") as f:
             f.write(txt)
 
         # TODO remove this because recording.json contain the sample_rate natively
-        tmpdir = sorter_output_folder / 'tmp'
+        tmpdir = sorter_output_folder / "tmp"
         tmpdir.mkdir(parents=True, exist_ok=True)
-        samplerate_fname = str(tmpdir / 'samplerate.txt')
-        with open(samplerate_fname, 'w') as f:
-            f.write('{}'.format(samplerate))
+        samplerate_fname = str(tmpdir / "samplerate.txt")
+        with open(samplerate_fname, "w") as f:
+            f.write("{}".format(samplerate))
 
     @classmethod
     def _run_from_folder(cls, sorter_output_folder, params, verbose):
-        dataset_dir = (sorter_output_folder / 'ironclust_dataset').absolute()
+        dataset_dir = (sorter_output_folder / "ironclust_dataset").absolute()
         source_dir = (Path(__file__).parent).absolute()
 
-        tmpdir = (sorter_output_folder / 'tmp').absolute()
+        tmpdir = (sorter_output_folder / "tmp").absolute()
 
         if verbose:
-            print('Running ironclust in {tmpdir}..'.format(tmpdir=str(tmpdir)))
+            print("Running ironclust in {tmpdir}..".format(tmpdir=str(tmpdir)))
 
         if cls.check_compiled():
-            shell_cmd = '''
+            shell_cmd = """
                 #!/bin/bash
                 p_ironclust {tmpdir} {dataset_dir}/raw.mda {dataset_dir}/geom.csv '' '' {tmpdir}/firings.mda {dataset_dir}/argfile.txt
-            '''.format(tmpdir=str(tmpdir), dataset_dir=str(dataset_dir))
+            """.format(
+                tmpdir=str(tmpdir), dataset_dir=str(dataset_dir)
+            )
         else:
-            cmd = '''
+            cmd = """
                 addpath('{source_dir}');
                 addpath('{ironclust_path}', '{ironclust_path}/matlab', '{ironclust_path}/matlab/mdaio');
                 try
                     p_ironclust('{tmpdir}', '{dataset_dir}/raw.mda', '{dataset_dir}/geom.csv', '', '', '{tmpdir}/firings.mda', '{dataset_dir}/argfile.txt');
                 catch
                     fprintf('----------------------------------------');
                     fprintf(lasterr());
                     quit(1);
                 end
                 quit(0);
-            '''
-            cmd = cmd.format(ironclust_path=IronClustSorter.ironclust_path, tmpdir=str(tmpdir),
-                             dataset_dir=str(dataset_dir), source_dir=str(source_dir))
+            """
+            cmd = cmd.format(
+                ironclust_path=IronClustSorter.ironclust_path,
+                tmpdir=str(tmpdir),
+                dataset_dir=str(dataset_dir),
+                source_dir=str(source_dir),
+            )
 
-            matlab_cmd = ShellScript(cmd, script_path=str(tmpdir / 'run_ironclust.m'))
+            matlab_cmd = ShellScript(cmd, script_path=str(tmpdir / "run_ironclust.m"))
             matlab_cmd.write()
 
-            if 'win' in sys.platform and sys.platform != 'darwin':
-                shell_cmd = '''
+            if "win" in sys.platform and sys.platform != "darwin":
+                shell_cmd = """
                     {disk_move}
                     cd {tmpdir}
                     matlab -nosplash -wait -log -r run_ironclust
-                '''.format(disk_move=str(tmpdir)[:2], tmpdir=tmpdir)
+                """.format(
+                    disk_move=str(tmpdir)[:2], tmpdir=tmpdir
+                )
             else:
-                shell_cmd = '''
+                shell_cmd = """
                     #!/bin/bash
                     cd "{tmpdir}"
                     matlab -nosplash -nodisplay -log -r run_ironclust
-                '''.format(tmpdir=tmpdir)
-
-        shell_script = ShellScript(shell_cmd, script_path=sorter_output_folder / f'run_{cls.sorter_name}',
-                                   log_path=sorter_output_folder / f'{cls.sorter_name}.log', verbose=verbose)
+                """.format(
+                    tmpdir=tmpdir
+                )
+
+        shell_script = ShellScript(
+            shell_cmd,
+            script_path=sorter_output_folder / f"run_{cls.sorter_name}",
+            log_path=sorter_output_folder / f"{cls.sorter_name}.log",
+            verbose=verbose,
+        )
         shell_script.start()
 
         retcode = shell_script.wait()
 
         if retcode != 0:
-            raise Exception('ironclust returned a non-zero exit code')
+            raise Exception("ironclust returned a non-zero exit code")
 
-        result_fname = tmpdir / 'firings.mda'
+        result_fname = tmpdir / "firings.mda"
         if not result_fname.is_file():
-            raise Exception(f'Result file does not exist: {result_fname}')
+            raise Exception(f"Result file does not exist: {result_fname}")
 
     @classmethod
     def _get_result_from_folder(cls, sorter_output_folder):
         sorter_output_folder = Path(sorter_output_folder)
-        tmpdir = sorter_output_folder / 'tmp'
+        tmpdir = sorter_output_folder / "tmp"
 
-        result_fname = str(tmpdir / 'firings.mda')
-        samplerate_fname = str(tmpdir / 'samplerate.txt')
-        with open(samplerate_fname, 'r') as f:
+        result_fname = str(tmpdir / "firings.mda")
+        samplerate_fname = str(tmpdir / "samplerate.txt")
+        with open(samplerate_fname, "r") as f:
             samplerate = float(f.read())
 
         sorting = MdaSortingExtractor(file_path=result_fname, sampling_frequency=samplerate)
 
         return sorting
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sorters/external/kilosort.py` & `spikeinterface-0.98.0/src/spikeinterface/sorters/external/kilosort.py`

 * *Files 14% similar despite different names*

```diff
@@ -13,51 +13,55 @@
         return False
     assert isinstance(kilosort_path, str)
 
     if kilosort_path.startswith('"'):
         kilosort_path = kilosort_path[1:-1]
     kilosort_path = str(Path(kilosort_path).absolute())
 
-    if (Path(kilosort_path) / 'preprocessData.m').is_file():
+    if (Path(kilosort_path) / "preprocessData.m").is_file():
         return True
     else:
         return False
 
 
 class KilosortSorter(KilosortBase, BaseSorter):
     """Kilosort Sorter object."""
 
-    sorter_name: str = 'kilosort'
-    compiled_name: str = 'ks_compiled'
-    kilosort_path: Union[str, None] = os.getenv('KILOSORT_PATH', None)
+    sorter_name: str = "kilosort"
+    compiled_name: str = "ks_compiled"
+    kilosort_path: Union[str, None] = os.getenv("KILOSORT_PATH", None)
     requires_locations = False
-    requires_gpu = 'nvidia-optional'
+    requires_gpu = "nvidia-optional"
 
     _default_params = {
-        'detect_threshold': 6,
-        'car': True,
-        'useGPU': True,
-        'freq_min': 300,
-        'freq_max': 6000,
-        'ntbuff': 64,
-        'Nfilt': None,
-        'NT': None,
-        'wave_length': 61,
+        "detect_threshold": 6,
+        "car": True,
+        "useGPU": True,
+        "freq_min": 300,
+        "freq_max": 6000,
+        "ntbuff": 64,
+        "Nfilt": None,
+        "NT": None,
+        "wave_length": 61,
+        "delete_tmp_files": True,
+        "delete_recording_dat": False,
     }
 
     _params_description = {
-        'detect_threshold': "Threshold for spike detection",
-        'car': "Enable or disable common reference",
-        'useGPU': "Enable or disable GPU usage",
-        'freq_min': "High-pass filter cutoff frequency",
-        'freq_max': "Low-pass filter cutoff frequency",
-        'ntbuff': "Samples of symmetrical buffer for whitening and spike detection",
-        'Nfilt': "Number of clusters to use (if None it is automatically computed)",
-        'NT': "Batch size (if None it is automatically computed)",
-        'wave_length': "size of the waveform extracted around each detected peak, (Default 61, maximum 81)",
+        "detect_threshold": "Threshold for spike detection",
+        "car": "Enable or disable common reference",
+        "useGPU": "Enable or disable GPU usage",
+        "freq_min": "High-pass filter cutoff frequency",
+        "freq_max": "Low-pass filter cutoff frequency",
+        "ntbuff": "Samples of symmetrical buffer for whitening and spike detection",
+        "Nfilt": "Number of clusters to use (if None it is automatically computed)",
+        "NT": "Batch size (if None it is automatically computed)",
+        "wave_length": "size of the waveform extracted around each detected peak, (Default 61, maximum 81)",
+        "delete_tmp_files": "Whether to delete all temporary files after a successful run",
+        "delete_recording_dat": "Whether to delete the 'recording.dat' file after a successful run",
     }
 
     sorter_description = """Kilosort is a GPU-accelerated and efficient template-matching spike sorter.
     For more information see https://papers.nips.cc/paper/6326-fast-and-accurate-spike-sorting-of-high-channel-count-probes-with-kilosort"""
 
     installation_mesg = """\nTo use Kilosort run:\n
         >>> git clone https://github.com/cortex-lab/KiloSort
@@ -75,26 +79,26 @@
         if cls.check_compiled():
             return True
         return check_if_installed(cls.kilosort_path)
 
     @classmethod
     def get_sorter_version(cls):
         if cls.check_compiled():
-            return 'compiled'
-        commit = get_git_commit(os.getenv('KILOSORT_PATH', None))
+            return "compiled"
+        commit = get_git_commit(os.getenv("KILOSORT_PATH", None))
         if commit is None:
-            return 'unknown'
+            return "unknown"
         else:
-            return 'git-' + commit
+            return "git-" + commit
 
     @classmethod
     def use_gpu(cls, params):
-        if 'useGPU' in params:
-            return params['useGPU']
-        return cls.default_params()['useGPU']
+        if "useGPU" in params:
+            return params["useGPU"]
+        return cls.default_params()["useGPU"]
 
     @classmethod
     def set_kilosort_path(cls, kilosort_path: str):
         kilosort_path = str(Path(kilosort_path).absolute())
         KilosortSorter.kilosort_path = kilosort_path
         try:
             print("Setting KILOSORT_PATH environment variable for subprocess calls to:", kilosort_path)
@@ -102,28 +106,28 @@
         except Exception as e:
             print("Could not set KILOSORT_PATH environment variable:", e)
 
     @classmethod
     def _check_params(cls, recording, output_folder, params):
         p = params
         nchan = recording.get_num_channels()
-        if p['Nfilt'] is None:
-            p['Nfilt'] = (nchan // 32) * 32 * 8
+        if p["Nfilt"] is None:
+            p["Nfilt"] = (nchan // 32) * 32 * 8
         else:
-            p['Nfilt'] = p['Nfilt'] // 32 * 32
-        if p['Nfilt'] == 0:
-            p['Nfilt'] = nchan * 8
-        if p['NT'] is None:
-            p['NT'] = 64 * 1024 + p['ntbuff']
+            p["Nfilt"] = p["Nfilt"] // 32 * 32
+        if p["Nfilt"] == 0:
+            p["Nfilt"] = nchan * 8
+        if p["NT"] is None:
+            p["NT"] = 64 * 1024 + p["ntbuff"]
         else:
-            p['NT'] = p['NT'] // 32 * 32  # make sure is multiple of 32
-        if p['wave_length'] % 2 != 1:
-            p['wave_length'] = p['wave_length'] + 1 # The wave_length must be odd
-        if p['wave_length'] > 81:
-            p['wave_length'] = 81 # The wave_length must be less than 81.
+            p["NT"] = p["NT"] // 32 * 32  # make sure is multiple of 32
+        if p["wave_length"] % 2 != 1:
+            p["wave_length"] = p["wave_length"] + 1  # The wave_length must be odd
+        if p["wave_length"] > 81:
+            p["wave_length"] = 81  # The wave_length must be less than 81.
         return p
 
     @classmethod
     def _get_specific_options(cls, ops, params):
         """
         Adds specific options for Kilosort in the ops dict and returns the final dict
 
@@ -137,63 +141,73 @@
         Returns
         ----------
         ops: dict
             Final ops data
         """
 
         # TODO: Check GPU option!
-        ops['GPU'] = params['useGPU']  # whether to run this code on an Nvidia GPU (much faster, mexGPUall first)
-        ops['parfor'] = 0.0  # whether to use parfor to accelerate some parts of the algorithm
-        ops['verbose'] = 1.0  # whether to print command line progress
-        ops['showfigures'] = 0.0  # whether to plot figures during optimization
-
-        ops['Nfilt'] = params['Nfilt']  # number of clusters to use (2-4 times more than Nchan, should be a multiple of 32)
-        ops['nNeighPC'] = min(12.0, ops['Nchan'])  # visualization only (Phy): number of channnels to mask the PCs, leave empty to skip (12)
-        ops['nNeigh'] = 16.0  # visualization only (Phy): number of neighboring templates to retain projections of (16)
+        ops["GPU"] = params["useGPU"]  # whether to run this code on an Nvidia GPU (much faster, mexGPUall first)
+        ops["parfor"] = 0.0  # whether to use parfor to accelerate some parts of the algorithm
+        ops["verbose"] = 1.0  # whether to print command line progress
+        ops["showfigures"] = 0.0  # whether to plot figures during optimization
+
+        ops["Nfilt"] = params[
+            "Nfilt"
+        ]  # number of clusters to use (2-4 times more than Nchan, should be a multiple of 32)
+        ops["nNeighPC"] = min(
+            12.0, ops["Nchan"]
+        )  # visualization only (Phy): number of channnels to mask the PCs, leave empty to skip (12)
+        ops["nNeigh"] = 16.0  # visualization only (Phy): number of neighboring templates to retain projections of (16)
 
         # options for channel whitening
-        ops['whitening'] = 'full'  # type of whitening (default 'full', for 'noSpikes' set options for spike detection below)
-        ops['nSkipCov'] = 1.0  # compute whitening matrix from every N-th batch (1)
-        ops['whiteningRange'] = 32.0  # how many channels to whiten together (Inf for whole probe whitening, should be fine if Nchan<=32)
+        ops[
+            "whitening"
+        ] = "full"  # type of whitening (default 'full', for 'noSpikes' set options for spike detection below)
+        ops["nSkipCov"] = 1.0  # compute whitening matrix from every N-th batch (1)
+        ops[
+            "whiteningRange"
+        ] = 32.0  # how many channels to whiten together (Inf for whole probe whitening, should be fine if Nchan<=32)
 
         # ops['criterionNoiseChannels'] = 0.2  # fraction of "noise" templates allowed to span all channel groups (see createChannelMapFile for more info).
 
         # other options for controlling the model and optimization
-        ops['Nrank'] = 3.0  # matrix rank of spike template model (3)
-        ops['nfullpasses'] = 6.0  # number of complete passes through data during optimization (6)
-        ops['maxFR'] = 20000  # maximum number of spikes to extract per batch (20000)
-        ops['fshigh'] = params['freq_min']  # frequency for high pass filtering
-        ops['fslow'] =  params['freq_max']  # frequency for low pass filtering (optional)
-        ops['ntbuff'] = params['ntbuff']  # samples of symmetrical buffer for whitening and spike detection
-        ops['scaleproc'] = 200.0  # int16 scaling of whitened data
-        ops['NT'] = params['NT']  # 32*1024+ ops.ntbuff;
+        ops["Nrank"] = 3.0  # matrix rank of spike template model (3)
+        ops["nfullpasses"] = 6.0  # number of complete passes through data during optimization (6)
+        ops["maxFR"] = 20000  # maximum number of spikes to extract per batch (20000)
+        ops["fshigh"] = params["freq_min"]  # frequency for high pass filtering
+        ops["fslow"] = params["freq_max"]  # frequency for low pass filtering (optional)
+        ops["ntbuff"] = params["ntbuff"]  # samples of symmetrical buffer for whitening and spike detection
+        ops["scaleproc"] = 200.0  # int16 scaling of whitened data
+        ops["NT"] = params["NT"]  # 32*1024+ ops.ntbuff;
         # this is the batch size (try decreasing if out of memory)
         # for GPU should be multiple of 32 + ntbuff
 
         # the following options can improve/deteriorate results.
         # when multiple values are provided for an option, the first two are beginning and ending anneal values,
         # the third is the value used in the final pass.
-        ops['Th'] = [4.0, 10.0, 10.0]  # threshold for detecting spikes on template-filtered data ([6 12 12])
-        ops['lam'] = [5.0, 5.0, 5.0]  # large means amplitudes are forced around the mean ([10 30 30])
-        ops['nannealpasses'] = 4.0  # should be less than nfullpasses (4)
-        ops['momentum'] = [1/20, 1/400]  # start with high momentum and anneal (1./[20 1000])
-        ops['shuffle_clusters'] = 1.0  # allow merges and splits during optimization (1)
-        ops['mergeT'] = 0.1  # upper threshold for merging (.1)
-        ops['splitT'] = 0.1  # lower threshold for splitting (.1)
-
-        ops['initialize'] = 'fromData'  # 'fromData' or 'no'
-        ops['spkTh'] = -params['detect_threshold']  # spike threshold in standard deviations (-6)
-        ops['loc_range'] = [3.0, 1.0]  # ranges to detect peaks; plus/minus in time and channel ([3 1])
-        ops['long_range'] = [30.0, 6.0]  # ranges to detect isolated peaks ([30 6])
-        ops['maskMaxChannels'] = 5.0  # how many channels to mask up/down ([5])
-        ops['crit'] = 0.65  # upper criterion for discarding spike repeates (0.65)
-        ops['nFiltMax'] = 10000.0  # maximum "unique" spikes to consider (10000)
+        ops["Th"] = [4.0, 10.0, 10.0]  # threshold for detecting spikes on template-filtered data ([6 12 12])
+        ops["lam"] = [5.0, 5.0, 5.0]  # large means amplitudes are forced around the mean ([10 30 30])
+        ops["nannealpasses"] = 4.0  # should be less than nfullpasses (4)
+        ops["momentum"] = [1 / 20, 1 / 400]  # start with high momentum and anneal (1./[20 1000])
+        ops["shuffle_clusters"] = 1.0  # allow merges and splits during optimization (1)
+        ops["mergeT"] = 0.1  # upper threshold for merging (.1)
+        ops["splitT"] = 0.1  # lower threshold for splitting (.1)
+
+        ops["initialize"] = "fromData"  # 'fromData' or 'no'
+        ops["spkTh"] = -params["detect_threshold"]  # spike threshold in standard deviations (-6)
+        ops["loc_range"] = [3.0, 1.0]  # ranges to detect peaks; plus/minus in time and channel ([3 1])
+        ops["long_range"] = [30.0, 6.0]  # ranges to detect isolated peaks ([30 6])
+        ops["maskMaxChannels"] = 5.0  # how many channels to mask up/down ([5])
+        ops["crit"] = 0.65  # upper criterion for discarding spike repeates (0.65)
+        ops["nFiltMax"] = 10000.0  # maximum "unique" spikes to consider (10000)
 
         # options for posthoc merges (under construction)
-        ops['fracse'] = 0.1  # binning step along discriminant axis for posthoc merges (in units of sd)
-        ops['epu'] = np.Inf
+        ops["fracse"] = 0.1  # binning step along discriminant axis for posthoc merges (in units of sd)
+        ops["epu"] = np.Inf
 
-        ops['ForceMaxRAMforDat'] = 20e9  # maximum RAM the algorithm will try to use; on Windows it will autodetect.
+        ops["ForceMaxRAMforDat"] = 20e9  # maximum RAM the algorithm will try to use; on Windows it will autodetect.
 
         ## option for wavelength
-        ops['nt0'] = params['wave_length'] # size of the waveform extracted around each detected peak. Be sure to make it odd to make alignment easier.
+        ops["nt0"] = params[
+            "wave_length"
+        ]  # size of the waveform extracted around each detected peak. Be sure to make it odd to make alignment easier.
         return ops
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sorters/external/kilosort2.py` & `spikeinterface-0.98.0/src/spikeinterface/sorters/external/kilosort2.py`

 * *Files 18% similar despite different names*

```diff
@@ -13,60 +13,70 @@
     if kilosort2_path is None:
         return False
     assert isinstance(kilosort2_path, str)
 
     if kilosort2_path.startswith('"'):
         kilosort2_path = kilosort2_path[1:-1]
     kilosort2_path = str(Path(kilosort2_path).absolute())
-    if (Path(kilosort2_path) / 'master_kilosort.m').is_file() or (Path(kilosort2_path) / 'main_kilosort.m').is_file():
+    if (Path(kilosort2_path) / "master_kilosort.m").is_file() or (Path(kilosort2_path) / "main_kilosort.m").is_file():
         return True
     else:
         return False
 
 
 class Kilosort2Sorter(KilosortBase, BaseSorter):
     """Kilosort2 Sorter object."""
 
-    sorter_name: str = 'kilosort2'
-    compiled_name: str = 'ks2_compiled'
-    kilosort2_path: Union[str, None] = os.getenv('KILOSORT2_PATH', None)
+    sorter_name: str = "kilosort2"
+    compiled_name: str = "ks2_compiled"
+    kilosort2_path: Union[str, None] = os.getenv("KILOSORT2_PATH", None)
     requires_locations = False
 
     _default_params = {
-        'detect_threshold': 6,
-        'projection_threshold': [10, 4],
-        'preclust_threshold': 8,
-        'car': True,
-        'minFR': 0.1,
-        'minfr_goodchannels': 0.1,
-        'freq_min': 150,
-        'sigmaMask': 30,
-        'nPCs': 3,
-        'ntbuff': 64,
-        'nfilt_factor': 4,
-        'NT': None,
-        'wave_length': 61,
-        'keep_good_only': False,
+        "detect_threshold": 6,
+        "projection_threshold": [10, 4],
+        "preclust_threshold": 8,
+        "car": True,
+        "minFR": 0.1,
+        "minfr_goodchannels": 0.1,
+        "freq_min": 150,
+        "sigmaMask": 30,
+        "nPCs": 3,
+        "ntbuff": 64,
+        "nfilt_factor": 4,
+        "NT": None,
+        "wave_length": 61,
+        "keep_good_only": False,
+        "skip_kilosort_preprocessing": False,
+        "scaleproc": None,
+        "save_rez_to_mat": False,
+        "delete_tmp_files": True,
+        "delete_recording_dat": False,
     }
 
     _params_description = {
-        'detect_threshold': "Threshold for spike detection",
-        'projection_threshold': "Threshold on projections",
-        'preclust_threshold': "Threshold crossings for pre-clustering (in PCA projection space)",
-        'car': "Enable or disable common reference",
-        'minFR': "Minimum spike rate (Hz), if a cluster falls below this for too long it gets removed",
-        'minfr_goodchannels': "Minimum firing rate on a 'good' channel",
-        'freq_min': "High-pass filter cutoff frequency",
-        'sigmaMask': "Spatial constant in um for computing residual variance of spike",
-        'nPCs': "Number of PCA dimensions",
-        'ntbuff': "Samples of symmetrical buffer for whitening and spike detection",
-        'nfilt_factor': "Max number of clusters per good channel (even temporary ones) 4",
-        'NT': "Batch size (if None it is automatically computed)",
-        'wave_length': "size of the waveform extracted around each detected peak, (Default 61, maximum 81)",
-        'keep_good_only': "If True only 'good' units are returned",
+        "detect_threshold": "Threshold for spike detection",
+        "projection_threshold": "Threshold on projections",
+        "preclust_threshold": "Threshold crossings for pre-clustering (in PCA projection space)",
+        "car": "Enable or disable common reference",
+        "minFR": "Minimum spike rate (Hz), if a cluster falls below this for too long it gets removed",
+        "minfr_goodchannels": "Minimum firing rate on a 'good' channel",
+        "freq_min": "High-pass filter cutoff frequency",
+        "sigmaMask": "Spatial constant in um for computing residual variance of spike",
+        "nPCs": "Number of PCA dimensions",
+        "ntbuff": "Samples of symmetrical buffer for whitening and spike detection",
+        "nfilt_factor": "Max number of clusters per good channel (even temporary ones) 4",
+        "NT": "Batch size (if None it is automatically computed)",
+        "wave_length": "size of the waveform extracted around each detected peak, (Default 61, maximum 81)",
+        "keep_good_only": "If True only 'good' units are returned",
+        "skip_kilosort_preprocessing": "Can optionaly skip the internal kilosort preprocessing",
+        "scaleproc": "int16 scaling of whitened data, if None set to 200.",
+        "save_rez_to_mat": "Save the full rez internal struc to mat file",
+        "delete_tmp_files": "Whether to delete all temporary files after a successful run",
+        "delete_recording_dat": "Whether to delete the 'recording.dat' file after a successful run",
     }
 
     sorter_description = """Kilosort2 is a GPU-accelerated and efficient template-matching spike sorter. On top of its
     predecessor Kilosort, it implements a drift-correction strategy.
     For more information see https://github.com/MouseLand/Kilosort2"""
 
     installation_mesg = """\nTo use Kilosort2 run:\n
@@ -85,42 +95,42 @@
         if cls.check_compiled():
             return True
         return check_if_installed(cls.kilosort2_path)
 
     @classmethod
     def get_sorter_version(cls):
         if cls.check_compiled():
-            return 'compiled'
-        commit = get_git_commit(os.getenv('KILOSORT2_PATH', None))
+            return "compiled"
+        commit = get_git_commit(os.getenv("KILOSORT2_PATH", None))
         if commit is None:
-            return 'unknown'
+            return "unknown"
         else:
-            return 'git-' + commit
+            return "git-" + commit
 
     @classmethod
     def set_kilosort2_path(cls, kilosort2_path: PathType):
         kilosort2_path = str(Path(kilosort2_path).absolute())
         Kilosort2Sorter.kilosort2_path = kilosort2_path
         try:
             print("Setting KILOSORT2_PATH environment variable for subprocess calls to:", kilosort2_path)
             os.environ["KILOSORT2_PATH"] = kilosort2_path
         except Exception as e:
             print("Could not set KILOSORT2_PATH environment variable:", e)
 
     @classmethod
     def _check_params(cls, recording, output_folder, params):
         p = params
-        if p['NT'] is None:
-            p['NT'] = 64 * 1024 + p['ntbuff']
+        if p["NT"] is None:
+            p["NT"] = 64 * 1024 + p["ntbuff"]
         else:
-            p['NT'] = p['NT'] // 32 * 32  # make sure is multiple of 32
-        if p['wave_length'] % 2 != 1:
-            p['wave_length'] = p['wave_length'] + 1 # The wave_length must be odd
-        if p['wave_length'] > 81:
-            p['wave_length'] = 81 # The wave_length must be less than 81.
+            p["NT"] = p["NT"] // 32 * 32  # make sure is multiple of 32
+        if p["wave_length"] % 2 != 1:
+            p["wave_length"] = p["wave_length"] + 1  # The wave_length must be odd
+        if p["wave_length"] > 81:
+            p["wave_length"] = 81  # The wave_length must be less than 81.
         return p
 
     @classmethod
     def _get_specific_options(cls, ops, params):
         """
         Adds specific options for Kilosort2 in the ops dict and returns the final dict
 
@@ -134,54 +144,71 @@
         Returns
         ----------
         ops: dict
             Final ops data
         """
 
         # frequency for high pass filtering (150)
-        ops['fshigh'] = params['freq_min']
+        ops["fshigh"] = params["freq_min"]
 
         # minimum firing rate on a "good" channel (0 to skip)
-        ops['minfr_goodchannels'] = params['minfr_goodchannels']
+        ops["minfr_goodchannels"] = params["minfr_goodchannels"]
 
-        projection_threshold = [float(pt) for pt in params['projection_threshold']]
+        projection_threshold = [float(pt) for pt in params["projection_threshold"]]
         # threshold on projections (like in Kilosort1, can be different for last pass like [10 4])
-        ops['Th'] = projection_threshold
+        ops["Th"] = projection_threshold
 
         # how important is the amplitude penalty (like in Kilosort1, 0 means not used, 10 is average, 50 is a lot)
-        ops['lam'] = 10.0
+        ops["lam"] = 10.0
 
         # splitting a cluster at the end requires at least this much isolation for each sub-cluster (max = 1)
-        ops['AUCsplit'] = 0.9
+        ops["AUCsplit"] = 0.9
 
         # minimum spike rate (Hz), if a cluster falls below this for too long it gets removed
-        ops['minFR'] = params['minFR']
+        ops["minFR"] = params["minFR"]
 
         # number of samples to average over (annealed from first to second value)
-        ops['momentum'] = [20.0, 400.0]
+        ops["momentum"] = [20.0, 400.0]
 
         # spatial constant in um for computing residual variance of spike
-        ops['sigmaMask'] = params['sigmaMask']
+        ops["sigmaMask"] = params["sigmaMask"]
 
         # threshold crossings for pre-clustering (in PCA projection space)
-        ops['ThPre'] = params['preclust_threshold']
+        ops["ThPre"] = params["preclust_threshold"]
 
         ## danger, changing these settings can lead to fatal errors
         # options for determining PCs
-        ops['spkTh'] = -params['detect_threshold']  # spike threshold in standard deviations (-6)
-        ops['reorder'] = 1.0  # whether to reorder batches for drift correction.
-        ops['nskip'] = 25.0  # how many batches to skip for determining spike PCs
+        ops["spkTh"] = -params["detect_threshold"]  # spike threshold in standard deviations (-6)
+        ops["reorder"] = 1.0  # whether to reorder batches for drift correction.
+        ops["nskip"] = 25.0  # how many batches to skip for determining spike PCs
 
-        ops['GPU'] = 1.0  # has to be 1, no CPU version yet, sorry
+        ops["GPU"] = 1.0  # has to be 1, no CPU version yet, sorry
         # ops['Nfilt'] = 1024 # max number of clusters
-        ops['nfilt_factor'] = params['nfilt_factor']  # max number of clusters per good channel (even temporary ones)
-        ops['ntbuff'] = params['ntbuff']  # samples of symmetrical buffer for whitening and spike detection
-        ops['NT'] = params['NT']  # must be multiple of 32 + ntbuff. This is the batch size (try decreasing if out of memory).
-        ops['whiteningRange'] = 32.0  # number of channels to use for whitening each channel
-        ops['nSkipCov'] = 25.0  # compute whitening matrix from every N-th batch
-        ops['scaleproc'] = 200.0  # int16 scaling of whitened data
-        ops['nPCs'] = params['nPCs']  # how many PCs to project the spikes into
-        ops['useRAM'] = 0.0  # not yet available
+        ops["nfilt_factor"] = params["nfilt_factor"]  # max number of clusters per good channel (even temporary ones)
+        ops["ntbuff"] = params["ntbuff"]  # samples of symmetrical buffer for whitening and spike detection
+        ops["NT"] = params[
+            "NT"
+        ]  # must be multiple of 32 + ntbuff. This is the batch size (try decreasing if out of memory).
+        ops["whiteningRange"] = 32.0  # number of channels to use for whitening each channel
+        ops["nSkipCov"] = 25.0  # compute whitening matrix from every N-th batch
+        ops["nPCs"] = params["nPCs"]  # how many PCs to project the spikes into
+        ops["useRAM"] = 0.0  # not yet available
 
         ## option for wavelength
-        ops['nt0'] = params['wave_length'] # size of the waveform extracted around each detected peak. Be sure to make it odd to make alignment easier.
+        ops["nt0"] = params[
+            "wave_length"
+        ]  # size of the waveform extracted around each detected peak. Be sure to make it odd to make alignment easier.
+
+        ops["skip_kilosort_preprocessing"] = params["skip_kilosort_preprocessing"]
+        if params["skip_kilosort_preprocessing"]:
+            ops["fproc"] = ops["fbinary"]
+            assert (
+                params["scaleproc"] is not None
+            ), "When skip_kilosort_preprocessing=True scaleproc must explicitly given"
+
+        # int16 scaling of whitened data, when None then scaleproc is set to 200.
+        scaleproc = params["scaleproc"]
+        ops["scaleproc"] = scaleproc if scaleproc is not None else 200.0
+
+        ops["save_rez_to_mat"] = params["save_rez_to_mat"]
+
         return ops
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sorters/external/kilosort2_5.py` & `spikeinterface-0.98.0/src/spikeinterface/sorters/external/kilosort2_5.py`

 * *Files 23% similar despite different names*

```diff
@@ -15,67 +15,78 @@
         return False
     assert isinstance(kilosort2_5_path, str)
 
     if kilosort2_5_path.startswith('"'):
         kilosort2_5_path = kilosort2_5_path[1:-1]
     kilosort2_5_path = str(Path(kilosort2_5_path).absolute())
 
-    if (Path(kilosort2_5_path) / 'master_kilosort.m').is_file() or (
-            Path(kilosort2_5_path) / 'main_kilosort.m').is_file():
+    if (Path(kilosort2_5_path) / "master_kilosort.m").is_file() or (
+        Path(kilosort2_5_path) / "main_kilosort.m"
+    ).is_file():
         return True
     else:
         return False
 
 
 class Kilosort2_5Sorter(KilosortBase, BaseSorter):
     """Kilosort2.5 Sorter object."""
 
-    sorter_name: str = 'kilosort2_5'
-    compiled_name: str = 'ks2_5_compiled'
-    kilosort2_5_path: Union[str, None] = os.getenv('KILOSORT2_5_PATH', None)
+    sorter_name: str = "kilosort2_5"
+    compiled_name: str = "ks2_5_compiled"
+    kilosort2_5_path: Union[str, None] = os.getenv("KILOSORT2_5_PATH", None)
     requires_locations = False
 
     _default_params = {
-        'detect_threshold': 6,
-        'projection_threshold': [10, 4],
-        'preclust_threshold': 8,
-        'car': True,
-        'minFR': 0.1,
-        'minfr_goodchannels': 0.1,
-        'nblocks': 5,
-        'sig': 20,
-        'freq_min': 150,
-        'sigmaMask': 30,
-        'nPCs': 3,
-        'ntbuff': 64,
-        'nfilt_factor': 4,
-        'NT': None,
-        'do_correction': True,
-        'wave_length': 61,
-        'keep_good_only': False,
+        "detect_threshold": 6,
+        "projection_threshold": [10, 4],
+        "preclust_threshold": 8,
+        "car": True,
+        "minFR": 0.1,
+        "minfr_goodchannels": 0.1,
+        "nblocks": 5,
+        "sig": 20,
+        "freq_min": 150,
+        "sigmaMask": 30,
+        "nPCs": 3,
+        "ntbuff": 64,
+        "nfilt_factor": 4,
+        "NT": None,
+        "do_correction": True,
+        "wave_length": 61,
+        "keep_good_only": False,
+        "skip_kilosort_preprocessing": False,
+        "scaleproc": None,
+        "save_rez_to_mat": False,
+        "delete_tmp_files": True,
+        "delete_recording_dat": False,
     }
 
     _params_description = {
-        'detect_threshold': "Threshold for spike detection",
-        'projection_threshold': "Threshold on projections",
-        'preclust_threshold': "Threshold crossings for pre-clustering (in PCA projection space)",
-        'car': "Enable or disable common reference",
-        'minFR': "Minimum spike rate (Hz), if a cluster falls below this for too long it gets removed",
-        'minfr_goodchannels': "Minimum firing rate on a 'good' channel",
-        'nblocks': "blocks for registration. 0 turns it off, 1 does rigid registration. Replaces 'datashift' option.",
-        'sig': "spatial smoothness constant for registration",
-        'freq_min': "High-pass filter cutoff frequency",
-        'sigmaMask': "Spatial constant in um for computing residual variance of spike",
-        'nPCs': "Number of PCA dimensions",
-        'ntbuff': "Samples of symmetrical buffer for whitening and spike detection",
-        'nfilt_factor': "Max number of clusters per good channel (even temporary ones) 4",
+        "detect_threshold": "Threshold for spike detection",
+        "projection_threshold": "Threshold on projections",
+        "preclust_threshold": "Threshold crossings for pre-clustering (in PCA projection space)",
+        "car": "Enable or disable common reference",
+        "minFR": "Minimum spike rate (Hz), if a cluster falls below this for too long it gets removed",
+        "minfr_goodchannels": "Minimum firing rate on a 'good' channel",
+        "nblocks": "blocks for registration. 0 turns it off, 1 does rigid registration. Replaces 'datashift' option.",
+        "sig": "spatial smoothness constant for registration",
+        "freq_min": "High-pass filter cutoff frequency",
+        "sigmaMask": "Spatial constant in um for computing residual variance of spike",
+        "nPCs": "Number of PCA dimensions",
+        "ntbuff": "Samples of symmetrical buffer for whitening and spike detection",
+        "nfilt_factor": "Max number of clusters per good channel (even temporary ones) 4",
         "do_correction": "If True drift registration is applied",
-        'NT': "Batch size (if None it is automatically computed)",
-        'keep_good_only': "If True only 'good' units are returned",
-        'wave_length': "size of the waveform extracted around each detected peak, (Default 61, maximum 81)",
+        "NT": "Batch size (if None it is automatically computed)",
+        "keep_good_only": "If True only 'good' units are returned",
+        "wave_length": "size of the waveform extracted around each detected peak, (Default 61, maximum 81)",
+        "skip_kilosort_preprocessing": "Can optionaly skip the internal kilosort preprocessing",
+        "scaleproc": "int16 scaling of whitened data, if None set to 200.",
+        "save_rez_to_mat": "Save the full rez internal struc to mat file",
+        "delete_tmp_files": "Whether to delete all temporary files after a successful run",
+        "delete_recording_dat": "Whether to delete the 'recording.dat' file after a successful run",
     }
 
     sorter_description = """Kilosort2_5 is a GPU-accelerated and efficient template-matching spike sorter. On top of its
     predecessor Kilosort, it implements a drift-correction strategy. Kilosort2.5 improves on Kilosort2 primarily in the
     type of drift correction we use. Where Kilosort2 modified templates as a function of time/drift (a drift tracking
     approach), Kilosort2.5 corrects the raw data directly via a sub-pixel registration process (a drift correction
     approach). Kilosort2.5 has not been as broadly tested as Kilosort2, but is expected to work out of the box on
@@ -100,42 +111,42 @@
         if cls.check_compiled():
             return True
         return check_if_installed(cls.kilosort2_5_path)
 
     @classmethod
     def get_sorter_version(cls):
         if cls.check_compiled():
-            return 'compiled'
-        commit = get_git_commit(os.getenv('KILOSORT2_5_PATH', None))
+            return "compiled"
+        commit = get_git_commit(os.getenv("KILOSORT2_5_PATH", None))
         if commit is None:
-            return 'unknown'
+            return "unknown"
         else:
-            return 'git-' + commit
+            return "git-" + commit
 
     @staticmethod
     def set_kilosort2_5_path(kilosort2_5_path: PathType):
         kilosort2_5_path = str(Path(kilosort2_5_path).absolute())
         Kilosort2_5Sorter.kilosort2_5_path = kilosort2_5_path
         try:
             print("Setting KILOSORT2_5_PATH environment variable for subprocess calls to:", kilosort2_5_path)
             os.environ["KILOSORT2_5_PATH"] = kilosort2_5_path
         except Exception as e:
             print("Could not set KILOSORT2_5_PATH environment variable:", e)
 
     @classmethod
     def _check_params(cls, recording, output_folder, params):
         p = params
-        if p['NT'] is None:
-            p['NT'] = 64 * 1024 + p['ntbuff']
+        if p["NT"] is None:
+            p["NT"] = 64 * 1024 + p["ntbuff"]
         else:
-            p['NT'] = p['NT'] // 32 * 32  # make sure is multiple of 32
-        if p['wave_length'] % 2 != 1:
-            p['wave_length'] = p['wave_length'] + 1 # The wave_length must be odd
-        if p['wave_length'] > 81:
-            p['wave_length'] = 81 # The wave_length must be less than 81.
+            p["NT"] = p["NT"] // 32 * 32  # make sure is multiple of 32
+        if p["wave_length"] % 2 != 1:
+            p["wave_length"] = p["wave_length"] + 1  # The wave_length must be odd
+        if p["wave_length"] > 81:
+            p["wave_length"] = 81  # The wave_length must be less than 81.
         return p
 
     @classmethod
     def _get_specific_options(cls, ops, params):
         """
         Adds specific options for Kilosort2_5 in the ops dict and returns the final dict
 
@@ -148,63 +159,80 @@
 
         Returns
         ----------
         ops: dict
             Final ops data
         """
         # frequency for high pass filtering (300)
-        ops['fshigh'] = params['freq_min']
+        ops["fshigh"] = params["freq_min"]
 
         # minimum firing rate on a "good" channel (0 to skip)
-        ops['minfr_goodchannels'] = params['minfr_goodchannels']
+        ops["minfr_goodchannels"] = params["minfr_goodchannels"]
 
         # number of blocks to use for nonrigid registration
-        ops['nblocks'] = params['nblocks']
+        ops["nblocks"] = params["nblocks"]
 
         # spatial smoothness constant for registration
-        ops['sig'] = params['sig']
+        ops["sig"] = params["sig"]
 
-        projection_threshold = [float(pt) for pt in params['projection_threshold']]
+        projection_threshold = [float(pt) for pt in params["projection_threshold"]]
         # threshold on projections (like in Kilosort1, can be different for last pass like [10 4])
-        ops['Th'] = projection_threshold
+        ops["Th"] = projection_threshold
 
         # how important is the amplitude penalty (like in Kilosort1, 0 means not used, 10 is average, 50 is a lot)
-        ops['lam'] = 10.0
+        ops["lam"] = 10.0
 
         # splitting a cluster at the end requires at least this much isolation for each sub-cluster (max = 1)
-        ops['AUCsplit'] = 0.9
+        ops["AUCsplit"] = 0.9
 
         # minimum spike rate (Hz), if a cluster falls below this for too long it gets removed
-        ops['minFR'] = params['minFR']
+        ops["minFR"] = params["minFR"]
 
         # number of samples to average over (annealed from first to second value)
-        ops['momentum'] = [20.0, 400.0]
+        ops["momentum"] = [20.0, 400.0]
 
         # spatial constant in um for computing residual variance of spike
-        ops['sigmaMask'] = params['sigmaMask']
+        ops["sigmaMask"] = params["sigmaMask"]
 
         # threshold crossings for pre-clustering (in PCA projection space)
-        ops['ThPre'] = params['preclust_threshold']
+        ops["ThPre"] = params["preclust_threshold"]
 
         ## danger, changing these settings can lead to fatal errors
         # options for determining PCs
-        ops['spkTh'] = -params['detect_threshold']  # spike threshold in standard deviations (-6)
-        ops['reorder'] = 1.0  # whether to reorder batches for drift correction.
-        ops['nskip'] = 25.0  # how many batches to skip for determining spike PCs
+        ops["spkTh"] = -params["detect_threshold"]  # spike threshold in standard deviations (-6)
+        ops["reorder"] = 1.0  # whether to reorder batches for drift correction.
+        ops["nskip"] = 25.0  # how many batches to skip for determining spike PCs
 
-        ops['GPU'] = 1.0  # has to be 1, no CPU version yet, sorry
+        ops["GPU"] = 1.0  # has to be 1, no CPU version yet, sorry
         # ops['Nfilt'] = 1024 # max number of clusters
-        ops['nfilt_factor'] = params['nfilt_factor']  # max number of clusters per good channel (even temporary ones)
-        ops['ntbuff'] = params['ntbuff']  # samples of symmetrical buffer for whitening and spike detection
-        ops['NT'] = params['NT']  # must be multiple of 32 + ntbuff. This is the batch size (try decreasing if out of memory).
-        ops['whiteningRange'] = 32.0  # number of channels to use for whitening each channel
-        ops['nSkipCov'] = 25.0  # compute whitening matrix from every N-th batch
-        ops['scaleproc'] = 200.0  # int16 scaling of whitened data
-        ops['nPCs'] = params['nPCs']  # how many PCs to project the spikes into
-        ops['useRAM'] = 0.0  # not yet available
+        ops["nfilt_factor"] = params["nfilt_factor"]  # max number of clusters per good channel (even temporary ones)
+        ops["ntbuff"] = params["ntbuff"]  # samples of symmetrical buffer for whitening and spike detection
+        ops["NT"] = params[
+            "NT"
+        ]  # must be multiple of 32 + ntbuff. This is the batch size (try decreasing if out of memory).
+        ops["whiteningRange"] = 32.0  # number of channels to use for whitening each channel
+        ops["nSkipCov"] = 25.0  # compute whitening matrix from every N-th batch
+        ops["nPCs"] = params["nPCs"]  # how many PCs to project the spikes into
+        ops["useRAM"] = 0.0  # not yet available
 
         # drift correction
-        ops['do_correction'] = params['do_correction']
+        ops["do_correction"] = params["do_correction"]
 
         ## option for wavelength
-        ops['nt0'] = params['wave_length'] # size of the waveform extracted around each detected peak. Be sure to make it odd to make alignment easier.
+        ops["nt0"] = params[
+            "wave_length"
+        ]  # size of the waveform extracted around each detected peak. Be sure to make it odd to make alignment easier.
+
+        ops["skip_kilosort_preprocessing"] = params["skip_kilosort_preprocessing"]
+        if params["skip_kilosort_preprocessing"]:
+            ops["fproc"] = ops["fbinary"]
+            assert (
+                params["scaleproc"] is not None
+            ), "When skip_kilosort_preprocessing=True scaleproc must explicitly given"
+
+        # int16 scaling of whitened data, when None then scaleproc is set to 200.
+        scaleproc = params["scaleproc"]
+        ops["scaleproc"] = scaleproc if scaleproc is not None else 200.0
+
+        ops["save_rez_to_mat"] = params["save_rez_to_mat"]
+
         return ops
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sorters/external/kilosort3.py` & `spikeinterface-0.98.0/src/spikeinterface/sorters/external/kilosort3.py`

 * *Files 22% similar despite different names*

```diff
@@ -14,66 +14,76 @@
         return False
     assert isinstance(kilosort3_path, str)
 
     if kilosort3_path.startswith('"'):
         kilosort3_path = kilosort3_path[1:-1]
     kilosort3_path = str(Path(kilosort3_path).absolute())
 
-    if (Path(kilosort3_path) / 'main_kilosort3.m').is_file():
+    if (Path(kilosort3_path) / "main_kilosort3.m").is_file():
         return True
     else:
         return False
 
 
 class Kilosort3Sorter(KilosortBase, BaseSorter):
     """Kilosort3 Sorter object."""
 
-    sorter_name: str = 'kilosort3'
-    compiled_name: str = 'ks3_compiled'
-    kilosort3_path: Union[str, None] = os.getenv('KILOSORT3_PATH', None)
+    sorter_name: str = "kilosort3"
+    compiled_name: str = "ks3_compiled"
+    kilosort3_path: Union[str, None] = os.getenv("KILOSORT3_PATH", None)
     requires_locations = False
 
     _default_params = {
-        'detect_threshold': 6,
-        'projection_threshold': [9, 9],
-        'preclust_threshold': 8,
-        'car': True,
-        'minFR': 0.2,
-        'minfr_goodchannels': 0.2,
-        'nblocks': 5,
-        'sig': 20,
-        'freq_min': 300,
-        'sigmaMask': 30,
-        'nPCs': 3,
-        'ntbuff': 64,
-        'nfilt_factor': 4,
-        'do_correction': True,
-        'NT': None,
-        'wave_length': 61,
-        'keep_good_only': False,
+        "detect_threshold": 6,
+        "projection_threshold": [9, 9],
+        "preclust_threshold": 8,
+        "car": True,
+        "minFR": 0.2,
+        "minfr_goodchannels": 0.2,
+        "nblocks": 5,
+        "sig": 20,
+        "freq_min": 300,
+        "sigmaMask": 30,
+        "nPCs": 3,
+        "ntbuff": 64,
+        "nfilt_factor": 4,
+        "do_correction": True,
+        "NT": None,
+        "wave_length": 61,
+        "keep_good_only": False,
+        "skip_kilosort_preprocessing": False,
+        "scaleproc": None,
+        "save_rez_to_mat": False,
+        "delete_tmp_files": True,
+        "delete_recording_dat": False,
     }
 
     _params_description = {
-        'detect_threshold': "Threshold for spike detection",
-        'projection_threshold': "Threshold on projections",
-        'preclust_threshold': "Threshold crossings for pre-clustering (in PCA projection space)",
-        'car': "Enable or disable common reference",
-        'minFR': "Minimum spike rate (Hz), if a cluster falls below this for too long it gets removed",
-        'minfr_goodchannels': "Minimum firing rate on a 'good' channel",
-        'nblocks': "blocks for registration. 0 turns it off, 1 does rigid registration. Replaces 'datashift' option.",
-        'sig': "spatial smoothness constant for registration",
-        'freq_min': "High-pass filter cutoff frequency",
-        'sigmaMask': "Spatial constant in um for computing residual variance of spike",
-        'nPCs': "Number of PCA dimensions",
-        'ntbuff': "Samples of symmetrical buffer for whitening and spike detection",
-        'nfilt_factor': "Max number of clusters per good channel (even temporary ones) 4",
+        "detect_threshold": "Threshold for spike detection",
+        "projection_threshold": "Threshold on projections",
+        "preclust_threshold": "Threshold crossings for pre-clustering (in PCA projection space)",
+        "car": "Enable or disable common reference",
+        "minFR": "Minimum spike rate (Hz), if a cluster falls below this for too long it gets removed",
+        "minfr_goodchannels": "Minimum firing rate on a 'good' channel",
+        "nblocks": "blocks for registration. 0 turns it off, 1 does rigid registration. Replaces 'datashift' option.",
+        "sig": "spatial smoothness constant for registration",
+        "freq_min": "High-pass filter cutoff frequency",
+        "sigmaMask": "Spatial constant in um for computing residual variance of spike",
+        "nPCs": "Number of PCA dimensions",
+        "ntbuff": "Samples of symmetrical buffer for whitening and spike detection",
+        "nfilt_factor": "Max number of clusters per good channel (even temporary ones) 4",
         "do_correction": "If True drift registration is applied",
-        'NT': "Batch size (if None it is automatically computed)",
-        'wave_length': "size of the waveform extracted around each detected peak, (Default 61, maximum 81)",
-        'keep_good_only': "If True only 'good' units are returned",
+        "NT": "Batch size (if None it is automatically computed)",
+        "wave_length": "size of the waveform extracted around each detected peak, (Default 61, maximum 81)",
+        "keep_good_only": "If True only 'good' units are returned",
+        "skip_kilosort_preprocessing": "Can optionaly skip the internal kilosort preprocessing",
+        "scaleproc": "int16 scaling of whitened data, if None set to 200.",
+        "save_rez_to_mat": "Save the full rez internal struc to mat file",
+        "delete_tmp_files": "Whether to delete all temporary files after a successful run",
+        "delete_recording_dat": "Whether to delete the 'recording.dat' file after a successful run",
     }
 
     sorter_description = """Kilosort3 is a GPU-accelerated and efficient template-matching spike sorter. On top of its
     predecessor Kilosort, it implements a drift-correction strategy. Kilosort3 improves on Kilosort2 primarily in the
     type of drift correction we use. Where Kilosort2 modified templates as a function of time/drift (a drift tracking
     approach), Kilosort3 corrects the raw data directly via a sub-pixel registration process (a drift correction
     approach). Kilosort3 has not been as broadly tested as Kilosort2, but is expected to work out of the box on
@@ -98,42 +108,42 @@
         if cls.check_compiled():
             return True
         return check_if_installed(cls.kilosort3_path)
 
     @classmethod
     def get_sorter_version(cls):
         if cls.check_compiled():
-            return 'compiled'
-        commit = get_git_commit(os.getenv('KILOSORT3_PATH', None))
+            return "compiled"
+        commit = get_git_commit(os.getenv("KILOSORT3_PATH", None))
         if commit is None:
-            return 'unknown'
+            return "unknown"
         else:
-            return 'git-' + commit
+            return "git-" + commit
 
     @staticmethod
     def set_kilosort3_path(kilosort3_path: PathType):
         kilosort3_path = str(Path(kilosort3_path).absolute())
         Kilosort3Sorter.kilosort3_path = kilosort3_path
         try:
             print("Setting KILOSORT3_PATH environment variable for subprocess calls to:", kilosort3_path)
             os.environ["KILOSORT3_PATH"] = kilosort3_path
         except Exception as e:
             print("Could not set KILOSORT3_PATH environment variable:", e)
 
     @classmethod
     def _check_params(cls, recording, output_folder, params):
         p = params
-        if p['NT'] is None:
-            p['NT'] = 64 * 1024 + p['ntbuff']
+        if p["NT"] is None:
+            p["NT"] = 64 * 1024 + p["ntbuff"]
         else:
-            p['NT'] = p['NT'] // 32 * 32  # make sure is multiple of 32
-        if p['wave_length'] % 2 != 1:
-            p['wave_length'] = p['wave_length'] + 1 # The wave_length must be odd
-        if p['wave_length'] > 81:
-            p['wave_length'] = 81 # The wave_length must be less than 81.
+            p["NT"] = p["NT"] // 32 * 32  # make sure is multiple of 32
+        if p["wave_length"] % 2 != 1:
+            p["wave_length"] = p["wave_length"] + 1  # The wave_length must be odd
+        if p["wave_length"] > 81:
+            p["wave_length"] = 81  # The wave_length must be less than 81.
 
         return p
 
     @classmethod
     def _get_specific_options(cls, ops, params):
         """
         Adds specific options for Kilosort3 in the ops dict and returns the final dict
@@ -147,60 +157,77 @@
 
         Returns
         ----------
         ops: dict
             Final ops data
         """
         # frequency for high pass filtering (150)
-        ops['fshigh'] = params['freq_min']
+        ops["fshigh"] = params["freq_min"]
 
-        projection_threshold = [float(pt) for pt in params['projection_threshold']]
+        projection_threshold = [float(pt) for pt in params["projection_threshold"]]
         # threshold on projections (like in Kilosort1, can be different for last pass like [10 4])
-        ops['Th'] = projection_threshold
+        ops["Th"] = projection_threshold
 
         # how important is the amplitude penalty (like in Kilosort1, 0 means not used, 10 is average, 50 is a lot)
-        ops['lam'] = 20.0
+        ops["lam"] = 20.0
 
         # splitting a cluster at the end requires at least this much isolation for each sub-cluster (max = 1)
-        ops['AUCsplit'] = 0.8
+        ops["AUCsplit"] = 0.8
 
         # minimum firing rate on a "good" channel (0 to skip)
-        ops['minfr_goodchannels'] = params['minfr_goodchannels']
+        ops["minfr_goodchannels"] = params["minfr_goodchannels"]
 
         # minimum spike rate (Hz), if a cluster falls below this for too long it gets removed
-        ops['minFR'] = params['minFR']
+        ops["minFR"] = params["minFR"]
 
         # spatial constant in um for computing residual variance of spike
-        ops['sigmaMask'] = params['sigmaMask']
+        ops["sigmaMask"] = params["sigmaMask"]
 
         # threshold crossings for pre-clustering (in PCA projection space)
-        ops['ThPre'] = params['preclust_threshold']
+        ops["ThPre"] = params["preclust_threshold"]
 
         # spatial scale for datashift kernel
-        ops['sig'] = params['sig']
+        ops["sig"] = params["sig"]
 
         # type of data shifting (0 = none, 1 = rigid, 2 = nonrigid)
-        ops['nblocks'] = params['nblocks']
+        ops["nblocks"] = params["nblocks"]
 
         ## danger, changing these settings can lead to fatal errors
         # options for determining PCs
-        ops['spkTh'] = -params['detect_threshold']  # spike threshold in standard deviations (-6)
-        ops['reorder'] = 1.0  # whether to reorder batches for drift correction.
-        ops['nskip'] = 25.0  # how many batches to skip for determining spike PCs
+        ops["spkTh"] = -params["detect_threshold"]  # spike threshold in standard deviations (-6)
+        ops["reorder"] = 1.0  # whether to reorder batches for drift correction.
+        ops["nskip"] = 25.0  # how many batches to skip for determining spike PCs
 
-        ops['GPU'] = 1.0  # has to be 1, no CPU version yet, sorry
+        ops["GPU"] = 1.0  # has to be 1, no CPU version yet, sorry
         # ops['Nfilt'] = 1024 # max number of clusters
-        ops['nfilt_factor'] = params['nfilt_factor']  # max number of clusters per good channel (even temporary ones)
-        ops['ntbuff'] = params['ntbuff']  # samples of symmetrical buffer for whitening and spike detection
-        ops['NT'] = params['NT']  # must be multiple of 32 + ntbuff. This is the batch size (try decreasing if out of memory).
-        ops['whiteningRange'] = 32.0  # number of channels to use for whitening each channel
-        ops['nSkipCov'] = 25.0  # compute whitening matrix from every N-th batch
-        ops['scaleproc'] = 200.0  # int16 scaling of whitened data
-        ops['nPCs'] = params['nPCs']  # how many PCs to project the spikes into
-        ops['useRAM'] = 0.0  # not yet available
+        ops["nfilt_factor"] = params["nfilt_factor"]  # max number of clusters per good channel (even temporary ones)
+        ops["ntbuff"] = params["ntbuff"]  # samples of symmetrical buffer for whitening and spike detection
+        ops["NT"] = params[
+            "NT"
+        ]  # must be multiple of 32 + ntbuff. This is the batch size (try decreasing if out of memory).
+        ops["whiteningRange"] = 32.0  # number of channels to use for whitening each channel
+        ops["nSkipCov"] = 25.0  # compute whitening matrix from every N-th batch
+        ops["scaleproc"] = 200.0  # int16 scaling of whitened data
+        ops["nPCs"] = params["nPCs"]  # how many PCs to project the spikes into
+        ops["useRAM"] = 0.0  # not yet available
 
         # drift correction
-        ops['do_correction'] = params['do_correction']
+        ops["do_correction"] = params["do_correction"]
 
         ## option for wavelength
-        ops['nt0'] = params['wave_length'] # size of the waveform extracted around each detected peak. Be sure to make it odd to make alignment easier.
+        ops["nt0"] = params[
+            "wave_length"
+        ]  # size of the waveform extracted around each detected peak. Be sure to make it odd to make alignment easier.
+
+        ops["skip_kilosort_preprocessing"] = params["skip_kilosort_preprocessing"]
+        if params["skip_kilosort_preprocessing"]:
+            ops["fproc"] = ops["fbinary"]
+            assert (
+                params["scaleproc"] is not None
+            ), "When skip_kilosort_preprocessing=True scaleproc must explicitly given"
+
+        # int16 scaling of whitened data, when None then scaleproc is set to 200.
+        scaleproc = params["scaleproc"]
+        ops["scaleproc"] = scaleproc if scaleproc is not None else 200.0
+
+        ops["save_rez_to_mat"] = params["save_rez_to_mat"]
         return ops
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sorters/external/kilosort_master.m` & `spikeinterface-0.98.0/src/spikeinterface/sorters/external/kilosort_master.m`

 * *Files identical despite different names*

### Comparing `spikeinterface-0.97.1/spikeinterface/sorters/external/kilosortbase.py` & `spikeinterface-0.98.0/src/spikeinterface/sorters/external/kilosortbase.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,31 +1,32 @@
 from pathlib import Path
 from warnings import warn
 import json
 import shutil
 import sys
 
 import numpy as np
-import scipy.io
 
 from ..utils import ShellScript, get_matlab_shell_name, get_bash_path
 from ..basesorter import get_job_kwargs
 from spikeinterface.extractors import KiloSortSortingExtractor
 from spikeinterface.core import write_binary_recording
+from spikeinterface.preprocessing.zero_channel_pad import TracePaddedRecording
 
 
 class KilosortBase:
     """
     Shared class for all kilosort implementation:
       * _generate_channel_map_file
       * _generate_ops_file
       * _run_from_folder
       * _get_result_from_folder
     """
-    gpu_capability = 'nvidia-required'
+
+    gpu_capability = "nvidia-required"
     requires_binary_data = True
 
     @staticmethod
     def _generate_channel_map_file(recording, sorter_output_folder):
         """
         This function generates channel map data for kilosort and saves as `chanMap.mat`
 
@@ -42,31 +43,33 @@
         # prepare electrode positions for this group (only one group, the split is done in basesorter)
         groups = [1] * recording.get_num_channels()
         positions = np.array(recording.get_channel_locations())
         if positions.shape[1] != 2:
             raise RuntimeError("3D 'location' are not supported. Set 2D locations instead")
 
         nchan = recording.get_num_channels()
-        xcoords = [p[0] for p in positions],
-        ycoords = [p[1] for p in positions],
-        kcoords = groups,
+        xcoords = ([p[0] for p in positions],)
+        ycoords = ([p[1] for p in positions],)
+        kcoords = (groups,)
 
         channel_map = {}
-        channel_map['Nchannels'] = nchan
-        channel_map['connected'] = np.full((nchan, 1), True)
-        channel_map['chanMap0ind'] = np.arange(nchan)
-        channel_map['chanMap'] = channel_map['chanMap0ind'] + 1
-
-        channel_map['xcoords'] = np.array(xcoords).astype(float)
-        channel_map['ycoords'] = np.array(ycoords).astype(float)
-        channel_map['kcoords'] = np.array(kcoords).astype(float)
+        channel_map["Nchannels"] = nchan
+        channel_map["connected"] = np.full((nchan, 1), True)
+        channel_map["chanMap0ind"] = np.arange(nchan)
+        channel_map["chanMap"] = channel_map["chanMap0ind"] + 1
+
+        channel_map["xcoords"] = np.array(xcoords).astype(float)
+        channel_map["ycoords"] = np.array(ycoords).astype(float)
+        channel_map["kcoords"] = np.array(kcoords).astype(float)
 
         sample_rate = recording.get_sampling_frequency()
-        channel_map['fs'] = float(sample_rate)
-        scipy.io.savemat(str(sorter_output_folder / 'chanMap.mat'), channel_map)
+        channel_map["fs"] = float(sample_rate)
+        import scipy.io
+
+        scipy.io.savemat(str(sorter_output_folder / "chanMap.mat"), channel_map)
 
     @classmethod
     def _generate_ops_file(cls, recording, params, sorter_output_folder, binary_file_path):
         """
         This function generates ops (configs) data for kilosort and saves as `ops.mat`
 
         Loading example in Matlab (shouldn't be assigned to a variable):
@@ -80,114 +83,158 @@
             Custom parameters dictionary for kilosort
         sorter_output_folder: pathlib.Path
             Path object to save `ops.mat`
         """
         ops = {}
 
         nchan = float(recording.get_num_channels())
-        ops['NchanTOT'] = nchan  # total number of channels (omit if already in chanMap file)
-        ops['Nchan'] = nchan  # number of active channels (omit if already in chanMap file)
+        ops["NchanTOT"] = nchan  # total number of channels (omit if already in chanMap file)
+        ops["Nchan"] = nchan  # number of active channels (omit if already in chanMap file)
 
-        ops['datatype'] = 'dat'  # binary ('dat', 'bin') or 'openEphys'
-        ops['fbinary'] = str(binary_file_path.absolute())  # will be created for 'openEphys'
-        ops['fproc'] = str((sorter_output_folder / 'temp_wh.dat').absolute())  # residual from RAM of preprocessed data
-        ops['root'] = str(sorter_output_folder.absolute())  # 'openEphys' only: where raw files are
-        ops['trange'] = [0, np.Inf] #  time range to sort
-        ops['chanMap'] = str((sorter_output_folder / 'chanMap.mat').absolute())
+        ops["datatype"] = "dat"  # binary ('dat', 'bin') or 'openEphys'
+        ops["fbinary"] = str(binary_file_path.absolute())  # will be created for 'openEphys'
+        ops["fproc"] = str((sorter_output_folder / "temp_wh.dat").absolute())  # residual from RAM of preprocessed data
+        ops["root"] = str(sorter_output_folder.absolute())  # 'openEphys' only: where raw files are
+        ops["trange"] = [0, np.Inf]  #  time range to sort
+        ops["chanMap"] = str((sorter_output_folder / "chanMap.mat").absolute())
 
-        ops['fs'] = recording.get_sampling_frequency() # sample rate
-        ops['CAR'] = 1.0 if params['car'] else 0.0
+        ops["fs"] = recording.get_sampling_frequency()  # sample rate
+        ops["CAR"] = 1.0 if params["car"] else 0.0
 
         ops = cls._get_specific_options(ops, params)
 
         # Converting integer values into float
         # Kilosort interprets ops fields as double
         for k, v in ops.items():
             if isinstance(v, int):
                 ops[k] = float(v)
 
-        ops = {'ops': ops}
-        scipy.io.savemat(str(sorter_output_folder / 'ops.mat'), ops)
+        ops = {"ops": ops}
+        import scipy.io
+
+        scipy.io.savemat(str(sorter_output_folder / "ops.mat"), ops)
 
     @classmethod
     def _get_specific_options(cls, ops, params):
         """Specific options should be implemented in subclass"""
         return ops
 
     @classmethod
     def _setup_recording(cls, recording, sorter_output_folder, params, verbose):
         cls._generate_channel_map_file(recording, sorter_output_folder)
-        
-        if recording.binary_compatible_with(dtype='int16', time_axis=0, file_paths_lenght=1):
+
+        skip_kilosort_preprocessing = params.get("skip_kilosort_preprocessing", False)
+
+        if (
+            recording.binary_compatible_with(dtype="int16", time_axis=0, file_paths_lenght=1)
+            and not skip_kilosort_preprocessing
+        ):
             # no copy
-            d = recording.get_binary_description()
-            binary_file_path = Path(d['file_paths'][0])
+            binary_description = recording.get_binary_description()
+            binary_file_path = Path(binary_description["file_paths"][0])
         else:
             # local copy needed
-            binary_file_path = sorter_output_folder / 'recording.dat'
-            write_binary_recording(recording, file_paths=[binary_file_path],
-                                   dtype='int16', verbose=False, **get_job_kwargs(params, verbose))
+            binary_file_path = sorter_output_folder / "recording.dat"
+            if skip_kilosort_preprocessing:
+                # when we skip the kilosort preprocessing we need to extend the file with zero pad to ensure
+                # that the number of sample is compatible with batch size (NT)
+                nt = params["NT"]
+                num_samples = recording.get_num_samples()
+                pad = nt - num_samples % nt
+
+                padding_start = 0
+                padding_end = pad
+                padded_recording = TracePaddedRecording(
+                    parent_recording=recording,
+                    padding_start=padding_start,
+                    padding_end=padding_end,
+                )
+            else:
+                padded_recording = recording
+            write_binary_recording(
+                recording=padded_recording,
+                file_paths=[binary_file_path],
+                dtype="int16",
+                **get_job_kwargs(params, verbose),
+            )
 
         cls._generate_ops_file(recording, params, sorter_output_folder, binary_file_path)
 
     @classmethod
     def _run_from_folder(cls, sorter_output_folder, params, verbose):
         sorter_output_folder = sorter_output_folder.absolute()
         if cls.check_compiled():
-            shell_cmd = f'''
+            shell_cmd = f"""
                 #!/bin/bash
                 {cls.compiled_name} "{sorter_output_folder}"
-            '''
+            """
         else:
             source_dir = Path(Path(__file__).parent)
             external_dir = source_dir.parent
-            shutil.copy(str(source_dir / f'{cls.sorter_name}_master.m'), str(sorter_output_folder))
-            shutil.copy(str(external_dir / 'utils' / 'writeNPY.m'), str(sorter_output_folder))
-            shutil.copy(str(external_dir / 'utils' / 'constructNPYheader.m'), str(sorter_output_folder))
+            shutil.copy(str(source_dir / f"{cls.sorter_name}_master.m"), str(sorter_output_folder))
+            shutil.copy(str(external_dir / "utils" / "writeNPY.m"), str(sorter_output_folder))
+            shutil.copy(str(external_dir / "utils" / "constructNPYheader.m"), str(sorter_output_folder))
 
-            sorter_path = getattr(cls, f'{cls.sorter_name}_path')
+            sorter_path = getattr(cls, f"{cls.sorter_name}_path")
             sorter_path = Path(sorter_path).absolute()
-            if 'win' in sys.platform and sys.platform != 'darwin':
+            if "win" in sys.platform and sys.platform != "darwin":
                 disk_move = str(sorter_output_folder)[:2]
-                shell_cmd = f'''
+                shell_cmd = f"""
                     {disk_move}
                     cd {sorter_output_folder}
                     matlab -nosplash -wait -r "{cls.sorter_name}_master('{sorter_output_folder}', '{sorter_path}')"
-                '''
+                """
             else:
-                if get_matlab_shell_name() == 'fish':
+                if get_matlab_shell_name() == "fish":
                     # Avoid MATLAB's 'copyfile' function failing due to MATLAB using fish as a shell
                     bash_path = get_bash_path()
-                    warn(f"Avoid Kilosort failing due to MATLAB using 'fish' as a shell: setting `MATLAB_SHELL` env variable to `{bash_path}`.")
-                    matlab_shell_str = f'''
+                    warn(
+                        f"Avoid Kilosort failing due to MATLAB using 'fish' as a shell: setting `MATLAB_SHELL` env variable to `{bash_path}`."
+                    )
+                    matlab_shell_str = f"""
                     export MATLAB_SHELL="{bash_path}"
                     echo "Set MATLAB shell to $MATLAB_SHELL"
-                    '''
+                    """
                 else:
                     matlab_shell_str = ""
-                shell_cmd = f'''
+                shell_cmd = f"""
                     #!/bin/bash
                     {matlab_shell_str}
                     cd "{sorter_output_folder}"
                     matlab -nosplash -nodisplay -r "{cls.sorter_name}_master('{sorter_output_folder}', '{sorter_path}')"
-                '''
-        shell_script = ShellScript(shell_cmd, script_path=sorter_output_folder / f'run_{cls.sorter_name}',
-                                   log_path=sorter_output_folder / f'{cls.sorter_name}.log', verbose=verbose)
+                """
+        shell_script = ShellScript(
+            shell_cmd,
+            script_path=sorter_output_folder / f"run_{cls.sorter_name}",
+            log_path=sorter_output_folder / f"{cls.sorter_name}.log",
+            verbose=verbose,
+        )
         shell_script.start()
         retcode = shell_script.wait()
 
         if retcode != 0:
-            raise Exception(f'{cls.sorter_name} returned a non-zero exit code')
+            raise Exception(f"{cls.sorter_name} returned a non-zero exit code")
+
+        # Clean-up temporary files
+        if params["delete_tmp_files"]:
+            for temp_file in sorter_output_folder.glob("*.m"):
+                temp_file.unlink()
+            for temp_file in sorter_output_folder.glob("*.mat"):
+                temp_file.unlink()
+            if (sorter_output_folder / "temp_wh.dat").exists():
+                (sorter_output_folder / "temp_wh.dat").unlink()
+        if params["delete_recording_dat"] and (recording_file := sorter_output_folder / "recording.dat").exists():
+            recording_file.unlink()
 
     @classmethod
     def _get_result_from_folder(cls, sorter_output_folder):
         sorter_output_folder = Path(sorter_output_folder)
-        if (sorter_output_folder.parent / 'spikeinterface_params.json').is_file():
-            params_file = sorter_output_folder.parent / 'spikeinterface_params.json'
+        if (sorter_output_folder.parent / "spikeinterface_params.json").is_file():
+            params_file = sorter_output_folder.parent / "spikeinterface_params.json"
         else:
             # back-compatibility
-            params_file = sorter_output_folder / 'spikeinterface_params.json'
-        with params_file.open('r') as f:
-            sorter_params = json.load(f)['sorter_params']
-        keep_good_only = sorter_params.get('keep_good_only', False)
+            params_file = sorter_output_folder / "spikeinterface_params.json"
+        with params_file.open("r") as f:
+            sorter_params = json.load(f)["sorter_params"]
+        keep_good_only = sorter_params.get("keep_good_only", False)
         sorting = KiloSortSortingExtractor(folder_path=sorter_output_folder, keep_good_only=keep_good_only)
         return sorting
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sorters/external/klusta.py` & `spikeinterface-0.98.0/src/spikeinterface/sorters/external/klusta.py`

 * *Files 14% similar despite different names*

```diff
@@ -19,42 +19,42 @@
 except ImportError:
     HAVE_KLUSTA = False
 
 
 class KlustaSorter(BaseSorter):
     """Klusta Sorter object."""
 
-    sorter_name = 'klusta'
+    sorter_name = "klusta"
 
     requires_locations = False
     requires_binary_data = True
 
     _default_params = {
-        'adjacency_radius': None,
-        'threshold_strong_std_factor': 5,
-        'threshold_weak_std_factor': 2,
-        'detect_sign': -1,
-        'extract_s_before': 16,
-        'extract_s_after': 32,
-        'n_features_per_channel': 3,
-        'pca_n_waveforms_max': 10000,
-        'num_starting_clusters': 50,
+        "adjacency_radius": None,
+        "threshold_strong_std_factor": 5,
+        "threshold_weak_std_factor": 2,
+        "detect_sign": -1,
+        "extract_s_before": 16,
+        "extract_s_after": 32,
+        "n_features_per_channel": 3,
+        "pca_n_waveforms_max": 10000,
+        "num_starting_clusters": 50,
     }
 
     _params_description = {
-        'adjacency_radius': "Radius in um to build channel neighborhood ",
-        'threshold_strong_std_factor': "Strong threshold for spike detection",
-        'threshold_weak_std_factor': "Weak threshold for spike detection",
-        'detect_sign': "Use -1 (negative), 1 (positive) or 0 (both) depending "
-                       "on the sign of the spikes in the recording",
-        'extract_s_before': "Number of samples to cut out before the peak",
-        'extract_s_after': "Number of samples to cut out after the peak",
-        'n_features_per_channel': "Number of PCA features per channel",
-        'pca_n_waveforms_max': "Maximum number of waveforms for PCA",
-        'num_starting_clusters': "Number of initial clusters",
+        "adjacency_radius": "Radius in um to build channel neighborhood ",
+        "threshold_strong_std_factor": "Strong threshold for spike detection",
+        "threshold_weak_std_factor": "Weak threshold for spike detection",
+        "detect_sign": "Use -1 (negative), 1 (positive) or 0 (both) depending "
+        "on the sign of the spikes in the recording",
+        "extract_s_before": "Number of samples to cut out before the peak",
+        "extract_s_after": "Number of samples to cut out after the peak",
+        "n_features_per_channel": "Number of PCA features per channel",
+        "pca_n_waveforms_max": "Maximum number of waveforms for PCA",
+        "num_starting_clusters": "Number of initial clusters",
     }
 
     sorter_description = """Klusta is a density-based spike sorter that uses a masked EM approach for clustering.
     For more information see https://doi.org/10.1038/nn.4268"""
 
     installation_mesg = """\nTo use Klusta run:\n
        >>> pip install Cython h5py tqdm
@@ -76,86 +76,99 @@
     @classmethod
     def _setup_recording(cls, recording, sorter_output_folder, params, verbose):
         source_dir = Path(__file__).parent
 
         # alias to params
         p = params
 
-        experiment_name = sorter_output_folder / 'recording'
+        experiment_name = sorter_output_folder / "recording"
 
         # save prb file
-        prb_file = sorter_output_folder / 'probe.prb'
+        prb_file = sorter_output_folder / "probe.prb"
         probegroup = recording.get_probegroup()
-        write_prb(prb_file, probegroup, radius=p['adjacency_radius'])
+        write_prb(prb_file, probegroup, radius=p["adjacency_radius"])
 
         # source file
         if recording.binary_compatible_with(time_axis=0, file_offset=0):
             # no copy
             d = recording.get_binary_description()
-            raw_filename = str(d['file_paths'][0])
-            dtype = str(d['dtype'])
-            if not recording.binary_compatible_with(file_suffix='.dat'):
+            raw_filename = str(d["file_paths"][0])
+            dtype = str(d["dtype"])
+            if not recording.binary_compatible_with(file_suffix=".dat"):
                 # copy and change suffix
                 print("Binary file is not a .dat file. Making a copy!")
                 shutil.copy(raw_filename, sorter_output_folder / "recording.dat")
                 raw_filename = sorter_output_folder / "recording.dat"
         else:
             # save binary file (chunk by chunk) into a new file
-            raw_filename = sorter_output_folder / 'recording.dat'
-            dtype = 'int16'
-            write_binary_recording(recording, file_paths=[raw_filename], verbose=False, 
-                                   dtype=dtype, **get_job_kwargs(params, verbose))
-
-        if p['detect_sign'] < 0:
-            detect_sign = 'negative'
-        elif p['detect_sign'] > 0:
-            detect_sign = 'positive'
+            raw_filename = sorter_output_folder / "recording.dat"
+            dtype = "int16"
+            write_binary_recording(recording, file_paths=[raw_filename], dtype=dtype, **get_job_kwargs(params, verbose))
+
+        if p["detect_sign"] < 0:
+            detect_sign = "negative"
+        elif p["detect_sign"] > 0:
+            detect_sign = "positive"
         else:
-            detect_sign = 'both'
+            detect_sign = "both"
 
         # set up klusta config file
-        with (source_dir / 'klusta_config_default.prm').open('r') as f:
+        with (source_dir / "klusta_config_default.prm").open("r") as f:
             klusta_config = f.readlines()
 
         # Note: should use format with dict approach here
-        klusta_config = ''.join(klusta_config).format(experiment_name,
-                                                      prb_file, raw_filename,
-                                                      float(recording.get_sampling_frequency()),
-                                                      recording.get_num_channels(), "'{}'".format(dtype),
-                                                      p['threshold_strong_std_factor'], p['threshold_weak_std_factor'],
-                                                      "'" + detect_sign + "'",
-                                                      p['extract_s_before'], p['extract_s_after'],
-                                                      p['n_features_per_channel'],
-                                                      p['pca_n_waveforms_max'], p['num_starting_clusters']
-                                                      )
+        klusta_config = "".join(klusta_config).format(
+            experiment_name,
+            prb_file,
+            raw_filename,
+            float(recording.get_sampling_frequency()),
+            recording.get_num_channels(),
+            "'{}'".format(dtype),
+            p["threshold_strong_std_factor"],
+            p["threshold_weak_std_factor"],
+            "'" + detect_sign + "'",
+            p["extract_s_before"],
+            p["extract_s_after"],
+            p["n_features_per_channel"],
+            p["pca_n_waveforms_max"],
+            p["num_starting_clusters"],
+        )
 
-        with (sorter_output_folder / 'config.prm').open('w') as f:
+        with (sorter_output_folder / "config.prm").open("w") as f:
             f.writelines(klusta_config)
 
     @classmethod
     def _run_from_folder(cls, sorter_output_folder, params, verbose):
-        if 'win' in sys.platform and sys.platform != 'darwin':
-            shell_cmd = '''
+        if "win" in sys.platform and sys.platform != "darwin":
+            shell_cmd = """
                         klusta --overwrite {klusta_config}
-                    '''.format(klusta_config=sorter_output_folder / 'config.prm')
+                    """.format(
+                klusta_config=sorter_output_folder / "config.prm"
+            )
         else:
-            shell_cmd = '''
+            shell_cmd = """
                         #!/bin/bash
                         klusta {klusta_config} --overwrite
-                    '''.format(klusta_config=sorter_output_folder / 'config.prm')
-
-        shell_script = ShellScript(shell_cmd, script_path=sorter_output_folder / f'run_{cls.sorter_name}',
-                                   log_path=sorter_output_folder / f'{cls.sorter_name}.log', verbose=verbose)
+                    """.format(
+                klusta_config=sorter_output_folder / "config.prm"
+            )
+
+        shell_script = ShellScript(
+            shell_cmd,
+            script_path=sorter_output_folder / f"run_{cls.sorter_name}",
+            log_path=sorter_output_folder / f"{cls.sorter_name}.log",
+            verbose=verbose,
+        )
         shell_script.start()
 
         retcode = shell_script.wait()
 
         if retcode != 0:
-            raise Exception('klusta returned a non-zero exit code')
+            raise Exception("klusta returned a non-zero exit code")
 
-        if not (sorter_output_folder / 'recording.kwik').is_file():
-            raise Exception('Klusta did not run successfully')
+        if not (sorter_output_folder / "recording.kwik").is_file():
+            raise Exception("Klusta did not run successfully")
 
     @classmethod
     def _get_result_from_folder(cls, sorter_output_folder):
-        sorting = KlustaSortingExtractor(file_or_folder_path=Path(sorter_output_folder) / 'recording.kwik')
+        sorting = KlustaSortingExtractor(file_or_folder_path=Path(sorter_output_folder) / "recording.kwik")
         return sorting
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sorters/external/klusta_config_default.prm` & `spikeinterface-0.98.0/src/spikeinterface/sorters/external/klusta_config_default.prm`

 * *Files identical despite different names*

### Comparing `spikeinterface-0.97.1/spikeinterface/sorters/external/mountainsort4.py` & `spikeinterface-0.98.0/src/spikeinterface/sorters/external/mountainsort4.py`

 * *Files 14% similar despite different names*

```diff
@@ -10,48 +10,47 @@
 
 from spikeinterface.extractors import NpzSortingExtractor, NumpySorting
 
 
 class Mountainsort4Sorter(BaseSorter):
     """Mountainsort4 Sorter object."""
 
-    sorter_name = 'mountainsort4'
+    sorter_name = "mountainsort4"
     requires_locations = False
-    compatible_with_parallel = {'loky': True, 'multiprocessing': False, 'threading': False}
+    compatible_with_parallel = {"loky": True, "multiprocessing": False, "threading": False}
 
     _default_params = {
-        'detect_sign': -1,  # Use -1, 0, or 1, depending on the sign of the spikes in the recording
-        'adjacency_radius': -1,  # Use -1 to include all channels in every neighborhood
-        'freq_min': 300,  # Use None for no bandpass filtering
-        'freq_max': 6000,
-        'filter': True,
-        'whiten': True,  # Whether to do channel whitening as part of preprocessing
-        'num_workers': 1,
-        'clip_size': 50,
-        'detect_threshold': 3,
-        'detect_interval': 10,  # Minimum number of timepoints between events detected on the same channel
-        'tempdir': None
+        "detect_sign": -1,  # Use -1, 0, or 1, depending on the sign of the spikes in the recording
+        "adjacency_radius": -1,  # Use -1 to include all channels in every neighborhood
+        "freq_min": 300,  # Use None for no bandpass filtering
+        "freq_max": 6000,
+        "filter": True,
+        "whiten": True,  # Whether to do channel whitening as part of preprocessing
+        "num_workers": 1,
+        "clip_size": 50,
+        "detect_threshold": 3,
+        "detect_interval": 10,  # Minimum number of timepoints between events detected on the same channel
+        "tempdir": None,
     }
 
     _params_description = {
-        'detect_sign': "Use -1 (negative) or 1 (positive) depending "
-                       "on the sign of the spikes in the recording",
+        "detect_sign": "Use -1 (negative) or 1 (positive) depending " "on the sign of the spikes in the recording",
         # Use -1, 0, or 1, depending on the sign of the spikes in the recording
-        'adjacency_radius': "Radius in um to build channel neighborhood "
-                            "(Use -1 to include all channels in every neighborhood)",
+        "adjacency_radius": "Radius in um to build channel neighborhood "
+        "(Use -1 to include all channels in every neighborhood)",
         # Use -1 to include all channels in every neighborhood
-        'freq_min': "High-pass filter cutoff frequency",
-        'freq_max': "Low-pass filter cutoff frequency",
-        'filter': "Enable or disable filter",
-        'whiten': "Enable or disable whitening",
-        'num_workers': "Number of workers (if None, half of the cpu number is used)",
-        'clip_size': "Number of samples per waveform",
-        'detect_threshold': "Threshold for spike detection",
-        'detect_interval': "Minimum number of timepoints between events detected on the same channel",
-        'tempdir': "Temporary directory for mountainsort (available for ms4 >= 1.0.2)s"
+        "freq_min": "High-pass filter cutoff frequency",
+        "freq_max": "Low-pass filter cutoff frequency",
+        "filter": "Enable or disable filter",
+        "whiten": "Enable or disable whitening",
+        "num_workers": "Number of workers (if None, half of the cpu number is used)",
+        "clip_size": "Number of samples per waveform",
+        "detect_threshold": "Threshold for spike detection",
+        "detect_interval": "Minimum number of timepoints between events detected on the same channel",
+        "tempdir": "Temporary directory for mountainsort (available for ms4 >= 1.0.2)s",
     }
 
     sorter_description = """Mountainsort4 is a fully automatic density-based spike sorter using the isosplit clustering
     method and automatic curation procedures. For more information see https://doi.org/10.1016/j.neuron.2017.08.030"""
 
     installation_mesg = """\nTo use Mountainsort4 run:\n
        >>> pip install mountainsort4
@@ -60,87 +59,93 @@
       * https://github.com/flatironinstitute/mountainsort
     """
 
     @classmethod
     def is_installed(cls):
         try:
             import mountainsort4
+
             HAVE_MS4 = True
         except ImportError:
             HAVE_MS4 = False
         return HAVE_MS4
 
     @staticmethod
     def get_sorter_version():
         import mountainsort4
-        if hasattr(mountainsort4, '__version__'):
+
+        if hasattr(mountainsort4, "__version__"):
             return mountainsort4.__version__
-        return 'unknown'
+        return "unknown"
 
     @classmethod
     def _check_apply_filter_in_params(cls, params):
-        return params['filter']
+        return params["filter"]
 
     @classmethod
     def _setup_recording(cls, recording, sorter_output_folder, params, verbose):
         pass
 
     @classmethod
     def _run_from_folder(cls, sorter_output_folder, params, verbose):
         import mountainsort4
 
-        recording = load_extractor(sorter_output_folder.parent / 'spikeinterface_recording.json')
+        recording = load_extractor(
+            sorter_output_folder.parent / "spikeinterface_recording.json", base_folder=sorter_output_folder.parent
+        )
 
         # alias to params
         p = params
 
         samplerate = recording.get_sampling_frequency()
 
         # Bandpass filter
-        if p['filter'] and p['freq_min'] is not None and p['freq_max'] is not None:
+        if p["filter"] and p["freq_min"] is not None and p["freq_max"] is not None:
             if verbose:
-                print('filtering')
-            recording = bandpass_filter(recording=recording, freq_min=p['freq_min'], freq_max=p['freq_max'])
+                print("filtering")
+            recording = bandpass_filter(recording=recording, freq_min=p["freq_min"], freq_max=p["freq_max"])
 
         # Whiten
-        if p['whiten']:
+        if p["whiten"]:
             if verbose:
-                print('whitening')
-            recording = whiten(recording=recording)
+                print("whitening")
+            recording = whiten(recording=recording, dtype="float32")
 
-        print('Mountainsort4 use the OLD spikeextractors mapped with NewToOldRecording')
+        print("Mountainsort4 use the OLD spikeextractors mapped with NewToOldRecording")
         old_api_recording = NewToOldRecording(recording)
-        
-        ms4_params = dict(recording=old_api_recording,
-                          detect_sign=p['detect_sign'],
-                          adjacency_radius=p['adjacency_radius'],
-                          clip_size=p['clip_size'],
-                          detect_threshold=p['detect_threshold'],
-                          detect_interval=p['detect_interval'],
-                          num_workers=p['num_workers'],
-                          verbose=verbose)
-        
+
+        ms4_params = dict(
+            recording=old_api_recording,
+            detect_sign=p["detect_sign"],
+            adjacency_radius=p["adjacency_radius"],
+            clip_size=p["clip_size"],
+            detect_threshold=p["detect_threshold"],
+            detect_interval=p["detect_interval"],
+            num_workers=p["num_workers"],
+            verbose=verbose,
+        )
+
         # temporary folder
         ms4_version = Mountainsort4Sorter.get_sorter_version()
 
         if ms4_version != "unknown" and parse(ms4_version) >= parse("1.0.3"):
             if p["tempdir"] is not None:
                 p["tempdir"] = str(p["tempdir"])
             if verbose:
                 print(f'Using temporary directory {p["tempdir"]}')
-            ms4_params.update(tempdir=p['tempdir'])
+            ms4_params.update(tempdir=p["tempdir"])
 
         # Check location no more needed done in basesorter
         old_api_sorting = mountainsort4.mountainsort4(**ms4_params)
 
         # convert sorting to new API and save it
         unit_ids = old_api_sorting.get_unit_ids()
         units_dict_list = [{u: old_api_sorting.get_unit_spike_train(u) for u in unit_ids}]
         new_api_sorting = NumpySorting.from_dict(units_dict_list, samplerate)
-        NpzSortingExtractor.write_sorting(new_api_sorting, str(sorter_output_folder / 'firings.npz'))
+        NpzSortingExtractor.write_sorting(new_api_sorting, str(sorter_output_folder / "firings.npz"))
 
     @classmethod
     def _get_result_from_folder(cls, sorter_output_folder):
         sorter_output_folder = Path(sorter_output_folder)
-        result_fname = sorter_output_folder / 'firings.npz'
+        result_fname = sorter_output_folder / "firings.npz"
         sorting = NpzSortingExtractor(result_fname)
         return sorting
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sorters/external/pykilosort.py` & `spikeinterface-0.98.0/src/spikeinterface/sorters/external/pykilosort.py`

 * *Files 7% similar despite different names*

```diff
@@ -16,42 +16,42 @@
 except ImportError:
     HAVE_PYKILOSORT = False
 
 
 class PyKilosortSorter(BaseSorter):
     """Pykilosort Sorter object."""
 
-    sorter_name = 'pykilosort'
+    sorter_name = "pykilosort"
     requires_locations = False
-    gpu_capability = 'nvidia-required'
+    gpu_capability = "nvidia-required"
     requires_binary_data = True
-    compatible_with_parallel = {'loky': True, 'multiprocessing': False, 'threading': False}
+    compatible_with_parallel = {"loky": True, "multiprocessing": False, "threading": False}
 
     _default_params = {
         "low_memory": False,
         "seed": 42,
-        "preprocessing_function": 'kilosort2',
+        "preprocessing_function": "kilosort2",
         "save_drift_spike_detections": False,
         "perform_drift_registration": False,
         "do_whitening": True,
         "save_temp_files": True,
         "fshigh": 300.0,
         "fslow": None,
-        "minfr_goodchannels": .1,
+        "minfr_goodchannels": 0.1,
         "genericSpkTh": 8.0,
         "nblocks": 5,
         "sig_datashift": 20.0,
         "stable_mode": True,
         "deterministic_mode": True,
         "datashift": None,
-        "Th": [10,4],
+        "Th": [10, 4],
         "ThPre": 8,
         "lam": 10,
-        "minFR": 1.0/50,
-        "momentum": [20,400],
+        "minFR": 1.0 / 50,
+        "momentum": [20, 400],
         "sigmaMask": 30,
         "spkTh": -6,
         "reorder": 1,
         "nSkipCov": 25,
         "ntbuff": 64,
         "whiteningRange": 32,
         "scaleproc": 200,
@@ -59,42 +59,42 @@
         "nt0": 61,
         "nup": 10,
         "sig": 1,
         "gain": 1,
         "templateScaling": 20.0,
         "loc_range": [5, 4],
         "long_range": [30, 6],
-        "keep_good_only": False
+        "keep_good_only": False,
     }
 
     _params_description = {
         "low_memory": "low memory setting for running chronic recordings",
         "seed": "seed for deterministic output",
         "preprocessing_function": 'pre-processing function used choices are "kilosort2" or "destriping"',
-        "save_drift_spike_detections": 'save detected spikes in drift correction',
-        "perform_drift_registration": 'Estimate electrode drift and apply registration',
-        "do_whitening": 'whether or not to whiten data, if disabled channels are individually z-scored',
+        "save_drift_spike_detections": "save detected spikes in drift correction",
+        "perform_drift_registration": "Estimate electrode drift and apply registration",
+        "do_whitening": "whether or not to whiten data, if disabled channels are individually z-scored",
         "fs": "sample rate",
-        "probe": 'data type of raw data',
-        "data_dtype": 'data type of raw data',
+        "probe": "data type of raw data",
+        "data_dtype": "data type of raw data",
         "save_temp_files": "keep temporary files created while running",
         "fshigh": "high pass filter frequency",
         "fslow": "low pass filter frequency",
         "minfr_goodchannels": "minimum firing rate on a 'good' channel (0 to skip)",
         "genericSpkTh": "threshold for crossings with generic templates",
         "nblocks": "number of blocks used to segment the probe when tracking drift, 0 == don't track, 1 == rigid, > 1 == non-rigid",
         "output_filename": "optionally save registered data to a new binary file",
         "overwrite": "overwrite proc file with shifted data",
         "sig_datashift": "sigma for the Gaussian process smoothing",
         "stable_mode": "make output more stable",
         "deterministic_mode": "make output deterministic by sorting spikes before applying kernels",
         "datashift": "parameters for 'datashift' drift correction. not required",
         "Th": "threshold on projections (like in Kilosort1, can be different for last pass like [10 4])",
         "ThPre": "threshold crossings for pre-clustering (in PCA projection space)",
-        "lam":  "how important is the amplitude penalty (like in Kilosort1, 0 means not used, 10 is average, 50 is a lot)",
+        "lam": "how important is the amplitude penalty (like in Kilosort1, 0 means not used, 10 is average, 50 is a lot)",
         "minFR": " minimum spike rate (Hz), if a cluster falls below this for too long it gets removed",
         "momentum": "number of samples to average over (annealed from first to second value)",
         "sigmaMask": "spatial constant in um for computing residual variance of spike",
         "spkTh": "spike threshold in standard deviations",
         "reorder": "whether to reorder batches for drift correction.",
         "nSkipCov": "compute whitening matrix from every nth batch",
         "ntbuff": "samples of symmetrical buffer for whitening and spike detection; Must be multiple of 32 + ntbuff. This is the batch size (try decreasing if out of memory).",
@@ -104,15 +104,15 @@
         "nt0": None,
         "nup": None,
         "sig": None,
         "gain": None,
         "templateScaling": None,
         "loc_range": None,
         "long_range": None,
-        "keep_good_only": "If True only 'good' units are returned"
+        "keep_good_only": "If True only 'good' units are returned",
     }
 
     sorter_description = """pykilosort is a port of kilosort to python"""
 
     installation_mesg = """\nTo use pykilosort:\n
        >>> pip install cupy
         >>> git clone https://github.com/MouseLand/pykilosort
@@ -136,28 +136,33 @@
     def _check_params(cls, recording, sorter_output_folder, params):
         return params
 
     @classmethod
     def _setup_recording(cls, recording, sorter_output_folder, params, verbose):
         if not recording.binary_compatible_with(time_axis=0, file_paths_lenght=1):
             # local copy needed
-            write_binary_recording(recording, file_paths=sorter_output_folder / 'recording.dat',
-                                                     verbose=False, **get_job_kwargs(params, verbose))
+            write_binary_recording(
+                recording,
+                file_paths=sorter_output_folder / "recording.dat",
+                **get_job_kwargs(params, verbose),
+            )
 
     @classmethod
     def _run_from_folder(cls, sorter_output_folder, params, verbose):
-        recording = load_extractor(sorter_output_folder.parent / 'spikeinterface_recording.json')
+        recording = load_extractor(
+            sorter_output_folder.parent / "spikeinterface_recording.json", base_folder=sorter_output_folder.parent
+        )
 
         if not recording.binary_compatible_with(time_axis=0, file_paths_lenght=1):
             # saved by setup recording
-            dat_path = sorter_output_folder / 'recording.dat'
+            dat_path = sorter_output_folder / "recording.dat"
         else:
             # no copy
             d = recording.get_binary_description()
-            dat_path = d['file_paths'][0]
+            dat_path = d["file_paths"][0]
 
         num_chans = recording.get_num_channels()
         locations = recording.get_channel_locations()
         params["n_channels"] = num_chans
 
         # ks_probe is not probeinterface Probe at all
         ks_probe = Bunch()
@@ -173,20 +178,22 @@
         ks_probe.Nchans = num_chans
         ks_probe.NchanTOT = num_chans
         ks_probe.chanMap = np.arange(num_chans)
         ks_probe.kcoords = np.ones(num_chans)
         ks_probe.xc = locations[:, 0]
         ks_probe.yc = locations[:, 1]
         ks_probe.shank = None
+        ks_probe.channel_labels = np.zeros(num_chans, dtype=int)
+
         if recording.get_channel_gains() is not None:
             gains = recording.get_channel_gains()
             if len(np.unique(gains)) == 1:
                 ks_probe.sample2volt = gains[0] * 1e-6
             else:
-                warnings.warn('Multiple gains detected for different channels. Median gain will be used')
+                warnings.warn("Multiple gains detected for different channels. Median gain will be used")
                 ks_probe.sample2volt = np.median(gains) * 1e-6
         else:
             ks_probe.sample2volt = 1e-6
 
         run(
             dat_path,
             dir_path=sorter_output_folder,
@@ -195,18 +202,17 @@
             fs=recording.get_sampling_frequency(),
             **params,
         )
 
     @classmethod
     def _get_result_from_folder(cls, sorter_output_folder):
         sorter_output_folder = Path(sorter_output_folder)
-        if (sorter_output_folder.parent / 'spikeinterface_params.json').is_file():
-            params_file = sorter_output_folder.parent / 'spikeinterface_params.json'
+        if (sorter_output_folder.parent / "spikeinterface_params.json").is_file():
+            params_file = sorter_output_folder.parent / "spikeinterface_params.json"
         else:
             # back-compatibility
-            params_file = sorter_output_folder / 'spikeinterface_params.json'
-        with params_file.open('r') as f:
-            sorter_params = json.load(f)['sorter_params']
-        keep_good_only = sorter_params.get('keep_good_only', False)
-        sorting = KiloSortSortingExtractor(folder_path=sorter_output_folder / "output",
-                                           keep_good_only=keep_good_only)
+            params_file = sorter_output_folder / "spikeinterface_params.json"
+        with params_file.open("r") as f:
+            sorter_params = json.load(f)["sorter_params"]
+        keep_good_only = sorter_params.get("keep_good_only", False)
+        sorting = KiloSortSortingExtractor(folder_path=sorter_output_folder / "output", keep_good_only=keep_good_only)
         return sorting
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sorters/external/sc_config_default.params` & `spikeinterface-0.98.0/src/spikeinterface/sorters/external/sc_config_default.params`

 * *Files identical despite different names*

### Comparing `spikeinterface-0.97.1/spikeinterface/sorters/external/spyking_circus.py` & `spikeinterface-0.98.0/src/spikeinterface/sorters/external/spyking_circus.py`

 * *Files 12% similar despite different names*

```diff
@@ -11,42 +11,42 @@
 
 from probeinterface import write_prb
 
 
 class SpykingcircusSorter(BaseSorter):
     """SpykingCircus Sorter object."""
 
-    sorter_name = 'spykingcircus'
+    sorter_name = "spykingcircus"
     requires_locations = False
 
     _default_params = {
-        'detect_sign': -1,  # -1 - 1 - 0
-        'adjacency_radius': 100,  # Channel neighborhood adjacency radius corresponding to geom file
-        'detect_threshold': 6,  # Threshold for detection
-        'template_width_ms': 3,  # Spyking circus parameter
-        'filter': True,
-        'merge_spikes': True,
-        'auto_merge': 0.75,
-        'num_workers': None,
-        'whitening_max_elts': 1000,  # I believe it relates to subsampling and affects compute time
-        'clustering_max_elts': 10000,  # I believe it relates to subsampling and affects compute time
+        "detect_sign": -1,  # -1 - 1 - 0
+        "adjacency_radius": 100,  # Channel neighborhood adjacency radius corresponding to geom file
+        "detect_threshold": 6,  # Threshold for detection
+        "template_width_ms": 3,  # Spyking circus parameter
+        "filter": True,
+        "merge_spikes": True,
+        "auto_merge": 0.75,
+        "num_workers": None,
+        "whitening_max_elts": 1000,  # I believe it relates to subsampling and affects compute time
+        "clustering_max_elts": 10000,  # I believe it relates to subsampling and affects compute time
     }
 
     _params_description = {
-        'detect_sign': "Use -1 (negative), 1 (positive) or 0 (both) depending "
-                       "on the sign of the spikes in the recording",
-        'adjacency_radius': "Radius in um to build channel neighborhood",
-        'detect_threshold': "Threshold for spike detection",
-        'template_width_ms': "Template width in ms. Recommended values: 3 for in vivo - 5 for in vitro",
-        'filter': "Enable or disable filter",
-        'merge_spikes': "Enable or disable automatic mergind",
-        'auto_merge': "Automatic merging threshold",
-        'num_workers': "Number of workers (if None, half of the cpu number is used)",
-        'whitening_max_elts': "Max number of events per electrode for whitening",
-        'clustering_max_elts': "Max number of events per electrode for clustering",
+        "detect_sign": "Use -1 (negative), 1 (positive) or 0 (both) depending "
+        "on the sign of the spikes in the recording",
+        "adjacency_radius": "Radius in um to build channel neighborhood",
+        "detect_threshold": "Threshold for spike detection",
+        "template_width_ms": "Template width in ms. Recommended values: 3 for in vivo - 5 for in vitro",
+        "filter": "Enable or disable filter",
+        "merge_spikes": "Enable or disable automatic mergind",
+        "auto_merge": "Automatic merging threshold",
+        "num_workers": "Number of workers (if None, half of the cpu number is used)",
+        "whitening_max_elts": "Max number of events per electrode for whitening",
+        "clustering_max_elts": "Max number of events per electrode for clustering",
     }
 
     sorter_description = """Spyking Circus uses a smart clustering and a greedy template matching approach for
     spike sorting. For more information see https://doi.org/10.7554/eLife.34518"""
 
     installation_mesg = """\nTo use Spyking-Circus run:\n
         >>> pip install spyking-circus
@@ -60,111 +60,126 @@
 
     handle_multi_segment = False
 
     @classmethod
     def is_installed(cls):
         try:
             import circus
+
             HAVE_SC = True
         except ImportError:
             HAVE_SC = False
         return HAVE_SC
 
     @staticmethod
     def get_sorter_version():
         import circus
+
         return circus.__version__
 
     @classmethod
     def _check_params(cls, recording, sorter_output_folder, params):
         # check and re dump params
         p = params
-        if p['num_workers'] is None:
-            p['num_workers'] = np.maximum(1, int(os.cpu_count() / 2))
+        if p["num_workers"] is None:
+            p["num_workers"] = np.maximum(1, int(os.cpu_count() / 2))
         return p
 
     @classmethod
     def _check_apply_filter_in_params(cls, params):
-        return params['filter']
+        return params["filter"]
 
     @classmethod
     def _setup_recording(cls, recording, sorter_output_folder, params, verbose):
         p = params
 
-        if p['detect_sign'] < 0:
-            detect_sign = 'negative'
-        elif p['detect_sign'] > 0:
-            detect_sign = 'positive'
+        if p["detect_sign"] < 0:
+            detect_sign = "negative"
+        elif p["detect_sign"] > 0:
+            detect_sign = "positive"
         else:
-            detect_sign = 'both'
-        if p['merge_spikes']:
-            auto = p['auto_merge']
+            detect_sign = "both"
+        if p["merge_spikes"]:
+            auto = p["auto_merge"]
         else:
             auto = 0
 
         source_dir = Path(__file__).parent
 
         # save prb file
         # note: only one group here, the split is done in basesorter
-        prb_file = sorter_output_folder / 'probe.prb'
+        prb_file = sorter_output_folder / "probe.prb"
         probegroup = recording.get_probegroup()
-        write_prb(prb_file, probegroup,
-                  total_nb_channels=recording.get_num_channels(),
-                  radius=p['adjacency_radius'])
+        write_prb(prb_file, probegroup, total_nb_channels=recording.get_num_channels(), radius=p["adjacency_radius"])
 
         # save binary file
-        file_name = 'recording'
+        file_name = "recording"
         # We should make this copy more efficient with chunks
 
         n_chan = recording.get_num_channels()
         n_frames = recording.get_num_frames(segment_index=0)
-        chunk_size = 2 ** 24 // n_chan
-        npy_file = str(sorter_output_folder / file_name) + '.npy'
-        data_file = open_memmap(npy_file, shape=(n_frames, n_chan), dtype=np.float32, mode='w+')
+        chunk_size = 2**24 // n_chan
+        npy_file = str(sorter_output_folder / file_name) + ".npy"
+        data_file = open_memmap(npy_file, shape=(n_frames, n_chan), dtype=np.float32, mode="w+")
         nb_chunks = n_frames // chunk_size
         for i in range(nb_chunks + 1):
             start_frame = i * chunk_size
             end_frame = min((i + 1) * chunk_size, n_frames)
-            data = recording.get_traces(start_frame=start_frame, end_frame=end_frame).astype('float32')
+            data = recording.get_traces(start_frame=start_frame, end_frame=end_frame).astype("float32")
             data_file[start_frame:end_frame, :] = data
 
         sample_rate = float(recording.get_sampling_frequency())
 
         # set up spykingcircus config file
-        with (source_dir / 'sc_config_default.params').open('r') as f:
+        with (source_dir / "sc_config_default.params").open("r") as f:
             circus_config = f.readlines()
-        circus_config = ''.join(circus_config).format(sample_rate, prb_file, p['template_width_ms'],
-                                                      p['detect_threshold'], detect_sign, p['filter'],
-                                                      p['whitening_max_elts'],
-                                                      p['clustering_max_elts'], auto)
-        with (sorter_output_folder / (file_name + '.params')).open('w') as f:
+        circus_config = "".join(circus_config).format(
+            sample_rate,
+            prb_file,
+            p["template_width_ms"],
+            p["detect_threshold"],
+            detect_sign,
+            p["filter"],
+            p["whitening_max_elts"],
+            p["clustering_max_elts"],
+            auto,
+        )
+        with (sorter_output_folder / (file_name + ".params")).open("w") as f:
             f.writelines(circus_config)
 
     @classmethod
     def _run_from_folder(cls, sorter_output_folder, params, verbose):
         sorter_name = cls.sorter_name
 
-        num_workers = params['num_workers']
+        num_workers = params["num_workers"]
 
-        if 'win' in sys.platform and sys.platform != 'darwin':
-            shell_cmd = '''
+        if "win" in sys.platform and sys.platform != "darwin":
+            shell_cmd = """
                         spyking-circus {recording} -c {num_workers}
-                    '''.format(recording=sorter_output_folder / 'recording.npy', num_workers=num_workers)
+                    """.format(
+                recording=sorter_output_folder / "recording.npy", num_workers=num_workers
+            )
         else:
-            shell_cmd = '''
+            shell_cmd = """
                         #!/bin/bash
                         spyking-circus {recording} -c {num_workers}
-                    '''.format(recording=sorter_output_folder / 'recording.npy', num_workers=num_workers)
-
-        shell_script = ShellScript(shell_cmd, script_path=sorter_output_folder / f'run_{sorter_name}',
-                                   log_path=sorter_output_folder / f'{sorter_name}.log', verbose=verbose)
+                    """.format(
+                recording=sorter_output_folder / "recording.npy", num_workers=num_workers
+            )
+
+        shell_script = ShellScript(
+            shell_cmd,
+            script_path=sorter_output_folder / f"run_{sorter_name}",
+            log_path=sorter_output_folder / f"{sorter_name}.log",
+            verbose=verbose,
+        )
         shell_script.start()
 
         retcode = shell_script.wait()
 
         if retcode != 0:
-            raise Exception('spykingcircus returned a non-zero exit code')
+            raise Exception("spykingcircus returned a non-zero exit code")
 
     @classmethod
     def _get_result_from_folder(cls, sorter_output_folder):
-        sorting = SpykingCircusSortingExtractor(folder_path=Path(sorter_output_folder) / 'recording')
+        sorting = SpykingCircusSortingExtractor(folder_path=Path(sorter_output_folder) / "recording")
         return sorting
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sorters/external/tridesclous.py` & `spikeinterface-0.98.0/src/spikeinterface/sorters/external/tridesclous.py`

 * *Files 10% similar despite different names*

```diff
@@ -13,35 +13,34 @@
 
 from probeinterface import write_prb
 
 
 class TridesclousSorter(BaseSorter):
     """Tridesclous Sorter object."""
 
-    sorter_name = 'tridesclous'
+    sorter_name = "tridesclous"
     requires_locations = False
-    compatible_with_parallel = {'loky': True, 'multiprocessing': False, 'threading': False}
+    compatible_with_parallel = {"loky": True, "multiprocessing": False, "threading": False}
     requires_binary_data = True
 
     _default_params = {
-        'freq_min': 400.,
-        'freq_max': 5000.,
-        'detect_sign': -1,
-        'detect_threshold': 5,
-        'common_ref_removal': False,
-        'nested_params': None,
+        "freq_min": 400.0,
+        "freq_max": 5000.0,
+        "detect_sign": -1,
+        "detect_threshold": 5,
+        "common_ref_removal": False,
+        "nested_params": None,
     }
 
     _params_description = {
-        'freq_min': "High-pass filter cutoff frequency",
-        'freq_max': "Low-pass filter cutoff frequency",
-        'detect_threshold': "Threshold for spike detection",
-        'detect_sign': "Use -1 (negative) or 1 (positive) depending "
-                       "on the sign of the spikes in the recording",
-        'common_ref_removal': 'remove common reference with median',
+        "freq_min": "High-pass filter cutoff frequency",
+        "freq_max": "Low-pass filter cutoff frequency",
+        "detect_threshold": "Threshold for spike detection",
+        "detect_sign": "Use -1 (negative) or 1 (positive) depending " "on the sign of the spikes in the recording",
+        "common_ref_removal": "remove common reference with median",
     }
 
     sorter_description = """Tridesclous is a template-matching spike sorter with a real-time engine.
     For more information see https://tridesclous.readthedocs.io"""
 
     installation_mesg = """\nTo use Tridesclous run:\n
        >>> pip install tridesclous
@@ -54,67 +53,75 @@
     # TODO make the TDC handle multi segment (should be easy)
     handle_multi_segment = True
 
     @classmethod
     def is_installed(cls):
         try:
             import tridesclous as tdc
+
             HAVE_TDC = True
         except ImportError:
             HAVE_TDC = False
         except:
-            print('tridesclous is installed, but it has some dependency problems, check numba or hdbscan installations!')
+            print(
+                "tridesclous is installed, but it has some dependency problems, check numba or hdbscan installations!"
+            )
             HAVE_TDC = False
         return HAVE_TDC
 
     @classmethod
     def get_sorter_version(cls):
         import tridesclous as tdc
+
         return tdc.__version__
 
     @classmethod
     def _check_params(cls, recording, sorter_output_folder, params):
         return params
 
     @classmethod
     def _setup_recording(cls, recording, sorter_output_folder, params, verbose):
         import tridesclous as tdc
 
         # save prb file
         probegroup = recording.get_probegroup()
-        prb_file = sorter_output_folder / 'probe.prb'
+        prb_file = sorter_output_folder / "probe.prb"
         write_prb(prb_file, probegroup)
 
         num_seg = recording.get_num_segments()
         sr = recording.get_sampling_frequency()
 
         if recording.binary_compatible_with(time_axis=0):
             # no copy
             d = recording.get_binary_description()
-            file_paths = d['file_paths']
-            dtype = str(d['dtype'])
-            num_chan = d['num_channels']
-            file_offset = d['file_offset']
+            file_paths = d["file_paths"]
+            dtype = str(d["dtype"])
+            num_channels = d["num_channels"]
+            file_offset = d["file_offset"]
         else:
             if verbose:
-                print('Local copy of recording')
+                print("Local copy of recording")
             # save binary file (chunk by chunk) into a new file
-            num_chan = recording.get_num_channels()
+            num_channels = recording.get_num_channels()
             dtype = recording.get_dtype().str
-            file_paths = [str(sorter_output_folder / f'raw_signals_{i}.raw') for i in range(num_seg)]
-            write_binary_recording(recording, file_paths=file_paths,
-                                   dtype=dtype, verbose=False, **get_job_kwargs(params, verbose))
+            file_paths = [str(sorter_output_folder / f"raw_signals_{i}.raw") for i in range(num_seg)]
+            write_binary_recording(recording, file_paths=file_paths, dtype=dtype, **get_job_kwargs(params, verbose))
             file_offset = 0
 
         # initialize source and probe file
         tdc_dataio = tdc.DataIO(dirname=str(sorter_output_folder))
 
-        tdc_dataio.set_data_source(type='RawData', filenames=file_paths,
-                                   dtype=dtype, sample_rate=float(sr),
-                                   total_channel=int(num_chan), offset=int(file_offset))
+        tdc_dataio.set_data_source(
+            type="RawData",
+            filenames=file_paths,
+            dtype=dtype,
+            sample_rate=float(sr),
+            total_channel=int(num_channels),
+            offset=int(file_offset),
+        )
         tdc_dataio.set_probe_file(str(prb_file))
         if verbose:
             print(tdc_dataio)
 
     @classmethod
     def _run_from_folder(cls, sorter_output_folder, params, verbose):
         import tridesclous as tdc
@@ -122,25 +129,24 @@
         tdc_dataio = tdc.DataIO(dirname=str(sorter_output_folder))
 
         params = params.copy()
 
         # make catalogue
         chan_grps = list(tdc_dataio.channel_groups.keys())
         for chan_grp in chan_grps:
-
             # parameters can change depending the group
             catalogue_nested_params = make_nested_tdc_params(tdc_dataio, chan_grp, **params)
 
             if verbose:
-                print('catalogue_nested_params')
+                print("catalogue_nested_params")
                 pprint(catalogue_nested_params)
 
             peeler_params = tdc.get_auto_params_for_peelers(tdc_dataio, chan_grp)
             if verbose:
-                print('peeler_params')
+                print("peeler_params")
                 pprint(peeler_params)
 
             cc = tdc.CatalogueConstructor(dataio=tdc_dataio, chan_grp=chan_grp)
             tdc.apply_all_catalogue_steps(cc, catalogue_nested_params, verbose=verbose)
 
             if verbose:
                 print(cc)
@@ -149,47 +155,47 @@
             initial_catalogue = tdc_dataio.load_catalogue(chan_grp=chan_grp)
             peeler = tdc.Peeler(tdc_dataio)
             peeler.change_params(catalogue=initial_catalogue, **peeler_params)
             t0 = time.perf_counter()
             peeler.run(duration=None, progressbar=False)
             if verbose:
                 t1 = time.perf_counter()
-                print('peeler.tun', t1 - t0)
+                print("peeler.tun", t1 - t0)
 
     @classmethod
     def _get_result_from_folder(cls, sorter_output_folder):
         sorting = TridesclousSortingExtractor(folder_path=sorter_output_folder)
         return sorting
 
 
 def make_nested_tdc_params(tdc_dataio, chan_grp, **new_params):
     import tridesclous as tdc
 
     params = tdc.get_auto_params_for_catalogue(tdc_dataio, chan_grp=chan_grp)
 
-    if 'freq_min' in new_params:
-        params['preprocessor']['highpass_freq'] = new_params['freq_min']
+    if "freq_min" in new_params:
+        params["preprocessor"]["highpass_freq"] = new_params["freq_min"]
 
-    if 'freq_max' in new_params:
-        params['preprocessor']['lowpass_freq'] = new_params['freq_max']
+    if "freq_max" in new_params:
+        params["preprocessor"]["lowpass_freq"] = new_params["freq_max"]
 
-    if 'common_ref_removal' in new_params:
-        params['preprocessor']['common_ref_removal'] = new_params['common_ref_removal']
+    if "common_ref_removal" in new_params:
+        params["preprocessor"]["common_ref_removal"] = new_params["common_ref_removal"]
 
-    if 'detect_sign' in new_params:
-        detect_sign = new_params['detect_sign']
+    if "detect_sign" in new_params:
+        detect_sign = new_params["detect_sign"]
         if detect_sign == -1:
-            params['peak_detector']['peak_sign'] = '-'
+            params["peak_detector"]["peak_sign"] = "-"
         elif detect_sign == 1:
-            params['peak_detector']['peak_sign'] = '+'
+            params["peak_detector"]["peak_sign"] = "+"
 
-    if 'detect_threshold' in new_params:
-        params['peak_detector']['relative_threshold'] = new_params['detect_threshold']
+    if "detect_threshold" in new_params:
+        params["peak_detector"]["relative_threshold"] = new_params["detect_threshold"]
 
-    nested_params = new_params.get('nested_params', None)
+    nested_params = new_params.get("nested_params", None)
     if nested_params is not None:
         for k, v in nested_params.items():
             if isinstance(v, dict):
                 params[k].update(v)
             else:
                 params[k] = v
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sorters/external/waveclus.py` & `spikeinterface-0.98.0/src/spikeinterface/sorters/external/waveclus.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,15 +1,14 @@
 from pathlib import Path
 import os
 from typing import Union
 import shutil
 import sys
 import json
 
-import scipy.io
 
 from ..basesorter import BaseSorter
 from ..utils import ShellScript
 
 from spikeinterface.core import write_to_h5_dataset_format
 from spikeinterface.extractors import WaveClusSortingExtractor
 from spikeinterface.core.channelslice import ChannelSliceRecording
@@ -30,85 +29,85 @@
         return False
     assert isinstance(waveclus_path, str)
 
     if waveclus_path.startswith('"'):
         waveclus_path = waveclus_path[1:-1]
     waveclus_path = str(Path(waveclus_path).absolute())
 
-    if (Path(waveclus_path) / 'wave_clus.m').is_file():
+    if (Path(waveclus_path) / "wave_clus.m").is_file():
         return True
     else:
         return False
 
 
 class WaveClusSorter(BaseSorter):
     """WaveClus Sorter object."""
 
-    sorter_name: str = 'waveclus'
-    compiled_name: str = 'waveclus_compiled'
-    waveclus_path: Union[str, None] = os.getenv('WAVECLUS_PATH', None)
+    sorter_name: str = "waveclus"
+    compiled_name: str = "waveclus_compiled"
+    waveclus_path: Union[str, None] = os.getenv("WAVECLUS_PATH", None)
     requires_locations = False
 
     _default_params = {
-        'detect_threshold': 5,
-        'detect_sign': -1,  # -1 - 1 - 0
-        'feature_type': 'wav',
-        'scales': 4,
-        'min_clus': 20,
-        'maxtemp': 0.251,
-        'template_sdnum': 3,
-        'enable_detect_filter': True,
-        'enable_sort_filter': True,
-        'detect_filter_fmin': 300,
-        'detect_filter_fmax': 3000,
-        'detect_filter_order': 4,
-        'sort_filter_fmin': 300,
-        'sort_filter_fmax': 3000,
-        'sort_filter_order': 2,
-        'mintemp': 0,
-        'w_pre': 20,
-        'w_post': 44,
-        'alignment_window': 10,
-        'stdmax': 50,
-        'max_spk': 40000,
-        'ref_ms': 1.5,
-        'interpolation': True,
-        'keep_good_only': True,
-        'chunk_memory': '500M'
+        "detect_threshold": 5,
+        "detect_sign": -1,  # -1 - 1 - 0
+        "feature_type": "wav",
+        "scales": 4,
+        "min_clus": 20,
+        "maxtemp": 0.251,
+        "template_sdnum": 3,
+        "enable_detect_filter": True,
+        "enable_sort_filter": True,
+        "detect_filter_fmin": 300,
+        "detect_filter_fmax": 3000,
+        "detect_filter_order": 4,
+        "sort_filter_fmin": 300,
+        "sort_filter_fmax": 3000,
+        "sort_filter_order": 2,
+        "mintemp": 0,
+        "w_pre": 20,
+        "w_post": 44,
+        "alignment_window": 10,
+        "stdmax": 50,
+        "max_spk": 40000,
+        "ref_ms": 1.5,
+        "interpolation": True,
+        "keep_good_only": True,
+        "chunk_memory": "500M",
     }
 
     _params_description = {
-        'detect_threshold': "Threshold for spike detection",
-        'detect_sign': "Use -1 (negative), 1 (positive), or 0 (both) depending "
-                       "on the sign of the spikes in the recording",
-        'feature_type': "wav (for wavelets) or pca, type of feature extraction applied to the spikes",
-        'scales': "Levels of the wavelet decomposition used as features",
-        'min_clus': "Minimum increase of cluster sizes used by the peak selection on the temperature map",
-        'maxtemp': "Maximum temperature calculated by the SPC method",
-        'template_sdnum': "Maximum distance (in total variance of the cluster) from the mean waveform to force a "
-                          "spike into a cluster",
-        'enable_detect_filter': "Enable or disable filter on detection",
-        'enable_sort_filter': "Enable or disable filter on sorting",
-        'detect_filter_fmin': "High-pass filter cutoff frequency for detection",
-        'detect_filter_fmax': "Low-pass filter cutoff frequency for detection",
-        'detect_filter_order': "Order of the detection filter",
-        'sort_filter_fmin': "High-pass filter cutoff frequency for sorting",
-        'sort_filter_fmax': "Low-pass filter cutoff frequency for sorting",
-        'sort_filter_order': "Order of the sorting filter",
-        'mintemp': "Minimum temperature calculated by the SPC algorithm",
-        'w_pre': "Number of samples from the beginning of the spike waveform up to (including) the peak",
-        'w_post': "Number of samples from the peak (excluding it) to the end of the waveform",
-        'alignment_window': "Number of samples between peaks of different channels",
-        'stdmax': "The events with a value over this number of noise standard deviations will be discarded",
-        'max_spk': "Maximum number of spikes used by the SPC algorithm",
-        'ref_ms': "Refractory time in milliseconds, all the threshold crossing inside this period are detected as the "
-                  "same spike",
-        'interpolation': "Enable or disable interpolation to improve the alignments of the spikes",
-        'keep_good_only': "If True only 'good' units are returned",
-        'chunk_memory': 'Chunk size in Mb to write h5 file (default 500Mb)'
+        "detect_threshold": "Threshold for spike detection",
+        "detect_sign": "Use -1 (negative), 1 (positive), or 0 (both) depending "
+        "on the sign of the spikes in the recording",
+        "feature_type": "wav (for wavelets) or pca, type of feature extraction applied to the spikes",
+        "scales": "Levels of the wavelet decomposition used as features",
+        "min_clus": "Minimum increase of cluster sizes used by the peak selection on the temperature map",
+        "maxtemp": "Maximum temperature calculated by the SPC method",
+        "template_sdnum": "Maximum distance (in total variance of the cluster) from the mean waveform to force a "
+        "spike into a cluster",
+        "enable_detect_filter": "Enable or disable filter on detection",
+        "enable_sort_filter": "Enable or disable filter on sorting",
+        "detect_filter_fmin": "High-pass filter cutoff frequency for detection",
+        "detect_filter_fmax": "Low-pass filter cutoff frequency for detection",
+        "detect_filter_order": "Order of the detection filter",
+        "sort_filter_fmin": "High-pass filter cutoff frequency for sorting",
+        "sort_filter_fmax": "Low-pass filter cutoff frequency for sorting",
+        "sort_filter_order": "Order of the sorting filter",
+        "mintemp": "Minimum temperature calculated by the SPC algorithm",
+        "w_pre": "Number of samples from the beginning of the spike waveform up to (including) the peak",
+        "w_post": "Number of samples from the peak (excluding it) to the end of the waveform",
+        "alignment_window": "Number of samples between peaks of different channels",
+        "stdmax": "The events with a value over this number of noise standard deviations will be discarded",
+        "max_spk": "Maximum number of spikes used by the SPC algorithm",
+        "ref_ms": "Refractory time in milliseconds, all the threshold crossing inside this period are detected as the "
+        "same spike",
+        "interpolation": "Enable or disable interpolation to improve the alignments of the spikes",
+        "keep_good_only": "If True only 'good' units are returned",
+        "chunk_memory": "Chunk size in Mb to write h5 file (default 500Mb)",
     }
 
     sorter_description = """Wave Clus combines a wavelet-based feature extraction and paramagnetic clustering with a
     template-matching approach. It is mainly designed for monotrodes and low-channel count probes.
     For more information see https://doi.org/10.1152/jn.00339.2018"""
 
     installation_mesg = """\nTo use WaveClus run:\n
@@ -125,20 +124,20 @@
         if cls.check_compiled():
             return True
         return check_if_installed(cls.waveclus_path)
 
     @classmethod
     def get_sorter_version(cls):
         if cls.check_compiled():
-            return 'compiled'
-        p = os.getenv('WAVECLUS_PATH', None)
+            return "compiled"
+        p = os.getenv("WAVECLUS_PATH", None)
         if p is None:
-            return 'unknown'
+            return "unknown"
         else:
-            with open(str(Path(p) / 'version.txt'), mode='r', encoding='utf8') as f:
+            with open(str(Path(p) / "version.txt"), mode="r", encoding="utf8") as f:
                 version = f.readline()
         return version
 
     @classmethod
     def set_waveclus_path(cls, waveclus_path: PathType):
         waveclus_path = str(Path(waveclus_path).absolute())
         WaveClusSorter.waveclus_path = waveclus_path
@@ -146,94 +145,106 @@
             print("Setting WAVECLUS_PATH environment variable for subprocess calls to:", waveclus_path)
             os.environ["WAVECLUS_PATH"] = waveclus_path
         except Exception as e:
             print("Could not set WAVECLUS_PATH environment variable:", e)
 
     @classmethod
     def _check_apply_filter_in_params(cls, params):
-        return (params['enable_detect_filter'] or params['enable_sort_filter'])
+        return params["enable_detect_filter"] or params["enable_sort_filter"]
 
     @classmethod
     def _setup_recording(cls, recording, sorter_output_folder, params, verbose):
         # Generate mat files in the dataset directory
         for nch, id in enumerate(recording.get_channel_ids()):
-            vcFile_h5 = str(sorter_output_folder / ('raw' + str(nch + 1) + '.h5'))
-            with h5py.File(vcFile_h5, mode='w') as f:
-                f.create_dataset(
-                    "sr", data=[recording.get_sampling_frequency()], dtype='float32')
+            vcFile_h5 = str(sorter_output_folder / ("raw" + str(nch + 1) + ".h5"))
+            with h5py.File(vcFile_h5, mode="w") as f:
+                f.create_dataset("sr", data=[recording.get_sampling_frequency()], dtype="float32")
                 rec_sliced = ChannelSliceRecording(recording, channel_ids=[id])
-                write_to_h5_dataset_format(rec_sliced, dataset_path='/data', segment_index=0,
-                                           file_handle=f, time_axis=0, single_axis=True,
-                                           chunk_memory=params['chunk_memory'], return_scaled=rec_sliced.has_scaled())
+                write_to_h5_dataset_format(
+                    rec_sliced,
+                    dataset_path="/data",
+                    segment_index=0,
+                    file_handle=f,
+                    time_axis=0,
+                    single_axis=True,
+                    chunk_memory=params["chunk_memory"],
+                    return_scaled=rec_sliced.has_scaled(),
+                )
 
         if verbose:
             samplerate = recording.get_sampling_frequency()
             num_timepoints = recording.get_num_frames(segment_index=0)
             num_channels = recording.get_num_channels()
             duration_minutes = num_timepoints / samplerate / 60
-            print('Num. channels = {}, Num. timepoints = {}, duration = {} minutes'.format(
-                num_channels, num_timepoints, duration_minutes))
+            print(
+                "Num. channels = {}, Num. timepoints = {}, duration = {} minutes".format(
+                    num_channels, num_timepoints, duration_minutes
+                )
+            )
 
     @classmethod
     def _run_from_folder(cls, sorter_output_folder, params, verbose):
         sorter_output_folder = sorter_output_folder.absolute()
 
         cls._generate_par_file(params, sorter_output_folder)
         if verbose:
-            print(f'Running waveclus in {sorter_output_folder}..')
+            print(f"Running waveclus in {sorter_output_folder}..")
 
         if cls.check_compiled():
-            shell_cmd = f'''
+            shell_cmd = f"""
                 #!/bin/bash
                 {cls.compiled_name} {sorter_output_folder}
-            '''
+            """
         else:
             source_dir = Path(__file__).parent
-            shutil.copy(str(source_dir / f'waveclus_master.m'), str(sorter_output_folder))
+            shutil.copy(str(source_dir / f"waveclus_master.m"), str(sorter_output_folder))
 
             sorter_path = Path(cls.waveclus_path).absolute()
-            if 'win' in sys.platform and sys.platform != 'darwin':
+            if "win" in sys.platform and sys.platform != "darwin":
                 disk_move = str(sorter_output_folder.absolute())[:2]
-                shell_cmd = f'''
+                shell_cmd = f"""
                     {disk_move}
                     cd {sorter_output_folder}
                     matlab -nosplash -wait -log -r "waveclus_master('{sorter_output_folder}', '{sorter_path}')"
-                '''
+                """
             else:
-                shell_cmd = f'''
+                shell_cmd = f"""
                     #!/bin/bash
                     cd "{sorter_output_folder}"
                     matlab -nosplash -nodisplay -log -r "waveclus_master('{sorter_output_folder}', '{sorter_path}')"
-                '''
-        shell_cmd = ShellScript(shell_cmd, script_path=sorter_output_folder / f'run_{cls.sorter_name}',
-                                log_path=sorter_output_folder / f'{cls.sorter_name}.log', verbose=verbose)
+                """
+        shell_cmd = ShellScript(
+            shell_cmd,
+            script_path=sorter_output_folder / f"run_{cls.sorter_name}",
+            log_path=sorter_output_folder / f"{cls.sorter_name}.log",
+            verbose=verbose,
+        )
         shell_cmd.start()
         retcode = shell_cmd.wait()
 
         if retcode != 0:
-            raise Exception('waveclus returned a non-zero exit code')
+            raise Exception("waveclus returned a non-zero exit code")
 
-        result_fname = sorter_output_folder / 'times_results.mat'
+        result_fname = sorter_output_folder / "times_results.mat"
         if not result_fname.is_file():
-            raise Exception(f'Result file does not exist: {result_fname}')
+            raise Exception(f"Result file does not exist: {result_fname}")
 
     @classmethod
     def _get_result_from_folder(cls, sorter_output_folder):
         sorter_output_folder = Path(sorter_output_folder)
-        result_fname = str(sorter_output_folder / 'times_results.mat')
-        if (sorter_output_folder.parent / 'spikeinterface_params.json').is_file():
-            params_file = sorter_output_folder.parent / 'spikeinterface_params.json'
+        result_fname = str(sorter_output_folder / "times_results.mat")
+        if (sorter_output_folder.parent / "spikeinterface_params.json").is_file():
+            params_file = sorter_output_folder.parent / "spikeinterface_params.json"
         else:
             # back-compatibility
-            params_file = sorter_output_folder / 'spikeinterface_params.json'
-        with params_file.open('r') as f:
-            sorter_params = json.load(f)['sorter_params']
-        keep_good_only = sorter_params.get('keep_good_only', True)
-        sorting = WaveClusSortingExtractor(
-            file_path=result_fname, keep_good_only=keep_good_only)
+            params_file = sorter_output_folder / "spikeinterface_params.json"
+        with params_file.open("r") as f:
+            sorter_params = json.load(f)["sorter_params"]
+        keep_good_only = sorter_params.get("keep_good_only", True)
+        sorting = WaveClusSortingExtractor(file_path=result_fname, keep_good_only=keep_good_only)
         return sorting
 
     @staticmethod
     def _generate_par_file(params, sorter_output_folder):
         """
         This function generates parameters data for waveclus and saves as `par_input.mat`
 
@@ -244,48 +255,56 @@
         ----------
         params: dict
             Custom parameters dictionary for waveclus
         sorter_output_folder: pathlib.Path
             Path object to save `par_input.mat`
         """
         p = params.copy()
-        if p['detect_sign'] < 0:
-            p['detect_sign'] = 'neg'
-        elif p['detect_sign'] > 0:
-            p['detect_sign'] = 'pos'
+        if p["detect_sign"] < 0:
+            p["detect_sign"] = "neg"
+        elif p["detect_sign"] > 0:
+            p["detect_sign"] = "pos"
         else:
-            p['detect_sign'] = 'both'
+            p["detect_sign"] = "both"
 
-        if not p['enable_detect_filter']:
-            p['detect_filter_order'] = 0
-        del p['enable_detect_filter']
-
-        if not p['enable_sort_filter']:
-            p['sort_filter_order'] = 0
-        del p['enable_sort_filter']
+        if not p["enable_detect_filter"]:
+            p["detect_filter_order"] = 0
+        del p["enable_detect_filter"]
+
+        if not p["enable_sort_filter"]:
+            p["sort_filter_order"] = 0
+        del p["enable_sort_filter"]
 
-        if p['interpolation']:
-            p['interpolation'] = 'y'
+        if p["interpolation"]:
+            p["interpolation"] = "y"
         else:
-            p['interpolation'] = 'n'
+            p["interpolation"] = "n"
 
-        par_renames = {'detect_sign': 'detection', 'detect_threshold': 'stdmin',
-                       'feature_type': 'features', 'detect_filter_fmin': 'detect_fmin',
-                       'detect_filter_fmax': 'detect_fmax', 'detect_filter_order': 'detect_order',
-                       'sort_filter_fmin': 'sort_fmin', 'sort_filter_fmax': 'sort_fmax',
-                       'sort_filter_order': 'sort_order'}
+        par_renames = {
+            "detect_sign": "detection",
+            "detect_threshold": "stdmin",
+            "feature_type": "features",
+            "detect_filter_fmin": "detect_fmin",
+            "detect_filter_fmax": "detect_fmax",
+            "detect_filter_order": "detect_order",
+            "sort_filter_fmin": "sort_fmin",
+            "sort_filter_fmax": "sort_fmax",
+            "sort_filter_order": "sort_order",
+        }
         par_input = {}
         for key, value in p.items():
             if type(value) == bool:
-                value = '{}'.format(value).lower()
+                value = "{}".format(value).lower()
             if key in par_renames:
                 key = par_renames[key]
             par_input[key] = value
 
         # Converting integer values into float
         # matlab interprets numerical fields as double by default
         for k, v in par_input.items():
             if isinstance(v, int):
                 par_input[k] = float(v)
 
-        par_input = {'par_input': par_input}
-        scipy.io.savemat(str(sorter_output_folder / 'par_input.mat'), par_input)
+        par_input = {"par_input": par_input}
+        import scipy.io
+
+        scipy.io.savemat(str(sorter_output_folder / "par_input.mat"), par_input)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sorters/external/waveclus_master.m` & `spikeinterface-0.98.0/src/spikeinterface/sorters/external/waveclus_master.m`

 * *Ordering differences only*

 * *Files 0% similar despite different names*

```diff
@@ -127,8 +127,8 @@
 
     % TEMPLATE MATCHING
     par.match = 'y';                    % for template matching
     %par.match = 'n';                   % for no template matching
     par.max_spk = 40000;                % max. # of spikes before starting templ. match.
     par.permut = 'y';                   % for selection of random 'par.max_spk' spikes before starting templ. match.
     % par.permut = 'n';                 % for selection of the first 'par.max_spk' spikes before starting templ. match.
-end
+end
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sorters/external/waveclus_snippets.py` & `spikeinterface-0.98.0/src/spikeinterface/sorters/external/waveclus_snippets.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,72 +1,72 @@
 from pathlib import Path
 import os
 from typing import Union
 import shutil
 import sys
 import json
 
-import scipy.io
 
 from ..basesorter import BaseSorter
 from ..utils import ShellScript
 
 from spikeinterface.extractors import WaveClusSortingExtractor
 from spikeinterface.extractors import WaveClusSnippetsExtractor
+
 PathType = Union[str, Path]
 
 
 def check_if_installed(waveclus_path: Union[str, None]):
     if waveclus_path is None:
         return False
     assert isinstance(waveclus_path, str)
 
     if waveclus_path.startswith('"'):
         waveclus_path = waveclus_path[1:-1]
     waveclus_path = str(Path(waveclus_path).absolute())
 
-    if (Path(waveclus_path) / 'wave_clus.m').is_file():
+    if (Path(waveclus_path) / "wave_clus.m").is_file():
         return True
     else:
         return False
 
 
 class WaveClusSnippetsSorter(BaseSorter):
     """WaveClus Sorter object."""
 
-    sorter_name: str = 'waveclus_snippets'
-    compiled_name: str = 'waveclus_snippets_compiled'
-    waveclus_path: Union[str, None] = os.getenv('WAVECLUS_PATH', None)
+    sorter_name: str = "waveclus_snippets"
+    compiled_name: str = "waveclus_snippets_compiled"
+    waveclus_path: Union[str, None] = os.getenv("WAVECLUS_PATH", None)
     requires_locations = False
 
     _default_params = {
-        'feature_type': 'wav',
-        'scales': 4,
-        'min_clus': 20,
-        'maxtemp': 0.251,
-        'template_sdnum': 3,
-        'mintemp': 0,
-        'stdmax': 50,
-        'max_spk': 40000,
-        'keep_good_only': True,
-        'chunk_memory': '500M'
+        "feature_type": "wav",
+        "scales": 4,
+        "min_clus": 20,
+        "maxtemp": 0.251,
+        "template_sdnum": 3,
+        "mintemp": 0,
+        "stdmax": 50,
+        "max_spk": 40000,
+        "keep_good_only": True,
+        "chunk_memory": "500M",
     }
 
     _params_description = {
-        'feature_type': "wav (for wavelets) or pca, type of feature extraction applied to the spikes",
-        'scales': "Levels of the wavelet decomposition used as features",
-        'min_clus': "Minimum increase of cluster sizes used by the peak selection on the temperature map",
-        'maxtemp': "Maximum temperature calculated by the SPC method",
-        'template_sdnum': "Maximum distance (in total variance of the cluster) from the mean waveform to force a "
-                          "spike into a cluster",
-        'mintemp': "Minimum temperature calculated by the SPC algorithm",
-        'stdmax': "The events with a value over this number of noise standard deviations will be discarded",
-        'max_spk': "Maximum number of spikes used by the SPC algorithm",
-        'keep_good_only': "If True only 'good' units are returned",
-        'chunk_memory': 'Chunk size in Mb to write h5 file (default 500Mb)'
+        "feature_type": "wav (for wavelets) or pca, type of feature extraction applied to the spikes",
+        "scales": "Levels of the wavelet decomposition used as features",
+        "min_clus": "Minimum increase of cluster sizes used by the peak selection on the temperature map",
+        "maxtemp": "Maximum temperature calculated by the SPC method",
+        "template_sdnum": "Maximum distance (in total variance of the cluster) from the mean waveform to force a "
+        "spike into a cluster",
+        "mintemp": "Minimum temperature calculated by the SPC algorithm",
+        "stdmax": "The events with a value over this number of noise standard deviations will be discarded",
+        "max_spk": "Maximum number of spikes used by the SPC algorithm",
+        "keep_good_only": "If True only 'good' units are returned",
+        "chunk_memory": "Chunk size in Mb to write h5 file (default 500Mb)",
     }
 
     sorter_description = """Wave Clus combines a wavelet-based feature extraction and paramagnetic clustering with a
     template-matching approach. It is mainly designed for monotrodes and low-channel count probes.
     For more information see https://doi.org/10.1152/jn.00339.2018"""
 
     installation_mesg = """\nTo use WaveClus run:\n
@@ -83,20 +83,20 @@
         if cls.check_compiled():
             return True
         return check_if_installed(cls.waveclus_path)
 
     @classmethod
     def get_sorter_version(cls):
         if cls.check_compiled():
-            return 'compiled'
-        p = os.getenv('WAVECLUS_PATH', None)
+            return "compiled"
+        p = os.getenv("WAVECLUS_PATH", None)
         if p is None:
-            return 'unknown'
+            return "unknown"
         else:
-            with open(str(Path(p) / 'version.txt'), mode='r', encoding='utf8') as f:
+            with open(str(Path(p) / "version.txt"), mode="r", encoding="utf8") as f:
                 version = f.readline()
         return version
 
     @classmethod
     def set_waveclus_path(cls, waveclus_path: PathType):
         waveclus_path = str(Path(waveclus_path).absolute())
         WaveClusSnippetsSorter.waveclus_path = waveclus_path
@@ -110,79 +110,81 @@
     def _check_apply_filter_in_params(cls, params):
         return False
 
     @classmethod
     def _setup_recording(cls, snippets, sorter_output_folder, params, verbose):
         # Generate mat files in the dataset directory
 
-        WaveClusSnippetsExtractor.write_snippets(snippets, sorter_output_folder / 'results_spikes.mat')
+        WaveClusSnippetsExtractor.write_snippets(snippets, sorter_output_folder / "results_spikes.mat")
 
         if verbose:
             num_snippets = snippets.get_total_snippets()
             num_channels = snippets.get_num_channels()
-            print('Num. channels = {}, Num. snippets = {}'.format(
-                num_channels, num_snippets))
+            print("Num. channels = {}, Num. snippets = {}".format(num_channels, num_snippets))
 
     @classmethod
     def _run_from_folder(cls, sorter_output_folder, params, verbose):
         sorter_output_folder = sorter_output_folder.absolute()
 
         cls._generate_par_file(params, sorter_output_folder)
         if verbose:
-            print(f'Running waveclus in {sorter_output_folder}..')
+            print(f"Running waveclus in {sorter_output_folder}..")
 
         if cls.check_compiled():
-            shell_cmd = f'''
+            shell_cmd = f"""
                 #!/bin/bash
                 {cls.compiled_name} {sorter_output_folder}
-            '''
+            """
         else:
             source_dir = Path(__file__).parent
-            shutil.copy(str(source_dir / f'waveclus_snippets_master.m'), str(sorter_output_folder))
+            shutil.copy(str(source_dir / f"waveclus_snippets_master.m"), str(sorter_output_folder))
 
             sorter_path = Path(cls.waveclus_path).absolute()
-            if 'win' in sys.platform and sys.platform != 'darwin':
+            if "win" in sys.platform and sys.platform != "darwin":
                 disk_move = str(sorter_output_folder.absolute())[:2]
-                shell_cmd = f'''
+                shell_cmd = f"""
                     {disk_move}
                     cd {sorter_output_folder}
                     matlab -nosplash -wait -log -r "waveclus_snippets_master('{sorter_output_folder}', '{sorter_path}')"
-                '''
+                """
             else:
-                shell_cmd = f'''
+                shell_cmd = f"""
                     #!/bin/bash
                     cd "{sorter_output_folder}"
                     matlab -nosplash -nodisplay -log -r "waveclus_snippets_master('{sorter_output_folder}', '{sorter_path}')"
-                '''
-        shell_cmd = ShellScript(shell_cmd, script_path=sorter_output_folder / f'run_{cls.sorter_name}',
-                                log_path=sorter_output_folder / f'{cls.sorter_name}.log', verbose=verbose)
+                """
+        shell_cmd = ShellScript(
+            shell_cmd,
+            script_path=sorter_output_folder / f"run_{cls.sorter_name}",
+            log_path=sorter_output_folder / f"{cls.sorter_name}.log",
+            verbose=verbose,
+        )
         shell_cmd.start()
         retcode = shell_cmd.wait()
 
         if retcode != 0:
-            raise Exception('waveclus returned a non-zero exit code')
+            raise Exception("waveclus returned a non-zero exit code")
 
-        result_fname = sorter_output_folder / 'times_results.mat'
+        result_fname = sorter_output_folder / "times_results.mat"
         if not result_fname.is_file():
-            raise Exception(f'Result file does not exist: {result_fname}')
+            raise Exception(f"Result file does not exist: {result_fname}")
 
     @classmethod
     def _get_result_from_folder(cls, sorter_output_folder):
         sorter_output_folder = Path(sorter_output_folder)
-        result_fname = str(sorter_output_folder / 'times_results.mat')
-        if (sorter_output_folder.parent / 'spikeinterface_params.json').is_file():
-            params_file = sorter_output_folder.parent / 'spikeinterface_params.json'
+        result_fname = str(sorter_output_folder / "times_results.mat")
+        if (sorter_output_folder.parent / "spikeinterface_params.json").is_file():
+            params_file = sorter_output_folder.parent / "spikeinterface_params.json"
         else:
             # back-compatibility
-            params_file = sorter_output_folder / 'spikeinterface_params.json'
-        with params_file.open('r') as f:
-            sorter_params = json.load(f)['sorter_params']
-        keep_good_only = sorter_params.get('keep_good_only', True)
-        sorting = WaveClusSortingExtractor(
-            file_path=result_fname, keep_good_only=keep_good_only)
+            params_file = sorter_output_folder / "spikeinterface_params.json"
+        with params_file.open("r") as f:
+            sorter_params = json.load(f)["sorter_params"]
+        keep_good_only = sorter_params.get("keep_good_only", True)
+        sorting = WaveClusSortingExtractor(file_path=result_fname, keep_good_only=keep_good_only)
         return sorting
 
     @staticmethod
     def _generate_par_file(params, sorter_output_folder):
         """
         This function generates parameters data for waveclus and saves as `par_input.mat`
 
@@ -194,24 +196,26 @@
         params: dict
             Custom parameters dictionary for waveclus
         sorter_output_folder: pathlib.Path
             Path object to save `par_input.mat`
         """
         p = params.copy()
 
-        par_renames = {'feature_type': 'features'}
+        par_renames = {"feature_type": "features"}
         par_input = {}
         for key, value in p.items():
             if type(value) == bool:
-                value = '{}'.format(value).lower()
+                value = "{}".format(value).lower()
             if key in par_renames:
                 key = par_renames[key]
             par_input[key] = value
 
         # Converting integer values into float
         # matlab interprets numerical fields as double by default
         for k, v in par_input.items():
             if isinstance(v, int):
                 par_input[k] = float(v)
 
-        par_input = {'par_input': par_input}
-        scipy.io.savemat(str(sorter_output_folder / 'par_input.mat'), par_input)
+        par_input = {"par_input": par_input}
+        import scipy.io
+
+        scipy.io.savemat(str(sorter_output_folder / "par_input.mat"), par_input)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sorters/external/waveclus_snippets_master.m` & `spikeinterface-0.98.0/src/spikeinterface/sorters/external/waveclus_snippets_master.m`

 * *Ordering differences only*

 * *Files 0% similar despite different names*

```diff
@@ -71,8 +71,8 @@
 
     % TEMPLATE MATCHING
     par.match = 'y';                    % for template matching
     %par.match = 'n';                   % for no template matching
     par.max_spk = 40000;                % max. # of spikes before starting templ. match.
     par.permut = 'y';                   % for selection of random 'par.max_spk' spikes before starting templ. match.
     % par.permut = 'n';                 % for selection of the first 'par.max_spk' spikes before starting templ. match.
-end
+end
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sorters/external/yass.py` & `spikeinterface-0.98.0/src/spikeinterface/sorters/external/yass.py`

 * *Files 19% similar despite different names*

```diff
@@ -2,96 +2,86 @@
 import os
 import numpy as np
 import sys
 
 from ..basesorter import BaseSorter, get_job_kwargs
 from ..utils import ShellScript
 
-from spikeinterface.core import load_extractor
-
 from spikeinterface.core import write_binary_recording
 from spikeinterface.extractors import YassSortingExtractor
 
 
 class YassSorter(BaseSorter):
     """YASS Sorter object."""
 
-    sorter_name = 'yass'
+    sorter_name = "yass"
     requires_locations = False
-    gpu_capability = 'nvidia-required'
+    gpu_capability = "nvidia-required"
     requires_binary_data = True
 
     # #################################################
 
     _default_params = {
-        'dtype': 'int16',  # the only datatype that Yass currently accepts;
-
+        "dtype": "int16",  # the only datatype that Yass currently accepts;
         # Filtering and processing params
-        'freq_min': 300,  # "High-pass filter cutoff frequency",
-        'freq_max': 0.3,  # "Low-pass filter cutoff frequency as proportion of sampling rate",
-        'neural_nets_path': None,  # default NNs are set to None - Yass will always retrain on dataset;
-        'multi_processing': 1,  # 0: single core; 1: multi CPU core
-        'n_processors': 1,  # default is a single core; autosearch for more cores
-        'n_gpu_processors': 1,  # default is the first installed GPU
-        'n_sec_chunk': 10,  # Length of processing chunk in seconds for multi-processing stages
-        'n_sec_chunk_gpu_detect': 0.5,  # n_sec_chunk for gpu detection (lower if you get memory error during detection)
-        'n_sec_chunk_gpu_deconv': 5,  # n_sec_chunk for gpu deconvolution (lower if you get memory error during deconv)
-        'gpu_id': 0,  # which gpu to use, default is 0, i.e. first gpu;
-        'generate_phy': 0,  # generate phy visualization files; 0 - do not run; 1: generate phy files
-        'phy_percent_spikes': 0.05,
+        "freq_min": 300,  # "High-pass filter cutoff frequency",
+        "freq_max": 0.3,  # "Low-pass filter cutoff frequency as proportion of sampling rate",
+        "neural_nets_path": None,  # default NNs are set to None - Yass will always retrain on dataset;
+        "multi_processing": 1,  # 0: single core; 1: multi CPU core
+        "n_processors": 1,  # default is a single core; autosearch for more cores
+        "n_gpu_processors": 1,  # default is the first installed GPU
+        "n_sec_chunk": 10,  # Length of processing chunk in seconds for multi-processing stages
+        "n_sec_chunk_gpu_detect": 0.5,  # n_sec_chunk for gpu detection (lower if you get memory error during detection)
+        "n_sec_chunk_gpu_deconv": 5,  # n_sec_chunk for gpu deconvolution (lower if you get memory error during deconv)
+        "gpu_id": 0,  # which gpu to use, default is 0, i.e. first gpu;
+        "generate_phy": 0,  # generate phy visualization files; 0 - do not run; 1: generate phy files
+        "phy_percent_spikes": 0.05,
         # generate phy visualization files; ratio of spikes that are processed for phy visualization
         # decrease if memory issues are present
-
         # params related to NN and clustering;
-        'spatial_radius': 70,  # channels spatial radius to consider them neighbors, see
+        "spatial_radius": 70,  # channels spatial radius to consider them neighbors, see
         # yass.geometry.find_channel_neighbors for details
-
-        'spike_size_ms': 5,  # temporal length of templates in ms. It must capture
+        "spike_size_ms": 5,  # temporal length of templates in ms. It must capture
         # the full shape of waveforms on all channels
         # (reminder: there is a propagation delay in waveform shape across channels)
         # but longer means slower
-        'clustering_chunk': [0, 300],  # time (in sec) to run clustering and get initial templates
+        "clustering_chunk": [0, 300],  # time (in sec) to run clustering and get initial templates
         # leave blank to run clustering step on entire recording;
         # deconv is then run on the entire dataset using clustering stage templates
-
         # Params for deconv stage
-        'update_templates': 0,  # update templates during deconvolution step
-        'neuron_discover': 0,  # recluster during deconvolution and search for new stable neurons;
-        'template_update_time': 300,  # if templates being updated, time (in sec) of segment in which to search for
+        "update_templates": 0,  # update templates during deconvolution step
+        "neuron_discover": 0,  # recluster during deconvolution and search for new stable neurons;
+        "template_update_time": 300,  # if templates being updated, time (in sec) of segment in which to search for
         # new clusters
     }
 
     _params_description = {
-
-        'dtype': 'int16 : the only datatype that Yass currently accepts',
-
+        "dtype": "int16 : the only datatype that Yass currently accepts",
         # Filtering and processing params
-        'freq_min': "300; High-pass filter cutoff frequency",
-        'freq_max': "0.3; Low-pass filter cutoff frequency as proportion of sampling rate",
-        'neural_nets_path': ' None;  default NNs are set to None - Yass will always retrain on dataset',
-        'multi_processing': '1; 0: single core; 1: multi CPU core',
-        'n_processors': ' 1; default is a single core; TODO: auto-detect # of corse on node',
-        'n_gpu_processors': '1: default is the first installed GPU',
-        'n_sec_chunk': '10;  Length of processing chunk in seconds for multi-processing stages. Lower this if running out of memory',
-        'n_sec_chunk_gpu_detect': '0.5; n_sec_chunk for gpu detection (lower if you get memory error during detection)',
-        'n_sec_chunk_gpu_deconv': '5; n_sec_chunk for gpu deconvolution (lower if you get memory error during deconv)',
-        'gpu_id': '0; which gpu ID to use, default is 0, i.e. first gpu',
-        'generate_phy': '1; generate phy visualization files; 0 - do not run; 1: generate phy files',
-        'phy_percent_spikes': '0.05;  ratio of spikes that are processed for phy visualization; decrease if memory issues are present',
-
+        "freq_min": "300; High-pass filter cutoff frequency",
+        "freq_max": "0.3; Low-pass filter cutoff frequency as proportion of sampling rate",
+        "neural_nets_path": " None;  default NNs are set to None - Yass will always retrain on dataset",
+        "multi_processing": "1; 0: single core; 1: multi CPU core",
+        "n_processors": " 1; default is a single core; TODO: auto-detect # of corse on node",
+        "n_gpu_processors": "1: default is the first installed GPU",
+        "n_sec_chunk": "10;  Length of processing chunk in seconds for multi-processing stages. Lower this if running out of memory",
+        "n_sec_chunk_gpu_detect": "0.5; n_sec_chunk for gpu detection (lower if you get memory error during detection)",
+        "n_sec_chunk_gpu_deconv": "5; n_sec_chunk for gpu deconvolution (lower if you get memory error during deconv)",
+        "gpu_id": "0; which gpu ID to use, default is 0, i.e. first gpu",
+        "generate_phy": "1; generate phy visualization files; 0 - do not run; 1: generate phy files",
+        "phy_percent_spikes": "0.05;  ratio of spikes that are processed for phy visualization; decrease if memory issues are present",
         # params related to NN and clustering;
-        'spatial_radius': '70; spatial radius to consider 2 channels neighbors; required for NN stages to work',
-        'spike_size_ms': '5; temporal length of templates in ms; longer is more processing time, but slight more accurate',
+        "spatial_radius": "70; spatial radius to consider 2 channels neighbors; required for NN stages to work",
+        "spike_size_ms": "5; temporal length of templates in ms; longer is more processing time, but slight more accurate",
         # but longer means slower
-        'clustering_chunk': '[0, 300]; period of time (in sec) to run clustering and get initial templates; leave blank to run clustering step on entire recording;',
-
+        "clustering_chunk": "[0, 300]; period of time (in sec) to run clustering and get initial templates; leave blank to run clustering step on entire recording;",
         # Params for deconv stage
-        'update_templates': '0; update templates during deconvolution step 1; do not update 0',
-        'neuron_discover': '0, recluster during deconvolution and search for new stable neurons: 1; do not recluster 0',
-        'template_update_time': '300; if reculstiner on, time (in sec) of segment in which to search for new clusters ',
+        "update_templates": "0; update templates during deconvolution step 1; do not update 0",
+        "neuron_discover": "0, recluster during deconvolution and search for new stable neurons: 1; do not recluster 0",
+        "template_update_time": "300; if reculstiner on, time (in sec) of segment in which to search for new clusters ",
     }
 
     # #################################################
 
     sorter_description = """Yass is a deconvolution and neural network based spike sorting algorithm designed for
                             recordings with no drift (such as retinal recordings).
 
@@ -117,153 +107,163 @@
                         """
 
     @classmethod
     def is_installed(cls):
         try:
             import yaml
             import yass
+
             HAVE_YASS = True
         except ImportError:
             HAVE_YASS = False
         return HAVE_YASS
 
     @classmethod
     def get_sorter_version(cls):
         import yass
+
         return yass.__version__
 
     @classmethod
     def _setup_recording(cls, recording, sorter_output_folder, params, verbose):
         import yaml
 
         p = params
 
         source_dir = Path(__file__).parent
-        config_default_location = os.path.join(source_dir, 'yass_config_default.yaml')
+        config_default_location = os.path.join(source_dir, "yass_config_default.yaml")
 
         with open(config_default_location) as file:
             yass_params = yaml.load(file, Loader=yaml.FullLoader)
 
             # update root folder
-        yass_params['data']['root_folder'] = str(sorter_output_folder.absolute())
+        yass_params["data"]["root_folder"] = str(sorter_output_folder.absolute())
 
         #  geometry
-        probe_file_txt = os.path.join(sorter_output_folder, 'geom.txt')
+        probe_file_txt = os.path.join(sorter_output_folder, "geom.txt")
         geom_txt = recording.get_channel_locations()
         np.savetxt(probe_file_txt, geom_txt)
 
         #   params
-        yass_params['recordings']['sampling_rate'] = recording.get_sampling_frequency()
-        yass_params['recordings']['n_channels'] = recording.get_num_channels()
+        yass_params["recordings"]["sampling_rate"] = recording.get_sampling_frequency()
+        yass_params["recordings"]["n_channels"] = recording.get_num_channels()
 
         # save to int16 raw
-        input_file_path = os.path.join(sorter_output_folder, 'data.bin')
-        dtype = 'int16'  # HARD CODE THIS FOR YASS
-        input_file_path = sorter_output_folder / 'data.bin'
-        
+        input_file_path = os.path.join(sorter_output_folder, "data.bin")
+        dtype = "int16"  # HARD CODE THIS FOR YASS
+        input_file_path = sorter_output_folder / "data.bin"
+
         write_binary_recording(recording, file_paths=[input_file_path], dtype=dtype, **get_job_kwargs(params, verbose))
 
         retrain = False
-        if params['neural_nets_path'] is None:
-            params['neural_nets_path'] = str(sorter_output_folder / 'tmp' / 'nn_train')
+        if params["neural_nets_path"] is None:
+            params["neural_nets_path"] = str(sorter_output_folder / "tmp" / "nn_train")
             retrain = True
 
         # MERGE yass_params with self.params that could be changed by the user
         merge_params = merge_params_dict(yass_params, params)
 
         # to yaml
-        fname_config = sorter_output_folder / 'config.yaml'
-        with open(fname_config, 'w') as file:
+        fname_config = sorter_output_folder / "config.yaml"
+        with open(fname_config, "w") as file:
             documents = yaml.dump(merge_params, file)
 
         # RunNN training on existing
-        neural_nets_path = p['neural_nets_path']
+        neural_nets_path = p["neural_nets_path"]
 
         if retrain:
             # retrain NNs
             YassSorter.train(recording, sorter_output_folder, verbose)
 
             # update NN folder location
-            neural_nets_path = sorter_output_folder / 'tmp' / 'nn_train'
+            neural_nets_path = sorter_output_folder / "tmp" / "nn_train"
         else:
             #   load previous NNs
             if verbose:
-                print("USING PREVIOUSLY TRAINED NNs FROM THIS LOCATION: ", params['neural_nets_path'])
+                print("USING PREVIOUSLY TRAINED NNs FROM THIS LOCATION: ", params["neural_nets_path"])
             # use previously trained NN folder location
-            neural_nets_path = Path(params['neural_nets_path'])
+            neural_nets_path = Path(params["neural_nets_path"])
 
-        merge_params['neuralnetwork']['denoise']['filename'] = str(neural_nets_path.absolute() / 'denoise.pt')
-        merge_params['neuralnetwork']['detect']['filename'] = str(neural_nets_path.absolute() / 'detect.pt')
+        merge_params["neuralnetwork"]["denoise"]["filename"] = str(neural_nets_path.absolute() / "denoise.pt")
+        merge_params["neuralnetwork"]["detect"]["filename"] = str(neural_nets_path.absolute() / "detect.pt")
 
         # to yaml again (for NNs update)
-        fname_config = sorter_output_folder / 'config.yaml'
-        with open(fname_config, 'w') as file:
+        fname_config = sorter_output_folder / "config.yaml"
+        with open(fname_config, "w") as file:
             yaml.dump(merge_params, file)
 
     @classmethod
     def _run_from_folder(cls, sorter_output_folder, params, verbose):
-        '''
-        '''
-        config_file = sorter_output_folder.absolute() / 'config.yaml'
-        if 'win' in sys.platform and sys.platform != 'darwin':
-            shell_cmd = f'''yass sort {config_file}'''
+        """ """
+        config_file = sorter_output_folder.absolute() / "config.yaml"
+        if "win" in sys.platform and sys.platform != "darwin":
+            shell_cmd = f"""yass sort {config_file}"""
         else:
-            shell_cmd = f'''
+            shell_cmd = f"""
                         #!/bin/bash
-                        yass sort {config_file}'''
+                        yass sort {config_file}"""
 
-        shell_script = ShellScript(shell_cmd,
-                                   #  script_path=os.path.join(sorter_output_folder, self.sorter_name),
-                                   script_path=sorter_output_folder / 'run_yass',
-                                   log_path=sorter_output_folder / (cls.sorter_name + '.log'),
-                                   verbose=verbose)
+        shell_script = ShellScript(
+            shell_cmd,
+            #  script_path=os.path.join(sorter_output_folder, self.sorter_name),
+            script_path=sorter_output_folder / "run_yass",
+            log_path=sorter_output_folder / (cls.sorter_name + ".log"),
+            verbose=verbose,
+        )
         shell_script.start()
 
         retcode = shell_script.wait()
 
         if retcode != 0:
-            raise Exception('yass returned a non-zero exit code')
+            raise Exception("yass returned a non-zero exit code")
 
     # Alessio might not want to put here;
     # better option to have a parameter "tune_nn" which
     @classmethod
     def train(cls, recording, sorter_output_folder, verbose):
-        ''' Train NNs on yass prior to running yass sort'''
+        """Train NNs on yass prior to running yass sort"""
 
         if verbose:
             print(
-                "TRAINING YASS (Note: using default spike width, neighbour chan radius; to change, see parameter files)")
+                "TRAINING YASS (Note: using default spike width, neighbour chan radius; to change, see parameter files)"
+            )
             print("To use previously-trained NNs, change the NNs prior to running: ")
             print("            ss.set_NNs('path_to_NNs') (or set params['neural_nets_path'] = path_toNNs)")
             print("prior to running ss.run_sorter()")
 
-        config_file = sorter_output_folder.absolute() / 'config.yaml'
-        if 'win' in sys.platform and sys.platform != 'darwin':
-            shell_cmd = f'yass train {config_file}'
+        config_file = sorter_output_folder.absolute() / "config.yaml"
+        if "win" in sys.platform and sys.platform != "darwin":
+            shell_cmd = f"yass train {config_file}"
         else:
-            shell_cmd = f'''
+            shell_cmd = f"""
                         #!/bin/bash
-                        yass train {config_file}'''
+                        yass train {config_file}"""
 
-        shell_script = ShellScript(shell_cmd,
-                                   script_path=sorter_output_folder / 'run_yass_train',
-                                   # os.path.join(sorter_output_folder, cls.sorter_name),
-                                   log_path=sorter_output_folder / (cls.sorter_name + '_train.log'),
-                                   verbose=verbose)
+        shell_script = ShellScript(
+            shell_cmd,
+            script_path=sorter_output_folder / "run_yass_train",
+            # os.path.join(sorter_output_folder, cls.sorter_name),
+            log_path=sorter_output_folder / (cls.sorter_name + "_train.log"),
+            verbose=verbose,
+        )
         shell_script.start()
 
         retcode = shell_script.wait()
 
         if retcode != 0:
-            raise Exception('yass returned a non-zero exit code')
+            raise Exception("yass returned a non-zero exit code")
 
         if verbose:
-            print("TRAINING COMPLETED. NNs located at: ", sorter_output_folder,
-                  "/tmp/nn_train/detect.pt and ",
-                  sorter_output_folder, "/tmp/nn_train/denoise.pt")
+            print(
+                "TRAINING COMPLETED. NNs located at: ",
+                sorter_output_folder,
+                "/tmp/nn_train/detect.pt and ",
+                sorter_output_folder,
+                "/tmp/nn_train/denoise.pt",
+            )
 
     @classmethod
     def _get_result_from_folder(cls, sorter_output_folder):
         sorting = YassSortingExtractor(folder_path=Path(sorter_output_folder))
         return sorting
 
     # TODO integrate this logic somewhere or remove ????
@@ -274,40 +274,40 @@
     # self.merge_params['neuralnetwork']['detect']['filename'] = 'detect.pt'
     # fname_config = os.path.join(sorter_output_folder, 'config.yaml')
     # with open(fname_config, 'w') as file:
     # documents = yaml.dump(self.merge_params, file)
 
 
 def merge_params_dict(yass_params, params):
-    ''' This function merges params with yass_params (default)
-        to make a larger exposed params dictionary
-    '''
+    """This function merges params with yass_params (default)
+    to make a larger exposed params dictionary
+    """
     # self.params
     # self.yass_params
 
     merge_params = yass_params.copy()
 
-    merge_params['preprocess']['filter']['low_pass_freq'] = params['freq_min']
-    merge_params['preprocess']['filter']['high_factor'] = params['freq_max']
+    merge_params["preprocess"]["filter"]["low_pass_freq"] = params["freq_min"]
+    merge_params["preprocess"]["filter"]["high_factor"] = params["freq_max"]
 
-    merge_params['neuralnetwork']['detect']['filename'] = os.path.join(params['neural_nets_path'], 'detect.pt')
-    merge_params['neuralnetwork']['denoise']['filename'] = os.path.join(params['neural_nets_path'], 'denoise.pt')
+    merge_params["neuralnetwork"]["detect"]["filename"] = os.path.join(params["neural_nets_path"], "detect.pt")
+    merge_params["neuralnetwork"]["denoise"]["filename"] = os.path.join(params["neural_nets_path"], "denoise.pt")
 
-    merge_params['resources']['multi_processing'] = params['multi_processing']
-    merge_params['resources']['n_processors'] = params['n_processors']
-    merge_params['resources']['n_gpu_processors'] = params['n_gpu_processors']
-    merge_params['resources']['n_sec_chunk'] = params['n_sec_chunk']
-    merge_params['resources']['n_sec_chunk_gpu_detect'] = params['n_sec_chunk_gpu_detect']
-    merge_params['resources']['n_sec_chunk_gpu_deconv'] = params['n_sec_chunk_gpu_deconv']
-    merge_params['resources']['gpu_id'] = params['gpu_id']
-    merge_params['resources']['generate_phy'] = params['generate_phy']
-    merge_params['resources']['phy_percent_spikes'] = params['phy_percent_spikes']
-
-    merge_params['recordings']['spatial_radius'] = params['spatial_radius']
-    merge_params['recordings']['spike_size_ms'] = params['spike_size_ms']
-    merge_params['recordings']['clustering_chunk'] = params['clustering_chunk']
-
-    merge_params['deconvolution']['update_templates'] = params['update_templates']
-    merge_params['deconvolution']['neuron_discover'] = params['neuron_discover']
-    merge_params['deconvolution']['template_update_time'] = params['template_update_time']
+    merge_params["resources"]["multi_processing"] = params["multi_processing"]
+    merge_params["resources"]["n_processors"] = params["n_processors"]
+    merge_params["resources"]["n_gpu_processors"] = params["n_gpu_processors"]
+    merge_params["resources"]["n_sec_chunk"] = params["n_sec_chunk"]
+    merge_params["resources"]["n_sec_chunk_gpu_detect"] = params["n_sec_chunk_gpu_detect"]
+    merge_params["resources"]["n_sec_chunk_gpu_deconv"] = params["n_sec_chunk_gpu_deconv"]
+    merge_params["resources"]["gpu_id"] = params["gpu_id"]
+    merge_params["resources"]["generate_phy"] = params["generate_phy"]
+    merge_params["resources"]["phy_percent_spikes"] = params["phy_percent_spikes"]
+
+    merge_params["recordings"]["spatial_radius"] = params["spatial_radius"]
+    merge_params["recordings"]["spike_size_ms"] = params["spike_size_ms"]
+    merge_params["recordings"]["clustering_chunk"] = params["clustering_chunk"]
+
+    merge_params["deconvolution"]["update_templates"] = params["update_templates"]
+    merge_params["deconvolution"]["neuron_discover"] = params["neuron_discover"]
+    merge_params["deconvolution"]["template_update_time"] = params["template_update_time"]
 
     return merge_params
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sorters/external/yass_config_default.yaml` & `spikeinterface-0.98.0/src/spikeinterface/sorters/external/yass_config_default.yaml`

 * *Files identical despite different names*

### Comparing `spikeinterface-0.97.1/spikeinterface/sorters/internal/si_based.py` & `spikeinterface-0.98.0/src/spikeinterface/sorters/internal/si_based.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,28 +1,27 @@
 from spikeinterface.core import load_extractor
 
 from spikeinterface.sorters import BaseSorter
 
+
 class ComponentsBasedSorter(BaseSorter):
     """
     This is a based class for sorter based on spikeinterface.sortingcomponents
     """
-    
+
     @classmethod
     def is_installed(cls):
-        return True     
+        return True
 
     @classmethod
     def _setup_recording(cls, recording, output_folder, params, verbose):
         # nothing to do here because the spikeinterface_recording.json is here anyway
         pass
 
     @classmethod
     def _get_result_from_folder(cls, output_folder):
         sorting = load_extractor(output_folder / "sorting")
-        return sorting 
+        return sorting
 
     @classmethod
     def _check_apply_filter_in_params(cls, params):
         return False
-    
-
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sorters/internal/spyking_circus2.py` & `spikeinterface-0.98.0/src/spikeinterface/sorters/internal/spyking_circus2.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,160 +1,164 @@
 from .si_based import ComponentsBasedSorter
 
 import os
 import shutil
 import numpy as np
 import os
 
-from spikeinterface.core import (NumpySorting,  load_extractor, BaseRecording,
-                                 get_noise_levels, extract_waveforms)
+from spikeinterface.core import NumpySorting, load_extractor, BaseRecording, get_noise_levels, extract_waveforms
 from spikeinterface.core.job_tools import fix_job_kwargs
 from spikeinterface.preprocessing import bandpass_filter, common_reference, zscore
 
 try:
     import hdbscan
+
     HAVE_HDBSCAN = True
 except:
     HAVE_HDBSCAN = False
 
-class Spykingcircus2Sorter(ComponentsBasedSorter):
 
-    sorter_name = 'spykingcircus2'
+class Spykingcircus2Sorter(ComponentsBasedSorter):
+    sorter_name = "spykingcircus2"
 
     _default_params = {
-        'general' : {'ms_before' : 2, 'ms_after' : 2, 'local_radius_um' : 100},
-        'waveforms' : {'max_spikes_per_unit' : 200, 'overwrite' : True},
-        'filtering' : {'dtype' : 'float32'},
-        'detection' : {'peak_sign': 'neg', 'detect_threshold': 5},
-        'selection' : {'n_peaks_per_channel' : 5000, 'min_n_peaks' : 20000},
-        'localization' : {},
-        'clustering': {},
-        'matching':  {},
-        'registration' : {},
-        'apply_preprocessing': True,
-        'shared_memory' : False,
-        'job_kwargs' : {}
+        "general": {"ms_before": 2, "ms_after": 2, "local_radius_um": 100},
+        "waveforms": {"max_spikes_per_unit": 200, "overwrite": True},
+        "filtering": {"dtype": "float32"},
+        "detection": {"peak_sign": "neg", "detect_threshold": 5},
+        "selection": {"n_peaks_per_channel": 5000, "min_n_peaks": 20000},
+        "localization": {},
+        "clustering": {},
+        "matching": {},
+        "registration": {},
+        "apply_preprocessing": True,
+        "shared_memory": False,
+        "job_kwargs": {},
     }
 
     @classmethod
     def get_sorter_version(cls):
         return "2.0"
 
     @classmethod
     def _run_from_folder(cls, sorter_output_folder, params, verbose):
+        assert HAVE_HDBSCAN, "spykingcircus2 needs hdbscan to be installed"
 
-        assert HAVE_HDBSCAN, 'spykingcircus2 needs hdbscan to be installed'
-    
         # this is importanted only on demand because numba import are too heavy
         from spikeinterface.sortingcomponents.peak_detection import detect_peaks
         from spikeinterface.sortingcomponents.peak_selection import select_peaks
         from spikeinterface.sortingcomponents.clustering import find_cluster_from_peaks
         from spikeinterface.sortingcomponents.matching import find_spikes_from_templates
 
-        job_kwargs = params['job_kwargs'].copy()
+        job_kwargs = params["job_kwargs"].copy()
         job_kwargs = fix_job_kwargs(job_kwargs)
-        job_kwargs['verbose'] = verbose
-        job_kwargs['progress_bar'] = verbose
+        job_kwargs["verbose"] = verbose
+        job_kwargs["progress_bar"] = verbose
 
-        recording = load_extractor(sorter_output_folder.parent / 'spikeinterface_recording.json')
+        recording = load_extractor(
+            sorter_output_folder.parent / "spikeinterface_recording.json", base_folder=sorter_output_folder.parent
+        )
         sampling_rate = recording.get_sampling_frequency()
         num_channels = recording.get_num_channels()
 
         ## First, we are filtering the data
-        filtering_params = params['filtering'].copy()
-        if params['apply_preprocessing']:
-            #if recording.is_filtered == True:
+        filtering_params = params["filtering"].copy()
+        if params["apply_preprocessing"]:
+            # if recording.is_filtered == True:
             #    print('Looks like the recording is already filtered, check preprocessing!')
             recording_f = bandpass_filter(recording, **filtering_params)
             recording_f = common_reference(recording_f)
         else:
             recording_f = recording
 
-        recording_f = zscore(recording_f, dtype='float32')
+        recording_f = zscore(recording_f, dtype="float32")
 
         ## Then, we are detecting peaks with a locally_exclusive method
-        detection_params = params['detection'].copy()
+        detection_params = params["detection"].copy()
         detection_params.update(job_kwargs)
-        if 'local_radius_um' not in detection_params:
-            detection_params['local_radius_um'] = params['general']['local_radius_um']
-        if 'exclude_sweep_ms' not in detection_params:
-            detection_params['exclude_sweep_ms'] = max(params['general']['ms_before'], params['general']['ms_after'])
+        if "local_radius_um" not in detection_params:
+            detection_params["local_radius_um"] = params["general"]["local_radius_um"]
+        if "exclude_sweep_ms" not in detection_params:
+            detection_params["exclude_sweep_ms"] = max(params["general"]["ms_before"], params["general"]["ms_after"])
 
-        peaks = detect_peaks(recording_f, method='locally_exclusive', 
-                             **detection_params)
+        peaks = detect_peaks(recording_f, method="locally_exclusive", **detection_params)
 
         if verbose:
-            print('We found %d peaks in total' %len(peaks))
+            print("We found %d peaks in total" % len(peaks))
 
         ## We subselect a subset of all the peaks, by making the distributions os SNRs over all
         ## channels as flat as possible
-        selection_params = params['selection']
-        selection_params['n_peaks'] = params['selection']['n_peaks_per_channel'] * num_channels
-        selection_params['n_peaks'] = max(selection_params['min_n_peaks'], selection_params['n_peaks'])
+        selection_params = params["selection"]
+        selection_params["n_peaks"] = params["selection"]["n_peaks_per_channel"] * num_channels
+        selection_params["n_peaks"] = max(selection_params["min_n_peaks"], selection_params["n_peaks"])
 
         noise_levels = np.ones(num_channels, dtype=np.float32)
-        selection_params.update({'noise_levels' : noise_levels})
-        selected_peaks = select_peaks(peaks, method='smart_sampling_amplitudes', select_per_channel=False, 
-                                      **selection_params)
+        selection_params.update({"noise_levels": noise_levels})
+        selected_peaks = select_peaks(
+            peaks, method="smart_sampling_amplitudes", select_per_channel=False, **selection_params
+        )
 
         if verbose:
-            print('We kept %d peaks for clustering' %len(selected_peaks))
+            print("We kept %d peaks for clustering" % len(selected_peaks))
 
         ## We launch a clustering (using hdbscan) relying on positions and features extracted on
         ## the fly from the snippets
-        clustering_params = params['clustering'].copy()
-        clustering_params.update(params['waveforms'])
-        clustering_params.update(params['general'])
-        clustering_params.update(dict(shared_memory=params['shared_memory']))
-        clustering_params['job_kwargs'] = job_kwargs
-        clustering_params['tmp_folder'] = sorter_output_folder / "clustering"
-
-        labels, peak_labels = find_cluster_from_peaks(recording_f, selected_peaks, method='random_projections',
-            method_kwargs=clustering_params)
+        clustering_params = params["clustering"].copy()
+        clustering_params.update(params["waveforms"])
+        clustering_params.update(params["general"])
+        clustering_params.update(dict(shared_memory=params["shared_memory"]))
+        clustering_params["job_kwargs"] = job_kwargs
+        clustering_params["tmp_folder"] = sorter_output_folder / "clustering"
+
+        labels, peak_labels = find_cluster_from_peaks(
+            recording_f, selected_peaks, method="random_projections", method_kwargs=clustering_params
+        )
 
         ## We get the labels for our peaks
         mask = peak_labels > -1
-        sorting = NumpySorting.from_times_labels(selected_peaks['sample_ind'][mask], peak_labels[mask], sampling_rate)
+        sorting = NumpySorting.from_times_labels(selected_peaks["sample_index"][mask], peak_labels[mask], sampling_rate)
         clustering_folder = sorter_output_folder / "clustering"
         if clustering_folder.exists():
             shutil.rmtree(clustering_folder)
 
         sorting = sorting.save(folder=clustering_folder)
 
         ## We get the templates our of such a clustering
-        waveforms_params = params['waveforms'].copy()
+        waveforms_params = params["waveforms"].copy()
         waveforms_params.update(job_kwargs)
 
-        if params['shared_memory']:
-            mode = 'memory'
+        if params["shared_memory"]:
+            mode = "memory"
             waveforms_folder = None
         else:
-            mode = 'folder'
+            mode = "folder"
             waveforms_folder = sorter_output_folder / "waveforms"
 
-        we = extract_waveforms(recording_f, sorting, waveforms_folder, mode=mode, **waveforms_params, return_scaled=False)
+        we = extract_waveforms(
+            recording_f, sorting, waveforms_folder, mode=mode, **waveforms_params, return_scaled=False
+        )
 
         ## We launch a OMP matching pursuit by full convolution of the templates and the raw traces
-        matching_params = params['matching'].copy()
-        matching_params['waveform_extractor'] = we
-        matching_params.update({'noise_levels' : noise_levels})
+        matching_params = params["matching"].copy()
+        matching_params["waveform_extractor"] = we
+        matching_params.update({"noise_levels": noise_levels})
 
         matching_job_params = job_kwargs.copy()
-        matching_job_params['chunk_duration'] = '100ms'
+        matching_job_params["chunk_duration"] = "100ms"
 
-        spikes = find_spikes_from_templates(recording_f, method='circus-omp', 
-            method_kwargs=matching_params, **matching_job_params)
+        spikes = find_spikes_from_templates(
+            recording_f, method="circus-omp", method_kwargs=matching_params, **matching_job_params
+        )
 
         if verbose:
-            print('We found %d spikes' %len(spikes))
+            print("We found %d spikes" % len(spikes))
 
         ## And this is it! We have a spyking circus
-        sorting = NumpySorting.from_times_labels(spikes['sample_ind'], spikes['cluster_ind'], sampling_rate)
+        sorting = NumpySorting.from_times_labels(spikes["sample_index"], spikes["cluster_index"], sampling_rate)
         sorting_folder = sorter_output_folder / "sorting"
 
         if sorting_folder.exists():
             shutil.rmtree(sorting_folder)
 
         sorting = sorting.save(folder=sorting_folder)
 
         return sorting
-
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sorters/internal/tridesclous2.py` & `spikeinterface-0.98.0/src/spikeinterface/sorters/internal/tridesclous2.py`

 * *Files 16% similar despite different names*

```diff
@@ -2,142 +2,149 @@
 
 from spikeinterface.core import load_extractor, BaseRecording, get_noise_levels, extract_waveforms, NumpySorting
 from spikeinterface.core.job_tools import fix_job_kwargs
 from spikeinterface.preprocessing import bandpass_filter, common_reference, zscore
 
 import numpy as np
 
+
 class Tridesclous2Sorter(ComponentsBasedSorter):
-    sorter_name = 'tridesclous2'
+    sorter_name = "tridesclous2"
 
     _default_params = {
-        'apply_preprocessing': True,
-    
-        'general' : {'ms_before' : 2.5, 'ms_after' : 3.5, 'local_radius_um' : 100},
-        
-        'filtering' : {'freq_min' : 300, 'freq_max': 8000.},
-        'detection' : {'peak_sign': 'neg', 'detect_threshold': 5, 'exclude_sweep_ms': 0.4},
-        
-        'hdbscan_kwargs' : {"min_cluster_size" : 25,  "allow_single_cluster" : True, "core_dist_n_jobs" : -1, "cluster_selection_method" : "leaf"},
-        
-        'waveforms' : { 'max_spikes_per_unit' : 300},
-        'selection' : {'n_peaks_per_channel' : 5000, 'min_n_peaks' : 20000},
-        'localization' : {'max_distance_um':1000, 'optimizer': 'minimize_with_log_penality'},
-        'matching':  {'peak_shift_ms':  0.2,},
-        'job_kwargs' : {}
+        "apply_preprocessing": True,
+        "general": {"ms_before": 2.5, "ms_after": 3.5, "local_radius_um": 100},
+        "filtering": {"freq_min": 300, "freq_max": 8000.0},
+        "detection": {"peak_sign": "neg", "detect_threshold": 5, "exclude_sweep_ms": 0.4},
+        "hdbscan_kwargs": {
+            "min_cluster_size": 25,
+            "allow_single_cluster": True,
+            "core_dist_n_jobs": -1,
+            "cluster_selection_method": "leaf",
+        },
+        "waveforms": {"max_spikes_per_unit": 300},
+        "selection": {"n_peaks_per_channel": 5000, "min_n_peaks": 20000},
+        "localization": {"max_distance_um": 1000, "optimizer": "minimize_with_log_penality"},
+        "matching": {
+            "peak_shift_ms": 0.2,
+        },
+        "job_kwargs": {},
     }
 
     @classmethod
     def get_sorter_version(cls):
         return "2.0"
 
     @classmethod
     def _run_from_folder(cls, sorter_output_folder, params, verbose):
-        job_kwargs = params['job_kwargs'].copy()
+        job_kwargs = params["job_kwargs"].copy()
         job_kwargs = fix_job_kwargs(job_kwargs)
-        job_kwargs['progress_bar'] = verbose
-    
+        job_kwargs["progress_bar"] = verbose
+
         # this is importanted only on demand because numba import are too heavy
         from spikeinterface.sortingcomponents.peak_detection import detect_peaks
         from spikeinterface.sortingcomponents.peak_localization import localize_peaks
         from spikeinterface.sortingcomponents.peak_selection import select_peaks
         from spikeinterface.sortingcomponents.clustering import find_cluster_from_peaks
         from spikeinterface.sortingcomponents.matching import find_spikes_from_templates
-        
+
         import hdbscan
 
-        recording_raw = load_extractor(sorter_output_folder.parent / 'spikeinterface_recording.json')
-        
+        recording_raw = load_extractor(
+            sorter_output_folder.parent / "spikeinterface_recording.json", base_folder=sorter_output_folder.parent
+        )
+
         num_chans = recording_raw.get_num_channels()
         sampling_frequency = recording_raw.get_sampling_frequency()
 
-        
         # preprocessing
-        if params['apply_preprocessing']:
-            recording = bandpass_filter(recording_raw, **params['filtering'])
+        if params["apply_preprocessing"]:
+            recording = bandpass_filter(recording_raw, **params["filtering"])
             recording = common_reference(recording)
-            recording = zscore(recording, dtype='float32')
-            noise_levels = np.ones(num_chans, dtype='float32')
+            recording = zscore(recording, dtype="float32")
+            noise_levels = np.ones(num_chans, dtype="float32")
         else:
             recording = recording_raw
             noise_levels = get_noise_levels(recording, return_scaled=False)
-        
-        
+
         # detection
-        detection_params = params['detection'].copy()
-        detection_params['local_radius_um'] = params['general']['local_radius_um']
-        detection_params['noise_levels'] = noise_levels
-        peaks = detect_peaks(recording, method='locally_exclusive',  **detection_params, **job_kwargs)
+        detection_params = params["detection"].copy()
+        detection_params["local_radius_um"] = params["general"]["local_radius_um"]
+        detection_params["noise_levels"] = noise_levels
+        peaks = detect_peaks(recording, method="locally_exclusive", **detection_params, **job_kwargs)
 
         if verbose:
-            print('We found %d peaks in total' %len(peaks))
-        
+            print("We found %d peaks in total" % len(peaks))
+
         # selection
-        selection_params = params['selection'].copy()
-        selection_params['n_peaks'] = params['selection']['n_peaks_per_channel'] * num_chans
-        selection_params['n_peaks'] = max(selection_params['min_n_peaks'], selection_params['n_peaks'])
-        selection_params['noise_levels'] = noise_levels
-        some_peaks = select_peaks(peaks, method='smart_sampling_amplitudes', select_per_channel=False,
-                                  **selection_params)
+        selection_params = params["selection"].copy()
+        selection_params["n_peaks"] = params["selection"]["n_peaks_per_channel"] * num_chans
+        selection_params["n_peaks"] = max(selection_params["min_n_peaks"], selection_params["n_peaks"])
+        selection_params["noise_levels"] = noise_levels
+        some_peaks = select_peaks(
+            peaks, method="smart_sampling_amplitudes", select_per_channel=False, **selection_params
+        )
 
         if verbose:
-            print('We kept %d peaks for clustering' %len(some_peaks))
+            print("We kept %d peaks for clustering" % len(some_peaks))
 
         # localization
-        localization_params = params['localization'].copy()
-        localization_params['local_radius_um'] = params['general']['local_radius_um']
-        peak_locations = localize_peaks(recording, some_peaks, method='monopolar_triangulation',
-                                        **localization_params, **job_kwargs)
-        
-        #~ print(peak_locations.dtype)
-        
+        localization_params = params["localization"].copy()
+        localization_params["local_radius_um"] = params["general"]["local_radius_um"]
+        peak_locations = localize_peaks(
+            recording, some_peaks, method="monopolar_triangulation", **localization_params, **job_kwargs
+        )
+
+        # ~ print(peak_locations.dtype)
+
         # features = localisations only
-        peak_features = np.zeros((peak_locations.size, 3), dtype='float64')
-        for i, dim in enumerate(['x', 'y', 'z']):
+        peak_features = np.zeros((peak_locations.size, 3), dtype="float64")
+        for i, dim in enumerate(["x", "y", "z"]):
             peak_features[:, i] = peak_locations[dim]
-        
+
         # clusering is hdbscan
-        
-        
-        out = hdbscan.hdbscan(peak_features, **params['hdbscan_kwargs'])
+
+        out = hdbscan.hdbscan(peak_features, **params["hdbscan_kwargs"])
         peak_labels = out[0]
-        
+
         mask = peak_labels >= 0
         labels = np.unique(peak_labels[mask])
-        
+
         # extract waveform for template matching
-        sorting_temp = NumpySorting.from_times_labels(some_peaks['sample_ind'][mask], peak_labels[mask],
-                                                      sampling_frequency)
-        sorting_temp = sorting_temp.save(folder=sorter_output_folder / 'sorting_temp')
-        waveforms_params = params['waveforms'].copy()
-        waveforms_params['ms_before'] = params['general']['ms_before']
-        waveforms_params['ms_after'] = params['general']['ms_after']
-        we = extract_waveforms(recording, sorting_temp, sorter_output_folder / "waveforms_temp",
-                               **waveforms_params, **job_kwargs)
-        
+        sorting_temp = NumpySorting.from_times_labels(
+            some_peaks["sample_index"][mask], peak_labels[mask], sampling_frequency
+        )
+        sorting_temp = sorting_temp.save(folder=sorter_output_folder / "sorting_temp")
+        waveforms_params = params["waveforms"].copy()
+        waveforms_params["ms_before"] = params["general"]["ms_before"]
+        waveforms_params["ms_after"] = params["general"]["ms_after"]
+        we = extract_waveforms(
+            recording, sorting_temp, sorter_output_folder / "waveforms_temp", **waveforms_params, **job_kwargs
+        )
+
         ## We launch a OMP matching pursuit by full convolution of the templates and the raw traces
-        matching_params = params['matching'].copy()
-        matching_params['waveform_extractor'] = we
-        matching_params['noise_levels'] = noise_levels
-        matching_params['peak_sign'] = params['detection']['peak_sign']
-        matching_params['detect_threshold'] = params['detection']['detect_threshold']
-        matching_params['local_radius_um'] = params['general']['local_radius_um']
+        matching_params = params["matching"].copy()
+        matching_params["waveform_extractor"] = we
+        matching_params["noise_levels"] = noise_levels
+        matching_params["peak_sign"] = params["detection"]["peak_sign"]
+        matching_params["detect_threshold"] = params["detection"]["detect_threshold"]
+        matching_params["local_radius_um"] = params["general"]["local_radius_um"]
 
         # TODO: route that params
-        #~ 'num_closest' : 5,
-        #~ 'sample_shift': 3,
-        #~ 'ms_before': 0.8,
-        #~ 'ms_after': 1.2,
-        #~ 'num_peeler_loop':  2,
-        #~ 'num_template_try' : 1,
-        
-        spikes = find_spikes_from_templates(recording, method='tridesclous',  method_kwargs=matching_params,
-                                            **job_kwargs)
+        # ~ 'num_closest' : 5,
+        # ~ 'sample_shift': 3,
+        # ~ 'ms_before': 0.8,
+        # ~ 'ms_after': 1.2,
+        # ~ 'num_peeler_loop':  2,
+        # ~ 'num_template_try' : 1,
+
+        spikes = find_spikes_from_templates(
+            recording, method="tridesclous", method_kwargs=matching_params, **job_kwargs
+        )
 
         if verbose:
-            print('We found %d spikes' %len(spikes))
+            print("We found %d spikes" % len(spikes))
 
-        sorting = NumpySorting.from_times_labels(spikes['sample_ind'], spikes['cluster_ind'], sampling_frequency)
+        sorting = NumpySorting.from_times_labels(spikes["sample_index"], spikes["cluster_index"], sampling_frequency)
         sorting = sorting.save(folder=sorter_output_folder / "sorting")
 
         return sorting
-
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sorters/launcher.py` & `spikeinterface-0.98.0/src/spikeinterface/sorters/launcher.py`

 * *Files 4% similar despite different names*

```diff
@@ -16,53 +16,68 @@
 
 from .sorterlist import sorter_dict
 from .runsorter import run_sorter, run_sorter
 
 
 def _run_one(arg_list):
     # the multiprocessing python module force to have one unique tuple argument
-    (sorter_name, recording, output_folder, verbose, sorter_params,
-        docker_image, singularity_image, with_output) = arg_list
+    (
+        sorter_name,
+        recording,
+        output_folder,
+        verbose,
+        sorter_params,
+        docker_image,
+        singularity_image,
+        with_output,
+    ) = arg_list
 
     if isinstance(recording, dict):
         recording = load_extractor(recording)
     else:
         recording = recording
 
     # because this is checks in run_sorters before this call
     remove_existing_folder = False
     # result is retrieve later
     delete_output_folder = False
     # because we won't want the loop/worker to break
     raise_error = False
 
-    run_sorter(sorter_name, recording, output_folder=output_folder,
-               remove_existing_folder=remove_existing_folder,
-               delete_output_folder=delete_output_folder,
-               verbose=verbose, raise_error=raise_error,
-               docker_image=docker_image, singularity_image=singularity_image,
-               with_output=with_output, **sorter_params)
-
-
-
-
-_implemented_engine = ('loop', 'joblib', 'dask', 'slurm')
-
-
-def run_sorter_by_property(sorter_name,
-                           recording,
-                           grouping_property,
-                           working_folder,
-                           mode_if_folder_exists='raise',
-                           engine='loop',
-                           engine_kwargs={},
-                           verbose=False,
-                           docker_image=None,
-                           singularity_image=None,
-                           **sorter_params):
+    run_sorter(
+        sorter_name,
+        recording,
+        output_folder=output_folder,
+        remove_existing_folder=remove_existing_folder,
+        delete_output_folder=delete_output_folder,
+        verbose=verbose,
+        raise_error=raise_error,
+        docker_image=docker_image,
+        singularity_image=singularity_image,
+        with_output=with_output,
+        **sorter_params,
+    )
+
+
+_implemented_engine = ("loop", "joblib", "dask", "slurm")
+
+
+def run_sorter_by_property(
+    sorter_name,
+    recording,
+    grouping_property,
+    working_folder,
+    mode_if_folder_exists="raise",
+    engine="loop",
+    engine_kwargs={},
+    verbose=False,
+    docker_image=None,
+    singularity_image=None,
+    **sorter_params,
+):
     """
     Generic function to run a sorter on a recording after splitting by a 'grouping_property' (e.g. 'group').
 
     Internally, the function works as follows:
         * the recording is split based on the provided 'grouping_property' (using the 'split_by' function)
         * the 'run_sorters' function is run on the split recordings
         * sorting outputs are aggregated using the 'aggregate_units' function
@@ -109,59 +124,66 @@
 
     >>> sorting = si.run_sorter_by_property("tridesclous", recording, grouping_property="group",
                                             working_folder="sort_by_group", engine="joblib",
                                             engine_kwargs={"n_jobs": 4})
 
     """
 
-    assert grouping_property in recording.get_property_keys(), f"The 'grouping_property' {grouping_property} is not " \
-                                                               f"a recording property!"
-    recording_dict = recording.split_by(grouping_property)    
-    sorting_output = run_sorters([sorter_name], recording_dict, working_folder,
-                                 mode_if_folder_exists=mode_if_folder_exists,
-                                 engine=engine,
-                                 engine_kwargs=engine_kwargs,
-                                 verbose=verbose,
-                                 with_output=True,
-                                 docker_images={sorter_name: docker_image},
-                                 singularity_images={sorter_name: singularity_image},
-                                 sorter_params={sorter_name: sorter_params})
+    assert grouping_property in recording.get_property_keys(), (
+        f"The 'grouping_property' {grouping_property} is not " f"a recording property!"
+    )
+    recording_dict = recording.split_by(grouping_property)
+    sorting_output = run_sorters(
+        [sorter_name],
+        recording_dict,
+        working_folder,
+        mode_if_folder_exists=mode_if_folder_exists,
+        engine=engine,
+        engine_kwargs=engine_kwargs,
+        verbose=verbose,
+        with_output=True,
+        docker_images={sorter_name: docker_image},
+        singularity_images={sorter_name: singularity_image},
+        sorter_params={sorter_name: sorter_params},
+    )
 
     grouping_property_values = None
     sorting_list = []
-    for (output_name, sorting) in sorting_output.items():
+    for output_name, sorting in sorting_output.items():
         prop_name, sorter_name = output_name
         sorting_list.append(sorting)
         if grouping_property_values is None:
-            grouping_property_values = np.array([prop_name] * len(sorting.get_unit_ids()),
-                                                dtype=np.dtype(type(prop_name)))
+            grouping_property_values = np.array(
+                [prop_name] * len(sorting.get_unit_ids()), dtype=np.dtype(type(prop_name))
+            )
         else:
             grouping_property_values = np.concatenate(
-                (grouping_property_values, [prop_name] * len(sorting.get_unit_ids())))
+                (grouping_property_values, [prop_name] * len(sorting.get_unit_ids()))
+            )
 
     aggregate_sorting = aggregate_units(sorting_list)
-    aggregate_sorting.set_property(
-        key=grouping_property, values=grouping_property_values)
+    aggregate_sorting.set_property(key=grouping_property, values=grouping_property_values)
     aggregate_sorting.register_recording(recording)
 
     return aggregate_sorting
 
 
-def run_sorters(sorter_list,
-                recording_dict_or_list,
-                working_folder,
-                sorter_params={},
-                mode_if_folder_exists='raise',
-                engine='loop',
-                engine_kwargs={},
-                verbose=False,
-                with_output=True,
-                docker_images={},
-                singularity_images={},
-                ):
+def run_sorters(
+    sorter_list,
+    recording_dict_or_list,
+    working_folder,
+    sorter_params={},
+    mode_if_folder_exists="raise",
+    engine="loop",
+    engine_kwargs={},
+    verbose=False,
+    with_output=True,
+    docker_images={},
+    singularity_images={},
+):
     """Run several sorter on several recordings.
 
     Parameters
     ----------
     sorter_list: list of str
         List of sorter names.
     recording_dict_or_list: dict or list
@@ -197,182 +219,186 @@
     Returns
     -------
     results : dict
         The output is nested dict[(rec_name, sorter_name)] of SortingExtractor.
     """
     working_folder = Path(working_folder)
 
-    mode_if_folder_exists in ('raise', 'keep', 'overwrite')
+    mode_if_folder_exists in ("raise", "keep", "overwrite")
 
-    if mode_if_folder_exists == 'raise' and working_folder.is_dir():
-        raise Exception('working_folder already exists, please remove it')
+    if mode_if_folder_exists == "raise" and working_folder.is_dir():
+        raise Exception("working_folder already exists, please remove it")
 
-    assert engine in _implemented_engine, f'engine must be in {_implemented_engine}'
+    assert engine in _implemented_engine, f"engine must be in {_implemented_engine}"
 
     if isinstance(sorter_list, str):
         sorter_list = [sorter_list]
 
     for sorter_name in sorter_list:
-        assert sorter_name in sorter_dict, f'{sorter_name} is not in sorter list'
+        assert sorter_name in sorter_dict, f"{sorter_name} is not in sorter list"
 
     if isinstance(recording_dict_or_list, list):
         # in case of list
-        recording_dict = {'recording_{}'.format(
-            i): rec for i, rec in enumerate(recording_dict_or_list)}
+        recording_dict = {"recording_{}".format(i): rec for i, rec in enumerate(recording_dict_or_list)}
     elif isinstance(recording_dict_or_list, dict):
         recording_dict = recording_dict_or_list
     else:
-        raise ValueError('bad recording dict')
-    
+        raise ValueError("bad recording dict")
+
     dtype_rec_name = np.dtype(type(list(recording_dict.keys())[0]))
     assert dtype_rec_name.kind in ("i", "u", "S", "U"), "Dict keys can only be integers or strings!"
 
-    need_dump = engine != 'loop'
+    need_dump = engine != "loop"
     task_args_list = []
     for rec_name, recording in recording_dict.items():
         for sorter_name in sorter_list:
-
             output_folder = working_folder / str(rec_name) / sorter_name
 
             if output_folder.is_dir():
                 # sorter folder exists
-                if mode_if_folder_exists == 'raise':
-                    raise Exception(f'output folder already exists for {rec_name} {sorter_name}')
-                elif mode_if_folder_exists == 'overwrite':
+                if mode_if_folder_exists == "raise":
+                    raise Exception(f"output folder already exists for {rec_name} {sorter_name}")
+                elif mode_if_folder_exists == "overwrite":
                     shutil.rmtree(str(output_folder))
-                elif mode_if_folder_exists == 'keep':
+                elif mode_if_folder_exists == "keep":
                     if is_log_ok(output_folder):
                         continue
                     else:
                         shutil.rmtree(str(output_folder))
 
             params = sorter_params.get(sorter_name, {})
             docker_image = docker_images.get(sorter_name, None)
             singularity_image = singularity_images.get(sorter_name, None)
-            _check_container_images(
-                docker_image, singularity_image, sorter_name)
+            _check_container_images(docker_image, singularity_image, sorter_name)
 
             if need_dump:
-                if not recording.is_dumpable:
-                    raise Exception(
-                        'recording not dumpable call recording.save() before')
+                if not recording.check_if_dumpable():
+                    raise Exception("recording not dumpable call recording.save() before")
                 recording_arg = recording.to_dict()
             else:
                 recording_arg = recording
 
-            task_args = (sorter_name, recording_arg, output_folder,
-                         verbose, params, docker_image, singularity_image, with_output)
-            task_args_list.append(task_args)            
+            task_args = (
+                sorter_name,
+                recording_arg,
+                output_folder,
+                verbose,
+                params,
+                docker_image,
+                singularity_image,
+                with_output,
+            )
+            task_args_list.append(task_args)
 
-    if engine == 'loop':
+    if engine == "loop":
         # simple loop in main process
         for task_args in task_args_list:
             _run_one(task_args)
 
-    elif engine == 'joblib':
+    elif engine == "joblib":
         from joblib import Parallel, delayed
-        n_jobs = engine_kwargs.get('n_jobs', -1)
-        backend = engine_kwargs.get('backend', 'loky')
-        Parallel(n_jobs=n_jobs, backend=backend)(
-            delayed(_run_one)(task_args) for task_args in task_args_list)
-
-    elif engine == 'dask':
-        client = engine_kwargs.get('client', None)
-        assert client is not None, 'For dask engine you have to provide : client = dask.distributed.Client(...)'
+
+        n_jobs = engine_kwargs.get("n_jobs", -1)
+        backend = engine_kwargs.get("backend", "loky")
+        Parallel(n_jobs=n_jobs, backend=backend)(delayed(_run_one)(task_args) for task_args in task_args_list)
+
+    elif engine == "dask":
+        client = engine_kwargs.get("client", None)
+        assert client is not None, "For dask engine you have to provide : client = dask.distributed.Client(...)"
 
         tasks = []
         for task_args in task_args_list:
             task = client.submit(_run_one, task_args)
             tasks.append(task)
 
         for task in tasks:
             task.result()
-    
-    elif engine == 'slurm':
+
+    elif engine == "slurm":
         # generate python script for slurm
-        tmp_script_folder = engine_kwargs.get('tmp_script_folder', None)
+        tmp_script_folder = engine_kwargs.get("tmp_script_folder", None)
         if tmp_script_folder is None:
-            tmp_script_folder = tempfile.mkdtemp(prefix='spikeinterface_slurm_')
+            tmp_script_folder = tempfile.mkdtemp(prefix="spikeinterface_slurm_")
         tmp_script_folder = Path(tmp_script_folder)
-        cpus_per_task = engine_kwargs.get('cpus_per_task', 1)
-        mem = engine_kwargs.get('mem', '1G')
+        cpus_per_task = engine_kwargs.get("cpus_per_task", 1)
+        mem = engine_kwargs.get("mem", "1G")
 
         for i, task_args in enumerate(task_args_list):
-            script_name = tmp_script_folder / f'si_script_{i}.py'
-            with open(script_name, 'w') as f:
-                arg_list_txt = '(\n'
+            script_name = tmp_script_folder / f"si_script_{i}.py"
+            with open(script_name, "w") as f:
+                arg_list_txt = "(\n"
                 for j, arg in enumerate(task_args):
-                    arg_list_txt += '\t'
-                    if j !=1:
+                    arg_list_txt += "\t"
+                    if j != 1:
                         if isinstance(arg, str):
                             arg_list_txt += f'"{arg}"'
                         elif isinstance(arg, Path):
                             arg_list_txt += f'"{str(arg.absolute())}"'
                         else:
-                            arg_list_txt += f'{arg}'
+                            arg_list_txt += f"{arg}"
                     else:
-                        arg_list_txt += 'recording'
-                    arg_list_txt += ',\r'
-                arg_list_txt += ')'
-                
+                        arg_list_txt += "recording"
+                    arg_list_txt += ",\r"
+                arg_list_txt += ")"
+
                 recording_dict = task_args[1]
-                slurm_script = _slurm_script.format(python=sys.executable, recording_dict=recording_dict, arg_list_txt=arg_list_txt)
+                slurm_script = _slurm_script.format(
+                    python=sys.executable, recording_dict=recording_dict, arg_list_txt=arg_list_txt
+                )
                 f.write(slurm_script)
-                os.fchmod(f.fileno(), mode = stat.S_IRWXU)
-                
+                os.fchmod(f.fileno(), mode=stat.S_IRWXU)
+
                 print(slurm_script)
 
-            subprocess.Popen(['sbatch', str(script_name.absolute()), f'-cpus-per-task={cpus_per_task}', f'-mem={mem}'] )
+            subprocess.Popen(["sbatch", str(script_name.absolute()), f"-cpus-per-task={cpus_per_task}", f"-mem={mem}"])
 
-    non_blocking_engine = ('loop', 'joblib')
+    non_blocking_engine = ("loop", "joblib")
     if engine in non_blocking_engine:
         # dump spikeinterface_job.json
         # only for non blocking engine
         for rec_name, recording in recording_dict.items():
             for sorter_name in sorter_list:
                 output_folder = working_folder / str(rec_name) / sorter_name
                 with open(output_folder / "spikeinterface_job.json", "w") as f:
-                    dump_dict = {"rec_name": rec_name,
-                                 "sorter_name": sorter_name,
-                                 "engine": engine}
+                    dump_dict = {"rec_name": rec_name, "sorter_name": sorter_name, "engine": engine}
                     if engine != "dask":
                         dump_dict.update({"engine_kwargs": engine_kwargs})
                     json.dump(check_json(dump_dict), f)
 
     if with_output:
         if engine not in non_blocking_engine:
-            print(f'Warning!! With engine="{engine}" you cannot have directly output results\n'
-                  'Use : run_sorters(..., with_output=False)\n'
-                  'And then: results = collect_sorting_outputs(output_folders)')
+            print(
+                f'Warning!! With engine="{engine}" you cannot have directly output results\n'
+                "Use : run_sorters(..., with_output=False)\n"
+                "And then: results = collect_sorting_outputs(output_folders)"
+            )
             return
 
         results = collect_sorting_outputs(working_folder)
         return results
 
 
-
 _slurm_script = """#! {python}
 from numpy import array
 from spikeinterface.sorters.launcher import _run_one
 
 recording = {recording_dict}
 
 arg_list = {arg_list_txt}
 
 _run_one(arg_list)
 """
 
 
-
 def is_log_ok(output_folder):
     # log is OK when run_time is not None
-    if (output_folder / 'spikeinterface_log.json').is_file():
-        with open(output_folder / 'spikeinterface_log.json', mode='r', encoding='utf8') as logfile:
+    if (output_folder / "spikeinterface_log.json").is_file():
+        with open(output_folder / "spikeinterface_log.json", mode="r", encoding="utf8") as logfile:
             log = json.load(logfile)
-            run_time = log.get('run_time', None)
+            run_time = log.get("run_time", None)
             ok = run_time is not None
             return ok
     return False
 
 
 def iter_working_folder(working_folder):
     working_folder = Path(working_folder)
@@ -410,14 +436,13 @@
     The output is a  dict with double key access results[(rec_name, sorter_name)] of SortingExtractor.
     """
     results = {}
     for rec_name, sorter_name, sorting in iter_sorting_output(working_folder):
         results[(rec_name, sorter_name)] = sorting
     return results
 
+
 def _check_container_images(docker_image, singularity_image, sorter_name):
     if docker_image is not None:
-        assert singularity_image is None, (f"Provide either a docker or a singularity image "
-                                           f"for sorter {sorter_name}")
+        assert singularity_image is None, f"Provide either a docker or a singularity image " f"for sorter {sorter_name}"
     if singularity_image is not None:
-        assert docker_image is None, (f"Provide either a docker or a singularity image "
-                                      f"for sorter {sorter_name}")
+        assert docker_image is None, f"Provide either a docker or a singularity image " f"for sorter {sorter_name}"
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sorters/runsorter.py` & `spikeinterface-0.98.0/src/spikeinterface/sorters/runsorter.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 import shutil
 import os
 from pathlib import Path
 import json
 import platform
+from warnings import warn
 from typing import Optional, Union
 
 from ..core import BaseRecording
 from .. import __version__ as si_version
 from spikeinterface.core.npzsortingextractor import NpzSortingExtractor
 from spikeinterface.core.core_tools import check_json, recursive_path_modifier
 from .sorterlist import sorter_dict
@@ -14,41 +15,39 @@
 
 try:
     HAS_DOCKER = True
     import docker
 except ModuleNotFoundError:
     HAS_DOCKER = False
 
-REGISTRY = 'spikeinterface'
+REGISTRY = "spikeinterface"
 
 SORTER_DOCKER_MAP = dict(
-    combinato='combinato',
-    herdingspikes='herdingspikes',
-    klusta='klusta',
-    mountainsort4='mountainsort4',
-    pykilosort='pykilosort',
-    spykingcircus='spyking-circus',
-    spykingcircus2='spyking-circus2',
-    tridesclous='tridesclous',
-    yass='yass',
+    combinato="combinato",
+    herdingspikes="herdingspikes",
+    klusta="klusta",
+    mountainsort4="mountainsort4",
+    mountainsort5="mountainsort5",
+    pykilosort="pykilosort",
+    spykingcircus="spyking-circus",
+    spykingcircus2="spyking-circus2",
+    tridesclous="tridesclous",
+    yass="yass",
     # Matlab compiled sorters:
-    hdsort='hdsort-compiled',
-    ironclust='ironclust-compiled',
-    kilosort='kilosort-compiled',
-    kilosort2='kilosort2-compiled',
-    kilosort2_5='kilosort2_5-compiled',
-    kilosort3='kilosort3-compiled',
-    waveclus='waveclus-compiled',
-    waveclus_snippets='waveclus-compiled',
+    hdsort="hdsort-compiled",
+    ironclust="ironclust-compiled",
+    kilosort="kilosort-compiled",
+    kilosort2="kilosort2-compiled",
+    kilosort2_5="kilosort2_5-compiled",
+    kilosort3="kilosort3-compiled",
+    waveclus="waveclus-compiled",
+    waveclus_snippets="waveclus-compiled",
 )
 
-SORTER_DOCKER_MAP = {
-    k: f'{REGISTRY}/{v}-base'
-    for k, v in SORTER_DOCKER_MAP.items()
-}
+SORTER_DOCKER_MAP = {k: f"{REGISTRY}/{v}-base" for k, v in SORTER_DOCKER_MAP.items()}
 
 
 _common_param_doc = """
     Parameters
     ----------
     sorter_name: str
         The sorter name
@@ -66,18 +65,22 @@
         If True, an error is raised if spike sorting fails (default).
         If False, the process continues and the error is logged in the log file.
     docker_image: bool or str
         If True, pull the default docker container for the sorter and run the sorter in that container using docker.
         Use a str to specify a non-default container. If that container is not local it will be pulled from docker hub.
         If False, the sorter is run locally.
     singularity_image: bool or str
-        If True, pull the default docker container for the sorter and run the sorter in that container using 
-        singularity. Use a str to specify a non-default container. If that container is not local it will be pulled 
+        If True, pull the default docker container for the sorter and run the sorter in that container using
+        singularity. Use a str to specify a non-default container. If that container is not local it will be pulled
         from Docker Hub.
         If False, the sorter is run locally.
+    delete_container_files: bool
+        If True, the container temporary files are deleted after the sorting is done (default False).
+    with_output: bool
+        If True, the output Sorting is returned as a Sorting (default True).
     **sorter_params: keyword args
         Spike sorter specific arguments (they can be retrieved with 'get_default_params(sorter_name_or_class)'
 
     Returns
     -------
     sortingextractor: SortingExtractor
         The spike sorted data
@@ -90,14 +93,15 @@
     output_folder: Optional[str] = None,
     remove_existing_folder: bool = True,
     delete_output_folder: bool = False,
     verbose: bool = False,
     raise_error: bool = True,
     docker_image: Optional[Union[bool, str]] = False,
     singularity_image: Optional[Union[bool, str]] = False,
+    delete_container_files: bool = True,
     with_output: bool = True,
     **sorter_params,
 ):
     """
     Generic function to run a sorter via function approach.
 
     {}
@@ -116,14 +120,15 @@
         verbose=verbose,
         raise_error=raise_error,
         with_output=with_output,
         **sorter_params,
     )
 
     if docker_image or singularity_image:
+        common_kwargs.update(dict(delete_container_files=delete_container_files))
         if docker_image:
             mode = "docker"
             assert not singularity_image
             if isinstance(docker_image, bool):
                 container_image = None
             else:
                 container_image = docker_image
@@ -141,28 +146,34 @@
         )
 
     return run_sorter_local(**common_kwargs)
 
 
 run_sorter.__doc__ = run_sorter.__doc__.format(_common_param_doc)
 
-def run_sorter_local(sorter_name, recording, output_folder=None,
-                     remove_existing_folder=True, delete_output_folder=False,
-                     verbose=False, raise_error=True, with_output=True, **sorter_params):
+
+def run_sorter_local(
+    sorter_name,
+    recording,
+    output_folder=None,
+    remove_existing_folder=True,
+    delete_output_folder=False,
+    verbose=False,
+    raise_error=True,
+    with_output=True,
+    **sorter_params,
+):
     if isinstance(recording, list):
-        raise Exception(
-            'You you want to run several sorters/recordings use run_sorters(...)')
+        raise Exception("You you want to run several sorters/recordings use run_sorters(...)")
 
     SorterClass = sorter_dict[sorter_name]
 
     # only classmethod call not instance (stateless at instance level but state is in folder)
-    output_folder = SorterClass.initialize_folder(
-        recording, output_folder, verbose, remove_existing_folder)
-    SorterClass.set_params_to_folder(
-        recording, output_folder, sorter_params, verbose)
+    output_folder = SorterClass.initialize_folder(recording, output_folder, verbose, remove_existing_folder)
+    SorterClass.set_params_to_folder(recording, output_folder, sorter_params, verbose)
     SorterClass.setup_recording(recording, output_folder, verbose=verbose)
     SorterClass.run_from_folder(output_folder, raise_error, verbose)
     if with_output:
         sorting = SorterClass.get_result_from_folder(output_folder)
     else:
         sorting = None
     sorter_output_folder = output_folder / "sorter_output"
@@ -176,48 +187,48 @@
     folders_to_mount = []
 
     def append_parent_folder(p):
         p = Path(p)
         folders_to_mount.append(p.resolve().absolute().parent)
         return p
 
-    _ = recursive_path_modifier(d, append_parent_folder, target='path', copy=True)
+    _ = recursive_path_modifier(d, append_parent_folder, target="path", copy=True)
 
-    try: # this will fail if on different drives (Windows)
+    try:  # this will fail if on different drives (Windows)
         base_folders_to_mount = [Path(os.path.commonpath(folders_to_mount))]
     except ValueError:
         base_folders_to_mount = folders_to_mount
 
     # let's not mount root if dries are /home/..., /mnt1/...
     if len(base_folders_to_mount) == 1:
         if len(str(base_folders_to_mount[0])) == 1:
             base_folders_to_mount = folders_to_mount
 
     return base_folders_to_mount
 
 
-
 def path_to_unix(path):
     path = Path(path)
-    if platform.system() == 'Windows':
-        path = Path(str(path)[str(path).find(":") + 1:])
+    if platform.system() == "Windows":
+        path = Path(str(path)[str(path).find(":") + 1 :])
     return path.as_posix()
 
 
 def windows_extractor_dict_to_unix(d):
-    d = recursive_path_modifier(d, path_to_unix, target='path', copy=True)
+    d = recursive_path_modifier(d, path_to_unix, target="path", copy=True)
     return d
 
 
 class ContainerClient:
     """
     Small abstraction class to run commands in:
       * docker with "docker" python package
       * singularity with  "spython" python package
     """
+
     def __init__(self, mode, container_image, volumes, py_user_base, extra_kwargs):
         """
         Parameters
         ----------
         mode: str
             "docker" or "singularity" strings
         container_image: str
@@ -226,122 +237,118 @@
             dict of volumes to bind
         py_user_base: str
             Python user base folder to set as PYTHONUSERBASE env var in Singularity mode
             Prevents from overwriting user's packages when running pip install
         extra_kwargs: dict
             Extra kwargs to start container
         """
-        assert mode in ('docker', 'singularity')
+        assert mode in ("docker", "singularity")
         self.mode = mode
         self.py_user_base = py_user_base
-        container_requires_gpu = extra_kwargs.get(
-            'container_requires_gpu', None)
+        container_requires_gpu = extra_kwargs.get("container_requires_gpu", None)
 
-        if mode == 'docker':
+        if mode == "docker":
             if not HAS_DOCKER:
                 raise ModuleNotFoundError("No module named 'docker'")
             client = docker.from_env()
             if container_requires_gpu is not None:
-                extra_kwargs.pop('container_requires_gpu')
-                extra_kwargs["device_requests"] = [
-                    docker.types.DeviceRequest(count=-1, capabilities=[['gpu']])]
+                extra_kwargs.pop("container_requires_gpu")
+                extra_kwargs["device_requests"] = [docker.types.DeviceRequest(count=-1, capabilities=[["gpu"]])]
 
             if self._get_docker_image(container_image) is None:
                 print(f"Docker: pulling image {container_image}")
                 client.images.pull(container_image)
 
-            self.docker_container = client.containers.create(
-                container_image,
-                tty=True,
-                volumes=volumes,
-                **extra_kwargs
-            )
+            self.docker_container = client.containers.create(container_image, tty=True, volumes=volumes, **extra_kwargs)
 
-        elif mode == 'singularity':
-            assert self.py_user_base, 'py_user_base folder must be set in singularity mode'
+        elif mode == "singularity":
+            assert self.py_user_base, "py_user_base folder must be set in singularity mode"
             from spython.main import Client
+
             # load local image file if it exists, otherwise search dockerhub
             sif_file = Client._get_filename(container_image)
             singularity_image = None
             if Path(container_image).exists():
                 singularity_image = container_image
             elif Path(sif_file).exists():
                 singularity_image = sif_file
             else:
                 if HAS_DOCKER:
                     docker_image = self._get_docker_image(container_image)
                     if docker_image and len(docker_image.tags) > 0:
                         tag = docker_image.tags[0]
-                        print(f'Building singularity image from local docker image: {tag}')
-                        singularity_image = Client.build(f'docker-daemon://{tag}', sif_file, sudo=False)
+                        print(f"Building singularity image from local docker image: {tag}")
+                        singularity_image = Client.build(f"docker-daemon://{tag}", sif_file, sudo=False)
                 if not singularity_image:
                     print(f"Singularity: pulling image {container_image}")
-                    singularity_image = Client.pull(f'docker://{container_image}')
+                    singularity_image = Client.pull(f"docker://{container_image}")
 
             if not Path(singularity_image).exists():
-                raise FileNotFoundError(f'Unable to locate container image {container_image}')
+                raise FileNotFoundError(f"Unable to locate container image {container_image}")
 
             # bin options
-            singularity_bind = ','.join([f'{volume_src}:{volume["bind"]}' for volume_src, volume in volumes.items()])
-            options = ['--bind', singularity_bind]
+            singularity_bind = ",".join([f'{volume_src}:{volume["bind"]}' for volume_src, volume in volumes.items()])
+            options = ["--bind", singularity_bind]
 
             # gpu options
             if container_requires_gpu:
                 # only nvidia at the moment
-                options += ['--nv']
+                options += ["--nv"]
 
             self.client_instance = Client.instance(singularity_image, start=False, options=options)
 
     @staticmethod
     def _get_docker_image(container_image):
         docker_client = docker.from_env(timeout=300)
         try:
             docker_image = docker_client.images.get(container_image)
         except docker.errors.ImageNotFound:
             docker_image = None
         return docker_image
 
     def start(self):
-        if self.mode == 'docker':
+        if self.mode == "docker":
             self.docker_container.start()
-        elif self.mode == 'singularity':
+        elif self.mode == "singularity":
             self.client_instance.start()
 
     def stop(self):
-        if self.mode == 'docker':
+        if self.mode == "docker":
             self.docker_container.stop()
             self.docker_container.remove(force=True)
-        elif self.mode == 'singularity':
+        elif self.mode == "singularity":
             self.client_instance.stop()
 
     def run_command(self, command):
-        if self.mode == 'docker':
+        if self.mode == "docker":
             res = self.docker_container.exec_run(command)
-            return str(res.output)
-        elif self.mode == 'singularity':
+            return res.output.decode(encoding="utf-8", errors="ignore")
+        elif self.mode == "singularity":
             from spython.main import Client
-            options = ['--cleanenv', '--env', f'PYTHONUSERBASE={self.py_user_base}']
+
+            options = ["--cleanenv", "--env", f"PYTHONUSERBASE={self.py_user_base}"]
             res = Client.execute(self.client_instance, command, options=options)
             if isinstance(res, dict):
-                res = res['message']
+                res = res["message"]
             return res
 
 
 def run_sorter_container(
     sorter_name: str,
     recording: BaseRecording,
     mode: str,
     container_image: Optional[str] = None,
     output_folder: Optional[str] = None,
     remove_existing_folder: bool = True,
     delete_output_folder: bool = False,
     verbose: bool = False,
     raise_error: bool = True,
     with_output: bool = True,
-    extra_requirements = None,
+    delete_container_files: bool = True,
+    extra_requirements=None,
     **sorter_params,
 ):
     """
 
     Parameters
     ----------
     sorter_name: str
@@ -350,53 +357,56 @@
     container_image: str, optional
     output_folder: str, optional
     remove_existing_folder: bool, optional
     delete_output_folder: bool, optional
     verbose: bool, optional
     raise_error: bool, optional
     with_output: bool, optional
+    delete_container_files: bool, optional
     extra_requirements: list, optional
     sorter_params:
 
     """
 
     if extra_requirements is None:
         extra_requirements = []
 
     # common code for docker and singularity
     if output_folder is None:
-        output_folder = sorter_name + '_output'
+        output_folder = sorter_name + "_output"
 
     if container_image is None:
         if sorter_name in SORTER_DOCKER_MAP:
             container_image = SORTER_DOCKER_MAP[sorter_name]
         else:
             raise ValueError(f"sorter {sorter_name} not in SORTER_DOCKER_MAP. Please specify a container_image.")
 
     SorterClass = sorter_dict[sorter_name]
     output_folder = Path(output_folder).absolute().resolve()
     parent_folder = output_folder.parent.absolute().resolve()
-    output_folder.mkdir(parents=True, exist_ok=True)
+    parent_folder.mkdir(parents=True, exist_ok=True)
 
     # find input folder of recording for folder bind
-    rec_dict = recording.to_dict()
+    rec_dict = recording.to_dict(recursive=True)
     recording_input_folders = find_recording_folders(rec_dict)
 
-    if platform.system() == 'Windows':
+    if platform.system() == "Windows":
         rec_dict = windows_extractor_dict_to_unix(rec_dict)
 
     # create 3 files for communication with container
     # recording dict inside
-    (parent_folder / 'in_container_recording.json').write_text(
-        json.dumps(check_json(rec_dict), indent=4), encoding='utf8')
+    (parent_folder / "in_container_recording.json").write_text(
+        json.dumps(check_json(rec_dict), indent=4), encoding="utf8"
+    )
     # need to share specific parameters
-    (parent_folder / 'in_container_params.json').write_text(
-        json.dumps(check_json(sorter_params), indent=4), encoding='utf8')
+    (parent_folder / "in_container_params.json").write_text(
+        json.dumps(check_json(sorter_params), indent=4), encoding="utf8"
+    )
 
-    npz_sorting_path = output_folder / 'in_container_sorting'
+    npz_sorting_path = output_folder / "in_container_sorting"
 
     # if in Windows, skip C:
     parent_folder_unix = path_to_unix(parent_folder)
     output_folder_unix = path_to_unix(output_folder)
     recording_input_folders_unix = [path_to_unix(rf) for rf in recording_input_folders]
     npz_sorting_path_unix = path_to_unix(npz_sorting_path)
 
@@ -415,331 +425,428 @@
     with open('{parent_folder_unix}/in_container_params.json', encoding='utf8', mode='r') as f:
         sorter_params = json.load(f)
 
     # run in container
     output_folder = '{output_folder_unix}'
     sorting = run_sorter_local(
         '{sorter_name}', recording, output_folder=output_folder,
-         remove_existing_folder={remove_existing_folder}, delete_output_folder=False,
-          verbose={verbose}, raise_error={raise_error}, with_output=True, **sorter_params
+        remove_existing_folder={remove_existing_folder}, delete_output_folder=False,
+        verbose={verbose}, raise_error={raise_error}, with_output=True, **sorter_params
     )
     sorting.save_to_folder(folder='{npz_sorting_path_unix}')
 """
-    (parent_folder / 'in_container_sorter_script.py').write_text(py_script, encoding='utf8')
+    (parent_folder / "in_container_sorter_script.py").write_text(py_script, encoding="utf8")
 
     volumes = {}
     for recording_folder, recording_folder_unix in zip(recording_input_folders, recording_input_folders_unix):
         # handle duplicates
         if str(recording_folder) not in volumes:
-            volumes[str(recording_folder)] = {
-                'bind': str(recording_folder_unix), 'mode': 'ro'}
-    volumes[str(parent_folder)] = {'bind': str(parent_folder_unix), 'mode': 'rw'}
-    
-    si_dev_path = os.getenv('SPIKEINTERFACE_DEV_PATH', None)
-        
+            volumes[str(recording_folder)] = {"bind": str(recording_folder_unix), "mode": "ro"}
+    volumes[str(parent_folder)] = {"bind": str(parent_folder_unix), "mode": "rw"}
+
+    si_dev_path = os.getenv("SPIKEINTERFACE_DEV_PATH", None)
+
     install_si_from_source = False
-    if 'dev' in si_version and si_dev_path is not None:
+    if "dev" in si_version and si_dev_path is not None:
         install_si_from_source = True
         # Making sure to get rid of last / or \
         si_dev_path = str(Path(si_dev_path).absolute().resolve())
         si_dev_path_unix = path_to_unix(si_dev_path)
-        volumes[si_dev_path] = {'bind': si_dev_path_unix, 'mode': 'ro'}
+        volumes[si_dev_path] = {"bind": si_dev_path_unix, "mode": "ro"}
 
     extra_kwargs = {}
-    
+
     use_gpu = SorterClass.use_gpu(sorter_params)
     gpu_capability = SorterClass.gpu_capability
     if use_gpu:
-        if gpu_capability == 'nvidia-required':
+        if gpu_capability == "nvidia-required":
             assert has_nvidia(), "The container requires a NVIDIA GPU capability, but it is not available"
-            extra_kwargs['container_requires_gpu'] = True
-        elif gpu_capability == 'nvidia-optional':
+            extra_kwargs["container_requires_gpu"] = True
+        elif gpu_capability == "nvidia-optional":
             if has_nvidia():
-                extra_kwargs['container_requires_gpu'] = True
+                extra_kwargs["container_requires_gpu"] = True
             else:
                 if verbose:
-                    print(f"{SorterClass.sorter_name} supports GPU, but no GPU is available.\n"
-                          f"Running the sorter without GPU")
+                    print(
+                        f"{SorterClass.sorter_name} supports GPU, but no GPU is available.\n"
+                        f"Running the sorter without GPU"
+                    )
         else:
             # TODO: make opencl machanism
             raise NotImplementedError("Only nvidia support is available")
 
     # Creating python user base folder
     py_user_base_unix = None
-    if mode == 'singularity':
-        py_user_base_folder = (parent_folder / 'in_container_python_base')
+    if mode == "singularity":
+        py_user_base_folder = parent_folder / "in_container_python_base"
         py_user_base_folder.mkdir(parents=True, exist_ok=True)
         py_user_base_unix = path_to_unix(py_user_base_folder)
         si_source_folder = f"{py_user_base_unix}/sources"
     else:
         si_source_folder = "/sources"
     container_client = ContainerClient(mode, container_image, volumes, py_user_base_unix, extra_kwargs)
     if verbose:
-        print('Starting container')
+        print("Starting container")
     container_client.start()
 
     if verbose and install_si_from_source:
         print("******")
         print("Container started with the following paths")
         print(si_dev_path_unix, si_source_folder)
-    
+
     # check if container contains spikeinterface already
-    cmd_1 = ['python', '-c', 'import spikeinterface; print(spikeinterface.__version__)']
-    cmd_2 = ['python', '-c', 'from spikeinterface.sorters import run_sorter_local']
-    res_output = ''
+    cmd_1 = ["python", "-c", "import spikeinterface; print(spikeinterface.__version__)"]
+    cmd_2 = ["python", "-c", "from spikeinterface.sorters import run_sorter_local"]
+    res_output = ""
     for cmd in [cmd_1, cmd_2]:
         res_output += str(container_client.run_command(cmd))
-    need_si_install = 'ModuleNotFoundError' in res_output
+    need_si_install = "ModuleNotFoundError" in res_output
 
     if need_si_install:
-        if 'dev' in si_version:
+        if "dev" in si_version:
             if verbose:
                 print(f"Installing spikeinterface from sources in {container_image}")
 
             # TODO later check output
             if install_si_from_source:
-                si_source = 'local machine'
+                si_source = "local machine"
                 # install in local copy of host SI folder in sources/spikeinterface to avoid permission errors
-                cmd = f'mkdir {si_source_folder}'
+                cmd = f"mkdir {si_source_folder}"
                 res_output = container_client.run_command(cmd)
-                cmd = f'cp -r {si_dev_path_unix} {si_source_folder}'
+                cmd = f"cp -r {si_dev_path_unix} {si_source_folder}"
                 res_output = container_client.run_command(cmd)
-                cmd = f'pip install {si_source_folder}/spikeinterface[full]'
+                cmd = f"pip install {si_source_folder}/spikeinterface[full]"
             else:
-                si_source = 'remote repository'
-                cmd = 'pip install --upgrade --no-input git+https://github.com/SpikeInterface/spikeinterface.git#egg=spikeinterface[full]'
+                si_source = "remote repository"
+                cmd = "pip install --upgrade --no-input git+https://github.com/SpikeInterface/spikeinterface.git#egg=spikeinterface[full]"
             if verbose:
-                print(f'Installing dev spikeinterface from {si_source}')
+                print(f"Installing dev spikeinterface from {si_source}")
             res_output = container_client.run_command(cmd)
-            cmd = 'pip install --upgrade --no-input https://github.com/NeuralEnsemble/python-neo/archive/master.zip'
+            cmd = "pip install --upgrade --no-input https://github.com/NeuralEnsemble/python-neo/archive/master.zip"
             res_output = container_client.run_command(cmd)
         else:
             if verbose:
                 print(f"Installing spikeinterface=={si_version} in {container_image}")
-            cmd = f'pip install --upgrade --no-input spikeinterface[full]=={si_version}'
+            cmd = f"pip install --upgrade --no-input spikeinterface[full]=={si_version}"
             res_output = container_client.run_command(cmd)
     else:
         # TODO version checking
         if verbose:
-            print(f'spikeinterface is already installed in {container_image}')
+            print(f"spikeinterface is already installed in {container_image}")
 
-    if hasattr(recording, 'extra_requirements'):
+    if hasattr(recording, "extra_requirements"):
         extra_requirements.extend(recording.extra_requirements)
 
     # install additional required dependencies
     if extra_requirements:
         if verbose:
-            print(f'Installing extra requirements: {extra_requirements}')
+            print(f"Installing extra requirements: {extra_requirements}")
         cmd = f"pip install --upgrade --no-input {' '.join(extra_requirements)}"
         res_output = container_client.run_command(cmd)
 
     # run sorter on folder
     if verbose:
-        print(f'Running {sorter_name} sorter inside {container_image}')
+        print(f"Running {sorter_name} sorter inside {container_image}")
 
     # this do not work with singularity:
     # cmd = 'python "{}"'.format(parent_folder/'in_container_sorter_script.py')
     # this approach is better
-    in_container_script_path_unix = (Path(parent_folder_unix) / 'in_container_sorter_script.py').as_posix()
-    cmd = ['python', f'{in_container_script_path_unix}']
+    in_container_script_path_unix = (Path(parent_folder_unix) / "in_container_sorter_script.py").as_posix()
+    cmd = ["python", f"{in_container_script_path_unix}"]
     res_output = container_client.run_command(cmd)
     run_sorter_output = res_output
 
     # chown folder to user uid
     if platform.system() != "Windows":
         uid = os.getuid()
         # this do not work with singularity:
-        # cmd = f'chown {uid}:{uid} -R "{output_folder}"'
+        # cmd = f'chown {uid}:{uid} -R "{output_folder}"'
         # this approach is better
-        cmd = ['chown', f'{uid}:{uid}', '-R', f'{output_folder}']
+        cmd = ["chown", f"{uid}:{uid}", "-R", f"{output_folder}"]
         res_output = container_client.run_command(cmd)
     else:
         # not needed for Windows
         pass
 
     if verbose:
-        print('Stopping container')
+        print("Stopping container")
     container_client.stop()
 
     # clean useless files
-    os.remove(parent_folder / 'in_container_recording.json')
-    os.remove(parent_folder / 'in_container_params.json')
-    os.remove(parent_folder / 'in_container_sorter_script.py')
-    if mode == 'singularity':
-        shutil.rmtree(py_user_base_folder)
+    if delete_container_files:
+        os.remove(parent_folder / "in_container_recording.json")
+        os.remove(parent_folder / "in_container_params.json")
+        os.remove(parent_folder / "in_container_sorter_script.py")
+        if mode == "singularity":
+            shutil.rmtree(py_user_base_folder)
 
     # check error
     output_folder = Path(output_folder)
-    log_file = output_folder / 'spikeinterface_log.json'
+    log_file = output_folder / "spikeinterface_log.json"
     if not log_file.is_file():
         run_error = True
     else:
-        with log_file.open('r', encoding='utf8') as f:
+        with log_file.open("r", encoding="utf8") as f:
             log = json.load(f)
-        run_error = bool(log['error'])
+        run_error = bool(log["error"])
 
     sorting = None
     if run_error:
         if raise_error:
-            raise SpikeSortingError(
-                f"Spike sorting in {mode} failed with the following error:\n{run_sorter_output}")
+            raise SpikeSortingError(f"Spike sorting in {mode} failed with the following error:\n{run_sorter_output}")
     else:
         if with_output:
             try:
                 sorting = SorterClass.get_result_from_folder(output_folder)
             except Exception as e:
                 if verbose:
-                    print('Failed to get result with sorter specific extractor.\n'
-                          f'Error Message: {e}\n'
-                          'Getting result from in-container saved NpzSortingExtractor')
+                    print(
+                        "Failed to get result with sorter specific extractor.\n"
+                        f"Error Message: {e}\n"
+                        "Getting result from in-container saved NpzSortingExtractor"
+                    )
                 try:
                     sorting = NpzSortingExtractor.load_from_folder(npz_sorting_path)
                 except FileNotFoundError:
                     SpikeSortingError(f"Spike sorting in {mode} failed with the following error:\n{run_sorter_output}")
 
+    sorter_output_folder = output_folder / "sorter_output"
     if delete_output_folder:
-        shutil.rmtree(output_folder)
+        shutil.rmtree(sorter_output_folder)
 
     return sorting
 
 
-_common_run_doc = """
+_common_run_doc = (
+    """
     Runs {} sorter
-    """ + _common_param_doc
+    """
+    + _common_param_doc
+)
 
 
 def read_sorter_folder(output_folder, raise_error=True):
     """
     Load a sorting object from a spike sorting output folder.
     The 'output_folder' must contain a valid 'spikeinterface_log.json' file
     """
     output_folder = Path(output_folder)
-    log_file = output_folder / 'spikeinterface_log.json'
+    log_file = output_folder / "spikeinterface_log.json"
 
     if not log_file.is_file():
-        raise Exception(
-            f'This folder {output_folder} does not have spikeinterface_log.json')
+        raise Exception(f"This folder {output_folder} does not have spikeinterface_log.json")
 
-    with log_file.open('r', encoding='utf8') as f:
+    with log_file.open("r", encoding="utf8") as f:
         log = json.load(f)
 
-    run_error = bool(log['error'])
+    run_error = bool(log["error"])
     if run_error:
         if raise_error:
-            raise SpikeSortingError(
-                f"Spike sorting failed for {output_folder}")
+            raise SpikeSortingError(f"Spike sorting failed for {output_folder}")
         else:
             return
 
-    sorter_name = log['sorter_name']
+    sorter_name = log["sorter_name"]
     SorterClass = sorter_dict[sorter_name]
     sorting = SorterClass.get_result_from_folder(output_folder)
     return sorting
 
 
 def run_hdsort(*args, **kwargs):
-    return run_sorter('hdsort', *args, **kwargs)
+    warn(
+        "run_hdsort is deprecated. Use run_sorter(sorter_name='hdsort') instead.",
+        DeprecationWarning,
+        stacklevel=2,
+    )
+    return run_sorter("hdsort", *args, **kwargs)
 
 
-run_hdsort.__doc__ = _common_run_doc.format('hdsort')
+run_hdsort.__doc__ = _common_run_doc.format("hdsort")
 
 
 def run_klusta(*args, **kwargs):
-    return run_sorter('klusta', *args, **kwargs)
+    warn(
+        "run_klusta is deprecated. Use run_sorter(sorter_name='klusta') instead.",
+        DeprecationWarning,
+        stacklevel=2,
+    )
+    return run_sorter("klusta", *args, **kwargs)
 
 
-run_klusta.__doc__ = _common_run_doc.format('klusta')
+run_klusta.__doc__ = _common_run_doc.format("klusta")
 
 
 def run_tridesclous(*args, **kwargs):
-    return run_sorter('tridesclous', *args, **kwargs)
+    warn(
+        "run_tridesclous is deprecated. Use run_sorter(sorter_name='tridesclous') instead.",
+        DeprecationWarning,
+        stacklevel=2,
+    )
+    return run_sorter("tridesclous", *args, **kwargs)
 
 
-run_tridesclous.__doc__ = _common_run_doc.format('tridesclous')
+run_tridesclous.__doc__ = _common_run_doc.format("tridesclous")
 
 
 def run_mountainsort4(*args, **kwargs):
-    return run_sorter('mountainsort4', *args, **kwargs)
+    warn(
+        "run_mountainsort4 is deprecated. Use run_sorter(sorter_name='mountainsort4') instead.",
+        DeprecationWarning,
+        stacklevel=2,
+    )
+    return run_sorter("mountainsort4", *args, **kwargs)
+
+
+run_mountainsort4.__doc__ = _common_run_doc.format("mountainsort4")
+
+
+def run_mountainsort5(*args, **kwargs):
+    warn(
+        "run_mountainsort5 is deprecated. Use run_sorter(sorter_name='mountainsort5') instead.",
+        DeprecationWarning,
+        stacklevel=2,
+    )
+    return run_sorter("mountainsort5", *args, **kwargs)
 
 
-run_mountainsort4.__doc__ = _common_run_doc.format('mountainsort4')
+run_mountainsort5.__doc__ = _common_run_doc.format("mountainsort5")
 
 
 def run_ironclust(*args, **kwargs):
-    return run_sorter('ironclust', *args, **kwargs)
+    warn(
+        "run_ironclust is deprecated. Use run_sorter(sorter_name='ironclust') instead.",
+        DeprecationWarning,
+        stacklevel=2,
+    )
+    return run_sorter("ironclust", *args, **kwargs)
 
 
-run_ironclust.__doc__ = _common_run_doc.format('ironclust')
+run_ironclust.__doc__ = _common_run_doc.format("ironclust")
 
 
 def run_kilosort(*args, **kwargs):
-    return run_sorter('kilosort', *args, **kwargs)
+    warn(
+        "run_kilosort is deprecated. Use run_sorter(sorter_name='kilosort') instead.",
+        DeprecationWarning,
+        stacklevel=2,
+    )
+    return run_sorter("kilosort", *args, **kwargs)
 
 
-run_kilosort.__doc__ = _common_run_doc.format('kilosort')
+run_kilosort.__doc__ = _common_run_doc.format("kilosort")
 
 
 def run_kilosort2(*args, **kwargs):
-    return run_sorter('kilosort2', *args, **kwargs)
+    warn(
+        "run_kilosort2 is deprecated. Use run_sorter(sorter_name='kilosort2') instead.",
+        DeprecationWarning,
+        stacklevel=2,
+    )
+    return run_sorter("kilosort2", *args, **kwargs)
 
 
-run_kilosort2.__doc__ = _common_run_doc.format('kilosort2')
+run_kilosort2.__doc__ = _common_run_doc.format("kilosort2")
 
 
 def run_kilosort2_5(*args, **kwargs):
-    return run_sorter('kilosort2_5', *args, **kwargs)
+    warn(
+        "run_kilosort2_5 is deprecated. Use run_sorter(sorter_name='kilosort2_5') instead.",
+        DeprecationWarning,
+        stacklevel=2,
+    )
+    return run_sorter("kilosort2_5", *args, **kwargs)
 
 
-run_kilosort2_5.__doc__ = _common_run_doc.format('kilosort2_5')
+run_kilosort2_5.__doc__ = _common_run_doc.format("kilosort2_5")
 
 
 def run_kilosort3(*args, **kwargs):
-    return run_sorter('kilosort3', *args, **kwargs)
+    warn(
+        "run_kilosort3 is deprecated. Use run_sorter(sorter_name='kilosort3') instead.",
+        DeprecationWarning,
+        stacklevel=2,
+    )
+    return run_sorter("kilosort3", *args, **kwargs)
 
 
-run_kilosort3.__doc__ = _common_run_doc.format('kilosort3')
+run_kilosort3.__doc__ = _common_run_doc.format("kilosort3")
 
 
 def run_spykingcircus(*args, **kwargs):
-    return run_sorter('spykingcircus', *args, **kwargs)
+    warn(
+        "run_spykingcircus is deprecated. Use run_sorter(sorter_name='spykingcircus') instead.",
+        DeprecationWarning,
+        stacklevel=2,
+    )
+    return run_sorter("spykingcircus", *args, **kwargs)
 
 
-run_spykingcircus.__doc__ = _common_run_doc.format('spykingcircus')
+run_spykingcircus.__doc__ = _common_run_doc.format("spykingcircus")
 
 
 def run_herdingspikes(*args, **kwargs):
-    return run_sorter('herdingspikes', *args, **kwargs)
+    warn(
+        "run_herdingspikes is deprecated. Use run_sorter(sorter_name='herdingspikes') instead.",
+        DeprecationWarning,
+        stacklevel=2,
+    )
+    return run_sorter("herdingspikes", *args, **kwargs)
 
 
-run_herdingspikes.__doc__ = _common_run_doc.format('herdingspikes')
+run_herdingspikes.__doc__ = _common_run_doc.format("herdingspikes")
 
 
 def run_waveclus(*args, **kwargs):
-    return run_sorter('waveclus', *args, **kwargs)
+    warn(
+        "run_waveclus is deprecated. Use run_sorter(sorter_name='waveclus') instead.",
+        DeprecationWarning,
+        stacklevel=2,
+    )
+    return run_sorter("waveclus", *args, **kwargs)
 
 
-run_waveclus.__doc__ = _common_run_doc.format('waveclus')
+run_waveclus.__doc__ = _common_run_doc.format("waveclus")
 
 
 def run_waveclus_snippets(*args, **kwargs):
-    return run_sorter('waveclus_snippets', *args, **kwargs)
+    warn(
+        "run_waveclus_snippets is deprecated. Use run_sorter(sorter_name='waveclus_snippets') instead.",
+        DeprecationWarning,
+        stacklevel=2,
+    )
+    return run_sorter("waveclus_snippets", *args, **kwargs)
 
 
 def run_combinato(*args, **kwargs):
-    return run_sorter('combinato', *args, **kwargs)
+    warn(
+        "run_combinato is deprecated. Use run_sorter(sorter_name='combinato') instead.",
+        DeprecationWarning,
+        stacklevel=2,
+    )
+    return run_sorter("combinato", *args, **kwargs)
 
 
-run_combinato.__doc__ = _common_run_doc.format('combinato')
+run_combinato.__doc__ = _common_run_doc.format("combinato")
 
 
 def run_yass(*args, **kwargs):
-    return run_sorter('yass', *args, **kwargs)
+    warn(
+        "run_yass is deprecated. Use run_sorter(sorter_name='yass') instead.",
+        DeprecationWarning,
+        stacklevel=2,
+    )
+    return run_sorter("yass", *args, **kwargs)
 
 
-run_yass.__doc__ = _common_run_doc.format('yass')
+run_yass.__doc__ = _common_run_doc.format("yass")
 
 
 def run_pykilosort(*args, **kwargs):
-    return run_sorter('pykilosort', *args, **kwargs)
+    warn(
+        "run_pykilosort is deprecated. Use run_sorter(sorter_name='pykilosort') instead.",
+        DeprecationWarning,
+        stacklevel=2,
+    )
+    return run_sorter("pykilosort", *args, **kwargs)
 
 
-run_pykilosort.__doc__ = _common_run_doc.format('pykilosort')
+run_pykilosort.__doc__ = _common_run_doc.format("pykilosort")
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sorters/sorterlist.py` & `spikeinterface-0.98.0/src/spikeinterface/sorters/sorterlist.py`

 * *Files 2% similar despite different names*

```diff
@@ -5,14 +5,15 @@
 from .external.kilosort import KilosortSorter
 from .external.kilosort2 import Kilosort2Sorter
 from .external.kilosort2_5 import Kilosort2_5Sorter
 from .external.kilosort3 import Kilosort3Sorter
 from .external.pykilosort import PyKilosortSorter
 from .external.klusta import KlustaSorter
 from .external.mountainsort4 import Mountainsort4Sorter
+from .external.mountainsort5 import Mountainsort5Sorter
 from .external.spyking_circus import SpykingcircusSorter
 from .external.tridesclous import TridesclousSorter
 from .external.waveclus import WaveClusSorter
 from .external.waveclus_snippets import WaveClusSnippetsSorter
 from .external.yass import YassSorter
 
 # based on spikeinertface.sortingcomponents
@@ -28,23 +29,23 @@
     KilosortSorter,
     Kilosort2Sorter,
     Kilosort2_5Sorter,
     Kilosort3Sorter,
     PyKilosortSorter,
     KlustaSorter,
     Mountainsort4Sorter,
+    Mountainsort5Sorter,
     SpykingcircusSorter,
     TridesclousSorter,
     WaveClusSorter,
     WaveClusSnippetsSorter,
     YassSorter,
-
     # internal
     Spykingcircus2Sorter,
-    Tridesclous2Sorter
+    Tridesclous2Sorter,
 ]
 
 sorter_dict = {s.sorter_name: s for s in sorter_full_list}
 
 
 def available_sorters():
     """Lists available sorters."""
@@ -55,20 +56,20 @@
 def installed_sorters():
     """Lists installed sorters."""
 
     return sorted([s.sorter_name for s in sorter_full_list if s.is_installed()])
 
 
 def print_sorter_versions():
-    """"Prints the versions of the installed sorters."""
+    """ "Prints the versions of the installed sorters."""
 
-    txt = ''
+    txt = ""
     for name in installed_sorters():
         version = sorter_dict[name].get_sorter_version()
-        txt += '{}: {}\n'.format(name, version)
+        txt += "{}: {}\n".format(name, version)
     txt = txt[:-1]
     print(txt)
 
 
 def get_default_sorter_params(sorter_name_or_class):
     """Returns default parameters for the specified sorter.
 
@@ -84,15 +85,15 @@
     """
 
     if isinstance(sorter_name_or_class, str):
         SorterClass = sorter_dict[sorter_name_or_class]
     elif sorter_name_or_class in sorter_full_list:
         SorterClass = sorter_name_or_class
     else:
-        raise (ValueError('Unknown sorter'))
+        raise (ValueError("Unknown sorter"))
 
     return SorterClass.default_params()
 
 
 def get_sorter_params_description(sorter_name_or_class):
     """Returns a description of the parameters for the specified sorter.
 
@@ -108,15 +109,15 @@
     """
 
     if isinstance(sorter_name_or_class, str):
         SorterClass = sorter_dict[sorter_name_or_class]
     elif sorter_name_or_class in sorter_full_list:
         SorterClass = sorter_name_or_class
     else:
-        raise (ValueError('Unknown sorter'))
+        raise (ValueError("Unknown sorter"))
 
     return SorterClass.params_description()
 
 
 def get_sorter_description(sorter_name_or_class):
     """Returns a brief description for the specified sorter.
 
@@ -132,10 +133,10 @@
     """
 
     if isinstance(sorter_name_or_class, str):
         SorterClass = sorter_dict[sorter_name_or_class]
     elif sorter_name_or_class in sorter_full_list:
         SorterClass = sorter_name_or_class
     else:
-        raise (ValueError('Unknown sorter'))
+        raise (ValueError("Unknown sorter"))
 
     return SorterClass.sorter_description
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sorters/utils/constructNPYheader.m` & `spikeinterface-0.98.0/src/spikeinterface/sorters/utils/constructNPYheader.m`

 * *Files identical despite different names*

### Comparing `spikeinterface-0.97.1/spikeinterface/sorters/utils/misc.py` & `spikeinterface-0.98.0/src/spikeinterface/sorters/utils/misc.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,73 +1,80 @@
 from pathlib import Path
 from subprocess import check_output, CalledProcessError
 from typing import List, Union
 
 import numpy as np
 
+
 class SpikeSortingError(RuntimeError):
     """Raised whenever spike sorting fails"""
 
 
 def get_bash_path():
     """Return path to existing bash install."""
     try:
-        return check_output( ['which bash'], shell=True).decode().strip('\n')
+        return check_output(["which bash"], shell=True).decode().strip("\n")
     except CalledProcessError as e:
         raise Exception("Bash is not installed or accessible on your system.")
 
 
 def get_matlab_shell_name():
     """Return name of shell program used by MATLAB.
-    
+
     As per MATLAB docs:
     'On UNIX, MATLAB uses a shell program to execute the given command. It
     determines which shell program to use by checking environment variables on
     your system. MATLAB first checks the MATLAB_SHELL variable, and if either
     empty or not defined, then checks SHELL. If SHELL is also empty or not
     defined, MATLAB uses /bin/sh'
     """
     try:
         # Either of "", "bash", "zsh", "fish",...
         # CalledProcessError if not defined
-        matlab_shell_name = check_output(['which $MATLAB_SHELL'], shell=True).decode().strip('\n').split('/')[-1]  
+        matlab_shell_name = check_output(["which $MATLAB_SHELL"], shell=True).decode().strip("\n").split("/")[-1]
         return matlab_shell_name
     except CalledProcessError as e:
         pass
     try:
         # Either of "", "bash", "zsh", "fish",...
         # CalledProcessError if not defined
-        df_shell_name = check_output(['which $SHELL'], shell=True).decode().strip('\n').split('/')[-1]  
+        df_shell_name = check_output(["which $SHELL"], shell=True).decode().strip("\n").split("/")[-1]
         return df_shell_name
     except CalledProcessError as e:
         pass
     return "sh"
 
 
 def get_git_commit(git_folder, shorten=True):
     """
     Get commit to generate sorters version.
     """
     if git_folder is None:
         return None
     try:
-        commit = check_output(['git', 'rev-parse', 'HEAD'], cwd=git_folder).decode('utf8').strip()
+        commit = check_output(["git", "rev-parse", "HEAD"], cwd=git_folder).decode("utf8").strip()
         if shorten:
             commit = commit[:12]
     except:
         commit = None
     return commit
 
 
 def has_nvidia():
     """
     Checks if the machine has nvidia capability.
     """
-    from cuda import cuda
-        
+
+    try:
+        from cuda import cuda
+    except ModuleNotFoundError as err:
+        raise Exception(
+            "This sorter requires cuda, but the package 'cuda-python' is not installed. You can install it with:\npip install cuda-python"
+        ) from err
+
     try:
-        cu_result_init,  = cuda.cuInit(0)
+        (cu_result_init,) = cuda.cuInit(0)
         cu_result, cu_string = cuda.cuGetErrorString(cu_result_init)
         cu_result_device_count, device_count = cuda.cuDeviceGetCount()
         return device_count > 0
-    except RuntimeError: #  Failed to dlopen libcuda.so
-        return False
+    except RuntimeError:  #  Failed to dlopen libcuda.so
+        return False
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sorters/utils/shellscript.py` & `spikeinterface-0.98.0/src/spikeinterface/sorters/utils/shellscript.py`

 * *Files 6% similar despite different names*

```diff
@@ -7,87 +7,96 @@
 import time
 import sys
 from typing import Optional, List, Any, Union
 
 PathType = Union[str, Path]
 
 
-class ShellScript():
-    def __init__(self, script: str, script_path: Optional[PathType] = None, log_path: Optional[PathType] = None,
-                 keep_temp_files: bool = False, verbose: bool = False):
+class ShellScript:
+    def __init__(
+        self,
+        script: str,
+        script_path: Optional[PathType] = None,
+        log_path: Optional[PathType] = None,
+        keep_temp_files: bool = False,
+        verbose: bool = False,
+    ):
         lines = script.splitlines()
         lines = self._remove_initial_blank_lines(lines)
         if len(lines) > 0:
             num_initial_spaces = self._get_num_initial_spaces(lines[0])
             for ii, line in enumerate(lines):
                 if len(line.strip()) > 0:
                     n = self._get_num_initial_spaces(line)
                     if n < num_initial_spaces:
                         print(script)
-                        raise Exception('Problem in script. First line must not be indented relative to others')
+                        raise Exception("Problem in script. First line must not be indented relative to others")
                     lines[ii] = lines[ii][num_initial_spaces:]
-        self._script = '\n'.join(lines)
+        self._script = "\n".join(lines)
         self._script_path = script_path
         self._log_path = log_path
         self._keep_temp_files = keep_temp_files
         self._process: Optional[subprocess.Popen] = None
         self._files_to_remove: List[str] = []
         self._dirs_to_remove: List[str] = []
         self._start_time: Optional[float] = None
         self._verbose = verbose
 
     def __del__(self):
         self.cleanup()
 
     def substitute(self, old: str, new: Any) -> None:
-        self._script = self._script.replace(old, '{}'.format(new))
+        self._script = self._script.replace(old, "{}".format(new))
 
     def write(self, script_path: Optional[str] = None) -> None:
         if script_path is None:
             script_path = self._script_path
         if script_path is None:
-            raise Exception('Cannot write script. No path specified')
-        with open(script_path, 'w') as f:
+            raise Exception("Cannot write script. No path specified")
+        with open(script_path, "w") as f:
             f.write(self._script)
         os.chmod(script_path, 0o744)
 
     def start(self) -> None:
         if self._script_path is not None:
             script_path = Path(self._script_path)
-            if script_path.suffix == '':
-                if 'win' in sys.platform and sys.platform != 'darwin':
-                    script_path = script_path.parent / (script_path.name + '.bat')
+            if script_path.suffix == "":
+                if "win" in sys.platform and sys.platform != "darwin":
+                    script_path = script_path.parent / (script_path.name + ".bat")
                 else:
-                    script_path = script_path.parent / (script_path.name + '.sh')
+                    script_path = script_path.parent / (script_path.name + ".sh")
         else:
-            tempdir = Path(tempfile.mkdtemp(prefix='tmp_shellscript'))
-            if 'win' in sys.platform and sys.platform != 'darwin':
-                script_path = tempdir / 'script.bat'
+            tempdir = Path(tempfile.mkdtemp(prefix="tmp_shellscript"))
+            if "win" in sys.platform and sys.platform != "darwin":
+                script_path = tempdir / "script.bat"
             else:
-                script_path = tempdir / 'script.sh'
+                script_path = tempdir / "script.sh"
             self._dirs_to_remove.append(tempdir)
 
         if self._log_path is None:
-            script_log_path = script_path.parent / 'spike_sorters_log.txt'
+            script_log_path = script_path.parent / "spike_sorters_log.txt"
         else:
             script_log_path = Path(self._log_path)
-            if script_path.suffix == '':
-                script_log_path = script_log_path.parent / (script_log_path.name + '.txt')
+            if script_path.suffix == "":
+                script_log_path = script_log_path.parent / (script_log_path.name + ".txt")
 
         self.write(script_path)
         cmd = str(script_path)
         if self._verbose:
-            print('RUNNING SHELL SCRIPT: ' + cmd)
+            print("RUNNING SHELL SCRIPT: " + cmd)
         self._start_time = time.time()
-        self._process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, bufsize=1,
-                                         universal_newlines=True)
-        with open(script_log_path, 'w+') as script_log_file:
+        self._process = subprocess.Popen(
+            cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, bufsize=1, universal_newlines=True
+        )
+        with open(script_log_path, "w+") as script_log_file:
             for line in self._process.stdout:
                 script_log_file.write(line)
-                if self._verbose:  # Print onto console depending on the verbose property passed on from the sorter class
+                if (
+                    self._verbose
+                ):  # Print onto console depending on the verbose property passed on from the sorter class
                     print(line)
 
     def wait(self, timeout=None) -> Optional[int]:
         if not self.isRunning():
             return self.returnCode()
         assert self._process is not None, "Unexpected self._process is None even though it is running."
         try:
@@ -122,15 +131,15 @@
             return
 
         assert self._process is not None, "Unexpected self._process is None even though it is running."
         self._process.send_signal(signal.SIGKILL)
         try:
             self._process.wait(timeout=1)
         except:
-            print('WARNING: unable to kill shell script.')
+            print("WARNING: unable to kill shell script.")
             pass
 
     def stopWithSignal(self, sig, timeout) -> bool:
         if not self.isRunning():
             return True
 
         assert self._process is not None, "Unexpected self._process is None even though it is running."
@@ -158,40 +167,40 @@
     def isFinished(self) -> bool:
         if not self._process:
             return False
         return not self.isRunning()
 
     def returnCode(self) -> Optional[int]:
         if not self.isFinished():
-            raise Exception('Cannot get return code before process is finished.')
+            raise Exception("Cannot get return code before process is finished.")
         assert self._process is not None, "Unexpected self._process is None even though it is finished."
         return self._process.returncode
 
     def scriptPath(self) -> Optional[str]:
         return self._script_path
 
     def _remove_initial_blank_lines(self, lines: List[str]) -> List[str]:
         ii = 0
         while ii < len(lines) and len(lines[ii].strip()) == 0:
             ii = ii + 1
         return lines[ii:]
 
     def _get_num_initial_spaces(self, line: str) -> int:
         ii = 0
-        while ii < len(line) and line[ii] == ' ':
+        while ii < len(line) and line[ii] == " ":
             ii = ii + 1
         return ii
 
 
 def _rmdir_with_retries(dirname, num_retries, delay_between_tries=1):
     for retry_num in range(1, num_retries + 1):
         if not os.path.exists(dirname):
             return
         try:
             shutil.rmtree(dirname)
             break
         except:
             if retry_num < num_retries:
-                print('Retrying to remove directory: {}'.format(dirname))
+                print("Retrying to remove directory: {}".format(dirname))
                 time.sleep(delay_between_tries)
             else:
-                raise Exception('Unable to remove directory after {} tries: {}'.format(num_retries, dirname))
+                raise Exception("Unable to remove directory after {} tries: {}".format(num_retries, dirname))
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sorters/utils/writeNPY.m` & `spikeinterface-0.98.0/src/spikeinterface/sorters/utils/writeNPY.m`

 * *Ordering differences only*

 * *Files 0% similar despite different names*

```diff
@@ -16,8 +16,8 @@
 
 fid = fopen(filename, 'w');
 fwrite(fid, header, 'uint8');
 fwrite(fid, var, dataType);
 fclose(fid);
 
 
-end
+end
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sortingcomponents/benchmark/benchmark_clustering.py` & `spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/benchmark/benchmark_clustering.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,61 +1,66 @@
-
 from spikeinterface.core import extract_waveforms
 from spikeinterface.sortingcomponents.clustering import find_cluster_from_peaks, clustering_methods
 from spikeinterface.preprocessing import bandpass_filter, common_reference
 from spikeinterface.sortingcomponents.clustering import find_cluster_from_peaks
 from spikeinterface.extractors import read_mearec
 from spikeinterface.core import NumpySorting
 from spikeinterface.qualitymetrics import compute_quality_metrics
 from spikeinterface.comparison import GroundTruthComparison
-from spikeinterface.widgets import plot_probe_map, plot_agreement_matrix, plot_comparison_collision_by_similarity, plot_unit_templates, plot_unit_waveforms
+from spikeinterface.widgets import (
+    plot_probe_map,
+    plot_agreement_matrix,
+    plot_comparison_collision_by_similarity,
+    plot_unit_templates,
+    plot_unit_waveforms,
+)
 from spikeinterface.postprocessing import compute_principal_components
 from spikeinterface.comparison.comparisontools import make_matching_events
 from spikeinterface.postprocessing import get_template_extremum_channel
-from spikeinterface.core import get_noise_levels 
+from spikeinterface.core import get_noise_levels
 
 import time
 import string, random
 import pylab as plt
 import os
 import numpy as np
 
-class BenchmarkClustering:
 
+class BenchmarkClustering:
     def __init__(self, recording, gt_sorting, method, exhaustive_gt=True, tmp_folder=None, job_kwargs={}, verbose=True):
-
         self.method = method
 
-        assert method in clustering_methods, "Clustering method should be in %s" %clustering_methods.keys()
+        assert method in clustering_methods, "Clustering method should be in %s" % clustering_methods.keys()
 
         self.verbose = verbose
         self.recording = recording
         self.gt_sorting = gt_sorting
         self.job_kwargs = job_kwargs
         self.exhaustive_gt = exhaustive_gt
         self.recording_f = recording
         self.sampling_rate = self.recording_f.get_sampling_frequency()
         self.job_kwargs = job_kwargs
 
         self.tmp_folder = tmp_folder
         if self.tmp_folder is None:
-            self.tmp_folder = os.path.join('.', ''.join(random.choices(string.ascii_uppercase + string.digits, k=8)))
+            self.tmp_folder = os.path.join(".", "".join(random.choices(string.ascii_uppercase + string.digits, k=8)))
 
         self._peaks = None
         self._selected_peaks = None
         self._positions = None
         self._gt_positions = None
         self.gt_peaks = None
 
         self.waveforms = {}
         self.pcas = {}
         self.templates = {}
 
     def __del__(self):
         import shutil
+
         shutil.rmtree(self.tmp_folder)
 
     def set_peaks(self, peaks):
         self._peaks = peaks
 
     def set_positions(self, positions):
         self._positions = positions
@@ -83,155 +88,190 @@
 
     @property
     def gt_positions(self):
         if self._gt_positions is None:
             self.localize_gt_peaks()
         return self._gt_positions
 
-    def detect_peaks(self, method_kwargs={'method' : 'locally_exclusive'}):
+    def detect_peaks(self, method_kwargs={"method": "locally_exclusive"}):
         from spikeinterface.sortingcomponents.peak_detection import detect_peaks
+
         if self.verbose:
-            method = method_kwargs['method']
-            print(f'Detecting peaks with method {method}')
+            method = method_kwargs["method"]
+            print(f"Detecting peaks with method {method}")
         self._peaks = detect_peaks(self.recording_f, **method_kwargs, **self.job_kwargs)
 
-    def select_peaks(self, method_kwargs = {'method' : 'uniform', 'n_peaks' : 100}):
+    def select_peaks(self, method_kwargs={"method": "uniform", "n_peaks": 100}):
         from spikeinterface.sortingcomponents.peak_selection import select_peaks
+
         if self.verbose:
-            method = method_kwargs['method']
-            print(f'Selecting peaks with method {method}')
+            method = method_kwargs["method"]
+            print(f"Selecting peaks with method {method}")
         self._selected_peaks = select_peaks(self.peaks, **method_kwargs, **self.job_kwargs)
         if self.verbose:
-            ratio = len(self._selected_peaks)/len(self.peaks)
-            print(f'The ratio of peaks kept for clustering is {ratio}%')
+            ratio = len(self._selected_peaks) / len(self.peaks)
+            print(f"The ratio of peaks kept for clustering is {ratio}%")
 
-    def localize_peaks(self, method_kwargs = {'method' : 'center_of_mass'}):
+    def localize_peaks(self, method_kwargs={"method": "center_of_mass"}):
         from spikeinterface.sortingcomponents.peak_localization import localize_peaks
+
         if self.verbose:
-            method = method_kwargs['method']
-            print(f'Localizing peaks with method {method}')
+            method = method_kwargs["method"]
+            print(f"Localizing peaks with method {method}")
         self._positions = localize_peaks(self.recording_f, self.selected_peaks, **method_kwargs, **self.job_kwargs)
 
-    def localize_gt_peaks(self, method_kwargs = {'method' : 'center_of_mass'}):
+    def localize_gt_peaks(self, method_kwargs={"method": "center_of_mass"}):
         from spikeinterface.sortingcomponents.peak_localization import localize_peaks
+
         if self.verbose:
-            method = method_kwargs['method']
-            print(f'Localizing gt peaks with method {method}')
+            method = method_kwargs["method"]
+            print(f"Localizing gt peaks with method {method}")
         self._gt_positions = localize_peaks(self.recording_f, self.gt_peaks, **method_kwargs, **self.job_kwargs)
 
     def run(self, peaks=None, positions=None, method=None, method_kwargs={}, delta=0.2):
         t_start = time.time()
         if method is not None:
             self.method = method
         if peaks is not None:
             self._peaks = peaks
             self._selected_peaks = peaks
 
         nb_peaks = len(self.selected_peaks)
         if self.verbose:
-            print(f'Launching the {self.method} clustering algorithm with {nb_peaks} peaks')
+            print(f"Launching the {self.method} clustering algorithm with {nb_peaks} peaks")
 
         if positions is not None:
             self._positions = positions
 
-        labels, peak_labels = find_cluster_from_peaks(self.recording_f, self.selected_peaks, method=self.method, method_kwargs=method_kwargs, **self.job_kwargs)
+        labels, peak_labels = find_cluster_from_peaks(
+            self.recording_f, self.selected_peaks, method=self.method, method_kwargs=method_kwargs, **self.job_kwargs
+        )
         nb_clusters = len(labels)
         if self.verbose:
-            print(f'{nb_clusters} clusters have been found')
+            print(f"{nb_clusters} clusters have been found")
         self.noise = peak_labels == -1
         self.run_time = time.time() - t_start
         self.selected_peaks_labels = peak_labels
         self.labels = labels
 
-        
-        self.clustering = NumpySorting.from_times_labels(self.selected_peaks['sample_ind'][~self.noise], self.selected_peaks_labels[~self.noise], self.sampling_rate)
+        self.clustering = NumpySorting.from_times_labels(
+            self.selected_peaks["sample_index"][~self.noise],
+            self.selected_peaks_labels[~self.noise],
+            self.sampling_rate,
+        )
         if self.verbose:
             print("Performing the comparison with (sliced) ground truth")
 
         times1 = self.gt_sorting.get_all_spike_trains()[0]
         times2 = self.clustering.get_all_spike_trains()[0]
-        matches = make_matching_events(times1[0], times2[0], int(delta*self.sampling_rate/1000))
+        matches = make_matching_events(times1[0], times2[0], int(delta * self.sampling_rate / 1000))
 
         self.matches = matches
-        idx = matches['index1']
-        self.sliced_gt_sorting = NumpySorting.from_times_labels(times1[0][idx], times1[1][idx], self.sampling_rate, unit_ids = self.gt_sorting.unit_ids)
+        idx = matches["index1"]
+        self.sliced_gt_sorting = NumpySorting.from_times_labels(
+            times1[0][idx], times1[1][idx], self.sampling_rate, unit_ids=self.gt_sorting.unit_ids
+        )
 
         self.comp = GroundTruthComparison(self.sliced_gt_sorting, self.clustering, exhaustive_gt=self.exhaustive_gt)
 
-        for label, sorting in zip(['gt', 'clustering', 'full_gt'], [self.sliced_gt_sorting, self.clustering, self.gt_sorting]): 
-
+        for label, sorting in zip(
+            ["gt", "clustering", "full_gt"], [self.sliced_gt_sorting, self.clustering, self.gt_sorting]
+        ):
             tmp_folder = os.path.join(self.tmp_folder, label)
             if os.path.exists(tmp_folder):
                 import shutil
-                shutil.rmtree(tmp_folder)
 
-            if not (label == 'full_gt' and label in self.waveforms):
+                shutil.rmtree(tmp_folder)
 
+            if not (label == "full_gt" and label in self.waveforms):
                 if self.verbose:
                     print(f"Extracting waveforms for {label}")
 
-                self.waveforms[label] = extract_waveforms(self.recording_f, sorting, tmp_folder, load_if_exists=True,
-                                       ms_before=2.5, ms_after=3.5, max_spikes_per_unit=500, return_scaled=False, 
-                                       **self.job_kwargs)
+                self.waveforms[label] = extract_waveforms(
+                    self.recording_f,
+                    sorting,
+                    tmp_folder,
+                    load_if_exists=True,
+                    ms_before=2.5,
+                    ms_after=3.5,
+                    max_spikes_per_unit=500,
+                    return_scaled=False,
+                    **self.job_kwargs,
+                )
 
-                #self.pcas[label] = compute_principal_components(self.waveforms[label], load_if_exists=True,
+                # self.pcas[label] = compute_principal_components(self.waveforms[label], load_if_exists=True,
                 #                     n_components=5, mode='by_channel_local',
                 #                     whiten=True, dtype='float32')
 
-                self.templates[label] = self.waveforms[label].get_all_templates(mode='median')
-    
+                self.templates[label] = self.waveforms[label].get_all_templates(mode="median")
+
         if self.gt_peaks is None:
             if self.verbose:
                 print("Computing gt peaks")
             gt_peaks_ = self.gt_sorting.to_spike_vector()
-            self.gt_peaks = np.zeros(gt_peaks_.size, dtype=[('sample_ind', '<i8'), ('channel_ind', '<i8'), ('segment_ind', '<i8')])
-            self.gt_peaks['sample_ind'] = gt_peaks_['sample_ind']
-            self.gt_peaks['segment_ind'] = gt_peaks_['segment_ind']
-            max_channels = get_template_extremum_channel(self.waveforms['full_gt'], peak_sign='neg', outputs='index')
+            self.gt_peaks = np.zeros(
+                gt_peaks_.size, dtype=[("sample_index", "<i8"), ("channel_index", "<i8"), ("segment_index", "<i8")]
+            )
+            self.gt_peaks["sample_index"] = gt_peaks_["sample_index"]
+            self.gt_peaks["segment_index"] = gt_peaks_["segment_index"]
+            max_channels = get_template_extremum_channel(self.waveforms["full_gt"], peak_sign="neg", outputs="index")
 
-            for unit_ind, unit_id in enumerate(self.waveforms['full_gt'].sorting.unit_ids):
-                mask = gt_peaks_['unit_ind'] == unit_ind
+            for unit_ind, unit_id in enumerate(self.waveforms["full_gt"].sorting.unit_ids):
+                mask = gt_peaks_["unit_index"] == unit_ind
                 max_channel = max_channels[unit_id]
-                self.gt_peaks['channel_ind'][mask] = max_channel
+                self.gt_peaks["channel_index"][mask] = max_channel
 
         self.sliced_gt_peaks = self.gt_peaks[idx]
         self.sliced_gt_positions = self.gt_positions[idx]
-        self.sliced_gt_labels = self.sliced_gt_sorting.to_spike_vector()['unit_ind']
-        self.gt_labels = self.gt_sorting.to_spike_vector()['unit_ind']
-
-
-    def _scatter_clusters(self, xs, ys, sorting, colors=None, labels=None, ax=None, n_std=2.0, force_black_for=[], s=1, alpha=0.5, show_ellipses=True):
+        self.sliced_gt_labels = self.sliced_gt_sorting.to_spike_vector()["unit_index"]
+        self.gt_labels = self.gt_sorting.to_spike_vector()["unit_index"]
 
+    def _scatter_clusters(
+        self,
+        xs,
+        ys,
+        sorting,
+        colors=None,
+        labels=None,
+        ax=None,
+        n_std=2.0,
+        force_black_for=[],
+        s=1,
+        alpha=0.5,
+        show_ellipses=True,
+    ):
         if colors is None:
             from spikeinterface.widgets import get_unit_colors
+
             colors = get_unit_colors(sorting)
 
         from matplotlib.patches import Ellipse
         import matplotlib.transforms as transforms
+
         ax = ax or plt.gca()
         # scatter and collect gaussian info
         means = {}
         covs = {}
         labels_ids = sorting.get_all_spike_trains()[0][1]
 
         for unit_ind, unit_id in enumerate(sorting.unit_ids):
             where = np.flatnonzero(labels_ids == unit_id)
             xk = xs[where]
             yk = ys[where]
-            
+
             if unit_id not in force_black_for:
                 ax.scatter(xk, yk, s=s, color=colors[unit_id], alpha=alpha, marker=".")
                 x_mean, y_mean = xk.mean(), yk.mean()
                 xycov = np.cov(xk, yk)
                 means[unit_id] = x_mean, y_mean
                 covs[unit_id] = xycov
                 ax.annotate(unit_id, (x_mean, y_mean))
-                ax.scatter([x_mean], [y_mean], s=50, c='k')
+                ax.scatter([x_mean], [y_mean], s=50, c="k")
             else:
-                ax.scatter(xk, yk, s=s, color='k', alpha=alpha, marker=".")
+                ax.scatter(xk, yk, s=s, color="k", alpha=alpha, marker=".")
 
         for unit_id in means.keys():
             mean_x, mean_y = means[unit_id]
             cov = covs[unit_id]
 
             with np.errstate(invalid="ignore"):
                 vx, vy = cov[0, 0], cov[1, 1]
@@ -253,237 +293,288 @@
                     .rotate_deg(45)
                     .scale(n_std * np.sqrt(vx), n_std * np.sqrt(vy))
                     .translate(mean_x, mean_y)
                 )
                 ell.set_transform(transform + ax.transData)
                 ax.add_patch(ell)
 
-
     def plot_clusters(self, show_probe=True, show_ellipses=True):
-
         fig, axs = plt.subplots(ncols=3, nrows=1, figsize=(15, 10))
-        fig.suptitle(f'Clustering results with {self.method}')
+        fig.suptitle(f"Clustering results with {self.method}")
         ax = axs[0]
-        ax.set_title('Full gt clusters')
+        ax.set_title("Full gt clusters")
         if show_probe:
             plot_probe_map(self.recording_f, ax=ax)
 
         from spikeinterface.widgets import get_unit_colors
+
         colors = get_unit_colors(self.gt_sorting)
-        self._scatter_clusters(self.gt_positions['x'], self.gt_positions['y'], self.gt_sorting, colors, s=1, alpha=0.5, ax=ax, show_ellipses=show_ellipses)
+        self._scatter_clusters(
+            self.gt_positions["x"],
+            self.gt_positions["y"],
+            self.gt_sorting,
+            colors,
+            s=1,
+            alpha=0.5,
+            ax=ax,
+            show_ellipses=show_ellipses,
+        )
         xlim = ax.get_xlim()
         ylim = ax.get_ylim()
-        ax.set_xlabel('x')
-        ax.set_ylabel('y')
+        ax.set_xlabel("x")
+        ax.set_ylabel("y")
 
         ax = axs[1]
-        ax.set_title('Sliced gt clusters')
+        ax.set_title("Sliced gt clusters")
         if show_probe:
             plot_probe_map(self.recording_f, ax=ax)
 
-        self._scatter_clusters(self.sliced_gt_positions['x'], self.sliced_gt_positions['y'], self.sliced_gt_sorting, colors, s=1, alpha=0.5, ax=ax, show_ellipses=show_ellipses)
+        self._scatter_clusters(
+            self.sliced_gt_positions["x"],
+            self.sliced_gt_positions["y"],
+            self.sliced_gt_sorting,
+            colors,
+            s=1,
+            alpha=0.5,
+            ax=ax,
+            show_ellipses=show_ellipses,
+        )
         if self.exhaustive_gt:
             ax.set_xlim(xlim)
             ax.set_ylim(ylim)
-        ax.set_xlabel('x')
+        ax.set_xlabel("x")
         ax.set_yticks([], [])
 
         ax = axs[2]
-        ax.set_title('Found clusters')
+        ax.set_title("Found clusters")
         if show_probe:
             plot_probe_map(self.recording_f, ax=ax)
-        ax.scatter(self.positions['x'][self.noise], self.positions['y'][self.noise], c='k', s=1, alpha=0.1)
-        self._scatter_clusters(self.positions['x'][~self.noise], self.positions['y'][~self.noise], self.clustering, s=1, alpha=0.5, ax=ax, show_ellipses=show_ellipses)
-        
-        ax.set_xlabel('x')
+        ax.scatter(self.positions["x"][self.noise], self.positions["y"][self.noise], c="k", s=1, alpha=0.1)
+        self._scatter_clusters(
+            self.positions["x"][~self.noise],
+            self.positions["y"][~self.noise],
+            self.clustering,
+            s=1,
+            alpha=0.5,
+            ax=ax,
+            show_ellipses=show_ellipses,
+        )
+
+        ax.set_xlabel("x")
         if self.exhaustive_gt:
             ax.set_xlim(xlim)
             ax.set_ylim(ylim)
             ax.set_yticks([], [])
 
-
     def plot_found_clusters(self, show_probe=True, show_ellipses=True):
-
         fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(10, 10))
-        fig.suptitle(f'Clustering results with {self.method}')
-        ax.set_title('Found clusters')
+        fig.suptitle(f"Clustering results with {self.method}")
+        ax.set_title("Found clusters")
         if show_probe:
             plot_probe_map(self.recording_f, ax=ax)
-        ax.scatter(self.positions['x'][self.noise], self.positions['y'][self.noise], c='k', s=1, alpha=0.1)
-        self._scatter_clusters(self.positions['x'][~self.noise], self.positions['y'][~self.noise], self.clustering, s=1, alpha=0.5, ax=ax, show_ellipses=show_ellipses)
-        
-        ax.set_xlabel('x')
+        ax.scatter(self.positions["x"][self.noise], self.positions["y"][self.noise], c="k", s=1, alpha=0.1)
+        self._scatter_clusters(
+            self.positions["x"][~self.noise],
+            self.positions["y"][~self.noise],
+            self.clustering,
+            s=1,
+            alpha=0.5,
+            ax=ax,
+            show_ellipses=show_ellipses,
+        )
+
+        ax.set_xlabel("x")
         if self.exhaustive_gt:
             ax.set_yticks([], [])
 
-
-    def plot_statistics(self, metric='cosine', annotations=True, detect_threshold=5):
-
+    def plot_statistics(self, metric="cosine", annotations=True, detect_threshold=5):
         fig, axs = plt.subplots(ncols=3, nrows=2, figsize=(15, 10))
-        
-        fig.suptitle(f'Clustering results with {self.method}')
-        metrics = compute_quality_metrics(self.waveforms['gt'], metric_names=['snr'], load_if_exists=False)
+
+        fig.suptitle(f"Clustering results with {self.method}")
+        metrics = compute_quality_metrics(self.waveforms["gt"], metric_names=["snr"], load_if_exists=False)
 
         ax = axs[0, 0]
         plot_agreement_matrix(self.comp, ax=ax)
         scores = self.comp.get_ordered_agreement_scores()
         ymin, ymax = ax.get_ylim()
         xmin, xmax = ax.get_xlim()
         unit_ids1 = scores.index.values
         unit_ids2 = scores.columns.values
         inds_1 = self.comp.sorting1.ids_to_indices(unit_ids1)
-        snrs = metrics['snr'][inds_1]
+        snrs = metrics["snr"][inds_1]
 
         nb_detectable = len(unit_ids1)
 
         if detect_threshold is not None:
             for count, snr in enumerate(snrs):
                 if snr < detect_threshold:
-                    ax.plot([xmin, xmax], [count, count], 'k')
+                    ax.plot([xmin, xmax], [count, count], "k")
                     nb_detectable -= 1
 
-        ax.plot([nb_detectable+0.5, nb_detectable+0.5], [ymin, ymax], 'r')
+        ax.plot([nb_detectable + 0.5, nb_detectable + 0.5], [ymin, ymax], "r")
 
         # import MEArec as mr
         # mearec_recording = mr.load_recordings(self.mearec_file)
         # positions = mearec_recording.template_locations[:]
 
         # self.found_positions = np.zeros((len(self.labels), 2))
         # for i in range(len(self.labels)):
         #     data = self.positions[self.selected_peaks_labels == self.labels[i]]
         #     self.found_positions[i] = np.median(data['x']), np.median(data['y'])
 
-        
         unit_ids1 = scores.index.values
         unit_ids2 = scores.columns.values
         inds_1 = self.comp.sorting1.ids_to_indices(unit_ids1)
         inds_2 = self.comp.sorting2.ids_to_indices(unit_ids2)
 
-        a = self.templates['gt'].reshape(len(self.templates['gt']), -1)[inds_1]
-        b = self.templates['clustering'].reshape(len(self.templates['clustering']), -1)[inds_2]
-        
+        a = self.templates["gt"].reshape(len(self.templates["gt"]), -1)[inds_1]
+        b = self.templates["clustering"].reshape(len(self.templates["clustering"]), -1)[inds_2]
+
         import sklearn
-        if metric == 'cosine':
+
+        if metric == "cosine":
             distances = sklearn.metrics.pairwise.cosine_similarity(a, b)
         else:
             distances = sklearn.metrics.pairwise_distances(a, b, metric)
 
         ax = axs[0, 1]
-        nb_peaks = np.array([len(self.sliced_gt_sorting.get_unit_spike_train(i)) for i in self.sliced_gt_sorting.unit_ids])
+        nb_peaks = np.array(
+            [len(self.sliced_gt_sorting.get_unit_spike_train(i)) for i in self.sliced_gt_sorting.unit_ids]
+        )
 
         nb_potentials = np.sum(scores.max(1).values > 0.1)
 
-        ax.plot(metrics['snr'][unit_ids1][inds_1[:nb_potentials]], nb_peaks[inds_1[:nb_potentials]], markersize=10, marker='.', ls='', c='k', label='Cluster potentially found')
-        ax.plot(metrics['snr'][unit_ids1][inds_1[nb_potentials:]], nb_peaks[inds_1[nb_potentials:]], markersize=10, marker='.', ls='', c='r', label='Cluster clearly missed')
+        ax.plot(
+            metrics["snr"][unit_ids1][inds_1[:nb_potentials]],
+            nb_peaks[inds_1[:nb_potentials]],
+            markersize=10,
+            marker=".",
+            ls="",
+            c="k",
+            label="Cluster potentially found",
+        )
+        ax.plot(
+            metrics["snr"][unit_ids1][inds_1[nb_potentials:]],
+            nb_peaks[inds_1[nb_potentials:]],
+            markersize=10,
+            marker=".",
+            ls="",
+            c="r",
+            label="Cluster clearly missed",
+        )
 
         if annotations:
-            for l,x,y in zip(unit_ids1[:len(inds_2)], metrics['snr'][unit_ids1][inds_1[:len(inds_2)]], nb_peaks[inds_1[:len(inds_2)]]):
+            for l, x, y in zip(
+                unit_ids1[: len(inds_2)],
+                metrics["snr"][unit_ids1][inds_1[: len(inds_2)]],
+                nb_peaks[inds_1[: len(inds_2)]],
+            ):
                 ax.annotate(l, (x, y))
 
-            for l,x,y in zip(unit_ids1[len(inds_2):], metrics['snr'][unit_ids1][inds_1[len(inds_2):]], nb_peaks[inds_1[len(inds_2):]]):
-                ax.annotate(l, (x, y),c='r')
+            for l, x, y in zip(
+                unit_ids1[len(inds_2) :],
+                metrics["snr"][unit_ids1][inds_1[len(inds_2) :]],
+                nb_peaks[inds_1[len(inds_2) :]],
+            ):
+                ax.annotate(l, (x, y), c="r")
 
         if detect_threshold is not None:
             ymin, ymax = ax.get_ylim()
-            ax.plot([detect_threshold, detect_threshold], [ymin, ymax], 'k--')
+            ax.plot([detect_threshold, detect_threshold], [ymin, ymax], "k--")
 
         ax.legend()
-        ax.set_xlabel('template snr')
-        ax.set_ylabel('nb spikes')
-        ax.spines['top'].set_visible(False)
-        ax.spines['right'].set_visible(False)
+        ax.set_xlabel("template snr")
+        ax.set_ylabel("nb spikes")
+        ax.spines["top"].set_visible(False)
+        ax.spines["right"].set_visible(False)
 
         ax = axs[0, 2]
-        im = ax.imshow(distances, aspect='auto')
+        im = ax.imshow(distances, aspect="auto")
         ax.set_title(metric)
         fig.colorbar(im, ax=ax)
 
         if detect_threshold is not None:
             for count, snr in enumerate(snrs):
                 if snr < detect_threshold:
-                    ax.plot([xmin, xmax], [count, count], 'w')
+                    ax.plot([xmin, xmax], [count, count], "w")
 
             ymin, ymax = ax.get_ylim()
-            ax.plot([nb_detectable+0.5, nb_detectable+0.5], [ymin, ymax], 'r')
-
+            ax.plot([nb_detectable + 0.5, nb_detectable + 0.5], [ymin, ymax], "r")
 
         ax.set_yticks(np.arange(0, len(scores.index)))
         ax.set_yticklabels(scores.index, fontsize=8)
 
         res = []
         nb_spikes = []
         energy = []
         nb_channels = []
 
-        
         noise_levels = get_noise_levels(self.recording_f, return_scaled=False)
 
         for found, real in zip(unit_ids2, unit_ids1):
-            wfs = self.waveforms['clustering'].get_waveforms(found)
-            wfs_real = self.waveforms['gt'].get_waveforms(real)
-            template = self.waveforms['clustering'].get_template(found)
-            template_real = self.waveforms['gt'].get_template(real)
+            wfs = self.waveforms["clustering"].get_waveforms(found)
+            wfs_real = self.waveforms["gt"].get_waveforms(real)
+            template = self.waveforms["clustering"].get_template(found)
+            template_real = self.waveforms["gt"].get_template(real)
             nb_channels += [np.sum(np.std(template_real, 0) < noise_levels)]
 
             wfs = wfs.reshape(len(wfs), -1)
             template = template.reshape(template.size, 1).T
             template_real = template_real.reshape(template_real.size, 1).T
 
-            if metric == 'cosine':
+            if metric == "cosine":
                 dist = sklearn.metrics.pairwise.cosine_similarity(template, template_real, metric).flatten().tolist()
             else:
                 dist = sklearn.metrics.pairwise_distances(template, template_real, metric).flatten().tolist()
             res += dist
             nb_spikes += [self.sliced_gt_sorting.get_unit_spike_train(real).size]
             energy += [np.linalg.norm(template_real)]
 
-
         ax = axs[1, 0]
         res = np.array(res)
         nb_spikes = np.array(nb_spikes)
         nb_channels = np.array(nb_channels)
         energy = np.array(energy)
 
-        snrs = metrics['snr'][unit_ids1][inds_1[:len(inds_2)]]
+        snrs = metrics["snr"][unit_ids1][inds_1[: len(inds_2)]]
         cm = ax.scatter(snrs, nb_spikes, c=res)
-        ax.set_xlabel('template snr')
-        ax.set_ylabel('nb spikes')
-        ax.spines['top'].set_visible(False)
-        ax.spines['right'].set_visible(False)
+        ax.set_xlabel("template snr")
+        ax.set_ylabel("nb spikes")
+        ax.spines["top"].set_visible(False)
+        ax.spines["right"].set_visible(False)
         cb = fig.colorbar(cm, ax=ax)
         cb.set_label(metric)
         if detect_threshold is not None:
             ymin, ymax = ax.get_ylim()
-            ax.plot([detect_threshold, detect_threshold], [ymin, ymax], 'k--')
+            ax.plot([detect_threshold, detect_threshold], [ymin, ymax], "k--")
 
         if annotations:
-            for l,x,y in zip(unit_ids1[:len(inds_2)], snrs, nb_spikes):
+            for l, x, y in zip(unit_ids1[: len(inds_2)], snrs, nb_spikes):
                 ax.annotate(l, (x, y))
 
         ax = axs[1, 1]
         cm = ax.scatter(energy, nb_channels, c=res)
-        ax.set_xlabel('template energy')
-        ax.set_ylabel('nb channels')
-        ax.spines['top'].set_visible(False)
-        ax.spines['right'].set_visible(False)
+        ax.set_xlabel("template energy")
+        ax.set_ylabel("nb channels")
+        ax.spines["top"].set_visible(False)
+        ax.spines["right"].set_visible(False)
         cb = fig.colorbar(cm, ax=ax)
         cb.set_label(metric)
-        
+
         if annotations:
-            for l,x,y in zip(unit_ids1[:len(inds_2)], energy, nb_channels):
+            for l, x, y in zip(unit_ids1[: len(inds_2)], energy, nb_channels):
                 ax.annotate(l, (x, y))
 
-
         ax = axs[1, 2]
-        for performance_name in ['accuracy', 'recall', 'precision']:
+        for performance_name in ["accuracy", "recall", "precision"]:
             perf = self.comp.get_performance()[performance_name]
-            ax.plot(metrics['snr'], perf, markersize=10, marker='.', ls='', label=performance_name)
-        ax.set_xlabel('template snr')
-        ax.set_ylabel('performance')
-        ax.spines['top'].set_visible(False)
-        ax.spines['right'].set_visible(False)
+            ax.plot(metrics["snr"], perf, markersize=10, marker=".", ls="", label=performance_name)
+        ax.set_xlabel("template snr")
+        ax.set_ylabel("performance")
+        ax.spines["top"].set_visible(False)
+        ax.spines["right"].set_visible(False)
         ax.legend()
         if detect_threshold is not None:
             ymin, ymax = ax.get_ylim()
-            ax.plot([detect_threshold, detect_threshold], [ymin, ymax], 'k--')
+            ax.plot([detect_threshold, detect_threshold], [ymin, ymax], "k--")
 
-        plt.tight_layout()
+        plt.tight_layout()
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sortingcomponents/benchmark/benchmark_motion_correction.py` & `spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/benchmark/benchmark_motion_interpolation.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,566 +1,635 @@
-
-
 import numpy as np
 import pandas as pd
 
 from pathlib import Path
 import shutil
 
 from spikeinterface.core import extract_waveforms, precompute_sparsity, WaveformExtractor
 
 
 from spikeinterface.extractors import read_mearec
-from spikeinterface.preprocessing import bandpass_filter, zscore, common_reference
+from spikeinterface.preprocessing import bandpass_filter, zscore, common_reference, scale, highpass_filter, whiten
 from spikeinterface.sorters import run_sorter
 from spikeinterface.widgets import plot_unit_waveforms, plot_gt_performances
 
 from spikeinterface.comparison import GroundTruthComparison
-from spikeinterface.sortingcomponents.motion_correction import CorrectMotionRecording
+from spikeinterface.sortingcomponents.motion_interpolation import InterpolateMotionRecording
 from spikeinterface.sortingcomponents.benchmark.benchmark_tools import BenchmarkBase, _simpleaxis
 from spikeinterface.qualitymetrics import compute_quality_metrics
 from spikeinterface.widgets import plot_sorting_performance
 from spikeinterface.qualitymetrics import compute_quality_metrics
 from spikeinterface.curation import MergeUnitsSorting
+from spikeinterface.core import get_template_extremum_channel
 
 import sklearn
 
 import matplotlib.pyplot as plt
 
 import MEArec as mr
 
-class BenchmarkMotionCorrectionMearec(BenchmarkBase):
-    
-    _array_names = ('motion', 'temporal_bins', 'spatial_bins')
-    _waveform_names = ('static', 'drifting', 'corrected')
+
+class BenchmarkMotionInterpolationMearec(BenchmarkBase):
+    _array_names = ("gt_motion", "estimated_motion", "temporal_bins", "spatial_bins")
+    _waveform_names = ("static", "drifting", "corrected_gt", "corrected_estimated")
     _sorting_names = ()
 
     _array_names_from_parent = ()
-    _waveform_names_from_parent = ('static', 'drifting')
-    _sorting_names_from_parent = ('static', 'drifting')
+    _waveform_names_from_parent = ("static", "drifting")
+    _sorting_names_from_parent = ("static", "drifting")
 
-    def __init__(self, mearec_filename_drifting, mearec_filename_static, 
-                motion,
-                temporal_bins,
-                spatial_bins,
-                do_preprocessing=True,
-                correct_motion_kwargs={},
-                sparse_kwargs=dict( method="radius", radius_um=100.,),
-                sorter_cases={},
-                folder=None,
-                title='',
-                job_kwargs={'chunk_duration' : '1s', 'n_jobs' : -1, 'progress_bar':True, 'verbose' :True}, 
-                overwrite=False,
-                delete_output_folder=True,
-                parent_benchmark=None):
-
-        BenchmarkBase.__init__(self, folder=folder, title=title, overwrite=overwrite, job_kwargs=job_kwargs,
-                               parent_benchmark=parent_benchmark)
+    def __init__(
+        self,
+        mearec_filename_drifting,
+        mearec_filename_static,
+        gt_motion,
+        estimated_motion,
+        temporal_bins,
+        spatial_bins,
+        do_preprocessing=True,
+        correct_motion_kwargs={},
+        waveforms_kwargs=dict(
+            ms_before=1.0,
+            ms_after=3.0,
+            max_spikes_per_unit=500,
+        ),
+        sparse_kwargs=dict(
+            method="radius",
+            radius_um=100.0,
+        ),
+        sorter_cases={},
+        folder=None,
+        title="",
+        job_kwargs={"chunk_duration": "1s", "n_jobs": -1, "progress_bar": True, "verbose": True},
+        overwrite=False,
+        delete_output_folder=True,
+        parent_benchmark=None,
+    ):
+        BenchmarkBase.__init__(
+            self,
+            folder=folder,
+            title=title,
+            overwrite=overwrite,
+            job_kwargs=job_kwargs,
+            parent_benchmark=parent_benchmark,
+        )
 
-        self._args.extend([str(mearec_filename_drifting), str(mearec_filename_static), None, None, None ])
-        
+        self._args.extend([str(mearec_filename_drifting), str(mearec_filename_static), None, None, None, None])
 
         self.sorter_cases = sorter_cases.copy()
-        self.mearec_filenames = {}  
-        self.keys = ['static', 'drifting', 'corrected']
-        self.mearec_filenames['drifting'] = mearec_filename_drifting
-        self.mearec_filenames['static'] = mearec_filename_static
+        self.mearec_filenames = {}
+        self.keys = ["static", "drifting", "corrected_gt", "corrected_estimated"]
+        self.mearec_filenames["drifting"] = mearec_filename_drifting
+        self.mearec_filenames["static"] = mearec_filename_static
+
         self.temporal_bins = temporal_bins
         self.spatial_bins = spatial_bins
-        self.motion = motion
+        self.gt_motion = gt_motion
+        self.estimated_motion = estimated_motion
         self.do_preprocessing = do_preprocessing
         self.delete_output_folder = delete_output_folder
 
         self._recordings = None
-        _, self.sorting_gt = read_mearec(self.mearec_filenames['static'])
-        
+        _, self.sorting_gt = read_mearec(self.mearec_filenames["static"])
+
         self.correct_motion_kwargs = correct_motion_kwargs.copy()
         self.sparse_kwargs = sparse_kwargs.copy()
+        self.waveforms_kwargs = waveforms_kwargs.copy()
         self.comparisons = {}
         self.accuracies = {}
 
-        self._kwargs.update(dict(
+        self._kwargs.update(
+            dict(
                 correct_motion_kwargs=self.correct_motion_kwargs,
                 sorter_cases=self.sorter_cases,
                 do_preprocessing=do_preprocessing,
                 delete_output_folder=delete_output_folder,
+                waveforms_kwargs=waveforms_kwargs,
                 sparse_kwargs=sparse_kwargs,
-                
             )
         )
 
     @property
     def recordings(self):
         if self._recordings is None:
             self._recordings = {}
-            for key in ('drifting', 'static',):
-                rec, _  = read_mearec(self.mearec_filenames[key])
+
+            for key in (
+                "drifting",
+                "static",
+            ):
+                rec, _ = read_mearec(self.mearec_filenames[key])
+                self._recordings["raw_" + key] = rec
+
                 if self.do_preprocessing:
-                    rec = bandpass_filter(rec)
-                    rec = common_reference(rec)
-                    # rec = zscore(rec)
+                    # this processing chain is the same as the kilosort2.5
+                    # this is important if we want to skip the kilosort preprocessing
+                    #   * all computation are done in float32
+                    #   * 150um is more or less 30 channels for the whittening
+                    #   * the lastet gain step is super important it is what KS2.5 is doing because the whiten traces
+                    #     have magnitude around 1 so a factor (200) is needed to go back to int16
+                    rec = common_reference(rec, dtype="float32")
+                    rec = highpass_filter(rec, freq_min=150.0)
+                    rec = whiten(rec, mode="local", radius_um=150.0, num_chunks_per_segment=40, chunk_size=32000)
+                    rec = scale(rec, gain=200, dtype="int16")
                 self._recordings[key] = rec
 
-            rec = self._recordings['drifting']
-            self._recordings['corrected'] = CorrectMotionRecording(rec, self.motion, 
-                        self.temporal_bins, self.spatial_bins, **self.correct_motion_kwargs)
+            rec = self._recordings["drifting"]
+            self._recordings["corrected_gt"] = InterpolateMotionRecording(
+                rec, self.gt_motion, self.temporal_bins, self.spatial_bins, **self.correct_motion_kwargs
+            )
+
+            self._recordings["corrected_estimated"] = InterpolateMotionRecording(
+                rec, self.estimated_motion, self.temporal_bins, self.spatial_bins, **self.correct_motion_kwargs
+            )
 
         return self._recordings
 
     def run(self):
         self.extract_waveforms()
         self.save_to_folder()
         self.run_sorters()
         self.save_to_folder()
 
-
     def extract_waveforms(self):
-
         # the sparsity is estimated on the static recording and propagated to all of then
         if self.parent_benchmark is None:
-            sparsity = precompute_sparsity(self.recordings['static'], self.sorting_gt,
-                                       ms_before=2., ms_after=3., num_spikes_for_sparsity=200., unit_batch_size=10000,
-                                       **self.sparse_kwargs, **self.job_kwargs)
+            wf_kwargs = self.waveforms_kwargs.copy()
+            wf_kwargs.pop("max_spikes_per_unit", None)
+            sparsity = precompute_sparsity(
+                self.recordings["static"],
+                self.sorting_gt,
+                num_spikes_for_sparsity=200.0,
+                unit_batch_size=10000,
+                **wf_kwargs,
+                **self.sparse_kwargs,
+                **self.job_kwargs,
+            )
         else:
-            sparsity = self.waveforms['static'].sparsity
+            sparsity = self.waveforms["static"].sparsity
 
         for key in self.keys:
             if self.parent_benchmark is not None and key in self._waveform_names_from_parent:
                 continue
-            
+
             waveforms_folder = self.folder / "waveforms" / key
-            we = WaveformExtractor.create(self.recordings[key], self.sorting_gt, waveforms_folder, mode='folder',
-                                          sparsity=sparsity, remove_if_exists=True)
-            we.set_params(ms_before=2., ms_after=3., max_spikes_per_unit=500., return_scaled=True)
+            we = WaveformExtractor.create(
+                self.recordings[key],
+                self.sorting_gt,
+                waveforms_folder,
+                mode="folder",
+                sparsity=sparsity,
+                remove_if_exists=True,
+            )
+            we.set_params(**self.waveforms_kwargs, return_scaled=True)
             we.run_extract_waveforms(seed=22051977, **self.job_kwargs)
             self.waveforms[key] = we
 
-
     def run_sorters(self):
         for case in self.sorter_cases:
-            label = case['label']
-            print('run sorter', label)
-            sorter_name = case['sorter_name']
-            sorter_params = case['sorter_params']
-            recording = self.recordings[case['recording']]
-            output_folder = self.folder / f'tmp_sortings_{label}'
-            sorting = run_sorter(sorter_name, recording, output_folder, **sorter_params, delete_output_folder=self.delete_output_folder)
+            label = case["label"]
+            print("run sorter", label)
+            sorter_name = case["sorter_name"]
+            sorter_params = case["sorter_params"]
+            recording = self.recordings[case["recording"]]
+            output_folder = self.folder / f"tmp_sortings_{label}"
+            sorting = run_sorter(
+                sorter_name, recording, output_folder, **sorter_params, delete_output_folder=self.delete_output_folder
+            )
             self.sortings[label] = sorting
 
-
     def compute_distances_to_static(self, force=False):
-        if hasattr(self, 'distances') and not force:
+        if hasattr(self, "distances") and not force:
             return self.distances
 
         self.distances = {}
 
-        n = len(self.waveforms['static'].unit_ids)
+        n = len(self.waveforms["static"].unit_ids)
+
+        sparsity = self.waveforms["static"].sparsity
 
-        sparsity = self.waveforms['static'].sparsity
+        ref_templates = self.waveforms["static"].get_all_templates()
 
-        ref_templates = self.waveforms['static'].get_all_templates()
-        
-        # for key in ['drifting', 'corrected']:
         for key in self.keys:
+            if self.parent_benchmark is not None and key in ("drifting", "static"):
+                continue
+
+            print(key)
             dist = self.distances[key] = {
-                                        'norm_static' : np.zeros(n),
-                                        'template_euclidean' : np.zeros(n),
-                                        'template_cosine' : np.zeros(n),
-                                        'wf_euclidean_mean' : np.zeros(n),
-                                        'wf_euclidean_std' : np.zeros(n),
-                                        'wf_cosine_mean' : np.zeros(n),
-                                        'wf_cosine_std' : np.zeros(n),
-                                        }
+                "std": np.zeros(n),
+                "norm_std": np.zeros(n),
+                "template_norm_distance": np.zeros(n),
+                "template_cosine": np.zeros(n),
+            }
+
             templates = self.waveforms[key].get_all_templates()
+
+            extremum_channel = get_template_extremum_channel(self.waveforms["static"], outputs="index")
+
             for unit_ind, unit_id in enumerate(self.waveforms[key].sorting.unit_ids):
                 mask = sparsity.mask[unit_ind, :]
-                ref_template = ref_templates[unit_ind][:, mask].reshape(1, -1)
-                template = templates[unit_ind][:, mask].reshape(1, -1)
+                ref_template = ref_templates[unit_ind][:, mask]
+                template = templates[unit_ind][:, mask]
+
+                max_chan = extremum_channel[unit_id]
+                max_chan
+
+                max_chan_sparse = list(np.nonzero(mask)[0]).index(max_chan)
 
                 # this is already sparse
-                # ref_wfs = self.waveforms['static'].get_waveforms(unit_id)
-                # ref_wfs = ref_wfs.reshape(ref_wfs.shape[0], -1)
                 wfs = self.waveforms[key].get_waveforms(unit_id)
-                wfs = wfs.reshape(wfs.shape[0], -1)
-
-                dist['norm_static'][unit_ind] = np.linalg.norm(ref_template)
-                dist['template_euclidean'][unit_ind] = sklearn.metrics.pairwise_distances(ref_template, template)[0]
-                dist['template_cosine'][unit_ind] = sklearn.metrics.pairwise.cosine_similarity(ref_template, template)[0]
-
-                d = sklearn.metrics.pairwise_distances(ref_template, wfs)[0]
-                dist['wf_euclidean_mean'][unit_ind] = d.mean()
-                dist['wf_euclidean_std'][unit_ind] = d.std()
-
-                d = sklearn.metrics.pairwise.cosine_similarity(ref_template, wfs)[0]
-                dist['wf_cosine_mean'][unit_ind] = d.mean()
-                dist['wf_cosine_std'][unit_ind] = d.std()
+                ref_wfs = self.waveforms["static"].get_waveforms(unit_id)
 
+                rms = np.sqrt(np.mean(template**2))
+                ref_rms = np.sqrt(np.mean(ref_template**2))
+                if rms == 0:
+                    print(key, unit_id, unit_ind, rms, ref_rms)
+
+                dist["std"][unit_ind] = np.mean(np.std(wfs, axis=0), axis=(0, 1))
+                dist["norm_std"][unit_ind] = np.mean(np.std(wfs, axis=0), axis=(0, 1)) / rms
+                dist["template_norm_distance"][unit_ind] = np.sum((ref_template - template) ** 2) / ref_rms
+                dist["template_cosine"][unit_ind] = sklearn.metrics.pairwise.cosine_similarity(
+                    ref_template.reshape(1, -1), template.reshape(1, -1)
+                )[0]
 
         return self.distances
 
     def compute_residuals(self, force=True):
-
-        fr = int(self.recordings['static'].get_sampling_frequency())
-        duration = int(self.recordings['static'].get_total_duration())
+        fr = int(self.recordings["static"].get_sampling_frequency())
+        duration = int(self.recordings["static"].get_total_duration())
 
         t_start = 0
         t_stop = duration
 
-        if hasattr(self, 'residuals') and not force:
+        if hasattr(self, "residuals") and not force:
             return self.residuals, (t_start, t_stop)
-        
+
         self.residuals = {}
-        
-        
 
-        for key in ['corrected']:
-            difference = ResidualRecording(self.recordings['static'], self.recordings[key])
-            self.residuals[key] = np.zeros((self.recordings['static'].get_num_channels(), 0))
-            
-            for i in np.arange(t_start*fr, t_stop*fr, fr):
-                data = np.linalg.norm(difference.get_traces(start_frame=i, end_frame=i+fr), axis=0)/np.sqrt(fr)
-                self.residuals[key] = np.hstack((self.residuals[key], data[:,np.newaxis]))
-        
+        for key in ["corrected"]:
+            difference = ResidualRecording(self.recordings["static"], self.recordings[key])
+            self.residuals[key] = np.zeros((self.recordings["static"].get_num_channels(), 0))
+
+            for i in np.arange(t_start * fr, t_stop * fr, fr):
+                data = np.linalg.norm(difference.get_traces(start_frame=i, end_frame=i + fr), axis=0) / np.sqrt(fr)
+                self.residuals[key] = np.hstack((self.residuals[key], data[:, np.newaxis]))
+
         return self.residuals, (t_start, t_stop)
-    
+
     def compute_accuracies(self):
         for case in self.sorter_cases:
-            label = case['label']
+            label = case["label"]
             sorting = self.sortings[label]
             if label not in self.comparisons:
                 comp = GroundTruthComparison(self.sorting_gt, sorting, exhaustive_gt=True)
                 self.comparisons[label] = comp
-                self.accuracies[label] = comp.get_performance()['accuracy'].values
-
-    def _plot_accuracy(self, accuracies, mode='ordered_accuracy', figsize=(15, 5), ls='-'):
+                self.accuracies[label] = comp.get_performance()["accuracy"].values
 
+    def _plot_accuracy(
+        self, accuracies, mode="ordered_accuracy", figsize=(15, 5), axes=None, ax=None, ls="-", legend=True, colors=None
+    ):
         if len(self.accuracies) != len(self.sorter_cases):
             self.compute_accuracies()
 
         n = len(self.sorter_cases)
 
-        if mode == 'ordered_accuracy':
-            fig, ax = plt.subplots(figsize=figsize)
+        if "depth" in mode:
+            # gt_unit_positions, _ = mr.extract_units_drift_vector(self.mearec_filenames['drifting'], time_vector=np.array([0., 1.]))
+            # unit_depth = gt_unit_positions[0, :]
+
+            template_locations = np.array(mr.load_recordings(self.mearec_filenames["drifting"]).template_locations)
+            assert len(template_locations.shape) == 3
+            mid = template_locations.shape[1] // 2
+            unit_depth = template_locations[:, mid, 2]
+
+            chan_locations = self.recordings["drifting"].get_channel_locations()
+
+        if mode == "ordered_accuracy":
+            if ax is None:
+                fig, ax = plt.subplots(figsize=figsize)
+            else:
+                fig = ax.figure
 
             order = None
-            for case in self.sorter_cases:
-                label = case['label']                
-                # comp = self.comparisons[label]
+            for i, case in enumerate(self.sorter_cases):
+                color = colors[i] if colors is not None else None
+                label = case["label"]
+                # comp = self.comparisons[label]
                 acc = accuracies[label]
                 order = np.argsort(acc)[::-1]
                 acc = acc[order]
-                ax.plot(acc, label=label, ls=ls)
-            ax.legend()
-            ax.set_ylabel('accuracy')
-            ax.set_xlabel('units ordered by accuracy')
-        
-        elif mode == 'depth_snr':
-            fig, axs = plt.subplots(nrows=n, figsize=figsize, sharey=True, sharex=True)
-
-            gt_unit_positions, _ = mr.extract_units_drift_vector(self.mearec_filenames['drifting'], time_vector=np.array([0., 1.]))
-            depth = gt_unit_positions[0, :]
+                ax.plot(acc, label=label, ls=ls, color=color)
+            if legend:
+                ax.legend()
+            ax.set_ylabel("accuracy")
+            ax.set_xlabel("units ordered by accuracy")
+
+        elif mode == "depth_snr":
+            if axes is None:
+                fig, axs = plt.subplots(nrows=n, figsize=figsize, sharey=True, sharex=True)
+            else:
+                fig = axes[0].figure
+                axs = axes
 
-            chan_locations = self.recordings['drifting'].get_channel_locations()
-
-            metrics = compute_quality_metrics(self.waveforms['static'], metric_names=['snr'], load_if_exists=True)
-            snr = metrics['snr'].values
+            metrics = compute_quality_metrics(self.waveforms["static"], metric_names=["snr"], load_if_exists=True)
+            snr = metrics["snr"].values
 
             for i, case in enumerate(self.sorter_cases):
                 ax = axs[i]
-                label = case['label']
+                label = case["label"]
                 acc = accuracies[label]
-                s = ax.scatter(depth, snr, c=acc)
-                s.set_clim(0., 1.)
+
+                points = ax.scatter(unit_depth, snr, c=acc)
+                points.set_clim(0.0, 1.0)
                 ax.set_title(label)
-                ax.axvline(np.min(chan_locations[:, 1]), ls='--', color='k')
-                ax.axvline(np.max(chan_locations[:, 1]), ls='--', color='k')
-                ax.set_ylabel('snr')
-            ax.set_xlabel('depth')
+                ax.axvline(np.min(chan_locations[:, 1]), ls="--", color="k")
+                ax.axvline(np.max(chan_locations[:, 1]), ls="--", color="k")
+                ax.set_ylabel("snr")
+            ax.set_xlabel("depth")
 
+            cbar = fig.colorbar(points, ax=axs[:], location="right", shrink=0.6)
+            cbar.ax.set_ylabel("accuracy")
 
-        elif mode == 'snr':
+        elif mode == "snr":
             fig, ax = plt.subplots(figsize=figsize)
 
-            metrics = compute_quality_metrics(self.waveforms['static'], metric_names=['snr'], load_if_exists=True)
-            snr = metrics['snr'].values
+            metrics = compute_quality_metrics(self.waveforms["static"], metric_names=["snr"], load_if_exists=True)
+            snr = metrics["snr"].values
 
             for i, case in enumerate(self.sorter_cases):
-                label = case['label']
+                label = case["label"]
                 acc = self.accuracies[label]
                 ax.scatter(snr, acc, label=label)
-            ax.set_xlabel('snr')
-            ax.set_ylabel('accuracy')
+            ax.set_xlabel("snr")
+            ax.set_ylabel("accuracy")
 
             ax.legend()
 
-
-        elif mode == 'depth':
+        elif mode == "depth":
             fig, ax = plt.subplots(figsize=figsize)
 
-            gt_unit_positions, _ = mr.extract_units_drift_vector(self.mearec_filenames['drifting'], time_vector=np.array([0., 1.]))
-            depth = gt_unit_positions[0, :]
-
-            chan_locations = self.recordings['drifting'].get_channel_locations()
-
             for i, case in enumerate(self.sorter_cases):
-                label = case['label']
+                label = case["label"]
                 acc = accuracies[label]
-                ax.scatter(depth, acc, label=label)
-            ax.axvline(np.min(chan_locations[:, 1]), ls='--', color='k')
-            ax.axvline(np.max(chan_locations[:, 1]), ls='--', color='k')
+
+                ax.scatter(unit_depth, acc, label=label)
+            ax.axvline(np.min(chan_locations[:, 1]), ls="--", color="k")
+            ax.axvline(np.max(chan_locations[:, 1]), ls="--", color="k")
             ax.legend()
-            ax.set_xlabel('depth')
-            ax.set_ylabel('accuracy')
+            ax.set_xlabel("depth")
+            ax.set_ylabel("accuracy")
 
-    def plot_sortings_accuracy(self, mode='ordered_accuracy', figsize=(15, 5)):
+        return fig
 
+    def plot_sortings_accuracy(self, **kwargs):
         if len(self.accuracies) != len(self.sorter_cases):
             self.compute_accuracies()
-        
-        self._plot_accuracy(self.accuracies, mode=mode, figsize=figsize, ls='-')
-
-    def plot_best_merges_accuracy(self, mode='ordered_accuracy', figsize=(15, 5)):
-
-        self._plot_accuracy(self.merged_accuracies, mode=mode, figsize=figsize, ls='--')
 
+        return self._plot_accuracy(self.accuracies, ls="-", **kwargs)
 
+    def plot_best_merges_accuracy(self, **kwargs):
+        return self._plot_accuracy(self.merged_accuracies, **kwargs, ls="--")
 
     def plot_sorting_units_categories(self):
-        
         if len(self.accuracies) != len(self.sorter_cases):
             self.compute_accuracies()
 
         for i, case in enumerate(self.sorter_cases):
-            label = case['label']
+            label = case["label"]
             comp = self.comparisons[label]
             count = comp.count_units_categories()
             if i == 0:
                 df = pd.DataFrame(columns=count.index)
             df.loc[label, :] = count
         df.plot.bar()
-    
-    def find_best_merges(self, merging_score = 0.2):
+
+    def find_best_merges(self, merging_score=0.2):
         # this find best merges having the ground truth
 
         self.merged_sortings = {}
         self.merged_comparisons = {}
         self.merged_accuracies = {}
         self.units_to_merge = {}
         for i, case in enumerate(self.sorter_cases):
-            label = case['label']
-            print()
-            print(label)
+            label = case["label"]
+            # print()
+            # print(label)
             gt_unit_ids = self.sorting_gt.unit_ids
             sorting = self.sortings[label]
             unit_ids = sorting.unit_ids
-            
+
             comp = self.comparisons[label]
             scores = comp.agreement_scores
-            
-            
+
             to_merge = []
             for gt_unit_id in gt_unit_ids:
-                inds,  = np.nonzero(scores.loc[gt_unit_id, :].values > merging_score)
+                (inds,) = np.nonzero(scores.loc[gt_unit_id, :].values > merging_score)
                 merge_ids = unit_ids[inds]
                 if merge_ids.size > 1:
                     to_merge.append(list(merge_ids))
 
             self.units_to_merge[label] = to_merge
             merged_sporting = MergeUnitsSorting(sorting, to_merge)
-            print(sorting)
-            print(merged_sporting)
             comp_merged = GroundTruthComparison(self.sorting_gt, merged_sporting, exhaustive_gt=True)
-            
+
             self.merged_sortings[label] = merged_sporting
             self.merged_comparisons[label] = comp_merged
-            self.merged_accuracies[label] = comp_merged.get_performance()['accuracy'].values
-
-            
-
-
-
-
-def plot_distances_to_static(benchmarks, metric='cosine', figsize=(15, 10)):
+            self.merged_accuracies[label] = comp_merged.get_performance()["accuracy"].values
 
 
+def plot_distances_to_static(benchmarks, metric="cosine", figsize=(15, 10)):
     fig = plt.figure(figsize=figsize)
     gs = fig.add_gridspec(4, 2)
 
     ax = fig.add_subplot(gs[0:2, 0])
     for count, bench in enumerate(benchmarks):
         distances = bench.compute_distances_to_static(force=False)
         print(distances.keys())
-        ax.scatter(distances['drifting'][f'template_{metric}'], distances['corrected'][f'template_{metric}'], c=f'C{count}', alpha=0.5, label=bench.title)
+        ax.scatter(
+            distances["drifting"][f"template_{metric}"],
+            distances["corrected"][f"template_{metric}"],
+            c=f"C{count}",
+            alpha=0.5,
+            label=bench.title,
+        )
 
     ax.legend()
 
-
     xmin, xmax = ax.get_xlim()
-    ax.plot([xmin, xmax], [xmin, xmax], 'k--')
+    ax.plot([xmin, xmax], [xmin, xmax], "k--")
     _simpleaxis(ax)
-    if metric == 'euclidean':
-        ax.set_xlabel(r'$\|drift - static\|_2$')
-        ax.set_ylabel(r'$\|corrected - static\|_2$')
-    elif metric == 'cosine':
-        ax.set_xlabel(r'$cosine(drift, static)$')
-        ax.set_ylabel(r'$cosine(corrected, static)$')
-
+    if metric == "euclidean":
+        ax.set_xlabel(r"$\|drift - static\|_2$")
+        ax.set_ylabel(r"$\|corrected - static\|_2$")
+    elif metric == "cosine":
+        ax.set_xlabel(r"$cosine(drift, static)$")
+        ax.set_ylabel(r"$cosine(corrected, static)$")
 
-    recgen = mr.load_recordings(benchmarks[0].mearec_filenames['static'])
+    recgen = mr.load_recordings(benchmarks[0].mearec_filenames["static"])
     nb_templates, nb_versions, _ = recgen.template_locations.shape
-    template_positions = recgen.template_locations[:, nb_versions//2, 1:3]
+    template_positions = recgen.template_locations[:, nb_versions // 2, 1:3]
     distances_to_center = template_positions[:, 1]
 
     ax_1 = fig.add_subplot(gs[0, 1])
     ax_2 = fig.add_subplot(gs[1, 1])
     ax_3 = fig.add_subplot(gs[2:, 1])
     ax_4 = fig.add_subplot(gs[2:, 0])
 
     for count, bench in enumerate(benchmarks):
-
         # results = bench._compute_snippets_variability(metric=metric, num_channels=num_channels)
         distances = bench.compute_distances_to_static(force=False)
 
-        m_differences = distances['corrected'][f'wf_{metric}_mean']/distances['static'][f'wf_{metric}_mean']
-        s_differences = distances['corrected'][f'wf_{metric}_std']/distances['static'][f'wf_{metric}_std']
+        m_differences = distances["corrected"][f"wf_{metric}_mean"] / distances["static"][f"wf_{metric}_mean"]
+        s_differences = distances["corrected"][f"wf_{metric}_std"] / distances["static"][f"wf_{metric}_std"]
 
-        ax_3.bar([count], [m_differences.mean()], yerr=[m_differences.std()], color=f'C{count}')
-        ax_4.bar([count], [s_differences.mean()], yerr=[s_differences.std()], color=f'C{count}')
+        ax_3.bar([count], [m_differences.mean()], yerr=[m_differences.std()], color=f"C{count}")
+        ax_4.bar([count], [s_differences.mean()], yerr=[s_differences.std()], color=f"C{count}")
         idx = np.argsort(distances_to_center)
-        ax_1.scatter(distances_to_center[idx], m_differences[idx], color=f'C{count}')
-        ax_2.scatter(distances_to_center[idx], s_differences[idx], color=f'C{count}')
+        ax_1.scatter(distances_to_center[idx], m_differences[idx], color=f"C{count}")
+        ax_2.scatter(distances_to_center[idx], s_differences[idx], color=f"C{count}")
 
     for a in [ax_1, ax_2, ax_3, ax_4]:
         _simpleaxis(a)
-    
-    if metric == 'euclidean':
-        ax_1.set_ylabel(r'$\Delta mean(\|~\|_2)$  (% static)')
-        ax_2.set_ylabel(r'$\Delta std(\|~\|_2)$  (% static)')
-        ax_3.set_ylabel(r'$\Delta mean(\|~\|_2)$  (% static)')
-        ax_4.set_ylabel(r'$\Delta std(\|~\|_2)$  (% static)')
-    elif metric == 'cosine':
-        ax_1.set_ylabel(r'$\Delta mean(cosine)$  (% static)')
-        ax_2.set_ylabel(r'$\Delta std(cosine)$  (% static)')
-        ax_3.set_ylabel(r'$\Delta mean(cosine)$  (% static)')
-        ax_4.set_ylabel(r'$\Delta std(cosine)$  (% static)')
+
+    if metric == "euclidean":
+        ax_1.set_ylabel(r"$\Delta mean(\|~\|_2)$  (% static)")
+        ax_2.set_ylabel(r"$\Delta std(\|~\|_2)$  (% static)")
+        ax_3.set_ylabel(r"$\Delta mean(\|~\|_2)$  (% static)")
+        ax_4.set_ylabel(r"$\Delta std(\|~\|_2)$  (% static)")
+    elif metric == "cosine":
+        ax_1.set_ylabel(r"$\Delta mean(cosine)$  (% static)")
+        ax_2.set_ylabel(r"$\Delta std(cosine)$  (% static)")
+        ax_3.set_ylabel(r"$\Delta mean(cosine)$  (% static)")
+        ax_4.set_ylabel(r"$\Delta std(cosine)$  (% static)")
     ax_3.set_xticks(np.arange(len(benchmarks)), [i.title for i in benchmarks])
     ax_4.set_xticks(np.arange(len(benchmarks)), [i.title for i in benchmarks])
     xmin, xmax = ax_3.get_xlim()
-    ax_3.plot([xmin, xmax], [1, 1], 'k--')
-    ax_4.plot([xmin, xmax], [1, 1], 'k--')
+    ax_3.plot([xmin, xmax], [1, 1], "k--")
+    ax_4.plot([xmin, xmax], [1, 1], "k--")
     ax_1.set_xticks([])
-    ax_2.set_xlabel('depth (um)')
+    ax_2.set_xlabel("depth (um)")
 
     xmin, xmax = ax_1.get_xlim()
-    ax_1.plot([xmin, xmax], [1, 1], 'k--')
-    ax_2.plot([xmin, xmax], [1, 1], 'k--')
+    ax_1.plot([xmin, xmax], [1, 1], "k--")
+    ax_2.plot([xmin, xmax], [1, 1], "k--")
     plt.tight_layout()
 
+
 def plot_snr_decrease(benchmarks, figsize=(15, 10)):
-    
     fig, axes = plt.subplots(2, 2, figsize=figsize, squeeze=False)
 
-    recgen = mr.load_recordings(benchmarks[0].mearec_filenames['static'])
+    recgen = mr.load_recordings(benchmarks[0].mearec_filenames["static"])
     nb_templates, nb_versions, _ = recgen.template_locations.shape
-    template_positions = recgen.template_locations[:, nb_versions//2, 1:3]
+    template_positions = recgen.template_locations[:, nb_versions // 2, 1:3]
     distances_to_center = template_positions[:, 1]
     idx = np.argsort(distances_to_center)
     _simpleaxis(axes[0, 0])
 
-    snr_static = compute_quality_metrics(benchmarks[0].waveforms['static'], metric_names=['snr'], load_if_exists=True)
-    snr_drifting = compute_quality_metrics(benchmarks[0].waveforms['drifting'], metric_names=['snr'], load_if_exists=True)
+    snr_static = compute_quality_metrics(benchmarks[0].waveforms["static"], metric_names=["snr"], load_if_exists=True)
+    snr_drifting = compute_quality_metrics(
+        benchmarks[0].waveforms["drifting"], metric_names=["snr"], load_if_exists=True
+    )
 
     m = np.max(snr_static)
-    axes[0, 0].scatter(snr_static.values, snr_drifting.values, c='0.5')
-    axes[0, 0].plot([0, m], [0, m], color='k')
-    
-    axes[0, 0].set_ylabel('units SNR for drifting')
+    axes[0, 0].scatter(snr_static.values, snr_drifting.values, c="0.5")
+    axes[0, 0].plot([0, m], [0, m], color="k")
+
+    axes[0, 0].set_ylabel("units SNR for drifting")
     _simpleaxis(axes[0, 0])
 
-    axes[0, 1].plot(distances_to_center[idx], (snr_drifting.values/snr_static.values)[idx], c='0.5')
-    axes[0, 1].plot(distances_to_center[idx], np.ones(len(idx)), 'k--')
+    axes[0, 1].plot(distances_to_center[idx], (snr_drifting.values / snr_static.values)[idx], c="0.5")
+    axes[0, 1].plot(distances_to_center[idx], np.ones(len(idx)), "k--")
     _simpleaxis(axes[0, 1])
     axes[0, 1].set_xticks([])
     axes[0, 0].set_xticks([])
 
     for count, bench in enumerate(benchmarks):
-
-        snr_corrected = compute_quality_metrics(bench.waveforms['corrected'], metric_names=['snr'], load_if_exists=True)
+        snr_corrected = compute_quality_metrics(bench.waveforms["corrected"], metric_names=["snr"], load_if_exists=True)
         axes[1, 0].scatter(snr_static.values, snr_corrected.values, label=bench.title)
-        axes[1, 0].plot([0, m], [0, m], color='k')
+        axes[1, 0].plot([0, m], [0, m], color="k")
 
-        axes[1, 1].plot(distances_to_center[idx], (snr_corrected.values/snr_static.values)[idx], c=f'C{count}')
+        axes[1, 1].plot(distances_to_center[idx], (snr_corrected.values / snr_static.values)[idx], c=f"C{count}")
 
-    axes[1, 0].set_xlabel('units SNR for static')
-    axes[1, 0].set_ylabel('units SNR for corrected')
-    axes[1, 1].plot(distances_to_center[idx], np.ones(len(idx)), 'k--')
+    axes[1, 0].set_xlabel("units SNR for static")
+    axes[1, 0].set_ylabel("units SNR for corrected")
+    axes[1, 1].plot(distances_to_center[idx], np.ones(len(idx)), "k--")
     axes[1, 0].legend()
     _simpleaxis(axes[1, 0])
     _simpleaxis(axes[1, 1])
-    axes[1, 1].set_ylabel(r'$\Delta(SNR)$')
-    axes[0, 1].set_ylabel(r'$\Delta(SNR)$')
+    axes[1, 1].set_ylabel(r"$\Delta(SNR)$")
+    axes[0, 1].set_ylabel(r"$\Delta(SNR)$")
 
-    axes[1, 1].set_xlabel('depth (um)')
+    axes[1, 1].set_xlabel("depth (um)")
 
 
 def plot_residuals_comparisons(benchmarks):
-
     fig, axes = plt.subplots(1, 3, figsize=(15, 5))
     for count, bench in enumerate(benchmarks):
         residuals, (t_start, t_stop) = bench.compute_residuals(force=False)
         time_axis = np.arange(t_start, t_stop)
-        axes[0].plot(time_axis, residuals['corrected'].mean(0), label=bench.title)
+        axes[0].plot(time_axis, residuals["corrected"].mean(0), label=bench.title)
     axes[0].legend()
-    axes[0].set_xlabel('time (s)')
-    axes[0].set_ylabel(r'$|S_{corrected} - S_{static}|$')
+    axes[0].set_xlabel("time (s)")
+    axes[0].set_ylabel(r"$|S_{corrected} - S_{static}|$")
     _simpleaxis(axes[0])
 
-    channel_positions = benchmarks[0].recordings['static'].get_channel_locations()
+    channel_positions = benchmarks[0].recordings["static"].get_channel_locations()
     distances_to_center = channel_positions[:, 1]
     idx = np.argsort(distances_to_center)
 
     for count, bench in enumerate(benchmarks):
         residuals, (t_start, t_stop) = bench.compute_residuals(force=False)
         time_axis = np.arange(t_start, t_stop)
-        axes[1].plot(distances_to_center[idx], residuals['corrected'].mean(1)[idx], label=bench.title, lw=2, c=f'C{count}')
-        axes[1].fill_between(distances_to_center[idx], residuals['corrected'].mean(1)[idx]-residuals['corrected'].std(1)[idx], 
-                    residuals['corrected'].mean(1)[idx]+residuals['corrected'].std(1)[idx], color=f'C{count}', alpha=0.25)
-    axes[1].set_xlabel('depth (um)')
+        axes[1].plot(
+            distances_to_center[idx], residuals["corrected"].mean(1)[idx], label=bench.title, lw=2, c=f"C{count}"
+        )
+        axes[1].fill_between(
+            distances_to_center[idx],
+            residuals["corrected"].mean(1)[idx] - residuals["corrected"].std(1)[idx],
+            residuals["corrected"].mean(1)[idx] + residuals["corrected"].std(1)[idx],
+            color=f"C{count}",
+            alpha=0.25,
+        )
+    axes[1].set_xlabel("depth (um)")
     _simpleaxis(axes[1])
 
     for count, bench in enumerate(benchmarks):
         residuals, (t_start, t_stop) = bench.compute_residuals(force=False)
-        axes[2].bar([count], [residuals['corrected'].mean()], yerr=[residuals['corrected'].std()], color=f'C{count}')
+        axes[2].bar([count], [residuals["corrected"].mean()], yerr=[residuals["corrected"].std()], color=f"C{count}")
 
     _simpleaxis(axes[2])
     axes[2].set_xticks(np.arange(len(benchmarks)), [i.title for i in benchmarks])
 
 
 from spikeinterface.preprocessing.basepreprocessor import BasePreprocessor, BasePreprocessorSegment
+
+
 class ResidualRecording(BasePreprocessor):
-    name = 'residual_recording'
+    name = "residual_recording"
+
     def __init__(self, recording_1, recording_2):
         assert recording_1.get_num_segments() == recording_2.get_num_segments()
         BasePreprocessor.__init__(self, recording_1)
 
-        for parent_recording_segment_1, parent_recording_segment_2 in zip(recording_1._recording_segments, recording_2._recording_segments):
+        for parent_recording_segment_1, parent_recording_segment_2 in zip(
+            recording_1._recording_segments, recording_2._recording_segments
+        ):
             rec_segment = DifferenceRecordingSegment(parent_recording_segment_1, parent_recording_segment_2)
             self.add_recording_segment(rec_segment)
 
         self._kwargs = dict(recording_1=recording_1, recording_2=recording_2)
 
 
 class DifferenceRecordingSegment(BasePreprocessorSegment):
     def __init__(self, parent_recording_segment_1, parent_recording_segment_2):
         BasePreprocessorSegment.__init__(self, parent_recording_segment_1)
         self.parent_recording_segment_1 = parent_recording_segment_1
         self.parent_recording_segment_2 = parent_recording_segment_2
 
     def get_traces(self, start_frame, end_frame, channel_indices):
-
         traces_1 = self.parent_recording_segment_1.get_traces(start_frame, end_frame, channel_indices)
         traces_2 = self.parent_recording_segment_2.get_traces(start_frame, end_frame, channel_indices)
 
         return traces_2 - traces_1
 
-colors = {'static' : 'C0', 'drifting' : 'C1', 'corrected' : 'C2'}
 
+colors = {"static": "C0", "drifting": "C1", "corrected": "C2"}
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sortingcomponents/benchmark/benchmark_motion_estimation.py` & `spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/benchmark/benchmark_motion_estimation.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,322 +1,323 @@
-
 import json
 import numpy as np
 import time
 from pathlib import Path
 
 
 from spikeinterface.core import get_noise_levels
 from spikeinterface.extractors import read_mearec
 from spikeinterface.sortingcomponents.peak_detection import detect_peaks
 from spikeinterface.sortingcomponents.peak_selection import select_peaks
 from spikeinterface.sortingcomponents.peak_localization import localize_peaks
 from spikeinterface.sortingcomponents.motion_estimation import estimate_motion
-from spikeinterface.sortingcomponents.motion_correction import correct_motion_on_peaks
+from spikeinterface.sortingcomponents.motion_interpolation import correct_motion_on_peaks
 from spikeinterface.preprocessing import bandpass_filter, zscore, common_reference
 
 from spikeinterface.sortingcomponents.benchmark.benchmark_tools import BenchmarkBase, _simpleaxis
 
 
 from spikeinterface.widgets import plot_probe_map
 
 import scipy.interpolate
 
 import matplotlib.pyplot as plt
 
 import MEArec as mr
 
-class BenchmarkMotionEstimationMearec(BenchmarkBase):
 
-    _array_names = ('noise_levels', 'gt_unit_positions',
-            'peaks', 'selected_peaks', 'motion', 'temporal_bins', 'spatial_bins', 'peak_locations', 'gt_motion')
-    
-    def __init__(self, mearec_filename, 
-                title='',
-                detect_kwargs={},
-                select_kwargs=None,
-                localize_kwargs={},
-                estimate_motion_kwargs={},
-                folder=None,
-                do_preprocessing=True, 
-                job_kwargs={'chunk_duration' : '1s', 'n_jobs' : -1, 'progress_bar':True, 'verbose' :True}, 
-                overwrite=False,
-                parent_benchmark=None,
-                ):
-        
-        BenchmarkBase.__init__(self, folder=folder, title=title, overwrite=overwrite,  job_kwargs=job_kwargs, 
-                               parent_benchmark=None)
+class BenchmarkMotionEstimationMearec(BenchmarkBase):
+    _array_names = (
+        "noise_levels",
+        "gt_unit_positions",
+        "peaks",
+        "selected_peaks",
+        "motion",
+        "temporal_bins",
+        "spatial_bins",
+        "peak_locations",
+        "gt_motion",
+    )
+
+    def __init__(
+        self,
+        mearec_filename,
+        title="",
+        detect_kwargs={},
+        select_kwargs=None,
+        localize_kwargs={},
+        estimate_motion_kwargs={},
+        folder=None,
+        do_preprocessing=True,
+        job_kwargs={"chunk_duration": "1s", "n_jobs": -1, "progress_bar": True, "verbose": True},
+        overwrite=False,
+        parent_benchmark=None,
+    ):
+        BenchmarkBase.__init__(
+            self, folder=folder, title=title, overwrite=overwrite, job_kwargs=job_kwargs, parent_benchmark=None
+        )
 
         self._args.extend([str(mearec_filename)])
 
         self.mearec_filename = mearec_filename
         self.raw_recording, self.gt_sorting = read_mearec(self.mearec_filename)
         self.do_preprocessing = do_preprocessing
-        
+
         self._recording = None
         self.detect_kwargs = detect_kwargs.copy()
         self.select_kwargs = select_kwargs.copy() if select_kwargs is not None else None
         self.localize_kwargs = localize_kwargs.copy()
         self.estimate_motion_kwargs = estimate_motion_kwargs.copy()
 
-        self._kwargs.update(dict(
+        self._kwargs.update(
+            dict(
                 detect_kwargs=self.detect_kwargs,
                 select_kwargs=self.select_kwargs,
                 localize_kwargs=self.localize_kwargs,
                 estimate_motion_kwargs=self.estimate_motion_kwargs,
             )
         )
 
-
     @property
     def recording(self):
         if self._recording is None:
             if self.do_preprocessing:
                 self._recording = bandpass_filter(self.raw_recording)
                 self._recording = common_reference(self._recording)
                 self._recording = zscore(self._recording)
             else:
                 self._recording = self.raw_recording
         return self._recording
 
     def run(self):
-
         if self.folder is not None:
             if self.folder.exists() and not self.overwrite:
                 raise ValueError(f"The folder {self.folder} is not empty")
 
         self.noise_levels = get_noise_levels(self.recording, return_scaled=False)
 
         t0 = time.perf_counter()
-        self.peaks = detect_peaks(self.recording, noise_levels=self.noise_levels, **self.detect_kwargs, **self.job_kwargs)
+        self.peaks = detect_peaks(
+            self.recording, noise_levels=self.noise_levels, **self.detect_kwargs, **self.job_kwargs
+        )
         t1 = time.perf_counter()
         if self.select_kwargs is not None:
             self.selected_peaks = select_peaks(self.peaks, **self.select_kwargs, **self.job_kwargs)
         else:
             self.selected_peaks = self.peaks
         t2 = time.perf_counter()
-        self.peak_locations = localize_peaks(self.recording, self.selected_peaks, **self.localize_kwargs, **self.job_kwargs)
+        self.peak_locations = localize_peaks(
+            self.recording, self.selected_peaks, **self.localize_kwargs, **self.job_kwargs
+        )
         t3 = time.perf_counter()
-        self.motion, self.temporal_bins, self.spatial_bins = estimate_motion(self.recording, self.selected_peaks, self.peak_locations, 
-                                        **self.estimate_motion_kwargs)
+        self.motion, self.temporal_bins, self.spatial_bins = estimate_motion(
+            self.recording, self.selected_peaks, self.peak_locations, **self.estimate_motion_kwargs
+        )
 
         t4 = time.perf_counter()
 
         self.run_times = dict(
-            detect_peaks=t1 -t0,
+            detect_peaks=t1 - t0,
             select_peaks=t2 - t1,
             localize_peaks=t3 - t2,
-            estimate_motion= t4 - t3,
+            estimate_motion=t4 - t3,
         )
 
         self.compute_gt_motion()
 
         # align globally gt_motion and motion to avoid offsets
         self.motion += np.median(self.gt_motion - self.motion)
 
         ## save folder
         if self.folder is not None:
             self.save_to_folder()
 
     def run_estimate_motion(self):
         # usefull to re run only the motion estimate with peak localization
         t3 = time.perf_counter()
-        self.motion, self.temporal_bins, self.spatial_bins = estimate_motion(self.recording, self.selected_peaks, self.peak_locations, 
-                                        **self.estimate_motion_kwargs)
+        self.motion, self.temporal_bins, self.spatial_bins = estimate_motion(
+            self.recording, self.selected_peaks, self.peak_locations, **self.estimate_motion_kwargs
+        )
         t4 = time.perf_counter()
 
         self.compute_gt_motion()
 
         # align globally gt_motion and motion to avoid offsets
         self.motion += np.median(self.gt_motion - self.motion)
-        self.run_times['estimate_motion'] = t4 - t3
+        self.run_times["estimate_motion"] = t4 - t3
 
         ## save folder
         if self.folder is not None:
             self.save_to_folder()
 
-
     def compute_gt_motion(self):
         self.gt_unit_positions, _ = mr.extract_units_drift_vector(self.mearec_filename, time_vector=self.temporal_bins)
 
         template_locations = np.array(mr.load_recordings(self.mearec_filename).template_locations)
         assert len(template_locations.shape) == 3
-        mid = template_locations.shape[1] //2
+        mid = template_locations.shape[1] // 2
         unit_mid_positions = template_locations[:, mid, 2]
-        
+
         unit_motions = self.gt_unit_positions - unit_mid_positions
         # unit_positions = np.mean(self.gt_unit_positions, axis=0)
 
         if self.spatial_bins is None:
             self.gt_motion = np.mean(unit_motions, axis=1)[:, None]
             channel_positions = self.recording.get_channel_locations()
             probe_y_min, probe_y_max = channel_positions[:, 1].min(), channel_positions[:, 1].max()
-            center = (probe_y_min + probe_y_max)//2
+            center = (probe_y_min + probe_y_max) // 2
             self.spatial_bins = np.array([center])
         else:
             # time, units
             self.gt_motion = np.zeros_like(self.motion)
             for t in range(self.gt_unit_positions.shape[0]):
                 f = scipy.interpolate.interp1d(unit_mid_positions, unit_motions[t, :], fill_value="extrapolate")
                 self.gt_motion[t, :] = f(self.spatial_bins)
 
     def plot_true_drift(self, scaling_probe=1.5, figsize=(15, 10), axes=None):
-                
         if axes is None:
             fig = plt.figure(figsize=figsize)
             gs = fig.add_gridspec(1, 8, wspace=0)
-        
+
         if axes is None:
             ax = fig.add_subplot(gs[:2])
         else:
             ax = axes[0]
         plot_probe_map(self.recording, ax=ax)
         _simpleaxis(ax)
 
         mr_recording = mr.load_recordings(self.mearec_filename)
-            
+
         for loc in mr_recording.template_locations[::2]:
             if len(mr_recording.template_locations.shape) == 3:
                 ax.plot([loc[0, 1], loc[-1, 1]], [loc[0, 2], loc[-1, 2]], alpha=0.7, lw=2)
             else:
                 ax.scatter([loc[1]], [loc[2]], alpha=0.7, s=100)
-    
+
         # ymin, ymax = ax.get_ylim()
-        ax.set_ylabel('depth (um)')
+        ax.set_ylabel("depth (um)")
         ax.set_xlabel(None)
-        #ax.set_yticks(np.arange(-600,600,100), np.arange(-600,600,100))
-
-
+        # ax.set_yticks(np.arange(-600,600,100), np.arange(-600,600,100))
 
         # ax.set_ylim(scaling_probe*probe_y_min, scaling_probe*probe_y_max)
         if axes is None:
             ax = fig.add_subplot(gs[2:7])
         else:
             ax = axes[1]
-        
+
         for i in range(self.gt_unit_positions.shape[1]):
-            ax.plot(self.temporal_bins, self.gt_unit_positions[:, i], alpha=0.5, ls='--', c='0.5')
-        
+            ax.plot(self.temporal_bins, self.gt_unit_positions[:, i], alpha=0.5, ls="--", c="0.5")
+
         for i in range(self.gt_motion.shape[1]):
             depth = self.spatial_bins[i]
-            ax.plot(self.temporal_bins, self.gt_motion[:, i] + depth, color='green', lw=4)
+            ax.plot(self.temporal_bins, self.gt_motion[:, i] + depth, color="green", lw=4)
 
         # ax.set_ylim(ymin, ymax)
-        ax.set_xlabel('time (s)')
+        ax.set_xlabel("time (s)")
         _simpleaxis(ax)
         ax.set_yticks([])
-        ax.spines['left'].set_visible(False)
+        ax.spines["left"].set_visible(False)
 
         channel_positions = self.recording.get_channel_locations()
         probe_y_min, probe_y_max = channel_positions[:, 1].min(), channel_positions[:, 1].max()
-        ax.set_ylim(scaling_probe*probe_y_min, scaling_probe*probe_y_max)
+        ax.set_ylim(scaling_probe * probe_y_min, scaling_probe * probe_y_max)
 
-        ax.axhline(probe_y_min, color='k', ls='--', alpha=0.5)
-        ax.axhline(probe_y_max, color='k', ls='--', alpha=0.5)
+        ax.axhline(probe_y_min, color="k", ls="--", alpha=0.5)
+        ax.axhline(probe_y_max, color="k", ls="--", alpha=0.5)
 
         if axes is None:
             ax = fig.add_subplot(gs[7])
         else:
             ax = axes[2]
         # plot_probe_map(self.recording, ax=ax)
         _simpleaxis(ax)
 
-        ax.hist(self.gt_unit_positions[30,:], 50, orientation='horizontal', color='0.5')
+        ax.hist(self.gt_unit_positions[30, :], 50, orientation="horizontal", color="0.5")
         ax.set_yticks([])
-        ax.set_xlabel('# neurons')
-
+        ax.set_xlabel("# neurons")
 
-    def plot_peaks_probe(self, alpha = 0.05, figsize=(15, 10)):
-            
+    def plot_peaks_probe(self, alpha=0.05, figsize=(15, 10)):
         fig, axs = plt.subplots(ncols=2, sharey=True, figsize=figsize)
         ax = axs[0]
         plot_probe_map(self.recording, ax=ax)
-        ax.scatter(self.peak_locations['x'], self.peak_locations['y'], color='k', s=1, alpha=alpha)
-        ax.set_xlabel('x')
-        ax.set_ylabel('y')
-        if 'z' in self.peak_locations.dtype.fields:
+        ax.scatter(self.peak_locations["x"], self.peak_locations["y"], color="k", s=1, alpha=alpha)
+        ax.set_xlabel("x")
+        ax.set_ylabel("y")
+        if "z" in self.peak_locations.dtype.fields:
             ax = axs[1]
-            ax.scatter(self.peak_locations['z'], self.peak_locations['y'], color='k', s=1, alpha=alpha)
-            ax.set_xlabel('z')
+            ax.scatter(self.peak_locations["z"], self.peak_locations["y"], color="k", s=1, alpha=alpha)
+            ax.set_xlabel("z")
             ax.set_xlim(0, 100)
 
     def plot_peaks(self, scaling_probe=1.5, show_drift=True, show_histogram=True, alpha=0.05, figsize=(15, 10)):
-
         fig = plt.figure(figsize=figsize)
         if show_histogram:
             gs = fig.add_gridspec(1, 4)
         else:
             gs = fig.add_gridspec(1, 3)
         # Create the Axes.
 
         ax0 = fig.add_subplot(gs[0])
         plot_probe_map(self.recording, ax=ax0)
         _simpleaxis(ax0)
 
         # ymin, ymax = ax.get_ylim()
-        ax0.set_ylabel('depth (um)')
+        ax0.set_ylabel("depth (um)")
         ax0.set_xlabel(None)
 
-
-
         ax = ax1 = fig.add_subplot(gs[1:3])
-        x = self.selected_peaks['sample_ind']/self.recording.get_sampling_frequency()
-        y = self.peak_locations['y']
-        ax.scatter(x, y, s=1, color='k', alpha=alpha)
-        
+        x = self.selected_peaks["sample_index"] / self.recording.get_sampling_frequency()
+        y = self.peak_locations["y"]
+        ax.scatter(x, y, s=1, color="k", alpha=alpha)
+
         ax.set_title(self.title)
         # xmin, xmax = ax.get_xlim()
         # ax.plot([xmin, xmax], [probe_y_min, probe_y_min], 'k--', alpha=0.5)
         # ax.plot([xmin, xmax], [probe_y_max, probe_y_max], 'k--', alpha=0.5)
 
         _simpleaxis(ax)
         # ax.set_yticks([])
         # ax.set_ylim(scaling_probe*probe_y_min, scaling_probe*probe_y_max)
-        ax.spines['left'].set_visible(False)
-        ax.set_xlabel('time (s)')
-
+        ax.spines["left"].set_visible(False)
+        ax.set_xlabel("time (s)")
 
         channel_positions = self.recording.get_channel_locations()
         probe_y_min, probe_y_max = channel_positions[:, 1].min(), channel_positions[:, 1].max()
-        ax.set_ylim(scaling_probe*probe_y_min, scaling_probe*probe_y_max)
-
-        ax.axhline(probe_y_min, color='k', ls='--', alpha=0.5)
-        ax.axhline(probe_y_max, color='k', ls='--', alpha=0.5)
+        ax.set_ylim(scaling_probe * probe_y_min, scaling_probe * probe_y_max)
 
+        ax.axhline(probe_y_min, color="k", ls="--", alpha=0.5)
+        ax.axhline(probe_y_max, color="k", ls="--", alpha=0.5)
 
         if show_drift:
             if self.spatial_bins is None:
-                center = (probe_y_min + probe_y_max)//2
-                ax.plot(self.temporal_bins, self.gt_motion[:, 0] + center, color='green', lw=1.5)
-                ax.plot(self.temporal_bins, self.motion[:, 0] + center, color='orange', lw=1.5)
+                center = (probe_y_min + probe_y_max) // 2
+                ax.plot(self.temporal_bins, self.gt_motion[:, 0] + center, color="green", lw=1.5)
+                ax.plot(self.temporal_bins, self.motion[:, 0] + center, color="orange", lw=1.5)
             else:
                 for i in range(self.gt_motion.shape[1]):
                     depth = self.spatial_bins[i]
-                    ax.plot(self.temporal_bins, self.gt_motion[:, i] + depth, color='green', lw=1.5)
-                    ax.plot(self.temporal_bins, self.motion[:, i] + depth, color='orange', lw=1.5)
+                    ax.plot(self.temporal_bins, self.gt_motion[:, i] + depth, color="green", lw=1.5)
+                    ax.plot(self.temporal_bins, self.motion[:, i] + depth, color="orange", lw=1.5)
 
         if show_histogram:
             ax2 = fig.add_subplot(gs[3])
-            ax2.hist(self.peak_locations['y'], bins=1000, orientation="horizontal")
+            ax2.hist(self.peak_locations["y"], bins=1000, orientation="horizontal")
 
-            ax2.axhline(probe_y_min, color='k', ls='--', alpha=0.5)
-            ax2.axhline(probe_y_max, color='k', ls='--', alpha=0.5)
+            ax2.axhline(probe_y_min, color="k", ls="--", alpha=0.5)
+            ax2.axhline(probe_y_max, color="k", ls="--", alpha=0.5)
 
-
-
-            ax2.set_xlabel('density')
+            ax2.set_xlabel("density")
             _simpleaxis(ax2)
             # ax.set_ylabel('')
             ax.set_yticks([])
             ax2.sharey(ax0)
 
         ax1.sharey(ax0)
 
     def plot_motion_corrected_peaks(self, scaling_probe=1.5, alpha=0.05, figsize=(15, 10), show_probe=True, axes=None):
-
         if axes is None:
             fig = plt.figure(figsize=figsize)
             if show_probe:
                 gs = fig.add_gridspec(1, 5)
             else:
                 gs = fig.add_gridspec(1, 4)
         # Create the Axes.
@@ -324,225 +325,287 @@
         if show_probe:
             if axes is None:
                 ax0 = ax = fig.add_subplot(gs[0])
             else:
                 ax0 = ax = axes[0]
             plot_probe_map(self.recording, ax=ax)
             _simpleaxis(ax)
-        
+
             ymin, ymax = ax.get_ylim()
-            ax.set_ylabel('depth (um)')
+            ax.set_ylabel("depth (um)")
             ax.set_xlabel(None)
 
         channel_positions = self.recording.get_channel_locations()
         probe_y_min, probe_y_max = channel_positions[:, 1].min(), channel_positions[:, 1].max()
 
         times = self.recording.get_times()
 
-        peak_locations_corrected = correct_motion_on_peaks(self.selected_peaks, self.peak_locations, times,
-                                    self.motion, self.temporal_bins, self.spatial_bins, direction='y')
+        peak_locations_corrected = correct_motion_on_peaks(
+            self.selected_peaks,
+            self.peak_locations,
+            times,
+            self.motion,
+            self.temporal_bins,
+            self.spatial_bins,
+            direction="y",
+        )
         if axes is None:
             if show_probe:
                 ax1 = ax = fig.add_subplot(gs[1:3])
             else:
                 ax1 = ax = fig.add_subplot(gs[0:2])
         else:
             if show_probe:
                 ax1 = ax = axes[1]
             else:
                 ax1 = ax = axes[0]
 
         _simpleaxis(ax)
 
-        x = self.selected_peaks['sample_ind']/self.recording.get_sampling_frequency()
-        y = self.peak_locations['y']
-        ax.scatter(x, y, s=1, color='k', alpha=alpha)
+        x = self.selected_peaks["sample_index"] / self.recording.get_sampling_frequency()
+        y = self.peak_locations["y"]
+        ax.scatter(x, y, s=1, color="k", alpha=alpha)
         ax.set_title(self.title)
 
+        ax.axhline(probe_y_min, color="k", ls="--", alpha=0.5)
+        ax.axhline(probe_y_max, color="k", ls="--", alpha=0.5)
 
-        ax.axhline(probe_y_min, color='k', ls='--', alpha=0.5)
-        ax.axhline(probe_y_max, color='k', ls='--', alpha=0.5)
-
-        ax.set_xlabel('time (s)')
+        ax.set_xlabel("time (s)")
 
         if axes is None:
             if show_probe:
                 ax2 = ax = fig.add_subplot(gs[3:5])
             else:
                 ax2 = ax = fig.add_subplot(gs[2:4])
         else:
             if show_probe:
                 ax2 = ax = axes[2]
             else:
                 ax2 = ax = axes[1]
-        
-        _simpleaxis(ax)
-        y = peak_locations_corrected['y']
-        ax.scatter(x, y, s=1, color='k', alpha=alpha)
 
-        ax.axhline(probe_y_min, color='k', ls='--', alpha=0.5)
-        ax.axhline(probe_y_max, color='k', ls='--', alpha=0.5)
+        _simpleaxis(ax)
+        y = peak_locations_corrected["y"]
+        ax.scatter(x, y, s=1, color="k", alpha=alpha)
 
+        ax.axhline(probe_y_min, color="k", ls="--", alpha=0.5)
+        ax.axhline(probe_y_max, color="k", ls="--", alpha=0.5)
 
-        ax.set_xlabel('time (s)')
+        ax.set_xlabel("time (s)")
 
         if show_probe:
-            ax0.set_ylim(scaling_probe*probe_y_min, scaling_probe*probe_y_max)
+            ax0.set_ylim(scaling_probe * probe_y_min, scaling_probe * probe_y_max)
             ax1.sharey(ax0)
             ax2.sharey(ax0)
         else:
-            ax1.set_ylim(scaling_probe*probe_y_min, scaling_probe*probe_y_max)
+            ax1.set_ylim(scaling_probe * probe_y_min, scaling_probe * probe_y_max)
             ax2.sharey(ax1)
 
-    def estimation_vs_depth(self, show_only=8, figsize=(15,10)):
+    def estimation_vs_depth(self, show_only=8, figsize=(15, 10)):
         fig, axs = plt.subplots(ncols=2, figsize=figsize, sharey=True)
 
         n = self.motion.shape[1]
         step = int(np.ceil(max(1, n / show_only)))
-        colors = plt.cm.get_cmap('jet', n)
+        colors = plt.cm.get_cmap("jet", n)
         for i in range(0, n, step):
             ax = axs[0]
-            ax.plot(self.temporal_bins, self.gt_motion[:, i], lw=1.5, ls='--', color=colors(i))
-            ax.plot(self.temporal_bins, self.motion[:, i], lw=1.5, ls='-', color=colors(i),
-                    label=f'{self.spatial_bins[i]:0.1f}')
+            ax.plot(self.temporal_bins, self.gt_motion[:, i], lw=1.5, ls="--", color=colors(i))
+            ax.plot(
+                self.temporal_bins,
+                self.motion[:, i],
+                lw=1.5,
+                ls="-",
+                color=colors(i),
+                label=f"{self.spatial_bins[i]:0.1f}",
+            )
 
             ax = axs[1]
-            ax.plot(self.temporal_bins, self.motion[:, i] - self.gt_motion[:, i], lw=1.5, ls='-', color=colors(i))
-        
+            ax.plot(self.temporal_bins, self.motion[:, i] - self.gt_motion[:, i], lw=1.5, ls="-", color=colors(i))
+
         ax = axs[0]
         ax.set_title(self.title)
         ax.legend()
-        ax.set_ylabel('drift estimated and GT(um)')
-        ax.set_xlabel('time (s)')
+        ax.set_ylabel("drift estimated and GT(um)")
+        ax.set_xlabel("time (s)")
         _simpleaxis(ax)
 
         ax = axs[1]
-        ax.set_ylabel('error (um)')
-        ax.set_xlabel('time (s)')
+        ax.set_ylabel("error (um)")
+        ax.set_xlabel("time (s)")
         _simpleaxis(ax)
 
     def view_errors(self, figsize=(15, 10), lim=None):
         fig = plt.figure(figsize=figsize)
         gs = fig.add_gridspec(2, 2)
 
         errors = self.gt_motion - self.motion
 
         channel_positions = self.recording.get_channel_locations()
         probe_y_min, probe_y_max = channel_positions[:, 1].min(), channel_positions[:, 1].max()
 
         ax = fig.add_subplot(gs[0, :])
-        im = ax.imshow(np.abs(errors).T, aspect='auto', interpolation='nearest', origin='lower', 
-        extent=(self.temporal_bins[0], self.temporal_bins[-1], self.spatial_bins[0], self.spatial_bins[-1]))
-        plt.colorbar(im, ax=ax, label='error')
-        ax.set_ylabel('depth (um)')
-        ax.set_xlabel('time (s)')
+        im = ax.imshow(
+            np.abs(errors).T,
+            aspect="auto",
+            interpolation="nearest",
+            origin="lower",
+            extent=(self.temporal_bins[0], self.temporal_bins[-1], self.spatial_bins[0], self.spatial_bins[-1]),
+        )
+        plt.colorbar(im, ax=ax, label="error")
+        ax.set_ylabel("depth (um)")
+        ax.set_xlabel("time (s)")
         ax.set_title(self.title)
         if lim is not None:
             im.set_clim(0, lim)
 
         ax = fig.add_subplot(gs[1, 0])
         mean_error = np.sqrt(np.mean((errors) ** 2, axis=1))
         ax.plot(self.temporal_bins, mean_error)
-        ax.set_xlabel('time (s)')
-        ax.set_ylabel('error')
+        ax.set_xlabel("time (s)")
+        ax.set_ylabel("error")
         _simpleaxis(ax)
         if lim is not None:
             ax.set_ylim(0, lim)
 
         ax = fig.add_subplot(gs[1, 1])
         depth_error = np.sqrt(np.mean((errors) ** 2, axis=0))
         ax.plot(self.spatial_bins, depth_error)
-        ax.axvline(probe_y_min, color='k', ls='--', alpha=0.5)
-        ax.axvline(probe_y_max, color='k', ls='--', alpha=0.5)
-        ax.set_xlabel('depth (um)')
-        ax.set_ylabel('error')
+        ax.axvline(probe_y_min, color="k", ls="--", alpha=0.5)
+        ax.axvline(probe_y_max, color="k", ls="--", alpha=0.5)
+        ax.set_xlabel("depth (um)")
+        ax.set_ylabel("error")
         _simpleaxis(ax)
         if lim is not None:
             ax.set_ylim(0, lim)
 
+        return fig
+
 
 def plot_errors_several_benchmarks(benchmarks, axes=None, show_legend=True, colors=None):
     if axes is None:
         fig, axes = plt.subplots(1, 3, figsize=(15, 5))
 
     for count, benchmark in enumerate(benchmarks):
         c = colors[count] if colors is not None else None
         errors = benchmark.gt_motion - benchmark.motion
         mean_error = np.sqrt(np.mean((errors) ** 2, axis=1))
         depth_error = np.sqrt(np.mean((errors) ** 2, axis=0))
 
         axes[0].plot(benchmark.temporal_bins, mean_error, label=benchmark.title, color=c)
         parts = axes[1].violinplot(mean_error, [count], showmeans=True)
         if c is not None:
-            for pc in parts['bodies']:
+            for pc in parts["bodies"]:
                 pc.set_facecolor(c)
                 pc.set_edgecolor(c)
             for k in parts:
                 if k != "bodies":
                     # for line in parts[k]:
                     parts[k].set_color(c)
         axes[2].plot(benchmark.spatial_bins, depth_error, label=benchmark.title, color=c)
 
     ax0 = ax = axes[0]
-    ax.set_xlabel('time (s)')
-    ax.set_ylabel('error')
+    ax.set_xlabel("time [s]")
+    ax.set_ylabel("error [um]")
     if show_legend:
         ax.legend()
     _simpleaxis(ax)
 
     ax1 = axes[1]
-    #ax.set_ylabel('error')
+    # ax.set_ylabel('error')
     ax1.set_yticks([])
     ax1.set_xticks([])
     _simpleaxis(ax1)
 
-    ax2 =  axes[2]
+    ax2 = axes[2]
     ax2.set_yticks([])
-    ax2.set_xlabel('depth (um)')
-    #ax.set_ylabel('error')
+    ax2.set_xlabel("depth [um]")
+    # ax.set_ylabel('error')
     channel_positions = benchmark.recording.get_channel_locations()
     probe_y_min, probe_y_max = channel_positions[:, 1].min(), channel_positions[:, 1].max()
-    ax2.axvline(probe_y_min, color='k', ls='--', alpha=0.5)
-    ax2.axvline(probe_y_max, color='k', ls='--', alpha=0.5)
+    ax2.axvline(probe_y_min, color="k", ls="--", alpha=0.5)
+    ax2.axvline(probe_y_max, color="k", ls="--", alpha=0.5)
 
     _simpleaxis(ax2)
 
-    ax1.sharey(ax0)
-    ax2.sharey(ax0)
+    # ax1.sharey(ax0)
+    # ax2.sharey(ax0)
+
+
+def plot_error_map_several_benchmarks(benchmarks, axes=None, lim=15, figsize=(10, 10)):
+    if axes is None:
+        fig, axes = plt.subplots(nrows=len(benchmarks), sharex=True, sharey=True, figsize=figsize)
+    else:
+        fig = axes[0].figure
+
+    for count, benchmark in enumerate(benchmarks):
+        errors = benchmark.gt_motion - benchmark.motion
+
+        channel_positions = benchmark.recording.get_channel_locations()
+        probe_y_min, probe_y_max = channel_positions[:, 1].min(), channel_positions[:, 1].max()
+
+        ax = axes[count]
+        im = ax.imshow(
+            np.abs(errors).T,
+            aspect="auto",
+            interpolation="nearest",
+            origin="lower",
+            extent=(
+                benchmark.temporal_bins[0],
+                benchmark.temporal_bins[-1],
+                benchmark.spatial_bins[0],
+                benchmark.spatial_bins[-1],
+            ),
+        )
+        fig.colorbar(im, ax=ax, label="error")
+        ax.set_ylabel("depth (um)")
+
+        ax.set_title(benchmark.title)
+        if lim is not None:
+            im.set_clim(0, lim)
+
+    axes[-1].set_xlabel("time (s)")
+
+    return fig
+
 
 def plot_motions_several_benchmarks(benchmarks):
     fig, ax = plt.subplots(figsize=(15, 5))
 
-    ax.plot(list(benchmarks)[0].temporal_bins, list(benchmarks)[0].gt_motion[:, 0], lw=2, c='k', label='real motion')
+    ax.plot(list(benchmarks)[0].temporal_bins, list(benchmarks)[0].gt_motion[:, 0], lw=2, c="k", label="real motion")
     for count, benchmark in enumerate(benchmarks):
-        ax.plot(benchmark.temporal_bins, benchmark.motion.mean(1), lw=1, c=f'C{count}', label=benchmark.title)
-        ax.fill_between(benchmark.temporal_bins, benchmark.motion.mean(1)-benchmark.motion.std(1), 
-                                benchmark.motion.mean(1) + benchmark.motion.std(1), color=f'C{count}', alpha=0.25)
-
-    #ax.legend()
-    ax.set_ylabel('depth (um)')
-    ax.set_xlabel('time (s)')
+        ax.plot(benchmark.temporal_bins, benchmark.motion.mean(1), lw=1, c=f"C{count}", label=benchmark.title)
+        ax.fill_between(
+            benchmark.temporal_bins,
+            benchmark.motion.mean(1) - benchmark.motion.std(1),
+            benchmark.motion.mean(1) + benchmark.motion.std(1),
+            color=f"C{count}",
+            alpha=0.25,
+        )
+
+    # ax.legend()
+    ax.set_ylabel("depth (um)")
+    ax.set_xlabel("time (s)")
     _simpleaxis(ax)
 
-def plot_speed_several_benchmarks(benchmarks, ax=None):
+
+def plot_speed_several_benchmarks(benchmarks, ax=None, colors=None):
     if ax is None:
         fig, ax = plt.subplots(figsize=(5, 5))
 
-    
     for count, benchmark in enumerate(benchmarks):
+        color = colors[count] if colors is not None else None
         bottom = 0
-        i=0
-        patterns = [ "/" , "\\" , "|" ,  "*" ]
+        i = 0
+        patterns = ["/", "\\", "|", "*"]
         for key, value in benchmark.run_times.items():
             if count == 0:
-                label = key
+                label = key.replace("_", " ")
             else:
                 label = None
-            ax.bar([count], [value], label=label, bottom=bottom, color=f'C{count}', edgecolor='black', hatch=patterns[i])
+            ax.bar([count], [value], label=label, bottom=bottom, color=color, edgecolor="black", hatch=patterns[i])
             bottom += value
-            i+=1
-        
-    ax.legend()
-    ax.set_ylabel('speed (s)')
+            i += 1
+
+    # ax.legend()
+    ax.set_ylabel("speed (s)")
     _simpleaxis(ax)
     ax.set_xticks([])
-    #ax.set_xticks(np.arange(len(benchmarks)), [i.title for i in benchmarks])
+    # ax.set_xticks(np.arange(len(benchmarks)), [i.title for i in benchmarks])
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sortingcomponents/benchmark/benchmark_peak_selection.py` & `spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/benchmark/benchmark_peak_selection.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,61 +1,69 @@
-
 from spikeinterface.core import extract_waveforms
 from spikeinterface.preprocessing import bandpass_filter, common_reference
 from spikeinterface.sortingcomponents.clustering import find_cluster_from_peaks
 from spikeinterface.core import NumpySorting
 from spikeinterface.qualitymetrics import compute_quality_metrics
 from spikeinterface.comparison import GroundTruthComparison
-from spikeinterface.widgets import plot_probe_map, plot_agreement_matrix, plot_comparison_collision_by_similarity, plot_unit_templates, plot_unit_waveforms
+from spikeinterface.widgets import (
+    plot_probe_map,
+    plot_agreement_matrix,
+    plot_comparison_collision_by_similarity,
+    plot_unit_templates,
+    plot_unit_waveforms,
+)
 from spikeinterface.postprocessing import compute_principal_components
 from spikeinterface.comparison.comparisontools import make_matching_events
-from spikeinterface.postprocessing import get_template_extremum_channel, get_template_extremum_amplitude, compute_center_of_mass
+from spikeinterface.postprocessing import (
+    get_template_extremum_channel,
+    get_template_extremum_amplitude,
+    compute_center_of_mass,
+)
 from spikeinterface.core import get_noise_levels
 
 import time
 import string, random
 import pylab as plt
 import os
 import numpy as np
 
-class BenchmarkPeakSelection:
 
+class BenchmarkPeakSelection:
     def __init__(self, recording, gt_sorting, exhaustive_gt=True, job_kwargs={}, tmp_folder=None, verbose=True):
         self.verbose = verbose
         self.recording = recording
         self.gt_sorting = gt_sorting
         self.job_kwargs = job_kwargs
         self.exhaustive_gt = exhaustive_gt
-        self.recording_f = recording
-        self.sampling_rate = self.recording_f.get_sampling_frequency()
+        self.sampling_rate = self.recording.get_sampling_frequency()
 
         self.tmp_folder = tmp_folder
         if self.tmp_folder is None:
-            self.tmp_folder = os.path.join('.', ''.join(random.choices(string.ascii_uppercase + string.digits, k=8)))
+            self.tmp_folder = os.path.join(".", "".join(random.choices(string.ascii_uppercase + string.digits, k=8)))
 
         self._peaks = None
         self._positions = None
         self._gt_positions = None
         self.gt_peaks = None
 
         self.waveforms = {}
         self.pcas = {}
         self.templates = {}
 
     def __del__(self):
         import shutil
+
         shutil.rmtree(self.tmp_folder)
 
     def set_peaks(self, peaks):
         self._peaks = peaks
 
     def set_positions(self, positions):
         self._positions = positions
 
-
     @property
     def peaks(self):
         if self._peaks is None:
             self.detect_peaks()
         return self._peaks
 
     @property
@@ -66,129 +74,163 @@
 
     @property
     def gt_positions(self):
         if self._gt_positions is None:
             self.localize_gt_peaks()
         return self._gt_positions
 
-    def detect_peaks(self, method_kwargs={'method' : 'locally_exclusive'}):
+    def detect_peaks(self, method_kwargs={"method": "locally_exclusive"}):
         from spikeinterface.sortingcomponents.peak_detection import detect_peaks
+
         if self.verbose:
-            method = method_kwargs['method']
-            print(f'Detecting peaks with method {method}')
-        self._peaks = detect_peaks(self.recording_f, **method_kwargs, **self.job_kwargs)
+            method = method_kwargs["method"]
+            print(f"Detecting peaks with method {method}")
+        self._peaks = detect_peaks(self.recording, **method_kwargs, **self.job_kwargs)
 
-    def localize_peaks(self, method_kwargs = {'method' : 'center_of_mass'}):
+    def localize_peaks(self, method_kwargs={"method": "center_of_mass"}):
         from spikeinterface.sortingcomponents.peak_localization import localize_peaks
+
         if self.verbose:
-            method = method_kwargs['method']
-            print(f'Localizing peaks with method {method}')
-        self._positions = localize_peaks(self.recording_f, self.peaks, **method_kwargs, **self.job_kwargs)
+            method = method_kwargs["method"]
+            print(f"Localizing peaks with method {method}")
+        self._positions = localize_peaks(self.recording, self.peaks, **method_kwargs, **self.job_kwargs)
 
-    def localize_gt_peaks(self, method_kwargs = {'method' : 'center_of_mass'}):
+    def localize_gt_peaks(self, method_kwargs={"method": "center_of_mass"}):
         from spikeinterface.sortingcomponents.peak_localization import localize_peaks
+
         if self.verbose:
-            method = method_kwargs['method']
-            print(f'Localizing gt peaks with method {method}')
-        self._gt_positions = localize_peaks(self.recording_f, self.gt_peaks, **method_kwargs, **self.job_kwargs)
+            method = method_kwargs["method"]
+            print(f"Localizing gt peaks with method {method}")
+        self._gt_positions = localize_peaks(self.recording, self.gt_peaks, **method_kwargs, **self.job_kwargs)
 
     def run(self, peaks=None, positions=None, delta=0.2):
         t_start = time.time()
-        
 
         if peaks is not None:
             self._peaks = peaks
 
         nb_peaks = len(self.peaks)
 
         if positions is not None:
             self._positions = positions
 
         times1 = self.gt_sorting.get_all_spike_trains()[0]
-        times2 = self.peaks['sample_ind']
+        times2 = self.peaks["sample_index"]
 
         print("The gt recording has {} peaks and {} have been detected".format(len(times1[0]), len(times2)))
-        
-        matches = make_matching_events(times1[0], times2, int(delta*self.sampling_rate/1000))
+
+        matches = make_matching_events(times1[0], times2, int(delta * self.sampling_rate / 1000))
         self.matches = matches
 
-        self.deltas = {'labels' : [], 'delta' : matches['delta_frame']}
-        self.deltas['labels'] = times1[1][matches['index1']]
+        self.deltas = {"labels": [], "delta": matches["delta_frame"]}
+        self.deltas["labels"] = times1[1][matches["index1"]]
 
-        #print(len(times1[0]), len(matches['index1']))
-        gt_matches = matches['index1']
-        self.sliced_gt_sorting = NumpySorting.from_times_labels(times1[0][gt_matches], times1[1][gt_matches], self.sampling_rate, unit_ids = self.gt_sorting.unit_ids)
-        ratio = 100*len(gt_matches)/len(times1[0])
+        # print(len(times1[0]), len(matches['index1']))
+        gt_matches = matches["index1"]
+        self.sliced_gt_sorting = NumpySorting.from_times_labels(
+            times1[0][gt_matches], times1[1][gt_matches], self.sampling_rate, unit_ids=self.gt_sorting.unit_ids
+        )
+        ratio = 100 * len(gt_matches) / len(times1[0])
         print("Only {0:.2f}% of gt peaks are matched to detected peaks".format(ratio))
 
-        matches = make_matching_events(times2, times1[0], int(delta*self.sampling_rate/1000))
-        self.good_matches = matches['index1']
+        matches = make_matching_events(times2, times1[0], int(delta * self.sampling_rate / 1000))
+        self.good_matches = matches["index1"]
 
         garbage_matches = ~np.in1d(np.arange(len(times2)), self.good_matches)
-        garbage_channels = self.peaks['channel_ind'][garbage_matches]
+        garbage_channels = self.peaks["channel_index"][garbage_matches]
         garbage_peaks = times2[garbage_matches]
         nb_garbage = len(garbage_peaks)
 
-        ratio = 100*len(garbage_peaks)/len(times2)
+        ratio = 100 * len(garbage_peaks) / len(times2)
         self.garbage_sorting = NumpySorting.from_times_labels(garbage_peaks, garbage_channels, self.sampling_rate)
-        
+
         print("The peaks have {0:.2f}% of garbage (without gt around)".format(ratio))
 
         self.comp = GroundTruthComparison(self.gt_sorting, self.sliced_gt_sorting, exhaustive_gt=self.exhaustive_gt)
 
-        for label, sorting in zip(['gt', 'full_gt', 'garbage'], [self.sliced_gt_sorting, self.gt_sorting, self.garbage_sorting]): 
-
+        for label, sorting in zip(
+            ["gt", "full_gt", "garbage"], [self.sliced_gt_sorting, self.gt_sorting, self.garbage_sorting]
+        ):
             tmp_folder = os.path.join(self.tmp_folder, label)
             if os.path.exists(tmp_folder):
                 import shutil
-                shutil.rmtree(tmp_folder)
 
-            if not (label == 'full_gt' and label in self.waveforms):
+                shutil.rmtree(tmp_folder)
 
+            if not (label == "full_gt" and label in self.waveforms):
                 if self.verbose:
                     print(f"Extracting waveforms for {label}")
 
-                self.waveforms[label] = extract_waveforms(self.recording_f, sorting, tmp_folder, load_if_exists=True,
-                                       ms_before=2.5, ms_after=3.5, max_spikes_per_unit=500, return_scaled=False, 
-                                       **self.job_kwargs)
+                self.waveforms[label] = extract_waveforms(
+                    self.recording,
+                    sorting,
+                    tmp_folder,
+                    load_if_exists=True,
+                    ms_before=2.5,
+                    ms_after=3.5,
+                    max_spikes_per_unit=500,
+                    return_scaled=False,
+                    **self.job_kwargs,
+                )
+
+                self.templates[label] = self.waveforms[label].get_all_templates(mode="median")
 
-                self.templates[label] = self.waveforms[label].get_all_templates(mode='median')
-    
         if self.gt_peaks is None:
             if self.verbose:
                 print("Computing gt peaks")
             gt_peaks_ = self.gt_sorting.to_spike_vector()
-            self.gt_peaks = np.zeros(gt_peaks_.size, dtype=[('sample_ind', '<i8'), ('channel_ind', '<i8'), ('segment_ind', '<i8'), ('amplitude', '<f8')])
-            self.gt_peaks['sample_ind'] = gt_peaks_['sample_ind']
-            self.gt_peaks['segment_ind'] = gt_peaks_['segment_ind']
-            max_channels = get_template_extremum_channel(self.waveforms['full_gt'], peak_sign='neg', outputs='index')
-            max_amplitudes = get_template_extremum_amplitude(self.waveforms['full_gt'], peak_sign='neg')
+            self.gt_peaks = np.zeros(
+                gt_peaks_.size,
+                dtype=[
+                    ("sample_index", "<i8"),
+                    ("channel_index", "<i8"),
+                    ("segment_index", "<i8"),
+                    ("amplitude", "<f8"),
+                ],
+            )
+            self.gt_peaks["sample_index"] = gt_peaks_["sample_index"]
+            self.gt_peaks["segment_index"] = gt_peaks_["segment_index"]
+            max_channels = get_template_extremum_channel(self.waveforms["full_gt"], peak_sign="neg", outputs="index")
+            max_amplitudes = get_template_extremum_amplitude(self.waveforms["full_gt"], peak_sign="neg")
 
-            for unit_ind, unit_id in enumerate(self.waveforms['full_gt'].sorting.unit_ids):
-                mask = gt_peaks_['unit_ind'] == unit_ind
+            for unit_ind, unit_id in enumerate(self.waveforms["full_gt"].sorting.unit_ids):
+                mask = gt_peaks_["unit_index"] == unit_ind
                 max_channel = max_channels[unit_id]
-                self.gt_peaks['channel_ind'][mask] = max_channel
-                self.gt_peaks['amplitude'][mask] = max_amplitudes[unit_id]
+                self.gt_peaks["channel_index"][mask] = max_channel
+                self.gt_peaks["amplitude"][mask] = max_amplitudes[unit_id]
 
         self.sliced_gt_peaks = self.gt_peaks[gt_matches]
         self.sliced_gt_positions = self.gt_positions[gt_matches]
-        self.sliced_gt_labels = self.sliced_gt_sorting.to_spike_vector()['unit_ind']
-        self.gt_labels = self.gt_sorting.to_spike_vector()['unit_ind']
+        self.sliced_gt_labels = self.sliced_gt_sorting.to_spike_vector()["unit_index"]
+        self.gt_labels = self.gt_sorting.to_spike_vector()["unit_index"]
         self.garbage_positions = self.positions[garbage_matches]
         self.garbage_peaks = self.peaks[garbage_matches]
 
-
-    def _scatter_clusters(self, xs, ys, sorting, colors=None, labels=None, ax=None, n_std=2.0, force_black_for=[], s=1, alpha=0.5, show_ellipses=True):
-
+    def _scatter_clusters(
+        self,
+        xs,
+        ys,
+        sorting,
+        colors=None,
+        labels=None,
+        ax=None,
+        n_std=2.0,
+        force_black_for=[],
+        s=1,
+        alpha=0.5,
+        show_ellipses=True,
+    ):
         if colors is None:
             from spikeinterface.widgets import get_unit_colors
+
             colors = get_unit_colors(sorting)
 
         from matplotlib.patches import Ellipse
         import matplotlib.transforms as transforms
+
         ax = ax or plt.gca()
         # scatter and collect gaussian info
         means = {}
         covs = {}
         labels_ids = sorting.get_all_spike_trains()[0][1]
 
         for unit_ind, unit_id in enumerate(sorting.unit_ids):
@@ -200,15 +242,15 @@
                 ax.scatter(xk, yk, s=s, color=colors[unit_id], alpha=alpha, marker=".")
                 x_mean, y_mean = xk.mean(), yk.mean()
                 xycov = np.cov(xk, yk)
                 means[unit_id] = x_mean, y_mean
                 covs[unit_id] = xycov
                 ax.annotate(unit_id, (x_mean, y_mean))
             else:
-                ax.scatter(xk, yk, s=s, colorun='k', alpha=alpha, marker=".")
+                ax.scatter(xk, yk, s=s, colorun="k", alpha=alpha, marker=".")
 
         for unit_id in means.keys():
             mean_x, mean_y = means[unit_id]
             cov = covs[unit_id]
 
             with np.errstate(invalid="ignore"):
                 vx, vy = cov[0, 0], cov[1, 1]
@@ -233,207 +275,235 @@
                 )
                 ell.set_transform(transform + ax.transData)
                 ax.add_patch(ell)
 
     def plot_clusters(self, title=None, show_probe=False, show_ellipses=True):
         fig, axs = plt.subplots(ncols=3, nrows=1, figsize=(15, 10))
         if title is not None:
-            fig.suptitle(f'Peak selection results with {title}')
+            fig.suptitle(f"Peak selection results with {title}")
 
         ax = axs[0]
-        ax.set_title('Full gt clusters')
+        ax.set_title("Full gt clusters")
         if show_probe:
-            plot_probe_map(self.recording_f, ax=ax)
+            plot_probe_map(self.recording, ax=ax)
 
         from spikeinterface.widgets import get_unit_colors
+
         colors = get_unit_colors(self.gt_sorting)
-        self._scatter_clusters(self.gt_positions['x'], self.gt_positions['y'],  self.gt_sorting, colors, s=1, alpha=0.5, ax=ax, show_ellipses=show_ellipses)
+        self._scatter_clusters(
+            self.gt_positions["x"],
+            self.gt_positions["y"],
+            self.gt_sorting,
+            colors,
+            s=1,
+            alpha=0.5,
+            ax=ax,
+            show_ellipses=show_ellipses,
+        )
         xlim = ax.get_xlim()
         ylim = ax.get_ylim()
-        ax.set_xlabel('x')
-        ax.set_ylabel('y')
+        ax.set_xlabel("x")
+        ax.set_ylabel("y")
 
         ax = axs[1]
-        ax.set_title('Sliced gt clusters')
+        ax.set_title("Sliced gt clusters")
         if show_probe:
-            plot_probe_map(self.recording_f, ax=ax)
+            plot_probe_map(self.recording, ax=ax)
 
-        self._scatter_clusters(self.sliced_gt_positions['x'], self.sliced_gt_positions['y'],  self.sliced_gt_sorting, colors, s=1, alpha=0.5, ax=ax, show_ellipses=show_ellipses)
+        self._scatter_clusters(
+            self.sliced_gt_positions["x"],
+            self.sliced_gt_positions["y"],
+            self.sliced_gt_sorting,
+            colors,
+            s=1,
+            alpha=0.5,
+            ax=ax,
+            show_ellipses=show_ellipses,
+        )
         if self.exhaustive_gt:
             ax.set_xlim(xlim)
             ax.set_ylim(ylim)
-        ax.set_xlabel('x')
+        ax.set_xlabel("x")
         ax.set_yticks([], [])
 
         ax = axs[2]
-        ax.set_title('Garbage')
+        ax.set_title("Garbage")
         if show_probe:
-            plot_probe_map(self.recording_f, ax=ax)
+            plot_probe_map(self.recording, ax=ax)
 
-        ax.scatter(self.garbage_positions['x'], self.garbage_positions['y'],  c='k', s=1, alpha=0.5)
+        ax.scatter(self.garbage_positions["x"], self.garbage_positions["y"], c="k", s=1, alpha=0.5)
         if self.exhaustive_gt:
             ax.set_xlim(xlim)
             ax.set_ylim(ylim)
             ax.set_yticks([], [])
-        ax.set_xlabel('x')
-        
+        ax.set_xlabel("x")
 
-    def plot_clusters_amplitudes(self, title=None, show_probe=False, clim=(-100, 0), cmap='viridis'):
+    def plot_clusters_amplitudes(self, title=None, show_probe=False, clim=(-100, 0), cmap="viridis"):
         fig, axs = plt.subplots(ncols=3, nrows=1, figsize=(15, 10))
         if title is not None:
-            fig.suptitle(f'Peak selection results with {title}')
+            fig.suptitle(f"Peak selection results with {title}")
 
         ax = axs[0]
-        ax.set_title('Full gt clusters')
+        ax.set_title("Full gt clusters")
         if show_probe:
-            plot_probe_map(self.recording_f, ax=ax)
-        
+            plot_probe_map(self.recording, ax=ax)
+
         from spikeinterface.widgets import get_unit_colors
-        channels = get_template_extremum_channel(self.waveforms['full_gt'], outputs='index')
 
-        #cb = fig.colorbar(cm, ax=ax)
-        #cb.set_label(metric)
+        channels = get_template_extremum_channel(self.waveforms["full_gt"], outputs="index")
+
+        # cb = fig.colorbar(cm, ax=ax)
+        # cb.set_label(metric)
 
         import matplotlib
+
         my_cmap = plt.get_cmap(cmap)
-        cNorm  = matplotlib.colors.Normalize(vmin=clim[0], vmax=clim[1])
+        cNorm = matplotlib.colors.Normalize(vmin=clim[0], vmax=clim[1])
         scalarMap = plt.cm.ScalarMappable(norm=cNorm, cmap=my_cmap)
 
-
-
         for unit_id in self.gt_sorting.unit_ids:
-            wfs = self.waveforms['full_gt'].get_waveforms(unit_id)
-            amplitudes = wfs[:, self.waveforms['full_gt'].nbefore, channels[unit_id]]
+            wfs = self.waveforms["full_gt"].get_waveforms(unit_id)
+            amplitudes = wfs[:, self.waveforms["full_gt"].nbefore, channels[unit_id]]
 
-            idx = self.waveforms['full_gt'].get_sampled_indices(unit_id)['spike_index']
-            all_spikes = self.waveforms['full_gt'].sorting.get_unit_spike_train(unit_id)
-            mask = np.in1d(self.gt_peaks['sample_ind'], all_spikes[idx])
-            colors = scalarMap.to_rgba(self.gt_peaks['amplitude'][mask])
-            ax.scatter(self.gt_positions['x'][mask], self.gt_positions['y'][mask], c=colors, s=1, alpha=0.5)
-            x_mean, y_mean = (self.gt_positions['x'][mask].mean(), self.gt_positions['y'][mask].mean())
+            idx = self.waveforms["full_gt"].get_sampled_indices(unit_id)["spike_index"]
+            all_spikes = self.waveforms["full_gt"].sorting.get_unit_spike_train(unit_id)
+            mask = np.in1d(self.gt_peaks["sample_index"], all_spikes[idx])
+            colors = scalarMap.to_rgba(self.gt_peaks["amplitude"][mask])
+            ax.scatter(self.gt_positions["x"][mask], self.gt_positions["y"][mask], c=colors, s=1, alpha=0.5)
+            x_mean, y_mean = (self.gt_positions["x"][mask].mean(), self.gt_positions["y"][mask].mean())
             ax.annotate(unit_id, (x_mean, y_mean))
 
         xlim = ax.get_xlim()
         ylim = ax.get_ylim()
-        ax.set_xlabel('x')
-        ax.set_ylabel('y')
+        ax.set_xlabel("x")
+        ax.set_ylabel("y")
 
         ax = axs[1]
-        ax.set_title('Sliced gt clusters')
+        ax.set_title("Sliced gt clusters")
         if show_probe:
-            plot_probe_map(self.recording_f, ax=ax)
-        
+            plot_probe_map(self.recording, ax=ax)
+
         from spikeinterface.widgets import get_unit_colors
-        channels = get_template_extremum_channel(self.waveforms['gt'], outputs='index')
+
+        channels = get_template_extremum_channel(self.waveforms["gt"], outputs="index")
 
         for unit_id in self.sliced_gt_sorting.unit_ids:
-            wfs = self.waveforms['gt'].get_waveforms(unit_id)
-            amplitudes = wfs[:, self.waveforms['gt'].nbefore, channels[unit_id]]
+            wfs = self.waveforms["gt"].get_waveforms(unit_id)
+            amplitudes = wfs[:, self.waveforms["gt"].nbefore, channels[unit_id]]
 
-            idx = self.waveforms['gt'].get_sampled_indices(unit_id)['spike_index']
-            all_spikes = self.waveforms['gt'].sorting.get_unit_spike_train(unit_id)
-            mask = np.in1d(self.sliced_gt_peaks['sample_ind'], all_spikes[idx])
-            colors = scalarMap.to_rgba(self.sliced_gt_peaks['amplitude'][mask])
-            ax.scatter(self.sliced_gt_positions['x'][mask], self.sliced_gt_positions['y'][mask],  c=colors, s=1, alpha=0.5)
-            x_mean, y_mean = (self.sliced_gt_positions['x'][mask].mean(), self.sliced_gt_positions['y'][mask].mean())
+            idx = self.waveforms["gt"].get_sampled_indices(unit_id)["spike_index"]
+            all_spikes = self.waveforms["gt"].sorting.get_unit_spike_train(unit_id)
+            mask = np.in1d(self.sliced_gt_peaks["sample_index"], all_spikes[idx])
+            colors = scalarMap.to_rgba(self.sliced_gt_peaks["amplitude"][mask])
+            ax.scatter(
+                self.sliced_gt_positions["x"][mask], self.sliced_gt_positions["y"][mask], c=colors, s=1, alpha=0.5
+            )
+            x_mean, y_mean = (self.sliced_gt_positions["x"][mask].mean(), self.sliced_gt_positions["y"][mask].mean())
             ax.annotate(unit_id, (x_mean, y_mean))
 
         xlim = ax.get_xlim()
         ylim = ax.get_ylim()
-        ax.set_xlabel('x')
+        ax.set_xlabel("x")
         ax.set_yticks([], [])
-        #ax.set_ylabel('y')
+        # ax.set_ylabel('y')
 
         ax = axs[2]
-        ax.set_title('Garbage')
+        ax.set_title("Garbage")
         if show_probe:
-            plot_probe_map(self.recording_f, ax=ax)
-        
+            plot_probe_map(self.recording, ax=ax)
+
         from spikeinterface.widgets import get_unit_colors
-        channels = get_template_extremum_channel(self.waveforms['garbage'], outputs='index')
+
+        channels = get_template_extremum_channel(self.waveforms["garbage"], outputs="index")
 
         for unit_id in self.garbage_sorting.unit_ids:
-            wfs = self.waveforms['garbage'].get_waveforms(unit_id)
-            amplitudes = wfs[:, self.waveforms['garbage'].nbefore, channels[unit_id]]
+            wfs = self.waveforms["garbage"].get_waveforms(unit_id)
+            amplitudes = wfs[:, self.waveforms["garbage"].nbefore, channels[unit_id]]
 
-            idx = self.waveforms['garbage'].get_sampled_indices(unit_id)['spike_index']
-            all_spikes = self.waveforms['garbage'].sorting.get_unit_spike_train(unit_id)
-            mask = np.in1d(self.garbage_peaks['sample_ind'], all_spikes[idx])
-            colors = scalarMap.to_rgba(self.garbage_peaks['amplitude'][mask])
-            ax.scatter(self.garbage_positions['x'][mask], self.garbage_positions['y'][mask],  c=colors, s=1, alpha=0.5)
-            x_mean, y_mean = (self.garbage_positions['x'][mask].mean(), self.garbage_positions['y'][mask].mean())
+            idx = self.waveforms["garbage"].get_sampled_indices(unit_id)["spike_index"]
+            all_spikes = self.waveforms["garbage"].sorting.get_unit_spike_train(unit_id)
+            mask = np.in1d(self.garbage_peaks["sample_index"], all_spikes[idx])
+            colors = scalarMap.to_rgba(self.garbage_peaks["amplitude"][mask])
+            ax.scatter(self.garbage_positions["x"][mask], self.garbage_positions["y"][mask], c=colors, s=1, alpha=0.5)
+            x_mean, y_mean = (self.garbage_positions["x"][mask].mean(), self.garbage_positions["y"][mask].mean())
             ax.annotate(unit_id, (x_mean, y_mean))
 
-
-
         xlim = ax.get_xlim()
         ylim = ax.get_ylim()
-        ax.set_xlabel('x')
+        ax.set_xlabel("x")
         ax.set_yticks([], [])
-        #ax.set_ylabel('y')
-
-    def plot_statistics(self, metric='cosine', annotations=True, detect_threshold=5):
+        # ax.set_ylabel('y')
 
+    def plot_statistics(self, metric="cosine", annotations=True, detect_threshold=5):
         fig, axs = plt.subplots(ncols=3, nrows=2, figsize=(15, 10))
-        
+
         ax = axs[0, 0]
         plot_agreement_matrix(self.comp, ax=ax)
 
         scores = self.comp.get_ordered_agreement_scores()
         unit_ids1 = scores.index.values
         unit_ids2 = scores.columns.values
         inds_1 = self.comp.sorting1.ids_to_indices(unit_ids1)
         inds_2 = self.comp.sorting2.ids_to_indices(unit_ids2)
 
-        a = self.templates['full_gt'].reshape(len(self.templates['full_gt']), -1)[inds_1]
-        b = self.templates['gt'].reshape(len(self.templates['gt']), -1)[inds_2]
+        a = self.templates["full_gt"].reshape(len(self.templates["full_gt"]), -1)[inds_1]
+        b = self.templates["gt"].reshape(len(self.templates["gt"]), -1)[inds_2]
 
         import sklearn
-        if metric == 'cosine':
+
+        if metric == "cosine":
             distances = sklearn.metrics.pairwise.cosine_similarity(a, b)
         else:
             distances = sklearn.metrics.pairwise_distances(a, b, metric)
 
         print(distances)
         ax = axs[0, 1]
-        im = ax.imshow(distances, aspect='auto')
+        im = ax.imshow(distances, aspect="auto")
         ax.set_title(metric)
         fig.colorbar(im, ax=ax)
 
         ax.set_yticks(np.arange(0, len(scores.index)))
         ax.set_yticklabels(scores.index, fontsize=8)
 
         ax.set_xticks(np.arange(0, len(scores.columns)))
         ax.set_xticklabels(scores.columns, fontsize=8)
 
         ax = axs[0, 2]
 
         ax.set_ylabel("Time mismatch (time step)")
         for unit_ind, unit_id in enumerate(self.gt_sorting.unit_ids):
-            mask = self.deltas['labels'] == unit_id
-            ax.violinplot(self.deltas['delta'][mask], [unit_ind], widths=2, showmeans=True, showmedians=False, showextrema=False)
+            mask = self.deltas["labels"] == unit_id
+            ax.violinplot(
+                self.deltas["delta"][mask], [unit_ind], widths=2, showmeans=True, showmedians=False, showextrema=False
+            )
         ax.set_xticks(np.arange(len(self.gt_sorting.unit_ids)), self.gt_sorting.unit_ids)
-        ax.spines['top'].set_visible(False)
-        ax.spines['right'].set_visible(False)
+        ax.spines["top"].set_visible(False)
+        ax.spines["right"].set_visible(False)
 
         ax = axs[1, 0]
 
-        noise_levels = get_noise_levels(self.recording_f, return_scaled=False)
-        snrs = self.peaks['amplitude']/noise_levels[self.peaks['channel_ind']]
-        garbage_snrs = self.garbage_peaks['amplitude']/noise_levels[self.garbage_peaks['channel_ind']]
+        noise_levels = get_noise_levels(self.recording, return_scaled=False)
+        snrs = self.peaks["amplitude"] / noise_levels[self.peaks["channel_index"]]
+        garbage_snrs = self.garbage_peaks["amplitude"] / noise_levels[self.garbage_peaks["channel_index"]]
         amin, amax = snrs.min(), snrs.max()
-        
-        ax.hist(snrs, np.linspace(amin, amax, 100), density=True, label='peaks')
-        #ax.hist(garbage_snrs, np.linspace(amin, amax, 100), density=True, label='garbage', alpha=0.5)
-        ax.hist(self.sliced_gt_peaks['amplitude']/noise_levels[self.sliced_gt_peaks['channel_ind']], np.linspace(amin, amax, 100), density=True, alpha=0.5, label='matched gt')
-        ax.spines['top'].set_visible(False)
-        ax.spines['right'].set_visible(False)
+
+        ax.hist(snrs, np.linspace(amin, amax, 100), density=True, label="peaks")
+        # ax.hist(garbage_snrs, np.linspace(amin, amax, 100), density=True, label='garbage', alpha=0.5)
+        ax.hist(
+            self.sliced_gt_peaks["amplitude"] / noise_levels[self.sliced_gt_peaks["channel_index"]],
+            np.linspace(amin, amax, 100),
+            density=True,
+            alpha=0.5,
+            label="matched gt",
+        )
+        ax.spines["top"].set_visible(False)
+        ax.spines["right"].set_visible(False)
         ax.legend()
-        ax.set_xlabel('snrs')
-        ax.set_ylabel('density')
+        ax.set_xlabel("snrs")
+        ax.set_ylabel("density")
 
         # dist = []
         # dist_real = []
 
         # for found, real in zip(unit_ids2, unit_ids1):
         #     wfs = self.waveforms['gt'].get_waveforms(found)
         #     wfs_real = self.waveforms['full_gt'].get_waveforms(real)
@@ -446,60 +516,57 @@
         #     if metric == 'cosine':
         #         dist += [sklearn.metrics.pairwise.cosine_similarity(template, wfs.reshape(len(wfs), -1), metric).flatten()]
         #         dist_real += [sklearn.metrics.pairwise.cosine_similarity(template_real, wfs_real.reshape(len(wfs_real), -1), metric).flatten()]
         #     else:
         #         dist += [sklearn.metrics.pairwise_distances(template, wfs.reshape(len(wfs), -1), metric).flatten()]
         #         dist_real += [sklearn.metrics.pairwise_distances(template_real, wfs_real.reshape(len(wfs_real), -1), metric).flatten()]
 
-        # ax.errorbar([a.mean() for a in dist], [a.mean() for a in dist_real], [a.std() for a in dist], [a.std() for a in dist_real], capsize=0, ls='none', color='black', 
+        # ax.errorbar([a.mean() for a in dist], [a.mean() for a in dist_real], [a.std() for a in dist], [a.std() for a in dist_real], capsize=0, ls='none', color='black',
         #     elinewidth=2)
         # ax.plot([0, 1], [0, 1], '--')
         # ax.set_xlabel('cosine dispersion tested')
         # ax.set_ylabel('cosine dispersion gt')
         # ax.spines['top'].set_visible(False)
         # ax.spines['right'].set_visible(False)
-        
-
 
         ax = axs[1, 1]
         nb_spikes_real = []
         nb_spikes = []
 
         for found, real in zip(unit_ids2, unit_ids1):
             a = self.gt_sorting.get_unit_spike_train(real).size
             b = self.sliced_gt_sorting.get_unit_spike_train(found).size
             nb_spikes_real += [a]
             nb_spikes += [b]
 
-
-        centers = compute_center_of_mass(self.waveforms['gt'])
+        centers = compute_center_of_mass(self.waveforms["gt"])
         times, labels = self.sliced_gt_sorting.get_all_spike_trains()[0]
         stds = []
         means = []
         for found, real in zip(unit_ids2, inds_1):
             mask = labels == found
-            center = np.array([self.sliced_gt_positions[mask]['x'], self.sliced_gt_positions[mask]['y']]).mean()
+            center = np.array([self.sliced_gt_positions[mask]["x"], self.sliced_gt_positions[mask]["y"]]).mean()
             means += [np.mean(center - centers[real])]
             stds += [np.std(center - centers[real])]
 
-        metrics = compute_quality_metrics(self.waveforms['full_gt'], metric_names=['snr'], load_if_exists=False)
-        ax.errorbar(metrics['snr'][inds_1], means, yerr=stds, ls='none')
+        metrics = compute_quality_metrics(self.waveforms["full_gt"], metric_names=["snr"], load_if_exists=False)
+        ax.errorbar(metrics["snr"][inds_1], means, yerr=stds, ls="none")
 
-        ax.set_xlabel('template snr')
-        ax.set_ylabel('position error')
-        ax.spines['top'].set_visible(False)
-        ax.spines['right'].set_visible(False)
+        ax.set_xlabel("template snr")
+        ax.set_ylabel("position error")
+        ax.spines["top"].set_visible(False)
+        ax.spines["right"].set_visible(False)
 
         if annotations:
-            for l,x,y in zip(unit_ids1, metrics['snr'][inds_1], means):
+            for l, x, y in zip(unit_ids1, metrics["snr"][inds_1], means):
                 ax.annotate(l, (x, y))
 
         if detect_threshold is not None:
             ymin, ymax = ax.get_ylim()
-            ax.plot([detect_threshold, detect_threshold], [ymin, ymax], 'k--')
+            ax.plot([detect_threshold, detect_threshold], [ymin, ymax], "k--")
 
         # ax.plot(nb_spikes, nb_spikes_real, '.', markersize=10)
         # ax.set_xlabel("# spikes tested")
         # ax.set_ylabel("# spikes gt")
         # ax.spines['top'].set_visible(False)
         # ax.spines['right'].set_visible(False)
         # xmin, xmax = ax.get_xlim()
@@ -508,60 +575,60 @@
 
         # if annotations:
         #     for l,x,y in zip(unit_ids1, nb_spikes, nb_spikes_real):
         #         ax.annotate(l, (x, y))
 
         # fs = self.recording_f.get_sampling_frequency()
         # tmax = self.recording_f.get_total_duration()
-        # ax.hist(self.peaks['sample_ind']/fs, np.linspace(0, tmax, 100), density=True)
+        # ax.hist(self.peaks['sample_index']/fs, np.linspace(0, tmax, 100), density=True)
         # ax.spines['top'].set_visible(False)
         # ax.spines['right'].set_visible(False)
         # ax.set_xlabel('time (s)')
         # ax.set_ylabel('density')
 
-        # for channel_ind in       
+        # for channel_index in
         #     ax.hist(snrs, np.linspace(amin, amax, 100), density=True, label='peaks')
         # ax.spines['top'].set_visible(False)
         # ax.spines['right'].set_visible(False)
         # ax.legend()
         # ax.set_xlabel('snrs')
         # ax.set_ylabel('density')
 
-
         ax = axs[1, 2]
-        metrics = compute_quality_metrics(self.waveforms['full_gt'], metric_names=['snr'], load_if_exists=False)
-        ratios = np.array(nb_spikes)/np.array(nb_spikes_real)
-        plt.plot(metrics['snr'][inds_1], ratios, '.')
-        ax.set_xlabel('template snr')
-        ax.set_ylabel('% gt spikes detected')
-        ax.spines['top'].set_visible(False)
-        ax.spines['right'].set_visible(False)
+        metrics = compute_quality_metrics(self.waveforms["full_gt"], metric_names=["snr"], load_if_exists=False)
+        ratios = np.array(nb_spikes) / np.array(nb_spikes_real)
+        plt.plot(metrics["snr"][inds_1], ratios, ".")
+        ax.set_xlabel("template snr")
+        ax.set_ylabel("% gt spikes detected")
+        ax.spines["top"].set_visible(False)
+        ax.spines["right"].set_visible(False)
 
         if annotations:
-            for l,x,y in zip(unit_ids1, metrics['snr'][inds_1], ratios):
+            for l, x, y in zip(unit_ids1, metrics["snr"][inds_1], ratios):
                 ax.annotate(l, (x, y))
 
         if detect_threshold is not None:
             ymin, ymax = ax.get_ylim()
-            ax.plot([detect_threshold, detect_threshold], [ymin, ymax], 'k--')
+            ax.plot([detect_threshold, detect_threshold], [ymin, ymax], "k--")
 
-    def explore_garbage(self, channel_ind, nb_bins=None, dt=None):
-        mask = self.garbage_peaks['channel_ind'] == channel_ind
-        times2 = self.garbage_peaks[mask]['sample_ind']
+    def explore_garbage(self, channel_index, nb_bins=None, dt=None):
+        mask = self.garbage_peaks["channel_index"] == channel_index
+        times2 = self.garbage_peaks[mask]["sample_index"]
         times1 = self.gt_sorting.get_all_spike_trains()[0]
         from spikeinterface.comparison.comparisontools import make_matching_events
+
         if dt is None:
-            delta = self.waveforms['garbage'].nafter
+            delta = self.waveforms["garbage"].nafter
         else:
             delta = dt
         matches = make_matching_events(times2, times1[0], delta)
-        units = times1[1][matches['index2']]
-        dt = matches['delta_frame']
+        units = times1[1][matches["index2"]]
+        dt = matches["delta_frame"]
         res = {}
         fig, ax = plt.subplots()
         if nb_bins is None:
-            nb_bins = 2*delta
+            nb_bins = 2 * delta
         xaxis = np.linspace(-delta, delta, nb_bins)
         for unit_id in np.unique(units):
             mask = units == unit_id
             res[unit_id] = dt[mask]
-            ax.hist(res[unit_id], bins=xaxis)
+            ax.hist(res[unit_id], bins=xaxis)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sortingcomponents/benchmark/benchmark_tools.py` & `spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/benchmark/benchmark_tools.py`

 * *Files 12% similar despite different names*

```diff
@@ -5,34 +5,37 @@
 
 
 from spikeinterface.core import load_waveforms, NpzSortingExtractor
 from spikeinterface.core.core_tools import check_json
 
 
 def _simpleaxis(ax):
-    ax.spines['top'].set_visible(False)
-    ax.spines['right'].set_visible(False)
+    ax.spines["top"].set_visible(False)
+    ax.spines["right"].set_visible(False)
     ax.get_xaxis().tick_bottom()
     ax.get_yaxis().tick_left()
 
 
 class BenchmarkBase:
     _array_names = ()
     _waveform_names = ()
     _sorting_names = ()
 
     _array_names_from_parent = ()
     _waveform_names_from_parent = ()
     _sorting_names_from_parent = ()
 
-
-    def __init__(self, folder=None, title='', overwrite=None, 
-                job_kwargs={'chunk_duration' : '1s', 'n_jobs' : -1, 'progress_bar':True, 'verbose' :True},
-                parent_benchmark=None,
-                ):
+    def __init__(
+        self,
+        folder=None,
+        title="",
+        overwrite=None,
+        job_kwargs={"chunk_duration": "1s", "n_jobs": -1, "progress_bar": True, "verbose": True},
+        parent_benchmark=None,
+    ):
         self.folder = Path(folder)
         self.title = title
         self.overwrite = overwrite
         self.job_kwargs = job_kwargs
         self.run_times = None
 
         self._args = []
@@ -40,118 +43,117 @@
 
         self.waveforms = {}
         self.sortings = {}
 
         self.parent_benchmark = parent_benchmark
 
         if self.parent_benchmark is not None:
-            
             for name in self._array_names_from_parent:
                 setattr(self, name, getattr(parent_benchmark, name))
-            
+
             for name in self._waveform_names_from_parent:
                 self.waveforms[name] = parent_benchmark.waveforms[name]
 
             for key in parent_benchmark.sortings.keys():
                 if isinstance(key, str) and key in self._sorting_names_from_parent:
                     self.sortings[key] = parent_benchmark.sortings[key]
                 elif isinstance(key, tuple) and key[0] in self._sorting_names_from_parent:
                     self.sortings[key] = parent_benchmark.sortings[key]
 
     def save_to_folder(self):
         if self.folder.exists():
             import glob, os
+
             pattern = "*.*"
             files = self.folder.glob(pattern)
             for file in files:
                 if file.is_file():
                     os.remove(file)
         else:
             self.folder.mkdir(parents=True)
 
         if self.parent_benchmark is None:
             parent_folder = None
         else:
-            parent_folder = self.parent_benchmark.folder
+            parent_folder = str(self.parent_benchmark.folder)
 
         info = {
-            'args': self._args,
-            'kwargs': self._kwargs,
-            'parent_folder': parent_folder,
+            "args": self._args,
+            "kwargs": self._kwargs,
+            "parent_folder": parent_folder,
         }
         info = check_json(info)
-        (self.folder / 'info.json').write_text(json.dumps(info, indent=4), encoding='utf8')
+        (self.folder / "info.json").write_text(json.dumps(info, indent=4), encoding="utf8")
 
         for name in self._array_names:
             if self.parent_benchmark is not None and name in self._array_names_from_parent:
                 continue
             value = getattr(self, name)
             if value is not None:
-                np.save(self.folder / f'{name}.npy', value)
+                np.save(self.folder / f"{name}.npy", value)
 
         if self.run_times is not None:
-            run_times_filename = self.folder / 'run_times.json'
-            run_times_filename.write_text(json.dumps(self.run_times, indent=4),encoding='utf8')
+            run_times_filename = self.folder / "run_times.json"
+            run_times_filename.write_text(json.dumps(self.run_times, indent=4), encoding="utf8")
 
         for key, sorting in self.sortings.items():
-            (self.folder / 'sortings').mkdir(exist_ok=True)
+            (self.folder / "sortings").mkdir(exist_ok=True)
             if isinstance(key, str):
-                npz_file = self.folder / 'sortings'  / (str(key) + '.npz')
+                npz_file = self.folder / "sortings" / (str(key) + ".npz")
             elif isinstance(key, tuple):
-                npz_file = self.folder / 'sortings' / ('_###_'.join(key) + '.npz')
+                npz_file = self.folder / "sortings" / ("_###_".join(key) + ".npz")
             NpzSortingExtractor.write_sorting(sorting, npz_file)
 
-
     @classmethod
     def load_from_folder(cls, folder, parent_benchmark=None):
         folder = Path(folder)
         assert folder.exists()
 
-        with open(folder / 'info.json', 'r') as f:
+        with open(folder / "info.json", "r") as f:
             info = json.load(f)
-        args = info['args']
-        kwargs = info['kwargs']
-        
-        if info['parent_folder'] is None:
+        args = info["args"]
+        kwargs = info["kwargs"]
+
+        if info["parent_folder"] is None:
             parent_benchmark = None
         else:
             if parent_benchmark is None:
-                parent_benchmark = cls.load_from_folder(info['parent_folder'])
+                parent_benchmark = cls.load_from_folder(info["parent_folder"])
 
         import os
-        kwargs['folder'] = folder
+
+        kwargs["folder"] = folder
 
         bench = cls(*args, **kwargs, parent_benchmark=parent_benchmark)
 
         for name in cls._array_names:
-            filename = folder / f'{name}.npy'
+            filename = folder / f"{name}.npy"
             if filename.exists():
                 arr = np.load(filename)
             else:
                 arr = None
             setattr(bench, name, arr)
 
-        if (folder / 'run_times.json').exists():
-            with open(folder / 'run_times.json', 'r') as f:
+        if (folder / "run_times.json").exists():
+            with open(folder / "run_times.json", "r") as f:
                 bench.run_times = json.load(f)
         else:
             bench.run_times = None
 
-
         for key in bench._waveform_names:
             if parent_benchmark is not None and key in bench._waveform_names_from_parent:
                 continue
-            waveforms_folder = folder / 'waveforms' / key
+            waveforms_folder = folder / "waveforms" / key
             if waveforms_folder.exists():
                 bench.waveforms[key] = load_waveforms(waveforms_folder, with_recording=True)
 
-        sorting_folder = folder / 'sortings'
+        sorting_folder = folder / "sortings"
         if sorting_folder.exists():
-            for npz_file in sorting_folder.glob('*.npz'):
+            for npz_file in sorting_folder.glob("*.npz"):
                 name = npz_file.stem
-                if '_###_' in name:
-                    key = tuple(name.split('_###_'))
+                if "_###_" in name:
+                    key = tuple(name.split("_###_"))
                 else:
                     key = name
                 bench.sortings[key] = NpzSortingExtractor(npz_file)
 
         return bench
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sortingcomponents/clustering/circus.py` & `spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/clustering/circus.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,14 +1,16 @@
 # """Sorting components: clustering"""
 from pathlib import Path
 
 import shutil
 import numpy as np
+
 try:
     import hdbscan
+
     HAVE_HDBSCAN = True
 except:
     HAVE_HDBSCAN = False
 
 import random, string, os
 from spikeinterface.core import get_global_tmp_folder, get_noise_levels, get_channel_distances
 from sklearn.preprocessing import QuantileTransformer, MaxAbsScaler
@@ -19,224 +21,273 @@
 from spikeinterface.core.recording_tools import get_channel_distances, get_random_data_chunks
 
 
 class CircusClustering:
     """
     hdbscan clustering on peak_locations previously done by localize_peaks()
     """
+
     _default_params = {
-        "peak_locations" : None,
-        "peak_localization_kwargs" : {"method" : "center_of_mass"},
-        "hdbscan_kwargs": {"min_cluster_size" : 50,  "allow_single_cluster" : True, "core_dist_n_jobs" : -1, "cluster_selection_method" : "leaf"},
-        "cleaning_kwargs" : {},
-        "tmp_folder" : None,
-        "local_radius_um" : 100,
-        "n_pca" : 10,
-        "max_spikes_per_unit" : 200,
-        "ms_before" : 1.5,
+        "peak_locations": None,
+        "peak_localization_kwargs": {"method": "center_of_mass"},
+        "hdbscan_kwargs": {
+            "min_cluster_size": 50,
+            "allow_single_cluster": True,
+            "core_dist_n_jobs": -1,
+            "cluster_selection_method": "leaf",
+        },
+        "cleaning_kwargs": {},
+        "tmp_folder": None,
+        "local_radius_um": 100,
+        "n_pca": 10,
+        "max_spikes_per_unit": 200,
+        "ms_before": 1.5,
         "ms_after": 2.5,
         "cleaning_method": "dip",
-        "waveform_mode" : "memmap",
-        "job_kwargs" : {"n_jobs" : -1, "chunk_memory" : "10M"},
+        "waveform_mode": "memmap",
+        "job_kwargs": {"n_jobs": -1, "chunk_memory": "10M"},
     }
 
     @classmethod
     def _check_params(cls, recording, peaks, params):
         d = params
         params2 = params.copy()
-        
-        tmp_folder = params['tmp_folder']
-        if params['waveform_mode'] == 'memmap':
+
+        tmp_folder = params["tmp_folder"]
+        if params["waveform_mode"] == "memmap":
             if tmp_folder is None:
-                name = ''.join(random.choices(string.ascii_uppercase + string.digits, k=8))
+                name = "".join(random.choices(string.ascii_uppercase + string.digits, k=8))
                 tmp_folder = Path(os.path.join(get_global_tmp_folder(), name))
             else:
                 tmp_folder = Path(tmp_folder)
             tmp_folder.mkdir()
-            params2['tmp_folder'] = tmp_folder
-        elif params['waveform_mode'] ==  'shared_memory':
-            assert tmp_folder is None, 'tmp_folder must be None for shared_memory'
+            params2["tmp_folder"] = tmp_folder
+        elif params["waveform_mode"] == "shared_memory":
+            assert tmp_folder is None, "tmp_folder must be None for shared_memory"
         else:
-            raise ValueError('shared_memory')        
-        
+            raise ValueError("shared_memory")
+
         return params2
 
     @classmethod
     def main_function(cls, recording, peaks, params):
-        assert HAVE_HDBSCAN, 'twisted clustering need hdbscan to be installed'
+        assert HAVE_HDBSCAN, "twisted clustering need hdbscan to be installed"
 
         params = cls._check_params(recording, peaks, params)
         d = params
 
-        if d['peak_locations'] is None:
+        if d["peak_locations"] is None:
             from spikeinterface.sortingcomponents.peak_localization import localize_peaks
-            peak_locations = localize_peaks(recording, peaks, **d['peak_localization_kwargs'], **d['job_kwargs'])
+
+            peak_locations = localize_peaks(recording, peaks, **d["peak_localization_kwargs"], **d["job_kwargs"])
         else:
-            peak_locations = d['peak_locations']
+            peak_locations = d["peak_locations"]
 
-        tmp_folder = d['tmp_folder']
+        tmp_folder = d["tmp_folder"]
         if tmp_folder is not None:
             tmp_folder.mkdir(exist_ok=True)
-    
-        location_keys = ['x', 'y']
+
+        location_keys = ["x", "y"]
         locations = np.stack([peak_locations[k] for k in location_keys], axis=1)
 
         chan_locs = recording.get_channel_locations()
 
-        peak_dtype = [('sample_ind', 'int64'), ('unit_ind', 'int64'), ('segment_ind', 'int64')]
+        peak_dtype = [("sample_index", "int64"), ("unit_index", "int64"), ("segment_index", "int64")]
         spikes = np.zeros(peaks.size, dtype=peak_dtype)
-        spikes['sample_ind'] = peaks['sample_ind']
-        spikes['segment_ind'] = peaks['segment_ind']
-        spikes['unit_ind'] = peaks['channel_ind']
+        spikes["sample_index"] = peaks["sample_index"]
+        spikes["segment_index"] = peaks["segment_index"]
+        spikes["unit_index"] = peaks["channel_index"]
 
         num_chans = recording.get_num_channels()
-        sparsity_mask = np.zeros((peaks.size, num_chans), dtype='bool')
-        
+        sparsity_mask = np.zeros((peaks.size, num_chans), dtype="bool")
+
         unit_inds = range(num_chans)
         chan_distances = get_channel_distances(recording)
-        
+
         for main_chan in unit_inds:
-            closest_chans, = np.nonzero(chan_distances[main_chan, :] <= params['local_radius_um'])
+            (closest_chans,) = np.nonzero(chan_distances[main_chan, :] <= params["local_radius_um"])
             sparsity_mask[main_chan, closest_chans] = True
 
-        if params['waveform_mode'] == 'shared_memory':
+        if params["waveform_mode"] == "shared_memory":
             wf_folder = None
         else:
-            assert params['tmp_folder'] is not None
-            wf_folder = params['tmp_folder'] / 'sparse_snippets'
+            assert params["tmp_folder"] is not None
+            wf_folder = params["tmp_folder"] / "sparse_snippets"
             wf_folder.mkdir()
 
         fs = recording.get_sampling_frequency()
-        nbefore = int(params['ms_before'] * fs / 1000.)
-        nafter = int(params['ms_after'] * fs / 1000.)
+        nbefore = int(params["ms_before"] * fs / 1000.0)
+        nafter = int(params["ms_after"] * fs / 1000.0)
         num_samples = nbefore + nafter
 
-        wfs_arrays = extract_waveforms_to_buffers(recording, spikes, unit_inds, nbefore, nafter,
-                                mode=params['waveform_mode'], return_scaled=False, folder=wf_folder, dtype=recording.get_dtype(),
-                                sparsity_mask=sparsity_mask,  copy=(params['waveform_mode'] == 'shared_memory'),
-                                **params['job_kwargs'])
+        wfs_arrays = extract_waveforms_to_buffers(
+            recording,
+            spikes,
+            unit_inds,
+            nbefore,
+            nafter,
+            mode=params["waveform_mode"],
+            return_scaled=False,
+            folder=wf_folder,
+            dtype=recording.get_dtype(),
+            sparsity_mask=sparsity_mask,
+            copy=(params["waveform_mode"] == "shared_memory"),
+            **params["job_kwargs"],
+        )
 
         n_loc = len(location_keys)
         import sklearn.decomposition, hdbscan
 
         noise_levels = get_noise_levels(recording, return_scaled=False)
 
         nb_clusters = 0
         peak_labels = np.zeros(len(spikes), dtype=np.int32)
 
-        noise = get_random_data_chunks(recording, return_scaled=False,
-                        num_chunks_per_segment=params['max_spikes_per_unit'], chunk_size=nbefore+nafter, concatenated=False, seed=None)
+        noise = get_random_data_chunks(
+            recording,
+            return_scaled=False,
+            num_chunks_per_segment=params["max_spikes_per_unit"],
+            chunk_size=nbefore + nafter,
+            concatenated=False,
+            seed=None,
+        )
         noise = np.stack(noise, axis=0)
 
         for main_chan, waveforms in wfs_arrays.items():
-            idx = np.where(spikes['unit_ind'] == main_chan)[0]
-            channels, = np.nonzero(sparsity_mask[main_chan])
+            idx = np.where(spikes["unit_index"] == main_chan)[0]
+            (channels,) = np.nonzero(sparsity_mask[main_chan])
             sub_noise = noise[:, :, channels]
-        
+
             if len(waveforms) > 0:
                 sub_waveforms = waveforms
 
                 wfs = np.swapaxes(sub_waveforms, 1, 2).reshape(len(sub_waveforms), -1)
                 noise_wfs = np.swapaxes(sub_noise, 1, 2).reshape(len(sub_noise), -1)
 
-                n_pca = min(d['n_pca'], len(wfs))
+                n_pca = min(d["n_pca"], len(wfs))
                 pca = sklearn.decomposition.PCA(n_pca)
 
                 hdbscan_data = np.vstack((wfs, noise_wfs))
 
                 pca.fit(wfs)
                 hdbscan_data_pca = pca.transform(hdbscan_data)
-                clustering = hdbscan.hdbscan(hdbscan_data_pca, **d['hdbscan_kwargs'])
+                clustering = hdbscan.hdbscan(hdbscan_data_pca, **d["hdbscan_kwargs"])
 
-                noise_labels = clustering[0][len(wfs):]
-                valid_labels = clustering[0][:len(wfs)]
+                noise_labels = clustering[0][len(wfs) :]
+                valid_labels = clustering[0][: len(wfs)]
 
                 shared_indices = np.intersect1d(np.unique(noise_labels), np.unique(valid_labels))
                 for l in shared_indices:
                     idx_noise = noise_labels == l
                     idx_valid = valid_labels == l
                     if np.sum(idx_noise) > np.sum(idx_valid):
                         valid_labels[idx_valid] = -1
 
                 if np.unique(valid_labels).min() == -1:
                     valid_labels += 1
 
                 for l in np.unique(valid_labels):
                     idx_valid = valid_labels == l
-                    if np.sum(idx_valid) < d['hdbscan_kwargs']['min_cluster_size']:
+                    if np.sum(idx_valid) < d["hdbscan_kwargs"]["min_cluster_size"]:
                         valid_labels[idx_valid] = -1
 
                 peak_labels[idx] = valid_labels + nb_clusters
 
                 labels = np.unique(valid_labels)
                 labels = labels[labels >= 0]
                 nb_clusters += len(labels)
 
         labels = np.unique(peak_labels)
-        labels = labels[labels>=0]
+        labels = labels[labels >= 0]
 
         best_spikes = {}
         nb_spikes = 0
 
         all_indices = np.arange(0, peak_labels.size)
 
         for unit_ind in labels:
             mask = peak_labels == unit_ind
-            best_spikes[unit_ind] = np.random.permutation(all_indices[mask])[:params["max_spikes_per_unit"]]
+            best_spikes[unit_ind] = np.random.permutation(all_indices[mask])[: params["max_spikes_per_unit"]]
             nb_spikes += best_spikes[unit_ind].size
 
         spikes = np.zeros(nb_spikes, dtype=peak_dtype)
 
         mask = np.zeros(0, dtype=np.int32)
         for unit_ind in labels:
             mask = np.concatenate((mask, best_spikes[unit_ind]))
 
         idx = np.argsort(mask)
         mask = mask[idx]
 
-        spikes['sample_ind'] = peaks[mask]['sample_ind']
-        spikes['segment_ind'] = peaks[mask]['segment_ind']
-        spikes['unit_ind'] = peak_labels[mask]
+        spikes["sample_index"] = peaks[mask]["sample_index"]
+        spikes["segment_index"] = peaks[mask]["segment_index"]
+        spikes["unit_index"] = peak_labels[mask]
 
-        if params['waveform_mode'] == 'shared_memory':
+        if params["waveform_mode"] == "shared_memory":
             wf_folder = None
         else:
-            assert params['tmp_folder'] is not None
-            wf_folder = params['tmp_folder'] / 'dense_snippets'
+            assert params["tmp_folder"] is not None
+            wf_folder = params["tmp_folder"] / "dense_snippets"
             wf_folder.mkdir()
 
-
         cleaning_method = params["cleaning_method"]
 
-        print("We found %d raw clusters, starting to clean with %s..." %(len(labels), cleaning_method))
+        print("We found %d raw clusters, starting to clean with %s..." % (len(labels), cleaning_method))
 
         if cleaning_method == "cosine":
-
-            wfs_arrays = extract_waveforms_to_buffers(recording, spikes, labels, nbefore, nafter,
-                         mode=params['waveform_mode'], return_scaled=False, folder=wf_folder, dtype=recording.get_dtype(),
-                         sparsity_mask=None,  copy=(params['waveform_mode'] == 'shared_memory'),
-                         **params['job_kwargs'])
-
-            labels, peak_labels = remove_duplicates(wfs_arrays, noise_levels, peak_labels, num_samples, num_chans, **params['cleaning_kwargs'])
+            wfs_arrays = extract_waveforms_to_buffers(
+                recording,
+                spikes,
+                labels,
+                nbefore,
+                nafter,
+                mode=params["waveform_mode"],
+                return_scaled=False,
+                folder=wf_folder,
+                dtype=recording.get_dtype(),
+                sparsity_mask=None,
+                copy=(params["waveform_mode"] == "shared_memory"),
+                **params["job_kwargs"],
+            )
+
+            labels, peak_labels = remove_duplicates(
+                wfs_arrays, noise_levels, peak_labels, num_samples, num_chans, **params["cleaning_kwargs"]
+            )
 
         elif cleaning_method == "dip":
-
-            wfs_arrays = extract_waveforms_to_buffers(recording, spikes, labels, nbefore, nafter,
-                         mode=params['waveform_mode'], return_scaled=False, folder=wf_folder, dtype=recording.get_dtype(),
-                         sparsity_mask=None,  copy=(params['waveform_mode'] == 'shared_memory'),
-                         **params['job_kwargs'])
+            wfs_arrays = extract_waveforms_to_buffers(
+                recording,
+                spikes,
+                labels,
+                nbefore,
+                nafter,
+                mode=params["waveform_mode"],
+                return_scaled=False,
+                folder=wf_folder,
+                dtype=recording.get_dtype(),
+                sparsity_mask=None,
+                copy=(params["waveform_mode"] == "shared_memory"),
+                **params["job_kwargs"],
+            )
 
             labels, peak_labels = remove_duplicates_via_dip(wfs_arrays, peak_labels)
 
         elif cleaning_method == "matching":
-            name = ''.join(random.choices(string.ascii_uppercase + string.digits, k=8))
+            name = "".join(random.choices(string.ascii_uppercase + string.digits, k=8))
             tmp_folder = Path(os.path.join(get_global_tmp_folder(), name))
 
-            sorting = NumpySorting.from_times_labels(spikes['sample_ind'], spikes['unit_ind'], fs)
-            we = extract_waveforms(recording, sorting, tmp_folder, overwrite=True, ms_before=params['ms_before'], 
-                ms_after=params['ms_after'], **params['job_kwargs'])
-            labels, peak_labels = remove_duplicates_via_matching(we, peak_labels, job_kwargs=params['job_kwargs'])
+            sorting = NumpySorting.from_times_labels(spikes["sample_index"], spikes["unit_index"], fs)
+            we = extract_waveforms(
+                recording,
+                sorting,
+                tmp_folder,
+                overwrite=True,
+                ms_before=params["ms_before"],
+                ms_after=params["ms_after"],
+                **params["job_kwargs"],
+            )
+            labels, peak_labels = remove_duplicates_via_matching(we, peak_labels, job_kwargs=params["job_kwargs"])
             shutil.rmtree(tmp_folder)
 
-        print("We kept %d non-duplicated clusters..." %len(labels))
+        print("We kept %d non-duplicated clusters..." % len(labels))
 
         return labels, peak_labels
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sortingcomponents/clustering/clustering_tools.py` & `spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/clustering/clustering_tools.py`

 * *Files 11% similar despite different names*

```diff
@@ -3,608 +3,663 @@
 """
 
 import numpy as np
 from spikeinterface.core.job_tools import fix_job_kwargs
 from spikeinterface.postprocessing import check_equal_template_with_distribution_overlap
 
 
-def _split_waveforms(wfs_and_noise, noise_size, n_components_by_channel, n_components, hdbscan_params, probability_thr, debug):
-
+def _split_waveforms(
+    wfs_and_noise, noise_size, n_components_by_channel, n_components, hdbscan_params, probability_thr, debug
+):
     import sklearn.decomposition
     import hdbscan
-    
+
     valid_size = wfs_and_noise.shape[0] - noise_size
-    
+
     local_feature = np.zeros((wfs_and_noise.shape[0], n_components_by_channel * wfs_and_noise.shape[2]))
     tsvd = sklearn.decomposition.TruncatedSVD(n_components=n_components_by_channel)
     for c in range(wfs_and_noise.shape[2]):
-        local_feature[:, c*n_components_by_channel:(c+1)*n_components_by_channel] = tsvd.fit_transform(wfs_and_noise[:, :, c])
+        local_feature[:, c * n_components_by_channel : (c + 1) * n_components_by_channel] = tsvd.fit_transform(
+            wfs_and_noise[:, :, c]
+        )
     n_components = min(n_components, local_feature.shape[1])
     pca = sklearn.decomposition.PCA(n_components=n_components, whiten=True)
     local_feature = pca.fit_transform(local_feature)
-    
+
     # hdbscan on pca
     clustering = hdbscan.hdbscan(local_feature, **hdbscan_params)
     local_labels_with_noise = clustering[0]
     cluster_probability = clustering[2]
-    persistent_clusters, = np.nonzero(clustering[2] > probability_thr)
+    (persistent_clusters,) = np.nonzero(cluster_probability > probability_thr)
     local_labels_with_noise[~np.in1d(local_labels_with_noise, persistent_clusters)] = -1
-    
+
     # remove super small cluster
     labels, count = np.unique(local_labels_with_noise[:valid_size], return_counts=True)
     mask = labels >= 0
     labels, count = labels[mask], count[mask]
     minimum_cluster_size_ratio = 0.05
-    
-    #~ print(labels, count)
-    
-    to_remove = labels[(count / valid_size) <minimum_cluster_size_ratio]
-    #~ print('to_remove', to_remove, count / valid_size)
+
+    # ~ print(labels, count)
+
+    to_remove = labels[(count / valid_size) < minimum_cluster_size_ratio]
+    # ~ print('to_remove', to_remove, count / valid_size)
     if to_remove.size > 0:
         local_labels_with_noise[np.in1d(local_labels_with_noise, to_remove)] = -1
-    
+
     local_labels_with_noise[valid_size:] = -2
-    
 
     if debug:
         import matplotlib.pyplot as plt
         import umap
-        
+
         fig, ax = plt.subplots()
 
-        #~ reducer = umap.UMAP()
-        #~ local_feature_plot = reducer.fit_transform(local_feature)
+        # ~ reducer = umap.UMAP()
+        # ~ local_feature_plot = reducer.fit_transform(local_feature)
         local_feature_plot = local_feature
 
-        
         unique_lab = np.unique(local_labels_with_noise)
-        cmap = plt.get_cmap('jet', unique_lab.size)
-        cmap = { k: cmap(l) for l, k in enumerate(unique_lab) }
-        cmap[-1] = 'k'
+        cmap = plt.get_cmap("jet", unique_lab.size)
+        cmap = {k: cmap(l) for l, k in enumerate(unique_lab)}
+        cmap[-1] = "k"
         active_ind = np.arange(local_feature.shape[0])
         for k in unique_lab:
             plot_mask_1 = (active_ind < valid_size) & (local_labels_with_noise == k)
             plot_mask_2 = (active_ind >= valid_size) & (local_labels_with_noise == k)
-            ax.scatter(local_feature_plot[plot_mask_1, 0], local_feature_plot[plot_mask_1, 1], color=cmap[k], marker='o', alpha=0.3, s=1)
-            ax.scatter(local_feature_plot[plot_mask_2, 0], local_feature_plot[plot_mask_2, 1], color=cmap[k], marker='*', alpha=0.3, s=1)
-            
-        #~ plt.show()
+            ax.scatter(
+                local_feature_plot[plot_mask_1, 0],
+                local_feature_plot[plot_mask_1, 1],
+                color=cmap[k],
+                marker="o",
+                alpha=0.3,
+                s=1,
+            )
+            ax.scatter(
+                local_feature_plot[plot_mask_2, 0],
+                local_feature_plot[plot_mask_2, 1],
+                color=cmap[k],
+                marker="*",
+                alpha=0.3,
+                s=1,
+            )
+
+        # ~ plt.show()
 
-    
     return local_labels_with_noise
-    
 
-def _split_waveforms_nested(wfs_and_noise, noise_size, nbefore, n_components_by_channel, n_components, hdbscan_params, probability_thr, debug):
 
+def _split_waveforms_nested(
+    wfs_and_noise, noise_size, nbefore, n_components_by_channel, n_components, hdbscan_params, probability_thr, debug
+):
     import sklearn.decomposition
     import hdbscan
-    
+
     valid_size = wfs_and_noise.shape[0] - noise_size
-    
-    local_labels_with_noise =  np.zeros(wfs_and_noise.shape[0], dtype=np.int64)
-    
+
+    local_labels_with_noise = np.zeros(wfs_and_noise.shape[0], dtype=np.int64)
+
     local_count = 1
     while True:
-        #~ print('  local_count', local_count, np.sum(local_labels_with_noise[:-noise_size] == 0), local_labels_with_noise.size - noise_size)
-        
+        # ~ print('  local_count', local_count, np.sum(local_labels_with_noise[:-noise_size] == 0), local_labels_with_noise.size - noise_size)
+
         if np.all(local_labels_with_noise[:-noise_size] != 0):
             break
-        
-        active_ind, = np.nonzero(local_labels_with_noise == 0)
-        
+
+        (active_ind,) = np.nonzero(local_labels_with_noise == 0)
+
         # reduce dimention in 2 step
         active_wfs = wfs_and_noise[active_ind, :, :]
         local_feature = np.zeros((active_wfs.shape[0], n_components_by_channel * active_wfs.shape[2]))
         tsvd = sklearn.decomposition.TruncatedSVD(n_components=n_components_by_channel)
         for c in range(wfs_and_noise.shape[2]):
-            local_feature[:, c*n_components_by_channel:(c+1)*n_components_by_channel] = tsvd.fit_transform(active_wfs[:, :, c])
-        #~ n_components = min(n_components, local_feature.shape[1])
-        #~ pca = sklearn.decomposition.PCA(n_components=n_components, whiten=True)
-        #~ local_feature = pca.fit_transform(local_feature)
-        
+            local_feature[:, c * n_components_by_channel : (c + 1) * n_components_by_channel] = tsvd.fit_transform(
+                active_wfs[:, :, c]
+            )
+        # ~ n_components = min(n_components, local_feature.shape[1])
+        # ~ pca = sklearn.decomposition.PCA(n_components=n_components, whiten=True)
+        # ~ local_feature = pca.fit_transform(local_feature)
+
         # hdbscan on pca
         clustering = hdbscan.hdbscan(local_feature, **hdbscan_params)
         active_labels_with_noise = clustering[0]
         cluster_probability = clustering[2]
-        persistent_clusters, = np.nonzero(clustering[2] > probability_thr)
+        (persistent_clusters,) = np.nonzero(clustering[2] > probability_thr)
         active_labels_with_noise[~np.in1d(active_labels_with_noise, persistent_clusters)] = -1
-        
+
         active_labels = active_labels_with_noise[active_ind < valid_size]
         active_labels_set = np.unique(active_labels)
-        active_labels_set = active_labels_set[active_labels_set>=0]
+        active_labels_set = active_labels_set[active_labels_set >= 0]
         num_cluster = active_labels_set.size
 
         if debug:
             import matplotlib.pyplot as plt
+
             fig, ax = plt.subplots()
 
             import umap
 
             reducer = umap.UMAP()
             local_feature_plot = reducer.fit_transform(local_feature)
 
-            
             unique_lab = np.unique(active_labels_with_noise)
-            cmap = plt.get_cmap('jet', unique_lab.size)
-            cmap = { k: cmap(l) for l, k in enumerate(unique_lab) }
-            cmap[-1] = 'k'
-            cmap[-2] = 'b'
+            cmap = plt.get_cmap("jet", unique_lab.size)
+            cmap = {k: cmap(l) for l, k in enumerate(unique_lab)}
+            cmap[-1] = "k"
+            cmap[-2] = "b"
             for k in unique_lab:
                 plot_mask_1 = (active_ind < valid_size) & (active_labels_with_noise == k)
                 plot_mask_2 = (active_ind >= valid_size) & (active_labels_with_noise == k)
-                ax.scatter(local_feature_plot[plot_mask_1, 0], local_feature_plot[plot_mask_1, 1], color=cmap[k], marker='o')
-                ax.scatter(local_feature_plot[plot_mask_2, 0], local_feature_plot[plot_mask_2, 1], color=cmap[k], marker='*')
-                
-            #~ plt.show()
+                ax.scatter(
+                    local_feature_plot[plot_mask_1, 0], local_feature_plot[plot_mask_1, 1], color=cmap[k], marker="o"
+                )
+                ax.scatter(
+                    local_feature_plot[plot_mask_2, 0], local_feature_plot[plot_mask_2, 1], color=cmap[k], marker="*"
+                )
+
+            # ~ plt.show()
 
         if num_cluster > 1:
             # take the best one
             extremum_values = []
-            assert active_ind.size ==  active_labels_with_noise.size
+            assert active_ind.size == active_labels_with_noise.size
             for k in active_labels_set:
-                #~ sel = active_labels_with_noise == k
-                #~ sel[-noise_size:] = False
+                # ~ sel = active_labels_with_noise == k
+                # ~ sel[-noise_size:] = False
                 sel = (active_ind < valid_size) & (active_labels_with_noise == k)
                 if np.sum(sel) == 1:
                     # only one spike
                     extremum_values.append(0)
                 else:
                     v = np.mean(np.abs(np.mean(active_wfs[sel, nbefore, :], axis=0)))
                     extremum_values.append(v)
             best_label = active_labels_set[np.argmax(extremum_values)]
-            #~ inds = active_ind[active_labels_with_noise == best_label]
+            # ~ inds = active_ind[active_labels_with_noise == best_label]
             inds = active_ind[(active_ind < valid_size) & (active_labels_with_noise == best_label)]
-            #~ inds_no_noise = inds[inds < valid_size]
+            # ~ inds_no_noise = inds[inds < valid_size]
             if inds.size > 1:
                 # avoid cluster with one spike
                 local_labels_with_noise[inds] = local_count
                 local_count += 1
             else:
                 local_labels_with_noise[inds] = -1
-            
+
             local_count += 1
-            
+
         elif num_cluster == 1:
             best_label = active_labels_set[0]
-            #~ inds = active_ind[active_labels_with_noise == best_label]
-            #~ inds_no_noise = inds[inds < valid_size]
+            # ~ inds = active_ind[active_labels_with_noise == best_label]
+            # ~ inds_no_noise = inds[inds < valid_size]
             inds = active_ind[(active_ind < valid_size) & (active_labels_with_noise == best_label)]
             if inds.size > 1:
                 # avoid cluster with one spike
                 local_labels_with_noise[inds] = local_count
                 local_count += 1
             else:
                 local_labels_with_noise[inds] = -1
-                
+
             # last loop
-            local_labels_with_noise[active_ind[active_labels_with_noise==-1]] = -1
+            local_labels_with_noise[active_ind[active_labels_with_noise == -1]] = -1
         else:
             local_labels_with_noise[active_ind] = -1
             break
-    
-    #~ local_labels = local_labels_with_noise[:-noise_size]
-    local_labels_with_noise[local_labels_with_noise>0] -= 1
-    
-    return local_labels_with_noise
 
+    # ~ local_labels = local_labels_with_noise[:-noise_size]
+    local_labels_with_noise[local_labels_with_noise > 0] -= 1
+
+    return local_labels_with_noise
 
 
-def auto_split_clustering(wfs_arrays, sparsity_mask, labels, peak_labels,  nbefore, nafter, noise,
-                n_components_by_channel=3,
-                n_components=5,
-                hdbscan_params={},
-                probability_thr=0,
-                debug=False,
-                debug_folder=None,
-                ):
+def auto_split_clustering(
+    wfs_arrays,
+    sparsity_mask,
+    labels,
+    peak_labels,
+    nbefore,
+    nafter,
+    noise,
+    n_components_by_channel=3,
+    n_components=5,
+    hdbscan_params={},
+    probability_thr=0,
+    debug=False,
+    debug_folder=None,
+):
     """
     Loop over sparse waveform and try to over split.
     Internally used by PositionAndPCAClustering for the second step.
     """
-    
+
     import sklearn.decomposition
     import hdbscan
-    
+
     split_peak_labels = -1 * np.ones(peak_labels.size, dtype=np.int64)
     nb_clusters = 0
     main_channels = {}
     for l, label in enumerate(labels):
-        #~ print()
-        #~ print('auto_split_clustering', label, l, len(labels))
-        
-        chans, = np.nonzero(sparsity_mask[l, :])
-        
+        # ~ print()
+        # ~ print('auto_split_clustering', label, l, len(labels))
+
+        (chans,) = np.nonzero(sparsity_mask[l, :])
+
         wfs = wfs_arrays[label]
         valid_size = wfs.shape[0]
 
         wfs_and_noise = np.concatenate([wfs, noise[:, :, chans]], axis=0)
         noise_size = noise.shape[0]
-        
-        local_labels_with_noise = _split_waveforms(wfs_and_noise, noise_size, n_components_by_channel, n_components, hdbscan_params, probability_thr, debug)
-        # local_labels_with_noise = _split_waveforms_nested(wfs_and_noise, noise_size, nbefore, n_components_by_channel, n_components, hdbscan_params, probability_thr, debug)
+
+        local_labels_with_noise = _split_waveforms(
+            wfs_and_noise, noise_size, n_components_by_channel, n_components, hdbscan_params, probability_thr, debug
+        )
+        # local_labels_with_noise = _split_waveforms_nested(wfs_and_noise, noise_size, nbefore, n_components_by_channel, n_components, hdbscan_params, probability_thr, debug)
 
         local_labels = local_labels_with_noise[:valid_size]
         if noise_size > 0:
             local_labels_with_noise[valid_size:] = -2
-        
-        
-        for k in  np.unique(local_labels):
+
+        for k in np.unique(local_labels):
             if k < 0:
                 continue
             template = np.mean(wfs[local_labels == k, :, :], axis=1)
-            chan_inds, = np.nonzero(sparsity_mask[l, :])
+            (chan_inds,) = np.nonzero(sparsity_mask[l, :])
             assert wfs.shape[2] == chan_inds.size
             main_chan = chan_inds[np.argmax(np.max(np.abs(template), axis=0))]
             main_channels[k + nb_clusters] = main_chan
 
         if debug:
-            #~ local_labels_with_noise[-noise_size:] = -2
+            # ~ local_labels_with_noise[-noise_size:] = -2
             import matplotlib.pyplot as plt
-            #~ fig, axs = plt.subplots(ncols=3)
-            
+
+            # ~ fig, axs = plt.subplots(ncols=3)
+
             fig, ax = plt.subplots()
             plot_labels_set = np.unique(local_labels_with_noise)
-            cmap = plt.get_cmap('jet', plot_labels_set.size)
-            cmap = { k: cmap(l) for l, k in enumerate(plot_labels_set) }
-            cmap[-1] = 'k'
-            cmap[-2] = 'b'
-            
+            cmap = plt.get_cmap("jet", plot_labels_set.size)
+            cmap = {k: cmap(l) for l, k in enumerate(plot_labels_set)}
+            cmap[-1] = "k"
+            cmap[-2] = "b"
+
             for plot_label in plot_labels_set:
-                plot_mask = (local_labels_with_noise == plot_label)
+                plot_mask = local_labels_with_noise == plot_label
                 color = cmap[plot_label]
-                
+
                 wfs_flat = wfs_and_noise[plot_mask, :, :].swapaxes(1, 2).reshape(np.sum(plot_mask), -1).T
                 ax.plot(wfs_flat, color=color, alpha=0.1)
-                if plot_label >=0:
+                if plot_label >= 0:
                     ax.plot(wfs_flat.mean(1), color=color, lw=2)
-            
+
             for c in range(wfs.shape[2]):
-                ax.axvline(c * (nbefore + nafter) + nbefore, color='k', ls='-', alpha=0.5)
-            
+                ax.axvline(c * (nbefore + nafter) + nbefore, color="k", ls="-", alpha=0.5)
+
             if debug_folder is not None:
-                fig.savefig(debug_folder / f'auto_split_{label}.png')
-            
+                fig.savefig(debug_folder / f"auto_split_{label}.png")
+
             plt.show()
-        
+
         # remove noise labels
-        mask, = np.nonzero(peak_labels == label)
-        mask2, = np.nonzero(local_labels >= 0)
+        (mask,) = np.nonzero(peak_labels == label)
+        (mask2,) = np.nonzero(local_labels >= 0)
         split_peak_labels[mask[mask2]] = local_labels[mask2] + nb_clusters
 
         nb_clusters += local_labels.max() + 1
-    
-    return split_peak_labels, main_channels
-    
 
+    return split_peak_labels, main_channels
 
 
-def auto_clean_clustering(wfs_arrays, sparsity_mask, labels, peak_labels, nbefore, nafter, channel_distances,
-            radius_um=50, auto_merge_num_shift=7, auto_merge_quantile_limit=0.8, ratio_num_channel_intersect=0.8):
-    """
-    
-    
-    """
+def auto_clean_clustering(
+    wfs_arrays,
+    sparsity_mask,
+    labels,
+    peak_labels,
+    nbefore,
+    nafter,
+    channel_distances,
+    radius_um=50,
+    auto_merge_num_shift=7,
+    auto_merge_quantile_limit=0.8,
+    ratio_num_channel_intersect=0.8,
+):
+    """ """
     clean_peak_labels = peak_labels.copy()
 
-    #~ labels = np.unique(peak_labels)
-    #~ labels = labels[labels >= 0]
-    
+    # ~ labels = np.unique(peak_labels)
+    # ~ labels = labels[labels >= 0]
+
     # Check debug
     assert sparsity_mask.shape[0] == labels.shape[0]
-    
+
     # get main channel on new wfs
     templates = []
     main_channels = []
-    
+
     for l, label in enumerate(labels):
         wfs = wfs_arrays[label]
         template = np.mean(wfs, axis=0)
-        chan_inds, = np.nonzero(sparsity_mask[l, :])
+        (chan_inds,) = np.nonzero(sparsity_mask[l, :])
         assert wfs.shape[2] == chan_inds.size
         main_chan = chan_inds[np.argmax(np.max(np.abs(template), axis=0))]
         templates.append(template)
         main_channels.append(main_chan)
-        
-        #~ plot_debug = True
-        #~ if plot_debug:
-            #~ import matplotlib.pyplot as plt
-            #~ wfs_flat = wfs.swapaxes(1, 2).reshape(wfs.shape[0], -1).T
-            #~ fig, ax = plt.subplots()
-            #~ ax.plot(wfs_flat, color='g', alpha=0.1)
-            #~ ax.plot(template.T.flatten(), color='c', lw=2)
-            #~ plt.show()
+
+        # ~ plot_debug = True
+        # ~ if plot_debug:
+        # ~ import matplotlib.pyplot as plt
+        # ~ wfs_flat = wfs.swapaxes(1, 2).reshape(wfs.shape[0], -1).T
+        # ~ fig, ax = plt.subplots()
+        # ~ ax.plot(wfs_flat, color='g', alpha=0.1)
+        # ~ ax.plot(template.T.flatten(), color='c', lw=2)
+        # ~ plt.show()
 
     ## Step 1 : auto merge
     auto_merge_list = []
-    for l0 in  range(len(labels)):
-        for l1 in  range(l0+1, len(labels)):
-            
+    for l0 in range(len(labels)):
+        for l1 in range(l0 + 1, len(labels)):
             label0, label1 = labels[l0], labels[l1]
 
             main_chan0 = main_channels[l0]
             main_chan1 = main_channels[l1]
-            
+
             if channel_distances[main_chan0, main_chan1] > radius_um:
                 continue
-            
-            
-            channel_inds0, = np.nonzero(sparsity_mask[l0, :])
-            channel_inds1, = np.nonzero(sparsity_mask[l1, :])
-            
+
+            (channel_inds0,) = np.nonzero(sparsity_mask[l0, :])
+            (channel_inds1,) = np.nonzero(sparsity_mask[l1, :])
+
             intersect_chans = np.intersect1d(channel_inds0, channel_inds1)
-            # union_chans = np.union1d(channel_inds0, channel_inds1)
-            
+            # union_chans = np.union1d(channel_inds0, channel_inds1)
+
             # we use
-            radius_chans, = np.nonzero((channel_distances[main_chan0, :] <= radius_um) | (channel_distances[main_chan1, :] <= radius_um))
+            (radius_chans,) = np.nonzero(
+                (channel_distances[main_chan0, :] <= radius_um) | (channel_distances[main_chan1, :] <= radius_um)
+            )
             if radius_chans.size < (intersect_chans.size * ratio_num_channel_intersect):
-                #~ print('WARNING INTERSECT')
-                #~ print(intersect_chans.size, radius_chans.size, used_chans.size)
+                # ~ print('WARNING INTERSECT')
+                # ~ print(intersect_chans.size, radius_chans.size, used_chans.size)
                 continue
 
             used_chans = np.intersect1d(radius_chans, intersect_chans)
-            
+
             if used_chans.size == 0:
                 continue
-                
-            
+
             wfs0 = wfs_arrays[label0]
-            wfs0 = wfs0[:, :,np.in1d(channel_inds0, used_chans)]
+            wfs0 = wfs0[:, :, np.in1d(channel_inds0, used_chans)]
             wfs1 = wfs_arrays[label1]
-            wfs1 = wfs1[:, :,np.in1d(channel_inds1, used_chans)]
-            
+            wfs1 = wfs1[:, :, np.in1d(channel_inds1, used_chans)]
+
             # TODO : remove
             assert wfs0.shape[2] == wfs1.shape[2]
-            
-            equal, shift = check_equal_template_with_distribution_overlap(wfs0, wfs1,
-                            num_shift=auto_merge_num_shift, quantile_limit=auto_merge_quantile_limit, 
-                            return_shift=True)
-            
+
+            equal, shift = check_equal_template_with_distribution_overlap(
+                wfs0, wfs1, num_shift=auto_merge_num_shift, quantile_limit=auto_merge_quantile_limit, return_shift=True
+            )
+
             if equal:
                 auto_merge_list.append((l0, l1, shift))
 
             # DEBUG plot
-            #~ plot_debug = debug
-            #~ plot_debug = True
+            # ~ plot_debug = debug
+            # ~ plot_debug = True
             plot_debug = False
-            #~ plot_debug = equal
-            if plot_debug :
+            # ~ plot_debug = equal
+            if plot_debug:
                 import matplotlib.pyplot as plt
+
                 wfs_flat0 = wfs0.swapaxes(1, 2).reshape(wfs0.shape[0], -1).T
                 wfs_flat1 = wfs1.swapaxes(1, 2).reshape(wfs1.shape[0], -1).T
                 fig, ax = plt.subplots()
-                ax.plot(wfs_flat0, color='g', alpha=0.1)
-                ax.plot(wfs_flat1, color='r', alpha=0.1)
+                ax.plot(wfs_flat0, color="g", alpha=0.1)
+                ax.plot(wfs_flat1, color="r", alpha=0.1)
+
+                ax.plot(np.mean(wfs_flat0, axis=1), color="c", lw=2)
+                ax.plot(np.mean(wfs_flat1, axis=1), color="m", lw=2)
 
-                ax.plot(np.mean(wfs_flat0, axis=1), color='c', lw=2)
-                ax.plot(np.mean(wfs_flat1, axis=1), color='m', lw=2)
-                
                 for c in range(len(used_chans)):
-                    ax.axvline(c * (nbefore + nafter) + nbefore, color='k', ls='--')
-                ax.set_title(f'label0={label0} label1={label1} equal{equal} shift{shift} \n chans intersect{intersect_chans.size} radius{radius_chans.size} used{used_chans.size}')
+                    ax.axvline(c * (nbefore + nafter) + nbefore, color="k", ls="--")
+                ax.set_title(
+                    f"label0={label0} label1={label1} equal{equal} shift{shift} \n chans intersect{intersect_chans.size} radius{radius_chans.size} used{used_chans.size}"
+                )
                 plt.show()
-    
-    #~ print('auto_merge_list', auto_merge_list)
+
+    # ~ print('auto_merge_list', auto_merge_list)
     # merge in reverse order because of shift accumulation
-    peak_sample_shifts = np.zeros(peak_labels.size, dtype='int64')
-    for (l0, l1, shift) in auto_merge_list[::-1]:
+    peak_sample_shifts = np.zeros(peak_labels.size, dtype="int64")
+    for l0, l1, shift in auto_merge_list[::-1]:
         label0, label1 = labels[l0], labels[l1]
-        inds, = np.nonzero(peak_labels == label1)
+        (inds,) = np.nonzero(peak_labels == label1)
         clean_peak_labels[inds] = label0
         peak_sample_shifts[inds] += shift
 
     # update label list
     labels_clean = np.unique(clean_peak_labels)
     labels_clean = labels_clean[labels_clean >= 0]
-    
+
     # Step 2 : remove none aligner units
     # some unit have a secondary peak that can be detected
     # lets remove the secondary template
-    # to avoid recomputing template this is done on the original list
+    # to avoid recomputing template this is done on the original list
     auto_trash_list = []
     auto_trash_misalignment_shift = auto_merge_num_shift + 1
     for l, label in enumerate(labels):
         if label not in labels_clean:
             continue
-        
+
         template = templates[l]
         main_chan = main_channels[l]
-        chan_inds,  = np.nonzero(sparsity_mask[l, :])
+        (chan_inds,) = np.nonzero(sparsity_mask[l, :])
         max_ind = list(chan_inds).index(main_chan)
         sample_max = np.argmax(np.abs(template[:, max_ind]))
-        
+
         not_aligned = np.abs(sample_max - nbefore) >= auto_trash_misalignment_shift
         if not_aligned:
             auto_trash_list.append(label)
 
-        #~ plot_debug = not_aligned
-        #~ plot_debug = True
+        # ~ plot_debug = not_aligned
+        # ~ plot_debug = True
         plot_debug = False
-        #~ plot_debug = label in (23, )
-        if plot_debug :
+        # ~ plot_debug = label in (23, )
+        if plot_debug:
             import matplotlib.pyplot as plt
+
             fig, axs = plt.subplots(nrows=2)
             ax = axs[0]
             wfs_flat = template.T.flatten()
             ax.plot(wfs_flat)
             for c in range(template.shape[1]):
-                ax.axvline(c * (nbefore + nafter) + nbefore, color='k', ls='--')
+                ax.axvline(c * (nbefore + nafter) + nbefore, color="k", ls="--")
                 if c == max_ind:
-                    ax.axvline(c * (nbefore + nafter) + sample_max, color='b')
-                    
-            ax.set_title(f'label={label}  not_aligned{not_aligned} chan_inds={chan_inds} chan={chan_inds[max_ind]} max_ind{max_ind}')
+                    ax.axvline(c * (nbefore + nafter) + sample_max, color="b")
+
+            ax.set_title(
+                f"label={label}  not_aligned{not_aligned} chan_inds={chan_inds} chan={chan_inds[max_ind]} max_ind{max_ind}"
+            )
             ax = axs[1]
             ax.plot(template[:, max_ind])
             ax.axvline(sample_max)
-            ax.axvline(nbefore, color='r', ls='--')
+            ax.axvline(nbefore, color="r", ls="--")
             plt.show()
-    
-    #~ print('auto_trash_list', auto_trash_list)
+
+    # ~ print('auto_trash_list', auto_trash_list)
     for label in auto_trash_list:
-        inds, = np.nonzero(clean_peak_labels == label)
+        (inds,) = np.nonzero(clean_peak_labels == label)
         clean_peak_labels[inds] = -label
 
-
     return clean_peak_labels, peak_sample_shifts
 
 
-def remove_duplicates(wfs_arrays, noise_levels, peak_labels, num_samples, num_chans, cosine_threshold=0.975, sparsify_threshold=0.99):
-
-    import sklearn
-    nb_templates = len(wfs_arrays.keys())
-    templates = np.zeros((nb_templates, num_samples, num_chans), dtype=np.float32)
+def remove_duplicates(
+    wfs_arrays, noise_levels, peak_labels, num_samples, num_chans, cosine_threshold=0.975, sparsify_threshold=0.99
+):
+    num_templates = len(wfs_arrays.keys())
+    templates = np.zeros((num_templates, num_samples, num_chans), dtype=np.float32)
 
+    # All of this is to sparsity the templates
     for t, wfs in wfs_arrays.items():
-
         templates[t] = np.median(wfs, axis=0)
 
-        is_silent = templates[t].std(0) < 0.1*noise_levels
+        is_silent = templates[t].std(0) < 0.1 * noise_levels
         templates[t, :, is_silent] = 0
 
-        channel_norms = np.linalg.norm(templates[t], axis=0)**2
-        total_norm = np.linalg.norm(templates[t])**2
+        channel_norms = np.linalg.norm(templates[t], axis=0) ** 2
+        total_norm = np.linalg.norm(templates[t]) ** 2
 
         idx = np.argsort(channel_norms)[::-1]
-        explained_norms = np.cumsum(channel_norms[idx]/total_norm)
+        explained_norms = np.cumsum(channel_norms[idx] / total_norm)
         channel = np.searchsorted(explained_norms, sparsify_threshold)
         active_channels = np.sort(idx[:channel])
         templates[t, :, idx[channel:]] = 0
 
-    similarities = sklearn.metrics.pairwise.cosine_similarity(templates.reshape(nb_templates, -1))
-    for i in range(nb_templates):
+    from sklearn.metrics.pairwise import cosine_similarity
+
+    similarities = cosine_similarity(templates.reshape(num_templates, -1))
+
+    # Set the diagonal to -1
+    for i in range(num_templates):
         similarities[i, i] = -1
 
-    similar_templates = np.where(similarities > cosine_threshold)
+    similar_templates = np.nonzero(similarities > cosine_threshold)
 
     new_labels = peak_labels.copy()
 
     labels = np.unique(new_labels)
-    labels = labels[labels>=0]
-    
+    labels = labels[labels >= 0]
+
     for x, y in zip(similar_templates[0], similar_templates[1]):
         mask = new_labels == y
         new_labels[mask] = x
 
     labels = np.unique(new_labels)
-    labels = labels[labels>=0]
+    labels = labels[labels >= 0]
 
     return labels, new_labels
 
 
-
-def remove_duplicates_via_matching(waveform_extractor, noise_levels, peak_labels, sparsify_threshold=1,
-                                   method_kwargs={}, job_kwargs={}, tmp_folder=None):
-
+def remove_duplicates_via_matching(
+    waveform_extractor,
+    noise_levels,
+    peak_labels,
+    sparsify_threshold=1,
+    method_kwargs={},
+    job_kwargs={},
+    tmp_folder=None,
+):
     from spikeinterface.sortingcomponents.matching import find_spikes_from_templates
-    from spikeinterface import get_noise_levels 
+    from spikeinterface import get_noise_levels
     from spikeinterface.core import BinaryRecordingExtractor
     from spikeinterface.core import NumpySorting
     from spikeinterface.core import extract_waveforms
     from spikeinterface.core import get_global_tmp_folder
     from spikeinterface.sortingcomponents.matching.circus import get_scipy_shape
     import string, random, shutil, os
     from pathlib import Path
 
     job_kwargs = fix_job_kwargs(job_kwargs)
-    templates = waveform_extractor.get_all_templates(mode='median').copy()
+    templates = waveform_extractor.get_all_templates(mode="median").copy()
     nb_templates = len(templates)
     duration = waveform_extractor.nbefore + waveform_extractor.nafter
-    
+
     fs = waveform_extractor.recording.get_sampling_frequency()
     num_chans = waveform_extractor.recording.get_num_channels()
 
     for t in range(nb_templates):
         is_silent = templates[t].ptp(0) < sparsify_threshold
         templates[t, :, is_silent] = 0
 
     zdata = templates.reshape(nb_templates, -1)
 
     padding = 2 * duration
-    blanck = np.zeros(padding*num_chans, dtype=np.float32)
+    blanck = np.zeros(padding * num_chans, dtype=np.float32)
 
     if tmp_folder is None:
         tmp_folder = get_global_tmp_folder()
 
-    tmp_filename = tmp_folder / 'tmp.raw'
+    tmp_filename = tmp_folder / "tmp.raw"
 
-    f = open(tmp_filename, 'wb')
+    f = open(tmp_filename, "wb")
     f.write(blanck)
     f.write(zdata.flatten())
     f.write(blanck)
     f.close()
 
-    recording = BinaryRecordingExtractor(tmp_filename, num_chan=num_chans, sampling_frequency=fs, dtype='float32')
+    recording = BinaryRecordingExtractor(tmp_filename, num_chan=num_chans, sampling_frequency=fs, dtype="float32")
     recording.annotate(is_filtered=True)
 
     margin = 2 * max(waveform_extractor.nbefore, waveform_extractor.nafter)
-    half_marging = margin//2
+    half_marging = margin // 2
 
-    chunk_size = duration + 3*margin
+    chunk_size = duration + 3 * margin
 
     dummy_filter = np.empty((num_chans, duration), dtype=np.float32)
     dummy_traces = np.empty((num_chans, chunk_size), dtype=np.float32)
 
     fshape, axes = get_scipy_shape(dummy_filter, dummy_traces, axes=1)
 
-    method_kwargs.update({'waveform_extractor' : waveform_extractor, 
-                          'noise_levels' : noise_levels,
-                          'amplitudes' : [0.95, 1.05],
-                          'sparsify_threshold' : sparsify_threshold,
-                          'omp_min_sps' : 0.1,
-                          'templates' : None,
-                          'overlaps' : None})
+    method_kwargs.update(
+        {
+            "waveform_extractor": waveform_extractor,
+            "noise_levels": noise_levels,
+            "amplitudes": [0.95, 1.05],
+            "sparsify_threshold": sparsify_threshold,
+            "omp_min_sps": 0.1,
+            "templates": None,
+            "overlaps": None,
+        }
+    )
 
     ignore_ids = []
     similar_templates = [[], []]
 
     for i in range(nb_templates):
-
-        t_start = padding + i*duration
-        t_stop = padding + (i+1)*duration
+        t_start = padding + i * duration
+        t_stop = padding + (i + 1) * duration
 
         sub_recording = recording.frame_slice(t_start - half_marging, t_stop + half_marging)
 
-        method_kwargs.update({'ignored_ids' : ignore_ids + [i]})
-        spikes, computed = find_spikes_from_templates(sub_recording, method='circus-omp', method_kwargs=method_kwargs,
-                                                      extra_outputs=True, **job_kwargs)
-        method_kwargs.update({'overlaps' : computed['overlaps'],
-                              'templates' : computed['templates'],
-                              'norms' : computed['norms'],
-                              'sparsities' : computed['sparsities']})
-        valid = (spikes['sample_ind'] >= half_marging) * (spikes['sample_ind'] < duration + half_marging)
+        method_kwargs.update({"ignored_ids": ignore_ids + [i]})
+        spikes, computed = find_spikes_from_templates(
+            sub_recording, method="circus-omp", method_kwargs=method_kwargs, extra_outputs=True, **job_kwargs
+        )
+        method_kwargs.update(
+            {
+                "overlaps": computed["overlaps"],
+                "templates": computed["templates"],
+                "norms": computed["norms"],
+                "sparsities": computed["sparsities"],
+            }
+        )
+        valid = (spikes["sample_index"] >= half_marging) * (spikes["sample_index"] < duration + half_marging)
         if np.sum(valid) > 0:
             if np.sum(valid) == 1:
-                j = spikes[valid]['cluster_ind'][0]
+                j = spikes[valid]["cluster_index"][0]
                 ignore_ids += [i]
                 similar_templates[1] += [i]
                 similar_templates[0] += [j]
             elif np.sum(valid) > 1:
                 similar_templates[0] += [-1]
                 ignore_ids += [i]
                 similar_templates[1] += [i]
 
     new_labels = peak_labels.copy()
 
     labels = np.unique(new_labels)
-    labels = labels[labels>=0]
-    
+    labels = labels[labels >= 0]
+
     for x, y in zip(similar_templates[0], similar_templates[1]):
         mask = new_labels == y
         new_labels[mask] = x
 
     labels = np.unique(new_labels)
-    labels = labels[labels>=0]
+    labels = labels[labels >= 0]
 
     del recording, sub_recording
     os.remove(tmp_filename)
 
     return labels, new_labels
 
 
 def remove_duplicates_via_dip(wfs_arrays, peak_labels, dip_threshold=1, cosine_threshold=None):
-    
     import sklearn
 
     from spikeinterface.sortingcomponents.clustering.isocut5 import isocut5
 
     new_labels = peak_labels.copy()
 
     keep_merging = True
@@ -613,64 +668,64 @@
     templates = {}
     similarities = {}
     diptests = {}
     cosine = 0
 
     for i in wfs_arrays.keys():
         fused[i] = [i]
-        
+
     while keep_merging:
-        
         min_dip = np.inf
         to_merge = None
         labels = np.unique(new_labels)
-        labels = labels[labels>=0]
-        
+        labels = labels[labels >= 0]
+
         for i in labels:
             if len(fused[i]) > 1:
                 all_data_i = np.vstack([wfs_arrays[c] for c in fused[i]])
             else:
                 all_data_i = wfs_arrays[i]
             n_i = len(all_data_i)
             if n_i > 0:
                 if i in templates:
                     t_i = templates[i]
                 else:
                     t_i = np.median(all_data_i, axis=0).flatten()
                     templates[i] = t_i
                 data_i = all_data_i.reshape(n_i, -1)
-                
+
                 if i not in similarities:
                     similarities[i] = {}
 
                 if i not in diptests:
                     diptests[i] = {}
-                
-                for j in labels[i+1:]:
+
+                for j in labels[i + 1 :]:
                     if len(fused[j]) > 1:
                         all_data_j = np.vstack([wfs_arrays[c] for c in fused[j]])
                     else:
                         all_data_j = wfs_arrays[j]
                     n_j = len(all_data_j)
                     if n_j > 0:
                         if j in templates:
                             t_j = templates[j]
                         else:
                             t_j = np.median(all_data_j, axis=0).flatten()
                             templates[j] = t_j
-                        
+
                         if cosine_threshold is not None:
                             if j in similarities[i]:
                                 cosine = similarities[i][j]
                             else:
-                                cosine = sklearn.metrics.pairwise.cosine_similarity(t_i[np.newaxis, :], t_j[np.newaxis, :])[0][0]
+                                cosine = sklearn.metrics.pairwise.cosine_similarity(
+                                    t_i[np.newaxis, :], t_j[np.newaxis, :]
+                                )[0][0]
                                 similarities[i][j] = cosine
-                        
-                        if cosine_threshold is None or cosine > cosine_threshold:
 
+                        if cosine_threshold is None or cosine > cosine_threshold:
                             if j in diptests[i]:
                                 diptest = diptests[i][j]
                             else:
                                 data_j = all_data_j.reshape(n_j, -1)
                                 v = t_i - t_j
                                 pr_i = np.dot(data_i, v)
                                 pr_j = np.dot(data_j, v)
@@ -688,10 +743,10 @@
             templates.pop(to_merge[0])
             similarities.pop(to_merge[0])
             diptests.pop(to_merge[0])
         else:
             keep_merging = False
 
     labels = np.unique(new_labels)
-    labels = labels[labels>=0]
+    labels = labels[labels >= 0]
 
-    return labels, new_labels
+    return labels, new_labels
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sortingcomponents/clustering/isocut5.py` & `spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/clustering/isocut5.py`

 * *Files 12% similar despite different names*

```diff
@@ -64,40 +64,24 @@
             wsum[last_index] = x[j] * weights[j]
             wsumsqr[last_index] = x[j] * x[j] * weights[j]
             MSE[j] = MSE[j - 1]
 
             while True:
                 if last_index <= 0:
                     break
-                if (
-                    wsum[last_index - 1] / count[last_index - 1]
-                    < wsum[last_index] / count[last_index]
-                ):
+                if wsum[last_index - 1] / count[last_index - 1] < wsum[last_index] / count[last_index]:
                     break
 
-                prevMSE = (
-                    wsumsqr[last_index - 1]
-                    - wsum[last_index - 1]
-                    * wsum[last_index - 1]
-                    / count[last_index - 1]
-                )
-                prevMSE += (
-                    wsumsqr[last_index]
-                    - wsum[last_index] * wsum[last_index] / count[last_index]
-                )
+                prevMSE = wsumsqr[last_index - 1] - wsum[last_index - 1] * wsum[last_index - 1] / count[last_index - 1]
+                prevMSE += wsumsqr[last_index] - wsum[last_index] * wsum[last_index] / count[last_index]
                 unweightedcount[last_index - 1] += unweightedcount[last_index]
                 count[last_index - 1] += count[last_index]
                 wsum[last_index - 1] += wsum[last_index]
                 wsumsqr[last_index - 1] += wsumsqr[last_index]
-                newMSE = (
-                    wsumsqr[last_index - 1]
-                    - wsum[last_index - 1]
-                    * wsum[last_index - 1]
-                    / count[last_index - 1]
-                )
+                newMSE = wsumsqr[last_index - 1] - wsum[last_index - 1] * wsum[last_index - 1] / count[last_index - 1]
                 MSE[j] += newMSE - prevMSE
                 last_index -= 1
 
         ii = 0
         for k in range(last_index + 1):
             for cc in range(unweightedcount[k]):
                 y[ii + cc] = wsum[k] / count[k]
@@ -179,57 +163,47 @@
         sample_weights = sample_weights[sort]
 
         while True:
             intervals = updown_arange(num_bins, dtype=float)
             alpha = (N - 1) / intervals.sum()
             intervals *= alpha
             # this line is the only one to translate to 0-based
-            inds = np.floor(np.hstack((float_0, np.cumsum(intervals)))).astype(
-                np.int_
-            )
+            inds = np.floor(np.hstack((float_0, np.cumsum(intervals)))).astype(np.int_)
             if intervals.min() >= 1:
                 break
             else:
                 num_bins -= 1
 
         cumsum_sample_weights = np.cumsum(sample_weights)
         X_sub = X[inds]
         spacings = np.diff(X_sub)
         multiplicities = np.diff(cumsum_sample_weights[inds])
         densities = multiplicities / spacings
 
-        densities_unimodal_fit = up_down_isotonic_regression(
-            densities, multiplicities
-        )
+        densities_unimodal_fit = up_down_isotonic_regression(densities, multiplicities)
         peak_ind = np.argmax(densities_unimodal_fit)
 
         # difficult translation of indexing from 1-based to 0-based in
         # the following few lines. this has been checked thoroughly.
         ks_left, ks_left_ind = compute_ks5(
             multiplicities[0 : peak_ind + 1],
-            densities_unimodal_fit[0 : peak_ind + 1]
-            * spacings[0 : peak_ind + 1],
+            densities_unimodal_fit[0 : peak_ind + 1] * spacings[0 : peak_ind + 1],
         )
         ks_right, ks_right_ind = compute_ks5(
             multiplicities[peak_ind:][::-1],
-            densities_unimodal_fit[peak_ind:][::-1]
-            * spacings[peak_ind:][::-1],
+            densities_unimodal_fit[peak_ind:][::-1] * spacings[peak_ind:][::-1],
         )
         ks_right_ind = spacings.size - ks_right_ind
 
         if ks_left > ks_right:
             critical_range = slice(ks_left_ind)
             dipscore = ks_left
         else:
             critical_range = slice(ks_right_ind, spacings.size)
             dipscore = ks_right
 
-        densities_resid = (
-            densities[critical_range] - densities_unimodal_fit[critical_range]
-        )
-        densities_resid_fit = down_up_isotonic_regression(
-            densities_resid, spacings[critical_range]
-        )
+        densities_resid = densities[critical_range] - densities_unimodal_fit[critical_range]
+        densities_resid_fit = down_up_isotonic_regression(densities_resid, spacings[critical_range])
         cutpoint_ind = critical_range.start + np.argmin(densities_resid_fit)
         cutpoint = (X_sub[cutpoint_ind] + X_sub[cutpoint_ind + 1]) / 2
 
         return dipscore, cutpoint
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sortingcomponents/clustering/main.py` & `spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/clustering/main.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,16 +1,16 @@
 from .method_list import *
 
 from spikeinterface.core.job_tools import fix_job_kwargs
 
 
-def find_cluster_from_peaks(recording, peaks, method='stupid', method_kwargs={}, extra_outputs=False, **job_kwargs):
+def find_cluster_from_peaks(recording, peaks, method="stupid", method_kwargs={}, extra_outputs=False, **job_kwargs):
     """
     Find cluster from peaks.
-    
+
 
     Parameters
     ----------
     recording: RecordingExtractor
         The recording extractor object
     peaks: WaveformExtractor
         The waveform extractor
@@ -26,20 +26,21 @@
     labels: ndarray of int
         possible clusters list
     peak_labels: array of int
         peak_labels.shape[0] == peaks.shape[0]
     """
     job_kwargs = fix_job_kwargs(job_kwargs)
 
-    assert method in clustering_methods, f'Method for clustering do not exists, should be in {list(clustering_methods.keys())}'
-    
+    assert (
+        method in clustering_methods
+    ), f"Method for clustering do not exists, should be in {list(clustering_methods.keys())}"
+
     method_class = clustering_methods[method]
     params = method_class._default_params.copy()
     params.update(**method_kwargs)
-    
+
     labels, peak_labels = method_class.main_function(recording, peaks, params)
-    
+
     if extra_outputs:
         raise NotImplementedError
 
-
-    return labels, peak_labels
+    return labels, peak_labels
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sortingcomponents/clustering/method_list.py` & `spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/clustering/method_list.py`

 * *Files 8% similar despite different names*

```diff
@@ -13,9 +13,9 @@
     "position": PositionClustering,
     "position_ptp_scaled": PositionPTPScaledClustering,
     "position_and_pca": PositionAndPCAClustering,
     "sliding_hdbscan": SlidingHdbscanClustering,
     "sliding_nn": SlidingNNClustering,
     "position_and_features": PositionAndFeaturesClustering,
     "random_projections": RandomProjectionClustering,
-    "circus" : CircusClustering
-}
+    "circus": CircusClustering,
+}
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sortingcomponents/clustering/position.py` & `spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/clustering/position.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,71 +1,79 @@
 # """Sorting components: clustering"""
 from pathlib import Path
 
 import numpy as np
+
 try:
     import hdbscan
+
     HAVE_HDBSCAN = True
 except:
     HAVE_HDBSCAN = False
 
+
 class PositionClustering:
     """
     hdbscan clustering on peak_locations previously done by localize_peaks()
     """
+
     _default_params = {
-        "peak_locations" : None,
-        "use_amplitude" : True,
-        "peak_localization_kwargs" : {"method" : "center_of_mass"},
-        "hdbscan_kwargs": {"min_cluster_size" : 20,  "allow_single_cluster" : True, "core_dist_n_jobs" : -1},
-        "debug" : False,
-        "tmp_folder" : None,
-        'job_kwargs' : {'n_jobs' : -1, 'chunk_memory' : '10M'},
+        "peak_locations": None,
+        "use_amplitude": True,
+        "peak_localization_kwargs": {"method": "center_of_mass"},
+        "hdbscan_kwargs": {"min_cluster_size": 20, "allow_single_cluster": True, "core_dist_n_jobs": -1},
+        "debug": False,
+        "tmp_folder": None,
+        "job_kwargs": {"n_jobs": -1, "chunk_memory": "10M"},
     }
 
     @classmethod
     def main_function(cls, recording, peaks, params):
-        assert HAVE_HDBSCAN, 'position clustering need hdbscan to be installed'
+        assert HAVE_HDBSCAN, "position clustering need hdbscan to be installed"
         d = params
 
-        if d['peak_locations'] is None:
+        if d["peak_locations"] is None:
             from spikeinterface.sortingcomponents.peak_localization import localize_peaks
-            peak_locations = localize_peaks(recording, peaks, **d['peak_localization_kwargs'], **d['job_kwargs'])
+
+            peak_locations = localize_peaks(recording, peaks, **d["peak_localization_kwargs"], **d["job_kwargs"])
         else:
-            peak_locations = d['peak_locations']
+            peak_locations = d["peak_locations"]
 
-        tmp_folder = d['tmp_folder']
+        tmp_folder = d["tmp_folder"]
         if tmp_folder is not None:
             tmp_folder.mkdir(exist_ok=True)
-    
-        location_keys = ['x', 'y']
+
+        location_keys = ["x", "y"]
         locations = np.stack([peak_locations[k] for k in location_keys], axis=1)
-        
-        if d['use_amplitude']:
-            to_cluster_from = np.hstack((locations, peaks['amplitude'][:, np.newaxis]))
+
+        if d["use_amplitude"]:
+            to_cluster_from = np.hstack((locations, peaks["amplitude"][:, np.newaxis]))
         else:
             to_cluster_from = locations
 
-        clustering = hdbscan.hdbscan(to_cluster_from, **d['hdbscan_kwargs'])
+        clustering = hdbscan.hdbscan(to_cluster_from, **d["hdbscan_kwargs"])
         peak_labels = clustering[0]
-        
+
         labels = np.unique(peak_labels)
-        labels = labels[labels>=0]
+        labels = labels[labels >= 0]
 
-        if d['debug']:
+        if d["debug"]:
             import matplotlib.pyplot as plt
             import spikeinterface.full as si
+
             fig1, ax = plt.subplots()
-            kwargs = dict(probe_shape_kwargs=dict(facecolor='w', edgecolor='k', lw=0.5, alpha=0.3),
-                                    contacts_kargs = dict(alpha=0.5, edgecolor='k', lw=0.5, facecolor='w'))
+            kwargs = dict(
+                probe_shape_kwargs=dict(facecolor="w", edgecolor="k", lw=0.5, alpha=0.3),
+                contacts_kargs=dict(alpha=0.5, edgecolor="k", lw=0.5, facecolor="w"),
+            )
             si.plot_probe_map(recording, ax=ax, **kwargs)
-            ax.scatter(locations[:, 0], locations[:, 1], alpha=0.5, s=1, color='k')
+            ax.scatter(locations[:, 0], locations[:, 1], alpha=0.5, s=1, color="k")
 
             fig2, ax = plt.subplots()
             si.plot_probe_map(recording, ax=ax, **kwargs)
             ax.scatter(locations[:, 0], locations[:, 1], alpha=0.5, s=1, c=peak_labels)
 
             if tmp_folder is not None:
-                fig1.savefig(tmp_folder / 'peak_locations.png')
-                fig2.savefig(tmp_folder / 'peak_locations_clustered.png')
+                fig1.savefig(tmp_folder / "peak_locations.png")
+                fig2.savefig(tmp_folder / "peak_locations_clustered.png")
 
         return labels, peak_labels
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sortingcomponents/clustering/position_and_features.py` & `spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/clustering/position_and_features.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,14 +1,16 @@
 # """Sorting components: clustering"""
 from pathlib import Path
 
 import shutil
 import numpy as np
+
 try:
     import hdbscan
+
     HAVE_HDBSCAN = True
 except:
     HAVE_HDBSCAN = False
 
 import random, string, os
 from spikeinterface.core import get_global_tmp_folder, get_noise_levels, get_channel_distances
 from sklearn.preprocessing import QuantileTransformer, MaxAbsScaler
@@ -19,137 +21,167 @@
 from spikeinterface.sortingcomponents.features_from_peaks import compute_features_from_peaks
 
 
 class PositionAndFeaturesClustering:
     """
     hdbscan clustering on peak_locations previously done by localize_peaks()
     """
+
     _default_params = {
-        "peak_localization_kwargs" : {"method" : "center_of_mass"},
-        "hdbscan_kwargs": {"min_cluster_size" : 50,  "allow_single_cluster" : True, "core_dist_n_jobs" : -1, "cluster_selection_method" : "leaf"},
-        "cleaning_kwargs" : {},
-        "local_radius_um" : 100,
-        "max_spikes_per_unit" : 200,
-        "selection_method" : "random",
-        "ms_before" : 1.5,
+        "peak_localization_kwargs": {"method": "center_of_mass"},
+        "hdbscan_kwargs": {
+            "min_cluster_size": 50,
+            "allow_single_cluster": True,
+            "core_dist_n_jobs": -1,
+            "cluster_selection_method": "leaf",
+        },
+        "cleaning_kwargs": {},
+        "local_radius_um": 100,
+        "max_spikes_per_unit": 200,
+        "selection_method": "random",
+        "ms_before": 1.5,
         "ms_after": 1.5,
         "cleaning_method": "dip",
-        "job_kwargs" : {"n_jobs" : -1, "chunk_memory" : "10M", "verbose" : True, "progress_bar" : True},
+        "job_kwargs": {"n_jobs": -1, "chunk_memory": "10M", "verbose": True, "progress_bar": True},
     }
 
     @classmethod
     def main_function(cls, recording, peaks, params):
-        assert HAVE_HDBSCAN, 'twisted clustering need hdbscan to be installed'
-
+        assert HAVE_HDBSCAN, "twisted clustering need hdbscan to be installed"
 
         if "n_jobs" in params["job_kwargs"]:
             if params["job_kwargs"]["n_jobs"] == -1:
                 params["job_kwargs"]["n_jobs"] = os.cpu_count()
 
         if "core_dist_n_jobs" in params["hdbscan_kwargs"]:
             if params["hdbscan_kwargs"]["core_dist_n_jobs"] == -1:
                 params["hdbscan_kwargs"]["core_dist_n_jobs"] = os.cpu_count()
 
         d = params
 
-        peak_dtype = [('sample_ind', 'int64'), ('unit_ind', 'int64'), ('segment_ind', 'int64')]
+        peak_dtype = [("sample_index", "int64"), ("unit_index", "int64"), ("segment_index", "int64")]
 
         fs = recording.get_sampling_frequency()
-        nbefore = int(params['ms_before'] * fs / 1000.)
-        nafter = int(params['ms_after'] * fs / 1000.)
+        nbefore = int(params["ms_before"] * fs / 1000.0)
+        nafter = int(params["ms_after"] * fs / 1000.0)
         num_samples = nbefore + nafter
 
         position_method = d["peak_localization_kwargs"]["method"]
 
-        features_list = [position_method, 'ptp', 'energy']
-        features_params = {position_method : {'local_radius_um' : params['local_radius_um']},
-                           'ptp' : {'all_channels' : False, 'local_radius_um' : params['local_radius_um']},
-                           'energy': {'local_radius_um' : params['local_radius_um']}}
-
-        features_data = compute_features_from_peaks(recording, peaks, features_list, features_params, 
-            ms_before=1, ms_after=1, **params['job_kwargs'])
+        features_list = [position_method, "ptp", "energy"]
+        features_params = {
+            position_method: {"local_radius_um": params["local_radius_um"]},
+            "ptp": {"all_channels": False, "local_radius_um": params["local_radius_um"]},
+            "energy": {"local_radius_um": params["local_radius_um"]},
+        }
+
+        features_data = compute_features_from_peaks(
+            recording, peaks, features_list, features_params, ms_before=1, ms_after=1, **params["job_kwargs"]
+        )
 
         hdbscan_data = np.zeros((len(peaks), 4), dtype=np.float32)
-        hdbscan_data[:, 0] = features_data[0]['x']
-        hdbscan_data[:, 1] = features_data[0]['y']
+        hdbscan_data[:, 0] = features_data[0]["x"]
+        hdbscan_data[:, 1] = features_data[0]["y"]
         hdbscan_data[:, 2] = features_data[1]
         hdbscan_data[:, 3] = features_data[2]
 
-        preprocessing = QuantileTransformer(output_distribution='uniform')
+        preprocessing = QuantileTransformer(output_distribution="uniform")
         hdbscan_data = preprocessing.fit_transform(hdbscan_data)
 
-        import sklearn
-        clustering = hdbscan.hdbscan(hdbscan_data, **d['hdbscan_kwargs'])
-        peak_labels = clustering[0]
+        clusterer = hdbscan.HDBSCAN(**d["hdbscan_kwargs"])
+        clusterer.fit(X=hdbscan_data)
+        peak_labels = clusterer.labels_
 
         labels = np.unique(peak_labels)
-        labels = labels[labels >= 0]
+        labels = labels[labels >= 0]  #  Noisy samples are given the label -1 in hdbscan
 
         best_spikes = {}
-        nb_spikes = 0
+        num_spikes = 0
 
         all_indices = np.arange(0, peak_labels.size)
 
         max_spikes = params["max_spikes_per_unit"]
-        selection_method = params['selection_method']
+        selection_method = params["selection_method"]
+
+        import sklearn
 
         for unit_ind in labels:
             mask = peak_labels == unit_ind
-            if selection_method == 'closest_to_centroid':
+            if selection_method == "closest_to_centroid":
                 data = hdbscan_data[mask]
                 centroid = np.median(data, axis=0)
                 distances = sklearn.metrics.pairwise_distances(centroid[np.newaxis, :], data)[0]
                 best_spikes[unit_ind] = all_indices[mask][np.argsort(distances)[:max_spikes]]
-            elif selection_method == 'random':
+            elif selection_method == "random":
                 best_spikes[unit_ind] = np.random.permutation(all_indices[mask])[:max_spikes]
-            nb_spikes += best_spikes[unit_ind].size
+            num_spikes += best_spikes[unit_ind].size
 
-        spikes = np.zeros(nb_spikes, dtype=peak_dtype)
+        spikes = np.zeros(num_spikes, dtype=peak_dtype)
 
         mask = np.zeros(0, dtype=np.int32)
         for unit_ind in labels:
             mask = np.concatenate((mask, best_spikes[unit_ind]))
 
         idx = np.argsort(mask)
         mask = mask[idx]
 
-        spikes['sample_ind'] = peaks[mask]['sample_ind']
-        spikes['segment_ind'] = peaks[mask]['segment_ind']
-        spikes['unit_ind'] = peak_labels[mask]
+        spikes["sample_index"] = peaks[mask]["sample_index"]
+        spikes["segment_index"] = peaks[mask]["segment_index"]
+        spikes["unit_index"] = peak_labels[mask]
 
         cleaning_method = params["cleaning_method"]
 
-        print("We found %d raw clusters, starting to clean with %s..." %(len(labels), cleaning_method))
+        print("We found %d raw clusters, starting to clean with %s..." % (len(labels), cleaning_method))
 
         if cleaning_method == "cosine":
-
             num_chans = recording.get_num_channels()
-            wfs_arrays = extract_waveforms_to_buffers(recording, spikes, labels, nbefore, nafter,
-                         mode='shared_memory', return_scaled=False, folder=None, dtype=recording.get_dtype(),
-                         sparsity_mask=None,  copy=True,
-                         **params['job_kwargs'])
+            wfs_arrays = extract_waveforms_to_buffers(
+                recording,
+                spikes,
+                labels,
+                nbefore,
+                nafter,
+                mode="shared_memory",
+                return_scaled=False,
+                folder=None,
+                dtype=recording.get_dtype(),
+                sparsity_mask=None,
+                copy=True,
+                **params["job_kwargs"],
+            )
 
             noise_levels = get_noise_levels(recording, return_scaled=False)
-            labels, peak_labels = remove_duplicates(wfs_arrays, noise_levels, peak_labels, num_samples, num_chans, **params['cleaning_kwargs'])
+            labels, peak_labels = remove_duplicates(
+                wfs_arrays, noise_levels, peak_labels, num_samples, num_chans, **params["cleaning_kwargs"]
+            )
 
         elif cleaning_method == "dip":
-
             wfs_arrays = {}
             for label in labels:
                 mask = label == peak_labels
                 wfs_arrays[label] = hdbscan_data[mask]
 
-            labels, peak_labels = remove_duplicates_via_dip(wfs_arrays, peak_labels, **params['cleaning_kwargs'])
+            labels, peak_labels = remove_duplicates_via_dip(wfs_arrays, peak_labels, **params["cleaning_kwargs"])
 
         elif cleaning_method == "matching":
-            name = ''.join(random.choices(string.ascii_uppercase + string.digits, k=8))
+            name = "".join(random.choices(string.ascii_uppercase + string.digits, k=8))
             tmp_folder = Path(os.path.join(get_global_tmp_folder(), name))
 
-            sorting = NumpySorting.from_times_labels(spikes['sample_ind'], spikes['unit_ind'], fs)
-            we = extract_waveforms(recording, sorting, tmp_folder, overwrite=True, ms_before=params['ms_before'], 
-                ms_after=params['ms_after'], **params['job_kwargs'], return_scaled=False)
-            labels, peak_labels = remove_duplicates_via_matching(we, peak_labels, job_kwargs=params['job_kwargs'], **params['cleaning_kwargs'])
+            sorting = NumpySorting.from_times_labels(spikes["sample_index"], spikes["unit_index"], fs)
+            we = extract_waveforms(
+                recording,
+                sorting,
+                tmp_folder,
+                overwrite=True,
+                ms_before=params["ms_before"],
+                ms_after=params["ms_after"],
+                **params["job_kwargs"],
+                return_scaled=False,
+            )
+            labels, peak_labels = remove_duplicates_via_matching(
+                we, peak_labels, job_kwargs=params["job_kwargs"], **params["cleaning_kwargs"]
+            )
             shutil.rmtree(tmp_folder)
 
-        print("We kept %d non-duplicated clusters..." %len(labels))
+        print("We kept %d non-duplicated clusters..." % len(labels))
 
         return labels, peak_labels
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sortingcomponents/clustering/position_and_pca.py` & `spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/clustering/random_projections.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,196 +1,247 @@
 # """Sorting components: clustering"""
 from pathlib import Path
-import random
-import string
-import os
 
+import shutil
 import numpy as np
+
 try:
     import hdbscan
+
     HAVE_HDBSCAN = True
 except:
     HAVE_HDBSCAN = False
 
-from spikeinterface.core import get_global_tmp_folder
-from spikeinterface.core.recording_tools import get_channel_distances, get_random_data_chunks
+import random, string, os
+from spikeinterface.core import get_global_tmp_folder, get_noise_levels, get_channel_distances, get_random_data_chunks
+from sklearn.preprocessing import QuantileTransformer, MaxAbsScaler
 from spikeinterface.core.waveform_tools import extract_waveforms_to_buffers
-from .clustering_tools import auto_clean_clustering, auto_split_clustering
+from .clustering_tools import remove_duplicates, remove_duplicates_via_matching, remove_duplicates_via_dip
+from spikeinterface.core import NumpySorting
+from spikeinterface.core import extract_waveforms
+from spikeinterface.sortingcomponents.features_from_peaks import compute_features_from_peaks, EnergyFeature
 
-class PositionAndPCAClustering:
+
+class RandomProjectionClustering:
     """
-    Perform a hdbscan clustering on peak position then apply locals
-    PCA on waveform + hdbscan on every spatial clustering to check
-    if there a need to oversplit. Should be fairly close to spyking-circus
-    clustering
-    
+    hdbscan clustering on peak_locations previously done by localize_peaks()
     """
+
     _default_params = {
-        'peak_locations' : None,
-        'use_amplitude' : True,
-        'peak_localization_kwargs' : {'method' : 'center_of_mass'},
-        'ms_before': 1.5,
-        'ms_after': 2.5,
-        'n_components_by_channel' : 3,
-        'n_components': 5,
-        'job_kwargs' : {'n_jobs' : -1, 'chunk_memory' : '10M', 'verbose' : True, 'progress_bar' : True},
-        'hdbscan_global_kwargs': {'min_cluster_size' : 20, 'allow_single_cluster' : True, "core_dist_n_jobs" : -1},
-        'hdbscan_local_kwargs': {'min_cluster_size' : 20, 'allow_single_cluster' : True, "core_dist_n_jobs" : -1},
-        'waveform_mode': 'shared_memory',
-        'radius_um' : 50.,
-        'noise_size' : 300,
-        'debug' : False,
-        'tmp_folder' : None,
-        'auto_merge_num_shift': 3,
-        'auto_merge_quantile_limit': 0.8, 
-        'ratio_num_channel_intersect': 0.5,
+        "hdbscan_kwargs": {
+            "min_cluster_size": 20,
+            "allow_single_cluster": True,
+            "core_dist_n_jobs": os.cpu_count(),
+            "cluster_selection_method": "leaf",
+        },
+        "cleaning_kwargs": {},
+        "local_radius_um": 100,
+        "max_spikes_per_unit": 200,
+        "selection_method": "closest_to_centroid",
+        "nb_projections": {"ptp": 8, "energy": 2},
+        "ms_before": 1.5,
+        "ms_after": 1.5,
+        "random_seed": 42,
+        "cleaning_method": "matching",
+        "shared_memory": False,
+        "min_values": {"ptp": 0, "energy": 0},
+        "tmp_folder": None,
+        "job_kwargs": {"n_jobs": os.cpu_count(), "chunk_memory": "10M", "verbose": True, "progress_bar": True},
     }
 
     @classmethod
-    def _check_params(cls, recording, peaks, params):
+    def main_function(cls, recording, peaks, params):
+        assert HAVE_HDBSCAN, "random projections clustering need hdbscan to be installed"
+
+        if "n_jobs" in params["job_kwargs"]:
+            if params["job_kwargs"]["n_jobs"] == -1:
+                params["job_kwargs"]["n_jobs"] = os.cpu_count()
+
+        if "core_dist_n_jobs" in params["hdbscan_kwargs"]:
+            if params["hdbscan_kwargs"]["core_dist_n_jobs"] == -1:
+                params["hdbscan_kwargs"]["core_dist_n_jobs"] = os.cpu_count()
+
         d = params
-        params2 = params.copy()
-        
-        tmp_folder = params['tmp_folder']
-        if params['waveform_mode'] == 'memmap':
-            if tmp_folder is None:
-                name = ''.join(random.choices(string.ascii_uppercase + string.digits, k=8))
-                tmp_folder = Path(os.path.join(get_global_tmp_folder(), name))
-            else:
-                tmp_folder = Path(tmp_folder)
-            tmp_folder.mkdir()
-            params2['tmp_folder'] = tmp_folder
-        elif params['waveform_mode'] ==  'shared_memory':
-            assert tmp_folder is None, 'tmp_folder must be None for shared_memory'
-        else:
-            raise ValueError('shared_memory')        
-        
-        return params2
+        verbose = d["job_kwargs"]["verbose"]
 
-    @classmethod
-    def main_function(cls, recording, peaks, params):
+        peak_dtype = [("sample_index", "int64"), ("unit_index", "int64"), ("segment_index", "int64")]
 
-        #res = PositionClustering(recording, peaks, params)
+        fs = recording.get_sampling_frequency()
+        nbefore = int(params["ms_before"] * fs / 1000.0)
+        nafter = int(params["ms_after"] * fs / 1000.0)
+        num_samples = nbefore + nafter
+        num_chans = recording.get_num_channels()
 
-        assert HAVE_HDBSCAN, 'position_and_pca clustering need hdbscan to be installed'
+        noise_levels = get_noise_levels(recording, return_scaled=False)
 
-        params = cls._check_params(recording, peaks, params)
-        #wfs_arrays, sparsity_mask, noise = cls._initialize_folder(recording, peaks, params)
-        
-        # step1 : clustering on peak location
-        if params['peak_locations'] is None:
-            from spikeinterface.sortingcomponents.peak_localization import localize_peaks
-            peak_locations = localize_peaks(recording, peaks, **params['peak_localization_kwargs'], **params['job_kwargs'])
-        else:
-            peak_locations = params['peak_locations']
-        
-        location_keys = ['x', 'y']
-        locations = np.stack([peak_locations[k] for k in location_keys], axis=1)
-        
-        if params['use_amplitude']:
-            to_cluster_from = np.hstack((locations, peaks['amplitude'][:, np.newaxis]))
-        else:
-            to_cluster_from = locations
+        np.random.seed(d["random_seed"])
 
-        clustering = hdbscan.hdbscan(to_cluster_from, **params['hdbscan_global_kwargs'])
-        spatial_peak_labels = clustering[0]
-        
-        spatial_labels = np.unique(spatial_peak_labels)
-        spatial_labels = spatial_labels[spatial_labels >= 0]
-
-        # step2 : extract waveform by cluster
-        spatial_keep, = np.nonzero(spatial_peak_labels >= 0)
-
-        keep_peak_labels = spatial_peak_labels[spatial_keep]
-        
-        peak_dtype = [('sample_ind', 'int64'), ('unit_ind', 'int64'), ('segment_ind', 'int64')]
-        peaks2 = np.zeros(spatial_keep.size, dtype=peak_dtype)
-        peaks2['sample_ind'] = peaks['sample_ind'][spatial_keep]
-        peaks2['segment_ind'] = peaks['segment_ind'][spatial_keep]
+        features_params = {}
+        features_list = []
 
-        num_chans = recording.get_num_channels()
-        sparsity_mask = np.zeros((spatial_labels.size, num_chans), dtype='bool')
-        chan_locs = recording.get_channel_locations()
-        chan_distances = get_channel_distances(recording)
-        for l, label in enumerate(spatial_labels):
-            mask = keep_peak_labels == label
-            peaks2['unit_ind'][mask] = l
-
-            center = np.median(locations[spatial_keep][mask], axis=0)
-            main_chan = np.argmin(np.linalg.norm(chan_locs - center[np.newaxis, :], axis=1))
-            
-            # TODO take a radius that depend on the cluster dispertion itself
-            closest_chans, = np.nonzero(chan_distances[main_chan, :] <= params['radius_um'])
-            sparsity_mask[l, closest_chans] = True
+        noise_snippets = None
 
-        if params['waveform_mode'] == 'shared_memory':
-            wf_folder = None
+        for proj_type in ["ptp", "energy"]:
+            if d["nb_projections"][proj_type] > 0:
+                features_list += [f"random_projections_{proj_type}"]
+
+                if d["min_values"][proj_type] == "auto":
+                    if noise_snippets is None:
+                        num_segments = recording.get_num_segments()
+                        num_chunks = 3 * d["max_spikes_per_unit"] // num_segments
+                        noise_snippets = get_random_data_chunks(
+                            recording, num_chunks_per_segment=num_chunks, chunk_size=num_samples, seed=42
+                        )
+                        noise_snippets = noise_snippets.reshape(num_chunks, num_samples, num_chans)
+
+                    if proj_type == "energy":
+                        data = np.linalg.norm(noise_snippets, axis=1)
+                        min_values = np.median(data, axis=0)
+                    elif proj_type == "ptp":
+                        data = np.ptp(noise_snippets, axis=1)
+                        min_values = np.median(data, axis=0)
+                elif d["min_values"][proj_type] > 0:
+                    min_values = d["min_values"][proj_type]
+                else:
+                    min_values = None
+
+                projections = np.random.randn(num_chans, d["nb_projections"][proj_type])
+                features_params[f"random_projections_{proj_type}"] = {
+                    "local_radius_um": params["local_radius_um"],
+                    "projections": projections,
+                    "min_values": min_values,
+                }
+
+        features_data = compute_features_from_peaks(
+            recording, peaks, features_list, features_params, ms_before=1, ms_after=1, **params["job_kwargs"]
+        )
+
+        if len(features_data) > 1:
+            hdbscan_data = np.hstack((features_data[0], features_data[1]))
         else:
-            assert params['tmp_folder'] is not None
-            wf_folder = params['tmp_folder'] / 'sparse_snippets'
-            wf_folder.mkdir()
+            hdbscan_data = features_data[0]
 
-        fs = recording.get_sampling_frequency()
-        nbefore = int(params['ms_before'] * fs / 1000.)
-        nafter = int(params['ms_after'] * fs / 1000.)
+        import sklearn
 
-        ids = np.arange(num_chans, dtype='int64')
-        wfs_arrays = extract_waveforms_to_buffers(recording, peaks2, spatial_labels, nbefore, nafter,
-                                mode=params['waveform_mode'], return_scaled=False, folder=wf_folder, dtype=recording.get_dtype(),
-                                sparsity_mask=sparsity_mask,  copy=(params['waveform_mode'] == 'shared_memory'),
-                                **params['job_kwargs'])
-
-        noise = get_random_data_chunks(recording, return_scaled=False,
-                        num_chunks_per_segment=params['noise_size'], chunk_size=nbefore+nafter, concatenated=False, seed=None)
-        noise = np.stack(noise, axis=0)
-
-        print('Launching the local pca for splitting purposes')
-        split_peak_labels, main_channels = auto_split_clustering(wfs_arrays, sparsity_mask, spatial_labels, keep_peak_labels, nbefore, nafter, noise, 
-                n_components_by_channel=params['n_components_by_channel'],
-                n_components=params['n_components'],
-                hdbscan_params=params['hdbscan_local_kwargs'],
-                debug=params['debug'],
-                debug_folder=params['tmp_folder'],
-                )
-        
-        peak_labels = -2 * np.ones(peaks.size, dtype=np.int64)
-        peak_labels[spatial_keep] = split_peak_labels
-        
-        # auto clean
-        pre_clean_labels = np.unique(peak_labels)
-        pre_clean_labels = pre_clean_labels[pre_clean_labels>=0]
-        #~ print('labels before auto clean', pre_clean_labels.size, pre_clean_labels)
-
-        peaks3 = np.zeros(peaks.size, dtype=peak_dtype)
-        peaks3['sample_ind'] = peaks['sample_ind']
-        peaks3['segment_ind'] = peaks['segment_ind']
-        peaks3['unit_ind'][:] = -1
-        sparsity_mask3 = np.zeros((pre_clean_labels.size, num_chans), dtype='bool')
-        for l, label in enumerate(pre_clean_labels):
-            peaks3['unit_ind'][peak_labels == label] = l
-            main_chan = main_channels[label]
-            closest_chans, = np.nonzero(chan_distances[main_chan, :] <= params['radius_um'])
-            sparsity_mask3[l, closest_chans] = True
-        
+        clustering = hdbscan.hdbscan(hdbscan_data, **d["hdbscan_kwargs"])
+        peak_labels = clustering[0]
 
-        if params['waveform_mode'] == 'shared_memory':
-            wf_folder = None
-        else:
-            if params['tmp_folder'] is not None:
-                wf_folder = params['tmp_folder'] / 'waveforms_pre_autoclean'
-                wf_folder.mkdir()
-        
-        wfs_arrays3 = extract_waveforms_to_buffers(recording, peaks3, pre_clean_labels, nbefore, nafter,
-                                mode=params['waveform_mode'], return_scaled=False, folder=wf_folder, dtype=recording.get_dtype(),
-                                sparsity_mask=sparsity_mask3,  copy=(params['waveform_mode'] == 'shared_memory'),
-                                **params['job_kwargs'])
-        
-        clean_peak_labels, peak_sample_shifts = auto_clean_clustering(wfs_arrays3, sparsity_mask3, pre_clean_labels, peak_labels, nbefore, nafter, chan_distances,
-                                radius_um=params['radius_um'], auto_merge_num_shift=params['auto_merge_num_shift'],
-                                auto_merge_quantile_limit=params['auto_merge_quantile_limit'], ratio_num_channel_intersect=params['ratio_num_channel_intersect'])
-    
-        
-        # final
-        labels = np.unique(clean_peak_labels)
-        labels = labels[labels>=0]
-        
-        return labels, clean_peak_labels
+        labels = np.unique(peak_labels)
+        labels = labels[labels >= 0]
+
+        best_spikes = {}
+        nb_spikes = 0
+
+        all_indices = np.arange(0, peak_labels.size)
+
+        max_spikes = params["max_spikes_per_unit"]
+        selection_method = params["selection_method"]
+
+        for unit_ind in labels:
+            mask = peak_labels == unit_ind
+            if selection_method == "closest_to_centroid":
+                data = hdbscan_data[mask]
+                centroid = np.median(data, axis=0)
+                distances = sklearn.metrics.pairwise_distances(centroid[np.newaxis, :], data)[0]
+                best_spikes[unit_ind] = all_indices[mask][np.argsort(distances)[:max_spikes]]
+            elif selection_method == "random":
+                best_spikes[unit_ind] = np.random.permutation(all_indices[mask])[:max_spikes]
+            nb_spikes += best_spikes[unit_ind].size
+
+        spikes = np.zeros(nb_spikes, dtype=peak_dtype)
+
+        mask = np.zeros(0, dtype=np.int32)
+        for unit_ind in labels:
+            mask = np.concatenate((mask, best_spikes[unit_ind]))
+
+        idx = np.argsort(mask)
+        mask = mask[idx]
+
+        spikes["sample_index"] = peaks[mask]["sample_index"]
+        spikes["segment_index"] = peaks[mask]["segment_index"]
+        spikes["unit_index"] = peak_labels[mask]
+
+        cleaning_method = params["cleaning_method"]
+
+        if verbose:
+            print("We found %d raw clusters, starting to clean with %s..." % (len(labels), cleaning_method))
+
+        if cleaning_method == "cosine":
+            wfs_arrays = extract_waveforms_to_buffers(
+                recording,
+                spikes,
+                labels,
+                nbefore,
+                nafter,
+                mode="shared_memory",
+                return_scaled=False,
+                folder=None,
+                dtype=recording.get_dtype(),
+                sparsity_mask=None,
+                copy=True,
+                **params["job_kwargs"],
+            )
+
+            labels, peak_labels = remove_duplicates(
+                wfs_arrays, noise_levels, peak_labels, num_samples, num_chans, **params["cleaning_kwargs"]
+            )
+
+        elif cleaning_method == "dip":
+            wfs_arrays = {}
+            for label in labels:
+                mask = label == peak_labels
+                wfs_arrays[label] = hdbscan_data[mask]
+
+            labels, peak_labels = remove_duplicates_via_dip(wfs_arrays, peak_labels, **params["cleaning_kwargs"])
+
+        elif cleaning_method == "matching":
+            # create a tmp folder
+            if params["tmp_folder"] is None:
+                name = "".join(random.choices(string.ascii_uppercase + string.digits, k=8))
+                tmp_folder = get_global_tmp_folder() / name
+            else:
+                tmp_folder = Path(params["tmp_folder"])
+
+            if params["shared_memory"]:
+                waveform_folder = None
+                mode = "memory"
+            else:
+                waveform_folder = tmp_folder / "waveforms"
+                mode = "folder"
+
+            sorting_folder = tmp_folder / "sorting"
+            sorting = NumpySorting.from_times_labels(spikes["sample_index"], spikes["unit_index"], fs)
+            sorting = sorting.save(folder=sorting_folder)
+            we = extract_waveforms(
+                recording,
+                sorting,
+                waveform_folder,
+                ms_before=params["ms_before"],
+                ms_after=params["ms_after"],
+                **params["job_kwargs"],
+                return_scaled=False,
+                mode=mode,
+            )
+
+            cleaning_matching_params = params["job_kwargs"].copy()
+            cleaning_matching_params["chunk_duration"] = "100ms"
+            cleaning_matching_params["n_jobs"] = 1
+            cleaning_matching_params["verbose"] = False
+            cleaning_matching_params["progress_bar"] = False
+
+            cleaning_params = params["cleaning_kwargs"].copy()
+            cleaning_params["tmp_folder"] = tmp_folder
+
+            labels, peak_labels = remove_duplicates_via_matching(
+                we, noise_levels, peak_labels, job_kwargs=cleaning_matching_params, **cleaning_params
+            )
+
+            if params["tmp_folder"] is None:
+                shutil.rmtree(tmp_folder)
+            else:
+                shutil.rmtree(tmp_folder / "waveforms")
+                shutil.rmtree(tmp_folder / "sorting")
+
+        if verbose:
+            print("We kept %d non-duplicated clusters..." % len(labels))
+
+        return labels, peak_labels
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sortingcomponents/clustering/position_ptp_scaled.py` & `spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/clustering/position_ptp_scaled.py`

 * *Files 9% similar despite different names*

```diff
@@ -20,20 +20,20 @@
     """
 
     _default_params = {
         "peak_locations": None,
         "ptps": None,
         "scales": (1, 1, 10),
         "peak_localization_kwargs": {"method": "center_of_mass"},
-        'job_kwargs' : {'n_jobs' : -1, 'chunk_memory' : '10M', 'verbose' : True, 'progress_bar' : True},
+        "job_kwargs": {"n_jobs": -1, "chunk_memory": "10M", "verbose": True, "progress_bar": True},
         "hdbscan_kwargs": {
             "min_cluster_size": 20,
             "min_samples": 20,
             "allow_single_cluster": True,
-            "core_dist_n_jobs" : -1
+            "core_dist_n_jobs": -1,
         },
         "debug": False,
         "tmp_folder": None,
     }
 
     @classmethod
     def main_function(cls, recording, peaks, params):
@@ -41,57 +41,52 @@
         d = params
 
         if d["peak_locations"] is None:
             from spikeinterface.sortingcomponents.peak_localization import (
                 localize_peaks,
             )
 
-            peak_locations = localize_peaks(
-                recording, peaks, **d["peak_localization_kwargs"]
-            )
+            peak_locations = localize_peaks(recording, peaks, **d["peak_localization_kwargs"])
         else:
             peak_locations = d["peak_locations"]
 
         tmp_folder = d["tmp_folder"]
         if tmp_folder is not None:
             tmp_folder.mkdir(exist_ok=True)
 
         location_keys = ["x", "y"]
         locations = np.stack([peak_locations[k] for k in location_keys], axis=1)
 
         if d["ptps"] is None:
-            ptps, = compute_features_from_peaks(recording,
-                peaks,
-                ['ptp'],
-                feature_params={'ptp' : {'all_channels': True}},
-                **d["job_kwargs"])
+            (ptps,) = compute_features_from_peaks(
+                recording, peaks, ["ptp"], feature_params={"ptp": {"all_channels": True}}, **d["job_kwargs"]
+            )
         else:
             ptps = d["ptps"]
 
         maxptps = np.max(ptps, axis=1)
         logmaxptps = np.log(maxptps)
 
         to_cluster_from = np.hstack((locations, logmaxptps[:, np.newaxis]))
         to_cluster_from = to_cluster_from * d["scales"]
 
-        clustering = hdbscan.hdbscan(to_cluster_from, **d["hdbscan_kwargs"])
-        peak_labels = clustering[0]
+        clusterer = hdbscan.HDBSCAN(**d["hdbscan_kwargs"])
+        clusterer.fit(X=to_cluster_from)
+        peak_labels = clusterer.labels_
 
         labels = np.unique(peak_labels)
-        labels = labels[labels >= 0]
+        labels = labels[labels >= 0]  #  Noisy samples are given the label -1 in hdbscan
 
         if d["debug"]:
             import matplotlib.pyplot as plt
             import spikeinterface.full as si
 
             fig1, ax = plt.subplots()
             kwargs = dict(
-                probe_shape_kwargs=dict(
-                    facecolor="w", edgecolor="k", lw=0.5, alpha=0.3
-                ),
+                probe_shape_kwargs=dict(facecolor="w", edgecolor="k", lw=0.5, alpha=0.3),
                 contacts_kargs=dict(alpha=0.5, edgecolor="k", lw=0.5, facecolor="w"),
             )
             si.plot_probe_map(recording, ax=ax, **kwargs)
             ax.scatter(locations[:, 0], locations[:, 1], alpha=0.5, s=1, color="k")
 
             fig2, ax = plt.subplots()
             si.plot_probe_map(recording, ax=ax, **kwargs)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sortingcomponents/clustering/sliding_hdbscan.py` & `spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/clustering/sliding_hdbscan.py`

 * *Files 10% similar despite different names*

```diff
@@ -3,493 +3,534 @@
 import time
 import random
 import string
 
 import sklearn.decomposition
 
 import numpy as np
+
 try:
     import hdbscan
+
     HAVE_HDBSCAN = True
 except:
     HAVE_HDBSCAN = False
 
 
-from spikeinterface.core import (get_global_tmp_folder, get_channel_distances, 
-                                 get_random_data_chunks, extract_waveforms_to_buffers)
+from spikeinterface.core import (
+    get_global_tmp_folder,
+    get_channel_distances,
+    get_random_data_chunks,
+    extract_waveforms_to_buffers,
+)
 from .clustering_tools import auto_clean_clustering, auto_split_clustering
 
 
 class SlidingHdbscanClustering:
     """
     This is a port of the tridesclous clustering.
-    
-    This internally make many local hdbscan clustering on 
+
+    This internally make many local hdbscan clustering on
     a local radius. The dimention reduction (features) is done on the fly.
     This is done iteractively.
-    
+
     One advantage is that the high amplitude units do bias the PC after
     have been selected.
-    
+
     This method is a bit slow
     """
+
     _default_params = {
-        'waveform_mode': 'shared_memory',
-        'tmp_folder': None,
-        'ms_before': 1.5,
-        'ms_after': 2.5,
-        'noise_size' : 300,
-        'min_spike_on_channel' : 5,
-        'stop_explore_percent' : 0.05,
-        'min_cluster_size' : 10,
-        'radius_um': 50.,
-        'n_components_by_channel': 4,
-        'auto_merge_num_shift': 3,
-        'auto_merge_quantile_limit': 0.8, 
-        'ratio_num_channel_intersect': 0.5,
-        #~ 'auto_trash_misalignment_shift' : 4,
-        'job_kwargs' : {'n_jobs' : -1, 'chunk_memory' : '10M', 'verbose' : True, 'progress_bar' : True},
+        "waveform_mode": "shared_memory",
+        "tmp_folder": None,
+        "ms_before": 1.5,
+        "ms_after": 2.5,
+        "noise_size": 300,
+        "min_spike_on_channel": 5,
+        "stop_explore_percent": 0.05,
+        "min_cluster_size": 10,
+        "radius_um": 50.0,
+        "n_components_by_channel": 4,
+        "auto_merge_num_shift": 3,
+        "auto_merge_quantile_limit": 0.8,
+        "ratio_num_channel_intersect": 0.5,
+        # ~ 'auto_trash_misalignment_shift' : 4,
+        "job_kwargs": {"n_jobs": -1, "chunk_memory": "10M", "verbose": True, "progress_bar": True},
     }
 
     @classmethod
     def main_function(cls, recording, peaks, params):
-        assert HAVE_HDBSCAN, 'sliding_hdbscan clustering need hdbscan to be installed'
+        assert HAVE_HDBSCAN, "sliding_hdbscan clustering need hdbscan to be installed"
         params = cls._check_params(recording, peaks, params)
         wfs_arrays, sparsity_mask, noise = cls._initialize_folder(recording, peaks, params)
         peak_labels = cls._find_clusters(recording, peaks, wfs_arrays, sparsity_mask, noise, params)
-        
-        wfs_arrays2, sparsity_mask2 = cls._prepare_clean(recording, peaks, wfs_arrays, sparsity_mask, peak_labels, params)
-        
-        clean_peak_labels, peak_sample_shifts = cls._clean_cluster(recording, peaks, wfs_arrays2, sparsity_mask2, peak_labels, params)
-        
+
+        wfs_arrays2, sparsity_mask2 = cls._prepare_clean(
+            recording, peaks, wfs_arrays, sparsity_mask, peak_labels, params
+        )
+
+        clean_peak_labels, peak_sample_shifts = cls._clean_cluster(
+            recording, peaks, wfs_arrays2, sparsity_mask2, peak_labels, params
+        )
+
         labels = np.unique(clean_peak_labels)
         labels = labels[labels >= 0]
 
         return labels, peak_labels
 
     @classmethod
     def _check_params(cls, recording, peaks, params):
         d = params
         params2 = params.copy()
-        
-        tmp_folder = params['tmp_folder']
-        if d['waveform_mode'] ==  'memmap':
+
+        tmp_folder = params["tmp_folder"]
+        if d["waveform_mode"] == "memmap":
             if tmp_folder is None:
-                name = ''.join(random.choices(string.ascii_uppercase + string.digits, k=8))
-                tmp_folder = get_global_tmp_folder() / f'SlidingHdbscanClustering_{name}'
+                name = "".join(random.choices(string.ascii_uppercase + string.digits, k=8))
+                tmp_folder = get_global_tmp_folder() / f"SlidingHdbscanClustering_{name}"
             else:
                 tmp_folder = Path(tmp_folder)
             tmp_folder.mkdir()
-            params2['tmp_folder'] = tmp_folder
-        elif d['waveform_mode'] ==  'shared_memory':
-            assert tmp_folder is None, 'temp_folder must be None for shared_memory'
+            params2["tmp_folder"] = tmp_folder
+        elif d["waveform_mode"] == "shared_memory":
+            assert tmp_folder is None, "temp_folder must be None for shared_memory"
         else:
-            raise ValueError('shared_memory')        
-        
+            raise ValueError("shared_memory")
+
         return params2
-    
+
     @classmethod
     def _initialize_folder(cls, recording, peaks, params):
         d = params
-        tmp_folder = params['tmp_folder']
-        
+        tmp_folder = params["tmp_folder"]
+
         num_chans = recording.channel_ids.size
-        
+
         # important sparsity is 2 times radius sparsity because closest channel will be 1 time radius
         chan_distances = get_channel_distances(recording)
-        sparsity_mask = np.zeros((num_chans, num_chans), dtype='bool')
+        sparsity_mask = np.zeros((num_chans, num_chans), dtype="bool")
         for c in range(num_chans):
-            chans, = np.nonzero(chan_distances[c, :] <= ( 2 * d['radius_um']))
+            (chans,) = np.nonzero(chan_distances[c, :] <= (2 * d["radius_um"]))
             sparsity_mask[c, chans] = True
-        
+
         # create a new peak vector to extract waveforms
-        dtype = [('sample_ind', 'int64'), ('unit_ind', 'int64'), ('segment_ind', 'int64')]
+        dtype = [("sample_index", "int64"), ("unit_index", "int64"), ("segment_index", "int64")]
         peaks2 = np.zeros(peaks.size, dtype=dtype)
-        peaks2['sample_ind'] = peaks['sample_ind']
-        peaks2['unit_ind'] = peaks['channel_ind']
-        peaks2['segment_ind'] = peaks['segment_ind']
-        
+        peaks2["sample_index"] = peaks["sample_index"]
+        peaks2["unit_index"] = peaks["channel_index"]
+        peaks2["segment_index"] = peaks["segment_index"]
+
         fs = recording.get_sampling_frequency()
         dtype = recording.get_dtype()
-        
-        nbefore = int(d['ms_before'] * fs / 1000.)
-        nafter = int(d['ms_after'] * fs / 1000.)
-        
+
+        nbefore = int(d["ms_before"] * fs / 1000.0)
+        nafter = int(d["ms_after"] * fs / 1000.0)
+
         if tmp_folder is None:
             wf_folder = None
         else:
-            wf_folder = tmp_folder / 'waveforms'
+            wf_folder = tmp_folder / "waveforms"
             wf_folder.mkdir()
-            
-        ids = np.arange(num_chans, dtype='int64')
-        wfs_arrays = extract_waveforms_to_buffers(recording, peaks2, ids, nbefore, nafter,
-                                mode=d['waveform_mode'], return_scaled=False, folder=wf_folder, dtype=dtype,
-                                sparsity_mask=sparsity_mask,  copy=(d['waveform_mode'] == 'shared_memory'),
-                                **d['job_kwargs'])
+
+        ids = np.arange(num_chans, dtype="int64")
+        wfs_arrays = extract_waveforms_to_buffers(
+            recording,
+            peaks2,
+            ids,
+            nbefore,
+            nafter,
+            mode=d["waveform_mode"],
+            return_scaled=False,
+            folder=wf_folder,
+            dtype=dtype,
+            sparsity_mask=sparsity_mask,
+            copy=(d["waveform_mode"] == "shared_memory"),
+            **d["job_kwargs"],
+        )
 
         # noise
-        noise = get_random_data_chunks(recording, return_scaled=False,
-                        num_chunks_per_segment=d['noise_size'], chunk_size=nbefore+nafter, concatenated=False, seed=None)
+        noise = get_random_data_chunks(
+            recording,
+            return_scaled=False,
+            num_chunks_per_segment=d["noise_size"],
+            chunk_size=nbefore + nafter,
+            concatenated=False,
+            seed=None,
+        )
         noise = np.stack(noise, axis=0)
 
-
         return wfs_arrays, sparsity_mask, noise
-    
+
     @classmethod
     def _find_clusters(cls, recording, peaks, wfs_arrays, sparsity_mask, noise, d):
-        
         num_chans = recording.get_num_channels()
         fs = recording.get_sampling_frequency()
-        nbefore = int(d['ms_before'] * fs / 1000.)
-        nafter = int(d['ms_after'] * fs / 1000.)
-        
-        
-        
-        possible_channel_inds = np.unique(peaks['channel_ind'])
-        
+        nbefore = int(d["ms_before"] * fs / 1000.0)
+        nafter = int(d["ms_after"] * fs / 1000.0)
+
+        possible_channel_inds = np.unique(peaks["channel_index"])
+
         # channel neighborhood
         chan_distances = get_channel_distances(recording)
         closest_channels = []
         for c in range(num_chans):
-            chans, = np.nonzero(chan_distances[c, :] <= d['radius_um'])
+            (chans,) = np.nonzero(chan_distances[c, :] <= d["radius_um"])
             chans = np.intersect1d(possible_channel_inds, chans)
             closest_channels.append(chans)
-        
-        peak_labels = np.zeros(peaks.size, dtype='int64')
-        
+
+        peak_labels = np.zeros(peaks.size, dtype="int64")
+
         # create amplitudes percentile vector
         # this help to explore channel starting with high amplitudes
-        chan_amps = np.zeros(num_chans, dtype='float64')
-        remain_count = np.zeros(num_chans, dtype='int64')
-        remain_percent = np.zeros(num_chans, dtype='float64')
-        total_count = np.zeros(num_chans, dtype='int64')
+        chan_amps = np.zeros(num_chans, dtype="float64")
+        remain_count = np.zeros(num_chans, dtype="int64")
+        remain_percent = np.zeros(num_chans, dtype="float64")
+        total_count = np.zeros(num_chans, dtype="int64")
         for chan_ind in range(num_chans):
-            total_count[chan_ind] = np.sum(peaks['channel_ind'] == chan_ind)
+            total_count[chan_ind] = np.sum(peaks["channel_index"] == chan_ind)
 
         # this force compute compute at forst loop
-        prev_local_chan_inds = np.arange(num_chans, dtype='int64')
-        
+        prev_local_chan_inds = np.arange(num_chans, dtype="int64")
+
         actual_label = 1
-        
+
         while True:
             # update ampltiude percentile and count peak by channel
             for chan_ind in prev_local_chan_inds:
                 if total_count[chan_ind] == 0:
                     continue
-                #~ inds, = np.nonzero(np.in1d(peaks['channel_ind'], closest_channels[chan_ind]) & (peak_labels==0))
-                inds, = np.nonzero((peaks['channel_ind'] == chan_ind) & (peak_labels==0))
-                if inds.size <= d['min_spike_on_channel']:
-                    chan_amps[chan_ind] = 0.
+                # ~ inds, = np.nonzero(np.in1d(peaks['channel_index'], closest_channels[chan_ind]) & (peak_labels==0))
+                (inds,) = np.nonzero((peaks["channel_index"] == chan_ind) & (peak_labels == 0))
+                if inds.size <= d["min_spike_on_channel"]:
+                    chan_amps[chan_ind] = 0.0
                 else:
-                    amps = np.abs(peaks['amplitude'][inds])
+                    amps = np.abs(peaks["amplitude"][inds])
                     chan_amps[chan_ind] = np.percentile(amps, 90)
                 remain_count[chan_ind] = inds.size
                 remain_percent[chan_ind] = remain_count[chan_ind] / total_count[chan_ind]
-                if remain_percent[chan_ind] < d['stop_explore_percent']:
-                    chan_amps[chan_ind] = 0.
-            
-            
+                if remain_percent[chan_ind] < d["stop_explore_percent"]:
+                    chan_amps[chan_ind] = 0.0
+
             # get best channel
             if np.all(chan_amps == 0):
                 break
-            
+
             # try fist unexplore and high amplitude
-            # local_chan_ind = np.argmax(chan_amps)
+            # local_chan_ind = np.argmax(chan_amps)
             local_chan_ind = np.argmax(chan_amps * remain_percent)
             local_chan_inds = closest_channels[local_chan_ind]
-            
-            # take waveforms not label yet for channel in radius
-            #~ t0 = time.perf_counter()
+
+            # take waveforms not label yet for channel in radius
+            # ~ t0 = time.perf_counter()
             wfs = []
             local_peak_ind = []
             for chan_ind in local_chan_inds:
-                sel,  = np.nonzero(peaks['channel_ind'] == chan_ind)
-                inds,  = np.nonzero(peak_labels[sel] == 0)
+                (sel,) = np.nonzero(peaks["channel_index"] == chan_ind)
+                (inds,) = np.nonzero(peak_labels[sel] == 0)
                 local_peak_ind.append(sel[inds])
-                # here a unit is a channel index!!!
+                # here a unit is a channel index!!!
                 wfs_chan = wfs_arrays[chan_ind]
-                
+
                 # TODO: only for debug, remove later
                 assert wfs_chan.shape[0] == sel.size
-                
-                wf_chans, = np.nonzero(sparsity_mask[chan_ind])
+
+                (wf_chans,) = np.nonzero(sparsity_mask[chan_ind])
                 # TODO: only for debug, remove later
                 assert np.all(np.in1d(local_chan_inds, wf_chans))
-                
+
                 # none label spikes
                 wfs_chan = wfs_chan[inds, :, :]
                 # only some channels
                 wfs_chan = wfs_chan[:, :, np.in1d(wf_chans, local_chan_inds)]
                 wfs.append(wfs_chan)
 
             # put noise to enhance clusters
             wfs.append(noise[:, :, local_chan_inds])
             wfs = np.concatenate(wfs, axis=0)
             local_peak_ind = np.concatenate(local_peak_ind, axis=0)
-            #~ t1 = time.perf_counter()
-            #~ print('WFS time',  t1 - t0)
-            
+            # ~ t1 = time.perf_counter()
+            # ~ print('WFS time',  t1 - t0)
 
             # reduce dim : PCA
-            #~ t0 = time.perf_counter()
-            n = d['n_components_by_channel']
-            local_feature = np.zeros((wfs.shape[0], d['n_components_by_channel'] * len(local_chan_inds)))
-
-            #~ tsvd = sklearn.decomposition.TruncatedSVD(n_components=n)
-            #~ plot_labels = []
-            #~ for c in range(wfs.shape[2]):
-                #~ local_feature[:, c*n:(c+1)*n] = tsvd.fit_transform(wfs[:, :, c])
-            #~ pca = sklearn.decomposition.PCA(n_components=d['n_components_by_channel'], whiten=True)
-            #~ local_feature = pca.fit_transform(local_feature)
+            # ~ t0 = time.perf_counter()
+            n = d["n_components_by_channel"]
+            local_feature = np.zeros((wfs.shape[0], d["n_components_by_channel"] * len(local_chan_inds)))
+
+            # ~ tsvd = sklearn.decomposition.TruncatedSVD(n_components=n)
+            # ~ plot_labels = []
+            # ~ for c in range(wfs.shape[2]):
+            # ~ local_feature[:, c*n:(c+1)*n] = tsvd.fit_transform(wfs[:, :, c])
+            # ~ pca = sklearn.decomposition.PCA(n_components=d['n_components_by_channel'], whiten=True)
+            # ~ local_feature = pca.fit_transform(local_feature)
 
             pca = sklearn.decomposition.TruncatedSVD(n_components=n)
             for c, chan_ind in enumerate(local_chan_inds):
-                local_feature[:, c*n:(c+1)*n] = pca.fit_transform(wfs[:, :, c])
+                local_feature[:, c * n : (c + 1) * n] = pca.fit_transform(wfs[:, :, c])
 
             wfs_flat = wfs.reshape(wfs.shape[0], -1)
-            #~ t1 = time.perf_counter()
-            #~ print('PCA time',  t1 - t0)
+            # ~ t1 = time.perf_counter()
+            # ~ print('PCA time',  t1 - t0)
 
-            
             # find some clusters
-            #~ t0 = time.perf_counter()
-            clusterer = hdbscan.HDBSCAN(min_cluster_size=d['min_cluster_size'], allow_single_cluster=True, metric='l2')
+            # ~ t0 = time.perf_counter()
+            clusterer = hdbscan.HDBSCAN(min_cluster_size=d["min_cluster_size"], allow_single_cluster=True, metric="l2")
             all_labels = clusterer.fit_predict(local_feature)
-            #~ t1 = time.perf_counter()
-            #~ print('HDBSCAN time',  t1 - t0)
-            
-
-            #~ t0 = time.perf_counter()
-            local_labels = all_labels[:-noise.shape[0]]
-            noise_labels = all_labels[-noise.shape[0]:]
-            
+            # ~ t1 = time.perf_counter()
+            # ~ print('HDBSCAN time',  t1 - t0)
+
+            # ~ t0 = time.perf_counter()
+            local_labels = all_labels[: -noise.shape[0]]
+            noise_labels = all_labels[-noise.shape[0] :]
+
             local_labels_set = np.unique(local_labels)
-            
-            num_cluster = np.sum(local_labels_set>=0)
-            
+
+            num_cluster = np.sum(local_labels_set >= 0)
+
             if num_cluster > 1:
                 # take only the best cluster = best amplitude on central channel
                 # other cluster will be taken in a next loop
                 ind = local_chan_inds.tolist().index(local_chan_ind)
-                peak_values = wfs[:-noise.shape[0], nbefore, ind]
+                peak_values = wfs[: -noise.shape[0], nbefore, ind]
                 peak_values = np.abs(peak_values)
                 label_peak_values = np.zeros(local_labels_set.size)
                 for l, label in enumerate(local_labels_set):
                     if label == -1:
                         continue
                     mask = local_labels == label
                     label_peak_values[l] = np.mean(peak_values[mask])
                 best_label = local_labels_set[np.argmax(label_peak_values)]
                 final_peak_inds = local_peak_ind[local_labels == best_label]
 
                 # trash outliers from this channel (propably some collision)
-                outlier_inds, = np.nonzero((local_labels == -1) & (peaks[local_peak_ind]['channel_ind'] == local_chan_ind))
+                (outlier_inds,) = np.nonzero(
+                    (local_labels == -1) & (peaks[local_peak_ind]["channel_index"] == local_chan_ind)
+                )
                 if outlier_inds.size > 0:
                     peak_labels[local_peak_ind[outlier_inds]] = -1
             elif num_cluster == 1:
                 best_label = 0
                 final_peak_inds = local_peak_ind[local_labels >= 0]
             else:
                 best_label = None
-                final_peak_inds = np.array([], dtype='int64')
+                final_peak_inds = np.array([], dtype="int64")
                 # trash all peaks from this channel
-                to_trash_ind,  = np.nonzero(peaks[local_peak_ind]['channel_ind'] == local_chan_ind)
+                (to_trash_ind,) = np.nonzero(peaks[local_peak_ind]["channel_index"] == local_chan_ind)
                 peak_labels[local_peak_ind[to_trash_ind]] = -1
-                
+
             if best_label is not None:
-                if final_peak_inds.size >= d['min_cluster_size']:
+                if final_peak_inds.size >= d["min_cluster_size"]:
                     peak_labels[final_peak_inds] = actual_label
                 else:
                     peak_labels[final_peak_inds] = -actual_label
                 actual_label += 1
 
-            #~ t1 = time.perf_counter()
-            #~ print('label time',  t1 - t0)
-            
-            # this force recompute amplitude and count at next loop
+            # ~ t1 = time.perf_counter()
+            # ~ print('label time',  t1 - t0)
+
+            # this force recompute amplitude and count at next loop
             prev_local_chan_inds = local_chan_inds
 
-            
             # DEBUG plot
-            #~ plot_debug = True
+            # ~ plot_debug = True
             plot_debug = False
-            
+
             if plot_debug:
                 import matplotlib.pyplot as plt
                 import umap
 
                 reducer = umap.UMAP()
                 reduce_local_feature_all = reducer.fit_transform(local_feature)
-                reduce_local_feature = reduce_local_feature_all[:-noise.shape[0]]
-                reduce_local_feature_noise = reduce_local_feature_all[-noise.shape[0]:]
-                
-                wfs_no_noise = wfs[:-noise.shape[0]]
-                
+                reduce_local_feature = reduce_local_feature_all[: -noise.shape[0]]
+                reduce_local_feature_noise = reduce_local_feature_all[-noise.shape[0] :]
+
+                wfs_no_noise = wfs[: -noise.shape[0]]
+
                 fig, axs = plt.subplots(ncols=3)
-                cmap = plt.get_cmap('jet', np.unique(local_labels).size)
-                cmap = { label: cmap(l) for l, label in enumerate(local_labels_set) }
-                cmap[-1] = 'k'
+                cmap = plt.get_cmap("jet", np.unique(local_labels).size)
+                cmap = {label: cmap(l) for l, label in enumerate(local_labels_set)}
+                cmap[-1] = "k"
                 for label in local_labels_set:
                     color = cmap[label]
                     ax = axs[0]
-                    mask = (local_labels == label)
+                    mask = local_labels == label
                     ax.scatter(reduce_local_feature[mask, 0], reduce_local_feature[mask, 1], color=color)
-                    
+
                     # scatter noise
-                    mask_noise = (noise_labels == label)
+                    mask_noise = noise_labels == label
                     if np.any(mask_noise):
-                        ax.scatter(reduce_local_feature_noise[mask_noise, 0], reduce_local_feature_noise[mask_noise, 1], color=color, marker='*')
-                        
+                        ax.scatter(
+                            reduce_local_feature_noise[mask_noise, 0],
+                            reduce_local_feature_noise[mask_noise, 1],
+                            color=color,
+                            marker="*",
+                        )
+
                     ax = axs[1]
                     wfs_flat2 = wfs_no_noise[mask, :, :].swapaxes(1, 2).reshape(np.sum(mask), -1).T
                     ax.plot(wfs_flat2, color=color)
                     if label == best_label:
-                        ax.plot(np.mean(wfs_flat2, axis=1), color='m', lw=2)
+                        ax.plot(np.mean(wfs_flat2, axis=1), color="m", lw=2)
                     if num_cluster > 1:
                         if outlier_inds.size > 0:
                             wfs_flat2 = wfs_no_noise[outlier_inds, :, :].swapaxes(1, 2).reshape(outlier_inds.size, -1).T
-                            ax.plot(wfs_flat2, color='red', ls='--')
+                            ax.plot(wfs_flat2, color="red", ls="--")
                     if num_cluster > 1:
                         ax = axs[2]
                         count, bins = np.histogram(peak_values[mask], bins=35)
                         ax.plot(bins[:-1], count, color=color)
                 ax = axs[1]
                 for c in range(len(local_chan_inds)):
-                    ax.axvline(c * (nbefore + nafter) + nbefore, color='k', ls='--')
-                ax.set_title(f'n={local_peak_ind.size} labeled={final_peak_inds.size} chans={local_chan_ind} {local_chan_inds}')
+                    ax.axvline(c * (nbefore + nafter) + nbefore, color="k", ls="--")
+                ax.set_title(
+                    f"n={local_peak_ind.size} labeled={final_peak_inds.size} chans={local_chan_ind} {local_chan_inds}"
+                )
 
                 ax = axs[2]
-                sel, = np.nonzero((peaks['channel_ind'] == local_chan_ind) & (peak_labels == 0))
-                count, bins = np.histogram(np.abs(peaks['amplitude'][sel]), bins=200)
-                ax.plot(bins[:-1], count, color='k', alpha=0.5)
+                (sel,) = np.nonzero((peaks["channel_index"] == local_chan_ind) & (peak_labels == 0))
+                count, bins = np.histogram(np.abs(peaks["amplitude"][sel]), bins=200)
+                ax.plot(bins[:-1], count, color="k", alpha=0.5)
 
                 plt.show()
             # END DEBUG plot
-        
+
         peak_labels[peak_labels == 0] = -1
-        
+
         return peak_labels
 
     @classmethod
     def _prepare_clean(cls, recording, peaks, wfs_arrays, sparsity_mask, peak_labels, d):
-        
-        tmp_folder = d['tmp_folder']
+        tmp_folder = d["tmp_folder"]
         if tmp_folder is None:
             wf_folder = None
         else:
-            wf_folder = tmp_folder / 'waveforms_pre_clean'
+            wf_folder = tmp_folder / "waveforms_pre_clean"
             wf_folder.mkdir()
-        
-        
+
         num_chans = recording.get_num_channels()
         fs = recording.get_sampling_frequency()
-        nbefore = int(d['ms_before'] * fs / 1000.)
-        nafter = int(d['ms_after'] * fs / 1000.)
-        
-        possible_channel_inds = np.unique(peaks['channel_ind'])
+        nbefore = int(d["ms_before"] * fs / 1000.0)
+        nafter = int(d["ms_after"] * fs / 1000.0)
+
+        possible_channel_inds = np.unique(peaks["channel_index"])
         chan_distances = get_channel_distances(recording)
         closest_channels = []
         for c in range(num_chans):
-            chans, = np.nonzero(chan_distances[c, :] <= ( d['radius_um']) * 2)
+            (chans,) = np.nonzero(chan_distances[c, :] <= (d["radius_um"]) * 2)
             closest_channels.append(chans)
 
         labels = np.unique(peak_labels)
         labels = labels[labels >= 0]
-        
+
         # loop over label take wafevorm from channel and get main channel
         main_channels = []
         for l, label in enumerate(labels):
-            wfs, chan_inds = _collect_sparse_waveforms(peaks, wfs_arrays, closest_channels, peak_labels, sparsity_mask, label)
+            wfs, chan_inds = _collect_sparse_waveforms(
+                peaks, wfs_arrays, closest_channels, peak_labels, sparsity_mask, label
+            )
             template = np.mean(wfs, axis=0)
             main_chan = chan_inds[np.argmax(np.max(np.abs(template), axis=0))]
             main_channels.append(main_chan)
-        
+
         # extact again waveforms based on new sparsity mask depending on main_chan
         dtype = recording.get_dtype()
-        #~ return_scaled = False
-        peak_dtype = [('sample_ind', 'int64'), ('unit_ind', 'int64'), ('segment_ind', 'int64')]
-        keep = peak_labels>=0
+        # ~ return_scaled = False
+        peak_dtype = [("sample_index", "int64"), ("unit_index", "int64"), ("segment_index", "int64")]
+        keep = peak_labels >= 0
         num_keep = np.sum(keep)
         keep_peak_labels = peak_labels[keep]
         peaks2 = np.zeros(num_keep, dtype=peak_dtype)
-        peaks2['sample_ind'] = peaks['sample_ind'][keep]
-        peaks2['segment_ind'] = peaks['segment_ind'][keep]
-        sparsity_mask2 = np.zeros((labels.shape[0], num_chans), dtype='bool')
+        peaks2["sample_index"] = peaks["sample_index"][keep]
+        peaks2["segment_index"] = peaks["segment_index"][keep]
+        sparsity_mask2 = np.zeros((labels.shape[0], num_chans), dtype="bool")
         for l, label in enumerate(labels):
             main_chan = main_channels[l]
             mask = keep_peak_labels == label
-            peaks2['unit_ind'][mask] = l
+            peaks2["unit_index"][mask] = l
             # here we take a twice radius
-            closest_chans, = np.nonzero(chan_distances[main_chan, :] <= d['radius_um'] * 2)
+            (closest_chans,) = np.nonzero(chan_distances[main_chan, :] <= d["radius_um"] * 2)
             sparsity_mask2[l, closest_chans] = True
 
-        wfs_arrays2 = extract_waveforms_to_buffers(recording, peaks2, labels, nbefore, nafter,
-                                mode=d['waveform_mode'], return_scaled=False, folder=wf_folder, dtype=recording.get_dtype(),
-                                sparsity_mask=sparsity_mask2,  copy=(d['waveform_mode'] == 'shared_memory'),
-                                **d['job_kwargs'])
-        
+        wfs_arrays2 = extract_waveforms_to_buffers(
+            recording,
+            peaks2,
+            labels,
+            nbefore,
+            nafter,
+            mode=d["waveform_mode"],
+            return_scaled=False,
+            folder=wf_folder,
+            dtype=recording.get_dtype(),
+            sparsity_mask=sparsity_mask2,
+            copy=(d["waveform_mode"] == "shared_memory"),
+            **d["job_kwargs"],
+        )
+
         return wfs_arrays2, sparsity_mask2
-    
+
     @classmethod
     def _clean_cluster(cls, recording, peaks, wfs_arrays2, sparsity_mask2, peak_labels, d):
-
         fs = recording.get_sampling_frequency()
-        nbefore = int(d['ms_before'] * fs / 1000.)
-        nafter = int(d['ms_after'] * fs / 1000.)
+        nbefore = int(d["ms_before"] * fs / 1000.0)
+        nafter = int(d["ms_after"] * fs / 1000.0)
 
         labels = np.unique(peak_labels)
         labels = labels[labels >= 0]
 
         chan_locs = recording.get_channel_locations()
         channel_distances = get_channel_distances(recording)
-        
-        clean_peak_labels, peak_sample_shifts = auto_clean_clustering(wfs_arrays2, sparsity_mask2, labels, peak_labels, nbefore, nafter, channel_distances,
-                                radius_um=d['radius_um'], auto_merge_num_shift=d['auto_merge_num_shift'],
-                                auto_merge_quantile_limit=d['auto_merge_quantile_limit'], ratio_num_channel_intersect=d['ratio_num_channel_intersect'])
-        
-        return  clean_peak_labels, peak_sample_shifts
-        
 
+        clean_peak_labels, peak_sample_shifts = auto_clean_clustering(
+            wfs_arrays2,
+            sparsity_mask2,
+            labels,
+            peak_labels,
+            nbefore,
+            nafter,
+            channel_distances,
+            radius_um=d["radius_um"],
+            auto_merge_num_shift=d["auto_merge_num_shift"],
+            auto_merge_quantile_limit=d["auto_merge_quantile_limit"],
+            ratio_num_channel_intersect=d["ratio_num_channel_intersect"],
+        )
+
+        return clean_peak_labels, peak_sample_shifts
 
 
 def _collect_sparse_waveforms(peaks, wfs_arrays, closest_channels, peak_labels, sparsity_mask, label):
-    inds, = np.nonzero(peak_labels == label)
+    (inds,) = np.nonzero(peak_labels == label)
     local_peaks = peaks[inds]
-    label_chan_inds, count = np.unique(local_peaks['channel_ind'], return_counts=True)
+    label_chan_inds, count = np.unique(local_peaks["channel_index"], return_counts=True)
     main_chan = label_chan_inds[np.argmax(count)]
 
-    #only main channel sparsity
+    # only main channel sparsity
     wanted_chans = closest_channels[main_chan]
     for chan_ind in label_chan_inds:
         # remove channel non in common
         wanted_chans = np.intersect1d(wanted_chans, closest_channels[chan_ind])
     # print('wanted_chans', wanted_chans)
 
-
     wfs = []
     for chan_ind in label_chan_inds:
-        sel,  = np.nonzero(peaks['channel_ind'] == chan_ind)
+        (sel,) = np.nonzero(peaks["channel_index"] == chan_ind)
 
-        inds,  = np.nonzero(peak_labels[sel] == label)
-        
+        (inds,) = np.nonzero(peak_labels[sel] == label)
 
-        wf_chans, = np.nonzero(sparsity_mask[chan_ind])
+        (wf_chans,) = np.nonzero(sparsity_mask[chan_ind])
         # print('wf_chans', wf_chans)
         # TODO: only for debug, remove later
         assert np.all(np.in1d(wanted_chans, wf_chans))
         wfs_chan = wfs_arrays[chan_ind]
 
         # TODO: only for debug, remove later
         assert wfs_chan.shape[0] == sel.size
 
-        
         wfs_chan = wfs_chan[inds, :, :]
         # only some channels
         wfs_chan = wfs_chan[:, :, np.in1d(wf_chans, wanted_chans)]
         wfs.append(wfs_chan)
-    
+
     wfs = np.concatenate(wfs, axis=0)
-    
+
     # TODO DEBUG and check
     assert wanted_chans.shape[0] == wfs.shape[2]
-    
+
     return wfs, wanted_chans
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sortingcomponents/clustering/sliding_nn.py` & `spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/clustering/sliding_nn.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,174 +1,186 @@
-
 try:
     import numba
+
     HAVE_NUMBA = True
 except ImportError:
     HAVE_NUMBA = False
 
 import numpy as np
 from sklearn.utils import check_random_state
 
 try:
     from pynndescent import NNDescent
+
     HAVE_NNDESCENT = True
 except ImportError:
     HAVE_NNDESCENT = False
 
 from spikeinterface.core import get_channel_distances
 from tqdm.auto import tqdm
+
 try:
     import hdbscan
+
     HAVE_HDBSCAN = True
 except:
     HAVE_HDBSCAN = False
 import copy
 from scipy.sparse import coo_matrix
 
 try:
     import pymde
+
     HAVE_PYMDE = True
 except ImportError:
     HAVE_PYMDE = False
 
 try:
     import torch
+
     HAVE_TORCH = True
 except ImportError:
     HAVE_TORCH = False
 
 import datetime
 
 from spikeinterface.core.waveform_tools import extract_waveforms_to_buffers
 
 
 class SlidingNNClustering:
 
     """_summary_
 
-        TODO:
-            - 2D and higher d 
+    TODO:
+        - 2D and higher d
 
-        Args:
-            recording (_type_): _description_
-            peaks (_type_): _description_
-            time_window_s (_type_, optional): window for sampling nearest neighbors. Defaults to 60*5.
-            margin_ms (int, optional): margin for chunking. Defaults to 100.
-            ms_before (int, optional): time prior to peak. Defaults to 1.
-            ms_after (int, optional): time after peak. Defaults to 1.
-            n_channel_neighbors (int, optional): number of neighbors per channel. Defaults to 8.
-            n_neighbors (int, optional): number of neighbors for graph construction. Defaults to 5.
-            embedding_dim (int, optional): Number of embedding dimensions. Defaults to number of channels in recording.
-            knn_verbose (bool, optional):  whether to make knn computation verbose. Defaults to True.
-            low_memory (bool, optional): memory usage for nearest neighbor computation. Defaults to False.
-            n_jobs (int, optional): number of jobs to perform computations over. Defaults to -1.
-            suppress_tqdm (bool, optional): Whether to display tqdm progress bar. Defaults to False.
-        Returns:
-            nn_idx (array, # spikes x # 2*n_neighbors): Graph of nearest neighbor indices
-            nn_dist (array, # spikes x # 2*n_neighbors): Distances between nearest neighbor points
+    Args:
+        recording (_type_): _description_
+        peaks (_type_): _description_
+        time_window_s (_type_, optional): window for sampling nearest neighbors. Defaults to 60*5.
+        margin_ms (int, optional): margin for chunking. Defaults to 100.
+        ms_before (int, optional): time prior to peak. Defaults to 1.
+        ms_after (int, optional): time after peak. Defaults to 1.
+        n_channel_neighbors (int, optional): number of neighbors per channel. Defaults to 8.
+        n_neighbors (int, optional): number of neighbors for graph construction. Defaults to 5.
+        embedding_dim (int, optional): Number of embedding dimensions. Defaults to number of channels in recording.
+        knn_verbose (bool, optional):  whether to make knn computation verbose. Defaults to True.
+        low_memory (bool, optional): memory usage for nearest neighbor computation. Defaults to False.
+        n_jobs (int, optional): number of jobs to perform computations over. Defaults to -1.
+        suppress_tqdm (bool, optional): Whether to display tqdm progress bar. Defaults to False.
+    Returns:
+        nn_idx (array, # spikes x # 2*n_neighbors): Graph of nearest neighbor indices
+        nn_dist (array, # spikes x # 2*n_neighbors): Distances between nearest neighbor points
 
-        """
+    """
 
     _default_params = {
-        "time_window_s" : 5,
-        "hdbscan_kwargs": {"min_cluster_size" : 20,  "allow_single_cluster" : True},
-        "margin_ms" : 100,
-        "ms_before" : 1,
-        "ms_after" : 1,
-        "n_channel_neighbors" : 8,
-        "n_neighbors" : 5,
-        "embedding_dim" : None,
-        "low_memory" : True,
-        'waveform_mode' : 'shared_memory',
-        "mde_negative_to_positive_samples" : 5,
-        "mde_device" : "cpu",
-        "create_embedding" : True,
-        "cluster_embedding" : True,
-        "debug" : False,
-        "tmp_folder" : None,
-        "verbose" : False,
-        "tmp_folder" : None,
-        'job_kwargs' : {'n_jobs' : -1}
-    } 
+        "time_window_s": 5,
+        "hdbscan_kwargs": {"min_cluster_size": 20, "allow_single_cluster": True},
+        "margin_ms": 100,
+        "ms_before": 1,
+        "ms_after": 1,
+        "n_channel_neighbors": 8,
+        "n_neighbors": 5,
+        "embedding_dim": None,
+        "low_memory": True,
+        "waveform_mode": "shared_memory",
+        "mde_negative_to_positive_samples": 5,
+        "mde_device": "cpu",
+        "create_embedding": True,
+        "cluster_embedding": True,
+        "debug": False,
+        "tmp_folder": None,
+        "verbose": False,
+        "tmp_folder": None,
+        "job_kwargs": {"n_jobs": -1},
+    }
 
     @classmethod
     def _initialize_folder(cls, recording, peaks, params):
-
         assert HAVE_NUMBA, "SlidingNN needs numba to work"
         assert HAVE_TORCH, "SlidingNN needs torch to work"
         assert HAVE_NNDESCENT, "SlidingNN needs pynndescent to work"
         assert HAVE_PYMDE, "SlidingNN needs pymde to work"
         assert HAVE_HDBSCAN, "SlidingNN needs hdbscan to work"
 
         d = params
-        tmp_folder = params['tmp_folder']
-        
+        tmp_folder = params["tmp_folder"]
+
         num_chans = recording.channel_ids.size
-        
+
         # important sparsity is 2 times radius sparsity because closest channel will be 1 time radius
         chan_distances = get_channel_distances(recording)
-        sparsity_mask = np.zeros((num_chans, num_chans), dtype='bool')
+        sparsity_mask = np.zeros((num_chans, num_chans), dtype="bool")
         for c in range(num_chans):
-            chans, = np.nonzero(chan_distances[c, :] <= ( 2 * d['radius_um']))
+            (chans,) = np.nonzero(chan_distances[c, :] <= (2 * d["radius_um"]))
             sparsity_mask[c, chans] = True
-        
+
         # create a new peak vector to extract waveforms
-        dtype = [('sample_ind', 'int64'), ('unit_ind', 'int64'), ('segment_ind', 'int64')]
+        dtype = [("sample_index", "int64"), ("unit_index", "int64"), ("segment_index", "int64")]
         peaks2 = np.zeros(peaks.size, dtype=dtype)
-        peaks2['sample_ind'] = peaks['sample_ind']
-        peaks2['unit_ind'] = peaks['channel_ind']
-        peaks2['segment_ind'] = peaks['segment_ind']
-        
+        peaks2["sample_index"] = peaks["sample_index"]
+        peaks2["unit_index"] = peaks["channel_index"]
+        peaks2["segment_index"] = peaks["segment_index"]
+
         fs = recording.get_sampling_frequency()
         dtype = recording.get_dtype()
-        
-        nbefore = int(d['ms_before'] * fs / 1000.)
-        nafter = int(d['ms_after'] * fs / 1000.)
-        
+
+        nbefore = int(d["ms_before"] * fs / 1000.0)
+        nafter = int(d["ms_after"] * fs / 1000.0)
+
         if tmp_folder is None:
             wf_folder = None
         else:
-            wf_folder = tmp_folder / 'waveforms_pre_cluster'
+            wf_folder = tmp_folder / "waveforms_pre_cluster"
             wf_folder.mkdir()
-            
-        ids = np.arange(num_chans, dtype='int64')
-        wfs_arrays = extract_waveforms_to_buffers(recording, peaks2, ids, nbefore, nafter,
-                                mode=d['waveform_mode'], return_scaled=False, folder=wf_folder, dtype=dtype,
-                                sparsity_mask=sparsity_mask, copy=(d['waveform_mode'] == 'shared_memory'),
-                                **d['job_kwargs'])
+
+        ids = np.arange(num_chans, dtype="int64")
+        wfs_arrays = extract_waveforms_to_buffers(
+            recording,
+            peaks2,
+            ids,
+            nbefore,
+            nafter,
+            mode=d["waveform_mode"],
+            return_scaled=False,
+            folder=wf_folder,
+            dtype=dtype,
+            sparsity_mask=sparsity_mask,
+            copy=(d["waveform_mode"] == "shared_memory"),
+            **d["job_kwargs"],
+        )
 
         return wfs_arrays, sparsity_mask
 
     @classmethod
     def main_function(cls, recording, peaks, params):
+        d = params
 
-        d = params        
-
-        #wfs_arrays, sparsity_mask, noise = cls._initialize_folder(recording, peaks, params)
+        # wfs_arrays, sparsity_mask, noise = cls._initialize_folder(recording, peaks, params)
 
         # prepare neighborhood parameters
         fs = recording.get_sampling_frequency()
         n_frames = recording.get_num_frames()
         duration = n_frames / fs
-        time_window_frames = fs * d['time_window_s']
-        margin_frames = int(d['margin_ms'] / 1000 * fs)
-        spike_pre_frames = int(d['ms_before'] / 1000 * fs)
-        spike_post_frames = int(d['ms_after'] / 1000 * fs)
+        time_window_frames = fs * d["time_window_s"]
+        margin_frames = int(d["margin_ms"] / 1000 * fs)
+        spike_pre_frames = int(d["ms_before"] / 1000 * fs)
+        spike_post_frames = int(d["ms_after"] / 1000 * fs)
         n_channels = recording.get_num_channels()
         n_samples = spike_pre_frames + spike_post_frames
 
-        if d['embedding_dim'] is None:
-            d['embedding_dim'] = recording.get_num_channels()
+        if d["embedding_dim"] is None:
+            d["embedding_dim"] = recording.get_num_channels()
 
         # get channel distances from one another
         channel_distance = get_channel_distances(recording)
 
         # get nearest neighbors of channels
-        channel_neighbors = np.argsort(channel_distance, axis=1)[
-            :, :d['n_channel_neighbors']]
+        channel_neighbors = np.argsort(channel_distance, axis=1)[:, : d["n_channel_neighbors"]]
 
         # divide the recording into chunks of time_window_s seconds
         n_chunks = int(np.ceil(n_frames / time_window_frames))
         chunk_start_spike_idxs = np.zeros(n_chunks)
         chunk_end_spike_idxs = np.zeros(n_chunks)
 
         # prepare an array of nn indices and distances of shape (n_spikes, 2, n_neighbors)
@@ -179,165 +191,153 @@
         # |A|B| <- W1 (window 1)
         # .  |A|B| <- W2
         # .    |A|B| <- W3
         # in this array, neighbors are given as:
         # [XXXX][W1_B][W2_B][W3_B]
         # [W1_A][W2_A][W3_A][XXXX]
         n_spikes = len(peaks)
-        nn_index_array = np.zeros((n_spikes, 2, d['n_neighbors']), dtype=int) - 1
-        nn_distance_array = np.zeros((n_spikes, 2, d['n_neighbors']), dtype=float)
+        nn_index_array = np.zeros((n_spikes, 2, d["n_neighbors"]), dtype=int) - 1
+        nn_distance_array = np.zeros((n_spikes, 2, d["n_neighbors"]), dtype=float)
 
         # initialize empty array of embeddings
-        if d['create_embedding']:
-            embeddings_all = np.zeros((n_spikes, d['embedding_dim']))
-            if d['cluster_embedding']:
+        if d["create_embedding"]:
+            embeddings_all = np.zeros((n_spikes, d["embedding_dim"]))
+            if d["cluster_embedding"]:
                 # create an empty array of clusters and probabilities (from overlapping
                 # chunks)
                 clusters = np.zeros((len(peaks), 2), dtype=int) - 1
                 cluster_probabilities = np.zeros((len(peaks), 2), dtype=float)
 
         # for each chunk grab spike nearest neighbors
         # TODO: this can be parallelized (although nearest neighbors is already
         #    parallelized, and bandwidth is limited for reading from raw data)
         end_last = -1
         explore = range(n_chunks - 1)
-        if d['verbose']:
+        if d["verbose"]:
             explore = tqdm(explore, desc="chunk")
 
         for chunk in explore:
             # set the start and end frame to grab for this chunk
             start_frame = int(chunk * time_window_frames)
             end_frame = int((chunk + 2) * time_window_frames)
             if end_frame > n_frames:
                 end_frame = n_frames
 
-            if d['verbose']:
+            if d["verbose"]:
                 print("Extracting waveforms: {}".format(datetime.datetime.now()))
             # grab all spikes
             all_spikes, all_chan_idx, peaks_in_chunk_idx = get_chunk_spike_waveforms(
                 recording,
                 start_frame,
                 end_frame,
                 peaks,
                 channel_neighbors,
                 spike_pre_frames=spike_pre_frames,
                 spike_post_frames=spike_post_frames,
-                n_channel_neighbors=d['n_channel_neighbors'],
+                n_channel_neighbors=d["n_channel_neighbors"],
                 margin_frames=margin_frames,
             )
             chunk_start_spike_idxs[chunk] = peaks_in_chunk_idx[0]
             chunk_end_spike_idxs[chunk] = peaks_in_chunk_idx[-1]
             idx_next = peaks_in_chunk_idx > end_last
             idx_cur = peaks_in_chunk_idx <= end_last
 
-            if d['verbose']:
+            if d["verbose"]:
                 print("Computing nearest neighbors: {}".format(datetime.datetime.now()))
             # grab nearest neighbors
             knn_indices, knn_distances = get_spike_nearest_neighbors(
                 all_spikes,
                 all_chan_idx=all_chan_idx,
                 n_samples=spike_post_frames + spike_pre_frames,
-                n_neighbors=d['n_neighbors'],
-                n_channel_neighbors=d['n_channel_neighbors'],
-                low_memory=d['low_memory'],
-                knn_verbose=d['verbose'],
-                n_jobs=d['job_kwargs']['n_jobs'],
+                n_neighbors=d["n_neighbors"],
+                n_channel_neighbors=d["n_channel_neighbors"],
+                low_memory=d["low_memory"],
+                knn_verbose=d["verbose"],
+                n_jobs=d["job_kwargs"]["n_jobs"],
             )
             # remove the first nearest neighbor (which should be self)
             knn_distances = knn_distances[:, 1:]
             knn_indices = knn_indices[:, 1:]
             # get the absolute index (spike number)
             knn_indices_abs = peaks_in_chunk_idx[knn_indices]
 
             # put new neighbors in first row
-            nn_index_array[
-                peaks_in_chunk_idx[idx_next], 0
-            ] = knn_indices_abs[idx_next]
+            nn_index_array[peaks_in_chunk_idx[idx_next], 0] = knn_indices_abs[idx_next]
             # put overlapping neighbors from previous in second row
-            nn_index_array[
-                peaks_in_chunk_idx[idx_cur], 1
-            ] = knn_indices_abs[idx_cur]
+            nn_index_array[peaks_in_chunk_idx[idx_cur], 1] = knn_indices_abs[idx_cur]
 
             # repeat for distances
-            nn_distance_array[
-                peaks_in_chunk_idx[idx_next], 0
-            ] = knn_distances[idx_next]
-            nn_distance_array[
-                peaks_in_chunk_idx[idx_cur], 1
-            ] = knn_distances[idx_cur]
+            nn_distance_array[peaks_in_chunk_idx[idx_next], 0] = knn_distances[idx_next]
+            nn_distance_array[peaks_in_chunk_idx[idx_cur], 1] = knn_distances[idx_cur]
             # double up on neighbors in the beginning, since we don't have an overlap
             if chunk == 0:
                 nn_index_array[peaks_in_chunk_idx, 1] = knn_indices
                 nn_distance_array[peaks_in_chunk_idx, 1] = knn_distances
             # double up neighbors in the end, since we only sample these once
-            if chunk == n_chunks-1:
-                nn_index_array[
-                    peaks_in_chunk_idx[idx_next], 1
-                ] = knn_indices[idx_next]
+            if chunk == n_chunks - 1:
+                nn_index_array[peaks_in_chunk_idx[idx_next], 1] = knn_indices[idx_next]
                 # repeat for distances
-                nn_distance_array[
-                    peaks_in_chunk_idx[idx_next], 1
-                ] = knn_distances[idx_next]
+                nn_distance_array[peaks_in_chunk_idx[idx_next], 1] = knn_distances[idx_next]
 
             # create embedding
-            if d['create_embedding']:
-                if d['verbose']:
+            if d["create_embedding"]:
+                if d["verbose"]:
                     print("Computing MDE embeddings: {}".format(datetime.datetime.now()))
 
-                chunk_csr = construct_symmetric_graph_from_idx_vals(
-                    knn_indices, knn_distances
-                )
+                chunk_csr = construct_symmetric_graph_from_idx_vals(knn_indices, knn_distances)
                 # number of current embeddings
                 n_cur = np.sum(idx_cur)
 
                 # if this is the first chunk, embed a new graph
                 # otherwise, embed a graph with fixed embedding locations
                 # from the previous embedding. This encourages stationary
                 # embeddings over time and provides additional info from
                 # the graph of the previous chunk into this chunk (through
                 # embedding points.
                 if chunk == 0:
-                    embeddings_chunk = embed_graph(chunk_csr, prev_embeddings=None, prev_idx=None,
-                                                   mde_device=d['mde_device'],
-                                                   embedding_dim=d['embedding_dim'],
-                                                   negative_to_positive_samples=d['mde_negative_to_positive_samples']
-                                                   )
+                    embeddings_chunk = embed_graph(
+                        chunk_csr,
+                        prev_embeddings=None,
+                        prev_idx=None,
+                        mde_device=d["mde_device"],
+                        embedding_dim=d["embedding_dim"],
+                        negative_to_positive_samples=d["mde_negative_to_positive_samples"],
+                    )
                 else:
                     embeddings_chunk = embed_graph(
                         chunk_csr,
                         prev_embeddings=embeddings_all[peaks_in_chunk_idx[idx_cur]],
                         prev_idx=np.arange(n_cur),
-                        mde_device=d['mde_device'],
-                        embedding_dim=d['embedding_dim'],
-                        negative_to_positive_samples=d['mde_negative_to_positive_samples']
-
+                        mde_device=d["mde_device"],
+                        embedding_dim=d["embedding_dim"],
+                        negative_to_positive_samples=d["mde_negative_to_positive_samples"],
                     )
-                embeddings_all[peaks_in_chunk_idx[idx_next]
-                               ] = embeddings_chunk[n_cur:]
+                embeddings_all[peaks_in_chunk_idx[idx_next]] = embeddings_chunk[n_cur:]
 
                 # cluster embedding
-                if d['cluster_embedding']:
-                    print("Clustering MDE embeddings (n={}): {}".format(
-                        embeddings_chunk.shape, datetime.datetime.now()))
+                if d["cluster_embedding"]:
+                    print(
+                        "Clustering MDE embeddings (n={}): {}".format(embeddings_chunk.shape, datetime.datetime.now())
+                    )
                     # TODO HDBSCAN can be done on GPU with NVIDIA RAPIDS for speed
                     clusterer = hdbscan.HDBSCAN(
                         prediction_data=True,
-                        core_dist_n_jobs=d['job_kwargs']['n_jobs'],
-                        **d['hdbscan_kwargs'],
+                        core_dist_n_jobs=d["job_kwargs"]["n_jobs"],
+                        **d["hdbscan_kwargs"],
                     ).fit(embeddings_chunk)
 
                     # set cluster labels and probabilities for this chunk
                     # put new clusters in first row
-                    clusters[
-                        peaks_in_chunk_idx[peaks_in_chunk_idx > end_last], 0
-                    ] = clusterer.labels_[peaks_in_chunk_idx > end_last]
+                    clusters[peaks_in_chunk_idx[peaks_in_chunk_idx > end_last], 0] = clusterer.labels_[
+                        peaks_in_chunk_idx > end_last
+                    ]
                     # put overlapping neighbors from previous in second row
-                    clusters[
-                        peaks_in_chunk_idx[peaks_in_chunk_idx <= end_last], 1
-                    ] = clusterer.labels_[peaks_in_chunk_idx <= end_last]
+                    clusters[peaks_in_chunk_idx[peaks_in_chunk_idx <= end_last], 1] = clusterer.labels_[
+                        peaks_in_chunk_idx <= end_last
+                    ]
                     # repeat for cluster probabilities
                     cluster_probabilities[
                         peaks_in_chunk_idx[peaks_in_chunk_idx > end_last], 0
                     ] = clusterer.probabilities_[peaks_in_chunk_idx > end_last]
                     # put overlapping neighbors from previous in second row
                     cluster_probabilities[
                         peaks_in_chunk_idx[peaks_in_chunk_idx <= end_last], 1
@@ -360,15 +360,17 @@
         #     cluster_probabilities,
         #     chunk_start_spike_idxs,
         #     chunk_end_spike_idxs
         # )
 
         return labels, peak_labels
 
+
 if HAVE_NUMBA:
+
     @numba.jit(fastmath=True, cache=True)
     def sparse_euclidean(x, y, n_samples, n_dense):
         """Euclidean distance metric over sparse vectors, where first n_dense
         elements are indices, and n_samples is the length of the second dimension
         """
         # break out sparse into columns and data
         x_best = x[:n_dense]  # dense indices
@@ -382,16 +384,15 @@
             calc = False
             yi = 0
             for yb in y_best:
                 if xb == yb:
                     calc = True
                     # calculate euclidean
                     for i in range(n_samples):
-                        result += (x[xi * n_samples + i] -
-                                y[yi * n_samples + i]) ** 2
+                        result += (x[xi * n_samples + i] - y[yi * n_samples + i]) ** 2
 
                 yi += 1
             if calc == False:
                 # add x squared
                 for i in range(n_samples):
                     result += x[xi * n_samples + i] ** 2
             xi += 1
@@ -407,17 +408,15 @@
                     result += y[yi * n_samples + i] ** 2
 
             yi += 1
         return np.sqrt(result)
 
 
 # HACK: this function only exists because I couldn't get the spikeinterface one to work...
-def retrieve_padded_trace(
-    recording, start_frame, end_frame, margin_frames, channel_ids=None
-):
+def retrieve_padded_trace(recording, start_frame, end_frame, margin_frames, channel_ids=None):
     """Grabs a chunk of recording trace, with padding
     NOTE: I tried using the built in spikeinterface function for this but
     recieved an error.
 
     Args:
         recording (_type_): _description_
         start_frame (_type_): _description_
@@ -429,24 +428,20 @@
         _type_: _description_
     """
     n_frames = recording.get_num_frames()
     # get the padding
     _pre = np.max([0, start_frame - margin_frames])
     _post = np.min([n_frames, end_frame + margin_frames])
 
-    traces = recording.get_traces(
-        start_frame=_pre, end_frame=_post, channel_ids=channel_ids
-    )
+    traces = recording.get_traces(start_frame=_pre, end_frame=_post, channel_ids=channel_ids)
     # append zeros if this chunk exists near the border
     if _pre < margin_frames:
-        traces = np.vstack(
-            [np.zeros((margin_frames - _pre, traces.shape[1])), traces])
+        traces = np.vstack([np.zeros((margin_frames - _pre, traces.shape[1])), traces])
     if _post < margin_frames:
-        traces = np.vstack(
-            [traces, np.zeros((margin_frames - _post, traces.shape[1]))])
+        traces = np.vstack([traces, np.zeros((margin_frames - _post, traces.shape[1]))])
     return traces
 
 
 def get_chunk_spike_waveforms(
     recording,
     start_frame,
     end_frame,
@@ -471,22 +466,18 @@
 
     Returns:
         all_spikes: spike waveforms
         all_chan_idx: channel indices of nearest neighbors
         peaks_in_chunk_idx: index of spikes in this chunk
     """
     # grab the trace
-    traces = retrieve_padded_trace(
-        recording, start_frame, end_frame, margin_frames, channel_ids=None
-    )
+    traces = retrieve_padded_trace(recording, start_frame, end_frame, margin_frames, channel_ids=None)
 
     # find the peaks that exist in this sample
-    peaks_in_chunk_mask = (peaks["sample_ind"] >= start_frame) & (
-        peaks["sample_ind"] <= end_frame
-    )
+    peaks_in_chunk_mask = (peaks["sample_index"] >= start_frame) & (peaks["sample_index"] <= end_frame)
 
     # get the peaks in this chunk
     peaks_chunk = peaks[peaks_in_chunk_mask]
     # get the index of which peaks are in this chunk
     peaks_in_chunk_idx = np.where(peaks_in_chunk_mask)[0]
     if len(peaks_in_chunk_idx) == 0:
         return None
@@ -501,23 +492,19 @@
             spike_pre_frames + spike_post_frames,
         )
     )
     # prepare an array of channels
     all_chan_idx = np.zeros((len(peaks_chunk), n_channel_neighbors))
 
     # for each spike in the sample, add it to the
-    for spike_i, (sample_ind, channel_ind, amplitude, segment_ind) in enumerate(
-        peaks_chunk
-    ):
-        spike_start = sample_ind + margin_frames - spike_pre_frames - start_frame
-        spike_end = sample_ind + margin_frames + spike_post_frames - start_frame
-        all_spikes[spike_i] = traces[
-            spike_start:spike_end, channel_neighbors[channel_ind]
-        ].T
-        all_chan_idx[spike_i] = channel_neighbors[channel_ind]
+    for spike_i, (sample_index, channel_index, amplitude, segment_index) in enumerate(peaks_chunk):
+        spike_start = sample_index + margin_frames - spike_pre_frames - start_frame
+        spike_end = sample_index + margin_frames + spike_post_frames - start_frame
+        all_spikes[spike_i] = traces[spike_start:spike_end, channel_neighbors[channel_index]].T
+        all_chan_idx[spike_i] = channel_neighbors[channel_index]
 
     return all_spikes, all_chan_idx, peaks_in_chunk_idx
 
 
 def get_spike_nearest_neighbors(
     all_spikes,
     n_neighbors,
@@ -525,16 +512,16 @@
     knn_verbose,
     all_chan_idx,
     n_samples=50,
     n_channel_neighbors=8,
     n_jobs=1,
     max_candidates=60,
 ):
-    """ Builds a graph of nearest neighbors from sparse spike array.
-    TODO: There are potentially faster ANN approaches. e.g. 
+    """Builds a graph of nearest neighbors from sparse spike array.
+    TODO: There are potentially faster ANN approaches. e.g.
         https://github.com/facebookresearch/pysparnn
     """
 
     # helper functions for nearest-neighbors search tree
     def get_n_trees_iters(X):
         n_trees = min(64, 5 + int(round((X.shape[0]) ** 0.5 / 20.0)))
         n_iters = max(5, int(round(np.log2(X.shape[0]))))
@@ -544,17 +531,15 @@
         i1 = l[idx1]
         i2 = l[idx2]
         l[idx1] = i2
         l[idx2] = i1
         return l
 
     # flatten spikes
-    all_spikes_flat = np.reshape(
-        all_spikes, (len(all_spikes), np.product(all_spikes.shape[1:]))
-    )
+    all_spikes_flat = np.reshape(all_spikes, (len(all_spikes), np.product(all_spikes.shape[1:])))
 
     # add channel indices to channel values (for graph construction)
     all_spikes_flat = np.hstack([all_chan_idx, all_spikes_flat])
 
     # get parameters for NN search tree
     n_trees, n_iters = get_n_trees_iters(all_spikes_flat)
 
@@ -582,23 +567,20 @@
     #   very small proportion of events
     # HACK: switch back errors
     # computed neighbors where an element is *not* closest to itself
     nn_errors = np.where(knn_indices[:, 0] != np.arange(len(knn_indices)))[0]
     # correct by swapping
     for nn_error in nn_errors:
         correct_match = np.where(knn_indices[nn_error] == nn_error)[0][0]
-        knn_indices[nn_error] = swap_elements(
-            knn_indices[nn_error], correct_match, 0)
+        knn_indices[nn_error] = swap_elements(knn_indices[nn_error], correct_match, 0)
 
     return knn_indices, knn_distances
 
 
-def merge_nn_dicts(
-    peaks, n_neighbors, peaks_in_chunk_idx_list, knn_indices_list, knn_distances_list
-):
+def merge_nn_dicts(peaks, n_neighbors, peaks_in_chunk_idx_list, knn_indices_list, knn_distances_list):
     """merge together peaks_in_chunk_idx_list and knn_indices_list
     to build final graph
 
     Args:
         peaks (_type_): array of peaks
         n_neighbors (_type_): number of neighbors
         peaks_in_chunk_idx_list (_type_): list of spike index
@@ -616,43 +598,43 @@
     nn_distance_array = np.zeros((len(peaks), n_neighbors * 2), dtype=float)
     end_last = -1
     # for each nn graph
     for idxi, (peaks_in_chunk_idx, knn_indices, knn_distances) in enumerate(
         zip(peaks_in_chunk_idx_list, knn_indices_list, knn_distances_list)
     ):
         # put new neighbors in first 5 rows
-        nn_index_array[
-            peaks_in_chunk_idx[peaks_in_chunk_idx > end_last], :n_neighbors
-        ] = knn_indices[peaks_in_chunk_idx > end_last]
+        nn_index_array[peaks_in_chunk_idx[peaks_in_chunk_idx > end_last], :n_neighbors] = knn_indices[
+            peaks_in_chunk_idx > end_last
+        ]
         # put overlapping neighbors from previous
-        nn_index_array[
-            peaks_in_chunk_idx[peaks_in_chunk_idx <= end_last], n_neighbors:
-        ] = knn_indices[peaks_in_chunk_idx <= end_last]
+        nn_index_array[peaks_in_chunk_idx[peaks_in_chunk_idx <= end_last], n_neighbors:] = knn_indices[
+            peaks_in_chunk_idx <= end_last
+        ]
 
         # repeat for distances
-        nn_distance_array[
-            peaks_in_chunk_idx[peaks_in_chunk_idx > end_last], :n_neighbors
-        ] = knn_distances[peaks_in_chunk_idx > end_last]
-        nn_distance_array[
-            peaks_in_chunk_idx[peaks_in_chunk_idx <= end_last], n_neighbors:
-        ] = knn_distances[peaks_in_chunk_idx <= end_last]
+        nn_distance_array[peaks_in_chunk_idx[peaks_in_chunk_idx > end_last], :n_neighbors] = knn_distances[
+            peaks_in_chunk_idx > end_last
+        ]
+        nn_distance_array[peaks_in_chunk_idx[peaks_in_chunk_idx <= end_last], n_neighbors:] = knn_distances[
+            peaks_in_chunk_idx <= end_last
+        ]
         # double up neighbors the beginning, since we only sample these once
 
         if idxi == 0:
             nn_index_array[peaks_in_chunk_idx, n_neighbors:] = knn_indices
             nn_distance_array[peaks_in_chunk_idx, n_neighbors:] = knn_distances
         # double up neighbors in the end, since we only sample these once
         if idxi == len(peaks_in_chunk_idx_list) - 1:
-            nn_index_array[
-                peaks_in_chunk_idx[peaks_in_chunk_idx > end_last], n_neighbors:
-            ] = knn_indices[peaks_in_chunk_idx > end_last]
+            nn_index_array[peaks_in_chunk_idx[peaks_in_chunk_idx > end_last], n_neighbors:] = knn_indices[
+                peaks_in_chunk_idx > end_last
+            ]
             # repeat for distances
-            nn_distance_array[
-                peaks_in_chunk_idx[peaks_in_chunk_idx > end_last], n_neighbors:
-            ] = knn_distances[peaks_in_chunk_idx > end_last]
+            nn_distance_array[peaks_in_chunk_idx[peaks_in_chunk_idx > end_last], n_neighbors:] = knn_distances[
+                peaks_in_chunk_idx > end_last
+            ]
 
         end_last = peaks_in_chunk_idx[-1]
     return nn_index_array, nn_distance_array
 
 
 def construct_symmetric_graph_from_idx_vals(graph_idx, graph_vals):
     rows = graph_idx.flatten()
@@ -669,63 +651,53 @@
         (np.ones(len(rows_)), (rows_, cols_)),
         shape=(len(graph_idx), len(graph_idx)),
     ).tocsr()
     return chunk_csr
 
 
 def embed_graph(
-    chunk_csr,
-    prev_embeddings=None,
-    prev_idx=None,
-    negative_to_positive_samples=5,
-    embedding_dim=2,
-    mde_device="cuda"
+    chunk_csr, prev_embeddings=None, prev_idx=None, negative_to_positive_samples=5, embedding_dim=2, mde_device="cuda"
 ):
     # graph size
     n_items = chunk_csr.shape[1]
 
     # grate graph as pymde graph
     knn_graph = pymde.Graph(chunk_csr)
 
     # constrain embeddings
     if prev_embeddings is not None:
         anchor_constraint = pymde.Anchored(
             anchors=torch.tensor(prev_idx, device=mde_device),
-            values=torch.tensor(
-                prev_embeddings, dtype=torch.float32, device=mde_device)
+            values=torch.tensor(prev_embeddings, dtype=torch.float32, device=mde_device),
         )
     else:
         anchor_constraint = None
 
     # initialize with quadratic embedding
     quadratic_mde = pymde.MDE(
         n_items=n_items,
         embedding_dim=embedding_dim,
         edges=knn_graph.edges,
         distortion_function=pymde.penalties.Quadratic(knn_graph.weights),
         constraint=anchor_constraint,
-        device=mde_device
+        device=mde_device,
     )
 
     # embed quadratic initialization
     x = quadratic_mde.embed(verbose=True).cpu()
 
     # get all nearest neighbor edges
     similar_edges = knn_graph.edges
 
     # sample a set of dissimilar edges
-    n_dis = int(len(similar_edges)*negative_to_positive_samples)
-    dissimilar_edges = pymde.preprocess.dissimilar_edges(
-        n_items=n_items, num_edges=n_dis, similar_edges=similar_edges
-    )
+    n_dis = int(len(similar_edges) * negative_to_positive_samples)
+    dissimilar_edges = pymde.preprocess.dissimilar_edges(n_items=n_items, num_edges=n_dis, similar_edges=similar_edges)
     # created a list of weights for similar and dissimilar edges
     edges = torch.cat([similar_edges, dissimilar_edges])
-    weights = torch.cat(
-        [knn_graph.weights, -1.0 * torch.ones(dissimilar_edges.shape[0])]
-    )
+    weights = torch.cat([knn_graph.weights, -1.0 * torch.ones(dissimilar_edges.shape[0])])
 
     # create a distortion penalty
     f = pymde.penalties.PushAndPull(
         weights=weights,
         attractive_penalty=pymde.penalties.Log1p,
         repulsive_penalty=pymde.penalties.Log,
     )
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sortingcomponents/clustering/triage.py` & `spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/clustering/triage.py`

 * *Files identical despite different names*

### Comparing `spikeinterface-0.97.1/spikeinterface/sortingcomponents/features_from_peaks.py` & `spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/features_from_peaks.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,27 +1,33 @@
 """Sorting components: peak waveform features."""
 import numpy as np
 
 from spikeinterface.core.job_tools import fix_job_kwargs
 from spikeinterface.core import get_channel_distances
 from spikeinterface.sortingcomponents.peak_localization import LocalizeCenterOfMass, LocalizeMonopolarTriangulation
-from spikeinterface.sortingcomponents.peak_pipeline import run_peak_pipeline, PipelineNode, ExtractDenseWaveforms
-
+from spikeinterface.sortingcomponents.peak_pipeline import (
+    run_node_pipeline,
+    PeakRetriever,
+    PipelineNode,
+    ExtractDenseWaveforms,
+)
 
 
 def compute_features_from_peaks(
     recording,
     peaks,
-    feature_list=["ptp", ],
+    feature_list=[
+        "ptp",
+    ],
     feature_params={},
-    ms_before=1.,
-    ms_after=1.,
+    ms_before=1.0,
+    ms_after=1.0,
     **job_kwargs,
 ):
-    """Extract features on the fly from the recording given a list of peaks. 
+    """Extract features on the fly from the recording given a list of peaks.
 
     Parameters
     ----------
     recording: RecordingExtractor
         The recording extractor object.
     peaks: array
         Peaks array, as returned by detect_peaks() in "compact_numpy" way.
@@ -42,63 +48,69 @@
     A tuple of features. Even if there is one feature.
     Every feature have shape[0] == peaks.shape[0].
     dtype and other dim depends on features.
 
     """
     job_kwargs = fix_job_kwargs(job_kwargs)
 
-    extract_dense_waveforms = ExtractDenseWaveforms(recording, ms_before=ms_before, ms_after=ms_after,  return_output=False)
+    peak_retriever = PeakRetriever(recording, peaks)
+    extract_dense_waveforms = ExtractDenseWaveforms(
+        recording, parents=[peak_retriever], ms_before=ms_before, ms_after=ms_after, return_output=False
+    )
     nodes = [
+        peak_retriever,
         extract_dense_waveforms,
     ]
     for feature_name in feature_list:
         Class = _features_class[feature_name]
         params = feature_params.get(feature_name, {}).copy()
-        node = Class(recording, parents=[extract_dense_waveforms], **params)
+        node = Class(recording, parents=[peak_retriever, extract_dense_waveforms], **params)
         nodes.append(node)
 
-    features = run_peak_pipeline(recording, peaks, nodes, job_kwargs, job_name='features_from_peaks', squeeze_output=False)
+    features = run_node_pipeline(recording, nodes, job_kwargs, job_name="features_from_peaks", squeeze_output=False)
 
     return features
 
 
 class AmplitudeFeature(PipelineNode):
-    def __init__(self, recording,  name='amplitude_feature', return_output=True, parents=None, 
-                        all_channels=False, peak_sign='neg'):
+    def __init__(
+        self, recording, name="amplitude_feature", return_output=True, parents=None, all_channels=False, peak_sign="neg"
+    ):
         PipelineNode.__init__(self, recording, return_output=return_output, parents=parents)
 
         self.all_channels = all_channels
         self.peak_sign = peak_sign
         self._kwargs.update(dict(all_channels=all_channels, peak_sign=peak_sign))
         self._dtype = recording.get_dtype()
 
     def get_dtype(self):
         return self._dtype
 
     def compute(self, traces, peaks, waveforms):
         if self.all_channels:
-            if self.peak_sign == 'neg':
+            if self.peak_sign == "neg":
                 amplitudes = np.min(waveforms, axis=1)
-            elif self.peak_sign == 'pos':
+            elif self.peak_sign == "pos":
                 amplitudes = np.max(waveforms, axis=1)
-            elif self.peak_sign == 'both':
+            elif self.peak_sign == "both":
                 amplitudes = np.max(np.abs(waveforms, axis=1))
         else:
-            if self.peak_sign == 'neg':
+            if self.peak_sign == "neg":
                 amplitudes = np.min(waveforms, axis=(1, 2))
-            elif self.peak_sign == 'pos':
+            elif self.peak_sign == "pos":
                 amplitudes = np.max(waveforms, axis=(1, 2))
-            elif self.peak_sign == 'both':
+            elif self.peak_sign == "both":
                 amplitudes = np.max(np.abs(waveforms), axis=(1, 2))
         return amplitudes
 
 
 class PeakToPeakFeature(PipelineNode):
-    def __init__(self, recording,  name='ptp_feature', return_output=True, parents=None,
-                   local_radius_um=150., all_channels=True):
+    def __init__(
+        self, recording, name="ptp_feature", return_output=True, parents=None, local_radius_um=150.0, all_channels=True
+    ):
         PipelineNode.__init__(self, recording, return_output=return_output, parents=parents)
 
         self.contact_locations = recording.get_channel_locations()
         self.channel_distance = get_channel_distances(recording)
         self.neighbours_mask = self.channel_distance < local_radius_um
         self.all_channels = all_channels
         self._kwargs.update(dict(local_radius_um=local_radius_um, all_channels=all_channels))
@@ -108,101 +120,123 @@
         return self._dtype
 
     def compute(self, traces, peaks, waveforms):
         if self.all_channels:
             all_ptps = np.ptp(waveforms, axis=1)
         else:
             all_ptps = np.zeros(peaks.size)
-            for main_chan in np.unique(peaks['channel_ind']):
-                idx, = np.nonzero(peaks['channel_ind'] == main_chan)
-                chan_inds, = np.nonzero(self.neighbours_mask[main_chan])
+            for main_chan in np.unique(peaks["channel_index"]):
+                (idx,) = np.nonzero(peaks["channel_index"] == main_chan)
+                (chan_inds,) = np.nonzero(self.neighbours_mask[main_chan])
                 wfs = waveforms[idx][:, :, chan_inds]
                 all_ptps[idx] = np.max(np.ptp(wfs, axis=1))
         return all_ptps
 
 
 class PeakToPeakLagsFeature(PipelineNode):
-    def __init__(self, recording,  name='ptp_lag_feature', return_output=True, parents=None,
-                   local_radius_um=150., all_channels=True):
+    def __init__(
+        self,
+        recording,
+        name="ptp_lag_feature",
+        return_output=True,
+        parents=None,
+        local_radius_um=150.0,
+        all_channels=True,
+    ):
         PipelineNode.__init__(self, recording, return_output=return_output, parents=parents)
 
         self.all_channels = all_channels
         self.local_radius_um = local_radius_um
 
         self.contact_locations = recording.get_channel_locations()
         self.channel_distance = get_channel_distances(recording)
         self.neighbours_mask = self.channel_distance < local_radius_um
-        
+
         self._kwargs.update(dict(local_radius_um=local_radius_um, all_channels=all_channels))
         self._dtype = recording.get_dtype()
 
     def get_dtype(self):
         return self._dtype
 
     def compute(self, traces, peaks, waveforms):
         if self.all_channels:
             all_maxs = np.argmax(waveforms, axis=1)
             all_mins = np.argmin(waveforms, axis=1)
             all_lags = all_maxs - all_mins
         else:
             all_lags = np.zeros(peaks.size)
-            for main_chan in np.unique(peaks['channel_ind']):
-                idx, = np.nonzero(peaks['channel_ind'] == main_chan)
-                chan_inds, = np.nonzero(self.neighbours_mask[main_chan])
+            for main_chan in np.unique(peaks["channel_index"]):
+                (idx,) = np.nonzero(peaks["channel_index"] == main_chan)
+                (chan_inds,) = np.nonzero(self.neighbours_mask[main_chan])
                 wfs = waveforms[idx][:, :, chan_inds]
                 maxs = np.argmax(wfs, axis=1)
                 mins = np.argmin(wfs, axis=1)
                 lags = maxs - mins
                 ptps = np.argmax(np.ptp(wfs, axis=1), axis=1)
                 all_lags[idx] = lags[np.arange(len(idx)), ptps]
         return all_lags
 
 
 class RandomProjectionsFeature(PipelineNode):
-
-    def __init__(self, recording,  name='random_projections_feature', return_output=True, parents=None,
-                   projections=None, local_radius_um=150., min_values=None):
+    def __init__(
+        self,
+        recording,
+        name="random_projections_feature",
+        return_output=True,
+        parents=None,
+        projections=None,
+        local_radius_um=150.0,
+        min_values=None,
+    ):
         PipelineNode.__init__(self, recording, return_output=return_output, parents=parents)
 
         self.projections = projections
         self.local_radius_um = local_radius_um
         self.min_values = min_values
 
         self.contact_locations = recording.get_channel_locations()
         self.channel_distance = get_channel_distances(recording)
         self.neighbours_mask = self.channel_distance < local_radius_um
-        
+
         self._kwargs.update(dict(projections=projections, local_radius_um=local_radius_um, min_values=min_values))
-    
+
         self._dtype = recording.get_dtype()
 
     def get_dtype(self):
         return self._dtype
 
     def compute(self, traces, peaks, waveforms):
         all_projections = np.zeros((peaks.size, self.projections.shape[1]), dtype=self._dtype)
-        for main_chan in np.unique(peaks['channel_ind']):
-            idx, = np.nonzero(peaks['channel_ind'] == main_chan)
-            chan_inds, = np.nonzero(self.neighbours_mask[main_chan])
+        for main_chan in np.unique(peaks["channel_index"]):
+            (idx,) = np.nonzero(peaks["channel_index"] == main_chan)
+            (chan_inds,) = np.nonzero(self.neighbours_mask[main_chan])
             local_projections = self.projections[chan_inds, :]
             wf_ptp = (waveforms[idx][:, :, chan_inds]).ptp(axis=1)
 
             if self.min_values is not None:
-                wf_ptp = (wf_ptp/self.min_values[chan_inds])**4
+                wf_ptp = (wf_ptp / self.min_values[chan_inds]) ** 4
 
             denom = np.sum(wf_ptp, axis=1)
             mask = denom != 0
 
-            all_projections[idx[mask]] = np.dot(wf_ptp[mask], local_projections)/(denom[mask][:, np.newaxis])
+            all_projections[idx[mask]] = np.dot(wf_ptp[mask], local_projections) / (denom[mask][:, np.newaxis])
         return all_projections
 
 
 class RandomProjectionsEnergyFeature(PipelineNode):
-    def __init__(self, recording,  name='random_projections_energy_feature', return_output=True, parents=None,
-                   projections=None, local_radius_um=150., min_values=None):
+    def __init__(
+        self,
+        recording,
+        name="random_projections_energy_feature",
+        return_output=True,
+        parents=None,
+        projections=None,
+        local_radius_um=150.0,
+        min_values=None,
+    ):
         PipelineNode.__init__(self, recording, return_output=return_output, parents=parents)
 
         self.contact_locations = recording.get_channel_locations()
         self.channel_distance = get_channel_distances(recording)
         self.neighbours_mask = self.channel_distance < local_radius_um
 
         self.projections = projections
@@ -212,139 +246,137 @@
         self._dtype = recording.get_dtype()
 
     def get_dtype(self):
         return self._dtype
 
     def compute(self, traces, peaks, waveforms):
         all_projections = np.zeros((peaks.size, self.projections.shape[1]), dtype=self._dtype)
-        for main_chan in np.unique(peaks['channel_ind']):
-            idx, = np.nonzero(peaks['channel_ind'] == main_chan)
-            chan_inds, = np.nonzero(self.neighbours_mask[main_chan])
+        for main_chan in np.unique(peaks["channel_index"]):
+            (idx,) = np.nonzero(peaks["channel_index"] == main_chan)
+            (chan_inds,) = np.nonzero(self.neighbours_mask[main_chan])
             local_projections = self.projections[chan_inds, :]
             energies = np.linalg.norm(waveforms[idx][:, :, chan_inds], axis=1)
 
             if self.min_values is not None:
-                energies = (energies/self.min_values[chan_inds])**4
+                energies = (energies / self.min_values[chan_inds]) ** 4
 
             denom = np.sum(energies, axis=1)
             mask = denom != 0
 
-            all_projections[idx[mask]] = np.dot(energies[mask], local_projections)/(denom[mask][:, np.newaxis])
+            all_projections[idx[mask]] = np.dot(energies[mask], local_projections) / (denom[mask][:, np.newaxis])
         return all_projections
 
 
 class StdPeakToPeakFeature(PipelineNode):
-    def __init__(self, recording,  name='std_ptp_feature', return_output=True, parents=None,
-                   local_radius_um=150.):
+    def __init__(self, recording, name="std_ptp_feature", return_output=True, parents=None, local_radius_um=150.0):
         PipelineNode.__init__(self, recording, return_output=return_output, parents=parents)
 
         self.contact_locations = recording.get_channel_locations()
         self.channel_distance = get_channel_distances(recording)
         self.neighbours_mask = self.channel_distance < local_radius_um
-        
+
         self._kwargs.update(dict(local_radius_um=local_radius_um))
 
         self._dtype = recording.get_dtype()
 
     def get_dtype(self):
         return self._dtype
 
     def compute(self, traces, peaks, waveforms):
         all_ptps = np.zeros(peaks.size)
-        for main_chan in np.unique(peaks['channel_ind']):
-            idx, = np.nonzero(peaks['channel_ind'] == main_chan)
-            chan_inds, = np.nonzero(self.neighbours_mask[main_chan])
+        for main_chan in np.unique(peaks["channel_index"]):
+            (idx,) = np.nonzero(peaks["channel_index"] == main_chan)
+            (chan_inds,) = np.nonzero(self.neighbours_mask[main_chan])
             wfs = waveforms[idx][:, :, chan_inds]
             all_ptps[idx] = np.std(np.ptp(wfs, axis=1), axis=1)
         return all_ptps
 
 
 class GlobalPeakToPeakFeature(PipelineNode):
-    def __init__(self, recording,  name='global_ptp_feature', return_output=True, parents=None,
-                   local_radius_um=150.):
+    def __init__(self, recording, name="global_ptp_feature", return_output=True, parents=None, local_radius_um=150.0):
         PipelineNode.__init__(self, recording, return_output=return_output, parents=parents)
 
         self.contact_locations = recording.get_channel_locations()
         self.channel_distance = get_channel_distances(recording)
         self.neighbours_mask = self.channel_distance < local_radius_um
-        
+
         self._kwargs.update(dict(local_radius_um=local_radius_um))
 
         self._dtype = recording.get_dtype()
 
     def get_dtype(self):
         return self._dtype
 
     def compute(self, traces, peaks, waveforms):
         all_ptps = np.zeros(peaks.size)
-        for main_chan in np.unique(peaks['channel_ind']):
-            idx, = np.nonzero(peaks['channel_ind'] == main_chan)
-            chan_inds, = np.nonzero(self.neighbours_mask[main_chan])
+        for main_chan in np.unique(peaks["channel_index"]):
+            (idx,) = np.nonzero(peaks["channel_index"] == main_chan)
+            (chan_inds,) = np.nonzero(self.neighbours_mask[main_chan])
             wfs = waveforms[idx][:, :, chan_inds]
             all_ptps[idx] = np.max(wfs, axis=(1, 2)) - np.min(wfs, axis=(1, 2))
         return all_ptps
 
+
 class KurtosisPeakToPeakFeature(PipelineNode):
-    def __init__(self, recording,  name='kurtosis_ptp_feature', return_output=True, parents=None,
-                   local_radius_um=150.):
+    def __init__(self, recording, name="kurtosis_ptp_feature", return_output=True, parents=None, local_radius_um=150.0):
         PipelineNode.__init__(self, recording, return_output=return_output, parents=parents)
 
         self.contact_locations = recording.get_channel_locations()
         self.channel_distance = get_channel_distances(recording)
         self.neighbours_mask = self.channel_distance < local_radius_um
-        
+
         self._kwargs.update(dict(local_radius_um=local_radius_um))
 
         self._dtype = recording.get_dtype()
 
     def get_dtype(self):
         return self._dtype
 
     def compute(self, traces, peaks, waveforms):
         all_ptps = np.zeros(peaks.size)
         import scipy
-        for main_chan in np.unique(peaks['channel_ind']):
-            idx, = np.nonzero(peaks['channel_ind'] == main_chan)
-            chan_inds, = np.nonzero(self.neighbours_mask[main_chan])
+
+        for main_chan in np.unique(peaks["channel_index"]):
+            (idx,) = np.nonzero(peaks["channel_index"] == main_chan)
+            (chan_inds,) = np.nonzero(self.neighbours_mask[main_chan])
             wfs = waveforms[idx][:, :, chan_inds]
             all_ptps[idx] = scipy.stats.kurtosis(np.ptp(wfs, axis=1), axis=1)
         return all_ptps
 
 
 class EnergyFeature(PipelineNode):
-    def __init__(self, recording,  name='energy_feature', return_output=True, parents=None,
-                   local_radius_um=50.):
+    def __init__(self, recording, name="energy_feature", return_output=True, parents=None, local_radius_um=50.0):
         PipelineNode.__init__(self, recording, return_output=return_output, parents=parents)
 
         self.contact_locations = recording.get_channel_locations()
         self.channel_distance = get_channel_distances(recording)
         self.neighbours_mask = self.channel_distance < local_radius_um
-        
+
         self._kwargs.update(dict(local_radius_um=local_radius_um))
 
     def get_dtype(self):
-        return np.dtype('float32')
+        return np.dtype("float32")
 
     def compute(self, traces, peaks, waveforms):
-        energy = np.zeros(peaks.size, dtype='float32')
-        for main_chan in np.unique(peaks['channel_ind']):
-            idx, = np.nonzero(peaks['channel_ind'] == main_chan)
-            chan_inds, = np.nonzero(self.neighbours_mask[main_chan])
+        energy = np.zeros(peaks.size, dtype="float32")
+        for main_chan in np.unique(peaks["channel_index"]):
+            (idx,) = np.nonzero(peaks["channel_index"] == main_chan)
+            (chan_inds,) = np.nonzero(self.neighbours_mask[main_chan])
 
             wfs = waveforms[idx][:, :, chan_inds]
             energy[idx] = np.linalg.norm(wfs, axis=(1, 2)) / chan_inds.size
         return energy
 
 
 _features_class = {
-    'amplitude': AmplitudeFeature,
-    'ptp' : PeakToPeakFeature,
-    'center_of_mass' : LocalizeCenterOfMass,
-    'monopolar_triangulation' : LocalizeMonopolarTriangulation,
-    'energy' : EnergyFeature,
-    'std_ptp' : StdPeakToPeakFeature,
-    'kurtosis_ptp' : KurtosisPeakToPeakFeature,
-    'random_projections_ptp' : RandomProjectionsFeature,
-    'random_projections_energy' : RandomProjectionsEnergyFeature,
-    'ptp_lag' : PeakToPeakLagsFeature,
-    'global_ptp' : GlobalPeakToPeakFeature
-}
+    "amplitude": AmplitudeFeature,
+    "ptp": PeakToPeakFeature,
+    "center_of_mass": LocalizeCenterOfMass,
+    "monopolar_triangulation": LocalizeMonopolarTriangulation,
+    "energy": EnergyFeature,
+    "std_ptp": StdPeakToPeakFeature,
+    "kurtosis_ptp": KurtosisPeakToPeakFeature,
+    "random_projections_ptp": RandomProjectionsFeature,
+    "random_projections_energy": RandomProjectionsEnergyFeature,
+    "ptp_lag": PeakToPeakLagsFeature,
+    "global_ptp": GlobalPeakToPeakFeature,
+}
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sortingcomponents/matching/main.py` & `spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/matching/main.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,130 +1,128 @@
-
 from threadpoolctl import threadpool_limits
 import numpy as np
 
 from spikeinterface.core.job_tools import ChunkRecordingExecutor, fix_job_kwargs
 from spikeinterface.core import get_chunk_with_margin
 
 
-
-def find_spikes_from_templates(recording, method='naive', method_kwargs={}, extra_outputs=False,
-                              **job_kwargs):
+def find_spikes_from_templates(recording, method="naive", method_kwargs={}, extra_outputs=False, **job_kwargs):
     """Find spike from a recording from given templates.
 
     Parameters
     ----------
     recording: RecordingExtractor
         The recording extractor object
-    waveform_extractor: WaveformExtractor
-        The waveform extractor
-    method: str 
-        Which method to use ('naive' | 'tridesclous' | 'circus')
+    method: str
+        Which method to use ('naive' | 'tridesclous' | 'circus' | 'circus-omp' | 'wobble')
     method_kwargs: dict, optional
         Keyword arguments for the chosen method
     extra_outputs: bool
-        If True then method_kwargs is also return
+        If True then method_kwargs is also returned
     job_kwargs: dict
         Parameters for ChunkRecordingExecutor
 
     Returns
     -------
     spikes: ndarray
         Spikes found from templates.
-    method_kwargs: 
+    method_kwargs:
         Optionaly returns for debug purpose.
 
     Notes
     -----
-    Templates are represented as WaveformExtractor so statistics can be extracted.
+    For all methods except 'wobble', templates are represented as a WaveformExtractor in method_kwargs
+    so statistics can be extracted.  For 'wobble' templates are represented as a numpy.ndarray.
     """
     from .method_list import matching_methods
-    assert method in matching_methods, "The method %s is not a valid one" %method
+
+    assert method in matching_methods, "The method %s is not a valid one" % method
 
     job_kwargs = fix_job_kwargs(job_kwargs)
 
     method_class = matching_methods[method]
-    
+
     # initialize
     method_kwargs = method_class.initialize_and_check_kwargs(recording, method_kwargs)
-    
-    # add 
-    method_kwargs['margin'] = method_class.get_margin(recording, method_kwargs)
-    
+
+    # add
+    method_kwargs["margin"] = method_class.get_margin(recording, method_kwargs)
+
     # serialiaze for worker
     method_kwargs_seralized = method_class.serialize_method_kwargs(method_kwargs)
-    
+
     # and run
     func = _find_spikes_chunk
     init_func = _init_worker_find_spikes
     init_args = (recording, method, method_kwargs_seralized)
-    processor = ChunkRecordingExecutor(recording, func, init_func, init_args,
-                                       handle_returns=True, job_name=f'find spikes ({method})', **job_kwargs)
+    processor = ChunkRecordingExecutor(
+        recording, func, init_func, init_args, handle_returns=True, job_name=f"find spikes ({method})", **job_kwargs
+    )
     spikes = processor.run()
 
     spikes = np.concatenate(spikes)
-    
+
     if extra_outputs:
         return spikes, method_kwargs
     else:
         return spikes
 
 
 def _init_worker_find_spikes(recording, method, method_kwargs):
     """Initialize worker for finding spikes."""
 
     from .method_list import matching_methods
+
     method_class = matching_methods[method]
     method_kwargs = method_class.unserialize_in_worker(method_kwargs)
 
-
     # create a local dict per worker
     worker_ctx = {}
-    worker_ctx['recording'] = recording
-    worker_ctx['method'] = method
-    worker_ctx['method_kwargs'] = method_kwargs
-    worker_ctx['function'] = method_class.main_function
-    
+    worker_ctx["recording"] = recording
+    worker_ctx["method"] = method
+    worker_ctx["method_kwargs"] = method_kwargs
+    worker_ctx["function"] = method_class.main_function
 
     return worker_ctx
 
 
 def _find_spikes_chunk(segment_index, start_frame, end_frame, worker_ctx):
     """Find spikes from a chunk of data."""
 
     # recover variables of the worker
-    recording = worker_ctx['recording']
-    method = worker_ctx['method']
-    method_kwargs = worker_ctx['method_kwargs']
-    margin = method_kwargs['margin']
-    
+    recording = worker_ctx["recording"]
+    method = worker_ctx["method"]
+    method_kwargs = worker_ctx["method_kwargs"]
+    margin = method_kwargs["margin"]
+
     # load trace in memory given some margin
     recording_segment = recording._recording_segments[segment_index]
-    traces, left_margin, right_margin = get_chunk_with_margin(recording_segment,
-                start_frame, end_frame, None, margin, add_zeros=True)
+    traces, left_margin, right_margin = get_chunk_with_margin(
+        recording_segment, start_frame, end_frame, None, margin, add_zeros=True
+    )
+
+    function = worker_ctx["function"]
 
-    
-    function = worker_ctx['function']
-    
     with threadpool_limits(limits=1):
         spikes = function(traces, method_kwargs)
-    
+
     # remove spikes in margin
     if margin > 0:
-        keep = (spikes['sample_ind']  >= margin) & (spikes['sample_ind']  < (traces.shape[0] - margin))
+        keep = (spikes["sample_index"] >= margin) & (spikes["sample_index"] < (traces.shape[0] - margin))
         spikes = spikes[keep]
 
-    spikes['sample_ind'] += (start_frame - margin)
-    spikes['segment_ind'] = segment_index
+    spikes["sample_index"] += start_frame - margin
+    spikes["segment_index"] = segment_index
     return spikes
 
+
 # generic class for template engine
 class BaseTemplateMatchingEngine:
     default_params = {}
-    
+
     @classmethod
     def initialize_and_check_kwargs(cls, recording, kwargs):
         """This function runs before loops"""
         # need to be implemented in subclass
         raise NotImplementedError
 
     @classmethod
@@ -145,8 +143,7 @@
         raise NotImplementedError
 
     @classmethod
     def main_function(cls, traces, method_kwargs):
         """This function returns the number of samples for the chunk margins"""
         # need to be implemented in subclass
         raise NotImplementedError
-
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sortingcomponents/matching/naive.py` & `spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/matching/naive.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,127 +1,131 @@
 """Sorting components: template matching."""
 
 import numpy as np
 from spikeinterface.core import WaveformExtractor
 from spikeinterface.core import get_noise_levels, get_channel_distances, get_chunk_with_margin, get_random_data_chunks
-from spikeinterface.postprocessing import (get_template_channel_sparsity, get_template_extremum_channel)
+from spikeinterface.postprocessing import get_template_channel_sparsity, get_template_extremum_channel
 
 from spikeinterface.sortingcomponents.peak_detection import DetectPeakLocallyExclusive
 
-spike_dtype = [('sample_ind', 'int64'), ('channel_ind', 'int64'), ('cluster_ind', 'int64'),
-               ('amplitude', 'float64'), ('segment_ind', 'int64')]
+spike_dtype = [
+    ("sample_index", "int64"),
+    ("channel_index", "int64"),
+    ("cluster_index", "int64"),
+    ("amplitude", "float64"),
+    ("segment_index", "int64"),
+]
 
 
 from .main import BaseTemplateMatchingEngine
 
 
 class NaiveMatching(BaseTemplateMatchingEngine):
     """
     This is a naive template matching that does not resolve collision
     and does not take in account sparsity.
     It just minimizes the distance to templates for detected peaks.
 
     It is implemented for benchmarking against this low quality template matching.
     And also as an example how to deal with methods_kwargs, margin, intit, func, ...
     """
+
     default_params = {
-        'waveform_extractor': None,
-        'peak_sign': 'neg',
-        'exclude_sweep_ms': 0.1,
-        'detect_threshold': 5,
-        'noise_levels': None,
-        'local_radius_um': 100,
-        'random_chunk_kwargs': {},
+        "waveform_extractor": None,
+        "peak_sign": "neg",
+        "exclude_sweep_ms": 0.1,
+        "detect_threshold": 5,
+        "noise_levels": None,
+        "local_radius_um": 100,
+        "random_chunk_kwargs": {},
     }
-    
 
     @classmethod
     def initialize_and_check_kwargs(cls, recording, kwargs):
         d = cls.default_params.copy()
         d.update(kwargs)
-        
-        assert d['waveform_extractor'] is not None
-        
-        we = d['waveform_extractor']
 
-        if d['noise_levels'] is None:
-            d['noise_levels'] = get_noise_levels(recording, **d['random_chunk_kwargs'])
+        assert d["waveform_extractor"] is not None
+
+        we = d["waveform_extractor"]
 
-        d['abs_threholds'] = d['noise_levels'] * d['detect_threshold']
+        if d["noise_levels"] is None:
+            d["noise_levels"] = get_noise_levels(recording, **d["random_chunk_kwargs"])
+
+        d["abs_threholds"] = d["noise_levels"] * d["detect_threshold"]
 
         channel_distance = get_channel_distances(recording)
-        d['neighbours_mask'] = channel_distance < d['local_radius_um']
+        d["neighbours_mask"] = channel_distance < d["local_radius_um"]
 
-        d['nbefore'] = we.nbefore
-        d['nafter'] = we.nafter
+        d["nbefore"] = we.nbefore
+        d["nafter"] = we.nafter
 
-        d['exclude_sweep_size'] = int(d['exclude_sweep_ms'] * recording.get_sampling_frequency() / 1000.)
+        d["exclude_sweep_size"] = int(d["exclude_sweep_ms"] * recording.get_sampling_frequency() / 1000.0)
 
         return d
-    
+
     @classmethod
     def get_margin(cls, recording, kwargs):
-        margin = max(kwargs['nbefore'], kwargs['nafter'])
+        margin = max(kwargs["nbefore"], kwargs["nafter"])
         return margin
 
     @classmethod
     def serialize_method_kwargs(cls, kwargs):
         kwargs = dict(kwargs)
-        
-        waveform_extractor = kwargs['waveform_extractor']
-        kwargs['waveform_extractor'] = str(waveform_extractor.folder)
-        
+
+        waveform_extractor = kwargs["waveform_extractor"]
+        kwargs["waveform_extractor"] = str(waveform_extractor.folder)
+
         return kwargs
 
     @classmethod
     def unserialize_in_worker(cls, kwargs):
-        
-        we = kwargs['waveform_extractor']
-        if  isinstance(we, str):
+        we = kwargs["waveform_extractor"]
+        if isinstance(we, str):
             we = WaveformExtractor.load(we)
-            kwargs['waveform_extractor'] = we
-        
-        templates = we.get_all_templates(mode='average')
-        
-        kwargs['templates'] = templates
-        
+            kwargs["waveform_extractor"] = we
+
+        templates = we.get_all_templates(mode="average")
+
+        kwargs["templates"] = templates
+
         return kwargs
 
     @classmethod
     def main_function(cls, traces, method_kwargs):
-        
-        peak_sign = method_kwargs['peak_sign']
-        abs_threholds = method_kwargs['abs_threholds']
-        exclude_sweep_size = method_kwargs['exclude_sweep_size']
-        neighbours_mask = method_kwargs['neighbours_mask']
-        templates = method_kwargs['templates']
-        
-        nbefore = method_kwargs['nbefore']
-        nafter = method_kwargs['nafter']
-        
-        margin = method_kwargs['margin']
-        
+        peak_sign = method_kwargs["peak_sign"]
+        abs_threholds = method_kwargs["abs_threholds"]
+        exclude_sweep_size = method_kwargs["exclude_sweep_size"]
+        neighbours_mask = method_kwargs["neighbours_mask"]
+        templates = method_kwargs["templates"]
+
+        nbefore = method_kwargs["nbefore"]
+        nafter = method_kwargs["nafter"]
+
+        margin = method_kwargs["margin"]
+
         if margin > 0:
             peak_traces = traces[margin:-margin, :]
         else:
             peak_traces = traces
-        peak_sample_ind, peak_chan_ind = DetectPeakLocallyExclusive.detect_peaks(peak_traces, peak_sign, abs_threholds, exclude_sweep_size, neighbours_mask)
+        peak_sample_ind, peak_chan_ind = DetectPeakLocallyExclusive.detect_peaks(
+            peak_traces, peak_sign, abs_threholds, exclude_sweep_size, neighbours_mask
+        )
         peak_sample_ind += margin
 
-
         spikes = np.zeros(peak_sample_ind.size, dtype=spike_dtype)
-        spikes['sample_ind'] = peak_sample_ind
-        spikes['channel_ind'] = peak_chan_ind  # TODO need to put the channel from template
-        
+        spikes["sample_index"] = peak_sample_ind
+        spikes["channel_index"] = peak_chan_ind  # TODO need to put the channel from template
+
         # naively take the closest template
         for i in range(peak_sample_ind.size):
             i0 = peak_sample_ind[i] - nbefore
             i1 = peak_sample_ind[i] + nafter
-            
+
             waveforms = traces[i0:i1, :]
-            dist = np.sum(np.sum((templates - waveforms[None, : , :])**2, axis=1), axis=1)
-            cluster_ind = np.argmin(dist)
+            dist = np.sum(np.sum((templates - waveforms[None, :, :]) ** 2, axis=1), axis=1)
+            cluster_index = np.argmin(dist)
 
-            spikes['cluster_ind'][i] = cluster_ind
-            spikes['amplitude'][i] = 0.
+            spikes["cluster_index"][i] = cluster_index
+            spikes["amplitude"][i] = 0.0
 
-        return spikes
+        return spikes
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sortingcomponents/matching/tdc.py` & `spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/matching/tdc.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,374 +1,385 @@
 import numpy as np
 import scipy
-from spikeinterface.core import (WaveformExtractor, get_noise_levels, get_channel_distances)
-from spikeinterface.postprocessing import (get_template_channel_sparsity, get_template_extremum_channel)
+from spikeinterface.core import (
+    WaveformExtractor,
+    get_noise_levels,
+    get_channel_distances,
+    compute_sparsity,
+    get_template_extremum_channel,
+)
 
 from spikeinterface.sortingcomponents.peak_detection import DetectPeakLocallyExclusive
 
-spike_dtype = [('sample_ind', 'int64'), ('channel_ind', 'int64'), ('cluster_ind', 'int64'),
-               ('amplitude', 'float64'), ('segment_ind', 'int64')]
+spike_dtype = [
+    ("sample_index", "int64"),
+    ("channel_index", "int64"),
+    ("cluster_index", "int64"),
+    ("amplitude", "float64"),
+    ("segment_index", "int64"),
+]
 
 from .main import BaseTemplateMatchingEngine
 
 try:
     import numba
     from numba import jit, prange
+
     HAVE_NUMBA = True
 except ImportError:
     HAVE_NUMBA = False
 
+
 class TridesclousPeeler(BaseTemplateMatchingEngine):
     """
     Template-matching ported from Tridesclous sorter.
-    
+
     The idea of this peeler is pretty simple.
     1. Find peaks
     2. order by best amplitues
     3. find nearest template
     4. remove it from traces.
     5. in the residual find peaks again
-    
+
     This method is quite fast but don't give exelent results to resolve
     spike collision when templates have high similarity.
     """
+
     default_params = {
-        'waveform_extractor': None,
-        'peak_sign': 'neg',
-        'peak_shift_ms':  0.2,
-        'detect_threshold': 5,
-        'noise_levels': None,
-        'local_radius_um': 100,
-        'num_closest' : 5,
-        'sample_shift': 3,
-        'ms_before': 0.8,
-        'ms_after': 1.2,
-        'num_peeler_loop':  2,
-        'num_template_try' : 1,
+        "waveform_extractor": None,
+        "peak_sign": "neg",
+        "peak_shift_ms": 0.2,
+        "detect_threshold": 5,
+        "noise_levels": None,
+        "local_radius_um": 100,
+        "num_closest": 5,
+        "sample_shift": 3,
+        "ms_before": 0.8,
+        "ms_after": 1.2,
+        "num_peeler_loop": 2,
+        "num_template_try": 1,
     }
-    
+
     @classmethod
     def initialize_and_check_kwargs(cls, recording, kwargs):
-        
         assert HAVE_NUMBA, "TridesclousPeeler need numba to be installed"
-        
+
         d = cls.default_params.copy()
         d.update(kwargs)
 
-        assert isinstance(d['waveform_extractor'], WaveformExtractor)
-        
-        we = d['waveform_extractor']
+        assert isinstance(d["waveform_extractor"], WaveformExtractor)
+
+        we = d["waveform_extractor"]
         unit_ids = we.unit_ids
         channel_ids = we.channel_ids
-        
-        sr = we.sampling_frequency
 
+        sr = we.sampling_frequency
 
         # TODO load as sharedmem
-        templates = we.get_all_templates(mode='average')
-        d['templates'] = templates
-
-        d['nbefore'] = we.nbefore
-        d['nafter'] = we.nafter
+        templates = we.get_all_templates(mode="average")
+        d["templates"] = templates
 
+        d["nbefore"] = we.nbefore
+        d["nafter"] = we.nafter
 
-        nbefore_short = int(d['ms_before'] * sr / 1000.)
-        nafter_short = int(d['ms_before'] * sr / 1000.)
+        nbefore_short = int(d["ms_before"] * sr / 1000.0)
+        nafter_short = int(d["ms_before"] * sr / 1000.0)
         assert nbefore_short <= we.nbefore
         assert nafter_short <= we.nafter
-        d['nbefore_short'] = nbefore_short
-        d['nafter_short'] = nafter_short
-        s0 = (we.nbefore - nbefore_short)
+        d["nbefore_short"] = nbefore_short
+        d["nafter_short"] = nafter_short
+        s0 = we.nbefore - nbefore_short
         s1 = -(we.nafter - nafter_short)
         if s1 == 0:
             s1 = None
-        templates_short = templates[:, slice(s0,s1), :].copy()
-        d['templates_short'] = templates_short
+        templates_short = templates[:, slice(s0, s1), :].copy()
+        d["templates_short"] = templates_short
 
-        
-        d['peak_shift'] = int(d['peak_shift_ms'] / 1000 * sr)
+        d["peak_shift"] = int(d["peak_shift_ms"] / 1000 * sr)
 
-        if d['noise_levels'] is None:
-            print('TridesclousPeeler : noise should be computed outside')
-            d['noise_levels'] = get_noise_levels(recording)
+        if d["noise_levels"] is None:
+            print("TridesclousPeeler : noise should be computed outside")
+            d["noise_levels"] = get_noise_levels(recording)
+
+        d["abs_threholds"] = d["noise_levels"] * d["detect_threshold"]
 
-        d['abs_threholds'] = d['noise_levels'] * d['detect_threshold']
-    
         channel_distance = get_channel_distances(recording)
-        d['neighbours_mask'] = channel_distance < d['local_radius_um']
-        
-        #
-        #~ template_sparsity_inds = get_template_channel_sparsity(we, method='radius',
-                                  #~ peak_sign=d['peak_sign'], outputs='index', radius_um=d['local_radius_um'])
-        template_sparsity_inds = get_template_channel_sparsity(we, method='snr',
-                                                               peak_sign=d['peak_sign'], outputs='index',
-                                                               threshold=d['detect_threshold'])
-        template_sparsity = np.zeros((unit_ids.size, channel_ids.size), dtype='bool')
+        d["neighbours_mask"] = channel_distance < d["local_radius_um"]
+
+        sparsity = compute_sparsity(we, method="snr", peak_sign=d["peak_sign"], threshold=d["detect_threshold"])
+        template_sparsity_inds = sparsity.unit_id_to_channel_indices
+        template_sparsity = np.zeros((unit_ids.size, channel_ids.size), dtype="bool")
         for unit_index, unit_id in enumerate(unit_ids):
             chan_inds = template_sparsity_inds[unit_id]
-            template_sparsity[unit_index, chan_inds]  = True
-        
-        d['template_sparsity'] = template_sparsity
-        
-        extremum_channel = get_template_extremum_channel(we, peak_sign=d['peak_sign'], outputs='index')
+            template_sparsity[unit_index, chan_inds] = True
+
+        d["template_sparsity"] = template_sparsity
+
+        extremum_channel = get_template_extremum_channel(we, peak_sign=d["peak_sign"], outputs="index")
         # as numpy vector
-        extremum_channel = np.array([extremum_channel[unit_id] for unit_id in unit_ids], dtype='int64')
-        d['extremum_channel'] = extremum_channel
-        
+        extremum_channel = np.array([extremum_channel[unit_id] for unit_id in unit_ids], dtype="int64")
+        d["extremum_channel"] = extremum_channel
+
         channel_locations = we.recording.get_channel_locations()
-        
+
         # TODO try it with real locaion
         unit_locations = channel_locations[extremum_channel]
-        #~ print(unit_locations)
-        
+        # ~ print(unit_locations)
+
         # distance between units
-        unit_distances = scipy.spatial.distance.cdist(unit_locations, unit_locations, metric='euclidean')
-        
+        unit_distances = scipy.spatial.distance.cdist(unit_locations, unit_locations, metric="euclidean")
+
         # seach for closet units and unitary discriminant vector
         closest_units = []
         for unit_ind, unit_id in enumerate(unit_ids):
             order = np.argsort(unit_distances[unit_ind, :])
             closest_u = np.arange(unit_ids.size)[order].tolist()
             closest_u.remove(unit_ind)
-            closest_u = np.array(closest_u[:d['num_closest']])
+            closest_u = np.array(closest_u[: d["num_closest"]])
 
             # compute unitary discriminent vector
-            chans, = np.nonzero(d['template_sparsity'][unit_ind, :])
+            (chans,) = np.nonzero(d["template_sparsity"][unit_ind, :])
             template_sparse = templates[unit_ind, :, :][:, chans]
             closest_vec = []
             # against N closets
             for u in closest_u:
-                vec = (templates[u, :, :][:, chans] - template_sparse)
-                vec /= np.sum(vec ** 2)
+                vec = templates[u, :, :][:, chans] - template_sparse
+                vec /= np.sum(vec**2)
                 closest_vec.append((u, vec))
             # against noise
-            closest_vec.append((None, - template_sparse / np.sum(template_sparse ** 2)))
-            
+            closest_vec.append((None, -template_sparse / np.sum(template_sparse**2)))
+
             closest_units.append(closest_vec)
 
-        d['closest_units'] = closest_units
-        
+        d["closest_units"] = closest_units
+
         # distance channel from unit
-        distances = scipy.spatial.distance.cdist(channel_locations, unit_locations, metric='euclidean')
-        near_cluster_mask = distances < d['local_radius_um']
+        distances = scipy.spatial.distance.cdist(channel_locations, unit_locations, metric="euclidean")
+        near_cluster_mask = distances < d["local_radius_um"]
 
         # nearby cluster for each channel
         possible_clusters_by_channel = []
-        for channel_ind in range(distances.shape[0]):
-            cluster_inds, = np.nonzero(near_cluster_mask[channel_ind, :])
+        for channel_index in range(distances.shape[0]):
+            (cluster_inds,) = np.nonzero(near_cluster_mask[channel_index, :])
             possible_clusters_by_channel.append(cluster_inds)
-        
-        d['possible_clusters_by_channel'] = possible_clusters_by_channel
-        d['possible_shifts'] = np.arange(-d['sample_shift'], d['sample_shift'] +1, dtype='int64')
 
-        return d        
+        d["possible_clusters_by_channel"] = possible_clusters_by_channel
+        d["possible_shifts"] = np.arange(-d["sample_shift"], d["sample_shift"] + 1, dtype="int64")
+
+        return d
 
     @classmethod
     def serialize_method_kwargs(cls, kwargs):
         kwargs = dict(kwargs)
-        
+
         # remove waveform_extractor
-        kwargs.pop('waveform_extractor')
+        kwargs.pop("waveform_extractor")
         return kwargs
 
     @classmethod
     def unserialize_in_worker(cls, kwargs):
         return kwargs
 
     @classmethod
     def get_margin(cls, recording, kwargs):
-        margin = 2 * (kwargs['nbefore'] + kwargs['nafter'])
+        margin = 2 * (kwargs["nbefore"] + kwargs["nafter"])
         return margin
 
     @classmethod
     def main_function(cls, traces, d):
-        
         traces = traces.copy()
-        
+
         all_spikes = []
         level = 0
         while True:
             spikes = _tdc_find_spikes(traces, d, level=level)
-            keep = (spikes['cluster_ind'] >= 0)
-            
+            keep = spikes["cluster_index"] >= 0
+
             if not np.any(keep):
                 break
             all_spikes.append(spikes[keep])
-            
+
             level += 1
-            
-            if level == d['num_peeler_loop']:
+
+            if level == d["num_peeler_loop"]:
                 break
-        
+
         if len(all_spikes) > 0:
             all_spikes = np.concatenate(all_spikes)
-            order = np.argsort(all_spikes['sample_ind'])
+            order = np.argsort(all_spikes["sample_index"])
             all_spikes = all_spikes[order]
         else:
             all_spikes = np.zeros(0, dtype=spike_dtype)
 
         return all_spikes
 
 
 def _tdc_find_spikes(traces, d, level=0):
-        peak_sign = d['peak_sign']
-        templates = d['templates']
-        templates_short = d['templates_short']
-        margin = d['margin']
-        possible_clusters_by_channel = d['possible_clusters_by_channel']
-
-        peak_traces = traces[margin // 2:-margin // 2, :]
-        peak_sample_ind, peak_chan_ind = DetectPeakLocallyExclusive.detect_peaks(peak_traces, peak_sign,
-                                                                                 d['abs_threholds'], d['peak_shift'],
-                                                                                 d['neighbours_mask'])
-        peak_sample_ind += margin // 2
-
-        peak_amplitude = traces[peak_sample_ind, peak_chan_ind]
-        order = np.argsort(np.abs(peak_amplitude))[::-1]
-        peak_sample_ind = peak_sample_ind[order]
-        peak_chan_ind = peak_chan_ind[order]
-
-        spikes = np.zeros(peak_sample_ind.size, dtype=spike_dtype)
-        spikes['sample_ind'] = peak_sample_ind
-        spikes['channel_ind'] = peak_chan_ind  # TODO need to put the channel from template
-
-        possible_shifts = d['possible_shifts']
-        distances_shift = np.zeros(possible_shifts.size)
-
-        for i in range(peak_sample_ind.size):
-            sample_ind = peak_sample_ind[i]
-
-            chan_ind = peak_chan_ind[i]
-            possible_clusters = possible_clusters_by_channel[chan_ind]
-            
-            if possible_clusters.size > 0:
-                #~ s0 = sample_ind - d['nbefore']
-                #~ s1 = sample_ind + d['nafter']
-
-                #~ wf = traces[s0:s1, :]
-
-                s0 = sample_ind - d['nbefore_short']
-                s1 = sample_ind + d['nafter_short']
-                wf_short = traces[s0:s1, :]
-                
-                ## pure numpy with cluster spasity
-                # distances = np.sum(np.sum((templates[possible_clusters, :, :] - wf[None, : , :])**2, axis=1), axis=1)
-
-                ## pure numpy with cluster+channel spasity
-                # union_channels, = np.nonzero(np.any(d['template_sparsity'][possible_clusters, :], axis=0))
-                # distances = np.sum(np.sum((templates[possible_clusters][:, :, union_channels] - wf[: , union_channels][None, : :])**2, axis=1), axis=1)
-                
-                ## numba with cluster+channel spasity
-                union_channels = np.any(d['template_sparsity'][possible_clusters, :], axis=0)
-                # distances = numba_sparse_dist(wf, templates, union_channels, possible_clusters)
-                distances = numba_sparse_dist(wf_short, templates_short, union_channels, possible_clusters)
-                
-                
-                # DEBUG
-                #~ ind = np.argmin(distances)
-                #~ cluster_ind = possible_clusters[ind]
-                
-                for ind in np.argsort(distances)[:d['num_template_try']]:
-                    cluster_ind = possible_clusters[ind]
-
-                    chan_sparsity = d['template_sparsity'][cluster_ind, :]
-                    template_sparse = templates[cluster_ind, :, :][:, chan_sparsity]
-
-                    # find best shift
-                    
-                    ## pure numpy version
-                    # for s, shift in enumerate(possible_shifts):
-                    #     wf_shift = traces[s0 + shift: s1 + shift, chan_sparsity]
-                    #     distances_shift[s] = np.sum((template_sparse - wf_shift)**2)
-                    # ind_shift = np.argmin(distances_shift)
-                    # shift = possible_shifts[ind_shift]
-                    
-                    ## numba version
-                    numba_best_shift(traces, templates[cluster_ind, :, :], sample_ind, d['nbefore'],
-                                     possible_shifts, distances_shift, chan_sparsity)
-                    ind_shift = np.argmin(distances_shift)
-                    shift = possible_shifts[ind_shift]
-
-                    sample_ind = sample_ind + shift
-                    s0 = sample_ind - d['nbefore']
-                    s1 = sample_ind + d['nafter']
-                    wf_sparse = traces[s0:s1, chan_sparsity]
-
-                    # accept or not
-
-                    centered = wf_sparse - template_sparse
-                    accepted = True
-                    for other_ind, other_vector in d['closest_units'][cluster_ind]:
-                        v = np.sum(centered * other_vector)
-                        if np.abs(v) >0.5:
-                            accepted = False
-                            break
-                    
-                    if accepted:
-                        #~ if ind != np.argsort(distances)[0]:
-                            #~ print('not first one', np.argsort(distances), ind)
+    peak_sign = d["peak_sign"]
+    templates = d["templates"]
+    templates_short = d["templates_short"]
+    margin = d["margin"]
+    possible_clusters_by_channel = d["possible_clusters_by_channel"]
+
+    peak_traces = traces[margin // 2 : -margin // 2, :]
+    peak_sample_ind, peak_chan_ind = DetectPeakLocallyExclusive.detect_peaks(
+        peak_traces, peak_sign, d["abs_threholds"], d["peak_shift"], d["neighbours_mask"]
+    )
+    peak_sample_ind += margin // 2
+
+    peak_amplitude = traces[peak_sample_ind, peak_chan_ind]
+    order = np.argsort(np.abs(peak_amplitude))[::-1]
+    peak_sample_ind = peak_sample_ind[order]
+    peak_chan_ind = peak_chan_ind[order]
+
+    spikes = np.zeros(peak_sample_ind.size, dtype=spike_dtype)
+    spikes["sample_index"] = peak_sample_ind
+    spikes["channel_index"] = peak_chan_ind  # TODO need to put the channel from template
+
+    possible_shifts = d["possible_shifts"]
+    distances_shift = np.zeros(possible_shifts.size)
+
+    for i in range(peak_sample_ind.size):
+        sample_index = peak_sample_ind[i]
+
+        chan_ind = peak_chan_ind[i]
+        possible_clusters = possible_clusters_by_channel[chan_ind]
+
+        if possible_clusters.size > 0:
+            # ~ s0 = sample_index - d['nbefore']
+            # ~ s1 = sample_index + d['nafter']
+
+            # ~ wf = traces[s0:s1, :]
+
+            s0 = sample_index - d["nbefore_short"]
+            s1 = sample_index + d["nafter_short"]
+            wf_short = traces[s0:s1, :]
+
+            ## pure numpy with cluster spasity
+            # distances = np.sum(np.sum((templates[possible_clusters, :, :] - wf[None, : , :])**2, axis=1), axis=1)
+
+            ## pure numpy with cluster+channel spasity
+            # union_channels, = np.nonzero(np.any(d['template_sparsity'][possible_clusters, :], axis=0))
+            # distances = np.sum(np.sum((templates[possible_clusters][:, :, union_channels] - wf[: , union_channels][None, : :])**2, axis=1), axis=1)
+
+            ## numba with cluster+channel spasity
+            union_channels = np.any(d["template_sparsity"][possible_clusters, :], axis=0)
+            # distances = numba_sparse_dist(wf, templates, union_channels, possible_clusters)
+            distances = numba_sparse_dist(wf_short, templates_short, union_channels, possible_clusters)
+
+            # DEBUG
+            # ~ ind = np.argmin(distances)
+            # ~ cluster_index = possible_clusters[ind]
+
+            for ind in np.argsort(distances)[: d["num_template_try"]]:
+                cluster_index = possible_clusters[ind]
+
+                chan_sparsity = d["template_sparsity"][cluster_index, :]
+                template_sparse = templates[cluster_index, :, :][:, chan_sparsity]
+
+                # find best shift
+
+                ## pure numpy version
+                # for s, shift in enumerate(possible_shifts):
+                #     wf_shift = traces[s0 + shift: s1 + shift, chan_sparsity]
+                #     distances_shift[s] = np.sum((template_sparse - wf_shift)**2)
+                # ind_shift = np.argmin(distances_shift)
+                # shift = possible_shifts[ind_shift]
+
+                ## numba version
+                numba_best_shift(
+                    traces,
+                    templates[cluster_index, :, :],
+                    sample_index,
+                    d["nbefore"],
+                    possible_shifts,
+                    distances_shift,
+                    chan_sparsity,
+                )
+                ind_shift = np.argmin(distances_shift)
+                shift = possible_shifts[ind_shift]
+
+                sample_index = sample_index + shift
+                s0 = sample_index - d["nbefore"]
+                s1 = sample_index + d["nafter"]
+                wf_sparse = traces[s0:s1, chan_sparsity]
+
+                # accept or not
+
+                centered = wf_sparse - template_sparse
+                accepted = True
+                for other_ind, other_vector in d["closest_units"][cluster_index]:
+                    v = np.sum(centered * other_vector)
+                    if np.abs(v) > 0.5:
+                        accepted = False
                         break
 
                 if accepted:
-                    amplitude = 1.
-                    
-                    # remove template
-                    template = templates[cluster_ind, :, :]
-                    s0 = sample_ind - d['nbefore']
-                    s1 = sample_ind + d['nafter']
-                    traces[s0:s1, :] -= template * amplitude
-                    
-                else:
-                    cluster_ind = -1
-                    amplitude = 0.
-                
+                    # ~ if ind != np.argsort(distances)[0]:
+                    # ~ print('not first one', np.argsort(distances), ind)
+                    break
+
+            if accepted:
+                amplitude = 1.0
+
+                # remove template
+                template = templates[cluster_index, :, :]
+                s0 = sample_index - d["nbefore"]
+                s1 = sample_index + d["nafter"]
+                traces[s0:s1, :] -= template * amplitude
+
             else:
-                cluster_ind = -1
-                amplitude = 0.
-            
-            spikes['cluster_ind'][i] = cluster_ind
-            spikes['amplitude'][i] =amplitude
+                cluster_index = -1
+                amplitude = 0.0
+
+        else:
+            cluster_index = -1
+            amplitude = 0.0
+
+        spikes["cluster_index"][i] = cluster_index
+        spikes["amplitude"][i] = amplitude
 
-        return spikes    
+    return spikes
 
 
 if HAVE_NUMBA:
+
     @jit(nopython=True)
     def numba_sparse_dist(wf, templates, union_channels, possible_clusters):
         """
-        numba implementation that compute distance from template with sparsity 
+        numba implementation that compute distance from template with sparsity
         handle by two separate vectors
         """
         total_cluster, width, num_chan = templates.shape
         num_cluster = possible_clusters.shape[0]
         distances = np.zeros((num_cluster,), dtype=np.float32)
         for i in prange(num_cluster):
-            cluster_ind = possible_clusters[i]
-            sum_dist = 0.
+            cluster_index = possible_clusters[i]
+            sum_dist = 0.0
             for chan_ind in range(num_chan):
                 if union_channels[chan_ind]:
                     for s in range(width):
                         v = wf[s, chan_ind]
-                        t = templates[cluster_ind, s, chan_ind]
+                        t = templates[cluster_index, s, chan_ind]
                         sum_dist += (v - t) ** 2
             distances[i] = sum_dist
         return distances
 
     @jit(nopython=True)
-    def numba_best_shift(traces, template, sample_ind, nbefore, possible_shifts, distances_shift, chan_sparsity):
+    def numba_best_shift(traces, template, sample_index, nbefore, possible_shifts, distances_shift, chan_sparsity):
         """
         numba implementation to compute several sample shift before template substraction
         """
         width, num_chan = template.shape
         n_shift = possible_shifts.size
         for i in range(n_shift):
             shift = possible_shifts[i]
-            sum_dist = 0.
+            sum_dist = 0.0
             for chan_ind in range(num_chan):
                 if chan_sparsity[chan_ind]:
                     for s in range(width):
-                        v = traces[sample_ind - nbefore + s +shift, chan_ind]
+                        v = traces[sample_index - nbefore + s + shift, chan_ind]
                         t = template[s, chan_ind]
                         sum_dist += (v - t) ** 2
             distances_shift[i] = sum_dist
-        
+
         return distances_shift
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sortingcomponents/motion_correction.py` & `spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/motion_interpolation.py`

 * *Files 8% similar despite different names*

```diff
@@ -12,17 +12,23 @@
 # try:
 #     import numba
 #     HAVE_NUMBA = True
 # except ImportError:
 #     HAVE_NUMBA = False
 
 
-def correct_motion_on_peaks(peaks, peak_locations, times,
-                            motion, temporal_bins, spatial_bins,
-                            direction='y', progress_bar=False):
+def correct_motion_on_peaks(
+    peaks,
+    peak_locations,
+    times,
+    motion,
+    temporal_bins,
+    spatial_bins,
+    direction="y",
+):
     """
     Given the output of estimate_motion(), apply inverse motion on peak locations.
 
     Parameters
     ----------
     peaks: np.array
         peaks vector
@@ -31,44 +37,54 @@
     times: np.array
         times vector of recording
     motion: np.array 2D
         motion.shape[0] equal temporal_bins.shape[0]
         motion.shape[1] equal 1 when "rigid" motion equal temporal_bins.shape[0] when "non-rigid"
     temporal_bins: np.array
         Temporal bins in second.
-    spatial_bins: None or np.array
-        Bins for non-rigid motion. If None, rigid motion is used
+    spatial_bins: np.array
+        Bins for non-rigid motion. If spatial_bins.sahpe[0] == 1 then rigid motion is used.
 
     Returns
     -------
     corrected_peak_locations: np.array
         Motion-corrected peak locations
     """
     corrected_peak_locations = peak_locations.copy()
 
-    if spatial_bins is None:
+    if spatial_bins.shape[0] == 1:
         # rigid motion interpolation 1D
         sample_bins = np.searchsorted(times, temporal_bins)
         f = scipy.interpolate.interp1d(sample_bins, motion[:, 0], bounds_error=False, fill_value="extrapolate")
-        shift = f(peaks['sample_ind'])
+        shift = f(peaks["sample_index"])
         corrected_peak_locations[direction] -= shift
     else:
         # non rigid motion = interpolation 2D
-        f = scipy.interpolate.RegularGridInterpolator((temporal_bins, spatial_bins), motion,
-                                                      method='linear', bounds_error=False, fill_value=None)
-        spike_times = times[peaks['sample_ind']]
+        f = scipy.interpolate.RegularGridInterpolator(
+            (temporal_bins, spatial_bins), motion, method="linear", bounds_error=False, fill_value=None
+        )
+        spike_times = times[peaks["sample_index"]]
         shift = f(np.c_[spike_times, peak_locations[direction]])
         corrected_peak_locations[direction] -= shift
 
     return corrected_peak_locations
 
 
-def correct_motion_on_traces(traces, times, channel_locations, motion, temporal_bins, spatial_bins, direction=1,
-                             channel_inds=None, spatial_interpolation_method='kriging',
-                             spatial_interpolation_kwargs={}):
+def interpolate_motion_on_traces(
+    traces,
+    times,
+    channel_locations,
+    motion,
+    temporal_bins,
+    spatial_bins,
+    direction=1,
+    channel_inds=None,
+    spatial_interpolation_method="kriging",
+    spatial_interpolation_kwargs={},
+):
     """
     Apply inverse motion with spatial interpolation on traces.
 
     Traces can be full traces, but also waveforms snippets.
 
     Parameters
     ----------
@@ -89,60 +105,64 @@
     channel_inds: None or list
         If not None, interpolate only a subset of channels.
     spatial_interpolation_method: str in ('idw', 'kriging')
         * idw : Inverse Distance Weighing
         * kriging : kilosort2.5 like
     spatial_interpolation_kwargs:
         * specific option for the interpolation method
-    
+
     Returns
     -------
     channel_motions: np.array
         Shift over time by channel
         Shape (times.shape[0], channel_location.shape[0])
     """
     # assert HAVE_NUMBA
     assert times.shape[0] == traces.shape[0]
 
     if channel_inds is None:
         traces_corrected = np.zeros(traces.shape, dtype=traces.dtype)
     else:
         channel_inds = np.asarray(channel_inds)
         traces_corrected = np.zeros((traces.shape[0], channel_inds.size), dtype=traces.dtype)
-    
+
     # regroup times by closet temporal_bins
     bin_inds = _get_closest_ind(temporal_bins, times)
 
-    # inperpolation kernel will be the same per temporal bin   
+    # inperpolation kernel will be the same per temporal bin
     for bin_ind in np.unique(bin_inds):
-
         # Step 1 : channel motion
-        if spatial_bins is None:
-            # rigid motion : same motion for all channels
+        if spatial_bins.shape[0] == 0:
+            # rigid motion : same motion for all channels
             channel_motions = motion[bin_ind, 0]
         else:
             # non rigid : interpolation channel motion for this temporal bin
-            f = scipy.interpolate.interp1d(spatial_bins, motion[bin_ind, :], kind='linear',
-                                           axis=0, bounds_error=False, fill_value="extrapolate")
+            f = scipy.interpolate.interp1d(
+                spatial_bins, motion[bin_ind, :], kind="linear", axis=0, bounds_error=False, fill_value="extrapolate"
+            )
             locs = channel_locations[:, direction]
             channel_motions = f(locs)
         channel_locations_moved = channel_locations.copy()
         channel_locations_moved[:, direction] += channel_motions
-        # channel_locations_moved[:, direction] -= channel_motions
+        # channel_locations_moved[:, direction] -= channel_motions
 
         if channel_inds is not None:
             channel_locations_moved = channel_locations_moved[channel_inds]
-        
-        drift_kernel = get_spatial_interpolation_kernel(channel_locations, channel_locations_moved, dtype='float32',
-                                                        method=spatial_interpolation_method,
-                                                        **spatial_interpolation_kwargs)
-        
-        i0 = np.searchsorted(bin_inds, bin_ind, side='left')
-        i1 = np.searchsorted(bin_inds, bin_ind, side='right')
-        
+
+        drift_kernel = get_spatial_interpolation_kernel(
+            channel_locations,
+            channel_locations_moved,
+            dtype="float32",
+            method=spatial_interpolation_method,
+            **spatial_interpolation_kwargs,
+        )
+
+        i0 = np.searchsorted(bin_inds, bin_ind, side="left")
+        i1 = np.searchsorted(bin_inds, bin_ind, side="right")
+
         # here we use a simple np.matmul even if dirft_kernel can be super sparse.
         # because the speed for a sparse matmul is not so good when we disable multi threaad (due multi processing
         # in ChunkRecordingExecutor)
         traces_corrected[i0:i1] = traces[i0:i1] @ drift_kernel
 
     return traces_corrected
 
@@ -159,39 +179,40 @@
 #         data_out: num_sample, num_chan_out
 #         sparse_chans: num_chan_out, num_sparse
 #         weights: num_chan_out, num_sparse
 #         """
 #         num_samples = data_in.shape[0]
 #         num_chan_out = data_out.shape[1]
 #         num_sparse = sparse_chans.shape[1]
-#         # for sample_ind in range(num_samples):
-#         for sample_ind in numba.prange(num_samples):
+#         # for sample_index in range(num_samples):
+#         for sample_index in numba.prange(num_samples):
 #             for out_chan in range(num_chan_out):
 #                 v = 0
 #                 for i in range(num_sparse):
 #                     in_chan = sparse_chans[out_chan, i]
-#                     v +=  weights[out_chan, i] * data_in[sample_ind, in_chan]
-#                 data_out[sample_ind, out_chan] = v
+#                     v +=  weights[out_chan, i] * data_in[sample_index, in_chan]
+#                 data_out[sample_index, out_chan] = v
 
 
 def _get_closest_ind(array, values):
     # https://stackoverflow.com/questions/2566412/find-nearest-value-in-numpy-array
 
     # get insert positions
     idxs = np.searchsorted(array, values, side="left")
 
     # find indexes where previous index is closer
-    prev_idx_is_less = ((idxs == len(array)) | (np.fabs(values - array[np.maximum(idxs-1, 0)]) <
-                                                np.fabs(values - array[np.minimum(idxs, len(array)-1)])))
+    prev_idx_is_less = (idxs == len(array)) | (
+        np.fabs(values - array[np.maximum(idxs - 1, 0)]) < np.fabs(values - array[np.minimum(idxs, len(array) - 1)])
+    )
     idxs[prev_idx_is_less] -= 1
 
     return idxs
 
 
-class CorrectMotionRecording(BasePreprocessor):
+class InterpolateMotionRecording(BasePreprocessor):
     """
     Recording that corrects motion on-the-fly given a motion vector estimation (rigid or non-rigid).
     This internally applies a spatial interpolation on the original traces after reversing the motion.
     `estimate_motion()` must be called before this to estimate the motion vector.
 
     Parameters
     ----------
@@ -226,123 +247,172 @@
 
         * 'remove_channels': remove channels on the border, the recording has less channels
         * 'force_extrapolate': keep all channel and force extrapolation (can lead to strange signal)
         * 'force_zeros': keep all channel but set zeros when outside (force_extrapolate=False)
 
     Returns
     -------
-    corrected_recording: CorrectMotionRecording
+    corrected_recording: InterpolateMotionRecording
         Recording after motion correction
     """
-    name = 'correct_motion'
 
-    def __init__(self, recording, motion, temporal_bins, spatial_bins, direction=1,
-                 border_mode='remove_channels', spatial_interpolation_method='kriging',
-                 sigma_um=20., p=1, num_closest=3):
-        assert recording.get_num_segments() == 1, 'correct_motion() is only available for single-segment recordings'
-        
+    name = "correct_motion"
+
+    def __init__(
+        self,
+        recording,
+        motion,
+        temporal_bins,
+        spatial_bins,
+        direction=1,
+        border_mode="remove_channels",
+        spatial_interpolation_method="kriging",
+        sigma_um=20.0,
+        p=1,
+        num_closest=3,
+    ):
+        assert recording.get_num_segments() == 1, "correct_motion() is only available for single-segment recordings"
+
         # force as arrays
         temporal_bins = np.asarray(temporal_bins)
         motion = np.asarray(motion)
-        if spatial_bins is not None:
-            spatial_bins = np.asarray(spatial_bins)
-
+        spatial_bins = np.asarray(spatial_bins)
 
         channel_locations = recording.get_channel_locations()
-        assert channel_locations.ndim >= direction, (f"'direction' {direction} not available. "
-                                                     f"Channel locations have {channel_locations.ndim} dimensions.")
+        assert channel_locations.ndim >= direction, (
+            f"'direction' {direction} not available. " f"Channel locations have {channel_locations.ndim} dimensions."
+        )
         spatial_interpolation_kwargs = dict(sigma_um=sigma_um, p=p, num_closest=num_closest)
-        if border_mode == 'remove_channels':
+        if border_mode == "remove_channels":
             locs = channel_locations[:, direction]
             l0, l1 = np.min(channel_locations[:, direction]), np.max(channel_locations[:, direction])
 
             # compute max and min motion (with interpolation)
             # and check if channels are inside
-            channel_inside = np.ones(locs.shape[0], dtype='bool')
+            channel_inside = np.ones(locs.shape[0], dtype="bool")
             for operator in (np.max, np.min):
-                if spatial_bins is None:
+                if spatial_bins.shape[0] == 1:
                     best_motions = operator(motion[:, 0])
                 else:
                     # non rigid : interpolation channel motion for this temporal bin
-                    f = scipy.interpolate.interp1d(spatial_bins, operator(motion[:, :], axis=0), kind='linear',
-                                           axis=0, bounds_error=False, fill_value="extrapolate")
+                    f = scipy.interpolate.interp1d(
+                        spatial_bins,
+                        operator(motion[:, :], axis=0),
+                        kind="linear",
+                        axis=0,
+                        bounds_error=False,
+                        fill_value="extrapolate",
+                    )
                     best_motions = f(locs)
                 channel_inside &= ((locs + best_motions) >= l0) & ((locs + best_motions) <= l1)
 
-            channel_inds,  = np.nonzero(channel_inside)
+            (channel_inds,) = np.nonzero(channel_inside)
             channel_ids = recording.channel_ids[channel_inds]
-            spatial_interpolation_kwargs['force_extrapolate'] = False
-        elif border_mode == 'force_extrapolate':
+            spatial_interpolation_kwargs["force_extrapolate"] = False
+        elif border_mode == "force_extrapolate":
             channel_inds = None
             channel_ids = recording.channel_ids
-            spatial_interpolation_kwargs['force_extrapolate'] = True
-        elif border_mode == 'force_zeros':
+            spatial_interpolation_kwargs["force_extrapolate"] = True
+        elif border_mode == "force_zeros":
             channel_inds = None
             channel_ids = recording.channel_ids
-            spatial_interpolation_kwargs['force_extrapolate'] = False
+            spatial_interpolation_kwargs["force_extrapolate"] = False
         else:
-            raise ValueError('Wrong border_mode')
-        
+            raise ValueError("Wrong border_mode")
+
         BasePreprocessor.__init__(self, recording, channel_ids=channel_ids)
 
-        if border_mode == 'remove_channels':
+        if border_mode == "remove_channels":
             # change the wiring of the probe
             # TODO this is also done in ChannelSliceRecording, this should be done in a common place
-            contact_vector = self.get_property('contact_vector')
+            contact_vector = self.get_property("contact_vector")
             if contact_vector is not None:
-                contact_vector['device_channel_indices'] = np.arange(len(channel_ids), dtype='int64')
-                self.set_property('contact_vector', contact_vector)
+                contact_vector["device_channel_indices"] = np.arange(len(channel_ids), dtype="int64")
+                self.set_property("contact_vector", contact_vector)
 
         for parent_segment in recording._recording_segments:
-            rec_segment = CorrectMotionRecordingSegment(parent_segment, channel_locations,
-                                                        motion, temporal_bins, spatial_bins, direction,
-                                                        spatial_interpolation_method, spatial_interpolation_kwargs, channel_inds, )
+            rec_segment = InterpolateMotionRecordingSegment(
+                parent_segment,
+                channel_locations,
+                motion,
+                temporal_bins,
+                spatial_bins,
+                direction,
+                spatial_interpolation_method,
+                spatial_interpolation_kwargs,
+                channel_inds,
+            )
             self.add_recording_segment(rec_segment)
 
-        self._kwargs = dict(recording=recording.to_dict(), motion=motion, temporal_bins=temporal_bins,
-                            spatial_bins=spatial_bins, direction=direction, border_mode=border_mode,
-                            spatial_interpolation_method=spatial_interpolation_method,
-                            sigma_um=sigma_um, p=p, num_closest=num_closest)
-        # self.is_dumpable= False
-
-
-class CorrectMotionRecordingSegment(BasePreprocessorSegment):
-    def __init__(self, parent_recording_segment, channel_locations, motion, temporal_bins, spatial_bins, direction,
-                 spatial_interpolation_method, spatial_interpolation_kwargs, channel_inds):
+        self._kwargs = dict(
+            recording=recording,
+            motion=motion,
+            temporal_bins=temporal_bins,
+            spatial_bins=spatial_bins,
+            direction=direction,
+            border_mode=border_mode,
+            spatial_interpolation_method=spatial_interpolation_method,
+            sigma_um=sigma_um,
+            p=p,
+            num_closest=num_closest,
+        )
+
+
+class InterpolateMotionRecordingSegment(BasePreprocessorSegment):
+    def __init__(
+        self,
+        parent_recording_segment,
+        channel_locations,
+        motion,
+        temporal_bins,
+        spatial_bins,
+        direction,
+        spatial_interpolation_method,
+        spatial_interpolation_kwargs,
+        channel_inds,
+    ):
         BasePreprocessorSegment.__init__(self, parent_recording_segment)
         self.channel_locations = channel_locations
         self.motion = motion
         self.temporal_bins = temporal_bins
         self.spatial_bins = spatial_bins
         self.direction = direction
         self.spatial_interpolation_method = spatial_interpolation_method
         self.spatial_interpolation_kwargs = spatial_interpolation_kwargs
         self.channel_inds = channel_inds
 
     def get_traces(self, start_frame, end_frame, channel_indices):
         if self.time_vector is not None:
-            raise NotImplementedError('time_vector for CorrectMotionRecording do not work because temporal_bins start from 0')
-            # times = np.asarray(self.time_vector[start_frame:end_frame])
+            raise NotImplementedError(
+                "time_vector for InterpolateMotionRecording do not work because temporal_bins start from 0"
+            )
+            # times = np.asarray(self.time_vector[start_frame:end_frame])
         else:
-            times = np.arange((end_frame or self.get_num_samples()) - (start_frame or 0), dtype='float64')
+            times = np.arange((end_frame or self.get_num_samples()) - (start_frame or 0), dtype="float64")
             times /= self.sampling_frequency
             t0 = start_frame / self.sampling_frequency
             # if self.t_start is not None:
             #     t0 = t0 + self.t_start
             times += t0
 
         traces = self.parent_recording_segment.get_traces(start_frame, end_frame, channel_indices=slice(None))
 
-        trace2 = correct_motion_on_traces(traces, times, self.channel_locations, self.motion,
-                                          self.temporal_bins, self.spatial_bins, direction=self.direction,
-                                          channel_inds=self.channel_inds,
-                                          spatial_interpolation_method=self.spatial_interpolation_method,
-                                          spatial_interpolation_kwargs=self.spatial_interpolation_kwargs)
+        trace2 = interpolate_motion_on_traces(
+            traces,
+            times,
+            self.channel_locations,
+            self.motion,
+            self.temporal_bins,
+            self.spatial_bins,
+            direction=self.direction,
+            channel_inds=self.channel_inds,
+            spatial_interpolation_method=self.spatial_interpolation_method,
+            spatial_interpolation_kwargs=self.spatial_interpolation_kwargs,
+        )
 
         if channel_indices is not None:
             trace2 = trace2[:, channel_indices]
 
         return trace2
 
 
-correct_motion = define_function_from_class(source_class=CorrectMotionRecording,
-                                            name="correct_motion")
+interpolate_motion = define_function_from_class(source_class=InterpolateMotionRecording, name="correct_motion")
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sortingcomponents/motion_estimation.py` & `spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/motion_estimation.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,28 +1,44 @@
 import numpy as np
 from tqdm.auto import tqdm, trange
 import scipy.interpolate
 
 try:
     import torch
     import torch.nn.functional as F
+
     HAVE_TORCH = True
 except ImportError:
     HAVE_TORCH = False
 
 from .tools import make_multi_method_doc
 
 
-def estimate_motion(recording, peaks, peak_locations,
-                    direction='y', bin_duration_s=10., bin_um=10., margin_um=0.,
-                    rigid=False, win_shape='gaussian', win_step_um=50., win_sigma_um=150.,
-                    post_clean=False, speed_threshold=30, sigma_smooth_s=None,
-                    method='decentralized',
-                    output_extra_check=False, progress_bar=False,
-                    upsample_to_histogram_bin=False, verbose=False, **method_kwargs):
+def estimate_motion(
+    recording,
+    peaks,
+    peak_locations,
+    direction="y",
+    bin_duration_s=10.0,
+    bin_um=10.0,
+    margin_um=0.0,
+    rigid=False,
+    win_shape="gaussian",
+    win_step_um=50.0,
+    win_sigma_um=150.0,
+    post_clean=False,
+    speed_threshold=30,
+    sigma_smooth_s=None,
+    method="decentralized",
+    output_extra_check=False,
+    progress_bar=False,
+    upsample_to_histogram_bin=False,
+    verbose=False,
+    **method_kwargs,
+):
     """
     Estimate motion for given peaks and after their localization.
 
     Note that the way you detect peak locations (center of mass/monopolar triangulation) have an impact on the result.
 
     Parameters
     ----------
@@ -92,56 +108,70 @@
         Shape (temporal bins, spatial bins)
         motion.shape[0] = temporal_bins.shape[0]
         motion.shape[1] = 1 (rigid) or spatial_bins.shape[1] (non rigid)
         If upsample_to_histogram_bin, motion.shape[1] corresponds to spatial
         bins given by bin_um.
     temporal_bins: numpy.array 1d
         temporal bins (bin center)
-    spatial_bins: numpy.array 1d or None
-        If rigid then None
-        else motion.shape[1]
+    spatial_bins: numpy.array 1d
+        Windows center.
+        spatial_bins.shape[0] == motion.shape[1]
+        If rigid then spatial_bins.shape[0] == 1
     extra_check: dict
         Optional output if `output_extra_check=True`
         This dict contain histogram, pairwise_displacement usefull for ploting.
     """
     # TODO handle multi segment one day
     assert recording.get_num_segments() == 1
 
     if output_extra_check:
         extra_check = {}
     else:
         extra_check = None
 
     # contact positions
     probe = recording.get_probe()
-    dim = ['x', 'y', 'z'].index(direction)
+    dim = ["x", "y", "z"].index(direction)
     contact_pos = probe.contact_positions[:, dim]
 
     # spatial bins
     spatial_bin_edges = get_spatial_bin_edges(recording, direction, margin_um, bin_um)
 
     # get windows
-    non_rigid_windows, non_rigid_window_centers = get_windows(rigid, bin_um, contact_pos, spatial_bin_edges,
-                                                              margin_um, win_step_um, win_sigma_um, win_shape)
+    non_rigid_windows, non_rigid_window_centers = get_windows(
+        rigid, bin_um, contact_pos, spatial_bin_edges, margin_um, win_step_um, win_sigma_um, win_shape
+    )
 
     if output_extra_check:
-        extra_check['non_rigid_windows'] = non_rigid_windows
+        extra_check["non_rigid_windows"] = non_rigid_windows
 
     # run method
     method_class = estimate_motion_methods[method]
-    motion, temporal_bins = method_class.run(recording, peaks, peak_locations, direction, bin_duration_s, bin_um,
-                                             spatial_bin_edges, non_rigid_windows, verbose, progress_bar, extra_check,
-                                             **method_kwargs)
+    motion, temporal_bins = method_class.run(
+        recording,
+        peaks,
+        peak_locations,
+        direction,
+        bin_duration_s,
+        bin_um,
+        spatial_bin_edges,
+        non_rigid_windows,
+        verbose,
+        progress_bar,
+        extra_check,
+        **method_kwargs,
+    )
 
     # replace nan by zeros
     motion[np.isnan(motion)] = 0
 
     if post_clean:
-        motion = clean_motion_vector(motion, temporal_bins, bin_duration_s,
-                                     speed_threshold=speed_threshold, sigma_smooth_s=sigma_smooth_s)
+        motion = clean_motion_vector(
+            motion, temporal_bins, bin_duration_s, speed_threshold=speed_threshold, sigma_smooth_s=sigma_smooth_s
+        )
 
     if upsample_to_histogram_bin is None:
         upsample_to_histogram_bin = not rigid
     if upsample_to_histogram_bin:
         extra_check["motion"] = motion
         extra_check["non_rigid_window_centers"] = non_rigid_window_centers
         non_rigid_windows = np.array(non_rigid_windows)
@@ -151,15 +181,14 @@
 
     if output_extra_check:
         return motion, temporal_bins, non_rigid_window_centers, extra_check
     else:
         return motion, temporal_bins, non_rigid_window_centers
 
 
-
 class DecentralizedRegistration:
     """
     Method developed by the Paninski's group from Columbia university:
     Charlie Windolf, Julien Boussard, Erdem Varol, Hyun Dong Lee
 
     This method is also known as DREDGe, but this implemenation does not use LFP signals.
 
@@ -174,16 +203,23 @@
 
 
     Here are some various implementations by the original team:
     https://github.com/int-brain-lab/spikes_localization_registration/blob/main/registration_pipeline/image_based_motion_estimate.py#L211
     https://github.com/cwindolf/spike-psvae/tree/main/spike_psvae
     https://github.com/evarol/DREDge
     """
-    name = 'decentralized'
+
+    name = "decentralized"
     params_doc = """
+    histogram_depth_smooth_um: None or float
+        Optional gaussian smoother on histogram on depth axis.
+        This is given as the sigma of the gaussian in micrometers.
+    histogram_time_smooth_s: None or float
+        Optional gaussian smoother on histogram on time axis.
+        This is given as the sigma of the gaussian in seconds.
     pairwise_displacement_method: 'conv' or 'phase_cross_correlation'
         How to estimate the displacement in the pairwise matrix.
     max_displacement_um: float
         Maximum possible discplacement in micrometers.
     weight_scale: 'linear' or 'exp'
         For parwaise displacement, how to to rescale the associated weight matrix.
     error_sigma: float 0.2
@@ -222,84 +258,137 @@
          - "time" : the displacement at a given time (in seconds) is subtracted
          - "mode_search" : an attempt is made to guess the mode. needs work.
     lsqr_robust_n_iter: int
         Number of iteration for convergence_method='lsqr_robust'.
     """
 
     @classmethod
-    def run(cls, recording, peaks, peak_locations, direction, bin_duration_s, bin_um, spatial_bin_edges,
-            non_rigid_windows, verbose, progress_bar, extra_check,
-            pairwise_displacement_method='conv', max_displacement_um=100., weight_scale='linear',
-            error_sigma=0.2, conv_engine=None, torch_device=None, batch_size=1, corr_threshold=0.0,
-            time_horizon_s=None, convergence_method='lsqr_robust', soft_weights=False, 
-            normalized_xcorr=True, centered_xcorr=True, temporal_prior=True, spatial_prior=False,
-            force_spatial_median_continuity=False, reference_displacement="median", reference_displacement_time_s=0,
-            robust_regression_sigma=2, lsqr_robust_n_iter=20, weight_with_amplitude=False):
-
+    def run(
+        cls,
+        recording,
+        peaks,
+        peak_locations,
+        direction,
+        bin_duration_s,
+        bin_um,
+        spatial_bin_edges,
+        non_rigid_windows,
+        verbose,
+        progress_bar,
+        extra_check,
+        histogram_depth_smooth_um=None,
+        histogram_time_smooth_s=None,
+        pairwise_displacement_method="conv",
+        max_displacement_um=100.0,
+        weight_scale="linear",
+        error_sigma=0.2,
+        conv_engine=None,
+        torch_device=None,
+        batch_size=1,
+        corr_threshold=0.0,
+        time_horizon_s=None,
+        convergence_method="lsqr_robust",
+        soft_weights=False,
+        normalized_xcorr=True,
+        centered_xcorr=True,
+        temporal_prior=True,
+        spatial_prior=False,
+        force_spatial_median_continuity=False,
+        reference_displacement="median",
+        reference_displacement_time_s=0,
+        robust_regression_sigma=2,
+        lsqr_robust_n_iter=20,
+        weight_with_amplitude=False,
+    ):
         # use torch if installed
         if conv_engine is None:
             conv_engine = "torch" if HAVE_TORCH else "numpy"
 
         # make 2D histogram raster
         if verbose:
-            print('Computing motion histogram')
+            print("Computing motion histogram")
+
+        motion_histogram, temporal_hist_bin_edges, spatial_hist_bin_edges = make_2d_motion_histogram(
+            recording,
+            peaks,
+            peak_locations,
+            direction=direction,
+            bin_duration_s=bin_duration_s,
+            spatial_bin_edges=spatial_bin_edges,
+            weight_with_amplitude=weight_with_amplitude,
+        )
+
+        if histogram_depth_smooth_um is not None:
+            bins = np.arange(motion_histogram.shape[1]) * bin_um
+            bins -= np.mean(bins)
+            smooth_kernel = np.exp(-(bins**2) / (2 * histogram_depth_smooth_um**2))
+            smooth_kernel /= np.sum(smooth_kernel)
+            motion_histogram = scipy.signal.fftconvolve(motion_histogram, smooth_kernel[None, :], mode="same", axes=1)
+
+        if histogram_time_smooth_s is not None:
+            bins = np.arange(motion_histogram.shape[0]) * bin_duration_s
+            bins -= np.mean(bins)
+            smooth_kernel = np.exp(-(bins**2) / (2 * histogram_time_smooth_s**2))
+            smooth_kernel /= np.sum(smooth_kernel)
+            motion_histogram = scipy.signal.fftconvolve(motion_histogram, smooth_kernel[:, None], mode="same", axes=0)
 
-        motion_histogram, temporal_hist_bin_edges, spatial_hist_bin_edges = \
-            make_2d_motion_histogram(recording, peaks,
-                                     peak_locations,
-                                     direction=direction,
-                                     bin_duration_s=bin_duration_s,
-                                     spatial_bin_edges=spatial_bin_edges,
-                                     weight_with_amplitude=weight_with_amplitude,)
         if extra_check is not None:
-            extra_check['motion_histogram'] = motion_histogram
-            extra_check['pairwise_displacement_list'] = []
-            extra_check['temporal_hist_bin_edges'] = temporal_hist_bin_edges
-            extra_check['spatial_hist_bin_edges'] = spatial_hist_bin_edges
+            extra_check["motion_histogram"] = motion_histogram
+            extra_check["pairwise_displacement_list"] = []
+            extra_check["temporal_hist_bin_edges"] = temporal_hist_bin_edges
+            extra_check["spatial_hist_bin_edges"] = spatial_hist_bin_edges
 
         # temporal bins are bin center
-        temporal_bins = temporal_hist_bin_edges[:-1] + bin_duration_s // 2.
+        temporal_bins = temporal_hist_bin_edges[:-1] + bin_duration_s // 2.0
 
         motion = np.zeros((temporal_bins.size, len(non_rigid_windows)), dtype=np.float64)
         windows_iter = non_rigid_windows
         if progress_bar:
             windows_iter = tqdm(windows_iter, desc="windows")
         if spatial_prior:
             all_pairwise_displacements = np.empty(
-                (len(non_rigid_windows), temporal_bins.size, temporal_bins.size),
-                dtype=np.float64)
+                (len(non_rigid_windows), temporal_bins.size, temporal_bins.size), dtype=np.float64
+            )
             all_pairwise_displacement_weights = np.empty(
-                (len(non_rigid_windows), temporal_bins.size, temporal_bins.size),
-                dtype=np.float64)
+                (len(non_rigid_windows), temporal_bins.size, temporal_bins.size), dtype=np.float64
+            )
         for i, win in enumerate(windows_iter):
             window_slice = np.flatnonzero(win > 1e-5)
             window_slice = slice(window_slice[0], window_slice[-1])
             if verbose:
-                print(f'Computing pairwise displacement: {i + 1} / {len(non_rigid_windows)}')
+                print(f"Computing pairwise displacement: {i + 1} / {len(non_rigid_windows)}")
 
-            pairwise_displacement, pairwise_displacement_weight = \
-                    compute_pairwise_displacement(motion_histogram[:, window_slice], bin_um,
-                                                  window=win[window_slice],
-                                                  method=pairwise_displacement_method, weight_scale=weight_scale,
-                                                  error_sigma=error_sigma, conv_engine=conv_engine,
-                                                  torch_device=torch_device, batch_size=batch_size,
-                                                  max_displacement_um=max_displacement_um,
-                                                  normalized_xcorr=normalized_xcorr, centered_xcorr=centered_xcorr,
-                                                  corr_threshold=corr_threshold, time_horizon_s=time_horizon_s,
-                                                  bin_duration_s=bin_duration_s, progress_bar=False)
+            pairwise_displacement, pairwise_displacement_weight = compute_pairwise_displacement(
+                motion_histogram[:, window_slice],
+                bin_um,
+                window=win[window_slice],
+                method=pairwise_displacement_method,
+                weight_scale=weight_scale,
+                error_sigma=error_sigma,
+                conv_engine=conv_engine,
+                torch_device=torch_device,
+                batch_size=batch_size,
+                max_displacement_um=max_displacement_um,
+                normalized_xcorr=normalized_xcorr,
+                centered_xcorr=centered_xcorr,
+                corr_threshold=corr_threshold,
+                time_horizon_s=time_horizon_s,
+                bin_duration_s=bin_duration_s,
+                progress_bar=False,
+            )
 
             if spatial_prior:
                 all_pairwise_displacements[i] = pairwise_displacement
                 all_pairwise_displacement_weights[i] = pairwise_displacement_weight
 
             if extra_check is not None:
-                extra_check['pairwise_displacement_list'].append(pairwise_displacement)
+                extra_check["pairwise_displacement_list"].append(pairwise_displacement)
 
             if verbose:
-                print(f'Computing global displacement: {i + 1} / {len(non_rigid_windows)}')
+                print(f"Computing global displacement: {i + 1} / {len(non_rigid_windows)}")
 
             # TODO: if spatial_prior, do this after the loop
             if not spatial_prior:
                 motion[:, i] = compute_global_displacement(
                     pairwise_displacement,
                     pairwise_displacement_weight=pairwise_displacement_weight,
                     convergence_method=convergence_method,
@@ -355,15 +444,14 @@
                     max_zeros = n_zeros
                     best_ref = ref
             motion -= best_ref
 
         return motion, temporal_bins
 
 
-
 class IterativeTemplateRegistration:
     """
     Alignment function implemented by Kilosort2.5 and ported from pykilosort:
     https://github.com/int-brain-lab/pykilosort/blob/ibl_prod/pykilosort/datashift2.py#L166
 
     The main difference with respect to the original implementation are:
      * scipy is used for gaussian smoothing
@@ -372,15 +460,16 @@
      * peak_locations are computed outside and so can either center fo mass or monopolar trianglation
        contrary to kilosort2.5 use exclusively center of mass
 
     See https://www.science.org/doi/abs/10.1126/science.abf4588?cookieSet=1
 
     Ported by Alessio Buccino in SpikeInterface
     """
-    name = 'iterative_template'
+
+    name = "iterative_template"
     params_doc = """
     num_amp_bins: int
         number ob bins in the histogram on the log amplitues dimension, by default 20.
     num_shifts_global: int
         Number of spatial bin shifts to consider for global alignment, by default 15
     num_iterations: int
         Number of iterations for global alignment procedure, by default 10
@@ -393,73 +482,94 @@
     kriging_p: foat
         p parameter for kriging_kernel function
     kriging_d: float
         d parameter for kriging_kernel function
     """
 
     @classmethod
-    def run(cls, recording, peaks, peak_locations, direction, bin_duration_s, bin_um, spatial_bin_edges,
-            non_rigid_windows, verbose, progress_bar, extra_check,
-            num_amp_bins=20, num_shifts_global=15, num_iterations=10, num_shifts_block=5,
-            smoothing_sigma=0.5, kriging_sigma=1, kriging_p=2, kriging_d=2):
-
+    def run(
+        cls,
+        recording,
+        peaks,
+        peak_locations,
+        direction,
+        bin_duration_s,
+        bin_um,
+        spatial_bin_edges,
+        non_rigid_windows,
+        verbose,
+        progress_bar,
+        extra_check,
+        num_amp_bins=20,
+        num_shifts_global=15,
+        num_iterations=10,
+        num_shifts_block=5,
+        smoothing_sigma=0.5,
+        kriging_sigma=1,
+        kriging_p=2,
+        kriging_d=2,
+    ):
         # make a 3D histogram
-        motion_histograms, temporal_hist_bin_edges, spatial_hist_bin_edges = \
-            make_3d_motion_histograms(recording, peaks, peak_locations,
-                                      direction=direction, num_amp_bins=num_amp_bins, bin_duration_s=bin_duration_s,
-                                      spatial_bin_edges=spatial_bin_edges)
+        motion_histograms, temporal_hist_bin_edges, spatial_hist_bin_edges = make_3d_motion_histograms(
+            recording,
+            peaks,
+            peak_locations,
+            direction=direction,
+            num_amp_bins=num_amp_bins,
+            bin_duration_s=bin_duration_s,
+            spatial_bin_edges=spatial_bin_edges,
+        )
         # temporal bins are bin center
-        temporal_bins = temporal_hist_bin_edges[:-1] + bin_duration_s // 2.
+        temporal_bins = temporal_hist_bin_edges[:-1] + bin_duration_s // 2.0
 
         # do alignment
-        shift_indices, target_histogram, shift_covs_block = \
-            iterative_template_registration(motion_histograms,
-                                            non_rigid_windows=non_rigid_windows,
-                                            num_shifts_global=num_shifts_global,
-                                            num_iterations=num_iterations,
-                                            num_shifts_block=num_shifts_block,
-                                            smoothing_sigma=smoothing_sigma,
-                                            kriging_sigma=kriging_sigma,
-                                            kriging_p=kriging_p,
-                                            kriging_d=kriging_d)
+        shift_indices, target_histogram, shift_covs_block = iterative_template_registration(
+            motion_histograms,
+            non_rigid_windows=non_rigid_windows,
+            num_shifts_global=num_shifts_global,
+            num_iterations=num_iterations,
+            num_shifts_block=num_shifts_block,
+            smoothing_sigma=smoothing_sigma,
+            kriging_sigma=kriging_sigma,
+            kriging_p=kriging_p,
+            kriging_d=kriging_d,
+        )
 
         # convert to um
         motion = -(shift_indices * bin_um)
 
         if extra_check:
-            extra_check['motion_histograms'] = motion_histograms
-            extra_check['target_histogram'] = target_histogram
-            extra_check['shift_covs_block'] = shift_covs_block
-            extra_check['temporal_hist_bin_edges'] = temporal_hist_bin_edges
-            extra_check['spatial_hist_bin_edges'] = spatial_hist_bin_edges
+            extra_check["motion_histograms"] = motion_histograms
+            extra_check["target_histogram"] = target_histogram
+            extra_check["shift_covs_block"] = shift_covs_block
+            extra_check["temporal_hist_bin_edges"] = temporal_hist_bin_edges
+            extra_check["spatial_hist_bin_edges"] = spatial_hist_bin_edges
 
         return motion, temporal_bins
 
 
 _methods_list = [DecentralizedRegistration, IterativeTemplateRegistration]
 estimate_motion_methods = {m.name: m for m in _methods_list}
 method_doc = make_multi_method_doc(_methods_list)
 estimate_motion.__doc__ = estimate_motion.__doc__.format(method_doc=method_doc)
 
 
-
 def get_spatial_bin_edges(recording, direction, margin_um, bin_um):
     # contact along one axis
     probe = recording.get_probe()
-    dim = ['x', 'y', 'z'].index(direction)
+    dim = ["x", "y", "z"].index(direction)
     contact_pos = probe.contact_positions[:, dim]
 
     min_ = np.min(contact_pos) - margin_um
     max_ = np.max(contact_pos) + margin_um
-    spatial_bins = np.arange(min_, max_+bin_um, bin_um)
+    spatial_bins = np.arange(min_, max_ + bin_um, bin_um)
 
     return spatial_bins
 
 
-
 def get_windows(rigid, bin_um, contact_pos, spatial_bin_edges, margin_um, win_step_um, win_sigma_um, win_shape):
     """
     Generate spatial windows (taper) for non-rigid motion.
     For rigid motion, this is equivalent to have one unique rectangular window that covers the entire probe.
     The windowing can be gaussian or rectangular.
 
     Parameters
@@ -490,56 +600,62 @@
 
     Notes
     -----
     Note that kilosort2.5 uses overlaping rectangular windows.
     Here by default we use gaussian window.
 
     """
-    bin_centers = spatial_bin_edges[:-1] + bin_um / 2.
+    bin_centers = spatial_bin_edges[:-1] + bin_um / 2.0
     n = bin_centers.size
 
     if rigid:
         # win_shape = 'rect' is forced
-        non_rigid_windows = [np.ones(n, dtype='float64')]
-        middle = (spatial_bin_edges[0] + spatial_bin_edges[-1]) / 2.
+        non_rigid_windows = [np.ones(n, dtype="float64")]
+        middle = (spatial_bin_edges[0] + spatial_bin_edges[-1]) / 2.0
         non_rigid_window_centers = np.array([middle])
     else:
-        assert win_sigma_um >= win_step_um, f'win_sigma_um too low {win_sigma_um} compared to win_step_um {win_step_um}'
+        assert win_sigma_um >= win_step_um, f"win_sigma_um too low {win_sigma_um} compared to win_step_um {win_step_um}"
 
         min_ = np.min(contact_pos) - margin_um
         max_ = np.max(contact_pos) + margin_um
         num_non_rigid_windows = int((max_ - min_) // win_step_um)
         border = ((max_ - min_) % win_step_um) / 2
         non_rigid_window_centers = np.arange(num_non_rigid_windows + 1) * win_step_um + min_ + border
         non_rigid_windows = []
 
         for win_center in non_rigid_window_centers:
-            if win_shape == 'gaussian':
-                win = np.exp(-(bin_centers - win_center) ** 2 / (2 * win_sigma_um ** 2))
-            elif win_shape == 'rect':
-                win = np.abs(bin_centers - win_center) < (win_sigma_um / 2.)
-                win = win.astype('float64')
-            elif win_shape == 'triangle':
+            if win_shape == "gaussian":
+                win = np.exp(-((bin_centers - win_center) ** 2) / (2 * win_sigma_um**2))
+            elif win_shape == "rect":
+                win = np.abs(bin_centers - win_center) < (win_sigma_um / 2.0)
+                win = win.astype("float64")
+            elif win_shape == "triangle":
                 center_dist = np.abs(bin_centers - win_center)
-                in_window = center_dist <= (win_sigma_um / 2.)
+                in_window = center_dist <= (win_sigma_um / 2.0)
                 win = -center_dist
                 win[~in_window] = 0
                 win[in_window] -= win[in_window].min()
                 win[in_window] /= win[in_window].max()
 
             non_rigid_windows.append(win)
 
     return non_rigid_windows, non_rigid_window_centers
 
 
-
-def make_2d_motion_histogram(recording, peaks, peak_locations,
-                             weight_with_amplitude=False, direction='y',
-                             bin_duration_s=1., bin_um=2., margin_um=50,
-                             spatial_bin_edges=None):
+def make_2d_motion_histogram(
+    recording,
+    peaks,
+    peak_locations,
+    weight_with_amplitude=False,
+    direction="y",
+    bin_duration_s=1.0,
+    bin_um=2.0,
+    margin_um=50,
+    spatial_bin_edges=None,
+):
     """
     Generate 2d motion histogram in depth and time.
 
     Parameters
     ----------
     recording : BaseRecording
         The input recording
@@ -574,38 +690,46 @@
     num_samples = recording.get_num_samples(segment_index=0)
     bin_sample_size = int(bin_duration_s * fs)
     sample_bin_edges = np.arange(0, num_samples + bin_sample_size, bin_sample_size)
     temporal_bin_edges = sample_bin_edges / fs
     if spatial_bin_edges is None:
         spatial_bin_edges = get_spatial_bin_edges(recording, direction, margin_um, bin_um)
 
-    arr = np.zeros((peaks.size, 2), dtype='float64')
-    arr[:, 0] = peaks['sample_ind']
+    arr = np.zeros((peaks.size, 2), dtype="float64")
+    arr[:, 0] = peaks["sample_index"]
     arr[:, 1] = peak_locations[direction]
-    
+
     if weight_with_amplitude:
-        weights = np.abs(peaks['amplitude'])
+        weights = np.abs(peaks["amplitude"])
     else:
         weights = None
 
     motion_histogram, edges = np.histogramdd(arr, bins=(sample_bin_edges, spatial_bin_edges), weights=weights)
 
     # average amplitude in each bin
     if weight_with_amplitude:
         bin_counts, _ = np.histogramdd(arr, bins=(sample_bin_edges, spatial_bin_edges))
         bin_counts[bin_counts == 0] = 1
         motion_histogram = motion_histogram / bin_counts
 
     return motion_histogram, temporal_bin_edges, spatial_bin_edges
 
 
-def make_3d_motion_histograms(recording, peaks, peak_locations,
-                              direction='y', bin_duration_s=1., bin_um=2.,
-                              margin_um=50, num_amp_bins=20,
-                              log_transform=True, spatial_bin_edges=None):
+def make_3d_motion_histograms(
+    recording,
+    peaks,
+    peak_locations,
+    direction="y",
+    bin_duration_s=1.0,
+    bin_um=2.0,
+    margin_um=50,
+    num_amp_bins=20,
+    log_transform=True,
+    spatial_bin_edges=None,
+):
     """
     Generate 3d motion histograms in depth, amplitude, and time.
     This is used by the "iterative_template_registration" (Kilosort2.5) method.
 
 
     Parameters
     ----------
@@ -648,57 +772,76 @@
 
     # pre-compute abs amplitude and ranges for scaling
     amplitude_bin_edges = np.linspace(0, 1, num_amp_bins + 1)
     abs_peaks = np.abs(peaks["amplitude"])
     max_peak_amp = np.max(abs_peaks)
     min_peak_amp = np.min(abs_peaks)
     # log amplitudes and scale between 0-1
-    abs_peaks_log_norm = (np.log10(abs_peaks) - np.log10(min_peak_amp)) / \
-        (np.log10(max_peak_amp) - np.log10(min_peak_amp))
+    abs_peaks_log_norm = (np.log10(abs_peaks) - np.log10(min_peak_amp)) / (
+        np.log10(max_peak_amp) - np.log10(min_peak_amp)
+    )
 
-    arr = np.zeros((peaks.size, 3), dtype='float64')
-    arr[:, 0] = peaks['sample_ind']
+    arr = np.zeros((peaks.size, 3), dtype="float64")
+    arr[:, 0] = peaks["sample_index"]
     arr[:, 1] = peak_locations[direction]
     arr[:, 2] = abs_peaks_log_norm
 
-    motion_histograms, edges = np.histogramdd(arr, bins=(sample_bin_edges, spatial_bin_edges, amplitude_bin_edges,))
+    motion_histograms, edges = np.histogramdd(
+        arr,
+        bins=(
+            sample_bin_edges,
+            spatial_bin_edges,
+            amplitude_bin_edges,
+        ),
+    )
 
     if log_transform:
         motion_histograms = np.log2(1 + motion_histograms)
 
     return motion_histograms, temporal_bin_edges, spatial_bin_edges
 
 
-def compute_pairwise_displacement(motion_hist, bin_um, method='conv',
-                                  weight_scale='linear', error_sigma=0.2,
-                                  conv_engine='numpy', torch_device=None,
-                                  batch_size=1, max_displacement_um=1500,
-                                  corr_threshold=0, time_horizon_s=None,
-                                  normalized_xcorr=True, centered_xcorr=True,
-                                  bin_duration_s=None, progress_bar=False,
-                                  window=None):
+def compute_pairwise_displacement(
+    motion_hist,
+    bin_um,
+    method="conv",
+    weight_scale="linear",
+    error_sigma=0.2,
+    conv_engine="numpy",
+    torch_device=None,
+    batch_size=1,
+    max_displacement_um=1500,
+    corr_threshold=0,
+    time_horizon_s=None,
+    normalized_xcorr=True,
+    centered_xcorr=True,
+    bin_duration_s=None,
+    progress_bar=False,
+    window=None,
+):
     """
     Compute pairwise displacement
     """
     from scipy import sparse
     from scipy import linalg
+
     assert conv_engine in ("torch", "numpy")
     size = motion_hist.shape[0]
-    pairwise_displacement = np.zeros((size, size), dtype='float32')
+    pairwise_displacement = np.zeros((size, size), dtype="float32")
 
     if time_horizon_s is not None:
         band_width = int(np.ceil(time_horizon_s / bin_duration_s))
         if band_width >= size:
             time_horizon_s = None
 
-    if conv_engine == 'torch':
+    if conv_engine == "torch":
         if torch_device is None:
             torch_device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
 
-    if method == 'conv':
+    if method == "conv":
         if max_displacement_um is None:
             n = motion_hist.shape[1] // 2
         else:
             n = min(
                 motion_hist.shape[1] // 2,
                 int(np.ceil(max_displacement_um // bin_um)),
             )
@@ -737,67 +880,63 @@
                 pairwise_displacement[i : i + batch_size] = best_disp
                 correlation[i : i + batch_size] = max_corr
 
         if corr_threshold is not None and corr_threshold > 0:
             which = correlation > corr_threshold
             correlation *= which
 
-    elif method == 'phase_cross_correlation':
+    elif method == "phase_cross_correlation":
         # this 'phase_cross_correlation' is an old idea from Julien/Charlie/Erden that is kept for testing
         # but this is not very releveant
         try:
             import skimage.registration
         except ImportError:
             raise ImportError("To use 'phase_cross_correlation' method install scikit-image")
 
-        errors = np.zeros((size, size), dtype='float32')
+        errors = np.zeros((size, size), dtype="float32")
         loop = range(size)
         if progress_bar:
             loop = tqdm(loop)
         for i in loop:
             for j in range(size):
-                shift, error, diffphase = skimage.registration.phase_cross_correlation(motion_hist[i, :],
-                                                                                       motion_hist[j, :])
+                shift, error, diffphase = skimage.registration.phase_cross_correlation(
+                    motion_hist[i, :], motion_hist[j, :]
+                )
                 pairwise_displacement[i, j] = shift * bin_um
                 errors[i, j] = error
         correlation = 1 - errors
 
     else:
-        raise ValueError(f'method does not exist for compute_pairwise_displacement {method}')
+        raise ValueError(f"method does not exist for compute_pairwise_displacement {method}")
 
-    if weight_scale == 'linear':
+    if weight_scale == "linear":
         # between 0 and 1
         pairwise_displacement_weight = correlation
-    elif weight_scale == 'exp':
+    elif weight_scale == "exp":
         pairwise_displacement_weight = np.exp((correlation - 1) / error_sigma)
 
     # handle the time horizon by multiplying the weights by a
     # matrix with the time horizon on its diagonal bands.
-    if (
-        method == "conv"
-        and time_horizon_s is not None and time_horizon_s > 0
-    ):
+    if method == "conv" and time_horizon_s is not None and time_horizon_s > 0:
         horizon_matrix = linalg.toeplitz(
-            np.r_[
-                np.ones(band_width, dtype=bool), np.zeros(size - band_width, dtype=bool)
-            ]
+            np.r_[np.ones(band_width, dtype=bool), np.zeros(size - band_width, dtype=bool)]
         )
         pairwise_displacement_weight *= horizon_matrix
 
     return pairwise_displacement, pairwise_displacement_weight
 
 
 def compute_global_displacement(
     pairwise_displacement,
     pairwise_displacement_weight=None,
     sparse_mask=None,
     temporal_prior=True,
     spatial_prior=True,
     soft_weights=False,
-    convergence_method='lsmr',
+    convergence_method="lsmr",
     robust_regression_sigma=2,
     lsqr_robust_n_iter=20,
     progress_bar=False,
 ):
     """
     Compute global displacement
 
@@ -806,15 +945,15 @@
     pairwise_displacement : time x time array
     pairwise_displacement_weight : time x time array
     sparse_mask : time x time array
     convergence_method : str
         One of "gradient"
 
     """
-    if convergence_method == 'gradient_descent':
+    if convergence_method == "gradient_descent":
         size = pairwise_displacement.shape[0]
         from scipy.optimize import minimize
         from scipy.sparse import csr_matrix
 
         D = pairwise_displacement
         if pairwise_displacement_weight is not None or sparse_mask is not None:
             # weighted problem
@@ -834,33 +973,32 @@
             Wsq = W.power(2)
 
             def obj(p):
                 return 0.5 * np.square(Wij * (Dij - (p[I] - p[J]))).sum()
 
             def jac(p):
                 return fixed_terms - 2 * (Wsq @ p) + 2 * p * diag_WW
+
         else:
             # unweighted problem, it's faster when we have no weights
             fixed_terms = -D.sum(axis=1) + D.sum(axis=0)
 
             def obj(p):
                 v = np.square((D - (p[:, None] - p[None, :]))).sum()
                 return 0.5 * v
 
             def jac(p):
                 return fixed_terms + 2 * (size * p - p.sum())
 
-        res = minimize(
-            fun=obj, jac=jac, x0=D.mean(axis=1), method="L-BFGS-B"
-        )
+        res = minimize(fun=obj, jac=jac, x0=D.mean(axis=1), method="L-BFGS-B")
         if not res.success:
             print("Global displacement gradient descent had an error")
         displacement = res.x
 
-    elif convergence_method == 'lsqr_robust':
+    elif convergence_method == "lsqr_robust":
         from scipy.sparse import csr_matrix
         from scipy.sparse.linalg import lsqr
         from scipy.stats import zscore
 
         if sparse_mask is not None:
             I, J = np.nonzero(sparse_mask > 0)
         elif pairwise_displacement_weight is not None:
@@ -882,22 +1020,22 @@
         else:
             V = pairwise_displacement[I, J]
         M = csr_matrix((nnz_ones, (range(I.shape[0]), I)), shape=(I.shape[0], pairwise_displacement.shape[0]))
         N = csr_matrix((nnz_ones, (range(I.shape[0]), J)), shape=(I.shape[0], pairwise_displacement.shape[0]))
         A = M - N
         idx = np.ones(A.shape[0], dtype=bool)
 
-        #TODO: this is already soft_weights
+        # TODO: this is already soft_weights
         xrange = trange if progress_bar else range
         for i in xrange(lsqr_robust_n_iter):
             p = lsqr(A[idx].multiply(W[idx]), V[idx] * W[idx][:, 0])[0]
             idx = np.nonzero(np.abs(zscore(A @ p - V)) <= robust_regression_sigma)
         displacement = p
 
-    elif convergence_method == 'lsmr':
+    elif convergence_method == "lsmr":
         from scipy import sparse
         from scipy.stats import zscore
 
         D = pairwise_displacement
 
         # weighted problem
         if pairwise_displacement_weight is None:
@@ -1013,36 +1151,41 @@
         if progress_bar and lsqr_robust_n_iter > 1:
             iters = tqdm(iters, desc="robust lsqr")
         for it in iters:
             # trim active set -- start with no trimming
             idx = slice(None)
             if it:
                 idx = np.flatnonzero(
-                    cannot_trim 
-                    | (np.abs(zscore(coefficients @ displacement - targets)) <= robust_regression_sigma)
+                    cannot_trim | (np.abs(zscore(coefficients @ displacement - targets)) <= robust_regression_sigma)
                 )
-            
+
             # solve trimmed ols problem
             displacement, *_ = sparse.linalg.lsmr(coefficients[idx], targets[idx], x0=p0)
-            
+
             # warm start next iteration
             p0 = displacement
-        
+
         displacement = displacement.reshape(B, T).T
     else:
         raise ValueError(f"Method {convergence_method} doesn't exist for compute_global_displacement")
 
     return np.squeeze(displacement)
 
 
-def iterative_template_registration(spikecounts_hist_images,
-                                    non_rigid_windows=None,
-                                    num_shifts_global=15, num_iterations=10,
-                                    num_shifts_block=5, smoothing_sigma=0.5,
-                                    kriging_sigma=1, kriging_p=2, kriging_d=2):
+def iterative_template_registration(
+    spikecounts_hist_images,
+    non_rigid_windows=None,
+    num_shifts_global=15,
+    num_iterations=10,
+    num_shifts_block=5,
+    smoothing_sigma=0.5,
+    kriging_sigma=1,
+    kriging_p=2,
+    kriging_d=2,
+):
     """
 
     Parameters
     ----------
 
     spikecounts_hist_images : np.ndarray
         Spike count histogram images (num_temporal_bins, num_spatial_bins, num_amps_bins)
@@ -1133,29 +1276,28 @@
             Fs = np.roll(Ftaper, shift, axis=0)
             F0taper = F0[window_slice] * np.tile(tiled_window, (1,) + F0.shape[1:])
             shift_covs_block[t, :, window_index] = np.mean(Fs * F0taper, axis=(0, 1))
 
     # gaussian smoothing:
     # here the original my_conv2_cpu is substituted with scipy gaussian_filters
     shift_covs_block_smooth = shift_covs_block.copy()
-    shifts_block_up = np.linspace(-num_shifts_block, num_shifts_block,
-                                  (2 * num_shifts_block * 10) + 1)
+    shifts_block_up = np.linspace(-num_shifts_block, num_shifts_block, (2 * num_shifts_block * 10) + 1)
     # 1. 2d smoothing over time and blocks dimensions for each shift
     for shift_index in range(num_shifts):
         shift_covs_block_smooth[shift_index, :, :] = gaussian_filter(
             shift_covs_block_smooth[shift_index, :, :], smoothing_sigma
         )  # some additional smoothing for robustness, across all dimensions
     # 2. 1d smoothing over shift dimension for each spatial block
     for window_index in range(num_non_rigid_windows):
         shift_covs_block_smooth[:, :, window_index] = gaussian_filter1d(
             shift_covs_block_smooth[:, :, window_index], smoothing_sigma, axis=0
         )  # some additional smoothing for robustness, across all dimensions
-    upsample_kernel = kriging_kernel(shifts_block[:, np.newaxis],
-                                     shifts_block_up[:, np.newaxis],
-                                     sigma=kriging_sigma, p=kriging_p, d=kriging_d)
+    upsample_kernel = kriging_kernel(
+        shifts_block[:, np.newaxis], shifts_block_up[:, np.newaxis], sigma=kriging_sigma, p=kriging_p, d=kriging_d
+    )
 
     optimal_shift_indices = np.zeros((num_temporal_bins, num_non_rigid_windows))
     for window_index in range(num_non_rigid_windows):
         # using the upsampling kernel K, get the upsampled cross-correlation
         # curves
         upsampled_cov = upsample_kernel.T @ shift_covs_block_smooth[:, :, window_index]
 
@@ -1268,21 +1410,17 @@
     cov = conv1d(x[:, None, :], wt, padding=padding)
     cov /= N
     if centered:
         cov -= Ex * Et
 
     # compute variances for denominator, using var X = E[X^2] - (EX)^2
     if normalized:
-        var_template = conv1d(
-            ones, wt * template[:, None, :], padding=padding
-        )
+        var_template = conv1d(ones, wt * template[:, None, :], padding=padding)
         var_template /= N
-        var_x = conv1d(
-            npx.square(x)[:, None, :], weights, padding=padding
-        )
+        var_x = conv1d(npx.square(x)[:, None, :], weights, padding=padding)
         var_x /= N
         if centered:
             var_template -= npx.square(Et)
             var_x -= npx.square(Ex)
 
     # now find the final normxcorr
     corr = cov  # renaming for clarity
@@ -1320,16 +1458,15 @@
     for m in range(n):
         for c in range(c_out):
             output[m, c] = correlate(input[m, 0], weights[c, 0], mode=mode)
 
     return output
 
 
-def clean_motion_vector(motion, temporal_bins, bin_duration_s,
-                        speed_threshold=30, sigma_smooth_s=None):
+def clean_motion_vector(motion, temporal_bins, bin_duration_s, speed_threshold=30, sigma_smooth_s=None):
     """
     Simple machinery to remove spurious fast bump in the motion vector.
     Also can applyt a smoothing.
 
 
     Arguments
     ---------
@@ -1355,45 +1492,46 @@
 
     # STEP 1 :
     #  * detect long plateau or small peak corssing the speed thresh
     #  * mask the period and interpolate
     for i in range(motion.shape[1]):
         one_motion = motion_clean[:, i]
         speed = np.diff(one_motion, axis=0) / bin_duration_s
-        inds,  = np.nonzero(np.abs(speed) > speed_threshold)
-        inds +=1
+        (inds,) = np.nonzero(np.abs(speed) > speed_threshold)
+        inds += 1
         if inds.size % 2 == 1:
             # more compicated case: number of of inds is odd must remove first or last
             # take the smallest duration sum
             inds0 = inds[:-1]
             inds1 = inds[1:]
             d0 = np.sum(inds0[1::2] - inds0[::2])
             d1 = np.sum(inds1[1::2] - inds1[::2])
             if d0 < d1:
                 inds = inds0
-        mask = np.ones(motion_clean.shape[0], dtype='bool')
+        mask = np.ones(motion_clean.shape[0], dtype="bool")
         for i in range(inds.size // 2):
-            mask[inds[i*2]:inds[i*2+1]] = False
+            mask[inds[i * 2] : inds[i * 2 + 1]] = False
         f = scipy.interpolate.interp1d(temporal_bins[mask], one_motion[mask])
         one_motion[~mask] = f(temporal_bins[~mask])
 
     # Step 2 : gaussian smooth
     if sigma_smooth_s is not None:
         half_size = motion_clean.shape[0] // 2
         if motion_clean.shape[0] % 2 == 0:
             # take care of the shift
             bins = (np.arange(motion_clean.shape[0]) - half_size + 1) * bin_duration_s
         else:
             bins = (np.arange(motion_clean.shape[0]) - half_size) * bin_duration_s
-        smooth_kernel = np.exp( -bins**2 / ( 2 * sigma_smooth_s **2))
+        smooth_kernel = np.exp(-(bins**2) / (2 * sigma_smooth_s**2))
         smooth_kernel /= np.sum(smooth_kernel)
         smooth_kernel = smooth_kernel[:, None]
-        motion_clean = scipy.signal.fftconvolve(motion_clean, smooth_kernel, mode='same', axes=0)
+        motion_clean = scipy.signal.fftconvolve(motion_clean, smooth_kernel, mode="same", axes=0)
 
     return motion_clean
 
 
 def kriging_kernel(source_location, target_location, sigma=1, p=2, d=2):
     from scipy.spatial.distance import cdist
-    dist_xy = cdist(source_location, target_location, metric='euclidean')
-    K = np.exp(-(dist_xy / sigma)**p / d)
+
+    dist_xy = cdist(source_location, target_location, metric="euclidean")
+    K = np.exp(-((dist_xy / sigma) ** p) / d)
     return K
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sortingcomponents/peak_detection.py` & `spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/peak_detection.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,174 +1,363 @@
 """Sorting components: peak detection."""
+import copy
+from typing import Tuple, Union, List, Dict, Any, Optional, Callable
 
 import numpy as np
 
-from spikeinterface.core.job_tools import (ChunkRecordingExecutor, _shared_job_kwargs_doc,
-                                           split_job_kwargs, fix_job_kwargs)
+from spikeinterface.core.job_tools import (
+    ChunkRecordingExecutor,
+    _shared_job_kwargs_doc,
+    split_job_kwargs,
+    fix_job_kwargs,
+)
 from spikeinterface.core.recording_tools import get_noise_levels, get_channel_distances
 
+from spikeinterface.core.baserecording import BaseRecording
+from spikeinterface.sortingcomponents.peak_pipeline import PeakDetector, WaveformsNode, ExtractSparseWaveforms
+
 from ..core import get_chunk_with_margin
 
-from .peak_pipeline import PipelineNode, check_graph, run_nodes
+from .peak_pipeline import PeakDetector, run_node_pipeline, base_peak_dtype
 from .tools import make_multi_method_doc
 
 try:
     import numba
+
     HAVE_NUMBA = True
 except ImportError:
     HAVE_NUMBA = False
 
 try:
     import torch
     import torch.nn.functional as F
+
     HAVE_TORCH = True
 except ImportError:
     HAVE_TORCH = False
 
-base_peak_dtype = [('sample_ind', 'int64'), ('channel_ind', 'int64'),
-                   ('amplitude', 'float64'), ('segment_ind', 'int64')]
+"""
+TODO:
+    * remove the wrapper class and move  all implementation to instance
+    *
+
+"""
 
 
-def detect_peaks(recording, method='by_channel', pipeline_nodes=None, **kwargs):
+def detect_peaks(
+    recording, method="by_channel", pipeline_nodes=None, gather_mode="memory", folder=None, names=None, **kwargs
+):
     """Peak detection based on threshold crossing in term of k x MAD.
 
     In 'by_channel' : peak are detected in each channel independently
     In 'locally_exclusive' : a single best peak is taken from a set of neighboring channels
 
-
     Parameters
     ----------
     recording: RecordingExtractor
         The recording extractor object.
     pipeline_nodes: None or list[PipelineNode]
         Optional additional PipelineNode need to computed just after detection time.
         This avoid reading the recording multiple times.
+    gather_mode: str
+        How to gather the results:
+
+        * "memory": results are returned as in-memory numpy arrays
+
+        * "npy": results are stored to .npy files in `folder`
+    folder: str or Path
+        If gather_mode is "npy", the folder where the files are created.
+    names: list
+        List of strings with file stems associated with returns.
+
     {method_doc}
     {job_doc}
 
     Returns
     -------
     peaks: array
         Detected peaks.
-
     Notes
     -----
     This peak detection ported from tridesclous into spikeinterface.
     """
 
     assert method in detect_peak_methods
 
     method_class = detect_peak_methods[method]
-    
-    method_kwargs, job_kwargs = split_job_kwargs(kwargs)
-    mp_context = method_class.preferred_mp_context
 
-    # prepare args
-    method_args = method_class.check_params(recording, **method_kwargs)
+    method_kwargs, job_kwargs = split_job_kwargs(kwargs)
+    job_kwargs["mp_context"] = method_class.preferred_mp_context
 
-    extra_margin = 0
-    if pipeline_nodes is not None:
-        check_graph(pipeline_nodes)
-        extra_margin = max(node.get_trace_margin() for node in pipeline_nodes)
-
-    func = _detect_peaks_chunk
-    init_func = _init_worker_detect_peaks
-    init_args = (recording, method, method_args, extra_margin, pipeline_nodes)
-    processor = ChunkRecordingExecutor(recording, func, init_func, init_args,
-                                       handle_returns=True, job_name='detect peaks',
-                                       mp_context=mp_context, **job_kwargs)
-    outputs = processor.run()
+    node0 = method_class(recording, **method_kwargs)
+    nodes = [node0]
 
+    job_name = f"detect peaks using {method}"
     if pipeline_nodes is None:
-        peaks = np.concatenate(outputs)
-        return peaks
-    else:
-        outs_concat = ()
-        for output_node in zip(*outputs):
-            outs_concat += (np.concatenate(output_node, axis=0), )
-        return outs_concat
-
-
-def _init_worker_detect_peaks(recording, method, method_args, extra_margin, pipeline_nodes):
-    """Initialize a worker for detecting peaks."""
-
-    if isinstance(recording, dict):
-        from spikeinterface.core import load_extractor
-        recording = load_extractor(recording)
-
-        if pipeline_nodes is not None:
-            pipeline_nodes = [cls.from_dict(recording, kwargs) for cls, kwargs in pipeline_nodes]
-
-    # create a local dict per worker
-    worker_ctx = {}
-    worker_ctx['recording'] = recording
-    worker_ctx['method'] = method
-    worker_ctx['method_class'] = detect_peak_methods[method]
-    worker_ctx['method_args'] = method_args
-    worker_ctx['extra_margin'] = extra_margin
-    worker_ctx['pipeline_nodes'] = pipeline_nodes
-    
-    return worker_ctx
-
-
-def _detect_peaks_chunk(segment_index, start_frame, end_frame, worker_ctx):
-
-    # recover variables of the worker
-    recording = worker_ctx['recording']
-    method_class = worker_ctx['method_class']
-    method_args = worker_ctx['method_args']
-    extra_margin = worker_ctx['extra_margin']
-    pipeline_nodes = worker_ctx['pipeline_nodes']
-
-    margin = method_class.get_method_margin(*method_args) + extra_margin
-
-    # load trace in memory
-    recording_segment = recording._recording_segments[segment_index]
-    traces, left_margin, right_margin = get_chunk_with_margin(recording_segment, start_frame, end_frame,
-                                                              None, margin, add_zeros=True)
-
-    if extra_margin > 0:
-        # remove extra margin for detection node
-        trace_detection = traces[extra_margin:-extra_margin]
+        squeeze_output = True
     else:
-        trace_detection = traces
+        squeeze_output = False
+        job_name += f"  + {len(pipeline_nodes)} nodes"
 
-    # TODO: handle waveform returns
-    peak_sample_ind, peak_chan_ind = method_class.detect_peaks(trace_detection, *method_args)
+        # because node are modified inplace (insert parent) they need to copy incase
+        # the same pipeline is run several times
+        pipeline_nodes = copy.deepcopy(pipeline_nodes)
+        for node in pipeline_nodes:
+            if node.parents is None:
+                node.parents = [node0]
+            else:
+                node.parents = [node0] + node.parents
+            nodes.append(node)
+
+    outs = run_node_pipeline(
+        recording,
+        nodes,
+        job_kwargs,
+        job_name=job_name,
+        gather_mode=gather_mode,
+        squeeze_output=squeeze_output,
+        folder=folder,
+        names=names,
+    )
+    return outs
 
-    if extra_margin > 0:
-        peak_sample_ind += extra_margin
 
-    peak_dtype = base_peak_dtype
-    peak_amplitude = traces[peak_sample_ind, peak_chan_ind]
+expanded_base_peak_dtype = np.dtype(base_peak_dtype + [("iteration", "int8")])
 
-    peaks = np.zeros(peak_sample_ind.size, dtype=peak_dtype)
-    peaks['sample_ind'] = peak_sample_ind
-    peaks['channel_ind'] = peak_chan_ind
-    peaks['amplitude'] = peak_amplitude
-    peaks['segment_ind'] = segment_index
 
-    if pipeline_nodes is not None:
-        outs = run_nodes(traces, peaks, pipeline_nodes)
+class IterativePeakDetector(PeakDetector):
+    """
+    A class that iteratively detects peaks in the recording by applying a peak detector, waveform extraction,
+    and waveform denoising node. The algorithm runs for a specified number of iterations or until no peaks are found.
+    """
 
-    # make absolute sample index
-    peaks['sample_ind'] += (start_frame - left_margin)
+    def __init__(
+        self,
+        recording: BaseRecording,
+        peak_detector_node: PeakDetector,
+        waveform_extraction_node: WaveformsNode,
+        waveform_denoising_node,
+        num_iterations: int = 2,
+        return_output: bool = True,
+        tresholds: Optional[List[float]] = None,
+    ):
+        """
+        Initialize the iterative peak detector.
 
-    if pipeline_nodes is None:
-        return peaks
-    else:
-        return (peaks, ) + outs
+        Parameters
+        ----------
+        recording : BaseRecording
+            The recording to process.
+        peak_detector_node : PeakDetector
+            The peak detector node to use.
+        waveform_extraction_node : WaveformsNode
+            The waveform extraction node to use.
+        waveform_denoising_node
+            The waveform denoising node to use.
+        num_iterations : int, optional, default=2
+            The number of iterations to run the algorithm.
+        return_output : bool, optional, default=True
+        """
+        PeakDetector.__init__(self, recording, return_output=return_output)
+        self.peak_detector_node = peak_detector_node
+        self.waveform_extraction_node = waveform_extraction_node
+        self.waveform_denoising_node = waveform_denoising_node
+        self.num_iterations = num_iterations
+        self.tresholds = tresholds
+
+    def get_trace_margin(self) -> int:
+        """
+        Calculate the maximum trace margin from the internal pipeline.
+        Using the strategy as use by the Node pipeline
 
 
-class DetectPeakByChannel:
-    """Detect peaks using the 'by channel' method.
-    """
+        Returns
+        -------
+        int
+            The maximum trace margin.
+        """
+        internal_pipeline = (self.peak_detector_node, self.waveform_extraction_node, self.waveform_denoising_node)
+        pipeline_margin = (node.get_trace_margin() for node in internal_pipeline if hasattr(node, "get_trace_margin"))
+        return max(pipeline_margin)
+
+    def compute(self, traces_chunk, start_frame, end_frame, segment_index, max_margin) -> Tuple[np.ndarray, np.ndarray]:
+        """
+        Perform the iterative peak detection, waveform extraction, and denoising.
+
+        Parameters
+        ----------
+        traces_chunk : array-like
+            The chunk of traces to process.
+        start_frame : int
+            The starting frame for the chunk.
+        end_frame : int
+            The ending frame for the chunk.
+        segment_index : int
+            The segment index.
+        max_margin : int
+            The maximum margin for the traces.
+
+        Returns
+        -------
+        tuple of ndarray
+            A tuple containing a single ndarray with the detected peaks.
+        """
+
+        traces_chunk = np.array(traces_chunk, copy=True, dtype="float32")
+        local_peaks_list = []
+        all_waveforms = []
+
+        for iteration in range(self.num_iterations):
+            # Hack because of lack of either attribute or named references
+            # I welcome suggestions on how to improve this but I think it is an architectural issue
+            if self.tresholds is not None:
+                old_args = self.peak_detector_node.args
+                old_detect_treshold = self.peak_detector_node.params["detect_threshold"]
+                old_abs_treshold = old_args[1]
+                new_abs_treshold = old_abs_treshold * self.tresholds[iteration] / old_detect_treshold
+
+                new_args = tuple(val if index != 1 else new_abs_treshold for index, val in enumerate(old_args))
+                self.peak_detector_node.args = new_args
+
+            (local_peaks,) = self.peak_detector_node.compute(
+                traces=traces_chunk,
+                start_frame=start_frame,
+                end_frame=end_frame,
+                segment_index=segment_index,
+                max_margin=max_margin,
+            )
+
+            local_peaks = self.add_iteration_to_peaks_dtype(local_peaks=local_peaks, iteration=iteration)
+            local_peaks_list.append(local_peaks)
+
+            # End algorith if no peak is found
+            if local_peaks.size == 0:
+                break
+
+            waveforms = self.waveform_extraction_node.compute(traces=traces_chunk, peaks=local_peaks)
+            denoised_waveforms = self.waveform_denoising_node.compute(
+                traces=traces_chunk, peaks=local_peaks, waveforms=waveforms
+            )
+
+            self.substract_waveforms_from_traces(
+                local_peaks=local_peaks,
+                traces_chunk=traces_chunk,
+                waveforms=denoised_waveforms,
+            )
+
+            all_waveforms.append(waveforms)
+        all_local_peaks = np.concatenate(local_peaks_list, axis=0)
+        all_waveforms = np.concatenate(all_waveforms, axis=0) if len(all_waveforms) != 0 else np.empty((0, 0, 0))
+
+        # Sort as iterative method implies peaks might not be discovered ordered in time
+        sorting_indices = np.argsort(all_local_peaks["sample_index"])
+        all_local_peaks = all_local_peaks[sorting_indices]
+        all_waveforms = all_waveforms[sorting_indices]
+
+        return (all_local_peaks, all_waveforms)
+
+    def substract_waveforms_from_traces(
+        self,
+        local_peaks: np.ndarray,
+        traces_chunk: np.ndarray,
+        waveforms: np.ndarray,
+    ):
+        """
+        Substract inplace the cleaned waveforms from the traces_chunk.
+
+        Parameters
+        ----------
+        sample_indices : ndarray
+            The indices where the waveforms are maximum (peaks["sample_index"]).
+        traces_chunk : ndarray
+            A chunk of the traces.
+        waveforms : ndarray
+            The waveforms extracted from the traces.
+        """
+
+        nbefore = self.waveform_extraction_node.nbefore
+        nafter = self.waveform_extraction_node.nafter
+        if isinstance(self.waveform_extraction_node, ExtractSparseWaveforms):
+            neighbours_mask = self.waveform_extraction_node.neighbours_mask
+        else:
+            neighbours_mask = None
+
+        for peak_index, peak in enumerate(local_peaks):
+            center_sample = peak["sample_index"]
+            first_sample = center_sample - nbefore
+            last_sample = center_sample + nafter
+            if neighbours_mask is None:
+                traces_chunk[first_sample:last_sample, :] -= waveforms[peak_index, :, :]
+            else:
+                (channels,) = np.nonzero(neighbours_mask[peak["channel_index"]])
+                traces_chunk[first_sample:last_sample, channels] -= waveforms[peak_index, :, : len(channels)]
+
+    def add_iteration_to_peaks_dtype(self, local_peaks, iteration) -> np.ndarray:
+        """
+        Add the iteration number to the peaks dtype.
+
+        Parameters
+        ----------
+        local_peaks : ndarray
+            The array of local peaks.
+        iteration : int
+            The iteration number.
 
-    name = 'by_channel'
-    engine = 'numpy'
+        Returns
+        -------
+        ndarray
+            An array of local peaks with the iteration number added.
+        """
+        # Expand dtype to also contain an iteration field
+        local_peaks_expanded = np.zeros_like(local_peaks, dtype=expanded_base_peak_dtype)
+        fields_in_base_type = np.dtype(base_peak_dtype).names
+        for field in fields_in_base_type:
+            local_peaks_expanded[field] = local_peaks[field]
+        local_peaks_expanded["iteration"] = iteration
+
+        return local_peaks_expanded
+
+
+class PeakDetectorWrapper(PeakDetector):
+    # transitory class to maintain instance based and class method based
+    # TODO later when in main: refactor in every old detector class:
+    #    * check_params
+    #    * get_method_margin
+    #  and move the logic in the init
+    #  but keep the class method "detect_peaks()" because it is convinient in template matching
+    def __init__(self, recording, **params):
+        PeakDetector.__init__(self, recording, return_output=True)
+
+        self.params = params
+        self.args = self.check_params(recording, **params)
+
+    def get_trace_margin(self):
+        return self.get_method_margin(*self.args)
+
+    def compute(self, traces, start_frame, end_frame, segment_index, max_margin):
+        peak_sample_ind, peak_chan_ind = self.detect_peaks(traces, *self.args)
+        if peak_sample_ind.size == 0 or peak_chan_ind.size == 0:
+            return (np.zeros(0, dtype=base_peak_dtype),)
+
+        peak_amplitude = traces[peak_sample_ind, peak_chan_ind]
+        local_peaks = np.zeros(peak_sample_ind.size, dtype=base_peak_dtype)
+        local_peaks["sample_index"] = peak_sample_ind
+        local_peaks["channel_index"] = peak_chan_ind
+        local_peaks["amplitude"] = peak_amplitude
+        local_peaks["segment_index"] = segment_index
+
+        # return is always a tuple
+        return (local_peaks,)
+
+
+class DetectPeakByChannel(PeakDetectorWrapper):
+    """Detect peaks using the 'by channel' method."""
+
+    name = "by_channel"
+    engine = "numpy"
     preferred_mp_context = None
     params_doc = """
     peak_sign: 'neg', 'pos', 'both'
         Sign of the peak.
     detect_threshold: float
         Threshold, in median absolute deviations (MAD), to use to detect peaks.
     exclude_sweep_ms: float or None
@@ -179,70 +368,77 @@
         Estimated noise levels to use, if already computed.
         If not provide then it is estimated from a random snippet of the data.
     random_chunk_kwargs: dict, optional
         A dict that contain option to randomize chunk for get_noise_levels().
         Only used if noise_levels is None."""
 
     @classmethod
-    def check_params(cls, recording, peak_sign='neg', detect_threshold=5,
-                     exclude_sweep_ms=0.1, noise_levels=None, random_chunk_kwargs={}):
-        
-        assert peak_sign in ('both', 'neg', 'pos')
+    def check_params(
+        cls,
+        recording,
+        peak_sign="neg",
+        detect_threshold=5,
+        exclude_sweep_ms=0.1,
+        noise_levels=None,
+        random_chunk_kwargs={},
+    ):
+        assert peak_sign in ("both", "neg", "pos")
 
         if noise_levels is None:
             noise_levels = get_noise_levels(recording, return_scaled=False, **random_chunk_kwargs)
         abs_threholds = noise_levels * detect_threshold
-        exclude_sweep_size = int(exclude_sweep_ms * recording.get_sampling_frequency() / 1000.)
+        exclude_sweep_size = int(exclude_sweep_ms * recording.get_sampling_frequency() / 1000.0)
 
         return (peak_sign, abs_threholds, exclude_sweep_size)
-    
+
     @classmethod
     def get_method_margin(cls, *args):
         exclude_sweep_size = args[2]
         return exclude_sweep_size
 
     @classmethod
     def detect_peaks(cls, traces, peak_sign, abs_threholds, exclude_sweep_size):
         traces_center = traces[exclude_sweep_size:-exclude_sweep_size, :]
         length = traces_center.shape[0]
 
-        if peak_sign in ('pos', 'both'):
+        if peak_sign in ("pos", "both"):
             peak_mask = traces_center > abs_threholds[None, :]
             for i in range(exclude_sweep_size):
-                peak_mask &= traces_center > traces[i:i + length, :]
-                peak_mask &= traces_center >= traces[exclude_sweep_size +
-                                                    i + 1:exclude_sweep_size + i + 1 + length, :]
+                peak_mask &= traces_center > traces[i : i + length, :]
+                peak_mask &= (
+                    traces_center >= traces[exclude_sweep_size + i + 1 : exclude_sweep_size + i + 1 + length, :]
+                )
 
-        if peak_sign in ('neg', 'both'):
-            if peak_sign == 'both':
+        if peak_sign in ("neg", "both"):
+            if peak_sign == "both":
                 peak_mask_pos = peak_mask.copy()
 
             peak_mask = traces_center < -abs_threholds[None, :]
             for i in range(exclude_sweep_size):
-                peak_mask &= traces_center < traces[i:i + length, :]
-                peak_mask &= traces_center <= traces[exclude_sweep_size +
-                                                    i + 1:exclude_sweep_size + i + 1 + length, :]
+                peak_mask &= traces_center < traces[i : i + length, :]
+                peak_mask &= (
+                    traces_center <= traces[exclude_sweep_size + i + 1 : exclude_sweep_size + i + 1 + length, :]
+                )
 
-            if peak_sign == 'both':
+            if peak_sign == "both":
                 peak_mask = peak_mask | peak_mask_pos
 
         # find peaks
         peak_sample_ind, peak_chan_ind = np.nonzero(peak_mask)
         # correct for time shift
         peak_sample_ind += exclude_sweep_size
 
         return peak_sample_ind, peak_chan_ind
 
 
-class DetectPeakByChannelTorch:
-    """Detect peaks using the 'by channel' method with pytorch.
-    """
+class DetectPeakByChannelTorch(PeakDetectorWrapper):
+    """Detect peaks using the 'by channel' method with pytorch."""
 
-    name = 'by_channel_torch'
-    engine = 'torch'
+    name = "by_channel_torch"
+    engine = "torch"
     preferred_mp_context = "spawn"
     params_doc = """
     peak_sign: 'neg', 'pos', 'both'
         Sign of the peak.
     detect_threshold: float
         Threshold, in median absolute deviations (MAD), to use to detect peaks.
     exclude_sweep_ms: float or None
@@ -257,201 +453,252 @@
     return_tensor : bool, optional
         If True, the output is returned as a tensor, otherwise as a numpy array, by default False
     random_chunk_kwargs: dict, optional
         A dict that contain option to randomize chunk for get_noise_levels().
         Only used if noise_levels is None."""
 
     @classmethod
-    def check_params(cls, recording, peak_sign='neg', detect_threshold=5,
-                     exclude_sweep_ms=0.1, noise_levels=None, device=None, return_tensor=False,
-                     random_chunk_kwargs={}):
+    def check_params(
+        cls,
+        recording,
+        peak_sign="neg",
+        detect_threshold=5,
+        exclude_sweep_ms=0.1,
+        noise_levels=None,
+        device=None,
+        return_tensor=False,
+        random_chunk_kwargs={},
+    ):
         if not HAVE_TORCH:
             raise ModuleNotFoundError('"by_channel_torch" needs torch which is not installed')
-        assert peak_sign in ('both', 'neg', 'pos')
+        assert peak_sign in ("both", "neg", "pos")
         if device is None:
             device = "cuda" if torch.cuda.is_available() else "cpu"
 
         if noise_levels is None:
             noise_levels = get_noise_levels(recording, return_scaled=False, **random_chunk_kwargs)
         abs_threholds = noise_levels * detect_threshold
-        exclude_sweep_size = int(exclude_sweep_ms * recording.get_sampling_frequency() / 1000.)
+        exclude_sweep_size = int(exclude_sweep_ms * recording.get_sampling_frequency() / 1000.0)
 
         return (peak_sign, abs_threholds, exclude_sweep_size, device, return_tensor)
-    
+
     @classmethod
     def get_method_margin(cls, *args):
         exclude_sweep_size = args[2]
         return exclude_sweep_size
 
     @classmethod
     def detect_peaks(cls, traces, peak_sign, abs_threholds, exclude_sweep_size, device, return_tensor):
         sample_inds, chan_inds = _torch_detect_peaks(traces, peak_sign, abs_threholds, exclude_sweep_size, None, device)
         if not return_tensor:
             sample_inds = np.array(sample_inds.cpu())
             chan_inds = np.array(chan_inds.cpu())
         return sample_inds, chan_inds
 
 
-class DetectPeakLocallyExclusive:
+class DetectPeakLocallyExclusive(PeakDetectorWrapper):
     """Detect peaks using the 'locally exclusive' method."""
 
-    name = 'locally_exclusive'
-    engine = 'numba'
+    name = "locally_exclusive"
+    engine = "numba"
     preferred_mp_context = None
-    params_doc = DetectPeakByChannel.params_doc + """
+    params_doc = (
+        DetectPeakByChannel.params_doc
+        + """
     local_radius_um: float
         The radius to use to select neighbour channels for locally exclusive detection.
     """
-    @classmethod
-    def check_params(cls, recording, peak_sign='neg', detect_threshold=5,
-                     exclude_sweep_ms=0.1, local_radius_um=50, noise_levels=None, random_chunk_kwargs={}):
+    )
 
+    @classmethod
+    def check_params(
+        cls,
+        recording,
+        peak_sign="neg",
+        detect_threshold=5,
+        exclude_sweep_ms=0.1,
+        local_radius_um=50,
+        noise_levels=None,
+        random_chunk_kwargs={},
+    ):
         if not HAVE_NUMBA:
             raise ModuleNotFoundError('"locally_exclusive" needs numba which is not installed')
 
-        args = DetectPeakByChannel.check_params(recording, peak_sign=peak_sign, detect_threshold=detect_threshold,
-                                                exclude_sweep_ms=exclude_sweep_ms, noise_levels=noise_levels,
-                                                random_chunk_kwargs=random_chunk_kwargs)
+        args = DetectPeakByChannel.check_params(
+            recording,
+            peak_sign=peak_sign,
+            detect_threshold=detect_threshold,
+            exclude_sweep_ms=exclude_sweep_ms,
+            noise_levels=noise_levels,
+            random_chunk_kwargs=random_chunk_kwargs,
+        )
 
         channel_distance = get_channel_distances(recording)
         neighbours_mask = channel_distance < local_radius_um
-        return args + (neighbours_mask, )
+        return args + (neighbours_mask,)
 
     @classmethod
     def get_method_margin(cls, *args):
         exclude_sweep_size = args[2]
         return exclude_sweep_size
 
     @classmethod
     def detect_peaks(cls, traces, peak_sign, abs_threholds, exclude_sweep_size, neighbours_mask):
-        assert HAVE_NUMBA, 'You need to install numba'
+        assert HAVE_NUMBA, "You need to install numba"
         traces_center = traces[exclude_sweep_size:-exclude_sweep_size, :]
 
-        if peak_sign in ('pos', 'both'):
+        if peak_sign in ("pos", "both"):
             peak_mask = traces_center > abs_threholds[None, :]
-            peak_mask = _numba_detect_peak_pos(traces, traces_center, peak_mask, exclude_sweep_size,
-                                            abs_threholds, peak_sign, neighbours_mask)
+            peak_mask = _numba_detect_peak_pos(
+                traces, traces_center, peak_mask, exclude_sweep_size, abs_threholds, peak_sign, neighbours_mask
+            )
 
-        if peak_sign in ('neg', 'both'):
-            if peak_sign == 'both':
+        if peak_sign in ("neg", "both"):
+            if peak_sign == "both":
                 peak_mask_pos = peak_mask.copy()
 
             peak_mask = traces_center < -abs_threholds[None, :]
-            peak_mask = _numba_detect_peak_neg(traces, traces_center, peak_mask, exclude_sweep_size,
-                                            abs_threholds, peak_sign, neighbours_mask)
+            peak_mask = _numba_detect_peak_neg(
+                traces, traces_center, peak_mask, exclude_sweep_size, abs_threholds, peak_sign, neighbours_mask
+            )
 
-            if peak_sign == 'both':
+            if peak_sign == "both":
                 peak_mask = peak_mask | peak_mask_pos
 
         # Find peaks and correct for time shift
         peak_sample_ind, peak_chan_ind = np.nonzero(peak_mask)
         peak_sample_ind += exclude_sweep_size
 
         return peak_sample_ind, peak_chan_ind
 
 
-class DetectPeakLocallyExclusiveTorch:
-    """Detect peaks using the 'locally exclusive' method with pytorch.
-    """
+class DetectPeakLocallyExclusiveTorch(PeakDetectorWrapper):
+    """Detect peaks using the 'locally exclusive' method with pytorch."""
 
-    name = 'locally_exclusive_torch'
-    engine = 'torch'
+    name = "locally_exclusive_torch"
+    engine = "torch"
     preferred_mp_context = "spawn"
-    params_doc = DetectPeakByChannel.params_doc + """
+    params_doc = (
+        DetectPeakByChannel.params_doc
+        + """
     local_radius_um: float
         The radius to use to select neighbour channels for locally exclusive detection.
     """
+    )
 
     @classmethod
-    def check_params(cls, recording, peak_sign='neg', detect_threshold=5,
-                     exclude_sweep_ms=0.1, noise_levels=None, device=None, local_radius_um=50, return_tensor=False,
-                     random_chunk_kwargs={}):
+    def check_params(
+        cls,
+        recording,
+        peak_sign="neg",
+        detect_threshold=5,
+        exclude_sweep_ms=0.1,
+        noise_levels=None,
+        device=None,
+        local_radius_um=50,
+        return_tensor=False,
+        random_chunk_kwargs={},
+    ):
         if not HAVE_TORCH:
             raise ModuleNotFoundError('"by_channel_torch" needs torch which is not installed')
-        args = DetectPeakByChannelTorch.check_params(recording, peak_sign=peak_sign, detect_threshold=detect_threshold,
-                                                     exclude_sweep_ms=exclude_sweep_ms, noise_levels=noise_levels,
-                                                     device=device, return_tensor=return_tensor, 
-                                                     random_chunk_kwargs=random_chunk_kwargs)
+        args = DetectPeakByChannelTorch.check_params(
+            recording,
+            peak_sign=peak_sign,
+            detect_threshold=detect_threshold,
+            exclude_sweep_ms=exclude_sweep_ms,
+            noise_levels=noise_levels,
+            device=device,
+            return_tensor=return_tensor,
+            random_chunk_kwargs=random_chunk_kwargs,
+        )
 
         channel_distance = get_channel_distances(recording)
         neighbour_indices_by_chan = []
         num_channels = recording.get_num_channels()
         for chan in range(num_channels):
             neighbour_indices_by_chan.append(np.nonzero(channel_distance[chan] < local_radius_um)[0])
         max_neighbs = np.max([len(neigh) for neigh in neighbour_indices_by_chan])
         neighbours_idxs = num_channels * np.ones((num_channels, max_neighbs), dtype=int)
         for i, neigh in enumerate(neighbour_indices_by_chan):
-            neighbours_idxs[i, :len(neigh)] = neigh
-        return args + (neighbours_idxs, )
-    
+            neighbours_idxs[i, : len(neigh)] = neigh
+        return args + (neighbours_idxs,)
+
     @classmethod
     def get_method_margin(cls, *args):
         exclude_sweep_size = args[2]
         return exclude_sweep_size
 
     @classmethod
     def detect_peaks(cls, traces, peak_sign, abs_threholds, exclude_sweep_size, device, return_tensor, neighbor_idxs):
-        sample_inds, chan_inds = _torch_detect_peaks(traces, peak_sign, abs_threholds, exclude_sweep_size, 
-                                                     neighbor_idxs, device)
-        if not return_tensor:
+        sample_inds, chan_inds = _torch_detect_peaks(
+            traces, peak_sign, abs_threholds, exclude_sweep_size, neighbor_idxs, device
+        )
+        if not return_tensor and isinstance(sample_inds, torch.Tensor) and isinstance(chan_inds, torch.Tensor):
             sample_inds = np.array(sample_inds.cpu())
             chan_inds = np.array(chan_inds.cpu())
         return sample_inds, chan_inds
 
 
 if HAVE_NUMBA:
+
     @numba.jit(parallel=False)
-    def _numba_detect_peak_pos(traces, traces_center, peak_mask, exclude_sweep_size,
-                               abs_threholds, peak_sign, neighbours_mask):
+    def _numba_detect_peak_pos(
+        traces, traces_center, peak_mask, exclude_sweep_size, abs_threholds, peak_sign, neighbours_mask
+    ):
         num_chans = traces_center.shape[1]
         for chan_ind in range(num_chans):
             for s in range(peak_mask.shape[0]):
                 if not peak_mask[s, chan_ind]:
                     continue
                 for neighbour in range(num_chans):
                     if not neighbours_mask[chan_ind, neighbour]:
                         continue
                     for i in range(exclude_sweep_size):
                         if chan_ind != neighbour:
                             peak_mask[s, chan_ind] &= traces_center[s, chan_ind] >= traces_center[s, neighbour]
                         peak_mask[s, chan_ind] &= traces_center[s, chan_ind] > traces[s + i, neighbour]
-                        peak_mask[s, chan_ind] &= traces_center[s, chan_ind] >= traces[exclude_sweep_size + s + i + 1, neighbour]
+                        peak_mask[s, chan_ind] &= (
+                            traces_center[s, chan_ind] >= traces[exclude_sweep_size + s + i + 1, neighbour]
+                        )
                         if not peak_mask[s, chan_ind]:
                             break
                     if not peak_mask[s, chan_ind]:
                         break
         return peak_mask
 
     @numba.jit(parallel=False)
-    def _numba_detect_peak_neg(traces, traces_center, peak_mask, exclude_sweep_size,
-                               abs_threholds, peak_sign, neighbours_mask):
+    def _numba_detect_peak_neg(
+        traces, traces_center, peak_mask, exclude_sweep_size, abs_threholds, peak_sign, neighbours_mask
+    ):
         num_chans = traces_center.shape[1]
         for chan_ind in range(num_chans):
             for s in range(peak_mask.shape[0]):
                 if not peak_mask[s, chan_ind]:
                     continue
                 for neighbour in range(num_chans):
                     if not neighbours_mask[chan_ind, neighbour]:
                         continue
                     for i in range(exclude_sweep_size):
                         if chan_ind != neighbour:
                             peak_mask[s, chan_ind] &= traces_center[s, chan_ind] <= traces_center[s, neighbour]
                         peak_mask[s, chan_ind] &= traces_center[s, chan_ind] < traces[s + i, neighbour]
-                        peak_mask[s, chan_ind] &= traces_center[s, chan_ind] <= traces[exclude_sweep_size + s + i + 1, neighbour]
+                        peak_mask[s, chan_ind] &= (
+                            traces_center[s, chan_ind] <= traces[exclude_sweep_size + s + i + 1, neighbour]
+                        )
                         if not peak_mask[s, chan_ind]:
                             break
                     if not peak_mask[s, chan_ind]:
                         break
         return peak_mask
 
 
 if HAVE_TORCH:
+
     @torch.no_grad()
-    def _torch_detect_peaks(traces, peak_sign, abs_thresholds, 
-                            exclude_sweep_size=5, neighbours_mask=None, device=None):
+    def _torch_detect_peaks(traces, peak_sign, abs_thresholds, exclude_sweep_size=5, neighbours_mask=None, device=None):
         """
         Voltage thresholding detection and deduplication with torch.
         Implementation from Charlie Windolf:
         https://github.com/cwindolf/spike-psvae/blob/ba0a985a075776af892f09adfd453b8d9db168b9/spike_psvae/detect.py#L350
         Parameters
         ----------
         traces : np.array
@@ -461,15 +708,15 @@
         peak_sign : str, optional
             "neg", "pos" or "both", by default "neg"
         exclude_sweep_size : int, optional
             How many temporal neighbors to compare with during argrelmin, by default 5
             Called `order` in original the implementation. The `max_window` parameter, used
             for deduplication, is now set as 2* exclude_sweep_size
         neighbor_mask : np.array, optional
-            If given, a matrix with shape (num_channels, num_neighbours) with 
+            If given, a matrix with shape (num_channels, num_neighbours) with
             neighbour indices for each channel. The matrix needs to be rectangular and
             padded to num_channels, by default None
         device : str, optional
             "cpu", "cuda", or None. If None and cuda is available, "cuda" is selected, by default None
 
         Returns
         -------
@@ -478,230 +725,240 @@
         """
         # TODO handle GPU-memory at chunk executor level
         # for now we keep the same batching mechanism from spike_psvae
         # this will be adjusted based on: num jobs, num gpus, num neighbors
         MAXCOPY = 8
 
         num_samples, num_channels = traces.shape
+        dtype = torch.float32
+        empty_return_value = (torch.tensor([], dtype=dtype), torch.tensor([], dtype=dtype))
 
-        # -- torch argrelmin
+        # The function uses maxpooling to look for maximum
         if peak_sign == "neg":
-            neg_traces = torch.as_tensor(
-                -traces, device=device, dtype=torch.float
-            )
+            traces = -traces
         elif peak_sign == "pos":
-            neg_traces = torch.as_tensor(
-                traces, device=device, dtype=torch.float
-            )
+            traces = traces
         elif peak_sign == "both":
-            neg_traces = torch.as_tensor(
-                -np.abs(traces), device=device, dtype=torch.float
-            )
+            traces = np.abs(traces)
+
+        traces_tensor = torch.as_tensor(traces, device=device, dtype=torch.float)
         thresholds_torch = torch.as_tensor(abs_thresholds, device=device, dtype=torch.float)
-        traces_norm = neg_traces / thresholds_torch
+        normalized_traces = traces_tensor / thresholds_torch
 
-        max_amps, inds = F.max_pool2d_with_indices(
-            traces_norm[None, None],
+        max_amplitudes, indices = F.max_pool2d_with_indices(
+            input=normalized_traces[None, None],
             kernel_size=[2 * exclude_sweep_size + 1, 1],
             stride=1,
             padding=[exclude_sweep_size, 0],
         )
-        max_amps = max_amps[0, 0]
-        inds = inds[0, 0]
-        # torch `inds` gives loc of argmax at each position
+        max_amplitudes = max_amplitudes[0, 0]
+        indices = indices[0, 0]
+        # torch `indices` gives loc of argmax at each position
         # find those which actually *were* the max
-        unique_inds = inds.unique()
-        window_max_inds = unique_inds[inds.view(-1)[unique_inds] == unique_inds]
+        unique_indices = indices.unique()
+        window_max_indices = unique_indices[indices.view(-1)[unique_indices] == unique_indices]
 
         # voltage threshold
-        max_amps_at_inds = max_amps.view(-1)[window_max_inds]
-        crossings = torch.nonzero(max_amps_at_inds > 1).squeeze()
+        max_amplitudes_at_indices = max_amplitudes.view(-1)[window_max_indices]
+        crossings = torch.nonzero(max_amplitudes_at_indices > 1).squeeze()
         if not crossings.numel():
-            return np.array([]), np.array([]), np.array([])
+            return empty_return_value
 
         # -- unravel the spike index
         # (right now the indices are into flattened recording)
-        peak_inds = window_max_inds[crossings]
-        sample_inds = torch.div(peak_inds, num_channels, rounding_mode="floor")
-        chan_inds = peak_inds % num_channels
-        amplitudes = max_amps_at_inds[crossings]
+        peak_indices = window_max_indices[crossings]
+        sample_indices = torch.div(peak_indices, num_channels, rounding_mode="floor")
+        channel_indices = peak_indices % num_channels
+        amplitudes = max_amplitudes_at_indices[crossings]
 
         # we need this due to the padding in convolution
-        valid_inds = torch.nonzero(
-            (0 < sample_inds) & (sample_inds < traces.shape[0] - 1)
-        ).squeeze()
-        if not sample_inds.numel():
-            return np.array([]), np.array([]), np.array([])
-        sample_inds = sample_inds[valid_inds]
-        chan_inds = chan_inds[valid_inds]
-        amplitudes = amplitudes[valid_inds]
+        valid_indices = torch.nonzero((0 < sample_indices) & (sample_indices < traces.shape[0] - 1)).squeeze()
+        if not sample_indices.numel():
+            return empty_return_value
+        sample_indices = sample_indices[valid_indices]
+        channel_indices = channel_indices[valid_indices]
+        amplitudes = amplitudes[valid_indices]
 
         # -- deduplication
         # We deduplicate if the channel index is provided.
         if neighbours_mask is not None:
-            neighbours_mask = torch.tensor(
-                neighbours_mask, device=device, dtype=torch.long
-            )
+            neighbours_mask = torch.tensor(neighbours_mask, device=device, dtype=torch.long)
 
             # -- temporal max pool
-            # still not sure why we can't just use `max_amps` instead of making
+            # still not sure why we can't just use `max_amplitudes` instead of making
             # this sparsely populated array, but it leads to a different result.
-            max_amps[:] = 0
-            max_amps[sample_inds, chan_inds] = amplitudes
+            max_amplitudes[:] = 0
+            max_amplitudes[sample_indices, channel_indices] = amplitudes
             max_window = 2 * exclude_sweep_size
-            max_amps = F.max_pool2d(
-                max_amps[None, None],
+            max_amplitudes = F.max_pool2d(
+                max_amplitudes[None, None],
                 kernel_size=[2 * max_window + 1, 1],
                 stride=1,
                 padding=[max_window, 0],
             )[0, 0]
 
             # -- spatial max pool with channel index
             # batch size heuristic, see __doc__
             max_neighbs = neighbours_mask.shape[1]
             batch_size = int(np.ceil(num_samples / (max_neighbs / MAXCOPY)))
             for bs in range(0, num_samples, batch_size):
                 be = min(num_samples, bs + batch_size)
-                max_amps[bs:be] = torch.max(
-                    F.pad(max_amps[bs:be], (0, 1))[:, neighbours_mask], 2
-                )[0]
+                max_amplitudes[bs:be] = torch.max(F.pad(max_amplitudes[bs:be], (0, 1))[:, neighbours_mask], 2)[0]
 
             # -- deduplication
-            dedup = torch.nonzero(
-                amplitudes >= max_amps[sample_inds, chan_inds] - 1e-8
+            deduplication_indices = torch.nonzero(
+                amplitudes >= max_amplitudes[sample_indices, channel_indices] - 1e-8
             ).squeeze()
-            if not dedup.numel():
-                return np.array([]), np.array([]), np.array([])
-            sample_inds = sample_inds[dedup]
-            chan_inds = chan_inds[dedup]
-            amplitudes = amplitudes[dedup]
-
-        return sample_inds, chan_inds
+            if not deduplication_indices.numel():
+                return empty_return_value
+            sample_indices = sample_indices[deduplication_indices]
+            channel_indices = channel_indices[deduplication_indices]
+            amplitudes = amplitudes[deduplication_indices]
+
+        return sample_indices, channel_indices
 
 
-class DetectPeakLocallyExclusiveOpenCL:
-    name = 'locally_exclusive_cl'
-    engine = 'opencl'
+class DetectPeakLocallyExclusiveOpenCL(PeakDetectorWrapper):
+    name = "locally_exclusive_cl"
+    engine = "opencl"
     preferred_mp_context = None
-    params_doc = DetectPeakLocallyExclusive.params_doc + """
+    params_doc = (
+        DetectPeakLocallyExclusive.params_doc
+        + """
     opencl_context_kwargs: None or dict
         kwargs to create the opencl context
     """
+    )
+
     @classmethod
-    def check_params(cls, recording, peak_sign='neg', detect_threshold=5,
-                     exclude_sweep_ms=0.1, local_radius_um=50, noise_levels=None, random_chunk_kwargs={}):
-        
+    def check_params(
+        cls,
+        recording,
+        peak_sign="neg",
+        detect_threshold=5,
+        exclude_sweep_ms=0.1,
+        local_radius_um=50,
+        noise_levels=None,
+        random_chunk_kwargs={},
+    ):
         # TODO refactor with other classes
-        assert peak_sign in ('both', 'neg', 'pos')
+        assert peak_sign in ("both", "neg", "pos")
         if noise_levels is None:
             noise_levels = get_noise_levels(recording, return_scaled=False, **random_chunk_kwargs)
         abs_threholds = noise_levels * detect_threshold
-        exclude_sweep_size = int(exclude_sweep_ms * recording.get_sampling_frequency() / 1000.)
+        exclude_sweep_size = int(exclude_sweep_ms * recording.get_sampling_frequency() / 1000.0)
         channel_distance = get_channel_distances(recording)
         neighbours_mask = channel_distance < local_radius_um
-        
+
         executor = OpenCLDetectPeakExecutor(abs_threholds, exclude_sweep_size, neighbours_mask, peak_sign)
-        
-        return (executor, )
+
+        return (executor,)
 
     @classmethod
     def get_method_margin(cls, *args):
         executor = args[0]
         return executor.exclude_sweep_size
 
     @classmethod
     def detect_peaks(cls, traces, executor):
         peak_sample_ind, peak_chan_ind = executor.detect_peak(traces)
-        
+
         return peak_sample_ind, peak_chan_ind
 
 
 class OpenCLDetectPeakExecutor:
     def __init__(self, abs_threholds, exclude_sweep_size, neighbours_mask, peak_sign):
         import pyopencl
+
         self.chunk_size = None
-        
-        self.abs_threholds = abs_threholds.astype('float32')
+
+        self.abs_threholds = abs_threholds.astype("float32")
         self.exclude_sweep_size = exclude_sweep_size
-        self.neighbours_mask = neighbours_mask.astype('uint8')
+        self.neighbours_mask = neighbours_mask.astype("uint8")
         self.peak_sign = peak_sign
         self.ctx = None
         self.queue = None
         self.x = 0
-    
+
     def create_buffers_and_compile(self, chunk_size):
         import pyopencl
+
         mf = pyopencl.mem_flags
         try:
             self.device = pyopencl.get_platforms()[0].get_devices()[0]
             self.ctx = pyopencl.Context(devices=[self.device])
         except Exception as e:
-            print('error create context ', e)
+            print("error create context ", e)
 
         self.queue = pyopencl.CommandQueue(self.ctx)
         self.max_wg_size = self.ctx.devices[0].get_info(pyopencl.device_info.MAX_WORK_GROUP_SIZE)
         self.chunk_size = chunk_size
 
-        self.neighbours_mask_cl = pyopencl.Buffer(self.ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=self.neighbours_mask)
+        self.neighbours_mask_cl = pyopencl.Buffer(
+            self.ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=self.neighbours_mask
+        )
         self.abs_threholds_cl = pyopencl.Buffer(self.ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=self.abs_threholds)
 
         num_channels = self.neighbours_mask.shape[0]
         self.traces_cl = pyopencl.Buffer(self.ctx, mf.READ_WRITE, size=int(chunk_size * num_channels * 4))
 
-        # TODO estimate smaller 
-        self.num_peaks = np.zeros(1, dtype='int32')
-        self.num_peaks_cl = pyopencl.Buffer(self.ctx, mf.READ_WRITE| mf.COPY_HOST_PTR, hostbuf=self.num_peaks)
+        # TODO estimate smaller
+        self.num_peaks = np.zeros(1, dtype="int32")
+        self.num_peaks_cl = pyopencl.Buffer(self.ctx, mf.READ_WRITE | mf.COPY_HOST_PTR, hostbuf=self.num_peaks)
 
         nb_max_spike_in_chunk = num_channels * chunk_size
-        self.peaks = np.zeros(nb_max_spike_in_chunk, dtype=[('sample_index', 'int32'), ('channel_index', 'int32')])
-        self.peaks_cl = pyopencl.Buffer(self.ctx, mf.READ_WRITE| mf.COPY_HOST_PTR, hostbuf=self.peaks)
+        self.peaks = np.zeros(nb_max_spike_in_chunk, dtype=[("sample_index", "int32"), ("channel_index", "int32")])
+        self.peaks_cl = pyopencl.Buffer(self.ctx, mf.READ_WRITE | mf.COPY_HOST_PTR, hostbuf=self.peaks)
 
         variables = dict(
             chunk_size=int(self.chunk_size),
             exclude_sweep_size=int(self.exclude_sweep_size),
-            peak_sign={'pos': 1, 'neg': -1}[self.peak_sign],
+            peak_sign={"pos": 1, "neg": -1}[self.peak_sign],
             num_channels=num_channels,
         )
 
         kernel_formated = processor_kernel % variables
         prg = pyopencl.Program(self.ctx, kernel_formated)
         self.opencl_prg = prg.build()  # options='-cl-mad-enable'
-        self.kern_detect_peaks = getattr(self.opencl_prg, 'detect_peaks')
+        self.kern_detect_peaks = getattr(self.opencl_prg, "detect_peaks")
 
-        self.kern_detect_peaks.set_args(self.traces_cl,
-                                        self.neighbours_mask_cl,
-                                        self.abs_threholds_cl,
-                                        self.peaks_cl,
-                                        self.num_peaks_cl)
+        self.kern_detect_peaks.set_args(
+            self.traces_cl, self.neighbours_mask_cl, self.abs_threholds_cl, self.peaks_cl, self.num_peaks_cl
+        )
 
         s = self.chunk_size - 2 * self.exclude_sweep_size
-        self.global_size = (s, )
+        self.global_size = (s,)
         self.local_size = None
 
-
     def detect_peak(self, traces):
         self.x += 1
 
         import pyopencl
+
         if self.chunk_size is None or self.chunk_size != traces.shape[0]:
             self.create_buffers_and_compile(traces.shape[0])
-        event = pyopencl.enqueue_copy(self.queue,  self.traces_cl, traces.astype('float32'))
+        event = pyopencl.enqueue_copy(self.queue, self.traces_cl, traces.astype("float32"))
 
-        pyopencl.enqueue_nd_range_kernel(self.queue,  self.kern_detect_peaks, self.global_size, self.local_size,)
+        pyopencl.enqueue_nd_range_kernel(
+            self.queue,
+            self.kern_detect_peaks,
+            self.global_size,
+            self.local_size,
+        )
 
-        event = pyopencl.enqueue_copy(self.queue,  self.traces_cl, traces.astype('float32'))
-        event = pyopencl.enqueue_copy(self.queue,  self.traces_cl, traces.astype('float32'))
-        event = pyopencl.enqueue_copy(self.queue,  self.num_peaks,self.num_peaks_cl)
-        event = pyopencl.enqueue_copy(self.queue,  self.peaks, self.peaks_cl)
+        event = pyopencl.enqueue_copy(self.queue, self.traces_cl, traces.astype("float32"))
+        event = pyopencl.enqueue_copy(self.queue, self.traces_cl, traces.astype("float32"))
+        event = pyopencl.enqueue_copy(self.queue, self.num_peaks, self.num_peaks_cl)
+        event = pyopencl.enqueue_copy(self.queue, self.peaks, self.peaks_cl)
         event.wait()
 
         n = self.num_peaks[0]
         peaks = self.peaks[:n]
-        peak_sample_ind = peaks['sample_index'].astype('int64')
-        peak_chan_ind = peaks['channel_index'].astype('int64')
+        peak_sample_ind = peaks["sample_index"].astype("int64")
+        peak_chan_ind = peaks["channel_index"].astype("int64")
 
         return peak_sample_ind, peak_chan_ind
 
 
 processor_kernel = """
 #define chunk_size %(chunk_size)d
 #define exclude_sweep_size %(exclude_sweep_size)d
@@ -721,67 +978,67 @@
                         __global  uchar *neighbours_mask,
                         __global  float *abs_threholds,
                         //out
                         __global  st_peak *peaks,
                         volatile __global int *num_peaks
                 ){
     int pos = get_global_id(0);
-    
+
     if (pos == 0){
         *num_peaks = 0;
     }
     // this barrier OK if the first group is run first
     barrier(CLK_GLOBAL_MEM_FENCE);
 
     if (pos>=(chunk_size - (2 * exclude_sweep_size))){
         return;
     }
-    
+
 
     float v;
     uchar peak;
     uchar is_neighbour;
-    
+
     int index;
-    
+
     int i_peak;
 
-    
+
     for (int chan=0; chan<num_channels; chan++){
-        
+
         //v = traces[(pos + exclude_sweep_size)*num_channels + chan];
         index = (pos + exclude_sweep_size) * num_channels + chan;
         v = traces[index];
-        
+
         if(peak_sign==1){
             if (v>abs_threholds[chan]){peak=1;}
             else {peak=0;}
         }
         else if(peak_sign==-1){
             if (v<-abs_threholds[chan]){peak=1;}
             else {peak=0;}
         }
-        
+
         if (peak == 1){
             for (int chan_neigh=0; chan_neigh<num_channels; chan_neigh++){
-            
+
                 is_neighbour = neighbours_mask[chan * num_channels + chan_neigh];
                 if (is_neighbour == 0){continue;}
                 //if (chan == chan_neigh){continue;}
 
                 index = (pos + exclude_sweep_size) * num_channels + chan_neigh;
                 if(peak_sign==1){
                     peak = peak && (v>=traces[index]);
                 }
                 else if(peak_sign==-1){
                     peak = peak && (v<=traces[index]);
                 }
-                
+
                 if (peak==0){break;}
-                
+
                 if(peak_sign==1){
                     for (int i=1; i<=exclude_sweep_size; i++){
                         peak = peak && (v>traces[(pos + exclude_sweep_size - i)*num_channels + chan_neigh]) && (v>=traces[(pos + exclude_sweep_size + i)*num_channels + chan_neigh]);
                         if (peak==0){break;}
                     }
                 }
                 else if(peak_sign==-1){
@@ -790,30 +1047,32 @@
                         if (peak==0){break;}
                     }
                 }
 
             }
 
         }
-        
+
         if (peak==1){
-            //append to 
+            //append to
             i_peak = atomic_inc(num_peaks);
             // sample_index is LOCAL to fifo
             peaks[i_peak].sample_index = pos + exclude_sweep_size;
             peaks[i_peak].channel_index = chan;
         }
     }
-    
+
 }
 """
 
 
 # TODO make a dict with name+engine entry later
-_methods_list = [DetectPeakByChannel, DetectPeakLocallyExclusive,
-                 DetectPeakLocallyExclusiveOpenCL,
-                 DetectPeakByChannelTorch, DetectPeakLocallyExclusiveTorch]
+_methods_list = [
+    DetectPeakByChannel,
+    DetectPeakLocallyExclusive,
+    DetectPeakLocallyExclusiveOpenCL,
+    DetectPeakByChannelTorch,
+    DetectPeakLocallyExclusiveTorch,
+]
 detect_peak_methods = {m.name: m for m in _methods_list}
 method_doc = make_multi_method_doc(_methods_list)
-detect_peaks.__doc__ = detect_peaks.__doc__.format(method_doc=method_doc,
-                                                   job_doc=_shared_job_kwargs_doc)
-
+detect_peaks.__doc__ = detect_peaks.__doc__.format(method_doc=method_doc, job_doc=_shared_job_kwargs_doc)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sortingcomponents/peak_pipeline.py` & `spikeinterface-0.98.0/src/spikeinterface/core/recording_tools.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,327 +1,405 @@
-"""
-Pipeline on peaks : functions that can be chained after peak detection
-to compute some additional features on-the-fly:
-  * peak localization
-  * peak-to-peak
-  * ...
-
-There are two ways for using theses "plugins":
-  * during `peak_detect()`
-  * when peaks are already detected and reduced with `select_peaks()`
-"""
-from typing import Optional, List, Type 
+from copy import deepcopy
+from typing import Literal
+import warnings
 
 import numpy as np
 
-from spikeinterface.core import BaseRecording, get_chunk_with_margin
-from spikeinterface.core.job_tools import ChunkRecordingExecutor, fix_job_kwargs, _shared_job_kwargs_doc
-from spikeinterface.core import get_channel_distances
-
-class PipelineNode:
-    def __init__(
-        self, recording: BaseRecording, return_output: bool = True, parents: Optional[List[Type["PipelineNode"]]] = None
-    ):
-        """
-        This is a generic object that will make some computation on peaks given a buffer of traces.
-        Typically used for exctrating features (amplitudes, localization, ...)
-
-        A Node can optionally connect to other nodes with the parents and receive inputs from them.
-
-        Parameters
-        ----------
-        recording : BaseRecording
-            The recording object.
-        parents : Optional[List[PipelineNode]], optional
-            Pass parents nodes to perform a previous computation, by default None
-        return_output : bool, optional
-            Whether or not the output of the node is returned by the pipeline, by default False
-        """
-
-        self.recording = recording
-        self.return_output = return_output
-        if isinstance(parents, str):
-            # only one parents is allowed
-            parents = [parents]
-        self.parents = parents
-        
-        self._kwargs = dict()
-        
-    def get_trace_margin(self):
-        # can optionaly be overwritten
-        return 0
-
-    def get_dtype(self):
-        raise NotImplementedError
-
-
-class WaveformExtractorNode(PipelineNode):
-    """Base class for waveform extractor"""
-
-    def __init__(
-        self,
-        recording: BaseRecording,
-        ms_before: float,
-        ms_after: float,
-        parents: Optional[List[PipelineNode]] = None,
-        return_output: bool = False,
-    ):
-        """
-        Base class for waveform extractor. Contains logic to handle the temporal interval in which to extract the
-        waveforms.
-
-        Parameters
-        ----------
-        recording : BaseRecording
-            The recording object.
-        parents : Optional[List[PipelineNode]], optional
-            Pass parents nodes to perform a previous computation, by default None
-        return_output : bool, optional
-            Whether or not the output of the node is returned by the pipeline, by default False
-        ms_before : float, optional
-            The number of milliseconds to include before the peak of the spike, by default 1.
-        ms_after : float, optional
-            The number of milliseconds to include after the peak of the spike, by default 1.
-        """
-
-        PipelineNode.__init__(self, recording=recording, parents=parents, return_output=return_output)
-        self.ms_before = ms_before
-        self.ms_after = ms_after
-        self.nbefore = int(ms_before * recording.get_sampling_frequency() / 1000.0)
-        self.nafter = int(ms_after * recording.get_sampling_frequency() / 1000.0)
-
-
-class ExtractDenseWaveforms(WaveformExtractorNode):
-    def __init__(
-        self,
-        recording: BaseRecording,
-        ms_before: float,
-        ms_after: float,
-        parents: Optional[List[PipelineNode]] = None,
-        return_output: bool = False,
-    ):
-        """
-        Extract dense waveforms from a recording. This is the default waveform extractor which extracts the waveforms
-        for further cmoputation on them.
-
-
-        Parameters
-        ----------
-        recording : BaseRecording
-            The recording object.
-        parents : Optional[List[PipelineNode]], optional
-            Pass parents nodes to perform a previous computation, by default None
-        return_output : bool, optional
-            Whether or not the output of the node is returned by the pipeline, by default False
-        ms_before : float, optional
-            The number of milliseconds to include before the peak of the spike, by default 1.
-        ms_after : float, optional
-            The number of milliseconds to include after the peak of the spike, by default 1.
-        """
-
-        WaveformExtractorNode.__init__(
-            self,
-            recording=recording,
-            parents=parents,
-            ms_before=ms_before,
-            ms_after=ms_after,
-            return_output=return_output,
-        )
-        # this is a bad hack to differentiate in the child if the parents is dense or not.
-        self.neighbours_mask = None
-
 
-    def get_trace_margin(self):
-        return max(self.nbefore, self.nafter)
+def get_random_data_chunks(
+    recording,
+    return_scaled=False,
+    num_chunks_per_segment=20,
+    chunk_size=10000,
+    concatenated=True,
+    seed=0,
+    margin_frames=0,
+):
+    """
+    Extract random chunks across segments
 
-    def compute(self, traces, peaks):
-        waveforms = traces[peaks["sample_ind"][:, None] + np.arange(-self.nbefore, self.nafter)]
-        return waveforms
-
-
-class ExtractSparseWaveforms(WaveformExtractorNode):
-    def __init__(
-        self,
-        recording: BaseRecording,
-        ms_before: float,
-        ms_after: float,
-        parents: Optional[List[PipelineNode]] = None,
-        return_output: bool = False,
-        local_radius_um: float = 100.0,
-    ):
-        """
-        Extract sparse waveforms from a recording. The strategy in this specific node is to reshape the waveforms
-        to eliminate their inactive channels. This is achieved by changing thei shape from
-        (num_waveforms, num_time_samples, num_channels) to (num_waveforms, num_time_samples, max_num_active_channels).
-
-        Where max_num_active_channels is the max number of active channels in the waveforms. This is done by selecting
-        the max number of non-zeros entries in the sparsity neighbourhood mask.
-
-        Note that not all waveforms will have the same number of active channels. Even in the reduced form some of
-        the channels will be inactive and are filled with zeros.
-
-        Parameters
-        ----------
-        recording : BaseRecording
-            The recording object.
-        parents : Optional[List[PipelineNode]], optional
-            Pass parents nodes to perform a previous computation, by default None
-        return_output : bool, optional
-            Whether or not the output of the node is returned by the pipeline, by default False
-        ms_before : float, optional
-            The number of milliseconds to include before the peak of the spike, by default 1.
-        ms_after : float, optional
-            The number of milliseconds to include after the peak of the spike, by default 1.
-
-
-        """
-        WaveformExtractorNode.__init__(
-            self,
-            recording=recording,
-            parents=parents,
-            ms_before=ms_before,
-            ms_after=ms_after,
-            return_output=return_output,
-        )
+    This is used for instance in get_noise_levels() to estimate noise on traces.
 
-        self.local_radius_um = local_radius_um
-        self.contact_locations = recording.get_channel_locations()
-        self.channel_distance = get_channel_distances(recording)
-        self.neighbours_mask = self.channel_distance < local_radius_um
-        self.max_num_chans = np.max(np.sum(self.neighbours_mask, axis=1))
+    Parameters
+    ----------
+    recording: BaseRecording
+        The recording to get random chunks from
+    return_scaled: bool
+        If True, returned chunks are scaled to uV
+    num_chunks_per_segment: int
+        Number of chunks per segment
+    chunk_size: int
+        Size of a chunk in number of frames
+    concatenated: bool (default True)
+        If True chunk are concatenated along time axis.
+    seed: int
+        Random seed
+    Returns
+    -------
+    chunk_list: np.array
+        Array of concatenate chunks per segment
+    """
+    # TODO: if segment have differents length make another sampling that dependant on the length of the segment
+    # Should be done by changing kwargs with total_num_chunks=XXX and total_duration=YYYY
+    # And randomize the number of chunk per segment weighted by segment duration
+
+    # check chunk size
+    num_segments = recording.get_num_segments()
+    for segment_index in range(num_segments):
+        if chunk_size > recording.get_num_frames(segment_index) - 2 * margin_frames:
+            error_message = (
+                f"chunk_size is greater than the number "
+                f"of samples for segment index {segment_index}. "
+                f"Use a smaller chunk_size!"
+            )
+            raise ValueError(error_message)
+
+    rng = np.random.default_rng(seed)
+    chunk_list = []
+    low = margin_frames
+    size = num_chunks_per_segment
+    for segment_index in range(num_segments):
+        num_frames = recording.get_num_frames(segment_index)
+        high = num_frames - chunk_size - margin_frames
+        random_starts = rng.integers(low=low, high=high, size=size)
+        segment_trace_chunk = [
+            recording.get_traces(
+                start_frame=start_frame,
+                end_frame=(start_frame + chunk_size),
+                segment_index=segment_index,
+                return_scaled=return_scaled,
+            )
+            for start_frame in random_starts
+        ]
 
+        chunk_list.extend(segment_trace_chunk)
 
-    def get_trace_margin(self):
-        return max(self.nbefore, self.nafter)
+    if concatenated:
+        return np.concatenate(chunk_list, axis=0)
+    else:
+        return chunk_list
 
-    def compute(self, traces, peaks):
-        sparse_wfs = np.zeros((peaks.shape[0], self.nbefore + self.nafter, self.max_num_chans), dtype=traces.dtype)
 
-        for i, peak in enumerate(peaks):
-            (chans,) = np.nonzero(self.neighbours_mask[peak["channel_ind"]])
-            sparse_wfs[i, :, : len(chans)] = traces[
-                peak["sample_ind"] - self.nbefore : peak["sample_ind"] + self.nafter, :
-            ][:, chans]
+def get_channel_distances(recording):
+    """
+    Distance between channel pairs
+    """
+    locations = recording.get_channel_locations()
+    channel_distances = np.linalg.norm(locations[:, np.newaxis] - locations[np.newaxis, :], axis=2)
 
-        return sparse_wfs
+    return channel_distances
 
 
+def get_closest_channels(recording, channel_ids=None, num_channels=None):
+    """Get closest channels + distances
 
-def check_graph(nodes):
+    Parameters
+    ----------
+    recording: RecordingExtractor
+        The recording extractor to get closest channels
+    channel_ids: list
+        List of channels ids to compute there near neighborhood
+    num_channels: int, optional
+        Maximum number of neighborhood channels to return
+
+    Returns
+    -------
+    closest_channels_inds : array (2d)
+        Closest channel indices in ascending order for each channel id given in input
+    dists: array (2d)
+        Distance in ascending order for each channel id given in input
     """
-    Check that node list is orderd in a good (parents are before children)
+    if channel_ids is None:
+        channel_ids = recording.get_channel_ids()
+    if num_channels is None:
+        num_channels = len(channel_ids) - 1
+
+    locations = recording.get_channel_locations(channel_ids=channel_ids)
+
+    closest_channels_inds = []
+    dists = []
+    for i in range(locations.shape[0]):
+        distances = np.linalg.norm(locations[i, :] - locations, axis=1)
+        order = np.argsort(distances)
+        closest_channels_inds.append(order[1 : num_channels + 1])
+        dists.append(distances[order][1 : num_channels + 1])
+
+    return np.array(closest_channels_inds), np.array(dists)
+
+
+def get_noise_levels(
+    recording: "BaseRecording",
+    return_scaled: bool = True,
+    method: Literal["mad", "std"] = "mad",
+    force_recompute: bool = False,
+    **random_chunk_kwargs,
+):
     """
-    
-    for i, node in enumerate(nodes):
-        assert isinstance(node, PipelineNode), f"Node {node} is not an instance of PipelineNode"
-        # check that parents exists and are before in chain
-        node_parents = node.parents if node.parents else []
-        for parent in node_parents:
-            assert parent in nodes, f"Node {node} has parent {parent} that was not passed in nodes"            
-            assert nodes.index(parent) < i, f"Node are ordered incorrectly: {node} before {parent} in the pipeline definition."
+    Estimate noise for each channel using MAD methods.
+    You can use standard deviation with `method='std'`
 
-    return nodes
+    Internally it samples some chunk across segment.
+    And then, it use MAD estimator (more robust than STD)
 
+    Parameters
+    ----------
 
-def run_peak_pipeline(recording, peaks, nodes, job_kwargs, job_name='peak_pipeline', squeeze_output=True):
+    recording: BaseRecording
+        The recording extractor to get noise levels
+    return_scaled: bool
+        If True, returned noise levels are scaled to uV
+    method: str
+        'mad' or 'std'
+    force_recompute: bool
+        If True, noise levels are recomputed even if they are already stored in the recording extractor
+    random_chunk_kwargs: dict
+        Kwargs for get_random_data_chunks
+
+    Returns
+    -------
+    noise_levels: array
+        Noise levels for each channel
     """
-    Run one or several PeakPipelineStep on already detected peaks.
+
+    if return_scaled:
+        key = "noise_level_scaled"
+    else:
+        key = "noise_level_raw"
+
+    if key in recording.get_property_keys() and not force_recompute:
+        noise_levels = recording.get_property(key=key)
+    else:
+        random_chunks = get_random_data_chunks(recording, return_scaled=return_scaled, **random_chunk_kwargs)
+
+        if method == "mad":
+            med = np.median(random_chunks, axis=0, keepdims=True)
+            # hard-coded so that core doesn't depend on scipy
+            noise_levels = np.median(np.abs(random_chunks - med), axis=0) / 0.6744897501960817
+        elif method == "std":
+            noise_levels = np.std(random_chunks, axis=0)
+        recording.set_property(key, noise_levels)
+
+    return noise_levels
+
+
+def get_chunk_with_margin(
+    rec_segment,
+    start_frame,
+    end_frame,
+    channel_indices,
+    margin,
+    add_zeros=False,
+    add_reflect_padding=False,
+    window_on_margin=False,
+    dtype=None,
+):
     """
-    check_graph(nodes)
+    Helper to get chunk with margin
 
-    job_kwargs = fix_job_kwargs(job_kwargs)
-    assert all(isinstance(node, PipelineNode) for node in nodes)
+    The margin is extracted from the recording when possible. If
+    at the edge of the recording, no margin is used unless one
+    of `add_zeros` or `add_reflect_padding` is True. In the first
+    case zero padding is used, in the second case np.pad is called
+    with mod="reflect".
+    """
+    length = rec_segment.get_num_samples()
 
-    # precompute segment slice
-    segment_slices = []
-    for segment_index in range(recording.get_num_segments()):
-        i0 = np.searchsorted(peaks['segment_ind'], segment_index)
-        i1 = np.searchsorted(peaks['segment_ind'], segment_index + 1)
-        segment_slices.append(slice(i0, i1))
+    if channel_indices is None:
+        channel_indices = slice(None)
 
+    if not (add_zeros or add_reflect_padding):
+        if window_on_margin and not add_zeros:
+            raise ValueError("window_on_margin requires add_zeros=True")
+
+        if start_frame is None:
+            left_margin = 0
+            start_frame = 0
+        elif start_frame < margin:
+            left_margin = start_frame
+        else:
+            left_margin = margin
+
+        if end_frame is None:
+            right_margin = 0
+            end_frame = length
+        elif end_frame > (length - margin):
+            right_margin = length - end_frame
+        else:
+            right_margin = margin
+
+        traces_chunk = rec_segment.get_traces(
+            start_frame - left_margin,
+            end_frame + right_margin,
+            channel_indices,
+        )
+
+    else:
+        # either add_zeros or reflect_padding
+        if start_frame is None:
+            start_frame = 0
+        if end_frame is None:
+            end_frame = length
+
+        chunk_size = end_frame - start_frame
+        full_size = chunk_size + 2 * margin
+
+        if start_frame < margin:
+            start_frame2 = 0
+            left_pad = margin - start_frame
+        else:
+            start_frame2 = start_frame - margin
+            left_pad = 0
+
+        if end_frame > (length - margin):
+            end_frame2 = length
+            right_pad = end_frame + margin - length
+        else:
+            end_frame2 = end_frame + margin
+            right_pad = 0
+
+        traces_chunk = rec_segment.get_traces(start_frame2, end_frame2, channel_indices)
+
+        if dtype is not None or window_on_margin or left_pad > 0 or right_pad > 0:
+            need_copy = True
+        else:
+            need_copy = False
+
+        left_margin = margin
+        right_margin = margin
+
+        if need_copy:
+            if dtype is None:
+                dtype = traces_chunk.dtype
+
+            left_margin = margin
+            if end_frame < (length + margin):
+                right_margin = margin
+            else:
+                right_margin = end_frame + margin - length
+
+            if add_zeros:
+                traces_chunk2 = np.zeros((full_size, traces_chunk.shape[1]), dtype=dtype)
+                i0 = left_pad
+                i1 = left_pad + traces_chunk.shape[0]
+                traces_chunk2[i0:i1, :] = traces_chunk
+                if window_on_margin:
+                    # apply inplace taper on border
+                    taper = (1 - np.cos(np.arange(margin) / margin * np.pi)) / 2
+                    taper = taper[:, np.newaxis]
+                    traces_chunk2[:margin] *= taper
+                    traces_chunk2[-margin:] *= taper[::-1]
+                traces_chunk = traces_chunk2
+            elif add_reflect_padding:
+                # in this case, we don't want to taper
+                traces_chunk = np.pad(
+                    traces_chunk.astype(dtype),
+                    [(left_pad, right_pad), (0, 0)],
+                    mode="reflect",
+                )
+            else:
+                # we need a copy to change the dtype
+                traces_chunk = np.asarray(traces_chunk, dtype=dtype)
 
-    init_args = (recording, peaks, nodes, segment_slices)
-        
-    processor = ChunkRecordingExecutor(recording, _compute_peak_step_chunk, _init_worker_peak_pipeline,
-                                       init_args, handle_returns=True, job_name=job_name, **job_kwargs)
+    return traces_chunk, left_margin, right_margin
 
-    outputs = processor.run()
-    # outputs is a list of tuple
 
-    # concatenation of every step stream
-    outs_concat = ()
-    for output_step in zip(*outputs):
-        outs_concat += (np.concatenate(output_step, axis=0), )
+def order_channels_by_depth(recording, channel_ids=None, dimensions=("x", "y")):
+    """
+    Order channels by depth, by first ordering the x-axis, and then the y-axis.
 
-    if len(outs_concat) == 1 and squeeze_output:
-        # when tuple size ==1  then remove the tuple
-        return outs_concat[0]
+    Parameters
+    ----------
+    recording : BaseRecording
+        The input recording
+    channel_ids : list/array or None
+        If given, a subset of channels to order locations for
+    dimensions : str or tuple
+        If str, it needs to be 'x', 'y', 'z'.
+        If tuple, it sorts the locations in two dimensions using lexsort.
+        This approach is recommended since there is less ambiguity, by default ('x', 'y')
+
+    Returns
+    -------
+    order_f : np.array
+        Array with sorted indices
+    order_r : np.array
+        Array with indices to revert sorting
+    """
+    locations = recording.get_channel_locations()
+    ndim = locations.shape[1]
+    channel_inds = recording.ids_to_indices(ids=channel_ids, prefer_slice=True)
+    locations = locations[channel_inds, :]
+
+    if isinstance(dimensions, str):
+        dim = ["x", "y", "z"].index(dimensions)
+        assert dim < ndim, "Invalid dimensions!"
+        order_f = np.argsort(locations[:, dim], kind="stable")
     else:
-        # always a tuple even of size 1
-        return outs_concat
+        assert isinstance(dimensions, tuple), "dimensions can be a str or a tuple"
+        locations_to_sort = ()
+        for dim in dimensions:
+            dim = ["x", "y", "z"].index(dim)
+            assert dim < ndim, "Invalid dimensions!"
+            locations_to_sort += (locations[:, dim],)
+        order_f = np.lexsort(locations_to_sort)
+    order_r = np.argsort(order_f, kind="stable")
+
+    return order_f, order_r
 
 
-def _init_worker_peak_pipeline(recording, peaks, nodes, segment_slices):
-    """Initialize worker for localizing peaks."""
-    
-    max_margin = max(node.get_trace_margin() for node in nodes)
-
-    # create a local dict per worker
-    worker_ctx = {}
-    worker_ctx['recording'] = recording
-    worker_ctx['peaks'] = peaks
-    worker_ctx['nodes'] = nodes
-    worker_ctx['max_margin'] = max_margin
-    worker_ctx['segment_slices'] = segment_slices
-    
-    return worker_ctx
-
-
-def _compute_peak_step_chunk(segment_index, start_frame, end_frame, worker_ctx):
-    recording = worker_ctx['recording']
-    margin = worker_ctx['max_margin']
-    peaks = worker_ctx['peaks']
-    nodes = worker_ctx['nodes']
-    segment_slices = worker_ctx['segment_slices']
-
-    recording_segment = recording._recording_segments[segment_index]
-    traces_chunk, left_margin, right_margin = get_chunk_with_margin(recording_segment, start_frame, end_frame,
-                                                              None, margin, add_zeros=True)
-
-    # get local peaks (sgment + start_frame/end_frame)
-    sl = segment_slices[segment_index]
-    peaks_in_segment = peaks[sl]
-    i0 = np.searchsorted(peaks_in_segment['sample_ind'], start_frame)
-    i1 = np.searchsorted(peaks_in_segment['sample_ind'], end_frame)
-    local_peaks = peaks_in_segment[i0:i1]
-
-    # make sample index local to traces
-    local_peaks = local_peaks.copy()
-    local_peaks['sample_ind'] -= (start_frame - left_margin)
-    
-    
-    outs = run_nodes(traces_chunk, local_peaks, nodes)
-    
-    return outs
-
-def run_nodes(traces_chunk, local_peaks, nodes):
-    # compute the graph
-    pipeline_outputs = {}
-    for node in nodes:
-        node_parents = node.parents if node.parents else list()
-        node_input_args = tuple()
-        for parent in node_parents:
-            parent_output = pipeline_outputs[parent]
-            parent_outputs_tuple = parent_output if isinstance(parent_output, tuple) else (parent_output, )
-            node_input_args += parent_outputs_tuple
-        
-        node_output = node.compute(traces_chunk, local_peaks, *node_input_args)
-        pipeline_outputs[node] = node_output
-
-    # propagate the output
-    pipeline_outputs_tuple = tuple()
-    for node in nodes:
-        if node.return_output:
-            out = pipeline_outputs[node]
-            pipeline_outputs_tuple += (out, )
-    
-    return pipeline_outputs_tuple
+def check_probe_do_not_overlap(probes):
+    """
+    When several probes this check that that they do not overlap in space
+    and so channel positions can be safly concatenated.
+    """
+    for i in range(len(probes)):
+        probe_i = probes[i]
+        # check that all positions in probe_j are outside probe_i boundaries
+        x_bounds_i = [
+            np.min(probe_i.contact_positions[:, 0]),
+            np.max(probe_i.contact_positions[:, 0]),
+        ]
+        y_bounds_i = [
+            np.min(probe_i.contact_positions[:, 1]),
+            np.max(probe_i.contact_positions[:, 1]),
+        ]
+
+        for j in range(i + 1, len(probes)):
+            probe_j = probes[j]
+
+            if np.any(
+                np.array(
+                    [
+                        x_bounds_i[0] < cp[0] < x_bounds_i[1] and y_bounds_i[0] < cp[1] < y_bounds_i[1]
+                        for cp in probe_j.contact_positions
+                    ]
+                )
+            ):
+                raise Exception("Probes are overlapping! Retrieve locations of single probes separately")
+
+
+def get_rec_attributes(recording):
+    """
+    Construct rec_attributes from recording object
+
+    Parameters
+    ----------
+    recording : BaseRecording
+        The recording object
+
+    Returns
+    -------
+    dict
+        The rec_attributes dictionary
+    """
+    properties_to_attrs = deepcopy(recording._properties)
+    if "contact_vector" in properties_to_attrs:
+        del properties_to_attrs["contact_vector"]
+    rec_attributes = dict(
+        channel_ids=recording.channel_ids,
+        sampling_frequency=recording.get_sampling_frequency(),
+        num_channels=recording.get_num_channels(),
+        num_samples=[recording.get_num_samples(seg_index) for seg_index in range(recording.get_num_segments())],
+        is_filtered=recording.is_filtered(),
+        properties=properties_to_attrs,
+    )
+    return rec_attributes
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sortingcomponents/peak_selection.py` & `spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/peak_selection.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 """Sorting components: peak selection"""
 
 import numpy as np
 from sklearn.preprocessing import QuantileTransformer
 
 
-def select_peaks(peaks, method='uniform', seed=None, return_indices=False, **method_kwargs):
+def select_peaks(peaks, method="uniform", seed=None, return_indices=False, **method_kwargs):
     """
     Method to select a subset of peaks from a set of peaks.
     Usually use for reducing computational foorptint of downstream methods.
     Parameters
     ----------
     peaks: the peaks that have been found
     method: 'uniform', 'uniform_locations', 'smart_sampling_amplitudes', 'smart_sampling_locations',
@@ -67,205 +67,193 @@
     selected_indices = select_peak_indices(peaks, method=method, seed=seed, **method_kwargs)
     selected_peaks = peaks[selected_indices]
     if return_indices:
         return selected_peaks, selected_indices
     else:
         return selected_peaks
 
+
 def select_peak_indices(peaks, method, seed, **method_kwargs):
     """
     Method to subsample all the found peaks before clustering.  Returns selected_indices.
-    
+
     This function is wrapped by select_peaks -- see
-    
+
     :py:func:`spikeinterface.sortingcomponents.peak_selection.select_peaks` for detailed documentation.
     """
-    
+
     selected_indices = []
 
-    
     seed = seed if seed else None
-    rng = np.random.default_rng(seed=seed)    
-    
+    rng = np.random.default_rng(seed=seed)
 
-    if method == 'uniform':
-
-        params = {'select_per_channel' : False, 
-                  'n_peaks' : None}
+    if method == "uniform":
+        params = {"select_per_channel": False, "n_peaks": None}
 
         params.update(method_kwargs)
 
-        assert params['n_peaks'] is not None, "n_peaks should be defined!"
-
-        if params['select_per_channel']:
+        assert params["n_peaks"] is not None, "n_peaks should be defined!"
 
+        if params["select_per_channel"]:
             ## This method will randomly select max_peaks_per_channel peaks per channels
-            for channel in np.unique(peaks['channel_ind']):
-                peaks_indices = np.where(peaks['channel_ind'] == channel)[0]
-                max_peaks = min(peaks_indices.size, params['n_peaks'])
+            for channel in np.unique(peaks["channel_index"]):
+                peaks_indices = np.where(peaks["channel_index"] == channel)[0]
+                max_peaks = min(peaks_indices.size, params["n_peaks"])
                 selected_indices += [rng.choice(peaks_indices, size=max_peaks, replace=False)]
         else:
-            num_peaks = min(peaks.size, params['n_peaks'])
+            num_peaks = min(peaks.size, params["n_peaks"])
             selected_indices = [rng.choice(peaks.size, size=num_peaks, replace=False)]
 
-    elif method in ['smart_sampling_amplitudes', 'smart_sampling_locations', 'smart_sampling_locations_and_time']:
-
-        if method == 'smart_sampling_amplitudes':
-
+    elif method in ["smart_sampling_amplitudes", "smart_sampling_locations", "smart_sampling_locations_and_time"]:
+        if method == "smart_sampling_amplitudes":
             ## This method will try to select around n_peaks per channel but in a non uniform manner
-            ## First, it will look at the distribution of the peaks amplitudes, per channel. 
+            ## First, it will look at the distribution of the peaks amplitudes, per channel.
             ## Once this distribution is known, it will sample from the peaks with a rejection probability
             ## such that the final distribution of the amplitudes, for the selected peaks, will be as
-            ## uniform as possible. In a nutshell, the method will try to sample as homogenously as possible 
+            ## uniform as possible. In a nutshell, the method will try to sample as homogenously as possible
             ## from the space of all the peaks, using the amplitude as a discriminative criteria
-            ## To do so, one must provide the noise_levels, detect_threshold used to detect the peaks, the 
+            ## To do so, one must provide the noise_levels, detect_threshold used to detect the peaks, the
             ## sign of the peaks, and the number of bins for the probability density histogram
 
-            params = {'n_peaks' : None, 
-                      'noise_levels' : None,
-                      'select_per_channel' : False}
+            params = {"n_peaks": None, "noise_levels": None, "select_per_channel": False}
 
             params.update(method_kwargs)
 
-            assert params['n_peaks'] is not None, "n_peaks should be defined!"
-            assert params['noise_levels'] is not None, "Noise levels should be provided"
-
-            if params['select_per_channel']:
-                for channel in np.unique(peaks['channel_ind']):
+            assert params["n_peaks"] is not None, "n_peaks should be defined!"
+            assert params["noise_levels"] is not None, "Noise levels should be provided"
 
-                    peaks_indices = np.where(peaks['channel_ind'] == channel)[0]                
-                    if params['n_peaks'] > peaks_indices.size:
+            if params["select_per_channel"]:
+                for channel in np.unique(peaks["channel_index"]):
+                    peaks_indices = np.where(peaks["channel_index"] == channel)[0]
+                    if params["n_peaks"] > peaks_indices.size:
                         selected_indices += [peaks_indices]
                     else:
                         sub_peaks = peaks[peaks_indices]
-                        snrs = sub_peaks['amplitude'] / params['noise_levels'][channel]
-                        preprocessing = QuantileTransformer(output_distribution='uniform', n_quantiles = min(100, len(snrs)))
+                        snrs = sub_peaks["amplitude"] / params["noise_levels"][channel]
+                        preprocessing = QuantileTransformer(
+                            output_distribution="uniform", n_quantiles=min(100, len(snrs))
+                        )
                         snrs = preprocessing.fit_transform(snrs[:, np.newaxis])
 
                         my_selection = np.zeros(0, dtype=np.int32)
                         all_index = np.arange(len(snrs))
-                        while my_selection.size < params['n_peaks']:
+                        while my_selection.size < params["n_peaks"]:
                             candidates = all_index[np.logical_not(np.isin(all_index, my_selection))]
                             probabilities = rng.random(size=len(candidates))
-                            valid = candidates[np.where(snrs[candidates,0] < probabilities)[0]]
+                            valid = candidates[np.where(snrs[candidates, 0] < probabilities)[0]]
                             my_selection = np.concatenate((my_selection, valid))
 
-                        selected_indices += [peaks_indices[rng.permutation(my_selection)[:params['n_peaks']]]]
+                        selected_indices += [peaks_indices[rng.permutation(my_selection)[: params["n_peaks"]]]]
 
             else:
-                if params['n_peaks'] > peaks.size:
+                if params["n_peaks"] > peaks.size:
                     selected_indices += [np.arange(peaks.size)]
                 else:
-                    snrs = peaks['amplitude'] / params['noise_levels'][peaks['channel_ind']]
-                    preprocessing = QuantileTransformer(output_distribution='uniform', n_quantiles=min(100, len(snrs)))
+                    snrs = peaks["amplitude"] / params["noise_levels"][peaks["channel_index"]]
+                    preprocessing = QuantileTransformer(output_distribution="uniform", n_quantiles=min(100, len(snrs)))
                     snrs = preprocessing.fit_transform(snrs[:, np.newaxis])
 
                     my_selection = np.zeros(0, dtype=np.int32)
                     all_index = np.arange(len(snrs))
-                    while my_selection.size < params['n_peaks']:
+                    while my_selection.size < params["n_peaks"]:
                         candidates = all_index[np.logical_not(np.isin(all_index, my_selection))]
                         probabilities = rng.random(size=len(candidates))
-                        valid = candidates[np.where(snrs[candidates,0] < probabilities)[0]]
+                        valid = candidates[np.where(snrs[candidates, 0] < probabilities)[0]]
                         my_selection = np.concatenate((my_selection, valid))
 
-                    selected_indices = [rng.permutation(my_selection)[:params['n_peaks']]]
-
-        elif method == 'smart_sampling_locations':
+                    selected_indices = [rng.permutation(my_selection)[: params["n_peaks"]]]
 
+        elif method == "smart_sampling_locations":
             ## This method will try to select around n_peaksbut in a non uniform manner
-            ## First, it will look at the distribution of the positions. 
+            ## First, it will look at the distribution of the positions.
             ## Once this distribution is known, it will sample from the peaks with a rejection probability
             ## such that the final distribution of the amplitudes, for the selected peaks, will be as
-            ## uniform as possible. In a nutshell, the method will try to sample as homogenously as possible 
+            ## uniform as possible. In a nutshell, the method will try to sample as homogenously as possible
             ## from the space of all the peaks, using the locations as a discriminative criteria
-            ## To do so, one must provide the peaks locations, and the number of bins for the 
+            ## To do so, one must provide the peaks locations, and the number of bins for the
             ## probability density histogram
 
-            params = {'peaks_locations' : None,
-                      'n_peaks' : None}
+            params = {"peaks_locations": None, "n_peaks": None}
 
             params.update(method_kwargs)
 
-            assert params['n_peaks'] is not None, "n_peaks should be defined!"
-            assert params['peaks_locations'] is not None, "peaks_locations should be d96efined!"
+            assert params["n_peaks"] is not None, "n_peaks should be defined!"
+            assert params["peaks_locations"] is not None, "peaks_locations should be d96efined!"
 
-            nb_spikes = len(params['peaks_locations']['x'])
+            nb_spikes = len(params["peaks_locations"]["x"])
 
-            if params['n_peaks'] > nb_spikes:
+            if params["n_peaks"] > nb_spikes:
                 selected_indices += [np.arange(peaks.size)]
             else:
-                
-                preprocessing = QuantileTransformer(output_distribution='uniform', n_quantiles=min(100, nb_spikes))
-                data = np.array([params['peaks_locations']['x'], params['peaks_locations']['y']]).T
+                preprocessing = QuantileTransformer(output_distribution="uniform", n_quantiles=min(100, nb_spikes))
+                data = np.array([params["peaks_locations"]["x"], params["peaks_locations"]["y"]]).T
                 data = preprocessing.fit_transform(data)
 
                 my_selection = np.zeros(0, dtype=np.int32)
                 all_index = np.arange(peaks.size)
-                while my_selection.size < params['n_peaks']:
+                while my_selection.size < params["n_peaks"]:
                     candidates = all_index[np.logical_not(np.isin(all_index, my_selection))]
 
                     probabilities = rng.random(size=len(candidates))
                     data_x = data[:, 0] < probabilities
 
                     probabilities = rng.random(size=len(candidates))
                     data_y = data[:, 1] < probabilities
 
                     valid = candidates[np.where(data_x * data_y)[0]]
                     my_selection = np.concatenate((my_selection, valid))
 
-                selected_indices = [rng.permutation(my_selection)[:params['n_peaks']]]
-
-        elif method == 'smart_sampling_locations_and_time':
+                selected_indices = [rng.permutation(my_selection)[: params["n_peaks"]]]
 
+        elif method == "smart_sampling_locations_and_time":
             ## This method will try to select around n_peaksbut in a non uniform manner
-            ## First, it will look at the distribution of the positions. 
+            ## First, it will look at the distribution of the positions.
             ## Once this distribution is known, it will sample from the peaks with a rejection probability
             ## such that the final distribution of the amplitudes, for the selected peaks, will be as
-            ## uniform as possible. In a nutshell, the method will try to sample as homogenously as possible 
+            ## uniform as possible. In a nutshell, the method will try to sample as homogenously as possible
             ## from the space of all the peaks, using the locations as a discriminative criteria
-            ## To do so, one must provide the peaks locations, and the number of bins for the 
+            ## To do so, one must provide the peaks locations, and the number of bins for the
             ## probability density histogram
 
-            params = {'peaks_locations' : None,
-                      'n_peaks' : None}
+            params = {"peaks_locations": None, "n_peaks": None}
 
             params.update(method_kwargs)
 
-            assert params['n_peaks'] is not None, "n_peaks should be defined!"
-            assert params['peaks_locations'] is not None, "peaks_locations should be defined!"
+            assert params["n_peaks"] is not None, "n_peaks should be defined!"
+            assert params["peaks_locations"] is not None, "peaks_locations should be defined!"
 
-            nb_spikes = len(params['peaks_locations']['x'])
+            nb_spikes = len(params["peaks_locations"]["x"])
 
-            if params['n_peaks'] > nb_spikes:
+            if params["n_peaks"] > nb_spikes:
                 selected_indices += [np.arange(peaks.size)]
             else:
-
-                preprocessing = QuantileTransformer(output_distribution='uniform', n_quantiles=min(100, nb_spikes))
-                data = np.array([params['peaks_locations']['x'], params['peaks_locations']['y'], peaks['sample_ind']]).T
+                preprocessing = QuantileTransformer(output_distribution="uniform", n_quantiles=min(100, nb_spikes))
+                data = np.array(
+                    [params["peaks_locations"]["x"], params["peaks_locations"]["y"], peaks["sample_index"]]
+                ).T
                 data = preprocessing.fit_transform(data)
 
                 my_selection = np.zeros(0, dtype=np.int32)
                 all_index = np.arange(peaks.size)
-                while my_selection.size < params['n_peaks']:
+                while my_selection.size < params["n_peaks"]:
                     candidates = all_index[np.logical_not(np.isin(all_index, my_selection))]
 
                     probabilities = rng.random(size=len(candidates))
                     data_x = data[:, 0] < probabilities
 
                     probabilities = rng.random(size=len(candidates))
                     data_y = data[:, 1] < probabilities
 
                     probabilities = rng.random(size=len(candidates))
                     data_t = data[:, 2] < probabilities
 
                     valid = candidates[np.where(data_x * data_y * data_t)[0]]
                     my_selection = np.concatenate((my_selection, valid))
 
-                selected_indices = [rng.permutation(my_selection)[:params['n_peaks']]]
+                selected_indices = [rng.permutation(my_selection)[: params["n_peaks"]]]
 
     else:
-
         raise NotImplementedError(f"No method {method} for peaks selection")
 
     selected_indices = np.concatenate(selected_indices)
-    selected_indices = selected_indices[np.argsort(peaks[selected_indices]['sample_ind'])]
+    selected_indices = selected_indices[np.argsort(peaks[selected_indices]["sample_index"])]
     return selected_indices
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sortingcomponents/waveforms/neural_network_denoiser.py` & `spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/waveforms/neural_network_denoiser.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,40 +1,53 @@
 from pathlib import Path
 import json
 from typing import List, Optional
 
-import torch
-from torch import nn
-from huggingface_hub import hf_hub_download
-
+try:
+    import torch
+    from torch import nn
+
+    HAVE_TORCH = True
+except ImportError:
+    HAVE_TORCH = False
+try:
+    from huggingface_hub import hf_hub_download
+
+    HAVE_HUGGINFACE = True
+except ImportError:
+    HAVE_HUGGINFACE = False
 
 from spikeinterface.core import BaseRecording
-from spikeinterface.sortingcomponents.peak_pipeline import PipelineNode, WaveformExtractorNode
+from spikeinterface.sortingcomponents.peak_pipeline import PipelineNode, WaveformsNode, find_parent_of_type
 from .waveform_utils import to_temporal_representation, from_temporal_representation
 
 
-class SingleChannelToyDenoiser(PipelineNode):
+class SingleChannelToyDenoiser(WaveformsNode):
     def __init__(
         self, recording: BaseRecording, return_output: bool = True, parents: Optional[List[PipelineNode]] = None
     ):
-        super().__init__(recording, return_output=return_output, parents=parents)
-
-        # Find waveform extractor in the parents
-        try:
-            waveform_extractor = next(parent for parent in self.parents if isinstance(parent, WaveformExtractorNode))
-        except (StopIteration, TypeError):
-            exception_string = f"Model should have a {WaveformExtractorNode.__name__} in its parents"
-            raise TypeError(exception_string)
+        assert HAVE_TORCH, "To use the SingleChannelToyDenoiser you need to install torch"
+        waveform_extractor = find_parent_of_type(parents, WaveformsNode)
+        if waveform_extractor is None:
+            raise TypeError(f"Model should have a {WaveformsNode.__name__} in its parents")
+
+        super().__init__(
+            recording,
+            waveform_extractor.ms_before,
+            waveform_extractor.ms_after,
+            return_output=return_output,
+            parents=parents,
+        )
 
         self.assert_model_and_waveform_temporal_match(waveform_extractor)
 
         # Load model
         self.denoiser = self.load_model()
 
-    def assert_model_and_waveform_temporal_match(self, waveform_extractor: WaveformExtractorNode):
+    def assert_model_and_waveform_temporal_match(self, waveform_extractor: WaveformsNode):
         """
         Asserts that the model and the waveform extractor have the same temporal parameters
         """
         # Extract temporal parameters from the waveform extractor
         waveforms_ms_before = waveform_extractor.ms_before
         waveforms_ms_after = waveform_extractor.ms_after
         waveforms_sampling_frequency = waveform_extractor.recording.get_sampling_frequency()
@@ -62,14 +75,16 @@
                 f"{model_ms_before=} and {waveforms_ms_after=} \n"
                 f"{model_ms_after=} and {waveforms_ms_after=} \n"
                 f"{model_sampling_frequency=} and {waveforms_sampling_frequency=} \n"
             )
             raise ValueError(exception_string)
 
     def load_model(self):
+        assert HAVE_HUGGINFACE, "To download models from Hugginface you need to install huggingface_hub"
+
         repo_id = "SpikeInterface/test_repo"
         subfolder = "mearec_toy_model"
         filename = "toy_model_marec.pt"
 
         model_path = hf_hub_download(repo_id=repo_id, subfolder=subfolder, filename=filename)
         denoiser = SingleChannel1dCNNDenoiser(pretrained_path=model_path, spike_size=128)
         denoiser = denoiser.load()
@@ -83,36 +98,38 @@
         temporal_waveforms = to_temporal_representation(waveforms)
         temporal_waveforms_tensor = torch.from_numpy(temporal_waveforms).float()
 
         # Denoise
         denoised_temporal_waveforms = self.denoiser(temporal_waveforms_tensor).detach().numpy()
 
         # Reconstruct representation with channels
-        desnoised_waveforms = from_temporal_representation(denoised_temporal_waveforms, num_channels)
+        denoised_waveforms = from_temporal_representation(denoised_temporal_waveforms, num_channels)
+
+        return denoised_waveforms
 
-        return desnoised_waveforms
 
+if HAVE_TORCH:
 
-class SingleChannel1dCNNDenoiser(nn.Module):
-    def __init__(self, pretrained_path=None, n_filters=[16, 8], filter_sizes=[5, 11], spike_size=121):
-        super().__init__()
-
-        out_channels_conv1, out_channels_conv_2 = n_filters
-        kernel_size_conv1, kernel_size_conv2 = filter_sizes
-        self.conv1 = nn.Sequential(nn.Conv1d(1, out_channels_conv1, kernel_size_conv1), nn.ReLU())
-        self.conv2 = nn.Sequential(nn.Conv1d(out_channels_conv1, out_channels_conv_2, kernel_size_conv2), nn.ReLU())
-        n_input_feat = out_channels_conv_2 * (spike_size - kernel_size_conv1 - kernel_size_conv2 + 2)
-        self.out = nn.Linear(n_input_feat, spike_size)
-        self.pretrained_path = pretrained_path
-
-    def forward(self, x):
-        x = x[:, None]
-        x = self.conv1(x)
-        x = self.conv2(x)
-        x = x.view(x.shape[0], -1)
-        x = self.out(x)
-        return x
-
-    def load(self, device="cpu"):
-        checkpoint = torch.load(self.pretrained_path, map_location=device)
-        self.load_state_dict(checkpoint)
-        return self
+    class SingleChannel1dCNNDenoiser(nn.Module):
+        def __init__(self, pretrained_path=None, n_filters=[16, 8], filter_sizes=[5, 11], spike_size=121):
+            super().__init__()
+
+            out_channels_conv1, out_channels_conv_2 = n_filters
+            kernel_size_conv1, kernel_size_conv2 = filter_sizes
+            self.conv1 = nn.Sequential(nn.Conv1d(1, out_channels_conv1, kernel_size_conv1), nn.ReLU())
+            self.conv2 = nn.Sequential(nn.Conv1d(out_channels_conv1, out_channels_conv_2, kernel_size_conv2), nn.ReLU())
+            n_input_feat = out_channels_conv_2 * (spike_size - kernel_size_conv1 - kernel_size_conv2 + 2)
+            self.out = nn.Linear(n_input_feat, spike_size)
+            self.pretrained_path = pretrained_path
+
+        def forward(self, x):
+            x = x[:, None]
+            x = self.conv1(x)
+            x = self.conv2(x)
+            x = x.view(x.shape[0], -1)
+            x = self.out(x)
+            return x
+
+        def load(self, device="cpu"):
+            checkpoint = torch.load(self.pretrained_path, map_location=device)
+            self.load_state_dict(checkpoint)
+            return self
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sortingcomponents/waveforms/temporal_pca.py` & `spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/waveforms/temporal_pca.py`

 * *Files 2% similar despite different names*

```diff
@@ -2,35 +2,45 @@
 import json
 from pathlib import Path
 from typing import List
 
 import numpy as np
 from sklearn.decomposition import IncrementalPCA
 
-from spikeinterface.sortingcomponents.peak_pipeline import PipelineNode, WaveformExtractorNode
+from spikeinterface.sortingcomponents.peak_pipeline import PipelineNode, WaveformsNode, find_parent_of_type
 from spikeinterface.sortingcomponents.peak_detection import detect_peaks
 from spikeinterface.sortingcomponents.peak_selection import select_peaks
 from spikeinterface.postprocessing import compute_principal_components
 from spikeinterface.core import BaseRecording
 from spikeinterface.core.sparsity import ChannelSparsity
 from spikeinterface import extract_waveforms, NumpySorting
 from spikeinterface.core.job_tools import _shared_job_kwargs_doc
 from .waveform_utils import to_temporal_representation, from_temporal_representation
 
 
-class TemporalPCBaseNode(PipelineNode):
+class TemporalPCBaseNode(WaveformsNode):
     def __init__(
         self, recording: BaseRecording, parents: List[PipelineNode], model_folder_path: str, return_output=True
     ):
         """
         Base class for PCA projection nodes. Contains the logic of the fit method that should be inherited by all the
         child classess. The child should implement a compute method that does a specific operation
         (e.g. project, denoise, etc)
         """
-        PipelineNode.__init__(self, recording=recording, parents=parents, return_output=return_output)
+        waveform_extractor = find_parent_of_type(parents, WaveformsNode)
+        if waveform_extractor is None:
+            raise TypeError(f"TemporalPCA should have a single {WaveformsNode.__name__} in its parents")
+
+        super().__init__(
+            recording,
+            waveform_extractor.ms_before,
+            waveform_extractor.ms_after,
+            return_output=return_output,
+            parents=parents,
+        )
 
         self.model_folder_path = model_folder_path
 
         if not Path(model_folder_path).is_dir() or model_folder_path is None:
             exception_string = (
                 f"model_path folder is not a folder or does not exist. \n"
                 f"A model can be trained by using{self.__class__.__name__}.fit(...)"
@@ -41,21 +51,17 @@
         model_path = Path(model_folder_path) / "pca_model.pkl"
         with open(model_path, "rb") as f:
             self.pca_model = pickle.load(f)
         params_path = Path(model_folder_path) / "params.json"
         with open(params_path, "rb") as f:
             self.params = json.load(f)
 
-        # Find waveform extractor in the parents
-        if self.parents is None or not (len(self.parents) == 1 and isinstance(self.parents[0], WaveformExtractorNode)):
-            exception_string = f"TemporalPCA should have a single {WaveformExtractorNode.__name__} in its parents"
-            raise TypeError(exception_string)
-        self.assert_model_and_waveform_temporal_match(self.parents[0])
+        self.assert_model_and_waveform_temporal_match(waveform_extractor)
 
-    def assert_model_and_waveform_temporal_match(self, waveform_extractor: WaveformExtractorNode):
+    def assert_model_and_waveform_temporal_match(self, waveform_extractor: WaveformsNode):
         """
         Asserts that the model and the waveform extractor have the same temporal parameters
         """
         # Extract the first waveform extractor in the parents
         waveforms_ms_before = waveform_extractor.ms_before
         waveforms_ms_after = waveform_extractor.ms_after
         waveforms_sampling_frequency = waveform_extractor.recording.get_sampling_frequency()
```

### Comparing `spikeinterface-0.97.1/spikeinterface/sortingcomponents/waveforms/waveform_utils.py` & `spikeinterface-0.98.0/src/spikeinterface/sortingcomponents/waveforms/waveform_utils.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,20 +1,21 @@
 def to_temporal_representation(waveforms):
     """
-    Transform waveforms to temporal representation. Collapses the channel dimension (spatial) leaving only 
-    temporal information. 
+    Transform waveforms to temporal representation. Collapses the channel dimension (spatial) leaving only
+    temporal information.
     """
     num_waveforms, num_time_samples, num_channels = waveforms.shape
     num_temporal_waveforms = num_waveforms * num_channels
     temporal_waveforms = waveforms.swapaxes(1, 2).reshape((num_temporal_waveforms, num_time_samples))
 
     return temporal_waveforms
 
+
 def from_temporal_representation(temporal_waveforms, num_channels):
     """
     Transform waveforms from temporal representation. The inverse of to_temporal_representation
     """
     num_temporal_waveforms, num_time_samples = temporal_waveforms.shape
     num_waveforms = num_temporal_waveforms // num_channels
 
     waveforms = temporal_waveforms.reshape(num_waveforms, num_channels, num_time_samples).swapaxes(2, 1)
-    return waveforms
+    return waveforms
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/__init__.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/__init__.py`

 * *Ordering differences only*

 * *Files 2% similar despite different names*

```diff
@@ -1,22 +1,25 @@
 # check if backend are available
 try:
     import matplotlib
+
     HAVE_MPL = True
 except:
     HAVE_MPL = False
 
 try:
     import sortingview
+
     HAVE_SV = True
 except:
     HAVE_SV = False
 
 try:
     import ipywidgets
+
     HAVE_IPYW = True
 except:
     HAVE_IPYW = False
 
 
 # theses import make the Widget.resgister() at import time
 if HAVE_MPL:
@@ -36,10 +39,7 @@
 from .base import set_default_plotter_backend, get_default_plotter_backend
 
 
 # we keep this to keep compatibility so we have all previous widgets
 # except the one that have been ported that are imported
 # with "from .widget_list import *" in the first line
 from ._legacy_mpl_widgets import *
-
-
-
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/__init__.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/__init__.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,68 +1,92 @@
-
 # basics
-# from .timeseries import plot_timeseries, TimeseriesWidget
+# from .timeseries import plot_timeseries, TimeseriesWidget
 from .rasters import plot_rasters, RasterWidget
 from .probemap import plot_probe_map, ProbeMapWidget
 
 # isi/ccg/acg
 from .isidistribution import plot_isi_distribution, ISIDistributionWidget
+
 # from .correlograms import (plot_crosscorrelograms, CrossCorrelogramsWidget,
 #                            plot_autocorrelograms, AutoCorrelogramsWidget)
 
 # peak activity
 from .activity import plot_peak_activity_map, PeakActivityMapWidget
 
-# drift/motion
-from .drift import (plot_drift_over_time, DriftOverTimeWidget,
-                    plot_pairwise_displacement, PairwiseDisplacementWidget,
-                    plot_displacement, DisplacementWidget)
-
 # waveform/PC related
 # from .unitwaveforms import plot_unit_waveforms, plot_unit_templates
 # from .unitwaveformdensitymap import plot_unit_waveform_density_map, UnitWaveformDensityMapWidget
-# from .amplitudes import plot_amplitudes_distribution
+# from .amplitudes import plot_amplitudes_distribution
 from .principalcomponent import plot_principal_component
+
 # from .unitlocalization import plot_unit_localization, UnitLocalizationWidget
 
 # units on probe
 from .unitprobemap import plot_unit_probe_map, UnitProbeMapWidget
-# from .depthamplitude import plot_units_depth_vs_amplitude
+
+# from .depthamplitude import plot_units_depth_vs_amplitude
 
 # comparison related
 from .confusionmatrix import plot_confusion_matrix, ConfusionMatrixWidget
 from .agreementmatrix import plot_agreement_matrix, AgreementMatrixWidget
 from .multicompgraph import (
-    plot_multicomp_graph, MultiCompGraphWidget,
-    plot_multicomp_agreement, MultiCompGlobalAgreementWidget,
-    plot_multicomp_agreement_by_sorter, MultiCompAgreementBySorterWidget)
+    plot_multicomp_graph,
+    MultiCompGraphWidget,
+    plot_multicomp_agreement,
+    MultiCompGlobalAgreementWidget,
+    plot_multicomp_agreement_by_sorter,
+    MultiCompAgreementBySorterWidget,
+)
 from .collisioncomp import (
-    plot_comparison_collision_pair_by_pair, ComparisonCollisionPairByPairWidget,
-    plot_comparison_collision_by_similarity,ComparisonCollisionBySimilarityWidget,
-    plot_study_comparison_collision_by_similarity, StudyComparisonCollisionBySimilarityWidget,
-    plot_study_comparison_collision_by_similarity_range, StudyComparisonCollisionBySimilarityRangeWidget,
-    StudyComparisonCollisionBySimilarityRangesWidget, plot_study_comparison_collision_by_similarity_ranges)
+    plot_comparison_collision_pair_by_pair,
+    ComparisonCollisionPairByPairWidget,
+    plot_comparison_collision_by_similarity,
+    ComparisonCollisionBySimilarityWidget,
+    plot_study_comparison_collision_by_similarity,
+    StudyComparisonCollisionBySimilarityWidget,
+    plot_study_comparison_collision_by_similarity_range,
+    StudyComparisonCollisionBySimilarityRangeWidget,
+    StudyComparisonCollisionBySimilarityRangesWidget,
+    plot_study_comparison_collision_by_similarity_ranges,
+)
 
 from .sortingperformance import plot_sorting_performance
 
 # ground truth study (=comparison over sorter)
-from .gtstudy import (StudyComparisonRunTimesWidget, plot_gt_study_run_times,
-    StudyComparisonUnitCountsWidget, StudyComparisonUnitCountsAveragesWidget, 
-    plot_gt_study_unit_counts, plot_gt_study_unit_counts_averages,
-    plot_gt_study_performances, plot_gt_study_performances_averages, StudyComparisonPerformancesWidget,
+from .gtstudy import (
+    StudyComparisonRunTimesWidget,
+    plot_gt_study_run_times,
+    StudyComparisonUnitCountsWidget,
+    StudyComparisonUnitCountsAveragesWidget,
+    plot_gt_study_unit_counts,
+    plot_gt_study_unit_counts_averages,
+    plot_gt_study_performances,
+    plot_gt_study_performances_averages,
+    StudyComparisonPerformancesWidget,
     StudyComparisonPerformancesAveragesWidget,
-    plot_gt_study_performances_by_template_similarity, StudyComparisonPerformancesByTemplateSimilarity,)
+    plot_gt_study_performances_by_template_similarity,
+    StudyComparisonPerformancesByTemplateSimilarity,
+)
 
 # ground truth comparions (=comparison over sorter)
-from .gtcomparison import (plot_gt_performances, plot_gt_performances_averages, ComparisonPerformancesWidget,
+from .gtcomparison import (
+    plot_gt_performances,
+    plot_gt_performances_averages,
+    ComparisonPerformancesWidget,
     ComparisonPerformancesAveragesWidget,
-    plot_gt_performances_by_template_similarity, ComparisonPerformancesByTemplateSimilarity,)
+    plot_gt_performances_by_template_similarity,
+    ComparisonPerformancesByTemplateSimilarity,
+)
 
 # unit summary
-# from .unitsummary import plot_unit_summary, UnitSummaryWidget
+# from .unitsummary import plot_unit_summary, UnitSummaryWidget
 
 # unit presence
 from .presence import plot_presence, PresenceWidget
 
 # correlogram comparison
-from .correlogramcomp import (StudyComparisonCorrelogramBySimilarityWidget, plot_study_comparison_correlogram_by_similarity,
-    StudyComparisonCorrelogramBySimilarityRangesMeanErrorWidget, plot_study_comparison_correlogram_by_similarity_ranges_mean_error)
+from .correlogramcomp import (
+    StudyComparisonCorrelogramBySimilarityWidget,
+    plot_study_comparison_correlogram_by_similarity,
+    StudyComparisonCorrelogramBySimilarityRangesMeanErrorWidget,
+    plot_study_comparison_correlogram_by_similarity_ranges_mean_error,
+)
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/activity.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/activity.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,15 +1,10 @@
 import numpy as np
-import matplotlib.pylab as plt
 from .basewidget import BaseWidget
 
-from matplotlib.animation import FuncAnimation
-
-from probeinterface.plotting import plot_probe
-
 
 class PeakActivityMapWidget(BaseWidget):
     """
     Plots spike rate (estimated with detect_peaks()) as 2D activity map.
 
     Can be static (bin_duration_s=None) or animated (bin_duration_s=60.)
 
@@ -40,22 +35,35 @@
 
     Returns
     -------
     W: ProbeMapWidget
         The output widget
     """
 
-    def __init__(self, recording, peaks=None, detect_peaks_kwargs={},
-                 weight_with_amplitudes=True, bin_duration_s=None,
-                 with_contact_color=True, with_interpolated_map=True,
-                 with_channel_ids=False, with_color_bar=True,
-                 figure=None, ax=None):
+    def __init__(
+        self,
+        recording,
+        peaks=None,
+        detect_peaks_kwargs={},
+        weight_with_amplitudes=True,
+        bin_duration_s=None,
+        with_contact_color=True,
+        with_interpolated_map=True,
+        with_channel_ids=False,
+        with_color_bar=True,
+        figure=None,
+        ax=None,
+    ):
+        import matplotlib.pylab as plt
+        from matplotlib.animation import FuncAnimation
+        from probeinterface.plotting import plot_probe
+
         BaseWidget.__init__(self, figure, ax)
 
-        assert recording.get_num_segments() == 1, 'Handle only one segment'
+        assert recording.get_num_segments() == 1, "Handle only one segment"
 
         self.recording = recording
         self.peaks = peaks
         self.detect_peaks_kwargs = detect_peaks_kwargs
         self.weight_with_amplitudes = weight_with_amplitudes
         self.bin_duration_s = bin_duration_s
         self.with_contact_color = with_contact_color
@@ -63,68 +71,75 @@
         self.with_channel_ids = with_channel_ids
 
     def plot(self):
         rec = self.recording
         peaks = self.peaks
         if peaks is None:
             from spikeinterface.sortingcomponents.peak_detection import detect_peaks
+
             peaks = detect_peaks(rec, **self.detect_peaks_kwargs)
 
         fs = rec.get_sampling_frequency()
         duration = rec.get_total_duration()
 
         probes = rec.get_probes()
-        assert len(probes) == 1, "Activity map is only available for a single probe. If you have a probe group, "\
-                                 "consider splitting the recording from different probes"
+        assert len(probes) == 1, (
+            "Activity map is only available for a single probe. If you have a probe group, "
+            "consider splitting the recording from different probes"
+        )
         probe = probes[0]
 
         if self.bin_duration_s is None:
             self._plot_one_bin(rec, probe, peaks, duration)
         else:
             bin_size = int(self.bin_duration_s * fs)
             num_frames = int(duration / self.bin_duration_s)
 
             def animate_func(i):
-                i0 = np.searchsorted(peaks['sample_ind'], bin_size * i)
-                i1 = np.searchsorted(peaks['sample_ind'], bin_size * (i + 1))
+                i0 = np.searchsorted(peaks["sample_index"], bin_size * i)
+                i1 = np.searchsorted(peaks["sample_index"], bin_size * (i + 1))
                 local_peaks = peaks[i0:i1]
                 artists = self._plot_one_bin(rec, probe, local_peaks, self.bin_duration_s)
                 return artists
 
-            self.animation = FuncAnimation(self.figure, animate_func, frames=num_frames,
-                                           interval=100, blit=True)
+            from matplotlib.animation import FuncAnimation
 
-    def _plot_one_bin(self, rec, probe, peaks, duration):
+            self.animation = FuncAnimation(self.figure, animate_func, frames=num_frames, interval=100, blit=True)
 
+    def _plot_one_bin(self, rec, probe, peaks, duration):
         # TODO: @alessio weight_with_amplitudes is not implemented yet
-        rates = np.zeros(rec.get_num_channels(), dtype='float64')
+        rates = np.zeros(rec.get_num_channels(), dtype="float64")
         for chan_ind, chan_id in enumerate(rec.channel_ids):
-            mask = peaks['channel_ind'] == chan_ind
+            mask = peaks["channel_index"] == chan_ind
             num_spike = np.sum(mask)
             rates[chan_ind] = num_spike / duration
 
         artists = ()
         if self.with_contact_color:
             text_on_contact = None
             if self.with_channel_ids:
                 text_on_contact = self.recording.channel_ids
-            
-                
-            poly, poly_contour = plot_probe(probe, ax=self.ax, contacts_values=rates,
-                                            probe_shape_kwargs={'facecolor': 'w', 'alpha': .1},
-                                            contacts_kargs={'alpha': 1.},
-                                            text_on_contact=text_on_contact,
-                                            )
+
+            from probeinterface.plotting import plot_probe
+
+            poly, poly_contour = plot_probe(
+                probe,
+                ax=self.ax,
+                contacts_values=rates,
+                probe_shape_kwargs={"facecolor": "w", "alpha": 0.1},
+                contacts_kargs={"alpha": 1.0},
+                text_on_contact=text_on_contact,
+            )
             artists = artists + (poly, poly_contour)
 
         if self.with_interpolated_map:
-            image, xlims, ylims = probe.to_image(rates, pixel_size=0.5,
-                                                 num_pixel=None, method='linear',
-                                                 xlims=None, ylims=None)
-            im = self.ax.imshow(image, extent=xlims + ylims, origin='lower', alpha=0.5)
+            image, xlims, ylims = probe.to_image(
+                rates, pixel_size=0.5, num_pixel=None, method="linear", xlims=None, ylims=None
+            )
+            im = self.ax.imshow(image, extent=xlims + ylims, origin="lower", alpha=0.5)
             artists = artists + (im,)
 
         return artists
 
 
 def plot_peak_activity_map(*args, **kwargs):
     W = PeakActivityMapWidget(*args, **kwargs)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/agreementmatrix.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/agreementmatrix.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,8 @@
 import numpy as np
-from matplotlib import pyplot as plt
 
 from .basewidget import BaseWidget
 
 
 class AgreementMatrixWidget(BaseWidget):
     """
     Plots sorting comparison confusion matrix.
@@ -22,22 +21,23 @@
         If True unit tick labels are displayed
     figure: matplotlib figure
         The figure to be used. If not given a figure is created
     ax: matplotlib axis
         The axis to be used. If not given an axis is created
     """
 
-    def __init__(self, sorting_comparison, ordered=True, count_text=True, unit_ticks=True,
-                 figure=None, ax=None):
+    def __init__(self, sorting_comparison, ordered=True, count_text=True, unit_ticks=True, figure=None, ax=None):
+        from matplotlib import pyplot as plt
+
         BaseWidget.__init__(self, figure, ax)
         self._sc = sorting_comparison
         self._ordered = ordered
         self._count_text = count_text
         self._unit_ticks = unit_ticks
-        self.name = 'ConfusionMatrix'
+        self.name = "ConfusionMatrix"
 
     def plot(self):
         self._do_plot()
 
     def _do_plot(self):
         # a dataframe
         if self._ordered:
@@ -48,24 +48,23 @@
         N1 = scores.shape[0]
         N2 = scores.shape[1]
 
         unit_ids1 = scores.index.values
         unit_ids2 = scores.columns.values
 
         # Using matshow here just because it sets the ticks up nicely. imshow is faster.
-        self.ax.matshow(scores.values, cmap='Greens')
+        self.ax.matshow(scores.values, cmap="Greens")
 
         if self._count_text:
             for i, u1 in enumerate(unit_ids1):
                 u2 = self._sc.best_match_12[u1]
                 if u2 != -1:
                     j = np.where(unit_ids2 == u2)[0][0]
 
-                    self.ax.text(j, i, '{:0.2f}'.format(scores.at[u1, u2]),
-                                 ha='center', va='center', color='white')
+                    self.ax.text(j, i, "{:0.2f}".format(scores.at[u1, u2]), ha="center", va="center", color="white")
 
         # Major ticks
         self.ax.set_xticks(np.arange(0, N2))
         self.ax.set_yticks(np.arange(0, N1))
         self.ax.xaxis.tick_bottom()
 
         # Labels for major ticks
@@ -73,15 +72,18 @@
             self.ax.set_yticklabels(scores.index, fontsize=12)
             self.ax.set_xticklabels(scores.columns, fontsize=12)
 
         self.ax.set_xlabel(self._sc.name_list[1], fontsize=20)
         self.ax.set_ylabel(self._sc.name_list[0], fontsize=20)
 
         self.ax.set_xlim(-0.5, N2 - 0.5)
-        self.ax.set_ylim(N1 - 0.5, -0.5, )
+        self.ax.set_ylim(
+            N1 - 0.5,
+            -0.5,
+        )
 
 
 def plot_agreement_matrix(*args, **kwargs):
     W = AgreementMatrixWidget(*args, **kwargs)
     W.plot()
     return W
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/amplitudes.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/amplitudes.py`

 * *Files 6% similar despite different names*

```diff
@@ -4,27 +4,25 @@
 from .basewidget import BaseWidget
 
 from ...postprocessing import compute_spike_amplitudes
 from .utils import get_unit_colors
 
 
 class AmplitudeBaseWidget(BaseWidget):
-    def __init__(self, waveform_extractor, unit_ids=None, 
-        compute_kwargs={},
-        unit_colors=None, figure=None, ax=None):
+    def __init__(self, waveform_extractor, unit_ids=None, compute_kwargs={}, unit_colors=None, figure=None, ax=None):
         BaseWidget.__init__(self, figure, ax)
 
         self.we = waveform_extractor
-        
-        if self.we.is_extension('spike_amplitudes'):
-            sac = self.we.load_extension('spike_amplitudes')
-            self.amplitudes = sac.get_data(outputs='by_unit')
+
+        if self.we.is_extension("spike_amplitudes"):
+            sac = self.we.load_extension("spike_amplitudes")
+            self.amplitudes = sac.get_data(outputs="by_unit")
         else:
-            self.amplitudes = compute_spike_amplitudes(self.we, outputs='by_unit', **compute_kwargs)
-        
+            self.amplitudes = compute_spike_amplitudes(self.we, outputs="by_unit", **compute_kwargs)
+
         if unit_ids is None:
             unit_ids = waveform_extractor.sorting.unit_ids
         self.unit_ids = unit_ids
 
         if unit_colors is None:
             unit_colors = get_unit_colors(self.we.sorting)
         self.unit_colors = unit_colors
@@ -64,19 +62,19 @@
             for segment_index in range(num_seg):
                 times = sorting.get_unit_spike_train(unit_id, segment_index=segment_index)
                 times = times / fs
                 amps = self.amplitudes[segment_index][unit_id]
                 ax.scatter(times, amps, color=self.unit_colors[unit_id], s=3, alpha=1)
 
                 if i == 0:
-                    ax.set_title(f'segment {segment_index}')
+                    ax.set_title(f"segment {segment_index}")
                 if i == len(self.unit_ids) - 1:
-                    ax.set_xlabel('Times [s]')
+                    ax.set_xlabel("Times [s]")
                 if segment_index == 0:
-                    ax.set_ylabel(f'Amplitude')
+                    ax.set_ylabel(f"Amplitude")
 
         ylims = ax.get_ylim()
         if np.max(ylims) < 0:
             ax.set_ylim(min(ylims), 0)
         if np.min(ylims) > 0:
             ax.set_ylim(0, max(ylims))
 
@@ -111,18 +109,18 @@
             amps = []
             for segment_index in range(num_seg):
                 amps.append(self.amplitudes[segment_index][unit_id])
             amps = np.concatenate(amps)
             unit_amps.append(amps)
         parts = ax.violinplot(unit_amps, showmeans=False, showmedians=False, showextrema=False)
 
-        for i, pc in enumerate(parts['bodies']):
+        for i, pc in enumerate(parts["bodies"]):
             color = self.unit_colors[unit_ids[i]]
             pc.set_facecolor(color)
-            pc.set_edgecolor('black')
+            pc.set_edgecolor("black")
             pc.set_alpha(1)
 
         ax.set_xticks(np.arange(len(unit_ids)) + 1)
         ax.set_xticklabels([str(unit_id) for unit_id in unit_ids])
 
         ylims = ax.get_ylim()
         if np.max(ylims) < 0:
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/basewidget.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/basewidget.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,28 +1,30 @@
-import matplotlib.pyplot as plt
-from matplotlib import gridspec
 import numpy as np
 
 
 # This class replace the old BaseWidget and BaseMultiWidget
 class BaseWidget:
     def __init__(self, figure=None, ax=None, axes=None, ncols=None, num_axes=None):
         """
         figure/ax/axes : only one of then can be not None
         """
+        import matplotlib.pyplot as plt
+
+        from matplotlib import gridspec
+
         if figure is not None:
-            assert ax is None and axes is None, 'figure/ax/axes : only one of then can be not None'
+            assert ax is None and axes is None, "figure/ax/axes : only one of then can be not None"
             ax = figure.add_subplot(111)
             axes = np.array([[ax]])
         elif ax is not None:
-            assert figure is None and axes is None, 'figure/ax/axes : only one of then can be not None'
+            assert figure is None and axes is None, "figure/ax/axes : only one of then can be not None"
             figure = ax.get_figure()
             axes = np.array([[ax]])
         elif axes is not None:
-            assert figure is None and ax is None, 'figure/ax/axes : only one of then can be not None'
+            assert figure is None and ax is None, "figure/ax/axes : only one of then can be not None"
             axes = np.asarray(axes)
             figure = axes.flatten()[0].get_figure()
         else:
             # one fig with one ax
             if num_axes is None:
                 figure, ax = plt.subplots()
                 axes = np.array([[ax]])
@@ -37,42 +39,43 @@
                     ax = figure.add_subplot(111)
                     axes = np.array([[ax]])
                 else:
                     assert ncols is not None
                     if num_axes < ncols:
                         ncols = num_axes
                     nrows = int(np.ceil(num_axes / ncols))
-                    figure, axes = plt.subplots(nrows=nrows, ncols=ncols, )
+                    figure, axes = plt.subplots(
+                        nrows=nrows,
+                        ncols=ncols,
+                    )
                     ax = None
                     # remove extra axes
                     if ncols * nrows > num_axes:
                         for extra_ax in axes.flatten()[num_axes:]:
                             extra_ax.remove()
 
         self.figure = figure
         self.ax = ax
         # axes is a 2D array of ax
         self.axes = axes
         # self.figure.axes is the flatten of all axes
-        
-        
-        
+
+
 class DataWidget:
-    
     def __init__(self) -> None:
         self.plotter = None
         pass
-    
+
     def _prepare_data(self):
         raise NotImplementedError
 
+
 # keep here just in case it is needed
 
 #    def create_tiled_ax(self, i, nrows, ncols, hspace=0.3, wspace=0.3, is_diag=False):
 #        gs = gridspec.GridSpecFromSubplotSpec(int(nrows), int(ncols), subplot_spec=self.ax,
 #                                                        hspace=hspace, wspace=wspace)
 #        r = int(i // ncols)
 #        c = int(np.mod(i, ncols))
 #        gs_sel = gs[r, c]
 #        ax = self.figure.add_subplot(gs_sel)
 #        return ax
-
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/collisioncomp.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/collisioncomp.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,16 +1,13 @@
 import numpy as np
 
-from matplotlib import pyplot as plt
-import matplotlib.gridspec as gridspec
-import matplotlib.colors
-
 from .basewidget import BaseWidget
 from spikeinterface.comparison.collisioncomparison import CollisionGTComparison
 
+
 class ComparisonCollisionPairByPairWidget(BaseWidget):
     """
     Plots CollisionGTComparison pair by pair.
 
     Parameters
     ----------
     comp: CollisionGTComparison
@@ -25,15 +22,19 @@
         The axis to be used. If not given an axis is created
 
     Returns
     -------
     W: MultiCompGraphWidget
         The output widget
     """
+
     def __init__(self, comp, unit_ids=None, figure=None, ax=None):
+        from matplotlib import pyplot as plt
+        import matplotlib.gridspec as gridspec
+        import matplotlib.colors
 
         BaseWidget.__init__(self, figure, ax)
         if unit_ids is None:
             # take all units
             unit_ids = comp.sorting1.get_unit_ids()
 
         self.comp = comp
@@ -64,60 +65,59 @@
 
         fs = self.comp.sorting1.get_sampling_frequency()
 
         lags = self.comp.bins / fs * 1000
         width = lags[1] - lags[0]
 
         for r in range(n):
-            for c in range(r+1, n):
-
+            for c in range(r + 1, n):
                 ax = axs[r, c]
 
                 u1 = self.unit_ids[r]
                 u2 = self.unit_ids[c]
                 ind1 = self.comp.sorting1.id_to_index(u1)
                 ind2 = self.comp.sorting1.id_to_index(u2)
 
                 tp = self.comp.all_tp[ind1, ind2, :]
                 fn = self.comp.all_fn[ind1, ind2, :]
-                ax.bar(lags[:-1], tp, width=width,  color='g', align='edge')
-                ax.bar(lags[:-1], fn, width=width, bottom=tp, color='r', align='edge')
+                ax.bar(lags[:-1], tp, width=width, color="g", align="edge")
+                ax.bar(lags[:-1], fn, width=width, bottom=tp, color="r", align="edge")
 
                 ax = axs[c, r]
                 tp = self.comp.all_tp[ind2, ind1, :]
                 fn = self.comp.all_fn[ind2, ind1, :]
-                ax.bar(lags[:-1], tp, width=width,  color='g', align='edge')
-                ax.bar(lags[:-1], fn, width=width, bottom=tp, color='r', align='edge')
+                ax.bar(lags[:-1], tp, width=width, color="g", align="edge")
+                ax.bar(lags[:-1], fn, width=width, bottom=tp, color="r", align="edge")
 
         for r in range(n):
             ax = axs[r, 0]
             u1 = self.unit_ids[r]
-            ax.set_ylabel(f'gt id{u1}')
+            ax.set_ylabel(f"gt id{u1}")
 
         for c in range(n):
             ax = axs[0, c]
             u2 = self.unit_ids[c]
-            ax.set_title(f'collision with \ngt id{u2}')
+            ax.set_title(f"collision with \ngt id{u2}")
 
         ax = axs[-1, 0]
-        ax.set_xlabel('collision lag [ms]')
+        ax.set_xlabel("collision lag [ms]")
 
 
 class ComparisonCollisionBySimilarityWidget(BaseWidget):
     """
     Plots CollisionGTComparison pair by pair orderer by cosine_similarity
 
     Parameters
     ----------
     comp: CollisionGTComparison
         The collision ground truth comparison object
     templates: array
         template of units
     mode: 'heatmap' or 'lines'
-        to see collision curves for every pairs ('heatmap') or as lines averaged over pairs. 
+        to see collision curves for every pairs ('heatmap') or as lines averaged over pairs.
     similarity_bins: array
         if mode is 'lines', the bins used to average the pairs
     cmap: string
         colormap used to show averages if mode is 'lines'
     metric: 'cosine_similarity'
         metric for ordering
     good_only: True
@@ -129,20 +129,37 @@
         List of considered units
     figure: matplotlib figure
         The figure to be used. If not given a figure is created
     ax: matplotlib axis
         The axis to be used. If not given an axis is created
     """
 
-    def __init__(self, comp, templates, unit_ids=None, metric='cosine_similarity', figure=None, ax=None, 
-        mode='heatmap', similarity_bins=np.linspace(-0.4, 1, 8), cmap='winter', good_only=True,  min_accuracy=0.9, show_legend=False,
-        ylim = (0, 1)):
+    def __init__(
+        self,
+        comp,
+        templates,
+        unit_ids=None,
+        metric="cosine_similarity",
+        figure=None,
+        ax=None,
+        mode="heatmap",
+        similarity_bins=np.linspace(-0.4, 1, 8),
+        cmap="winter",
+        good_only=True,
+        min_accuracy=0.9,
+        show_legend=False,
+        ylim=(0, 1),
+    ):
+        from matplotlib import pyplot as plt
+        import matplotlib.gridspec as gridspec
+        import matplotlib.colors
+
         BaseWidget.__init__(self, figure, ax)
 
-        assert mode in ['heatmap', 'lines']
+        assert mode in ["heatmap", "lines"]
 
         if unit_ids is None:
             # take all units
             unit_ids = comp.sorting1.get_unit_ids()
 
         self.comp = comp
         self.cmap = cmap
@@ -156,109 +173,123 @@
         self.good_only = good_only
         self.min_accuracy = min_accuracy
 
     def plot(self):
         self._do_plot()
 
     def _do_plot(self):
-
         import sklearn
 
         # compute similarity
         # take index of template (respect unit_ids order)
         all_unit_ids = list(self.comp.sorting1.get_unit_ids())
         template_inds = [all_unit_ids.index(u) for u in self.unit_ids]
 
         templates = self.templates[template_inds, :, :].copy()
         flat_templates = templates.reshape(templates.shape[0], -1)
-        if self.metric == 'cosine_similarity':
+        if self.metric == "cosine_similarity":
             similarity_matrix = sklearn.metrics.pairwise.cosine_similarity(flat_templates)
         else:
-            raise NotImplementedError('metric=...')
+            raise NotImplementedError("metric=...")
 
         fs = self.comp.sorting1.get_sampling_frequency()
         lags = self.comp.bins / fs * 1000
 
         n = len(self.unit_ids)
 
-        similarities, recall_scores, pair_names = self.comp.compute_collision_by_similarity(similarity_matrix, unit_ids=self.unit_ids, good_only=self.good_only, min_accuracy=self.min_accuracy)
-
-        if self.mode == 'heatmap':
+        similarities, recall_scores, pair_names = self.comp.compute_collision_by_similarity(
+            similarity_matrix, unit_ids=self.unit_ids, good_only=self.good_only, min_accuracy=self.min_accuracy
+        )
 
+        if self.mode == "heatmap":
             fig = self.figure
             for ax in fig.axes:
                 ax.remove()
 
             n_pair = len(similarities)
 
-            ax0 = fig.add_axes([0.1, 0.1, .25, 0.8])
-            ax1 = fig.add_axes([0.4, 0.1, .5, 0.8], sharey=ax0)
+            ax0 = fig.add_axes([0.1, 0.1, 0.25, 0.8])
+            ax1 = fig.add_axes([0.4, 0.1, 0.5, 0.8], sharey=ax0)
 
             plt.setp(ax1.get_yticklabels(), visible=False)
 
-            im = ax1.imshow(recall_scores[::-1, :],
-                        cmap='viridis',
-                        aspect='auto',
-                        interpolation='none',
-                        extent=(lags[0], lags[-1], -0.5, n_pair-0.5),
-                        )
-            im.set_clim(0,1)
+            im = ax1.imshow(
+                recall_scores[::-1, :],
+                cmap="viridis",
+                aspect="auto",
+                interpolation="none",
+                extent=(lags[0], lags[-1], -0.5, n_pair - 0.5),
+            )
+            im.set_clim(0, 1)
 
-            ax0.plot(similarities, np.arange(n_pair), color='k')
+            ax0.plot(similarities, np.arange(n_pair), color="k")
 
             ax0.set_yticks(np.arange(n_pair))
             ax0.set_yticklabels(pair_names)
             # ax0.set_xlim(0,1)
 
             ax0.set_xlabel(self.metric)
-            ax0.set_ylabel('pairs')
+            ax0.set_ylabel("pairs")
 
-            ax1.set_xlabel('lag (ms)')
-        elif self.mode == 'lines':
+            ax1.set_xlabel("lag (ms)")
+        elif self.mode == "lines":
             my_cmap = plt.get_cmap(self.cmap)
-            cNorm  = matplotlib.colors.Normalize(vmin=self.similarity_bins.min(), vmax=self.similarity_bins.max())
+            cNorm = matplotlib.colors.Normalize(vmin=self.similarity_bins.min(), vmax=self.similarity_bins.max())
             scalarMap = plt.cm.ScalarMappable(norm=cNorm, cmap=my_cmap)
 
             # plot by similarity bins
             if self.ax is None:
                 fig, ax = plt.subplots()
             else:
                 ax = self.ax
-            ax.spines['top'].set_visible(False)
-            ax.spines['right'].set_visible(False)
+            ax.spines["top"].set_visible(False)
+            ax.spines["right"].set_visible(False)
 
             order = np.argsort(similarities)
             similarities = similarities[order]
             recall_scores = recall_scores[order, :]
 
             for i in range(self.similarity_bins.size - 1):
                 cmin, cmax = self.similarity_bins[i], self.similarity_bins[i + 1]
 
                 amin, amax = np.searchsorted(similarities, [cmin, cmax])
                 mean_recall_scores = np.nanmean(recall_scores[amin:amax], axis=0)
 
-                colorVal = scalarMap.to_rgba((cmin+cmax)/2)
-                ax.plot(lags[:-1] + (lags[1]-lags[0]) / 2, mean_recall_scores, label='CS in [%g,%g]' %(cmin, cmax), c=colorVal)
+                colorVal = scalarMap.to_rgba((cmin + cmax) / 2)
+                ax.plot(
+                    lags[:-1] + (lags[1] - lags[0]) / 2,
+                    mean_recall_scores,
+                    label="CS in [%g,%g]" % (cmin, cmax),
+                    c=colorVal,
+                )
 
             if self.show_legend:
                 ax.legend()
             ax.set_ylim(self.ylim)
-            ax.set_xlabel('lags (ms)')
-            ax.set_ylabel('collision accuracy')      
-
+            ax.set_xlabel("lags (ms)")
+            ax.set_ylabel("collision accuracy")
 
 
 class StudyComparisonCollisionBySimilarityWidget(BaseWidget):
-
-
-    def __init__(self, study, metric='cosine_similarity',
-                 similarity_bins=np.linspace(-0.4, 1, 8), show_legend=False, ylim=(0.5, 1),
-                 good_only=True,
-                 min_accuracy=0.9,
-                 ncols=3, axes=None, cmap='winter'):
+    def __init__(
+        self,
+        study,
+        metric="cosine_similarity",
+        similarity_bins=np.linspace(-0.4, 1, 8),
+        show_legend=False,
+        ylim=(0.5, 1),
+        good_only=True,
+        min_accuracy=0.9,
+        ncols=3,
+        axes=None,
+        cmap="winter",
+    ):
+        from matplotlib import pyplot as plt
+        import matplotlib.gridspec as gridspec
+        import matplotlib.colors
 
         if axes is None:
             num_axes = len(study.sorter_names)
         else:
             num_axes = None
         BaseWidget.__init__(self, None, None, axes, ncols=ncols, num_axes=num_axes)
 
@@ -268,116 +299,132 @@
         self.cmap = cmap
         self.similarity_bins = np.asarray(similarity_bins)
         self.show_legend = show_legend
         self.ylim = ylim
         self.good_only = good_only
         self.min_accuracy = min_accuracy
 
-
     def plot(self):
-
         my_cmap = plt.get_cmap(self.cmap)
-        cNorm  = matplotlib.colors.Normalize(vmin=self.similarity_bins.min(), vmax=self.similarity_bins.max())
+        cNorm = matplotlib.colors.Normalize(vmin=self.similarity_bins.min(), vmax=self.similarity_bins.max())
         scalarMap = plt.cm.ScalarMappable(norm=cNorm, cmap=my_cmap)
         self.study.precompute_scores_by_similarities(self.good_only, min_accuracy=self.min_accuracy)
         lags = self.study.get_lags()
 
         for sorter_ind, sorter_name in enumerate(self.study.sorter_names):
-
             curves = self.study.get_lag_profile_over_similarity_bins(self.similarity_bins, sorter_name)
 
             # plot by similarity bins
             ax = self.axes.flatten()[sorter_ind]
-            ax.spines['top'].set_visible(False)
-            ax.spines['right'].set_visible(False)
+            ax.spines["top"].set_visible(False)
+            ax.spines["right"].set_visible(False)
 
             for i in range(self.similarity_bins.size - 1):
                 cmin, cmax = self.similarity_bins[i], self.similarity_bins[i + 1]
-                colorVal = scalarMap.to_rgba((cmin+cmax)/2)
-                ax.plot(lags[:-1] + (lags[1]-lags[0]) / 2, curves[(cmin, cmax)], label='CS in [%g,%g]' %(cmin, cmax), c=colorVal)
+                colorVal = scalarMap.to_rgba((cmin + cmax) / 2)
+                ax.plot(
+                    lags[:-1] + (lags[1] - lags[0]) / 2,
+                    curves[(cmin, cmax)],
+                    label="CS in [%g,%g]" % (cmin, cmax),
+                    c=colorVal,
+                )
 
             if np.mod(sorter_ind, self.ncols) == 0:
-                ax.set_ylabel('collision accuracy')
+                ax.set_ylabel("collision accuracy")
 
             if sorter_ind > (len(self.study.sorter_names) // self.ncols):
-                ax.set_xlabel('lags (ms)')
+                ax.set_xlabel("lags (ms)")
 
             ax.set_title(sorter_name)
             if self.show_legend:
                 ax.legend()
 
             if self.ylim is not None:
                 ax.set_ylim(self.ylim)
 
 
-
 class StudyComparisonCollisionBySimilarityRangeWidget(BaseWidget):
-
-
-    def __init__(self, study, metric='cosine_similarity',
-                 similarity_range=[0, 1], show_legend=False, ylim=(0.5, 1),
-                 good_only=True, min_accuracy=0.9, ax=None):
+    def __init__(
+        self,
+        study,
+        metric="cosine_similarity",
+        similarity_range=[0, 1],
+        show_legend=False,
+        ylim=(0.5, 1),
+        good_only=True,
+        min_accuracy=0.9,
+        ax=None,
+    ):
+        from matplotlib import pyplot as plt
+        import matplotlib.gridspec as gridspec
+        import matplotlib.colors
 
         BaseWidget.__init__(self, None, ax)
 
         self.study = study
         self.metric = metric
         self.similarity_range = similarity_range
         self.show_legend = show_legend
         self.ylim = ylim
         self.good_only = good_only
         self.min_accuracy = min_accuracy
 
-
     def plot(self):
-
         self.study.precompute_scores_by_similarities(self.good_only, min_accuracy=self.min_accuracy)
         lags = self.study.get_lags()
 
         for sorter_ind, sorter_name in enumerate(self.study.sorter_names):
-
             mean_recall_scores = self.study.get_mean_over_similarity_range(self.similarity_range, sorter_name)
-            self.ax.plot(lags[:-1] + (lags[1]-lags[0]) / 2, mean_recall_scores, label=sorter_name, c='C%d' %sorter_ind)
+            self.ax.plot(
+                lags[:-1] + (lags[1] - lags[0]) / 2, mean_recall_scores, label=sorter_name, c="C%d" % sorter_ind
+            )
 
-        self.ax.set_ylabel('collision accuracy')
-        self.ax.set_xlabel('lags (ms)')
+        self.ax.set_ylabel("collision accuracy")
+        self.ax.set_xlabel("lags (ms)")
 
         if self.show_legend:
             self.ax.legend()
 
         if self.ylim is not None:
             self.ax.set_ylim(self.ylim)
 
 
 class StudyComparisonCollisionBySimilarityRangesWidget(BaseWidget):
-
-
-    def __init__(self, study, metric='cosine_similarity',
-                 similarity_ranges=np.linspace(-0.4, 1, 8), show_legend=False, ylim=(0.5, 1),
-                 good_only=True, min_accuracy=0.9, ax=None, show_std=False):
+    def __init__(
+        self,
+        study,
+        metric="cosine_similarity",
+        similarity_ranges=np.linspace(-0.4, 1, 8),
+        show_legend=False,
+        ylim=(0.5, 1),
+        good_only=True,
+        min_accuracy=0.9,
+        ax=None,
+        show_std=False,
+    ):
+        from matplotlib import pyplot as plt
+        import matplotlib.gridspec as gridspec
+        import matplotlib.colors
 
         BaseWidget.__init__(self, None, ax)
 
         self.study = study
         self.metric = metric
         self.similarity_ranges = similarity_ranges
         self.show_legend = show_legend
         self.ylim = ylim
         self.good_only = good_only
         self.show_std = show_std
         self.min_accuracy = min_accuracy
 
-
     def plot(self):
-
         self.study.precompute_scores_by_similarities(self.good_only, min_accuracy=self.min_accuracy)
         lags = self.study.get_lags()
 
         for sorter_ind, sorter_name in enumerate(self.study.sorter_names):
-
             all_similarities = self.study.all_similarities[sorter_name]
             all_recall_scores = self.study.all_recall_scores[sorter_name]
 
             order = np.argsort(all_similarities)
             all_similarities = all_similarities[order]
             all_recall_scores = all_recall_scores[order, :]
 
@@ -385,55 +432,69 @@
             std_recall_scores = []
             for i in range(self.similarity_ranges.size - 1):
                 cmin, cmax = self.similarity_ranges[i], self.similarity_ranges[i + 1]
                 amin, amax = np.searchsorted(all_similarities, [cmin, cmax])
                 mean_recall_scores += [np.nanmean(all_recall_scores[amin:amax])]
                 std_recall_scores += [np.nanstd(all_recall_scores[amin:amax])]
 
-            xaxis = np.diff(self.similarity_ranges)/2 + self.similarity_ranges[:-1]
+            xaxis = np.diff(self.similarity_ranges) / 2 + self.similarity_ranges[:-1]
 
             if not self.show_std:
-                self.ax.plot(xaxis, mean_recall_scores, label=sorter_name, c='C%d' %sorter_ind)
+                self.ax.plot(xaxis, mean_recall_scores, label=sorter_name, c="C%d" % sorter_ind)
             else:
-                self.ax.errorbar(xaxis, mean_recall_scores, yerr=std_recall_scores, label=sorter_name, c='C%d' %sorter_ind)
+                self.ax.errorbar(
+                    xaxis, mean_recall_scores, yerr=std_recall_scores, label=sorter_name, c="C%d" % sorter_ind
+                )
 
-        self.ax.set_ylabel('collision accuracy')
-        self.ax.set_xlabel('similarity')
+        self.ax.set_ylabel("collision accuracy")
+        self.ax.set_xlabel("similarity")
 
         if self.show_legend:
             self.ax.legend()
 
         if self.ylim is not None:
             self.ax.set_ylim(self.ylim)
 
 
 def plot_comparison_collision_pair_by_pair(*args, **kwargs):
     W = ComparisonCollisionPairByPairWidget(*args, **kwargs)
     W.plot()
     return W
+
+
 plot_comparison_collision_pair_by_pair.__doc__ = ComparisonCollisionPairByPairWidget.__doc__
 
 
 def plot_comparison_collision_by_similarity(*args, **kwargs):
     W = ComparisonCollisionBySimilarityWidget(*args, **kwargs)
     W.plot()
     return W
+
+
 plot_comparison_collision_by_similarity.__doc__ = ComparisonCollisionBySimilarityWidget.__doc__
 
 
 def plot_study_comparison_collision_by_similarity(*args, **kwargs):
     W = StudyComparisonCollisionBySimilarityWidget(*args, **kwargs)
     W.plot()
     return W
+
+
 plot_study_comparison_collision_by_similarity.__doc__ = StudyComparisonCollisionBySimilarityWidget.__doc__
 
+
 def plot_study_comparison_collision_by_similarity_range(*args, **kwargs):
     W = StudyComparisonCollisionBySimilarityRangeWidget(*args, **kwargs)
     W.plot()
     return W
+
+
 plot_study_comparison_collision_by_similarity_range.__doc__ = StudyComparisonCollisionBySimilarityRangeWidget.__doc__
 
+
 def plot_study_comparison_collision_by_similarity_ranges(*args, **kwargs):
     W = StudyComparisonCollisionBySimilarityRangesWidget(*args, **kwargs)
     W.plot()
     return W
+
+
 plot_study_comparison_collision_by_similarity_ranges.__doc__ = StudyComparisonCollisionBySimilarityRangesWidget.__doc__
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/confusionmatrix.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/confusionmatrix.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,9 +1,8 @@
 import numpy as np
-from matplotlib import pyplot as plt
 
 from .basewidget import BaseWidget
 
 
 class ConfusionMatrixWidget(BaseWidget):
     """
     Plots sorting comparison confusion matrix.
@@ -23,64 +22,68 @@
 
     Returns
     -------
     W: ConfusionMatrixWidget
         The output widget
     """
 
-    def __init__(self, gt_comparison, count_text=True, unit_ticks=True,
-                 figure=None, ax=None):
+    def __init__(self, gt_comparison, count_text=True, unit_ticks=True, figure=None, ax=None):
+        from matplotlib import pyplot as plt
+
         BaseWidget.__init__(self, figure, ax)
         self._gtcomp = gt_comparison
         self._count_text = count_text
         self._unit_ticks = unit_ticks
-        self.name = 'ConfusionMatrix'
+        self.name = "ConfusionMatrix"
 
     def plot(self):
         self._do_plot()
 
     def _do_plot(self):
         # a dataframe
         confusion_matrix = self._gtcomp.get_confusion_matrix()
 
         N1 = confusion_matrix.shape[0] - 1
         N2 = confusion_matrix.shape[1] - 1
 
         # Using matshow here just because it sets the ticks up nicely. imshow is faster.
-        self.ax.matshow(confusion_matrix.values, cmap='Greens')
+        self.ax.matshow(confusion_matrix.values, cmap="Greens")
 
         if self._count_text:
             for (i, j), z in np.ndenumerate(confusion_matrix.values):
                 if z != 0:
-                    if z > np.max(confusion_matrix.values) / 2.:
-                        self.ax.text(j, i, '{:d}'.format(z), ha='center', va='center', color='white')
+                    if z > np.max(confusion_matrix.values) / 2.0:
+                        self.ax.text(j, i, "{:d}".format(z), ha="center", va="center", color="white")
                     else:
-                        self.ax.text(j, i, '{:d}'.format(z), ha='center', va='center', color='black')
+                        self.ax.text(j, i, "{:d}".format(z), ha="center", va="center", color="black")
 
-        self.ax.axhline(int(N1 - 1) + 0.5, color='black')
-        self.ax.axvline(int(N2 - 1) + 0.5, color='black')
+        self.ax.axhline(int(N1 - 1) + 0.5, color="black")
+        self.ax.axvline(int(N2 - 1) + 0.5, color="black")
 
         # Major ticks
         self.ax.set_xticks(np.arange(0, N2 + 1))
         self.ax.set_yticks(np.arange(0, N1 + 1))
         self.ax.xaxis.tick_bottom()
 
         # Labels for major ticks
         if self._unit_ticks:
             self.ax.set_yticklabels(confusion_matrix.index, fontsize=12)
             self.ax.set_xticklabels(confusion_matrix.columns, fontsize=12)
         else:
-            self.ax.set_xticklabels(np.append([''] * N2, 'FN'), fontsize=10)
-            self.ax.set_yticklabels(np.append([''] * N1, 'FP'), fontsize=10)
+            self.ax.set_xticklabels(np.append([""] * N2, "FN"), fontsize=10)
+            self.ax.set_yticklabels(np.append([""] * N1, "FP"), fontsize=10)
 
         self.ax.set_xlabel(self._gtcomp.name_list[1], fontsize=20)
         self.ax.set_ylabel(self._gtcomp.name_list[0], fontsize=20)
 
         self.ax.set_xlim(-0.5, N2 + 0.5)
-        self.ax.set_ylim(N1 + 0.5, -0.5, )
+        self.ax.set_ylim(
+            N1 + 0.5,
+            -0.5,
+        )
 
 
 def plot_confusion_matrix(*args, **kwargs):
     W = ConfusionMatrixWidget(*args, **kwargs)
     W.plot()
     return W
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/correlogramcomp.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/correlogramcomp.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,22 +1,27 @@
 import numpy as np
-from matplotlib import pyplot as plt
 
 from .basewidget import BaseWidget
 
-from matplotlib import pyplot as plt
-import matplotlib.gridspec as gridspec
-import matplotlib.colors
-
 
 class StudyComparisonCorrelogramBySimilarityWidget(BaseWidget):
-
-    def __init__(self, study, metric='cosine_similarity',
-                 similarity_bins=np.linspace(-0.4, 1, 8), show_legend=False,
-                 ncols=3, axes=None, cmap='winter', ylim=(0,0.5)):
+    def __init__(
+        self,
+        study,
+        metric="cosine_similarity",
+        similarity_bins=np.linspace(-0.4, 1, 8),
+        show_legend=False,
+        ncols=3,
+        axes=None,
+        cmap="winter",
+        ylim=(0, 0.5),
+    ):
+        from matplotlib import pyplot as plt
+        import matplotlib.gridspec as gridspec
+        import matplotlib.colors
 
         if axes is None:
             num_axes = len(study.sorter_names)
         else:
             num_axes = None
         BaseWidget.__init__(self, None, None, axes, ncols=ncols, num_axes=num_axes)
 
@@ -25,73 +30,76 @@
         self.study = study
         self.metric = metric
         self.similarity_bins = np.asarray(similarity_bins)
         self.show_legend = show_legend
         self.ylim = ylim
 
     def plot(self):
-
         my_cmap = plt.get_cmap(self.cmap)
-        cNorm  = matplotlib.colors.Normalize(vmin=self.similarity_bins.min(), vmax=self.similarity_bins.max())
+        cNorm = matplotlib.colors.Normalize(vmin=self.similarity_bins.min(), vmax=self.similarity_bins.max())
         scalarMap = plt.cm.ScalarMappable(norm=cNorm, cmap=my_cmap)
 
         self.study.precompute_scores_by_similarities()
         time_bins = self.study.time_bins
 
         for sorter_ind, sorter_name in enumerate(self.study.sorter_names):
-
             result = self.study.get_error_profile_over_similarity_bins(self.similarity_bins, sorter_name)
 
             # plot by similarity bins
             ax = self.axes.flatten()[sorter_ind]
-            ax.spines['top'].set_visible(False)
-            ax.spines['right'].set_visible(False)
+            ax.spines["top"].set_visible(False)
+            ax.spines["right"].set_visible(False)
 
             for i in range(self.similarity_bins.size - 1):
                 cmin, cmax = self.similarity_bins[i], self.similarity_bins[i + 1]
-                colorVal = scalarMap.to_rgba((cmin+cmax)/2)
-                ax.plot(time_bins, result[(cmin, cmax)], label='CS in [%g,%g]' %(cmin, cmax), c=colorVal)
+                colorVal = scalarMap.to_rgba((cmin + cmax) / 2)
+                ax.plot(time_bins, result[(cmin, cmax)], label="CS in [%g,%g]" % (cmin, cmax), c=colorVal)
 
             if np.mod(sorter_ind, self.ncols) == 0:
-                ax.set_ylabel('cc error')
+                ax.set_ylabel("cc error")
 
             if sorter_ind >= (len(self.study.sorter_names) // self.ncols):
-                ax.set_xlabel('lags (ms)')
+                ax.set_xlabel("lags (ms)")
 
             ax.set_title(sorter_name)
             if self.show_legend:
                 ax.legend()
 
             if self.ylim is not None:
                 ax.set_ylim(self.ylim)
 
 
 class StudyComparisonCorrelogramBySimilarityRangesMeanErrorWidget(BaseWidget):
-
-
-    def __init__(self, study, metric='cosine_similarity',
-                 similarity_ranges=np.linspace(-0.4, 1, 8), show_legend=False,
-                 ax=None, show_std=False, ylim=(0,0.5)):
+    def __init__(
+        self,
+        study,
+        metric="cosine_similarity",
+        similarity_ranges=np.linspace(-0.4, 1, 8),
+        show_legend=False,
+        ax=None,
+        show_std=False,
+        ylim=(0, 0.5),
+    ):
+        from matplotlib import pyplot as plt
+        import matplotlib.gridspec as gridspec
+        import matplotlib.colors
 
         BaseWidget.__init__(self, None, ax)
 
         self.study = study
         self.metric = metric
         self.show_std = show_std
         self.ylim = None
         self.similarity_ranges = np.asarray(similarity_ranges)
         self.show_legend = show_legend
 
     def plot(self):
-
         self.study.precompute_scores_by_similarities()
 
         for sorter_ind, sorter_name in enumerate(self.study.sorter_names):
-
-
             all_similarities = self.study.all_similarities[sorter_name]
             all_errors = self.study.all_errors[sorter_name]
 
             order = np.argsort(all_similarities)
             all_similarities = all_similarities[order]
             all_errors = all_errors[order, :]
 
@@ -99,42 +107,48 @@
             std_errors = []
             for i in range(self.similarity_ranges.size - 1):
                 cmin, cmax = self.similarity_ranges[i], self.similarity_ranges[i + 1]
                 amin, amax = np.searchsorted(all_similarities, [cmin, cmax])
                 mean_rerrors += [np.nanmean(all_errors[amin:amax])]
                 std_errors += [np.nanstd(all_errors[amin:amax])]
 
-            xaxis = np.diff(self.similarity_ranges)/2 + self.similarity_ranges[:-1]
+            xaxis = np.diff(self.similarity_ranges) / 2 + self.similarity_ranges[:-1]
 
             if not self.show_std:
-                self.ax.plot(xaxis, mean_rerrors, label=sorter_name, c='C%d' %sorter_ind)
+                self.ax.plot(xaxis, mean_rerrors, label=sorter_name, c="C%d" % sorter_ind)
             else:
-                self.ax.errorbar(xaxis, mean_rerrors, yerr=std_errors, label=sorter_name, c='C%d' %sorter_ind)
+                self.ax.errorbar(xaxis, mean_rerrors, yerr=std_errors, label=sorter_name, c="C%d" % sorter_ind)
 
-        self.ax.set_ylabel('cc error')
-        self.ax.set_xlabel('similarity')
+        self.ax.set_ylabel("cc error")
+        self.ax.set_xlabel("similarity")
 
         if self.show_legend:
             self.ax.legend()
 
         if self.ylim is not None:
             self.ax.set_ylim(self.ylim)
 
 
 def plot_study_comparison_correlogram_by_similarity(*args, **kwargs):
     W = StudyComparisonCorrelogramBySimilarityWidget(*args, **kwargs)
     W.plot()
     return W
+
+
 plot_study_comparison_correlogram_by_similarity.__doc__ = StudyComparisonCorrelogramBySimilarityWidget.__doc__
 
 # def plot_study_comparison_Correlogram_by_similarity_range(*args, **kwargs):
 #     W = StudyComparisonCorrelogramBySimilarityRangeWidget(*args, **kwargs)
 #     W.plot()
 #     return W
 # plot_study_comparison_Correlogram_by_similarity_range.__doc__ = StudyComparisonCorrelogramBySimilarityRangeWidget.__doc__
 
 
 def plot_study_comparison_correlogram_by_similarity_ranges_mean_error(*args, **kwargs):
     W = StudyComparisonCorrelogramBySimilarityRangesMeanErrorWidget(*args, **kwargs)
     W.plot()
     return W
-plot_study_comparison_correlogram_by_similarity_ranges_mean_error.__doc__ = StudyComparisonCorrelogramBySimilarityRangesMeanErrorWidget.__doc__
+
+
+plot_study_comparison_correlogram_by_similarity_ranges_mean_error.__doc__ = (
+    StudyComparisonCorrelogramBySimilarityRangesMeanErrorWidget.__doc__
+)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/correlograms_.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/correlograms_.py`

 * *Files 9% similar despite different names*

```diff
@@ -5,30 +5,28 @@
 from spikeinterface.postprocessing import compute_correlograms
 
 
 class CrossCorrelogramsWidget(BaseWidget):
     """
     Plots spike train cross-correlograms.
     The diagonal is auto-correlogram.
-    
+
     Parameters
     ----------
     sorting: SortingExtractor
         The sorting extractor object
     unit_ids: list
         List of unit ids
     bin_ms:  float
         bins duration in ms
     window_ms: float
         Window duration in ms
     """
 
-    def __init__(self, sorting, unit_ids=None,
-                 window_ms=100.0, bin_ms=1.0, axes=None):
-
+    def __init__(self, sorting, unit_ids=None, window_ms=100.0, bin_ms=1.0, axes=None):
         if unit_ids is not None:
             sorting = sorting.select_units(unit_ids)
         self.sorting = sorting
         self.compute_kwargs = dict(window_ms=window_ms, bin_ms=bin_ms)
 
         if axes is None:
             n = len(sorting.unit_ids)
@@ -40,22 +38,22 @@
         bin_width = bins[1] - bins[0]
         unit_ids = self.sorting.unit_ids
         for i, unit_id1 in enumerate(unit_ids):
             for j, unit_id2 in enumerate(unit_ids):
                 ccg = correlograms[i, j]
                 ax = self.axes[i, j]
                 if i == j:
-                    color = 'g'
+                    color = "g"
                 else:
-                    color = 'k'
-                ax.bar(x=bins[:-1], height=ccg, width=bin_width, color=color, align='edge')
+                    color = "k"
+                ax.bar(x=bins[:-1], height=ccg, width=bin_width, color=color, align="edge")
 
         for i, unit_id in enumerate(unit_ids):
             self.axes[0, i].set_title(str(unit_id))
-            self.axes[-1, i].set_xlabel('CCG (ms)')
+            self.axes[-1, i].set_xlabel("CCG (ms)")
 
 
 def plot_crosscorrelograms(*args, **kwargs):
     W = CrossCorrelogramsWidget(*args, **kwargs)
     W.plot()
     return W
 
@@ -74,18 +72,15 @@
         List of unit ids
     bin_ms:  float
         bins duration in ms
     window_ms: float
         Window duration in ms
     """
 
-    def __init__(self, sorting, unit_ids=None,
-                 window_ms=100.0, bin_ms=1.0,
-                 ncols=5, axes=None):
-
+    def __init__(self, sorting, unit_ids=None, window_ms=100.0, bin_ms=1.0, ncols=5, axes=None):
         if unit_ids is not None:
             sorting = sorting.select_units(unit_ids)
         self.sorting = sorting
         self.compute_kwargs = dict(window_ms=window_ms, bin_ms=bin_ms)
 
         if axes is None:
             num_axes = len(sorting.unit_ids)
@@ -94,16 +89,16 @@
     def plot(self):
         correlograms, bins = compute_correlograms(self.sorting, **self.compute_kwargs)
         bin_width = bins[1] - bins[0]
         unit_ids = self.sorting.unit_ids
         for i, unit_id in enumerate(unit_ids):
             ccg = correlograms[i, i]
             ax = self.axes.flatten()[i]
-            color = 'g'
-            ax.bar(x=bins[:-1], height=ccg, width=bin_width, color=color, align='edge')
+            color = "g"
+            ax.bar(x=bins[:-1], height=ccg, width=bin_width, color=color, align="edge")
             ax.set_title(str(unit_id))
 
 
 def plot_autocorrelograms(*args, **kwargs):
     W = AutoCorrelogramsWidget(*args, **kwargs)
     W.plot()
     return W
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/depthamplitude.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/depthamplitude.py`

 * *Files 4% similar despite different names*

```diff
@@ -4,31 +4,30 @@
 from .basewidget import BaseWidget
 
 from ...postprocessing import get_template_extremum_channel, get_template_extremum_amplitude
 from .utils import get_unit_colors
 
 
 class UnitsDepthAmplitudeWidget(BaseWidget):
-    def __init__(self, waveform_extractor, peak_sign='neg', depth_axis=1,
-                 unit_colors=None, figure=None, ax=None):
+    def __init__(self, waveform_extractor, peak_sign="neg", depth_axis=1, unit_colors=None, figure=None, ax=None):
         BaseWidget.__init__(self, figure, ax)
 
         self.we = waveform_extractor
         self.peak_sign = peak_sign
         self.depth_axis = depth_axis
         if unit_colors is None:
             unit_colors = get_unit_colors(self.we.sorting)
         self.unit_colors = unit_colors
 
     def plot(self):
         ax = self.ax
         we = self.we
         unit_ids = we.unit_ids
 
-        channels_index = get_template_extremum_channel(we, peak_sign=self.peak_sign, outputs='index')
+        channels_index = get_template_extremum_channel(we, peak_sign=self.peak_sign, outputs="index")
         contact_positions = we.get_channel_locations()
 
         channel_depth = contact_positions[:, self.depth_axis]
         unit_depth = [channel_depth[channels_index[unit_id]] for unit_id in unit_ids]
 
         unit_amplitude = get_template_extremum_amplitude(we, peak_sign=self.peak_sign)
         unit_amplitude = np.abs([unit_amplitude[unit_id] for unit_id in unit_ids])
@@ -41,16 +40,16 @@
                 st = we.sorting.get_unit_spike_train(unit_id=unit_id, segment_index=segment_index)
                 num_spikes[i] += st.size
 
         size = num_spikes / max(num_spikes) * 120
         ax.scatter(unit_amplitude, unit_depth, color=colors, s=size)
 
         ax.set_aspect(3)
-        ax.set_xlabel('amplitude')
-        ax.set_ylabel('depth [um]')
+        ax.set_xlabel("amplitude")
+        ax.set_ylabel("depth [um]")
         ax.set_xlim(0, max(unit_amplitude) * 1.2)
 
 
 def plot_units_depth_vs_amplitude(*args, **kwargs):
     W = UnitsDepthAmplitudeWidget(*args, **kwargs)
     W.plot()
     return W
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/gtcomparison.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/gtcomparison.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,16 +1,14 @@
 """
 Various widgets on top of GroundTruthStudy to summary results:
   * run times
   * performancess
   * count units
 """
 import numpy as np
-from matplotlib import pyplot as plt
-import pandas as pd
 
 from .basewidget import BaseWidget
 
 
 class ComparisonPerformancesWidget(BaseWidget):
     """
     Plot run times for a study.
@@ -20,41 +18,45 @@
     gt_comparison: GroundTruthComparison
         The ground truth sorting comparison object
     ax: matplotlib ax
         The ax to be used. If not given a figure is created
     cmap_name
 
     """
-    def __init__(self, gt_comp, palette='Set1',  ax=None):
+
+    def __init__(self, gt_comp, palette="Set1", ax=None):
+        from matplotlib import pyplot as plt
+        import pandas as pd
 
         self.gt_comp = gt_comp
         self.palette = palette
 
         BaseWidget.__init__(self, ax=ax)
 
     def plot(self):
         import seaborn as sns
+
         ax = self.ax
 
         sns.set_palette(sns.color_palette(self.palette))
 
         perf_by_units = self.gt_comp.get_performance()
         perf_by_units = perf_by_units.reset_index()
 
-        df = pd.melt(perf_by_units, var_name='Metric', value_name='Score',
-                    value_vars=('accuracy','precision', 'recall'))
+        df = pd.melt(
+            perf_by_units, var_name="Metric", value_name="Score", value_vars=("accuracy", "precision", "recall")
+        )
         import seaborn as sns
-        sns.swarmplot(data=df, x="Metric", y='Score', hue='Metric', dodge=True,
-                                    s=3, ax=ax) # order=sorter_list,
-        #~ ax.set_xticklabels(sorter_names_short, rotation=30, ha='center')
-        #~ ax.legend(bbox_to_anchor=(1.0, 1), loc=2, borderaxespad=0., frameon=False, fontsize=8, markerscale=0.5)
 
-        ax.set_ylim(0, 1.05)
-        ax.set_ylabel(f'Performance')
+        sns.swarmplot(data=df, x="Metric", y="Score", hue="Metric", dodge=True, s=3, ax=ax)  # order=sorter_list,
+        # ~ ax.set_xticklabels(sorter_names_short, rotation=30, ha='center')
+        # ~ ax.legend(bbox_to_anchor=(1.0, 1), loc=2, borderaxespad=0., frameon=False, fontsize=8, markerscale=0.5)
 
+        ax.set_ylim(0, 1.05)
+        ax.set_ylabel(f"Performance")
 
 
 class ComparisonPerformancesAveragesWidget(BaseWidget):
     """
     Plot run times for a study.
 
     Parameters
@@ -62,57 +64,60 @@
     gt_comparison: GroundTruthComparison
         The ground truth sorting comparison object
     ax: matplotlib ax
         The ax to be used. If not given a figure is created
     cmap_name
 
     """
-    def __init__(self, gt_comp, cmap_name='Set1',  ax=None):
+
+    def __init__(self, gt_comp, cmap_name="Set1", ax=None):
+        from matplotlib import pyplot as plt
+        import pandas as pd
 
         self.gt_comp = gt_comp
         self.cmap_name = cmap_name
 
         BaseWidget.__init__(self, ax=ax)
 
     def plot(self):
         import seaborn as sns
+
         ax = self.ax
 
         perf_by_units = self.gt_comp.get_performance()
         perf_by_units = perf_by_units.reset_index()
 
-        columns = ['accuracy','precision', 'recall']
+        columns = ["accuracy", "precision", "recall"]
         to_agg = {}
         ncol = len(columns)
 
         for column in columns:
-            perf_by_units[column] = pd.to_numeric(perf_by_units[column], downcast='float')
-            to_agg[column] = ['mean', 'std']
+            perf_by_units[column] = pd.to_numeric(perf_by_units[column], downcast="float")
+            to_agg[column] = ["mean", "std"]
 
         data = perf_by_units.agg(to_agg)
 
         m = data.mean()
 
         cmap = plt.get_cmap(self.cmap_name, 4)
 
         stds = data.std()
 
-        clean_labels = [col.replace('num_', '').replace('_', ' ').title() for col in columns]
+        clean_labels = [col.replace("num_", "").replace("_", " ").title() for col in columns]
 
-        width = 1/(ncol+2)
+        width = 1 / (ncol + 2)
 
         for c, col in enumerate(columns):
             x = 1 + c / (ncol + 2)
             yerr = stds[col]
             ax.bar(x, m[col], yerr=yerr, width=width, color=cmap(c), label=clean_labels[c])
 
         ax.legend()
-        ax.set_ylabel('metric')
-        #ax.set_xlim(0, 1)
-
+        ax.set_ylabel("metric")
+        # ax.set_xlim(0, 1)
 
 
 class ComparisonPerformancesByTemplateSimilarity(BaseWidget):
     """
     Plot run times for a study.
 
     Parameters
@@ -122,56 +127,66 @@
     similarity_matrix: matrix
         The similarity between the templates in the gt recording and the ones
         found by the sorter
     ax: matplotlib ax
         The ax to be used. If not given a figure is created
 
     """
+
     def __init__(self, gt_comp, similarity_matrix, ax=None, ylim=(0.6, 1)):
+        from matplotlib import pyplot as plt
+        import pandas as pd
 
         self.gt_comp = gt_comp
         self.similarity_matrix = similarity_matrix
         self.ylim = ylim
 
         BaseWidget.__init__(self, ax=ax)
 
     def plot(self):
-
-
-        all_results = {'similarity' : [], 'accuracy' : []}
+        all_results = {"similarity": [], "accuracy": []}
         comp = self.gt_comp
 
         for i, u1 in enumerate(comp.sorting1.unit_ids):
             u2 = comp.best_match_12[u1]
             if u2 != -1:
-                all_results['similarity'] += [self.similarity_matrix[comp.sorting1.id_to_index(u1), comp.sorting2.id_to_index(u2)]]
-                all_results['accuracy'] += [comp.agreement_scores.at[u1, u2]]
+                all_results["similarity"] += [
+                    self.similarity_matrix[comp.sorting1.id_to_index(u1), comp.sorting2.id_to_index(u2)]
+                ]
+                all_results["accuracy"] += [comp.agreement_scores.at[u1, u2]]
 
-        all_results['similarity'] = np.array(all_results['similarity'])
-        all_results['accuracy'] = np.array(all_results['accuracy'])
+        all_results["similarity"] = np.array(all_results["similarity"])
+        all_results["accuracy"] = np.array(all_results["accuracy"])
 
-        self.ax.plot(all_results['similarity'], all_results['accuracy'], '.')
+        self.ax.plot(all_results["similarity"], all_results["accuracy"], ".")
 
-        self.ax.set_ylabel('accuracy')
-        self.ax.set_xlabel('cosine similarity')
+        self.ax.set_ylabel("accuracy")
+        self.ax.set_xlabel("cosine similarity")
         if self.ylim is not None:
             self.ax.set_ylim(self.ylim)
 
 
 def plot_gt_performances(*args, **kwargs):
     W = ComparisonPerformancesWidget(*args, **kwargs)
     W.plot()
     return W
+
+
 plot_gt_performances.__doc__ = ComparisonPerformancesWidget.__doc__
 
+
 def plot_gt_performances_averages(*args, **kwargs):
     W = ComparisonPerformancesAveragesWidget(*args, **kwargs)
     W.plot()
     return W
+
+
 plot_gt_performances_averages.__doc__ = ComparisonPerformancesAveragesWidget.__doc__
 
 
 def plot_gt_performances_by_template_similarity(*args, **kwargs):
     W = ComparisonPerformancesByTemplateSimilarity(*args, **kwargs)
     W.plot()
     return W
+
+
 plot_gt_performances_by_template_similarity.__doc__ = ComparisonPerformancesByTemplateSimilarity.__doc__
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/gtstudy.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/gtstudy.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,19 +1,17 @@
 """
 Various widgets on top of GroundTruthStudy to summary results:
   * run times
   * performances
   * count units
 """
 import numpy as np
-from matplotlib import pyplot as plt
-import pandas as pd
 
-from .basewidget import BaseWidget
 
+from .basewidget import BaseWidget
 
 
 class StudyComparisonRunTimesWidget(BaseWidget):
     """
     Plot run times for a study.
 
     Parameters
@@ -22,49 +20,54 @@
         The study object to consider
     ax: matplotlib ax
         The ax to be used. If not given a figure is created
     color:
 
 
     """
-    def __init__(self, study, color='#F7DC6F',  ax=None):
+
+    def __init__(self, study, color="#F7DC6F", ax=None):
+        from matplotlib import pyplot as plt
+        import pandas as pd
 
         self.study = study
         self.color = color
 
         BaseWidget.__init__(self, ax=ax)
 
     def plot(self):
         study = self.study
         ax = self.ax
 
         all_run_times = study.aggregate_run_times()
-        av_run_times = all_run_times.reset_index().groupby('sorter_name')['run_time'].mean()
+        av_run_times = all_run_times.reset_index().groupby("sorter_name")["run_time"].mean()
 
         if len(study.rec_names) == 1:
             # no errors bars
             yerr = None
         else:
             # errors bars across recording
-            yerr = all_run_times.reset_index().groupby('sorter_name')['run_time'].std()
+            yerr = all_run_times.reset_index().groupby("sorter_name")["run_time"].std()
 
         sorter_names = av_run_times.index
 
         x = np.arange(sorter_names.size) + 1
         ax.bar(x, av_run_times.values, width=0.8, color=self.color, yerr=yerr)
-        ax.set_ylabel('run times (s)')
+        ax.set_ylabel("run times (s)")
         ax.set_xticks(x)
         ax.set_xticklabels(sorter_names, rotation=45)
         ax.set_xlim(0, sorter_names.size + 1)
 
 
 def plot_gt_study_run_times(*args, **kwargs):
     W = StudyComparisonRunTimesWidget(*args, **kwargs)
     W.plot()
     return W
+
+
 plot_gt_study_run_times.__doc__ = StudyComparisonRunTimesWidget.__doc__
 
 
 class StudyComparisonUnitCountsAveragesWidget(BaseWidget):
     """
     Plot averages over found units for a study.
 
@@ -74,71 +77,73 @@
         The study object to consider
     ax: matplotlib ax
         The ax to be used. If not given a figure is created
     cmap_name
     log_scale: if the y-axis should be displayed as log scaled
 
     """
-    def __init__(self, study, cmap_name='Set2', log_scale=False, ax=None):
+
+    def __init__(self, study, cmap_name="Set2", log_scale=False, ax=None):
+        from matplotlib import pyplot as plt
+        import pandas as pd
 
         self.study = study
         self.cmap_name = cmap_name
         self.log_scale = log_scale
 
         BaseWidget.__init__(self, ax=ax)
 
     def plot(self):
         study = self.study
         ax = self.ax
 
         count_units = study.aggregate_count_units()
 
         if study.exhaustive_gt:
-            columns = ['num_well_detected', 'num_false_positive',  'num_redundant', 'num_overmerged']
+            columns = ["num_well_detected", "num_false_positive", "num_redundant", "num_overmerged"]
         else:
-            columns = ['num_well_detected', 'num_redundant', 'num_overmerged']
+            columns = ["num_well_detected", "num_redundant", "num_overmerged"]
         ncol = len(columns)
 
         df = count_units.reset_index()
 
-        m = df.groupby('sorter_name')[columns].mean()
+        m = df.groupby("sorter_name")[columns].mean()
 
         cmap = plt.get_cmap(self.cmap_name, 4)
 
         if len(study.rec_names) == 1:
             # no errors bars
             stds = None
         else:
             # errors bars across recording
-            stds = df.groupby('sorter_name')[columns].std()
+            stds = df.groupby("sorter_name")[columns].std()
 
         sorter_names = m.index
-        clean_labels = [col.replace('num_', '').replace('_', ' ').title() for col in columns]
+        clean_labels = [col.replace("num_", "").replace("_", " ").title() for col in columns]
 
         for c, col in enumerate(columns):
             x = np.arange(sorter_names.size) + 1 + c / (ncol + 2)
             if stds is None:
                 yerr = None
             else:
                 yerr = stds[col].values
-            ax.bar(x, m[col].values, yerr=yerr, width=1/(ncol+2), color=cmap(c), label=clean_labels[c])
+            ax.bar(x, m[col].values, yerr=yerr, width=1 / (ncol + 2), color=cmap(c), label=clean_labels[c])
 
         ax.legend()
         if self.log_scale:
-            ax.set_yscale('log')
+            ax.set_yscale("log")
 
         ax.set_xticks(np.arange(sorter_names.size) + 1)
         ax.set_xticklabels(sorter_names, rotation=45)
-        ax.set_ylabel('# units')
+        ax.set_ylabel("# units")
         ax.set_xlim(0, sorter_names.size + 1)
 
-        if count_units['num_gt'].unique().size == 1:
-            num_gt = count_units['num_gt'].unique()[0]
-            ax.axhline(num_gt, ls='--', color='k')
-
+        if count_units["num_gt"].unique().size == 1:
+            num_gt = count_units["num_gt"].unique()[0]
+            ax.axhline(num_gt, ls="--", color="k")
 
 
 class StudyComparisonUnitCountsWidget(BaseWidget):
     """
     Plot averages over found units for a study.
 
     Parameters
@@ -147,15 +152,18 @@
         The study object to consider
     ax: matplotlib ax
         The ax to be used. If not given a figure is created
     cmap_name
     log_scale: if the y-axis should be displayed as log scaled
 
     """
-    def __init__(self, study, cmap_name='Set2',  log_scale=False, ax=None):
+
+    def __init__(self, study, cmap_name="Set2", log_scale=False, ax=None):
+        from matplotlib import pyplot as plt
+        import pandas as pd
 
         self.study = study
         self.cmap_name = cmap_name
         self.log_scale = log_scale
 
         num_rec = len(study.rec_names)
         if ax is None:
@@ -166,67 +174,72 @@
         BaseWidget.__init__(self, axes=axes)
 
     def plot(self):
         study = self.study
         ax = self.ax
 
         import seaborn as sns
+
         study = self.study
 
         count_units = study.aggregate_count_units()
         count_units = count_units.reset_index()
 
         if study.exhaustive_gt:
-            columns = ['num_well_detected', 'num_false_positive',  'num_redundant', 'num_overmerged']
+            columns = ["num_well_detected", "num_false_positive", "num_redundant", "num_overmerged"]
         else:
-            columns = ['num_well_detected', 'num_redundant', 'num_overmerged']
+            columns = ["num_well_detected", "num_redundant", "num_overmerged"]
 
         ncol = len(columns)
         cmap = plt.get_cmap(self.cmap_name, 4)
 
         for r, rec_name in enumerate(study.rec_names):
             ax = self.axes[r, 0]
             ax.set_title(rec_name)
-            df = count_units.loc[count_units['rec_name'] == rec_name, :]
-            m = df.groupby('sorter_name')[columns].mean()
+            df = count_units.loc[count_units["rec_name"] == rec_name, :]
+            m = df.groupby("sorter_name")[columns].mean()
             sorter_names = m.index
-            clean_labels = [col.replace('num_', '').replace('_', ' ').title() for col in columns]
+            clean_labels = [col.replace("num_", "").replace("_", " ").title() for col in columns]
 
             for c, col in enumerate(columns):
                 x = np.arange(sorter_names.size) + 1 + c / (ncol + 2)
-                ax.bar(x, m[col].values, width=1/(ncol+2), color=cmap(c), label=clean_labels[c])
-        
+                ax.bar(x, m[col].values, width=1 / (ncol + 2), color=cmap(c), label=clean_labels[c])
+
             if r == 0:
                 ax.legend()
 
             if self.log_scale:
-                ax.set_yscale('log')
-    
+                ax.set_yscale("log")
+
             if r == len(study.rec_names) - 1:
                 ax.set_xticks(np.arange(sorter_names.size) + 1)
                 ax.set_xticklabels(sorter_names, rotation=45)
-            ax.set_ylabel('# units')
+            ax.set_ylabel("# units")
             ax.set_xlim(0, sorter_names.size + 1)
 
-        if count_units['num_gt'].unique().size == 1:
-            num_gt = count_units['num_gt'].unique()[0]
-            ax.axhline(num_gt, ls='--', color='k')
+        if count_units["num_gt"].unique().size == 1:
+            num_gt = count_units["num_gt"].unique()[0]
+            ax.axhline(num_gt, ls="--", color="k")
 
 
 def plot_gt_study_unit_counts_averages(*args, **kwargs):
     W = StudyComparisonUnitCountsAveragesWidget(*args, **kwargs)
     W.plot()
     return W
+
+
 plot_gt_study_unit_counts_averages.__doc__ = StudyComparisonUnitCountsAveragesWidget.__doc__
 
 
 def plot_gt_study_unit_counts(*args, **kwargs):
     W = StudyComparisonUnitCountsWidget(*args, **kwargs)
     W.plot()
     return W
+
+
 plot_gt_study_unit_counts.__doc__ = StudyComparisonUnitCountsWidget.__doc__
 
 
 class StudyComparisonPerformancesWidget(BaseWidget):
     """
     Plot run times for a study.
 
@@ -235,125 +248,135 @@
     study: GroundTruthStudy
         The study object to consider
     ax: matplotlib ax
         The ax to be used. If not given a figure is created
     cmap_name
 
     """
-    def __init__(self, study, palette='Set1',  ax=None):
+
+    def __init__(self, study, palette="Set1", ax=None):
+        from matplotlib import pyplot as plt
+        import pandas as pd
 
         self.study = study
         self.palette = palette
 
         num_rec = len(study.rec_names)
         if ax is None:
             fig, axes = plt.subplots(ncols=1, nrows=num_rec, squeeze=False)
         else:
             axes = np.array([ax]).T
 
         BaseWidget.__init__(self, axes=axes)
 
     def plot(self, average=False):
         import seaborn as sns
-        study = self.study
 
+        study = self.study
 
         sns.set_palette(sns.color_palette(self.palette))
 
         perf_by_units = study.aggregate_performance_by_unit()
-        perf_by_units = perf_by_units.reset_index() 
+        perf_by_units = perf_by_units.reset_index()
 
         for r, rec_name in enumerate(study.rec_names):
             ax = self.axes[r, 0]
             ax.set_title(rec_name)
-            df = perf_by_units.loc[perf_by_units['rec_name'] == rec_name, :]
-            df = pd.melt(df, id_vars='sorter_name', var_name='Metric', value_name='Score',
-                    value_vars=('accuracy','precision', 'recall')).sort_values('sorter_name')
-            sns.swarmplot(data=df, x='sorter_name', y='Score', hue='Metric', dodge=True,
-                            s=3, ax=ax) # order=sorter_list,
-        #~ ax.set_xticklabels(sorter_names_short, rotation=30, ha='center')
-        #~ ax.legend(bbox_to_anchor=(1.0, 1), loc=2, borderaxespad=0., frameon=False, fontsize=8, markerscale=0.5)
+            df = perf_by_units.loc[perf_by_units["rec_name"] == rec_name, :]
+            df = pd.melt(
+                df,
+                id_vars="sorter_name",
+                var_name="Metric",
+                value_name="Score",
+                value_vars=("accuracy", "precision", "recall"),
+            ).sort_values("sorter_name")
+            sns.swarmplot(
+                data=df, x="sorter_name", y="Score", hue="Metric", dodge=True, s=3, ax=ax
+            )  # order=sorter_list,
+            # ~ ax.set_xticklabels(sorter_names_short, rotation=30, ha='center')
+            # ~ ax.legend(bbox_to_anchor=(1.0, 1), loc=2, borderaxespad=0., frameon=False, fontsize=8, markerscale=0.5)
 
             ax.set_ylim(0, 1.05)
-            ax.set_ylabel(f'Perfs for {rec_name}')
+            ax.set_ylabel(f"Perfs for {rec_name}")
             if r < len(study.rec_names) - 1:
-                ax.set_xlabel('')
+                ax.set_xlabel("")
                 ax.set(xticklabels=[])
 
 
-
-
-
 class StudyComparisonTemplateSimilarityWidget(BaseWidget):
     """
     Plot run times for a study.
 
     Parameters
     ----------
     study: GroundTruthStudy
         The study object to consider
     ax: matplotlib ax
         The ax to be used. If not given a figure is created
     cmap_name
 
     """
-    def __init__(self, study, cmap_name='Set1',  ax=None):
+
+    def __init__(self, study, cmap_name="Set1", ax=None):
+        from matplotlib import pyplot as plt
+        import pandas as pd
 
         self.study = study
         self.cmap_name = cmap_name
 
         BaseWidget.__init__(self, ax=ax)
 
     def plot(self):
         import seaborn as sns
+
         study = self.study
         ax = self.ax
 
         perf_by_units = study.aggregate_performance_by_unit()
         perf_by_units = perf_by_units.reset_index()
 
-        columns = ['accuracy','precision', 'recall']
+        columns = ["accuracy", "precision", "recall"]
         to_agg = {}
         ncol = len(columns)
 
         for column in columns:
-            perf_by_units[column] = pd.to_numeric(perf_by_units[column], downcast='float')
-            to_agg[column] = ['mean']
+            perf_by_units[column] = pd.to_numeric(perf_by_units[column], downcast="float")
+            to_agg[column] = ["mean"]
 
-        data = perf_by_units.groupby(['sorter_name', 'rec_name']).agg(to_agg)
+        data = perf_by_units.groupby(["sorter_name", "rec_name"]).agg(to_agg)
 
-        m = data.groupby('sorter_name').mean()
+        m = data.groupby("sorter_name").mean()
 
         cmap = plt.get_cmap(self.cmap_name, 4)
 
         if len(study.rec_names) == 1:
             # no errors bars
             stds = None
         else:
             # errors bars across recording
-            stds = data.groupby('sorter_name').std()
+            stds = data.groupby("sorter_name").std()
 
         sorter_names = m.index
-        clean_labels = [col.replace('num_', '').replace('_', ' ').title() for col in columns]
+        clean_labels = [col.replace("num_", "").replace("_", " ").title() for col in columns]
 
-        width = 1/(ncol+2)
+        width = 1 / (ncol + 2)
 
         for c, col in enumerate(columns):
             x = np.arange(sorter_names.size) + 1 + c / (ncol + 2)
             if stds is None:
                 yerr = None
             else:
                 yerr = stds[col].values
             ax.bar(x, m[col].values.flatten(), yerr=yerr.flatten(), width=width, color=cmap(c), label=clean_labels[c])
 
         ax.legend()
 
         ax.set_xticks(np.arange(sorter_names.size) + 1 + width)
         ax.set_xticklabels(sorter_names, rotation=45)
-        ax.set_ylabel('metric')
+        ax.set_ylabel("metric")
         ax.set_xlim(0, sorter_names.size + 1)
 
 
 class StudyComparisonPerformancesAveragesWidget(BaseWidget):
     """
     Plot run times for a study.
 
@@ -362,178 +385,190 @@
     study: GroundTruthStudy
         The study object to consider
     ax: matplotlib ax
         The ax to be used. If not given a figure is created
     cmap_name
 
     """
-    def __init__(self, study, cmap_name='Set1',  ax=None):
+
+    def __init__(self, study, cmap_name="Set1", ax=None):
+        from matplotlib import pyplot as plt
+        import pandas as pd
 
         self.study = study
         self.cmap_name = cmap_name
 
         BaseWidget.__init__(self, ax=ax)
 
     def plot(self):
         import seaborn as sns
+
         study = self.study
         ax = self.ax
 
         perf_by_units = study.aggregate_performance_by_unit()
         perf_by_units = perf_by_units.reset_index()
 
-        columns = ['accuracy','precision', 'recall']
+        columns = ["accuracy", "precision", "recall"]
         to_agg = {}
         ncol = len(columns)
 
         for column in columns:
-            perf_by_units[column] = pd.to_numeric(perf_by_units[column], downcast='float')
-            to_agg[column] = ['mean']
+            perf_by_units[column] = pd.to_numeric(perf_by_units[column], downcast="float")
+            to_agg[column] = ["mean"]
 
-        data = perf_by_units.groupby(['sorter_name', 'rec_name']).agg(to_agg)
+        data = perf_by_units.groupby(["sorter_name", "rec_name"]).agg(to_agg)
 
-        m = data.groupby('sorter_name').mean()
+        m = data.groupby("sorter_name").mean()
 
         cmap = plt.get_cmap(self.cmap_name, 4)
 
         if len(study.rec_names) == 1:
             # no errors bars
             stds = None
         else:
             # errors bars across recording
-            stds = data.groupby('sorter_name').std()
+            stds = data.groupby("sorter_name").std()
 
         sorter_names = m.index
-        clean_labels = [col.replace('num_', '').replace('_', ' ').title() for col in columns]
+        clean_labels = [col.replace("num_", "").replace("_", " ").title() for col in columns]
 
-        width = 1/(ncol+2)
+        width = 1 / (ncol + 2)
 
         for c, col in enumerate(columns):
             x = np.arange(sorter_names.size) + 1 + c / (ncol + 2)
             if stds is None:
                 yerr = None
             else:
                 yerr = stds[col].values
             ax.bar(x, m[col].values.flatten(), yerr=yerr.flatten(), width=width, color=cmap(c), label=clean_labels[c])
 
         ax.legend()
 
         ax.set_xticks(np.arange(sorter_names.size) + 1 + width)
         ax.set_xticklabels(sorter_names, rotation=45)
-        ax.set_ylabel('metric')
+        ax.set_ylabel("metric")
         ax.set_xlim(0, sorter_names.size + 1)
 
 
-
 class StudyComparisonPerformancesByTemplateSimilarity(BaseWidget):
     """
     Plot run times for a study.
 
     Parameters
     ----------
     study: GroundTruthStudy
         The study object to consider
     ax: matplotlib ax
         The ax to be used. If not given a figure is created
     cmap_name
 
     """
-    def __init__(self, study, cmap_name='Set1', ax=None, ylim=(0.6, 1), show_legend=True):
+
+    def __init__(self, study, cmap_name="Set1", ax=None, ylim=(0.6, 1), show_legend=True):
+        from matplotlib import pyplot as plt
+        import pandas as pd
 
         self.study = study
         self.cmap_name = cmap_name
         self.show_legend = show_legend
         self.ylim = ylim
 
         BaseWidget.__init__(self, ax=ax)
 
     def plot(self):
-
         import sklearn
 
         cmap = plt.get_cmap(self.cmap_name, len(self.study.sorter_names))
         colors = [cmap(i) for i in range(len(self.study.sorter_names))]
 
         flat_templates_gt = {}
         for rec_name in self.study.rec_names:
-
-            waveform_folder = self.study.study_folder / 'waveforms' / f'waveforms_GroundTruth_{rec_name}'
+            waveform_folder = self.study.study_folder / "waveforms" / f"waveforms_GroundTruth_{rec_name}"
             if not waveform_folder.is_dir():
                 self.study.compute_waveforms(rec_name)
 
             templates = self.study.get_templates(rec_name)
             flat_templates_gt[rec_name] = templates.reshape(templates.shape[0], -1)
 
         all_results = {}
 
         for sorter_name in self.study.sorter_names:
-
-            all_results[sorter_name] = {'similarity' : [], 'accuracy' : []}
+            all_results[sorter_name] = {"similarity": [], "accuracy": []}
 
             for rec_name in self.study.rec_names:
-
                 try:
-                    waveform_folder = self.study.study_folder / 'waveforms' / f'waveforms_{sorter_name}_{rec_name}'
+                    waveform_folder = self.study.study_folder / "waveforms" / f"waveforms_{sorter_name}_{rec_name}"
                     if not waveform_folder.is_dir():
                         self.study.compute_waveforms(rec_name, sorter_name)
                     templates = self.study.get_templates(rec_name, sorter_name)
                     flat_templates = templates.reshape(templates.shape[0], -1)
-                    similarity_matrix = sklearn.metrics.pairwise.cosine_similarity(flat_templates_gt[rec_name], flat_templates)
+                    similarity_matrix = sklearn.metrics.pairwise.cosine_similarity(
+                        flat_templates_gt[rec_name], flat_templates
+                    )
 
                     comp = self.study.comparisons[(rec_name, sorter_name)]
 
                     for i, u1 in enumerate(comp.sorting1.unit_ids):
                         u2 = comp.best_match_12[u1]
                         if u2 != -1:
-                            all_results[sorter_name]['similarity'] += [similarity_matrix[comp.sorting1.id_to_index(u1), comp.sorting2.id_to_index(u2)]]
-                            all_results[sorter_name]['accuracy'] += [comp.agreement_scores.at[u1, u2]]
+                            all_results[sorter_name]["similarity"] += [
+                                similarity_matrix[comp.sorting1.id_to_index(u1), comp.sorting2.id_to_index(u2)]
+                            ]
+                            all_results[sorter_name]["accuracy"] += [comp.agreement_scores.at[u1, u2]]
                 except Exception:
                     pass
 
-            all_results[sorter_name]['similarity'] = np.array(all_results[sorter_name]['similarity'])
-            all_results[sorter_name]['accuracy'] = np.array(all_results[sorter_name]['accuracy'])
+            all_results[sorter_name]["similarity"] = np.array(all_results[sorter_name]["similarity"])
+            all_results[sorter_name]["accuracy"] = np.array(all_results[sorter_name]["accuracy"])
 
         from matplotlib.patches import Ellipse
 
-        similarity_means = [all_results[sorter_name]['similarity'].mean() for sorter_name in self.study.sorter_names]
-        similarity_stds = [all_results[sorter_name]['similarity'].std() for sorter_name in self.study.sorter_names]
+        similarity_means = [all_results[sorter_name]["similarity"].mean() for sorter_name in self.study.sorter_names]
+        similarity_stds = [all_results[sorter_name]["similarity"].std() for sorter_name in self.study.sorter_names]
 
-        accuracy_means = [all_results[sorter_name]['accuracy'].mean() for sorter_name in self.study.sorter_names]
-        accuracy_stds = [all_results[sorter_name]['accuracy'].std() for sorter_name in self.study.sorter_names]
+        accuracy_means = [all_results[sorter_name]["accuracy"].mean() for sorter_name in self.study.sorter_names]
+        accuracy_stds = [all_results[sorter_name]["accuracy"].std() for sorter_name in self.study.sorter_names]
 
         scount = 0
-        for x,y, i,j in zip(similarity_means, accuracy_means, similarity_stds, accuracy_stds):
-            e = Ellipse((x,y), i, j)
+        for x, y, i, j in zip(similarity_means, accuracy_means, similarity_stds, accuracy_stds):
+            e = Ellipse((x, y), i, j)
             e.set_alpha(0.2)
             e.set_facecolor(colors[scount])
             self.ax.add_artist(e)
             self.ax.scatter([x], [y], c=colors[scount], label=self.study.sorter_names[scount])
             scount += 1
 
-        self.ax.set_ylabel('accuracy')
-        self.ax.set_xlabel('cosine similarity')
+        self.ax.set_ylabel("accuracy")
+        self.ax.set_xlabel("cosine similarity")
         if self.ylim is not None:
             self.ax.set_ylim(self.ylim)
 
         if self.show_legend:
             self.ax.legend()
 
 
-
 def plot_gt_study_performances(*args, **kwargs):
     W = StudyComparisonPerformancesWidget(*args, **kwargs)
     W.plot()
     return W
+
+
 plot_gt_study_performances.__doc__ = StudyComparisonPerformancesWidget.__doc__
 
+
 def plot_gt_study_performances_averages(*args, **kwargs):
     W = StudyComparisonPerformancesAveragesWidget(*args, **kwargs)
     W.plot()
     return W
+
+
 plot_gt_study_performances_averages.__doc__ = StudyComparisonPerformancesAveragesWidget.__doc__
 
 
 def plot_gt_study_performances_by_template_similarity(*args, **kwargs):
     W = StudyComparisonPerformancesByTemplateSimilarity(*args, **kwargs)
     W.plot()
     return W
+
+
 plot_gt_study_performances_by_template_similarity.__doc__ = StudyComparisonPerformancesByTemplateSimilarity.__doc__
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/isidistribution.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/isidistribution.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,8 @@
 import numpy as np
-from matplotlib import pyplot as plt
 from .basewidget import BaseWidget
 
 
 class ISIDistributionWidget(BaseWidget):
     """
     Plots spike train ISI distribution.
 
@@ -25,26 +24,26 @@
 
     Returns
     -------
     W: ISIDistributionWidget
         The output widget
     """
 
-    def __init__(self, sorting, unit_ids=None, window_ms=100.0, bin_ms=1.0,
-                 ncols=5, axes=None):
+    def __init__(self, sorting, unit_ids=None, window_ms=100.0, bin_ms=1.0, ncols=5, axes=None):
+        from matplotlib import pyplot as plt
 
         self._sorting = sorting
         if unit_ids is None:
             unit_ids = sorting.get_unit_ids()
         self._unit_ids = unit_ids
 
         self._sampling_frequency = sorting.get_sampling_frequency()
         self.window_ms = window_ms
         self.bin_ms = bin_ms
-        self.name = 'ISIDistribution'
+        self.name = "ISIDistribution"
 
         if axes is None:
             num_axes = len(unit_ids)
         else:
             num_axes = None
         BaseWidget.__init__(self, None, None, axes, ncols=ncols, num_axes=num_axes)
 
@@ -60,31 +59,34 @@
         num_ax = 0
         for i, unit_id in enumerate(unit_ids):
             ax = self.axes.flatten()[i]
 
             bins = np.arange(0, self.window_ms, self.bin_ms)
             bin_counts = None
             for segment_index in range(num_seg):
-                #~ ax = self.get_tiled_ax(num_ax, nrows, ncols)
-                times_ms = self._sorting.get_unit_spike_train(unit_id=unit_id, segment_index=segment_index) \
-                           / float(self._sampling_frequency) * 1000.
+                # ~ ax = self.get_tiled_ax(num_ax, nrows, ncols)
+                times_ms = (
+                    self._sorting.get_unit_spike_train(unit_id=unit_id, segment_index=segment_index)
+                    / float(self._sampling_frequency)
+                    * 1000.0
+                )
                 #  bin_counts, bin_edges = compute_isi_dist(times, bins=self._bins, maxwindow=self._window)
                 isi = np.diff(times_ms)
 
                 bin_counts_, bin_edges = np.histogram(isi, bins=bins, density=True)
                 if segment_index == 0:
                     bin_counts = bin_counts_
                 else:
                     bin_counts += bin_counts_
                     # TODO handle sensity when several segments
 
-            ax.bar(x=bin_edges[:-1], height=bin_counts, width=self.bin_ms, color='gray', align='edge')
+            ax.bar(x=bin_edges[:-1], height=bin_counts, width=self.bin_ms, color="gray", align="edge")
 
             if segment_index == 0:
-                ax.set_ylabel(f'{unit_id}')
+                ax.set_ylabel(f"{unit_id}")
 
 
 def plot_isi_distribution(*args, **kwargs):
     W = ISIDistributionWidget(*args, **kwargs)
     W.plot()
     return W
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/multicompgraph.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/multicompgraph.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,8 @@
 import numpy as np
-import matplotlib
-from matplotlib import pyplot as plt
 
 from .basewidget import BaseWidget
 
 
 class MultiCompGraphWidget(BaseWidget):
     """
     Plots multi comparison graph.
@@ -30,68 +28,97 @@
 
     Returns
     -------
     W: MultiCompGraphWidget
         The output widget
     """
 
-    def __init__(self, multi_comparison, draw_labels=False, node_cmap='viridis',
-                 edge_cmap='hot', alpha_edges=0.5, colorbar=False, figure=None, ax=None):
+    def __init__(
+        self,
+        multi_comparison,
+        draw_labels=False,
+        node_cmap="viridis",
+        edge_cmap="hot",
+        alpha_edges=0.5,
+        colorbar=False,
+        figure=None,
+        ax=None,
+    ):
+        import matplotlib
+        from matplotlib import pyplot as plt
+
         BaseWidget.__init__(self, figure, ax)
         self._msc = multi_comparison
         self._draw_labels = draw_labels
         self._node_cmap = node_cmap
         self._edge_cmap = edge_cmap
         self._colorbar = colorbar
         self._alpha_edges = alpha_edges
-        self.name = 'MultiCompGraph'
+        self.name = "MultiCompGraph"
 
     def plot(self):
         self._do_plot()
 
     def _do_plot(self):
         import networkx as nx
 
         g = self._msc.graph
         edge_col = []
         for e in g.edges(data=True):
             n1, n2, d = e
-            edge_col.append(d['weight'])
+            edge_col.append(d["weight"])
         nodes_col_dict = {}
         for i, sort_name in enumerate(self._msc.name_list):
             nodes_col_dict[sort_name] = i
         nodes_col = []
         for node in sorted(g.nodes):
             nodes_col.append(nodes_col_dict[node[0]])
         nodes_col = np.array(nodes_col) / len(self._msc.name_list)
+        import matplotlib.pyplot as plt
 
         _ = plt.set_cmap(self._node_cmap)
-        _ = nx.draw_networkx_nodes(g, pos=nx.circular_layout(sorted(g)), nodelist=sorted(g.nodes),
-                                   node_color=nodes_col, node_size=20, ax=self.ax)
-        _ = nx.draw_networkx_edges(g, pos=nx.circular_layout((sorted(g))), nodelist=sorted(g.nodes),
-                                   edge_color=edge_col, alpha=self._alpha_edges,
-                                   edge_cmap=plt.cm.get_cmap(self._edge_cmap), edge_vmin=self._msc.match_score,
-                                   edge_vmax=1, ax=self.ax)
+        _ = nx.draw_networkx_nodes(
+            g,
+            pos=nx.circular_layout(sorted(g)),
+            nodelist=sorted(g.nodes),
+            node_color=nodes_col,
+            node_size=20,
+            ax=self.ax,
+        )
+        _ = nx.draw_networkx_edges(
+            g,
+            pos=nx.circular_layout((sorted(g))),
+            nodelist=sorted(g.nodes),
+            edge_color=edge_col,
+            alpha=self._alpha_edges,
+            edge_cmap=plt.cm.get_cmap(self._edge_cmap),
+            edge_vmin=self._msc.match_score,
+            edge_vmax=1,
+            ax=self.ax,
+        )
         if self._draw_labels:
             labels = {key: f"{key[0]}_{key[1]}" for key in sorted(g.nodes)}
             pos = nx.circular_layout(sorted(g))
             # extend position radially
             pos_extended = {}
             for node, pos in pos.items():
                 pos_new = pos + 0.1 * pos
                 pos_extended[node] = pos_new
             _ = nx.draw_networkx_labels(g, pos=pos_extended, labels=labels, ax=self.ax)
 
         if self._colorbar:
+            import matplotlib
+            import matplotlib.pyplot as plt
+
             norm = matplotlib.colors.Normalize(vmin=self._msc.match_score, vmax=1)
             cmap = plt.cm.get_cmap(self._edge_cmap)
             m = plt.cm.ScalarMappable(norm=norm, cmap=cmap)
             self.figure.colorbar(m)
 
-        self.ax.axis('off')
+        self.ax.axis("off")
 
 
 class MultiCompGlobalAgreementWidget(BaseWidget):
     """
     Plots multi comparison agreement as pie or bar plot.
 
     Parameters
@@ -109,47 +136,61 @@
 
     Returns
     -------
     W: MultiCompGraphWidget
         The output widget
     """
 
-    def __init__(self, multi_comparison, plot_type='pie', cmap='YlOrRd', fs=10,
-                 figure=None, ax=None):
+    def __init__(self, multi_comparison, plot_type="pie", cmap="YlOrRd", fs=10, figure=None, ax=None):
         BaseWidget.__init__(self, figure, ax)
+        import matplotlib
+        import matplotlib.pyplot as plt
+
         self._msc = multi_comparison
         self._type = plot_type
         self._cmap = cmap
         self._fs = fs
-        self.name = 'MultiCompGlobalAgreement'
+        self.name = "MultiCompGlobalAgreement"
 
     def plot(self):
         self._do_plot()
 
     def _do_plot(self):
+        import matplotlib.pyplot as plt
+
         cmap = plt.get_cmap(self._cmap)
         colors = np.array([cmap(i) for i in np.linspace(0.1, 0.8, len(self._msc.name_list))])
         sg_names, sg_units = self._msc.compute_subgraphs()
         # fraction of units with agreement > threshold
         v, c = np.unique([len(np.unique(s)) for s in sg_names], return_counts=True)
-        if self._type == 'pie':
-            p = self.ax.pie(c, colors=colors[v - 1], autopct=lambda pct: _getabs(pct, c),
-                            pctdistance=1.25)
-            self.ax.legend(p[0], v, frameon=False, title='k=', handlelength=1, handletextpad=0.5,
-                           bbox_to_anchor=(1., 1.), loc=2, borderaxespad=0.5, labelspacing=0.15, fontsize=self._fs)
-        elif self._type == 'bar':
+        if self._type == "pie":
+            p = self.ax.pie(c, colors=colors[v - 1], autopct=lambda pct: _getabs(pct, c), pctdistance=1.25)
+            self.ax.legend(
+                p[0],
+                v,
+                frameon=False,
+                title="k=",
+                handlelength=1,
+                handletextpad=0.5,
+                bbox_to_anchor=(1.0, 1.0),
+                loc=2,
+                borderaxespad=0.5,
+                labelspacing=0.15,
+                fontsize=self._fs,
+            )
+        elif self._type == "bar":
             self.ax.bar(v, c, color=colors[v - 1])
-            x_labels = [f'k={vi}' for vi in v]
-            self.ax.spines['top'].set_visible(False)
-            self.ax.spines['right'].set_visible(False)
+            x_labels = [f"k={vi}" for vi in v]
+            self.ax.spines["top"].set_visible(False)
+            self.ax.spines["right"].set_visible(False)
             self.ax.set_xticks(v)
             self.ax.set_xticklabels(x_labels)
         else:
             raise AttributeError("Wrong plot_type. It can be 'pie' or 'bar'")
-        self.ax.set_title('Units agreed upon\nby k sorters')
+        self.ax.set_title("Units agreed upon\nby k sorters")
 
 
 class MultiCompAgreementBySorterWidget(BaseWidget):
     """
     Plots multi comparison agreement as pie or bar plot.
 
     Parameters
@@ -168,65 +209,83 @@
 
     Returns
     -------
     W: MultiCompGraphWidget
         The output widget
     """
 
-    def __init__(self, multi_comparison, plot_type='pie', cmap='YlOrRd', fs=9,
-                 axes=None, show_legend=True):
+    def __init__(self, multi_comparison, plot_type="pie", cmap="YlOrRd", fs=9, axes=None, show_legend=True):
+        import matplotlib.pyplot as plt
+
         self._msc = multi_comparison
         self._type = plot_type
         self._cmap = cmap
         self._fs = fs
         self._show_legend = show_legend
-        self.name = 'MultiCompAgreementBySorterWidget'
+        self.name = "MultiCompAgreementBySorterWidget"
 
         if axes is None:
             ncols = len(self._msc.name_list)
             fig, axes = plt.subplots(nrows=1, ncols=ncols, sharex=True, sharey=True)
         BaseWidget.__init__(self, None, None, axes)
 
     def plot(self):
         self._do_plot()
 
     def _do_plot(self):
         name_list = self._msc.name_list
+        import matplotlib.pyplot as plt
+
         cmap = plt.get_cmap(self._cmap)
         colors = np.array([cmap(i) for i in np.linspace(0.1, 0.8, len(self._msc.name_list))])
         sg_names, sg_units = self._msc.compute_subgraphs()
         # fraction of units with agreement > threshold
         for i, name in enumerate(name_list):
             ax = self.axes[i]
             v, c = np.unique([len(np.unique(sn)) for sn in sg_names if name in sn], return_counts=True)
-            if self._type == 'pie':
-                p = ax.pie(c, colors=colors[v - 1], textprops={'color': 'k', 'fontsize': self._fs},
-                           autopct=lambda pct: _getabs(pct, c), pctdistance=1.18)
+            if self._type == "pie":
+                p = ax.pie(
+                    c,
+                    colors=colors[v - 1],
+                    textprops={"color": "k", "fontsize": self._fs},
+                    autopct=lambda pct: _getabs(pct, c),
+                    pctdistance=1.18,
+                )
                 if (self._show_legend) and (i == len(name_list) - 1):
-                    plt.legend(p[0], v, frameon=False, title='k=', handlelength=1, handletextpad=0.5,
-                               bbox_to_anchor=(1.15, 1.25), loc=2, borderaxespad=0., labelspacing=0.15)
-            elif self._type == 'bar':
+                    plt.legend(
+                        p[0],
+                        v,
+                        frameon=False,
+                        title="k=",
+                        handlelength=1,
+                        handletextpad=0.5,
+                        bbox_to_anchor=(1.15, 1.25),
+                        loc=2,
+                        borderaxespad=0.0,
+                        labelspacing=0.15,
+                    )
+            elif self._type == "bar":
                 ax.bar(v, c, color=colors[v - 1])
-                x_labels = [f'k={vi}' for vi in v]
-                ax.spines['top'].set_visible(False)
-                ax.spines['right'].set_visible(False)
+                x_labels = [f"k={vi}" for vi in v]
+                ax.spines["top"].set_visible(False)
+                ax.spines["right"].set_visible(False)
                 ax.set_xticks(v)
                 ax.set_xticklabels(x_labels)
             else:
                 raise AttributeError("Wrong plot_type. It can be 'pie' or 'bar'")
             ax.set_title(name)
-        if self._type == 'bar':
+        if self._type == "bar":
             ylims = [np.max(ax_single.get_ylim()) for ax_single in self.axes]
             max_yval = np.max(ylims)
             for ax_single in self.axes:
                 ax_single.set_ylim([0, max_yval])
 
 
 def _getabs(pct, allvals):
-    absolute = int(np.round(pct / 100. * np.sum(allvals)))
+    absolute = int(np.round(pct / 100.0 * np.sum(allvals)))
     return "{:d}".format(absolute)
 
 
 def plot_multicomp_graph(*args, **kwargs):
     W = MultiCompGraphWidget(*args, **kwargs)
     W.plot()
     return W
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/presence.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/presence.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,12 +1,10 @@
 import numpy as np
-from matplotlib import pyplot as plt
 
 from .basewidget import BaseWidget
-from scipy.stats import gaussian_kde
 
 
 class PresenceWidget(BaseWidget):
     """
     Estimates of the probability density function for each unit using Gaussian kernels,
 
     Parameters
@@ -28,102 +26,101 @@
 
     Returns
     -------
     W: PresenceWidget
         The output widget
     """
 
-    def __init__(self, sorting, segment_index=None, unit_ids=None,
-                 time_range=None, figure=None, time_pixels=200, ax=None):
+    def __init__(
+        self, sorting, segment_index=None, unit_ids=None, time_range=None, figure=None, time_pixels=200, ax=None
+    ):
         BaseWidget.__init__(self, figure, ax)
         self._sorting = sorting
         self._time_pixels = time_pixels
         if segment_index is None:
             nseg = sorting.get_num_segments()
             if nseg != 1:
-                raise ValueError('You must provide segment_index=...')
+                raise ValueError("You must provide segment_index=...")
             else:
                 segment_index = 0
         self.segment_index = segment_index
         self._unit_ids = unit_ids
         self._figure = None
         self._sampling_frequency = sorting.get_sampling_frequency()
         self._max_frame = 0
         for unit_id in self._sorting.get_unit_ids():
-            spike_train = self._sorting.get_unit_spike_train(unit_id,
-                                                             segment_index=self.segment_index)
+            spike_train = self._sorting.get_unit_spike_train(unit_id, segment_index=self.segment_index)
             if len(spike_train) > 0:
                 curr_max_frame = np.max(spike_train)
                 if curr_max_frame > self._max_frame:
                     self._max_frame = curr_max_frame
         self._visible_trange = time_range
         if self._visible_trange is None:
             self._visible_trange = [0, self._max_frame]
         else:
-            assert len(
-                time_range) == 2, "'time_range' should be a list with start and end time in seconds"
-            self._visible_trange = [
-                int(t * self._sampling_frequency) for t in time_range]
+            assert len(time_range) == 2, "'time_range' should be a list with start and end time in seconds"
+            self._visible_trange = [int(t * self._sampling_frequency) for t in time_range]
 
         self._visible_trange = self._fix_trange(self._visible_trange)
-        self.name = 'Presence'
+        self.name = "Presence"
 
     def plot(self):
         self._do_plot()
 
     def _do_plot(self):
+        import matplotlib.pyplot as plt
+        from scipy.stats import gaussian_kde
+
         units_ids = self._unit_ids
         if units_ids is None:
             units_ids = self._sorting.get_unit_ids()
-        visible_start_frame = self._visible_trange[0] / \
-            self._sampling_frequency
+        visible_start_frame = self._visible_trange[0] / self._sampling_frequency
         visible_end_frame = self._visible_trange[1] / self._sampling_frequency
 
-        time_grid = np.linspace(visible_start_frame,
-                                visible_end_frame, self._time_pixels)
+        time_grid = np.linspace(visible_start_frame, visible_end_frame, self._time_pixels)
         time_den = []
 
-        self.ax.grid('both')
+        self.ax.grid("both")
         for u_i, unit_id in enumerate(units_ids):
-            spiketrain = self._sorting.get_unit_spike_train(unit_id,
-                                                            start_frame=self._visible_trange[0],
-                                                            end_frame=self._visible_trange[1],
-                                                            segment_index=self.segment_index)
+            spiketrain = self._sorting.get_unit_spike_train(
+                unit_id,
+                start_frame=self._visible_trange[0],
+                end_frame=self._visible_trange[1],
+                segment_index=self.segment_index,
+            )
             spiketimes = spiketrain / float(self._sampling_frequency)
 
             if spiketimes[0] != spiketimes[-1]:  # not always the same value
                 time_den.append(gaussian_kde(spiketimes).pdf(time_grid))
             else:
                 aux = np.zeros_like(time_grid)
                 aux[np.argmin(np.abs(time_grid - spiketimes))] = 1
                 time_den.append(aux)
 
-        self.ax.matshow(np.vstack(time_den),
-                        cmap=plt.cm.inferno, aspect='auto')
+        self.ax.matshow(np.vstack(time_den), cmap=plt.cm.inferno, aspect="auto")
 
-        self.ax.hlines(np.arange(len(units_ids)) + 0.5, 0,
-                       len(time_den[0]), color='k', linewidth=4)
+        self.ax.hlines(np.arange(len(units_ids)) + 0.5, 0, len(time_den[0]), color="k", linewidth=4)
 
-        self.ax.tick_params(axis='y', which='both', grid_linestyle='None')
+        self.ax.tick_params(axis="y", which="both", grid_linestyle="None")
 
         self.ax.set_xlim(0, self._time_pixels)
         new_labels = []
-        self.ax.xaxis.set_ticks_position('bottom')
+        self.ax.xaxis.set_ticks_position("bottom")
 
         for xt in self.ax.get_xticks():
             if xt < self._time_pixels:
-                new_labels.append('{:.1f}'.format(time_grid[int(xt)]))
+                new_labels.append("{:.1f}".format(time_grid[int(xt)]))
             else:
-                new_labels.append('{:.1f}'.format(visible_end_frame))
+                new_labels.append("{:.1f}".format(visible_end_frame))
         self.ax.set_xticks(self.ax.get_xticks())
         self.ax.set_xticklabels(new_labels)
         self.ax.set_yticks(np.arange(len(units_ids)))
         self.ax.set_yticklabels(units_ids)
-        self.ax.set_xlabel('time (s)')
-        self.ax.set_ylabel('Unit ID')
+        self.ax.set_xlabel("time (s)")
+        self.ax.set_ylabel("Unit ID")
 
     def _fix_trange(self, trange):
         if trange[1] > self._max_frame:
             trange[1] = self._max_frame
         if trange[0] < 0:
             trange[0] = 0
         return trange
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/principalcomponent.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/principalcomponent.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,41 +1,39 @@
 import numpy as np
-from matplotlib import pyplot as plt
 
 from .basewidget import BaseWidget
 from ...postprocessing import WaveformPrincipalComponent, compute_principal_components
 
 
 class PrincipalComponentWidget(BaseWidget):
     """
     Plots principal component.
 
     Parameters
     ----------
     waveform_extractor: WaveformExtractor
-    
+
     pc: None or WaveformPrincipalComponent
         If None then pc are recomputed
     """
 
-    def __init__(self, waveform_extractor, pc=None,
-                 figure=None, ax=None, axes=None, **pc_kwargs):
+    def __init__(self, waveform_extractor, pc=None, figure=None, ax=None, axes=None, **pc_kwargs):
         BaseWidget.__init__(self, figure, ax, axes)
 
         self.we = waveform_extractor
 
         if pc is not None:
             # amplitudes must be a list of dict
             assert isinstance(pc, WaveformPrincipalComponent)
             self.pc = pc
         else:
             self.pc = compute_principal_components(self.we, **pc_kwargs)
 
     def plot(self):
-        print('Not done yet')
+        print("Not done yet")
         # @alessio : this is for you
 
 
 def plot_principal_component(*args, **kwargs):
     W = PrincipalComponentWidget(*args, **kwargs)
     W.plot()
     return W
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/rasters.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/rasters.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,9 +1,8 @@
 import numpy as np
-from matplotlib import pyplot as plt
 
 from .basewidget import BaseWidget
 
 
 class RasterWidget(BaseWidget):
     """
     Plots spike train rasters.
@@ -27,73 +26,82 @@
 
     Returns
     -------
     W: RasterWidget
         The output widget
     """
 
-    def __init__(self, sorting, segment_index=None, unit_ids=None,
-                 time_range=None, color='k', figure=None, ax=None):
+    def __init__(self, sorting, segment_index=None, unit_ids=None, time_range=None, color="k", figure=None, ax=None):
+        from matplotlib import pyplot as plt
+
         BaseWidget.__init__(self, figure, ax)
         self._sorting = sorting
 
         if segment_index is None:
             nseg = sorting.get_num_segments()
             if nseg != 1:
-                raise ValueError('You must provide segment_index=...')
+                raise ValueError("You must provide segment_index=...")
             else:
                 segment_index = 0
         self.segment_index = segment_index
 
         self._unit_ids = unit_ids
         self._figure = None
         self._sampling_frequency = sorting.get_sampling_frequency()
         self._color = color
         self._max_frame = 0
         for unit_id in self._sorting.get_unit_ids():
-            spike_train = self._sorting.get_unit_spike_train(unit_id,
-                                                             segment_index=self.segment_index)
+            spike_train = self._sorting.get_unit_spike_train(unit_id, segment_index=self.segment_index)
             if len(spike_train) > 0:
                 curr_max_frame = np.max(spike_train)
                 if curr_max_frame > self._max_frame:
                     self._max_frame = curr_max_frame
         self._visible_trange = time_range
         if self._visible_trange is None:
             self._visible_trange = [0, self._max_frame]
         else:
             assert len(time_range) == 2, "'time_range' should be a list with start and end time in seconds"
             self._visible_trange = [int(t * self._sampling_frequency) for t in time_range]
 
         self._visible_trange = self._fix_trange(self._visible_trange)
-        self.name = 'Raster'
+        self.name = "Raster"
 
     def plot(self):
         self._do_plot()
 
     def _do_plot(self):
         units_ids = self._unit_ids
         if units_ids is None:
             units_ids = self._sorting.get_unit_ids()
+        import matplotlib.pyplot as plt
 
-        with plt.rc_context({'axes.edgecolor': 'gray'}):
+        with plt.rc_context({"axes.edgecolor": "gray"}):
             for u_i, unit_id in enumerate(units_ids):
-                spiketrain = self._sorting.get_unit_spike_train(unit_id,
-                                                                start_frame=self._visible_trange[0],
-                                                                end_frame=self._visible_trange[1],
-                                                                segment_index=self.segment_index)
+                spiketrain = self._sorting.get_unit_spike_train(
+                    unit_id,
+                    start_frame=self._visible_trange[0],
+                    end_frame=self._visible_trange[1],
+                    segment_index=self.segment_index,
+                )
                 spiketimes = spiketrain / float(self._sampling_frequency)
-                self.ax.plot(spiketimes, u_i * np.ones_like(spiketimes),
-                             marker='|', mew=1, markersize=3,
-                             ls='', color=self._color)
+                self.ax.plot(
+                    spiketimes,
+                    u_i * np.ones_like(spiketimes),
+                    marker="|",
+                    mew=1,
+                    markersize=3,
+                    ls="",
+                    color=self._color,
+                )
             visible_start_frame = self._visible_trange[0] / self._sampling_frequency
             visible_end_frame = self._visible_trange[1] / self._sampling_frequency
             self.ax.set_yticks(np.arange(len(units_ids)))
             self.ax.set_yticklabels(units_ids)
             self.ax.set_xlim(visible_start_frame, visible_end_frame)
-            self.ax.set_xlabel('time (s)')
+            self.ax.set_xlabel("time (s)")
 
     def _fix_trange(self, trange):
         if trange[1] > self._max_frame:
             # trange[0] += max_t - trange[1]
             trange[1] = self._max_frame
         if trange[0] < 0:
             # trange[1] += -trange[0]
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/sortingperformance.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/sortingperformance.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,9 +1,8 @@
 import numpy as np
-from matplotlib import pyplot as plt
 
 from .basewidget import BaseWidget
 
 from ...comparison import GroundTruthComparison
 
 
 class SortingPerformanceWidget(BaseWidget):
@@ -30,19 +29,31 @@
 
     Returns
     -------
     W: SortingPerformanceWidget
         The output widget
     """
 
-    def __init__(self, sorting_comparison, metrics,
-                 performance_name='accuracy', metric_name='snr',
-                 color='b', markersize=10, marker='.', figure=None, ax=None):
-        assert isinstance(sorting_comparison, GroundTruthComparison), \
-            "The 'sorting_comparison' object should be a GroundTruthComparison instance"
+    def __init__(
+        self,
+        sorting_comparison,
+        metrics,
+        performance_name="accuracy",
+        metric_name="snr",
+        color="b",
+        markersize=10,
+        marker=".",
+        figure=None,
+        ax=None,
+    ):
+        from matplotlib import pyplot as plt
+
+        assert isinstance(
+            sorting_comparison, GroundTruthComparison
+        ), "The 'sorting_comparison' object should be a GroundTruthComparison instance"
         BaseWidget.__init__(self, figure, ax)
         self.sorting_comparison = sorting_comparison
         self.metrics = metrics
         self.performance_name = performance_name
         self.metric_name = metric_name
         self.color = color
         self.markersize = markersize
@@ -55,15 +66,15 @@
         comp = self.sorting_comparison
         unit_ids = comp.sorting1.get_unit_ids()
         perf = comp.get_performance()[self.performance_name]
         metric = self.metrics[self.metric_name]
 
         ax = self.ax
 
-        ax.plot(metric, perf, marker=self.marker, markersize=int(self.markersize), ls='', color=self.color)
+        ax.plot(metric, perf, marker=self.marker, markersize=int(self.markersize), ls="", color=self.color)
         ax.set_xlabel(self.metric_name)
         ax.set_ylabel(self.performance_name)
         ax.set_ylim(0, 1.05)
 
 
 def plot_sorting_performance(*args, **kwargs):
     W = SortingPerformanceWidget(*args, **kwargs)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/timeseries_.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/timeseries_.py`

 * *Files 6% similar despite different names*

```diff
@@ -46,72 +46,86 @@
 
     Returns
     -------
     W: TimeseriesWidget
         The output widget
     """
 
-    def __init__(self, recording, segment_index=None, channel_ids=None, order_channel_by_depth=False,
-                 time_range=None, mode='auto', cmap='RdBu', show_channel_ids=False,
-                 color_groups=False, color=None, clim=None, with_colorbar=True,
-                 figure=None, ax=None, **plot_kwargs):
+    def __init__(
+        self,
+        recording,
+        segment_index=None,
+        channel_ids=None,
+        order_channel_by_depth=False,
+        time_range=None,
+        mode="auto",
+        cmap="RdBu",
+        show_channel_ids=False,
+        color_groups=False,
+        color=None,
+        clim=None,
+        with_colorbar=True,
+        figure=None,
+        ax=None,
+        **plot_kwargs,
+    ):
         BaseWidget.__init__(self, figure, ax)
         self.recording = recording
         self._sampling_frequency = recording.get_sampling_frequency()
         self.visible_channel_ids = channel_ids
         self._plot_kwargs = plot_kwargs
 
         if segment_index is None:
             nseg = recording.get_num_segments()
             if nseg != 1:
-                raise ValueError('You must provide segment_index=...')
+                raise ValueError("You must provide segment_index=...")
                 segment_index = 0
         self.segment_index = segment_index
 
         if self.visible_channel_ids is None:
             self.visible_channel_ids = recording.get_channel_ids()
 
         if order_channel_by_depth:
             locations = self.recording.get_channel_locations()
             channel_inds = self.recording.ids_to_indices(self.visible_channel_ids)
             locations = locations[channel_inds, :]
             origin = np.array([np.max(locations[:, 0]), np.min(locations[:, 1])])[None, :]
-            dist = scipy.spatial.distance.cdist(locations, origin, metric='euclidean')
+            dist = scipy.spatial.distance.cdist(locations, origin, metric="euclidean")
             dist = dist[:, 0]
             self.order = np.argsort(dist)
         else:
             self.order = None
 
         if channel_ids is None:
             channel_ids = recording.get_channel_ids()
 
         fs = recording.get_sampling_frequency()
         if time_range is None:
-            time_range = (0, 1.)
+            time_range = (0, 1.0)
         time_range = np.array(time_range)
 
-        assert mode in ('auto', 'line', 'map'), 'Mode must be in auto/line/map'
-        if mode == 'auto':
+        assert mode in ("auto", "line", "map"), "Mode must be in auto/line/map"
+        if mode == "auto":
             if len(channel_ids) <= 64:
-                mode = 'line'
+                mode = "line"
             else:
-                mode = 'map'
+                mode = "map"
         self.mode = mode
         self.cmap = cmap
 
         self.show_channel_ids = show_channel_ids
 
-        self._frame_range = (time_range * fs).astype('int64')
+        self._frame_range = (time_range * fs).astype("int64")
         a_max = self.recording.get_num_frames(segment_index=self.segment_index)
         self._frame_range = np.clip(self._frame_range, 0, a_max)
         self._time_range = [e / fs for e in self._frame_range]
-        
+
         self.clim = clim
         self.with_colorbar = with_colorbar
-        
+
         self._initialize_stats()
 
         # self._vspacing = self._mean_channel_std * 20
         self._vspacing = self._max_channel_amp * 1.5
 
         if recording.get_channel_groups() is None:
             color_groups = False
@@ -121,47 +135,49 @@
         if color_groups:
             self._colors = []
             self._group_color_map = {}
             all_groups = recording.get_channel_groups()
             groups = np.unique(all_groups)
             N = len(groups)
             import colorsys
+
             HSV_tuples = [(x * 1.0 / N, 0.5, 0.5) for x in range(N)]
             self._colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), HSV_tuples))
             color_idx = 0
             for group in groups:
                 self._group_color_map[group] = color_idx
                 color_idx += 1
-        self.name = 'TimeSeries'
+        self.name = "TimeSeries"
 
     def plot(self):
         self._do_plot()
 
     def _do_plot(self):
         chunk0 = self.recording.get_traces(
             segment_index=self.segment_index,
             channel_ids=self.visible_channel_ids,
             start_frame=self._frame_range[0],
-            end_frame=self._frame_range[1]
+            end_frame=self._frame_range[1],
         )
         if self.order is not None:
             chunk0 = chunk0[:, self.order]
             self.visible_channel_ids = np.array(self.visible_channel_ids)[self.order]
 
         ax = self.ax
 
         n = len(self.visible_channel_ids)
 
-        if self.mode == 'line':
-            ax.set_xlim(self._frame_range[0] / self._sampling_frequency,
-                        self._frame_range[1] / self._sampling_frequency)
+        if self.mode == "line":
+            ax.set_xlim(
+                self._frame_range[0] / self._sampling_frequency, self._frame_range[1] / self._sampling_frequency
+            )
             ax.set_ylim(-self._vspacing, self._vspacing * n)
-            ax.get_xaxis().set_major_locator(MaxNLocator(prune='both'))
+            ax.get_xaxis().set_major_locator(MaxNLocator(prune="both"))
             ax.get_yaxis().set_ticks([])
-            ax.set_xlabel('time (s)')
+            ax.set_xlabel("time (s)")
 
             self._plots = {}
             self._plot_offsets = {}
             offset0 = self._vspacing * (n - 1)
             times = np.arange(self._frame_range[0], self._frame_range[1]) / self._sampling_frequency
             for im, m in enumerate(self.visible_channel_ids):
                 self._plot_offsets[m] = offset0
@@ -174,38 +190,38 @@
                 self._plots[m] = ax.plot(times, self._plot_offsets[m] + chunk0[:, im], color=color, **self._plot_kwargs)
                 offset0 = offset0 - self._vspacing
 
             if self.show_channel_ids:
                 ax.set_yticks(np.arange(n) * self._vspacing)
                 ax.set_yticklabels([str(chan_id) for chan_id in self.visible_channel_ids[::-1]])
 
-        elif self.mode == 'map':
+        elif self.mode == "map":
             extent = (self._time_range[0], self._time_range[1], 0, self.recording.get_num_channels())
-            im = ax.imshow(chunk0.T, interpolation='nearest',
-                           origin='upper', aspect='auto', extent=extent, cmap=self.cmap)
-            
+            im = ax.imshow(
+                chunk0.T, interpolation="nearest", origin="upper", aspect="auto", extent=extent, cmap=self.cmap
+            )
+
             if self.clim is None:
                 im.set_clim(-self._max_channel_amp, self._max_channel_amp)
             else:
                 im.set_clim(*self.clim)
-            
+
             if self.with_colorbar:
                 self.figure.colorbar(im, ax=ax)
 
-            
             if self.show_channel_ids:
                 ax.set_yticks(np.arange(n) + 0.5)
                 ax.set_yticklabels([str(chan_id) for chan_id in self.visible_channel_ids[::-1]])
 
     def _initialize_stats(self):
         chunk0 = self.recording.get_traces(
             segment_index=self.segment_index,
             channel_ids=self.visible_channel_ids,
             start_frame=self._frame_range[0],
-            end_frame=self._frame_range[1]
+            end_frame=self._frame_range[1],
         )
 
         self._mean_channel_std = np.mean(np.std(chunk0, axis=0))
         self._max_channel_amp = np.max(np.max(np.abs(chunk0), axis=0))
 
 
 def plot_timeseries(*args, **kwargs):
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/unitlocalization_.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/unitlocalization_.py`

 * *Files 6% similar despite different names*

```diff
@@ -24,75 +24,83 @@
         Method used to estimate unit localization if 'unit_location' is None
     method_kwargs: dict
         Option for the method
     unit_colors: None or dict
         A dict key is unit_id and value is any color format handled by matplotlib.
         If None, then the get_unit_colors() is internally used.
     with_channel_ids: bool False default
-        add channel ids text on the probe        
+        add channel ids text on the probe
     figure: matplotlib figure
         The figure to be used. If not given a figure is created
     ax: matplotlib axis
         The axis to be used. If not given an axis is created
 
     Returns
     -------
     W: ProbeMapWidget
         The output widget
     """
 
-    def __init__(self, waveform_extractor, 
-                 method='center_of_mass', method_kwargs={},
-                 unit_colors=None, with_channel_ids=False,
-                 figure=None, ax=None):
+    def __init__(
+        self,
+        waveform_extractor,
+        method="center_of_mass",
+        method_kwargs={},
+        unit_colors=None,
+        with_channel_ids=False,
+        figure=None,
+        ax=None,
+    ):
         BaseWidget.__init__(self, figure, ax)
 
         self.waveform_extractor = waveform_extractor
         self.method = method
         self.method_kwargs = method_kwargs
 
         if unit_colors is None:
             unit_colors = get_unit_colors(waveform_extractor.sorting)
         self.unit_colors = unit_colors
-        
+
         self.with_channel_ids = with_channel_ids
 
     def plot(self):
         we = self.waveform_extractor
         unit_ids = we.unit_ids
 
-        if we.is_extension('unit_locations'):
-            unit_locations = we.load_extension('unit_locations').get_data()
+        if we.is_extension("unit_locations"):
+            unit_locations = we.load_extension("unit_locations").get_data()
         else:
             unit_locations = compute_unit_locations(we, method=self.method, **self.method_kwargs)
 
         ax = self.ax
         probegroup = we.get_probegroup()
-        probe_shape_kwargs = dict(facecolor='w', edgecolor='k', lw=0.5, alpha=1.)
-        contacts_kargs = dict(alpha=1., edgecolor='k', lw=0.5)
-        
-        for probe in probegroup.probes:
+        probe_shape_kwargs = dict(facecolor="w", edgecolor="k", lw=0.5, alpha=1.0)
+        contacts_kargs = dict(alpha=1.0, edgecolor="k", lw=0.5)
 
+        for probe in probegroup.probes:
             text_on_contact = None
             if self.with_channel_ids:
                 text_on_contact = self.waveform_extractor.recording.channel_ids
-            
-            poly_contact, poly_contour = plot_probe(probe, ax=ax,
-                                                    contacts_colors='w', contacts_kargs=contacts_kargs,
-                                                    probe_shape_kwargs=probe_shape_kwargs,
-                                                    text_on_contact=text_on_contact)
+
+            poly_contact, poly_contour = plot_probe(
+                probe,
+                ax=ax,
+                contacts_colors="w",
+                contacts_kargs=contacts_kargs,
+                probe_shape_kwargs=probe_shape_kwargs,
+                text_on_contact=text_on_contact,
+            )
             poly_contact.set_zorder(2)
             if poly_contour is not None:
                 poly_contour.set_zorder(1)
 
-        ax.set_title('')
+        ax.set_title("")
 
         color = np.array([self.unit_colors[unit_id] for unit_id in unit_ids])
-        loc = ax.scatter(
-            unit_locations[:, 0], unit_locations[:, 1], marker='1', color=color, s=80, lw=3)
+        loc = ax.scatter(unit_locations[:, 0], unit_locations[:, 1], marker="1", color=color, s=80, lw=3)
         loc.set_zorder(3)
 
 
 def plot_unit_localization(*args, **kwargs):
     W = UnitLocalizationWidget(*args, **kwargs)
     W.plot()
     return W
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/unitprobemap.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/unitprobemap.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,20 +1,16 @@
 import numpy as np
-from matplotlib import pyplot as plt
 
 from .basewidget import BaseWidget
-from matplotlib.animation import FuncAnimation
-
-from probeinterface.plotting import plot_probe
 
 
 class UnitProbeMapWidget(BaseWidget):
     """
     Plots unit map. Amplitude is color coded on probe contact.
-    
+
     Can be static (animated=False) or animated (animated=True)
 
     Parameters
     ----------
     waveform_extractor: WaveformExtractor
     unit_ids: list
         List of unit ids.
@@ -22,64 +18,83 @@
         The channel ids to display
     animated: True/False
         animation for amplitude on time
     with_channel_ids: bool False default
         add channel ids text on the probe
     """
 
-    def __init__(self, waveform_extractor, unit_ids=None, channel_ids=None,
-                 animated=None, with_channel_ids=False, colorbar=True,
-                 ncols=5,  axes=None):
+    def __init__(
+        self,
+        waveform_extractor,
+        unit_ids=None,
+        channel_ids=None,
+        animated=None,
+        with_channel_ids=False,
+        colorbar=True,
+        ncols=5,
+        axes=None,
+    ):
+        from matplotlib.animation import FuncAnimation
+        from matplotlib import pyplot as plt
+        from probeinterface.plotting import plot_probe
 
         self.waveform_extractor = waveform_extractor
         if unit_ids is None:
             unit_ids = waveform_extractor.sorting.unit_ids
         self.unit_ids = unit_ids
         if channel_ids is None:
             channel_ids = waveform_extractor.recording.channel_ids
         self.channel_ids = channel_ids
 
         self.animated = animated
         self.with_channel_ids = with_channel_ids
         self.colorbar = colorbar
-        
+
         probes = waveform_extractor.recording.get_probes()
-        assert len(probes) == 1, "Unit probe map is only available for a single probe. If you have a probe group, "\
-                                 "consider splitting the recording from different probes"
+        assert len(probes) == 1, (
+            "Unit probe map is only available for a single probe. If you have a probe group, "
+            "consider splitting the recording from different probes"
+        )
 
         # layout
         n = len(unit_ids)
         if n < ncols:
             ncols = n
         nrows = int(np.ceil(n / ncols))
         if axes is None:
             fig, axes = plt.subplots(nrows=nrows, ncols=ncols, sharex=True, sharey=True)
         BaseWidget.__init__(self, None, None, axes)
 
     def plot(self):
         we = self.waveform_extractor
         probe = we.get_probe()
 
-        probe_shape_kwargs = dict(facecolor='w', edgecolor='k', lw=0.5, alpha=1.)
+        probe_shape_kwargs = dict(facecolor="w", edgecolor="k", lw=0.5, alpha=1.0)
 
         all_poly_contact = []
         for i, unit_id in enumerate(self.unit_ids):
             ax = self.axes.flatten()[i]
             template = we.get_template(unit_id)
             # static
             if self.animated:
                 contacts_values = np.zeros(template.shape[1])
             else:
                 contacts_values = np.max(np.abs(template), axis=0)
             text_on_contact = None
             if self.with_channel_ids:
                 text_on_contact = self.channel_ids
-            poly_contact, poly_contour = plot_probe(probe, contacts_values=contacts_values,
-                                                    ax=ax, probe_shape_kwargs=probe_shape_kwargs,
-                                                    text_on_contact=text_on_contact)
+            from probeinterface.plotting import plot_probe
+
+            poly_contact, poly_contour = plot_probe(
+                probe,
+                contacts_values=contacts_values,
+                ax=ax,
+                probe_shape_kwargs=probe_shape_kwargs,
+                text_on_contact=text_on_contact,
+            )
 
             poly_contact.set_zorder(2)
             if poly_contour is not None:
                 poly_contour.set_zorder(1)
 
             if self.colorbar:
                 self.figure.colorbar(poly_contact, ax=ax)
@@ -96,16 +111,17 @@
                 for i, unit_id in enumerate(self.unit_ids):
                     template = we.get_template(unit_id)
                     contacts_values = np.abs(template[frame, :])
                     poly_contact = all_poly_contact[i]
                     poly_contact.set_array(contacts_values)
                 return all_poly_contact
 
-            self.animation = FuncAnimation(self.figure, animate_func, frames=num_frames,
-                                           interval=20, blit=True)
+            from matplotlib.animation import FuncAnimation
+
+            self.animation = FuncAnimation(self.figure, animate_func, frames=num_frames, interval=20, blit=True)
 
 
 def plot_unit_probe_map(*args, **kwargs):
     W = UnitProbeMapWidget(*args, **kwargs)
     W.plot()
     return W
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/unitsummary.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/unitsummary.py`

 * *Files 24% similar despite different names*

```diff
@@ -10,17 +10,17 @@
 from .unitwaveforms_ import plot_unit_waveforms
 from .isidistribution import plot_isi_distribution
 
 
 class UnitSummaryWidget(BaseWidget):
     """
     Plot a unit summary.
-    
+
     If amplitudes are alreday computed they are displayed.
-    
+
     Parameters
     ----------
     waveform_extractor: WaveformExtractor
         The waveform extractor object
     unit_id: into or str
         The unit id to plot the summary of
     unit_colors: list or None
@@ -32,22 +32,23 @@
 
     Returns
     -------
     W: UnitSummaryWidget
         The output widget
     """
 
-    def __init__(self, waveform_extractor, unit_id,
-                 unit_colors=None, figure=None, ax=None):
-
+    def __init__(self, waveform_extractor, unit_id, unit_colors=None, figure=None, ax=None):
         assert ax is None
         # ~ assert axes is None
 
         if figure is None:
-            figure = plt.figure(constrained_layout=False, figsize=(15, 7), )
+            figure = plt.figure(
+                constrained_layout=False,
+                figsize=(15, 7),
+            )
 
         BaseWidget.__init__(self, figure, None)
 
         self.waveform_extractor = waveform_extractor
         self.recording = waveform_extractor.recording
         self.sorting = waveform_extractor.sorting
         self.unit_id = unit_id
@@ -58,45 +59,44 @@
 
     def plot(self):
         we = self.waveform_extractor
 
         fig = self.figure
         self.ax.remove()
 
-        if we.is_extension('spike_amplitudes'):
+        if we.is_extension("spike_amplitudes"):
             nrows = 3
         else:
             nrows = 2
 
         gs = fig.add_gridspec(nrows, 6)
 
         ax = fig.add_subplot(gs[:, 0])
         plot_unit_probe_map(we, unit_ids=[self.unit_id], axes=[ax], colorbar=False)
-        ax.set_title('')
+        ax.set_title("")
 
         ax = fig.add_subplot(gs[0:2, 1:3])
         plot_unit_waveforms(we, unit_ids=[self.unit_id], radius_um=60, axes=[ax], unit_colors=self.unit_colors)
         ax.set_title(None)
 
         ax = fig.add_subplot(gs[0:2, 3:5])
         plot_unit_waveform_density_map(we, unit_ids=[self.unit_id], max_channels=1, ax=ax, same_axis=True)
         ax.set_ylabel(None)
 
         ax = fig.add_subplot(gs[0:2, 5])
         plot_isi_distribution(we.sorting, unit_ids=[self.unit_id], axes=[ax])
-        ax.set_title('')
+        ax.set_title("")
 
-        if we.is_extension('spike_amplitudes'):
+        if we.is_extension("spike_amplitudes"):
             ax = fig.add_subplot(gs[-1, 1:])
-            plot_amplitudes_timeseries(we, unit_ids=[self.unit_id], ax=ax,
-                                       unit_colors=self.unit_colors)
+            plot_amplitudes_timeseries(we, unit_ids=[self.unit_id], ax=ax, unit_colors=self.unit_colors)
             ax.set_ylabel(None)
             ax.set_title(None)
 
-        fig.suptitle(f'Unit ID: {self.unit_id}')
+        fig.suptitle(f"Unit ID: {self.unit_id}")
 
 
 def plot_unit_summary(*args, **kwargs):
     W = UnitSummaryWidget(*args, **kwargs)
     W.plot()
     return W
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/unitwaveformdensitymap_.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/unitwaveformdensitymap_.py`

 * *Files 4% similar despite different names*

```diff
@@ -33,19 +33,26 @@
         all channel per units.
     set_title: bool
         Create a plot title with the unit number if True.
     plot_channels: bool
         Plot channel locations below traces, only used if channel_locs is True
     """
 
-    def __init__(self, waveform_extractor, channel_ids=None, unit_ids=None,
-                 max_channels=None, radius_um=None, same_axis=False,
-                 unit_colors=None,
-                 ax=None, axes=None):
-
+    def __init__(
+        self,
+        waveform_extractor,
+        channel_ids=None,
+        unit_ids=None,
+        max_channels=None,
+        radius_um=None,
+        same_axis=False,
+        unit_colors=None,
+        ax=None,
+        axes=None,
+    ):
         self.waveform_extractor = waveform_extractor
         self.recording = waveform_extractor.recording
         self.sorting = waveform_extractor.sorting
 
         if unit_ids is None:
             unit_ids = self.sorting.get_unit_ids()
         self.unit_ids = unit_ids
@@ -55,17 +62,17 @@
         self.channel_ids = channel_ids
 
         if unit_colors is None:
             unit_colors = get_unit_colors(self.sorting)
         self.unit_colors = unit_colors
 
         if radius_um is not None:
-            assert max_channels is None, 'radius_um and max_channels are mutually exclusive'
+            assert max_channels is None, "radius_um and max_channels are mutually exclusive"
         if max_channels is not None:
-            assert radius_um is None, 'radius_um and max_channels are mutually exclusive'
+            assert radius_um is None, "radius_um and max_channels are mutually exclusive"
 
         self.radius_um = radius_um
         self.max_channels = max_channels
         self.same_axis = same_axis
 
         if axes is None and ax is None:
             if same_axis:
@@ -79,30 +86,31 @@
         BaseWidget.__init__(self, figure=None, ax=ax, axes=axes)
 
     def plot(self):
         we = self.waveform_extractor
 
         # channel sparsity
         if self.radius_um is not None:
-            channel_inds = get_template_channel_sparsity(we, method='radius', outputs='index', radius_um=self.radius_um)
+            channel_inds = get_template_channel_sparsity(we, method="radius", outputs="index", radius_um=self.radius_um)
         elif self.max_channels is not None:
-            channel_inds = get_template_channel_sparsity(we, method='best_channels', outputs='index',
-                                                         num_channels=self.max_channels)
+            channel_inds = get_template_channel_sparsity(
+                we, method="best_channels", outputs="index", num_channels=self.max_channels
+            )
         else:
             # all channels
             channel_inds = {unit_id: np.arange(len(self.channel_ids)) for unit_id in self.unit_ids}
         channel_inds = {unit_id: inds for unit_id, inds in channel_inds.items() if unit_id in self.unit_ids}
 
         if self.same_axis:
             # channel union
             inds = np.unique(np.concatenate([inds.tolist() for inds in channel_inds.values()]))
             channel_inds = {unit_id: inds for unit_id in self.unit_ids}
 
         # bins
-        templates = we.get_all_templates(unit_ids=self.unit_ids, mode='median')
+        templates = we.get_all_templates(unit_ids=self.unit_ids, mode="median")
         bin_min = np.min(templates) * 1.3
         bin_max = np.max(templates) * 1.3
         bin_size = (bin_max - bin_min) / 100
         bins = np.arange(bin_min, bin_max, bin_size)
 
         # 2d histograms
         all_hist2d = None
@@ -113,33 +121,45 @@
             wfs = wfs[:, :, chan_inds]
 
             # make histogram density
             wfs_flat = wfs.swapaxes(1, 2).reshape(wfs.shape[0], -1)
             hist2d = np.zeros((wfs_flat.shape[1], bins.size))
             indexes0 = np.arange(wfs_flat.shape[1])
 
-            wf_bined = np.floor((wfs_flat - bin_min) / bin_size).astype('int32')
+            wf_bined = np.floor((wfs_flat - bin_min) / bin_size).astype("int32")
             wf_bined = wf_bined.clip(0, bins.size - 1)
             for d in wf_bined:
                 hist2d[indexes0, d] += 1
 
             if self.same_axis:
                 if all_hist2d is None:
                     all_hist2d = hist2d
                 else:
                     all_hist2d += hist2d
             else:
                 ax = self.axes[unit_index]
-                im = ax.imshow(hist2d.T, interpolation='nearest',
-                               origin='lower', aspect='auto', extent=(0, hist2d.shape[0], bin_min, bin_max), cmap='hot')
+                im = ax.imshow(
+                    hist2d.T,
+                    interpolation="nearest",
+                    origin="lower",
+                    aspect="auto",
+                    extent=(0, hist2d.shape[0], bin_min, bin_max),
+                    cmap="hot",
+                )
 
         if self.same_axis:
             ax = self.ax
-            im = ax.imshow(all_hist2d.T, interpolation='nearest',
-                           origin='lower', aspect='auto', extent=(0, hist2d.shape[0], bin_min, bin_max), cmap='hot')
+            im = ax.imshow(
+                all_hist2d.T,
+                interpolation="nearest",
+                origin="lower",
+                aspect="auto",
+                extent=(0, hist2d.shape[0], bin_min, bin_max),
+                cmap="hot",
+            )
 
         # plot median
         for unit_index, unit_id in enumerate(self.unit_ids):
             if self.same_axis:
                 ax = self.ax
             else:
                 ax = self.axes[unit_index]
@@ -156,22 +176,22 @@
                 if unit_index != 0:
                     continue
             else:
                 ax = self.axes[unit_index]
             chan_inds = channel_inds[unit_id]
             for i, chan_ind in enumerate(chan_inds):
                 if i != 0:
-                    ax.axvline(i * wfs.shape[1], color='w', lw=3)
+                    ax.axvline(i * wfs.shape[1], color="w", lw=3)
                 channel_id = self.recording.channel_ids[chan_ind]
                 x = i * wfs.shape[1] + wfs.shape[1] // 2
-                y = (bin_max + bin_min) / 2.
-                ax.text(x, y, f'chan_id {channel_id}', color='w', ha='center', va='center')
+                y = (bin_max + bin_min) / 2.0
+                ax.text(x, y, f"chan_id {channel_id}", color="w", ha="center", va="center")
 
             ax.set_xticks([])
-            ax.set_ylabel(f'unit_id {unit_id}')
+            ax.set_ylabel(f"unit_id {unit_id}")
 
 
 def plot_unit_waveform_density_map(*args, **kwargs):
     W = UnitWaveformDensityMapWidget(*args, **kwargs)
     W.plot()
     return W
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/unitwaveforms_.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/unitwaveforms_.py`

 * *Files 3% similar despite different names*

```diff
@@ -33,31 +33,42 @@
         Equal aspect ratio for x and y axis, to visualize the array geometry to scale.
     lw: float
         Line width for the traces.
     unit_colors: None or dict
         A dict key is unit_id and value is any color format handled by matplotlib.
         If None, then the get_unit_colors() is internally used.
     unit_selected_waveforms: None or dict
-        A dict key is unit_id and value is the subset of waveforms indices that should be 
+        A dict key is unit_id and value is the subset of waveforms indices that should be
         be displayed
     show_all_channels: bool
         Show the whole probe if True, or only selected channels if False
         The axis to be used. If not given an axis is created
     axes: list of matplotlib axes
         The axes to be used for the individual plots. If not given the required axes are created. If provided, the ax
         and figure parameters are ignored
     """
 
-    def __init__(self, waveform_extractor, channel_ids=None, unit_ids=None,
-                 plot_waveforms=True, plot_templates=True, plot_channels=False,
-                 unit_colors=None, max_channels=None, radius_um=None,
-                 ncols=5, axes=None, lw=2, axis_equal=False, unit_selected_waveforms=None,
-                 set_title=True
-                 ):
-
+    def __init__(
+        self,
+        waveform_extractor,
+        channel_ids=None,
+        unit_ids=None,
+        plot_waveforms=True,
+        plot_templates=True,
+        plot_channels=False,
+        unit_colors=None,
+        max_channels=None,
+        radius_um=None,
+        ncols=5,
+        axes=None,
+        lw=2,
+        axis_equal=False,
+        unit_selected_waveforms=None,
+        set_title=True,
+    ):
         self.waveform_extractor = waveform_extractor
         self._recording = waveform_extractor.recording
         self._sorting = waveform_extractor.sorting
         sorting = waveform_extractor.sorting
 
         if unit_ids is None:
             unit_ids = self._sorting.get_unit_ids()
@@ -72,17 +83,17 @@
 
         self.ncols = ncols
         self._plot_waveforms = plot_waveforms
         self._plot_templates = plot_templates
         self._plot_channels = plot_channels
 
         if radius_um is not None:
-            assert max_channels is None, 'radius_um and max_channels are mutually exclusive'
+            assert max_channels is None, "radius_um and max_channels are mutually exclusive"
         if max_channels is not None:
-            assert radius_um is None, 'radius_um and max_channels are mutually exclusive'
+            assert radius_um is None, "radius_um and max_channels are mutually exclusive"
 
         self.radius_um = radius_um
         self.max_channels = max_channels
         self.unit_selected_waveforms = unit_selected_waveforms
 
         # TODO
         self._lw = lw
@@ -92,16 +103,14 @@
 
         if axes is None:
             num_axes = len(unit_ids)
         else:
             num_axes = None
         BaseWidget.__init__(self, None, None, axes, ncols=ncols, num_axes=num_axes)
 
-
-
     def plot(self):
         self._do_plot()
 
     def _do_plot(self):
         we = self.waveform_extractor
         unit_ids = self._unit_ids
         channel_ids = self._channel_ids
@@ -111,24 +120,24 @@
 
         xvectors, y_scale, y_offset = get_waveforms_scales(we, templates, channel_locations)
 
         ncols = min(self.ncols, len(unit_ids))
         nrows = int(np.ceil(len(unit_ids) / ncols))
 
         if self.radius_um is not None:
-            channel_inds = get_template_channel_sparsity(we, method='radius', outputs='index', radius_um=self.radius_um)
+            channel_inds = get_template_channel_sparsity(we, method="radius", outputs="index", radius_um=self.radius_um)
         elif self.max_channels is not None:
-            channel_inds = get_template_channel_sparsity(we, method='best_channels', outputs='index',
-                                                         num_channels=self.max_channels)
+            channel_inds = get_template_channel_sparsity(
+                we, method="best_channels", outputs="index", num_channels=self.max_channels
+            )
         else:
             # all channels
             channel_inds = {unit_id: slice(None) for unit_id in unit_ids}
 
         for i, unit_id in enumerate(unit_ids):
-
             ax = self.axes.flatten()[i]
             color = self.unit_colors[unit_id]
 
             chan_inds = channel_inds[unit_id]
             xvectors_flat = xvectors[:, chan_inds].T.flatten()
 
             # plot waveforms
@@ -142,43 +151,43 @@
                 wfs_flat = wfs.swapaxes(1, 2).reshape(wfs.shape[0], -1).T
                 ax.plot(xvectors_flat, wfs_flat, lw=1, alpha=0.3, color=color)
 
             # plot template
             if self._plot_templates:
                 template = templates[i, :, :][:, chan_inds] * y_scale + y_offset[:, chan_inds]
                 if self._plot_waveforms and self._plot_templates:
-                    color = 'k'
+                    color = "k"
                 ax.plot(xvectors_flat, template.T.flatten(), lw=1, color=color)
                 template_label = unit_ids[i]
-                ax.set_title(f'template {template_label}')
+                ax.set_title(f"template {template_label}")
 
             # plot channels
             if self._plot_channels:
                 # TODO enhance this
-                ax.scatter(channel_locations[:, 0], channel_locations[:, 1], color='k')
+                ax.scatter(channel_locations[:, 0], channel_locations[:, 1], color="k")
 
 
 def get_waveforms_scales(we, templates, channel_locations):
     """
     Return scales and x_vector for templates plotting
     """
     wf_max = np.max(templates)
     wf_min = np.max(templates)
 
     x_chans = np.unique(channel_locations[:, 0])
     if x_chans.size > 1:
         delta_x = np.min(np.diff(x_chans))
     else:
-        delta_x = 40.
+        delta_x = 40.0
 
     y_chans = np.unique(channel_locations[:, 1])
     if y_chans.size > 1:
         delta_y = np.min(np.diff(y_chans))
     else:
-        delta_y = 40.
+        delta_y = 40.0
 
     m = max(np.abs(wf_max), np.abs(wf_min))
     y_scale = delta_y / m * 0.7
 
     y_offset = channel_locations[:, 1][None, :]
 
     xvect = delta_x * (np.arange(we.nsamples) - we.nbefore) / we.nsamples * 0.7
@@ -196,14 +205,14 @@
     return W
 
 
 plot_unit_waveforms.__doc__ = UnitWaveformsWidget.__doc__
 
 
 def plot_unit_templates(*args, **kwargs):
-    kwargs['plot_waveforms'] = False
+    kwargs["plot_waveforms"] = False
     W = UnitWaveformsWidget(*args, **kwargs)
     W.plot()
     return W
 
 
 plot_unit_templates.__doc__ = UnitWaveformsWidget.__doc__
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/_legacy_mpl_widgets/utils.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/_legacy_mpl_widgets/utils.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,38 +1,37 @@
 import matplotlib.pyplot as plt
 
 import random
 
 try:
     import distinctipy
+
     HAVE_DISTINCTIPY = True
 except ImportError:
     HAVE_DISTINCTIPY = False
 
 
-def get_unit_colors(sorting, map_name='gist_ncar', format='RGBA', shuffle=False):
+def get_unit_colors(sorting, map_name="gist_ncar", format="RGBA", shuffle=False):
     """
     Return a dict colors per units.
     """
-    possible_formats = ('RGBA',)
-    assert format in possible_formats, f'format must be {possible_formats}'
-    
-    
+    possible_formats = ("RGBA",)
+    assert format in possible_formats, f"format must be {possible_formats}"
+
     unit_ids = sorting.unit_ids
-    
+
     if HAVE_DISTINCTIPY:
         colors = distinctipy.get_colors(unit_ids.size)
         # add the alpha
-        colors = [ color + (1., ) for color in colors]
+        colors = [color + (1.0,) for color in colors]
     else:
         # some map have black or white at border so +10
         margin = max(4, len(unit_ids) // 20) // 2
         cmap = plt.get_cmap(map_name, len(unit_ids) + 2 * margin)
 
-        colors = [cmap(i+margin) for i, unit_id in enumerate(unit_ids)]
+        colors = [cmap(i + margin) for i, unit_id in enumerate(unit_ids)]
         if shuffle:
             random.shuffle(colors)
-    
+
     dict_colors = dict(zip(unit_ids, colors))
-    
 
     return dict_colors
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/all_amplitudes_distributions.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/all_amplitudes_distributions.py`

 * *Files 6% similar despite different names*

```diff
@@ -5,46 +5,45 @@
 from .utils import get_some_colors
 
 from ..core.waveform_extractor import WaveformExtractor
 
 
 class AllAmplitudesDistributionsWidget(BaseWidget):
     """
-    Plots distributions of amplitudes as violon plot for all or some units.
+    Plots distributions of amplitudes as violin plot for all or some units.
 
     Parameters
     ----------
     waveform_extractor: WaveformExtractor
         The input waveform extractor
     unit_ids: list
-        List of unit ids.
+        List of unit ids, default None
     unit_colors: None or dict
-        Dict of colors
+        Dict of colors with key: unit, value: color, default None
     """
+
     possible_backends = {}
 
-    
-    def __init__(self, waveform_extractor: WaveformExtractor, unit_ids=None, unit_colors=None,
-                 backend=None, **backend_kwargs):
-        
+    def __init__(
+        self, waveform_extractor: WaveformExtractor, unit_ids=None, unit_colors=None, backend=None, **backend_kwargs
+    ):
         we = waveform_extractor
 
         self.check_extensions(we, "spike_amplitudes")
-        amplitudes = we.load_extension('spike_amplitudes').get_data(outputs='by_unit')
-        
+        amplitudes = we.load_extension("spike_amplitudes").get_data(outputs="by_unit")
+
         num_segments = we.get_num_segments()
-        
+
         if unit_ids is None:
             unit_ids = we.unit_ids
-        
+
         if unit_colors is None:
             unit_colors = get_some_colors(we.unit_ids)
 
-        
         plot_data = dict(
             unit_ids=unit_ids,
             unit_colors=unit_colors,
             num_segments=num_segments,
             amplitudes=amplitudes,
         )
 
-        BaseWidget.__init__(self, plot_data, backend=backend, **backend_kwargs)
+        BaseWidget.__init__(self, plot_data, backend=backend, **backend_kwargs)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/amplitudes.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/amplitudes.py`

 * *Files 6% similar despite different names*

```diff
@@ -9,62 +9,72 @@
 
 class AmplitudesWidget(BaseWidget):
     """
     Plots spike amplitudes
 
     Parameters
     ----------
-    waveform_extractor: WaveformExtractor
+    waveform_extractor : WaveformExtractor
         The input waveform extractor
-    unit_ids: list
-        List of unit ids.
-    segment_index: int
-        The segment index (or None if mono-segment)
-    max_spikes_per_unit: int
+    unit_ids : list
+        List of unit ids, default None
+    segment_index : int
+        The segment index (or None if mono-segment), default None
+    max_spikes_per_unit : int
         Number of max spikes per unit to display. Use None for all spikes.
         Default None.
     hide_unit_selector : bool
-        If True the unit selector is not displayed
+        If True the unit selector is not displayed, default False
         (sortingview backend)
     plot_histogram : bool
-        If True, an histogram of the amplitudes is plotted on the right axis 
+        If True, an histogram of the amplitudes is plotted on the right axis, default False
         (matplotlib backend)
     bins : int
         If plot_histogram is True, the number of bins for the amplitude histogram.
-        If None (default), this is automatically adjusted.
-    plot_legend: bool (default True)
-        plot or not the legend
+        If None this is automatically adjusted, default None
+    plot_legend : bool
+        True includes legend in plot, default True
     """
+
     possible_backends = {}
 
-    
-    def __init__(self, waveform_extractor: WaveformExtractor, unit_ids=None, unit_colors=None,
-                 segment_index=None, max_spikes_per_unit=None, hide_unit_selector=False, 
-                 plot_histograms=False, bins=None, plot_legend=True, backend=None, **backend_kwargs):
+    def __init__(
+        self,
+        waveform_extractor: WaveformExtractor,
+        unit_ids=None,
+        unit_colors=None,
+        segment_index=None,
+        max_spikes_per_unit=None,
+        hide_unit_selector=False,
+        plot_histograms=False,
+        bins=None,
+        plot_legend=True,
+        backend=None,
+        **backend_kwargs,
+    ):
         sorting = waveform_extractor.sorting
         self.check_extensions(waveform_extractor, "spike_amplitudes")
-        sac = waveform_extractor.load_extension('spike_amplitudes')
-        amplitudes = sac.get_data(outputs='by_unit')
+        sac = waveform_extractor.load_extension("spike_amplitudes")
+        amplitudes = sac.get_data(outputs="by_unit")
 
         if unit_ids is None:
             unit_ids = sorting.unit_ids
-    
+
         if unit_colors is None:
             unit_colors = get_some_colors(sorting.unit_ids)
 
         if sorting.get_num_segments() > 1:
             if segment_index is None:
                 warn("More than one segment available! Using segment_index 0")
                 segment_index = 0
         else:
             segment_index = 0
         amplitudes_segment = amplitudes[segment_index]
-        total_duration = waveform_extractor.recording.get_num_samples(segment_index) / \
-            waveform_extractor.sampling_frequency
-        
+        total_duration = waveform_extractor.get_num_samples(segment_index) / waveform_extractor.sampling_frequency
+
         spiketrains_segment = {}
         for i, unit_id in enumerate(sorting.unit_ids):
             times = sorting.get_unit_spike_train(unit_id, segment_index=segment_index)
             times = times / sorting.get_sampling_frequency()
             spiketrains_segment[unit_id] = times
 
         all_spiketrains = spiketrains_segment
@@ -98,10 +108,7 @@
             plot_histograms=plot_histograms,
             bins=bins,
             hide_unit_selector=hide_unit_selector,
             plot_legend=plot_legend,
         )
 
         BaseWidget.__init__(self, plot_data, backend=backend, **backend_kwargs)
-
-
-
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/base.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/base.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,119 +1,123 @@
 import inspect
 
 global default_backend_
-default_backend_ = 'matplotlib'
+default_backend_ = "matplotlib"
+
 
 def get_default_plotter_backend():
     """Return the default backend for spikeinterface widgets.
     The default backend is 'matplotlib' at init.
-    It can be be globaly set with `set_default_plotter_backend(backend)`
-
-    @jeremy: we could also used ENV variable if you prefer
+    It can be be globally set with `set_default_plotter_backend(backend)`
     """
 
     global default_backend_
     return default_backend_
 
 
 def set_default_plotter_backend(backend):
     global default_backend_
     default_backend_ = backend
-    
 
 
 class BaseWidget:
     # this need to be reset in the subclass
     possible_backends = None
-    
+
     def __init__(self, plot_data=None, backend=None, **backend_kwargs):
         # every widgets must prepare a dict "plot_data" in the init
         self.plot_data = plot_data
         self.backend = backend
         self.backend_kwargs = backend_kwargs
 
-        
     def check_backend(self, backend):
         if backend is None:
             backend = get_default_plotter_backend()
-        assert backend in self.possible_backends, (f"{backend} backend not available! Available backends are: "
-                                                   f"{list(self.possible_backends.keys())}")
+        assert backend in self.possible_backends, (
+            f"{backend} backend not available! Available backends are: " f"{list(self.possible_backends.keys())}"
+        )
         return backend
 
     def check_backend_kwargs(self, plotter, backend, **backend_kwargs):
         plotter_kwargs = plotter.default_backend_kwargs
         for k in backend_kwargs:
             if k not in plotter_kwargs:
-                raise Exception(f"{k} is not a valid plot argument or backend keyword argument. "
-                                f"Possible backend keyword arguments for {backend} are: {list(plotter_kwargs.keys())}")
+                raise Exception(
+                    f"{k} is not a valid plot argument or backend keyword argument. "
+                    f"Possible backend keyword arguments for {backend} are: {list(plotter_kwargs.keys())}"
+                )
 
     def do_plot(self, backend, **backend_kwargs):
         backend = self.check_backend(backend)
         plotter = self.possible_backends[backend]()
         self.check_backend_kwargs(plotter, backend, **backend_kwargs)
         plotter.do_plot(self.plot_data, **backend_kwargs)
         self.plotter = plotter
 
     @classmethod
     def register_backend(cls, backend_plotter):
-        cls.possible_backends[backend_plotter.backend] = backend_plotter   
+        cls.possible_backends[backend_plotter.backend] = backend_plotter
 
     @staticmethod
     def check_extensions(waveform_extractor, extensions):
         if isinstance(extensions, str):
             extensions = [extensions]
         error_msg = ""
         raise_error = False
         for extension in extensions:
             if not waveform_extractor.is_extension(extension):
                 raise_error = True
-                error_msg += f"The {extension} waveform extension is required for this widget. " \
-                             f"Run the `compute_{extension}` to compute it.\n"
+                error_msg += (
+                    f"The {extension} waveform extension is required for this widget. "
+                    f"Run the `compute_{extension}` to compute it.\n"
+                )
         if raise_error:
             raise Exception(error_msg)
 
 
-class BackendPlotter():
-    backend = ''
-    
+class BackendPlotter:
+    backend = ""
+
     @classmethod
     def register(cls, widget_cls):
         widget_cls.register_backend(cls)
 
     def update_backend_kwargs(self, **backend_kwargs):
         backend_kwargs_ = self.default_backend_kwargs.copy()
         backend_kwargs_.update(backend_kwargs)
         return backend_kwargs_
-    
+
+
 def copy_signature(source_fct):
     def copy(target_fct):
         target_fct.__signature__ = inspect.signature(source_fct)
         return target_fct
+
     return copy
 
 
 class to_attr(object):
     def __init__(self, d):
         """
         Helper function that transform a dict into
         an object where attributes are the keys of the dict
 
         d = {'a': 1, 'b': 'yep'}
         o = to_attr(d)
         print(o.a, o.b)
         """
         object.__init__(self)
-        object.__setattr__(self, '__d', d)
+        object.__setattr__(self, "__d", d)
 
     def __getattribute__(self, k):
-        d = object.__getattribute__(self, '__d')
+        d = object.__getattribute__(self, "__d")
         return d[k]
 
-def define_widget_function_from_class(widget_class, name):
 
+def define_widget_function_from_class(widget_class, name):
     @copy_signature(widget_class)
     def widget_func(*args, **kwargs):
         W = widget_class(*args, **kwargs)
         W.do_plot(W.backend, **W.backend_kwargs)
         return W.plotter
 
     widget_func.__doc__ = widget_class.__doc__
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/crosscorrelograms.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/crosscorrelograms.py`

 * *Files 5% similar despite different names*

```diff
@@ -11,42 +11,48 @@
     """
     Plots unit cross correlograms.
 
     Parameters
     ----------
     waveform_or_sorting_extractor : WaveformExtractor or BaseSorting
         The object to compute/get crosscorrelograms from
-    unit_ids: list
-        List of unit ids.
+    unit_ids  list
+        List of unit ids, default None
     window_ms : float
-        Window for CCGs in ms, by default 100 ms
+        Window for CCGs in ms, default 100.0 ms
     bin_ms : float
-        Bin size in ms, by default 1 ms
+        Bin size in ms, default 1.0 ms
     hide_unit_selector : bool
-        For sortingview backend, if True the unit selector is not displayed
+        For sortingview backend, if True the unit selector is not displayed, default False
     unit_colors: dict or None
-        Optional dict of colors for units.
+        If given, a dictionary with unit ids as keys and colors as values, default None
     """
+
     possible_backends = {}
 
-    def __init__(self, waveform_or_sorting_extractor: Union[WaveformExtractor, BaseSorting], 
-                 unit_ids=None, window_ms=100.0, bin_ms=1.0, hide_unit_selector=False,
-                 unit_colors=None,
-                 backend=None, **backend_kwargs):
+    def __init__(
+        self,
+        waveform_or_sorting_extractor: Union[WaveformExtractor, BaseSorting],
+        unit_ids=None,
+        window_ms=100.0,
+        bin_ms=1.0,
+        hide_unit_selector=False,
+        unit_colors=None,
+        backend=None,
+        **backend_kwargs,
+    ):
         if isinstance(waveform_or_sorting_extractor, WaveformExtractor):
             sorting = waveform_or_sorting_extractor.sorting
             self.check_extensions(waveform_or_sorting_extractor, "correlograms")
             ccc = waveform_or_sorting_extractor.load_extension("correlograms")
             ccgs, bins = ccc.get_data()
         else:
             sorting = waveform_or_sorting_extractor
-            ccgs, bins = compute_correlograms(sorting,
-                                              window_ms=window_ms,
-                                              bin_ms=bin_ms)
-            
+            ccgs, bins = compute_correlograms(sorting, window_ms=window_ms, bin_ms=bin_ms)
+
         if unit_ids is None:
             unit_ids = sorting.unit_ids
             correlograms = ccgs
         else:
             unit_indices = sorting.ids_to_indices(unit_ids)
             correlograms = ccgs[unit_indices][:, unit_indices]
 
@@ -55,10 +61,7 @@
             bins=bins,
             unit_ids=unit_ids,
             hide_unit_selector=hide_unit_selector,
             unit_colors=unit_colors,
         )
 
         BaseWidget.__init__(self, plot_data, backend=backend, **backend_kwargs)
-
-
-
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/ipywidgets/amplitudes.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/ipywidgets/metrics.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,104 +1,108 @@
 import numpy as np
 
 import matplotlib.pyplot as plt
 import ipywidgets.widgets as widgets
 
+from matplotlib.lines import Line2D
 
 from ..base import to_attr
 
 from .base_ipywidgets import IpywidgetsPlotter
 from .utils import make_unit_controller
 
-from ..amplitudes import AmplitudesWidget
-from ..matplotlib.amplitudes import AmplitudesPlotter as MplAmplitudesPlotter
+from ..matplotlib.metrics import MetricsPlotter as MplMetricsPlotter
 
 from IPython.display import display
 
 
-class AmplitudesPlotter(IpywidgetsPlotter):
-
+class MetricsPlotter(IpywidgetsPlotter):
     def do_plot(self, data_plot, **backend_kwargs):
-
         cm = 1 / 2.54
-        we = data_plot['waveform_extractor']
 
         backend_kwargs = self.update_backend_kwargs(**backend_kwargs)
         width_cm = backend_kwargs["width_cm"]
         height_cm = backend_kwargs["height_cm"]
 
         ratios = [0.15, 0.85]
 
         with plt.ioff():
             output = widgets.Output()
             with output:
                 fig = plt.figure(figsize=((ratios[1] * width_cm) * cm, height_cm * cm))
                 plt.show()
+        if data_plot["unit_ids"] is None:
+            data_plot["unit_ids"] = []
 
-        data_plot['unit_ids'] = data_plot['unit_ids'][:1]
-        unit_widget, unit_controller = make_unit_controller(data_plot['unit_ids'], we.unit_ids,
-                                                            ratios[0] * width_cm, height_cm)
-
-        plot_histograms = widgets.Checkbox(
-            value=data_plot["plot_histograms"],
-            description='plot histograms',
-            disabled=False,
+        unit_widget, unit_controller = make_unit_controller(
+            data_plot["unit_ids"], list(data_plot["unit_colors"].keys()), ratios[0] * width_cm, height_cm
         )
 
-        footer = plot_histograms
+        self.controller = unit_controller
 
-        self.controller = {"plot_histograms": plot_histograms}
-        self.controller.update(unit_controller)
-
-        mpl_plotter = MplAmplitudesPlotter()
+        mpl_plotter = MplMetricsPlotter()
 
         self.updater = PlotUpdater(data_plot, mpl_plotter, fig, self.controller)
         for w in self.controller.values():
             w.observe(self.updater)
 
         self.widget = widgets.AppLayout(
             center=fig.canvas,
             left_sidebar=unit_widget,
             pane_widths=ratios + [0],
-            footer=footer
         )
 
         # a first update
         self.updater(None)
 
         if backend_kwargs["display"]:
             self.check_backend()
             display(self.widget)
 
 
-
-AmplitudesPlotter.register(AmplitudesWidget)
-
-
 class PlotUpdater:
     def __init__(self, data_plot, mpl_plotter, fig, controller):
         self.data_plot = data_plot
         self.mpl_plotter = mpl_plotter
         self.fig = fig
         self.controller = controller
+        self.unit_colors = data_plot["unit_colors"]
 
-        self.we = data_plot['waveform_extractor']
         self.next_data_plot = data_plot.copy()
 
     def __call__(self, change):
-        self.fig.clear()
-
         unit_ids = self.controller["unit_ids"].value
-        plot_histograms = self.controller["plot_histograms"].value
 
         # matplotlib next_data_plot dict update at each call
-        data_plot = self.next_data_plot
-        data_plot['unit_ids'] = unit_ids
-        data_plot['plot_histograms'] = plot_histograms
-        
-        backend_kwargs = {}
-        backend_kwargs['figure'] = self.fig
-
-        self.mpl_plotter.do_plot(data_plot, **backend_kwargs)
+        all_units = list(self.unit_colors.keys())
+        colors = []
+        sizes = []
+        for unit in all_units:
+            color = "gray" if unit not in unit_ids else self.unit_colors[unit]
+            size = 1 if unit not in unit_ids else 5
+            colors.append(color)
+            sizes.append(size)
+
+        # here we do a trick: we just update colors
+        if hasattr(self.mpl_plotter, "patches"):
+            for p in self.mpl_plotter.patches:
+                p.set_color(colors)
+                p.set_sizes(sizes)
+        else:
+            backend_kwargs = {}
+            backend_kwargs["figure"] = self.fig
+            self.mpl_plotter.do_plot(self.data_plot, **backend_kwargs)
+
+        if len(unit_ids) > 0:
+            for l in self.fig.legends:
+                l.remove()
+            handles = [
+                Line2D([0], [0], ls="", marker="o", markersize=5, markeredgewidth=2, color=self.unit_colors[unit])
+                for unit in unit_ids
+            ]
+            labels = unit_ids
+            self.fig.legend(
+                handles, labels, loc="upper center", bbox_to_anchor=(0.5, 1.0), ncol=5, fancybox=True, shadow=True
+            )
 
         self.fig.canvas.draw()
         self.fig.canvas.flush_events()
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/ipywidgets/base_ipywidgets.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/ipywidgets/base_ipywidgets.py`

 * *Files 4% similar despite different names*

```diff
@@ -3,22 +3,18 @@
 import matplotlib as mpl
 import matplotlib.pyplot as plt
 from matplotlib import gridspec
 import numpy as np
 
 
 class IpywidgetsPlotter(BackendPlotter):
-    backend = 'ipywidgets'
+    backend = "ipywidgets"
     backend_kwargs_desc = {
         "width_cm": "Width of the figure in cm (default 10)",
         "height_cm": "Height of the figure in cm (default 6)",
-        "display": "If True, widgets are immediately displayed"
-    }
-    default_backend_kwargs = {
-        "width_cm": 25,
-        "height_cm": 10,
-        "display": True
+        "display": "If True, widgets are immediately displayed",
     }
+    default_backend_kwargs = {"width_cm": 25, "height_cm": 10, "display": True}
 
     def check_backend(self):
         mpl_backend = mpl.get_backend()
-        assert "ipympl" in mpl_backend, ("To use the 'ipywidgets' backend, you have to set %matplotlib widget")
+        assert "ipympl" in mpl_backend, "To use the 'ipywidgets' backend, you have to set %matplotlib widget"
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/ipywidgets/spike_locations.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/ipywidgets/spike_locations.py`

 * *Files 4% similar despite different names*

```diff
@@ -15,31 +15,29 @@
 )
 
 from IPython.display import display
 
 
 class SpikeLocationsPlotter(IpywidgetsPlotter):
     def do_plot(self, data_plot, **backend_kwargs):
-
         cm = 1 / 2.54
 
         backend_kwargs = self.update_backend_kwargs(**backend_kwargs)
         width_cm = backend_kwargs["width_cm"]
         height_cm = backend_kwargs["height_cm"]
 
         ratios = [0.15, 0.85]
 
         with plt.ioff():
             output = widgets.Output()
             with output:
-                fig, ax = plt.subplots(
-                    figsize=((ratios[1] * width_cm) * cm, height_cm * cm))
+                fig, ax = plt.subplots(figsize=((ratios[1] * width_cm) * cm, height_cm * cm))
                 plt.show()
 
-        data_plot['unit_ids'] = data_plot['unit_ids'][:1]
+        data_plot["unit_ids"] = data_plot["unit_ids"][:1]
 
         unit_widget, unit_controller = make_unit_controller(
             data_plot["unit_ids"],
             list(data_plot["unit_colors"].keys()),
             ratios[0] * width_cm,
             height_cm,
         )
@@ -83,16 +81,16 @@
 
         unit_ids = self.controller["unit_ids"].value
 
         # matplotlib next_data_plot dict update at each call
         data_plot = self.next_data_plot
         data_plot["unit_ids"] = unit_ids
         data_plot["plot_all_units"] = True
-        data_plot['plot_legend'] = True
-        data_plot['hide_axis'] = True
+        data_plot["plot_legend"] = True
+        data_plot["hide_axis"] = True
 
         backend_kwargs = {}
         backend_kwargs["ax"] = self.ax
 
         self.mpl_plotter.do_plot(data_plot, **backend_kwargs)
         fig = self.ax.get_figure()
         fig.canvas.draw()
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/ipywidgets/spikes_on_traces.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/ipywidgets/spikes_on_traces.py`

 * *Files 4% similar despite different names*

```diff
@@ -11,125 +11,118 @@
 from ..spikes_on_traces import SpikesOnTracesWidget
 from ..matplotlib.spikes_on_traces import SpikesOnTracesPlotter as MplSpikesOnTracesPlotter
 
 from IPython.display import display
 
 
 class SpikesOnTracesPlotter(IpywidgetsPlotter):
-
     def do_plot(self, data_plot, **backend_kwargs):
         ratios = [0.2, 0.8]
         backend_kwargs = self.update_backend_kwargs(**backend_kwargs)
         backend_kwargs_ts = backend_kwargs.copy()
         backend_kwargs_ts["width_cm"] = ratios[1] * backend_kwargs_ts["width_cm"]
         backend_kwargs_ts["display"] = False
         height_cm = backend_kwargs["height_cm"]
         width_cm = backend_kwargs["width_cm"]
 
-        
         # plot timeseries
         tsplotter = TimeseriesPlotter()
         data_plot["timeseries"]["add_legend"] = False
         tsplotter.do_plot(data_plot["timeseries"], **backend_kwargs_ts)
-        
+
         ts_w = tsplotter.widget
         ts_updater = tsplotter.updater
-        
-        we = data_plot['waveform_extractor']
-        unit_widget, unit_controller = make_unit_controller(data_plot['unit_ids'], we.unit_ids,
-                                                            ratios[0] * width_cm, height_cm)
-        
+
+        we = data_plot["waveform_extractor"]
+        unit_widget, unit_controller = make_unit_controller(
+            data_plot["unit_ids"], we.unit_ids, ratios[0] * width_cm, height_cm
+        )
+
         self.controller = ts_updater.controller
         self.controller.update(unit_controller)
-    
+
         mpl_plotter = MplSpikesOnTracesPlotter()
-        
+
         self.updater = PlotUpdater(data_plot, mpl_plotter, ts_updater, self.controller)
         for w in self.controller.values():
             w.observe(self.updater)
-        
-        
-        self.widget = widgets.AppLayout(
-            center=ts_w,
-            left_sidebar=unit_widget,
-            pane_widths=ratios + [0]
-        )
-        
+
+        self.widget = widgets.AppLayout(center=ts_w, left_sidebar=unit_widget, pane_widths=ratios + [0])
+
         # a first update
         self.updater(None)
-        
+
         if backend_kwargs["display"]:
             self.check_backend()
             display(self.widget)
 
 
-
 SpikesOnTracesPlotter.register(SpikesOnTracesWidget)
 
 
 class PlotUpdater:
     def __init__(self, data_plot, mpl_plotter, ts_updater, controller):
         self.data_plot = data_plot
         self.mpl_plotter = mpl_plotter
-        
+
         self.ts_updater = ts_updater
         self.ax = ts_updater.ax
         self.fig = self.ax.figure
         self.controller = controller
-    
+
     def __call__(self, change):
         self.ax.clear()
-        
+
         unit_ids = self.controller["unit_ids"].value
-        
+
         # update ts
         # self.ts_updater.__call__(change)
-        
+
         # update data plot
         data_plot = self.data_plot.copy()
         data_plot["timeseries"] = self.ts_updater.next_data_plot
         data_plot["unit_ids"] = unit_ids
-        
+
         backend_kwargs = {}
-        backend_kwargs['ax'] = self.ax
-        
+        backend_kwargs["ax"] = self.ax
+
         self.mpl_plotter.do_plot(data_plot, **backend_kwargs)
-        
+
         self.fig.canvas.draw()
         self.fig.canvas.flush_events()
-        
+
         # t = self.time_slider.value
         # d = self.win_sizer.value
-        
+
         # selected_layer = self.layer_selector.value
         # segment_index = self.seg_selector.value
         # mode = self.mode_selector.value
-        
+
         # t_stop = self.t_stops[segment_index]
         # if self.actual_segment_index != segment_index:
         #     # change time_slider limits
         #     self.time_slider.max = t_stop
         #     self.actual_segment_index = segment_index
 
         # # protect limits
         # if t >= t_stop - d:
         #     t = t_stop - d
 
         # time_range = np.array([t, t+d])
-        
+
         # if mode =='line':
         #     # plot all layer
         #     layer_keys = self.data_plot['layer_keys']
         #     recordings = self.recordings
         #     clims = None
         # elif mode =='map':
         #     layer_keys = [selected_layer]
         #     recordings = {selected_layer: self.recordings[selected_layer]}
         #     clims = {selected_layer: self.data_plot["clims"][selected_layer]}
-        
+
         # channel_ids = self.data_plot['channel_ids']
         # order =  self.data_plot['order']
         # times, list_traces, frame_range, order = _get_trace_list(recordings, channel_ids, time_range, order,
         #                                                          segment_index)
 
         # # matplotlib next_data_plot dict update at each call
         # data_plot = self.next_data_plot
@@ -142,12 +135,11 @@
         # data_plot['list_traces'] = list_traces
         # data_plot['times'] = times
         # data_plot['clims'] = clims
 
         # backend_kwargs = {}
         # backend_kwargs['ax'] = self.ax
         # self.mpl_plotter.do_plot(data_plot, **backend_kwargs)
-        
+
         # fig = self.ax.figure
         # fig.canvas.draw()
         # fig.canvas.flush_events()
-
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/ipywidgets/timeseries.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/ipywidgets/timeseries.py`

 * *Files 3% similar despite different names*

```diff
@@ -11,46 +11,50 @@
 from ..timeseries import TimeseriesWidget, _get_trace_list
 from ..matplotlib.timeseries import TimeseriesPlotter as MplTimeseriesPlotter
 
 from IPython.display import display
 
 
 class TimeseriesPlotter(IpywidgetsPlotter):
-
     def do_plot(self, data_plot, **backend_kwargs):
-        recordings = data_plot['recordings']
+        recordings = data_plot["recordings"]
 
         # first layer
-        rec0 = recordings[data_plot['layer_keys'][0]]
+        rec0 = recordings[data_plot["layer_keys"][0]]
 
         cm = 1 / 2.54
 
         backend_kwargs = self.update_backend_kwargs(**backend_kwargs)
         width_cm = backend_kwargs["width_cm"]
         height_cm = backend_kwargs["height_cm"]
         ratios = [0.1, 0.8, 0.2]
 
         with plt.ioff():
             output = widgets.Output()
             with output:
                 fig, ax = plt.subplots(figsize=(0.9 * ratios[1] * width_cm * cm, height_cm * cm))
                 plt.show()
 
-        t_start = 0.
+        t_start = 0.0
         t_stop = rec0.get_num_samples(segment_index=0) / rec0.get_sampling_frequency()
 
-        ts_widget, ts_controller = make_timeseries_controller(t_start, t_stop, data_plot['layer_keys'],
-                                                              rec0.get_num_segments(), data_plot['time_range'],
-                                                              data_plot['mode'], False, width_cm)
+        ts_widget, ts_controller = make_timeseries_controller(
+            t_start,
+            t_stop,
+            data_plot["layer_keys"],
+            rec0.get_num_segments(),
+            data_plot["time_range"],
+            data_plot["mode"],
+            False,
+            width_cm,
+        )
 
-        ch_widget, ch_controller = make_channel_controller(rec0, width_cm=ratios[2] * width_cm,
-                                                           height_cm=height_cm)
+        ch_widget, ch_controller = make_channel_controller(rec0, width_cm=ratios[2] * width_cm, height_cm=height_cm)
 
-        scale_widget, scale_controller = make_scale_controller(width_cm=ratios[0] * width_cm,
-                                                               height_cm=height_cm)
+        scale_widget, scale_controller = make_scale_controller(width_cm=ratios[0] * width_cm, height_cm=height_cm)
 
         self.controller = ts_controller
         self.controller.update(ch_controller)
         self.controller.update(scale_controller)
 
         mpl_plotter = MplTimeseriesPlotter()
 
@@ -63,15 +67,15 @@
 
         self.widget = widgets.AppLayout(
             center=fig.canvas,
             footer=ts_widget,
             left_sidebar=scale_widget,
             right_sidebar=ch_widget,
             pane_heights=[0, 6, 1],
-            pane_widths=ratios
+            pane_widths=ratios,
         )
 
         # a first update
         self.updater(None)
 
         if backend_kwargs["display"]:
             self.check_backend()
@@ -85,28 +89,30 @@
     def __init__(self, data_plot, mpl_plotter, ax, controller):
         self.data_plot = data_plot
         self.mpl_plotter = mpl_plotter
 
         self.ax = ax
         self.controller = controller
 
-        self.recordings = data_plot['recordings']
-        self.return_scaled = data_plot['return_scaled']
+        self.recordings = data_plot["recordings"]
+        self.return_scaled = data_plot["return_scaled"]
         self.next_data_plot = data_plot.copy()
         self.list_traces = None
 
         self.actual_segment_index = self.controller["segment_index"].value
 
-        self.rec0 = self.recordings[self.data_plot['layer_keys'][0]]
-        self.t_stops = [self.rec0.get_num_samples(segment_index=seg_index) / self.rec0.get_sampling_frequency()
-                        for seg_index in range(self.rec0.get_num_segments())]
+        self.rec0 = self.recordings[self.data_plot["layer_keys"][0]]
+        self.t_stops = [
+            self.rec0.get_num_samples(segment_index=seg_index) / self.rec0.get_sampling_frequency()
+            for seg_index in range(self.rec0.get_num_segments())
+        ]
 
     def __call__(self, change):
         self.ax.clear()
-        
+
         # if changing the layer_key, no need to retrieve and process traces
         retrieve_traces = True
         scale_up = False
         scale_down = False
         if change is not None:
             for cname, c in self.controller.items():
                 if isinstance(change, dict):
@@ -122,24 +128,24 @@
         window = self.controller["window"].value
         layer_key = self.controller["layer_key"].value
         segment_index = self.controller["segment_index"].value
         mode = self.controller["mode"].value
         chan_start, chan_stop = self.controller["channel_inds"].value
 
         if mode == "line":
-            self.controller["all_layers"].layout.visibility = 'visible'
+            self.controller["all_layers"].layout.visibility = "visible"
             all_layers = self.controller["all_layers"].value
         elif mode == "map":
-            self.controller["all_layers"].layout.visibility = 'hidden'
+            self.controller["all_layers"].layout.visibility = "hidden"
             all_layers = False
 
         if all_layers:
-            self.controller["layer_key"].layout.visibility = 'hidden'
+            self.controller["layer_key"].layout.visibility = "hidden"
         else:
-            self.controller["layer_key"].layout.visibility = 'visible'
+            self.controller["layer_key"].layout.visibility = "visible"
 
         if chan_start == chan_stop:
             chan_stop += 1
         channel_indices = np.arange(chan_start, chan_stop)
 
         t_stop = self.t_stops[segment_index]
         if self.actual_segment_index != segment_index:
@@ -147,77 +153,80 @@
             self.controller["t_start"].max = t_stop
             self.actual_segment_index = segment_index
 
         # protect limits
         if t_start >= t_stop - window:
             t_start = t_stop - window
 
-        time_range = np.array([t_start, t_start+window])
+        time_range = np.array([t_start, t_start + window])
         data_plot = self.next_data_plot
-        
+
         if retrieve_traces:
             all_channel_ids = self.recordings[list(self.recordings.keys())[0]].channel_ids
             if self.data_plot["order"] is not None:
                 all_channel_ids = all_channel_ids[self.data_plot["order"]]
             channel_ids = all_channel_ids[channel_indices]
-            if self.data_plot['order_channel_by_depth']:
+            if self.data_plot["order_channel_by_depth"]:
                 order, _ = order_channels_by_depth(self.rec0, channel_ids)
             else:
                 order = None
-            times, list_traces, frame_range, channel_ids = _get_trace_list(self.recordings, channel_ids, time_range,
-                                                                           segment_index, order, self.return_scaled)
+            times, list_traces, frame_range, channel_ids = _get_trace_list(
+                self.recordings, channel_ids, time_range, segment_index, order, self.return_scaled
+            )
             self.list_traces = list_traces
         else:
-            times = data_plot['times']
-            list_traces = data_plot['list_traces']
-            frame_range = data_plot['frame_range']
-            channel_ids = data_plot['channel_ids']
+            times = data_plot["times"]
+            list_traces = data_plot["list_traces"]
+            frame_range = data_plot["frame_range"]
+            channel_ids = data_plot["channel_ids"]
 
         if all_layers:
-            layer_keys = self.data_plot['layer_keys']
+            layer_keys = self.data_plot["layer_keys"]
             recordings = self.recordings
             list_traces_plot = self.list_traces
         else:
             layer_keys = [layer_key]
             recordings = {layer_key: self.recordings[layer_key]}
             list_traces_plot = [self.list_traces[list(self.recordings.keys()).index(layer_key)]]
-            
+
         if scale_up:
-            if mode == 'line':
+            if mode == "line":
                 data_plot["vspacing"] *= 0.8
-            elif mode == 'map':
-                data_plot["clims"] = {layer: (1.2 * val[0], 1.2 * val[1])
-                                      for layer, val in self.data_plot["clims"].items()}
+            elif mode == "map":
+                data_plot["clims"] = {
+                    layer: (1.2 * val[0], 1.2 * val[1]) for layer, val in self.data_plot["clims"].items()
+                }
         if scale_down:
-            if mode == 'line':
+            if mode == "line":
                 data_plot["vspacing"] *= 1.2
-            elif mode == 'map':
-                data_plot["clims"] = {layer: (0.8 * val[0], 0.8 * val[1])
-                                      for layer, val in self.data_plot["clims"].items()}
+            elif mode == "map":
+                data_plot["clims"] = {
+                    layer: (0.8 * val[0], 0.8 * val[1]) for layer, val in self.data_plot["clims"].items()
+                }
 
         self.next_data_plot["vspacing"] = data_plot["vspacing"]
         self.next_data_plot["clims"] = data_plot["clims"]
 
-        if mode == 'line':
+        if mode == "line":
             clims = None
-        elif mode == 'map':
+        elif mode == "map":
             clims = {layer_key: self.data_plot["clims"][layer_key]}
 
         # matplotlib next_data_plot dict update at each call
-        data_plot['mode'] = mode
-        data_plot['frame_range'] = frame_range
-        data_plot['time_range'] = time_range
-        data_plot['with_colorbar'] = False
-        data_plot['recordings'] = recordings
-        data_plot['layer_keys'] = layer_keys
-        data_plot['list_traces'] = list_traces_plot
-        data_plot['times'] = times
-        data_plot['clims'] = clims
-        data_plot['channel_ids'] = channel_ids
+        data_plot["mode"] = mode
+        data_plot["frame_range"] = frame_range
+        data_plot["time_range"] = time_range
+        data_plot["with_colorbar"] = False
+        data_plot["recordings"] = recordings
+        data_plot["layer_keys"] = layer_keys
+        data_plot["list_traces"] = list_traces_plot
+        data_plot["times"] = times
+        data_plot["clims"] = clims
+        data_plot["channel_ids"] = channel_ids
 
         backend_kwargs = {}
-        backend_kwargs['ax'] = self.ax
+        backend_kwargs["ax"] = self.ax
         self.mpl_plotter.do_plot(data_plot, **backend_kwargs)
 
         fig = self.ax.figure
         fig.canvas.draw()
         fig.canvas.flush_events()
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/ipywidgets/unit_locations.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/ipywidgets/unit_locations.py`

 * *Files 9% similar despite different names*

```diff
@@ -12,36 +12,33 @@
 from ..unit_locations import UnitLocationsWidget
 from ..matplotlib.unit_locations import UnitLocationsPlotter as MplUnitLocationsPlotter
 
 from IPython.display import display
 
 
 class UnitLocationsPlotter(IpywidgetsPlotter):
-
     def do_plot(self, data_plot, **backend_kwargs):
-
         cm = 1 / 2.54
 
         backend_kwargs = self.update_backend_kwargs(**backend_kwargs)
         width_cm = backend_kwargs["width_cm"]
         height_cm = backend_kwargs["height_cm"]
 
         ratios = [0.15, 0.85]
 
         with plt.ioff():
             output = widgets.Output()
             with output:
-                fig, ax = plt.subplots(
-                    figsize=((ratios[1] * width_cm) * cm, height_cm * cm))
+                fig, ax = plt.subplots(figsize=((ratios[1] * width_cm) * cm, height_cm * cm))
                 plt.show()
 
-        data_plot['unit_ids'] = data_plot['unit_ids'][:1]
-        unit_widget, unit_controller = make_unit_controller(data_plot['unit_ids'],
-                                                            list(data_plot['unit_colors'].keys()),
-                                                            ratios[0] * width_cm, height_cm)
+        data_plot["unit_ids"] = data_plot["unit_ids"][:1]
+        unit_widget, unit_controller = make_unit_controller(
+            data_plot["unit_ids"], list(data_plot["unit_colors"].keys()), ratios[0] * width_cm, height_cm
+        )
 
         self.controller = unit_controller
 
         mpl_plotter = MplUnitLocationsPlotter()
 
         self.updater = PlotUpdater(data_plot, mpl_plotter, ax, self.controller)
         for w in self.controller.values():
@@ -76,19 +73,19 @@
     def __call__(self, change):
         self.ax.clear()
 
         unit_ids = self.controller["unit_ids"].value
 
         # matplotlib next_data_plot dict update at each call
         data_plot = self.next_data_plot
-        data_plot['unit_ids'] = unit_ids
-        data_plot['plot_all_units'] = True
-        data_plot['plot_legend'] = True
-        data_plot['hide_axis'] = True
+        data_plot["unit_ids"] = unit_ids
+        data_plot["plot_all_units"] = True
+        data_plot["plot_legend"] = True
+        data_plot["hide_axis"] = True
 
         backend_kwargs = {}
-        backend_kwargs['ax'] = self.ax
+        backend_kwargs["ax"] = self.ax
 
         self.mpl_plotter.do_plot(data_plot, **backend_kwargs)
         fig = self.ax.get_figure()
         fig.canvas.draw()
         fig.canvas.flush_events()
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/ipywidgets/unit_waveforms.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/ipywidgets/unit_waveforms.py`

 * *Files 6% similar despite different names*

```diff
@@ -12,19 +12,17 @@
 from ..unit_waveforms import UnitWaveformsWidget
 from ..matplotlib.unit_waveforms import UnitWaveformPlotter as MplUnitWaveformPlotter
 
 from IPython.display import display
 
 
 class UnitWaveformPlotter(IpywidgetsPlotter):
-
     def do_plot(self, data_plot, **backend_kwargs):
-
         cm = 1 / 2.54
-        we = data_plot['waveform_extractor']
+        we = data_plot["waveform_extractor"]
 
         backend_kwargs = self.update_backend_kwargs(**backend_kwargs)
         width_cm = backend_kwargs["width_cm"]
         height_cm = backend_kwargs["height_cm"]
 
         ratios = [0.1, 0.7, 0.2]
 
@@ -34,130 +32,138 @@
                 fig_wf = plt.figure(figsize=((ratios[1] * width_cm) * cm, height_cm * cm))
                 plt.show()
             output2 = widgets.Output()
             with output2:
                 fig_probe, ax_probe = plt.subplots(figsize=((ratios[2] * width_cm) * cm, height_cm * cm))
                 plt.show()
 
-        data_plot['unit_ids'] = data_plot['unit_ids'][:1]
-        unit_widget, unit_controller = make_unit_controller(data_plot['unit_ids'], we.unit_ids,
-                                                            ratios[0] * width_cm, height_cm)
+        data_plot["unit_ids"] = data_plot["unit_ids"][:1]
+        unit_widget, unit_controller = make_unit_controller(
+            data_plot["unit_ids"], we.unit_ids, ratios[0] * width_cm, height_cm
+        )
 
         same_axis_button = widgets.Checkbox(
             value=False,
-            description='same axis',
+            description="same axis",
             disabled=False,
         )
 
         plot_templates_button = widgets.Checkbox(
             value=True,
-            description='plot templates',
+            description="plot templates",
             disabled=False,
         )
 
         hide_axis_button = widgets.Checkbox(
             value=True,
-            description='hide axis',
+            description="hide axis",
             disabled=False,
         )
 
         footer = widgets.HBox([same_axis_button, plot_templates_button, hide_axis_button])
 
-        self.controller = {"same_axis": same_axis_button, "plot_templates": plot_templates_button,
-                           "hide_axis": hide_axis_button}
+        self.controller = {
+            "same_axis": same_axis_button,
+            "plot_templates": plot_templates_button,
+            "hide_axis": hide_axis_button,
+        }
         self.controller.update(unit_controller)
 
         mpl_plotter = MplUnitWaveformPlotter()
 
         self.updater = PlotUpdater(data_plot, mpl_plotter, fig_wf, ax_probe, self.controller)
         for w in self.controller.values():
             w.observe(self.updater)
 
         self.widget = widgets.AppLayout(
             center=fig_wf.canvas,
             left_sidebar=unit_widget,
             right_sidebar=fig_probe.canvas,
             pane_widths=ratios,
-            footer=footer
+            footer=footer,
         )
 
         # a first update
         self.updater(None)
 
         if backend_kwargs["display"]:
             self.check_backend()
             display(self.widget)
 
 
-
 UnitWaveformPlotter.register(UnitWaveformsWidget)
 
 
 class PlotUpdater:
     def __init__(self, data_plot, mpl_plotter, fig_wf, ax_probe, controller):
         self.data_plot = data_plot
         self.mpl_plotter = mpl_plotter
         self.fig_wf = fig_wf
         self.ax_probe = ax_probe
         self.controller = controller
 
-        self.we = data_plot['waveform_extractor']
+        self.we = data_plot["waveform_extractor"]
         self.next_data_plot = data_plot.copy()
 
     def __call__(self, change):
         self.fig_wf.clear()
         self.ax_probe.clear()
 
         unit_ids = self.controller["unit_ids"].value
         same_axis = self.controller["same_axis"].value
         plot_templates = self.controller["plot_templates"].value
         hide_axis = self.controller["hide_axis"].value
 
         # matplotlib next_data_plot dict update at each call
         data_plot = self.next_data_plot
-        data_plot['unit_ids'] = unit_ids
-        data_plot['templates'] = self.we.get_all_templates(unit_ids=unit_ids)
-        data_plot['template_stds'] = self.we.get_all_templates(unit_ids=unit_ids, mode="std")
-        data_plot['same_axis'] = same_axis
-        data_plot['plot_templates'] = plot_templates
+        data_plot["unit_ids"] = unit_ids
+        data_plot["templates"] = self.we.get_all_templates(unit_ids=unit_ids)
+        data_plot["template_stds"] = self.we.get_all_templates(unit_ids=unit_ids, mode="std")
+        data_plot["same_axis"] = same_axis
+        data_plot["plot_templates"] = plot_templates
         if data_plot["plot_waveforms"]:
-            data_plot['wfs_by_ids'] = {unit_id: self.we.get_waveforms(unit_id) for unit_id in unit_ids}
+            data_plot["wfs_by_ids"] = {unit_id: self.we.get_waveforms(unit_id) for unit_id in unit_ids}
 
         backend_kwargs = {}
 
         if same_axis:
-            backend_kwargs['ax'] = self.fig_wf.add_subplot()
-            data_plot['set_title'] = False
+            backend_kwargs["ax"] = self.fig_wf.add_subplot()
+            data_plot["set_title"] = False
         else:
-            backend_kwargs['figure'] = self.fig_wf
+            backend_kwargs["figure"] = self.fig_wf
 
         self.mpl_plotter.do_plot(data_plot, **backend_kwargs)
         if same_axis:
             self.mpl_plotter.ax.axis("equal")
             if hide_axis:
                 self.mpl_plotter.ax.axis("off")
         else:
             if hide_axis:
                 for i in range(len(unit_ids)):
                     ax = self.mpl_plotter.axes.flatten()[i]
                     ax.axis("off")
 
         # update probe plot
         channel_locations = self.we.get_channel_locations()
-        self.ax_probe.plot(channel_locations[:, 0], channel_locations[:, 1], ls="", marker="o", color="gray",
-                           markersize=2, alpha=0.5)
+        self.ax_probe.plot(
+            channel_locations[:, 0], channel_locations[:, 1], ls="", marker="o", color="gray", markersize=2, alpha=0.5
+        )
         self.ax_probe.axis("off")
         self.ax_probe.axis("equal")
 
         for unit in unit_ids:
-            channel_inds = data_plot['sparsity'].unit_id_to_channel_indices[unit]
-            self.ax_probe.plot(channel_locations[channel_inds, 0],
-                               channel_locations[channel_inds, 1],
-                               ls="", marker="o", markersize=3,
-                               color=self.next_data_plot['unit_colors'][unit])
-        self.ax_probe.set_xlim(np.min(channel_locations[:, 0])-10, np.max(channel_locations[:, 0])+10)
+            channel_inds = data_plot["sparsity"].unit_id_to_channel_indices[unit]
+            self.ax_probe.plot(
+                channel_locations[channel_inds, 0],
+                channel_locations[channel_inds, 1],
+                ls="",
+                marker="o",
+                markersize=3,
+                color=self.next_data_plot["unit_colors"][unit],
+            )
+        self.ax_probe.set_xlim(np.min(channel_locations[:, 0]) - 10, np.max(channel_locations[:, 0]) + 10)
         fig_probe = self.ax_probe.get_figure()
 
         self.fig_wf.canvas.draw()
         self.fig_wf.canvas.flush_events()
         fig_probe.canvas.draw()
         fig_probe.canvas.flush_events()
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/ipywidgets/utils.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/ipywidgets/utils.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,95 +1,97 @@
-
-
 import ipywidgets.widgets as widgets
 import numpy as np
 
 
 def make_timeseries_controller(t_start, t_stop, layer_keys, num_segments, time_range, mode, all_layers, width_cm):
     time_slider = widgets.FloatSlider(
-        orientation='horizontal',
-        description='time:',
+        orientation="horizontal",
+        description="time:",
         value=time_range[0],
         min=t_start,
         max=t_stop,
         continuous_update=False,
-        layout=widgets.Layout(width=f'{width_cm}cm')
+        layout=widgets.Layout(width=f"{width_cm}cm"),
+    )
+    layer_selector = widgets.Dropdown(description="layer", options=layer_keys)
+    segment_selector = widgets.Dropdown(description="segment", options=list(range(num_segments)))
+    window_sizer = widgets.BoundedFloatText(value=np.diff(time_range)[0], step=0.1, min=0.005, description="win (s)")
+    mode_selector = widgets.Dropdown(options=["line", "map"], description="mode", value=mode)
+    all_layers = widgets.Checkbox(description="plot all layers", value=all_layers)
+
+    controller = {
+        "layer_key": layer_selector,
+        "segment_index": segment_selector,
+        "window": window_sizer,
+        "t_start": time_slider,
+        "mode": mode_selector,
+        "all_layers": all_layers,
+    }
+    widget = widgets.VBox(
+        [time_slider, widgets.HBox([all_layers, layer_selector, segment_selector, window_sizer, mode_selector])]
     )
-    layer_selector = widgets.Dropdown(description='layer', options=layer_keys)
-    segment_selector = widgets.Dropdown(description='segment', options=list(range(num_segments)))
-    window_sizer = widgets.BoundedFloatText(value=np.diff(time_range)[0], step=0.1, min=0.005, description='win (s)')
-    mode_selector = widgets.Dropdown(options=['line', 'map'], description='mode', value=mode)
-    all_layers = widgets.Checkbox(description='plot all layers', value=all_layers)
-
-    controller = {"layer_key": layer_selector, "segment_index": segment_selector, "window": window_sizer,
-                  "t_start": time_slider, "mode": mode_selector, "all_layers": all_layers}
-    widget = widgets.VBox([time_slider,
-                           widgets.HBox([all_layers, layer_selector, segment_selector, window_sizer, mode_selector])])
 
     return widget, controller
 
 
 def make_unit_controller(unit_ids, all_unit_ids, width_cm, height_cm):
     unit_label = widgets.Label(value="units:")
 
     unit_selector = widgets.SelectMultiple(
         options=all_unit_ids,
         value=list(unit_ids),
         disabled=False,
-        layout=widgets.Layout(width=f'{width_cm}cm', height=f'{height_cm}cm')
+        layout=widgets.Layout(width=f"{width_cm}cm", height=f"{height_cm}cm"),
     )
 
     controller = {"unit_ids": unit_selector}
     widget = widgets.VBox([unit_label, unit_selector])
 
     return widget, controller
 
 
 def make_channel_controller(recording, width_cm, height_cm):
-    channel_label = widgets.Label("channel indices:",
-                                  layout=widgets.Layout(justify_content="center"))
+    channel_label = widgets.Label("channel indices:", layout=widgets.Layout(justify_content="center"))
     channel_selector = widgets.IntRangeSlider(
         value=[0, recording.get_num_channels()],
         min=0,
         max=recording.get_num_channels(),
         step=1,
         disabled=False,
         continuous_update=False,
-        orientation='vertical',
+        orientation="vertical",
         readout=True,
-        readout_format='d',
-        layout=widgets.Layout(width=f'{0.8 * width_cm}cm', height=f'{height_cm}cm')
+        readout_format="d",
+        layout=widgets.Layout(width=f"{0.8 * width_cm}cm", height=f"{height_cm}cm"),
     )
 
     controller = {"channel_inds": channel_selector}
     widget = widgets.VBox([channel_label, channel_selector])
 
     return widget, controller
 
 
 def make_scale_controller(width_cm, height_cm):
-    scale_label = widgets.Label("Scale",
-                                layout=widgets.Layout(justify_content="center"))
+    scale_label = widgets.Label("Scale", layout=widgets.Layout(justify_content="center"))
 
     plus_selector = widgets.Button(
-        description='',
+        description="",
         disabled=False,
-        button_style='', # 'success', 'info', 'warning', 'danger' or ''
-        tooltip='Increase scale',
-        icon='arrow-up',
-        layout=widgets.Layout(width=f'{0.8 * width_cm}cm', height=f'{0.4 * height_cm}cm')
+        button_style="",  # 'success', 'info', 'warning', 'danger' or ''
+        tooltip="Increase scale",
+        icon="arrow-up",
+        layout=widgets.Layout(width=f"{0.8 * width_cm}cm", height=f"{0.4 * height_cm}cm"),
     )
 
     minus_selector = widgets.Button(
-        description='',
+        description="",
         disabled=False,
-        button_style='', # 'success', 'info', 'warning', 'danger' or ''
-        tooltip='Decrease scale',
-        icon='arrow-down',
-        layout=widgets.Layout(width=f'{0.8 * width_cm}cm', height=f'{0.4 * height_cm}cm')
+        button_style="",  # 'success', 'info', 'warning', 'danger' or ''
+        tooltip="Decrease scale",
+        icon="arrow-down",
+        layout=widgets.Layout(width=f"{0.8 * width_cm}cm", height=f"{0.4 * height_cm}cm"),
     )
 
     controller = {"plus": plus_selector, "minus": minus_selector}
     widget = widgets.VBox([scale_label, plus_selector, minus_selector])
 
     return widget, controller
-
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/__init__.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/__init__.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 from .all_amplitudes_distributions import AllAmplitudesDistributionsPlotter
 from .amplitudes import AmplitudesPlotter
 from .autocorrelograms import AutoCorrelogramsPlotter
 from .crosscorrelograms import CrossCorrelogramsPlotter
 from .quality_metrics import QualityMetricsPlotter
+from .motion import MotionPlotter
 from .spike_locations import SpikeLocationsPlotter
 from .spikes_on_traces import SpikesOnTracesPlotter
 from .template_metrics import TemplateMetricsPlotter
 from .template_similarity import TemplateSimilarityPlotter
 from .timeseries import TimeseriesPlotter
 from .unit_locations import UnitLocationsPlotter
 from .unit_templates import UnitTemplatesWidget
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/all_amplitudes_distributions.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/all_amplitudes_distributions.py`

 * *Files 6% similar despite different names*

```diff
@@ -2,47 +2,40 @@
 
 from ..base import to_attr
 from ..all_amplitudes_distributions import AllAmplitudesDistributionsWidget
 from .base_mpl import MplPlotter
 
 
 class AllAmplitudesDistributionsPlotter(MplPlotter):
-
     def do_plot(self, data_plot, **backend_kwargs):
         dp = to_attr(data_plot)
         backend_kwargs = self.update_backend_kwargs(**backend_kwargs)
         self.make_mpl_figure(**backend_kwargs)
-        
+
         ax = self.ax
-        
+
         unit_amps = []
         for i, unit_id in enumerate(dp.unit_ids):
             amps = []
             for segment_index in range(dp.num_segments):
                 amps.append(dp.amplitudes[segment_index][unit_id])
             amps = np.concatenate(amps)
             unit_amps.append(amps)
         parts = ax.violinplot(unit_amps, showmeans=False, showmedians=False, showextrema=False)
 
-        for i, pc in enumerate(parts['bodies']):
+        for i, pc in enumerate(parts["bodies"]):
             color = dp.unit_colors[dp.unit_ids[i]]
             pc.set_facecolor(color)
-            pc.set_edgecolor('black')
+            pc.set_edgecolor("black")
             pc.set_alpha(1)
 
         ax.set_xticks(np.arange(len(dp.unit_ids)) + 1)
         ax.set_xticklabels([str(unit_id) for unit_id in dp.unit_ids])
 
         ylims = ax.get_ylim()
         if np.max(ylims) < 0:
             ax.set_ylim(min(ylims), 0)
         if np.min(ylims) > 0:
             ax.set_ylim(0, max(ylims))
 
 
-
-
 AllAmplitudesDistributionsPlotter.register(AllAmplitudesDistributionsWidget)
-
-
-
-
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/amplitudes.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/amplitudes.py`

 * *Files 6% similar despite different names*

```diff
@@ -9,15 +9,14 @@
     def __init__(self) -> None:
         self.legend = None
 
     def do_plot(self, data_plot, **backend_kwargs):
         dp = to_attr(data_plot)
         backend_kwargs = self.update_backend_kwargs(**backend_kwargs)
 
-
         if backend_kwargs["axes"] is not None:
             axes = backend_kwargs["axes"]
             if dp.plot_histograms:
                 assert np.asarray(axes).size == 2
             else:
                 assert np.asarray(axes).size == 1
         elif backend_kwargs["ax"] is not None:
@@ -26,49 +25,45 @@
             if dp.plot_histograms:
                 backend_kwargs["num_axes"] = 2
                 backend_kwargs["ncols"] = 2
             else:
                 backend_kwargs["num_axes"] = None
 
         self.make_mpl_figure(**backend_kwargs)
-        
+
         scatter_ax = self.axes.flatten()[0]
-        
+
         for unit_id in dp.unit_ids:
             spiketrains = dp.spiketrains[unit_id]
             amps = dp.amplitudes[unit_id]
-            scatter_ax.scatter(spiketrains, amps,
-                               color=dp.unit_colors[unit_id], s=3, alpha=1,
-                               label=unit_id)
-            
+            scatter_ax.scatter(spiketrains, amps, color=dp.unit_colors[unit_id], s=3, alpha=1, label=unit_id)
+
             if dp.plot_histograms:
                 if dp.bins is None:
                     bins = int(len(spiketrains) / 30)
                 else:
                     bins = dp.bins
                 ax_hist = self.axes.flatten()[1]
-                ax_hist.hist(amps, bins=bins, orientation="horizontal", 
-                                  color=dp.unit_colors[unit_id],
-                                  alpha=0.8)
-        
+                ax_hist.hist(amps, bins=bins, orientation="horizontal", color=dp.unit_colors[unit_id], alpha=0.8)
+
         if dp.plot_histograms:
             ax_hist = self.axes.flatten()[1]
             ax_hist.set_ylim(scatter_ax.get_ylim())
             ax_hist.axis("off")
             self.figure.tight_layout()
-        
+
         if dp.plot_legend:
             if self.legend is not None:
                 self.legend.remove()
-            self.legend = self.figure.legend(loc='upper center', bbox_to_anchor=(0.5, 1.),
-                                             ncol=5, fancybox=True, shadow=True)
+            self.legend = self.figure.legend(
+                loc="upper center", bbox_to_anchor=(0.5, 1.0), ncol=5, fancybox=True, shadow=True
+            )
 
         scatter_ax.set_xlim(0, dp.total_duration)
-        scatter_ax.set_xlabel('Times [s]')
-        scatter_ax.set_ylabel(f'Amplitude')
+        scatter_ax.set_xlabel("Times [s]")
+        scatter_ax.set_ylabel(f"Amplitude")
         scatter_ax.spines["top"].set_visible(False)
         scatter_ax.spines["right"].set_visible(False)
         self.figure.subplots_adjust(bottom=0.1, top=0.9, left=0.1)
-        
-        
+
 
 AmplitudesPlotter.register(AmplitudesWidget)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/autocorrelograms.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/autocorrelograms.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,32 +1,30 @@
 from ..base import to_attr
 from ..autocorrelograms import AutoCorrelogramsWidget
 from .base_mpl import MplPlotter
 
 
 class AutoCorrelogramsPlotter(MplPlotter):
-
     def do_plot(self, data_plot, **backend_kwargs):
         dp = to_attr(data_plot)
         backend_kwargs = self.update_backend_kwargs(**backend_kwargs)
         backend_kwargs["num_axes"] = len(dp.unit_ids)
 
         self.make_mpl_figure(**backend_kwargs)
-        
+
         bins = dp.bins
         unit_ids = dp.unit_ids
         correlograms = dp.correlograms
         bin_width = bins[1] - bins[0]
 
         for i, unit_id in enumerate(unit_ids):
             ccg = correlograms[i, i]
             ax = self.axes.flatten()[i]
             if dp.unit_colors is None:
-                color = 'g'
+                color = "g"
             else:
                 color = dp.unit_colors[unit_id]
-            ax.bar(x=bins[:-1], height=ccg, width=bin_width, color=color, align='edge')
+            ax.bar(x=bins[:-1], height=ccg, width=bin_width, color=color, align="edge")
             ax.set_title(str(unit_id))
 
 
-
 AutoCorrelogramsPlotter.register(AutoCorrelogramsWidget)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/base_mpl.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/base_mpl.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,58 +1,50 @@
 from spikeinterface.widgets.base import BackendPlotter
 
 import matplotlib.pyplot as plt
 import numpy as np
 
 
 class MplPlotter(BackendPlotter):
-    backend = 'matplotlib'
+    backend = "matplotlib"
     backend_kwargs_desc = {
         "figure": "Matplotlib figure. When None, it is created. Default None",
         "ax": "Single matplotlib axis. When None, it is created. Default None",
         "axes": "Multiple matplotlib axes. When None, they is created. Default None",
         "ncols": "Number of columns to create in subplots.  Default 5",
         "figsize": "Size of matplotlib figure. Default None",
-        "figtitle": "The figure title. Default None"
-    }
-    default_backend_kwargs = {
-        "figure": None,
-        "ax": None,
-        "axes": None,
-        "ncols": 5,
-        "figsize": None,
-        "figtitle": None
+        "figtitle": "The figure title. Default None",
     }
+    default_backend_kwargs = {"figure": None, "ax": None, "axes": None, "ncols": 5, "figsize": None, "figtitle": None}
 
-    def make_mpl_figure(self, figure=None, ax=None, axes=None, ncols=None, num_axes=None,
-                        figsize=None, figtitle=None):
+    def make_mpl_figure(self, figure=None, ax=None, axes=None, ncols=None, num_axes=None, figsize=None, figtitle=None):
         """
         figure/ax/axes : only one of then can be not None
         """
         if figure is not None:
-            assert ax is None and axes is None, 'figure/ax/axes : only one of then can be not None'
-            if  num_axes is None:
+            assert ax is None and axes is None, "figure/ax/axes : only one of then can be not None"
+            if num_axes is None:
                 ax = figure.add_subplot(111)
                 axes = np.array([[ax]])
             else:
                 assert ncols is not None
                 axes = []
                 nrows = int(np.ceil(num_axes / ncols))
                 axes = np.full((nrows, ncols), fill_value=None, dtype=object)
                 for i in range(num_axes):
                     ax = figure.add_subplot(nrows, ncols, i + 1)
                     r = i // ncols
                     c = i % ncols
                     axes[r, c] = ax
         elif ax is not None:
-            assert figure is None and axes is None, 'figure/ax/axes : only one of then can be not None'
+            assert figure is None and axes is None, "figure/ax/axes : only one of then can be not None"
             figure = ax.get_figure()
             axes = np.array([[ax]])
         elif axes is not None:
-            assert figure is None and ax is None, 'figure/ax/axes : only one of then can be not None'
+            assert figure is None and ax is None, "figure/ax/axes : only one of then can be not None"
             axes = np.asarray(axes)
             figure = axes.flatten()[0].get_figure()
         else:
             # 'figure/ax/axes are all None
             if num_axes is None:
                 # one fig with one ax
                 figure, ax = plt.subplots(figsize=figsize)
@@ -68,45 +60,43 @@
                     ax = figure.add_subplot(111)
                     axes = np.array([[ax]])
                 else:
                     assert ncols is not None
                     if num_axes < ncols:
                         ncols = num_axes
                     nrows = int(np.ceil(num_axes / ncols))
-                    figure, axes = plt.subplots(
-                        nrows=nrows, ncols=ncols, figsize=figsize)
+                    figure, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)
                     ax = None
                     # remove extra axes
                     if ncols * nrows > num_axes:
                         for i, extra_ax in enumerate(axes.flatten()):
                             if i >= num_axes:
                                 extra_ax.remove()
                                 r = i // ncols
                                 c = i % ncols
                                 axes[r, c] = None
 
         self.figure = figure
         self.ax = ax
         # axes is always a 2D array of ax
         self.axes = axes
-        
+
         if figtitle is not None:
             self.figure.suptitle(figtitle)
 
 
-
 class to_attr(object):
     def __init__(self, d):
         """
         Helper function that transform a dict into
         an object where attributes are the keys of the dict
 
         d = {'a': 1, 'b': 'yep'}
         o = to_attr(d)
         print(o.a, o.b)
         """
         object.__init__(self)
-        object.__setattr__(self, '__d', d)
+        object.__setattr__(self, "__d", d)
 
     def __getattribute__(self, k):
-        d = object.__getattribute__(self, '__d')
+        d = object.__getattribute__(self, "__d")
         return d[k]
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/crosscorrelograms.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/crosscorrelograms.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,42 +1,39 @@
-
 from ..base import to_attr
 from ..crosscorrelograms import CrossCorrelogramsWidget
 from .base_mpl import MplPlotter
 
 
 class CrossCorrelogramsPlotter(MplPlotter):
-
     def do_plot(self, data_plot, **backend_kwargs):
         dp = to_attr(data_plot)
         backend_kwargs = self.update_backend_kwargs(**backend_kwargs)
         backend_kwargs["ncols"] = len(dp.unit_ids)
         backend_kwargs["num_axes"] = int(len(dp.unit_ids) ** 2)
 
         self.make_mpl_figure(**backend_kwargs)
         assert self.axes.ndim == 2
-        
+
         bins = dp.bins
         unit_ids = dp.unit_ids
         correlograms = dp.correlograms
         bin_width = bins[1] - bins[0]
-        
+
         for i, unit_id1 in enumerate(unit_ids):
             for j, unit_id2 in enumerate(unit_ids):
                 ccg = correlograms[i, j]
                 ax = self.axes[i, j]
                 if i == j:
                     if dp.unit_colors is None:
-                        color = 'g'
+                        color = "g"
                     else:
                         color = dp.unit_colors[unit_id1]
                 else:
-                    color = 'k'
-                ax.bar(x=bins[:-1], height=ccg, width=bin_width,
-                       color=color, align='edge')
+                    color = "k"
+                ax.bar(x=bins[:-1], height=ccg, width=bin_width, color=color, align="edge")
 
         for i, unit_id in enumerate(unit_ids):
             self.axes[0, i].set_title(str(unit_id))
-            self.axes[-1, i].set_xlabel('CCG (ms)')
+            self.axes[-1, i].set_xlabel("CCG (ms)")
 
 
 CrossCorrelogramsPlotter.register(CrossCorrelogramsWidget)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/metrics.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/metrics.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,49 +1,46 @@
 import numpy as np
 
 from ..base import to_attr
 from .base_mpl import MplPlotter
 
 
-
 class MetricsPlotter(MplPlotter):
-
     def do_plot(self, data_plot, **backend_kwargs):
         dp = to_attr(data_plot)
         metrics = dp.metrics
         num_metrics = len(metrics.columns)
 
-        if 'figsize' not in backend_kwargs:
-            backend_kwargs["figsize"] = (2*num_metrics, 2*num_metrics)
+        if "figsize" not in backend_kwargs:
+            backend_kwargs["figsize"] = (2 * num_metrics, 2 * num_metrics)
 
         backend_kwargs = self.update_backend_kwargs(**backend_kwargs)
-        backend_kwargs["num_axes"] = num_metrics ** 2
+        backend_kwargs["num_axes"] = num_metrics**2
         backend_kwargs["ncols"] = num_metrics
 
         all_unit_ids = metrics.index.values
 
         self.make_mpl_figure(**backend_kwargs)
         assert self.axes.ndim == 2
 
         if dp.unit_ids is None:
             colors = ["gray"] * len(all_unit_ids)
         else:
             colors = []
             for unit in all_unit_ids:
                 color = "gray" if unit not in dp.unit_ids else dp.unit_colors[unit]
                 colors.append(color)
-            
+
         self.patches = []
         for i, m1 in enumerate(metrics.columns):
             for j, m2 in enumerate(metrics.columns):
                 if i == j:
                     self.axes[i, j].hist(metrics[m1], color="gray")
                 else:
-                    p = self.axes[i, j].scatter(metrics[m1], metrics[m2], c=colors,
-                                                s=3, marker="o")
+                    p = self.axes[i, j].scatter(metrics[m1], metrics[m2], c=colors, s=3, marker="o")
                     self.patches.append(p)
                 if i == num_metrics - 1:
                     self.axes[i, j].set_xlabel(m2, fontsize=10)
                 if j == 0:
                     self.axes[i, j].set_ylabel(m1, fontsize=10)
                 self.axes[i, j].set_xticklabels([])
                 self.axes[i, j].set_yticklabels([])
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/spike_locations.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/spike_locations.py`

 * *Files 12% similar despite different names*

```diff
@@ -16,37 +16,40 @@
         self.legend = None
 
     def do_plot(self, data_plot, **backend_kwargs):
         dp = to_attr(data_plot)
         backend_kwargs = self.update_backend_kwargs(**backend_kwargs)
 
         self.make_mpl_figure(**backend_kwargs)
-        
+
         spike_locations = dp.spike_locations
-        
+
         probegroup = ProbeGroup.from_dict(dp.probegroup_dict)
-        probe_shape_kwargs = dict(facecolor='w', edgecolor='k', lw=0.5, alpha=1.)
-        contacts_kargs = dict(alpha=1., edgecolor='k', lw=0.5)
-        
-        for probe in probegroup.probes:
+        probe_shape_kwargs = dict(facecolor="w", edgecolor="k", lw=0.5, alpha=1.0)
+        contacts_kargs = dict(alpha=1.0, edgecolor="k", lw=0.5)
 
+        for probe in probegroup.probes:
             text_on_contact = None
             if dp.with_channel_ids:
                 text_on_contact = dp.channel_ids
-            
-            poly_contact, poly_contour = plot_probe(probe, ax=self.ax,
-                                                    contacts_colors='w', contacts_kargs=contacts_kargs,
-                                                    probe_shape_kwargs=probe_shape_kwargs,
-                                                    text_on_contact=text_on_contact)
+
+            poly_contact, poly_contour = plot_probe(
+                probe,
+                ax=self.ax,
+                contacts_colors="w",
+                contacts_kargs=contacts_kargs,
+                probe_shape_kwargs=probe_shape_kwargs,
+                text_on_contact=text_on_contact,
+            )
             poly_contact.set_zorder(2)
             if poly_contour is not None:
                 poly_contour.set_zorder(1)
 
-        self.ax.set_title('')
-        
+        self.ax.set_title("")
+
         if dp.plot_all_units:
             unit_colors = {}
             unit_ids = dp.all_unit_ids
             for unit in dp.all_unit_ids:
                 if unit not in dp.unit_ids:
                     unit_colors[unit] = "gray"
                 else:
@@ -54,38 +57,40 @@
         else:
             unit_ids = dp.unit_ids
             unit_colors = dp.unit_colors
         labels = dp.unit_ids
 
         for i, unit in enumerate(unit_ids):
             locs = spike_locations[unit]
-            
+
             zorder = 5 if unit in dp.unit_ids else 3
-            self.ax.scatter(locs["x"], locs["y"], s=2, alpha=0.3, color=unit_colors[unit],
-                            zorder=zorder)
-            
-        handles = [Line2D([0], [0], ls="", marker='o', markersize=5, markeredgewidth=2, 
-                          color=unit_colors[unit]) for unit in dp.unit_ids]
+            self.ax.scatter(locs["x"], locs["y"], s=2, alpha=0.3, color=unit_colors[unit], zorder=zorder)
+
+        handles = [
+            Line2D([0], [0], ls="", marker="o", markersize=5, markeredgewidth=2, color=unit_colors[unit])
+            for unit in dp.unit_ids
+        ]
         if dp.plot_legend:
             if self.legend is not None:
                 self.legend.remove()
-            self.legend = self.figure.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 1.),
-                                             ncol=5, fancybox=True, shadow=True)
+            self.legend = self.figure.legend(
+                handles, labels, loc="upper center", bbox_to_anchor=(0.5, 1.0), ncol=5, fancybox=True, shadow=True
+            )
 
         # set proper axis limits
         xlims, ylims = estimate_axis_lims(spike_locations)
-        
+
         ax_xlims = list(self.ax.get_xlim())
         ax_ylims = list(self.ax.get_ylim())
-        
+
         ax_xlims[0] = xlims[0] if xlims[0] < ax_xlims[0] else ax_xlims[0]
         ax_xlims[1] = xlims[1] if xlims[1] > ax_xlims[1] else ax_xlims[1]
         ax_ylims[0] = ylims[0] if ylims[0] < ax_ylims[0] else ax_ylims[0]
         ax_ylims[1] = ylims[1] if ylims[1] > ax_ylims[1] else ax_ylims[1]
-        
+
         self.ax.set_xlim(ax_xlims)
         self.ax.set_ylim(ax_ylims)
         if dp.hide_axis:
             self.ax.axis("off")
 
 
 SpikeLocationsPlotter.register(SpikeLocationsWidget)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/spikes_on_traces.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/spikes_on_traces.py`

 * *Files 10% similar despite different names*

```diff
@@ -6,88 +6,99 @@
 from .timeseries import TimeseriesPlotter
 
 from matplotlib.patches import Ellipse
 from matplotlib.lines import Line2D
 
 
 class SpikesOnTracesPlotter(MplPlotter):
-    
     def do_plot(self, data_plot, **backend_kwargs):
         dp = to_attr(data_plot)
 
         # first plot time series
         tsplotter = TimeseriesPlotter()
         data_plot["timeseries"]["add_legend"] = False
         tsplotter.do_plot(dp.timeseries, **backend_kwargs)
         self.ax = tsplotter.ax
         self.axes = tsplotter.axes
         self.figure = tsplotter.figure
-        
+
         ax = self.ax
-        
+
         we = dp.waveform_extractor
         sorting = dp.waveform_extractor.sorting
         frame_range = dp.timeseries["frame_range"]
         segment_index = dp.timeseries["segment_index"]
         min_y = np.min(dp.timeseries["channel_locations"][:, 1])
         max_y = np.max(dp.timeseries["channel_locations"][:, 1])
-        
+
         n = len(dp.timeseries["channel_ids"])
         order = dp.timeseries["order"]
         if order is None:
             order = np.arange(n)
-        
+
         if ax.get_legend() is not None:
             ax.get_legend().remove()
-        
+
         # loop through units and plot a scatter of spikes at estimated location
         handles = []
         labels = []
 
         for unit in dp.unit_ids:
             spike_frames = sorting.get_unit_spike_train(unit, segment_index=segment_index)
             spike_start, spike_end = np.searchsorted(spike_frames, frame_range)
-            
+
             chan_ids = dp.sparsity.unit_id_to_channel_ids[unit]
-            
+
             spike_frames_to_plot = spike_frames[spike_start:spike_end]
-            
+
             if dp.timeseries["mode"] == "map":
-                spike_times_to_plot = sorting.get_unit_spike_train(unit, segment_index=segment_index, 
-                                                                   return_times=True)[spike_start:spike_end]
+                spike_times_to_plot = sorting.get_unit_spike_train(
+                    unit, segment_index=segment_index, return_times=True
+                )[spike_start:spike_end]
                 unit_y_loc = min_y + max_y - dp.unit_locations[unit][1]
                 # markers = np.ones_like(spike_frames_to_plot) * (min_y + max_y - dp.unit_locations[unit][1])
                 width = 2 * 1e-3
-                ellipse_kwargs = dict(width=width, height=10, fc='none', ec=dp.unit_colors[unit], lw=2)
+                ellipse_kwargs = dict(width=width, height=10, fc="none", ec=dp.unit_colors[unit], lw=2)
                 patches = [Ellipse((s, unit_y_loc), **ellipse_kwargs) for s in spike_times_to_plot]
                 for p in patches:
                     ax.add_patch(p)
-                handles.append(Line2D([0], [0], ls="", marker='o', markersize=5, markeredgewidth=2, 
-                                      markeredgecolor=dp.unit_colors[unit], markerfacecolor='none'))
+                handles.append(
+                    Line2D(
+                        [0],
+                        [0],
+                        ls="",
+                        marker="o",
+                        markersize=5,
+                        markeredgewidth=2,
+                        markeredgecolor=dp.unit_colors[unit],
+                        markerfacecolor="none",
+                    )
+                )
                 labels.append(unit)
             else:
                 # construct waveforms
                 label_set = False
                 if len(spike_frames_to_plot) > 0:
                     vspacing = dp.timeseries["vspacing"]
                     traces = dp.timeseries["list_traces"][0]
                     waveform_idxs = spike_frames_to_plot[:, None] + np.arange(-we.nbefore, we.nafter) - frame_range[0]
-                    waveform_idxs = np.clip(waveform_idxs, 0, len(dp.timeseries['times'])-1)
+                    waveform_idxs = np.clip(waveform_idxs, 0, len(dp.timeseries["times"]) - 1)
 
                     times = dp.timeseries["times"][waveform_idxs]
                     # discontinuity
                     times[:, -1] = np.nan
                     times_r = times.reshape(times.shape[0] * times.shape[1])
-                    waveforms = traces[waveform_idxs] #[:, :, order]
+                    waveforms = traces[waveform_idxs]  # [:, :, order]
                     waveforms_r = waveforms.reshape((waveforms.shape[0] * waveforms.shape[1], waveforms.shape[2]))
-                    
+
                     for i, chan_id in enumerate(dp.timeseries["channel_ids"]):
-                        offset = vspacing * (n - 1 - i)
+                        offset = vspacing * i
                         if chan_id in chan_ids:
                             l = ax.plot(times_r, offset + waveforms_r[:, i], color=dp.unit_colors[unit])
                             if not label_set:
                                 handles.append(l[0])
                                 labels.append(unit)
                                 label_set = True
-        ax.legend(handles, labels)  
+        ax.legend(handles, labels)
+
 
 SpikesOnTracesPlotter.register(SpikesOnTracesWidget)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/template_similarity.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/template_similarity.py`

 * *Files 12% similar despite different names*

```diff
@@ -2,15 +2,14 @@
 
 from ..base import to_attr
 from ..template_similarity import TemplateSimilarityWidget
 from .base_mpl import MplPlotter
 
 
 class TemplateSimilarityPlotter(MplPlotter):
-
     def do_plot(self, data_plot, **backend_kwargs):
         dp = to_attr(data_plot)
         backend_kwargs = self.update_backend_kwargs(**backend_kwargs)
 
         self.make_mpl_figure(**backend_kwargs)
 
         im = self.ax.matshow(dp.similarity, cmap=dp.cmap)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/timeseries.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/timeseries.py`

 * *Files 8% similar despite different names*

```diff
@@ -3,62 +3,68 @@
 from ..base import to_attr
 from ..timeseries import TimeseriesWidget
 from .base_mpl import MplPlotter
 from matplotlib.ticker import MaxNLocator
 
 
 class TimeseriesPlotter(MplPlotter):
-    
     def do_plot(self, data_plot, **backend_kwargs):
         dp = to_attr(data_plot)
         backend_kwargs = self.update_backend_kwargs(**backend_kwargs)
 
         self.make_mpl_figure(**backend_kwargs)
         ax = self.ax
         n = len(dp.channel_ids)
         if dp.channel_locations is not None:
             y_locs = dp.channel_locations[:, 1]
         else:
             y_locs = np.arange(n)
         min_y = np.min(y_locs)
         max_y = np.max(y_locs)
 
-        if dp.mode == 'line':
+        if dp.mode == "line":
             offset = dp.vspacing * (n - 1)
 
             for layer_key, traces in zip(dp.layer_keys, dp.list_traces):
                 for i, chan_id in enumerate(dp.channel_ids):
                     offset = dp.vspacing * i
                     color = dp.colors[layer_key][chan_id]
                     ax.plot(dp.times, offset + traces[:, i], color=color)
                 ax.get_lines()[-1].set_label(layer_key)
 
             if dp.show_channel_ids:
                 ax.set_yticks(np.arange(n) * dp.vspacing)
                 channel_labels = np.array([str(chan_id) for chan_id in dp.channel_ids])
                 ax.set_yticklabels(channel_labels)
+            else:
+                ax.get_yaxis().set_visible(False)
+
             ax.set_xlim(*dp.time_range)
             ax.set_ylim(-dp.vspacing, dp.vspacing * n)
-            ax.get_xaxis().set_major_locator(MaxNLocator(prune='both'))
-            ax.set_xlabel('time (s)')
+            ax.get_xaxis().set_major_locator(MaxNLocator(prune="both"))
+            ax.set_xlabel("time (s)")
             if dp.add_legend:
-                ax.legend(loc='upper right')
+                ax.legend(loc="upper right")
 
-        elif dp.mode == 'map':
+        elif dp.mode == "map":
             assert len(dp.list_traces) == 1, 'plot_timeseries with mode="map" do not support multi recording'
             assert len(dp.clims) == 1
             clim = list(dp.clims.values())[0]
             extent = (dp.time_range[0], dp.time_range[1], min_y, max_y)
-            im = ax.imshow(dp.list_traces[0].T, interpolation='nearest',
-                           origin='lower', aspect='auto', extent=extent, cmap=dp.cmap)
+            im = ax.imshow(
+                dp.list_traces[0].T, interpolation="nearest", origin="lower", aspect="auto", extent=extent, cmap=dp.cmap
+            )
 
             im.set_clim(*clim)
 
             if dp.with_colorbar:
                 self.figure.colorbar(im, ax=ax)
 
             if dp.show_channel_ids:
                 ax.set_yticks(np.linspace(min_y, max_y, n) + (max_y - min_y) / n * 0.5)
                 channel_labels = np.array([str(chan_id) for chan_id in dp.channel_ids])
                 ax.set_yticklabels(channel_labels)
+            else:
+                ax.get_yaxis().set_visible(False)
+
 
 TimeseriesPlotter.register(TimeseriesWidget)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/unit_depths.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/unit_depths.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,23 +1,22 @@
 from ..base import to_attr
 from ..unit_depths import UnitDepthsWidget
 from .base_mpl import MplPlotter
 
 
 class UnitDepthsPlotter(MplPlotter):
-
     def do_plot(self, data_plot, **backend_kwargs):
         dp = to_attr(data_plot)
         backend_kwargs = self.update_backend_kwargs(**backend_kwargs)
         self.make_mpl_figure(**backend_kwargs)
 
         ax = self.ax
         size = dp.num_spikes / max(dp.num_spikes) * 120
         ax.scatter(dp.unit_amplitudes, dp.unit_depths, color=dp.colors, s=size)
 
         ax.set_aspect(3)
-        ax.set_xlabel('amplitude')
-        ax.set_ylabel('depth [um]')
+        ax.set_xlabel("amplitude")
+        ax.set_ylabel("depth [um]")
         ax.set_xlim(0, max(dp.unit_amplitudes) * 1.2)
 
 
 UnitDepthsPlotter.register(UnitDepthsWidget)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/unit_locations.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/unit_locations.py`

 * *Files 13% similar despite different names*

```diff
@@ -7,77 +7,89 @@
 from ..base import to_attr
 from ..unit_locations import UnitLocationsWidget
 from .base_mpl import MplPlotter
 
 from matplotlib.patches import Ellipse
 from matplotlib.lines import Line2D
 
+
 class UnitLocationsPlotter(MplPlotter):
     def __init__(self) -> None:
         self.legend = None
 
     def do_plot(self, data_plot, **backend_kwargs):
         dp = to_attr(data_plot)
         backend_kwargs = self.update_backend_kwargs(**backend_kwargs)
 
         self.make_mpl_figure(**backend_kwargs)
-        
+
         unit_locations = dp.unit_locations
-        
+
         probegroup = ProbeGroup.from_dict(dp.probegroup_dict)
-        probe_shape_kwargs = dict(facecolor='w', edgecolor='k', lw=0.5, alpha=1.)
-        contacts_kargs = dict(alpha=1., edgecolor='k', lw=0.5)
-        
-        for probe in probegroup.probes:
+        probe_shape_kwargs = dict(facecolor="w", edgecolor="k", lw=0.5, alpha=1.0)
+        contacts_kargs = dict(alpha=1.0, edgecolor="k", lw=0.5)
 
+        for probe in probegroup.probes:
             text_on_contact = None
             if dp.with_channel_ids:
                 text_on_contact = dp.channel_ids
-            
-            poly_contact, poly_contour = plot_probe(probe, ax=self.ax,
-                                                    contacts_colors='w', contacts_kargs=contacts_kargs,
-                                                    probe_shape_kwargs=probe_shape_kwargs,
-                                                    text_on_contact=text_on_contact)
+
+            poly_contact, poly_contour = plot_probe(
+                probe,
+                ax=self.ax,
+                contacts_colors="w",
+                contacts_kargs=contacts_kargs,
+                probe_shape_kwargs=probe_shape_kwargs,
+                text_on_contact=text_on_contact,
+            )
             poly_contact.set_zorder(2)
             if poly_contour is not None:
                 poly_contour.set_zorder(1)
 
-        self.ax.set_title('')
+        self.ax.set_title("")
 
         # color = np.array([dp.unit_colors[unit_id] for unit_id in dp.unit_ids])
         width = height = 10
         ellipse_kwargs = dict(width=width, height=height, lw=2)
-        
+
         if dp.plot_all_units:
             unit_colors = {}
             unit_ids = dp.all_unit_ids
             for unit in dp.all_unit_ids:
                 if unit not in dp.unit_ids:
                     unit_colors[unit] = "gray"
                 else:
                     unit_colors[unit] = dp.unit_colors[unit]
         else:
             unit_ids = dp.unit_ids
             unit_colors = dp.unit_colors
         labels = dp.unit_ids
 
-        patches = [Ellipse((unit_locations[unit]), color=unit_colors[unit], 
-                           zorder=5 if unit in dp.unit_ids else 3, 
-                           alpha=0.9 if unit in dp.unit_ids else 0.5,
-                            **ellipse_kwargs) for i, unit in enumerate(unit_ids)]
+        patches = [
+            Ellipse(
+                (unit_locations[unit]),
+                color=unit_colors[unit],
+                zorder=5 if unit in dp.unit_ids else 3,
+                alpha=0.9 if unit in dp.unit_ids else 0.5,
+                **ellipse_kwargs,
+            )
+            for i, unit in enumerate(unit_ids)
+        ]
         for p in patches:
             self.ax.add_patch(p)
-        handles = [Line2D([0], [0], ls="", marker='o', markersize=5, markeredgewidth=2, 
-                          color=unit_colors[unit]) for unit in dp.unit_ids]
+        handles = [
+            Line2D([0], [0], ls="", marker="o", markersize=5, markeredgewidth=2, color=unit_colors[unit])
+            for unit in dp.unit_ids
+        ]
 
         if dp.plot_legend:
             if self.legend is not None:
                 self.legend.remove()
-            self.legend = self.figure.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 1.),
-                                             ncol=5, fancybox=True, shadow=True)
+            self.legend = self.figure.legend(
+                handles, labels, loc="upper center", bbox_to_anchor=(0.5, 1.0), ncol=5, fancybox=True, shadow=True
+            )
 
         if dp.hide_axis:
             self.ax.axis("off")
 
 
-
 UnitLocationsPlotter.register(UnitLocationsWidget)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/unit_summary.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/unit_summary.py`

 * *Files 14% similar despite different names*

```diff
@@ -10,29 +10,27 @@
 from .unit_waveforms import UnitWaveformPlotter
 from .unit_waveforms_density_map import UnitWaveformDensityMapPlotter
 
 from .autocorrelograms import AutoCorrelogramsPlotter
 
 
 class UnitSummaryPlotter(MplPlotter):
-
     def do_plot(self, data_plot, **backend_kwargs):
-
         dp = to_attr(data_plot)
-        
+
         # force the figure without axes
-        if 'figsize' not in backend_kwargs:
-            backend_kwargs['figsize'] = (18, 7)
+        if "figsize" not in backend_kwargs:
+            backend_kwargs["figsize"] = (18, 7)
         backend_kwargs = self.update_backend_kwargs(**backend_kwargs)
-        backend_kwargs['num_axes'] = 0
-        backend_kwargs['ax'] = None
-        backend_kwargs['axes'] = None
-        
+        backend_kwargs["num_axes"] = 0
+        backend_kwargs["ax"] = None
+        backend_kwargs["axes"] = None
+
         self.make_mpl_figure(**backend_kwargs)
-        
+
         # and use custum grid spec
         fig = self.figure
         nrows = 2
         ncols = 3
         if dp.plot_data_acc is not None or dp.plot_data_amplitudes is not None:
             ncols += 1
         if dp.plot_data_amplitudes is not None:
@@ -44,31 +42,32 @@
             UnitLocationsPlotter().do_plot(dp.plot_data_unit_locations, ax=ax1)
             x, y = dp.unit_location[0], dp.unit_location[1]
             ax1.set_xlim(x - 80, x + 80)
             ax1.set_ylim(y - 250, y + 250)
             ax1.set_xticks([])
             ax1.set_xlabel(None)
             ax1.set_ylabel(None)
- 
+
         ax2 = fig.add_subplot(gs[:2, 1])
         UnitWaveformPlotter().do_plot(dp.plot_data_waveforms, ax=ax2)
         ax2.set_title(None)
-        
+
         ax3 = fig.add_subplot(gs[:2, 2])
         UnitWaveformDensityMapPlotter().do_plot(dp.plot_data_waveform_density, ax=ax3)
         ax3.set_ylabel(None)
-        
+
         if dp.plot_data_acc is not None:
             ax4 = fig.add_subplot(gs[:2, 3])
             AutoCorrelogramsPlotter().do_plot(dp.plot_data_acc, ax=ax4)
             ax4.set_title(None)
             ax4.set_yticks([])
- 
+
         if dp.plot_data_amplitudes is not None:
             ax5 = fig.add_subplot(gs[2, :3])
             ax6 = fig.add_subplot(gs[2, 3])
             axes = np.array([ax5, ax6])
             AmplitudesPlotter().do_plot(dp.plot_data_amplitudes, axes=axes)
-        
-        fig.suptitle(f'unit_id: {dp.unit_id}')
+
+        fig.suptitle(f"unit_id: {dp.unit_id}")
+
 
 UnitSummaryPlotter.register(UnitSummaryWidget)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/unit_waveforms.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/unit_waveforms.py`

 * *Files 2% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 
     def do_plot(self, data_plot, **backend_kwargs):
         dp = to_attr(data_plot)
 
         backend_kwargs = self.update_backend_kwargs(**backend_kwargs)
 
         if backend_kwargs["axes"] is not None:
-            assert len(backend_kwargs) >= len(dp.units)
+            assert len(backend_kwargs["axes"]) >= len(dp.unit_ids), "Provide as many 'axes' as neurons"
         elif backend_kwargs["ax"] is not None:
             assert dp.same_axis, "If 'same_axis' is not used, provide as many 'axes' as neurons"
         else:
             if dp.same_axis:
                 backend_kwargs["num_axes"] = 1
                 backend_kwargs["ncols"] = None
             else:
@@ -41,15 +41,15 @@
             # plot waveforms
             if dp.plot_waveforms:
                 wfs = dp.wfs_by_ids[unit_id]
                 if dp.unit_selected_waveforms is not None:
                     wfs = wfs[dp.unit_selected_waveforms[unit_id]]
                 elif dp.max_spikes_per_unit is not None:
                     if len(wfs) > dp.max_spikes_per_unit:
-                        random_idxs = np.random.permutation(len(wfs))[:dp.max_spikes_per_unit]
+                        random_idxs = np.random.permutation(len(wfs))[: dp.max_spikes_per_unit]
                         wfs = wfs[random_idxs]
                 wfs = wfs * dp.y_scale + dp.y_offset[None, :, chan_inds]
                 wfs_flat = wfs.swapaxes(1, 2).reshape(wfs.shape[0], -1).T
 
                 if dp.x_offset_units:
                     # 0.7 is to match spacing in xvect
                     xvec = xvectors_flat + i * 0.7 * dp.delta_x
@@ -67,27 +67,29 @@
 
                 if dp.x_offset_units:
                     # 0.7 is to match spacing in xvect
                     xvec = xvectors_flat + i * 0.7 * dp.delta_x
                 else:
                     xvec = xvectors_flat
 
-                ax.plot(xvec, template.T.flatten(), lw=dp.lw_templates, alpha=dp.alpha_templates,
-                        color=color, label=unit_id)
+                ax.plot(
+                    xvec, template.T.flatten(), lw=dp.lw_templates, alpha=dp.alpha_templates, color=color, label=unit_id
+                )
 
                 template_label = dp.unit_ids[i]
                 if dp.set_title:
-                    ax.set_title(f'template {template_label}')
+                    ax.set_title(f"template {template_label}")
 
             # plot channels
             if dp.plot_channels:
                 # TODO enhance this
-                ax.scatter(dp.channel_locations[:, 0], dp.channel_locations[:, 1], color='k')
-            
+                ax.scatter(dp.channel_locations[:, 0], dp.channel_locations[:, 1], color="k")
+
             if dp.same_axis and dp.plot_legend:
                 if self.legend is not None:
                     self.legend.remove()
-                self.legend = self.figure.legend(loc='upper center', bbox_to_anchor=(0.5, 1.),
-                                                 ncol=5, fancybox=True, shadow=True)
+                self.legend = self.figure.legend(
+                    loc="upper center", bbox_to_anchor=(0.5, 1.0), ncol=5, fancybox=True, shadow=True
+                )
 
 
 UnitWaveformPlotter.register(UnitWaveformsWidget)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/matplotlib/unit_waveforms_density_map.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/matplotlib/unit_waveforms_density_map.py`

 * *Files 24% similar despite different names*

```diff
@@ -2,43 +2,52 @@
 
 from ..base import to_attr
 from ..unit_waveforms_density_map import UnitWaveformDensityMapWidget
 from .base_mpl import MplPlotter
 
 
 class UnitWaveformDensityMapPlotter(MplPlotter):
-
     def do_plot(self, data_plot, **backend_kwargs):
         dp = to_attr(data_plot)
         backend_kwargs = self.update_backend_kwargs(**backend_kwargs)
 
         if backend_kwargs["axes"] is not None or backend_kwargs["ax"] is not None:
             self.make_mpl_figure(**backend_kwargs)
         else:
             if dp.same_axis:
                 num_axes = 1
             else:
                 num_axes = len(dp.unit_ids)
             backend_kwargs["ncols"] = 1
             backend_kwargs["num_axes"] = num_axes
             self.make_mpl_figure(**backend_kwargs)
-        
+
         if dp.same_axis:
             ax = self.ax
             hist2d = dp.all_hist2d
-            im = ax.imshow(hist2d.T, interpolation='nearest',
-                           origin='lower', aspect='auto',
-                           extent=(0, hist2d.shape[0], dp.bin_min, dp.bin_max), cmap='hot')
+            im = ax.imshow(
+                hist2d.T,
+                interpolation="nearest",
+                origin="lower",
+                aspect="auto",
+                extent=(0, hist2d.shape[0], dp.bin_min, dp.bin_max),
+                cmap="hot",
+            )
         else:
             for unit_index, unit_id in enumerate(dp.unit_ids):
                 hist2d = dp.all_hist2d[unit_id]
                 ax = self.axes.flatten()[unit_index]
-                im = ax.imshow(hist2d.T, interpolation='nearest',
-                    origin='lower', aspect='auto',
-                    extent=(0, hist2d.shape[0], dp.bin_min, dp.bin_max), cmap='hot')
+                im = ax.imshow(
+                    hist2d.T,
+                    interpolation="nearest",
+                    origin="lower",
+                    aspect="auto",
+                    extent=(0, hist2d.shape[0], dp.bin_min, dp.bin_max),
+                    cmap="hot",
+                )
 
         for unit_index, unit_id in enumerate(dp.unit_ids):
             if dp.same_axis:
                 ax = self.ax
             else:
                 ax = self.axes.flatten()[unit_index]
             color = dp.unit_colors[unit_id]
@@ -51,19 +60,18 @@
                 if unit_index != 0:
                     continue
             else:
                 ax = self.axes.flatten()[unit_index]
             chan_inds = dp.channel_inds[unit_id]
             for i, chan_ind in enumerate(chan_inds):
                 if i != 0:
-                    ax.axvline(i * dp.template_width, color='w', lw=3)
+                    ax.axvline(i * dp.template_width, color="w", lw=3)
                 channel_id = dp.channel_ids[chan_ind]
                 x = i * dp.template_width + dp.template_width // 2
-                y = (dp.bin_max + dp.bin_min) / 2.
-                ax.text(x, y, f'chan_id {channel_id}', color='w', ha='center', va='center')
+                y = (dp.bin_max + dp.bin_min) / 2.0
+                ax.text(x, y, f"chan_id {channel_id}", color="w", ha="center", va="center")
 
             ax.set_xticks([])
-            ax.set_ylabel(f'unit_id {unit_id}')
-
+            ax.set_ylabel(f"unit_id {unit_id}")
 
 
 UnitWaveformDensityMapPlotter.register(UnitWaveformDensityMapWidget)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/metrics.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/metrics.py`

 * *Files 5% similar despite different names*

```diff
@@ -9,24 +9,26 @@
     """
     Plots quality metrics distributions.
 
     Parameters
     ----------
     metrics: pandas.DataFrame
         Data frame with metrics
+    sorting: BaseSorting
+        The sorting object used for metrics calculations
     unit_ids: list
-        List of unit ids.
+        List of unit ids, default None
     skip_metrics: list or None
-        If given, a list of quality metrics to skip
+        If given, a list of quality metrics to skip, default None
     include_metrics: list or None
-        If given, a list of quality metrics to include
+        If given, a list of quality metrics to include, default None
     unit_colors :  dict or None
-        If given, a dictionary with unit ids as keys and colors as values
+        If given, a dictionary with unit ids as keys and colors as values, default None
     hide_unit_selector : bool
-        For sortingview backend, if True the unit selector is not displayed
+        For sortingview backend, if True the unit selector is not displayed, default False
     include_metrics_data :  bool
         If True, metrics data are included in unit table, by default True
     """
 
     possible_backends = {}
 
     def __init__(
@@ -36,15 +38,15 @@
         unit_ids=None,
         include_metrics=None,
         skip_metrics=None,
         unit_colors=None,
         hide_unit_selector=False,
         include_metrics_data=True,
         backend=None,
-        **backend_kwargs
+        **backend_kwargs,
     ):
         if unit_colors is None:
             unit_colors = get_unit_colors(sorting)
 
         if include_metrics is not None:
             selected_metrics = [m for m in metrics.columns if m in include_metrics]
             metrics = metrics[selected_metrics]
@@ -67,11 +69,11 @@
             metrics=metrics,
             sorting=sorting,
             unit_ids=unit_ids,
             include_metrics=include_metrics,
             skip_metrics=skip_metrics,
             unit_colors=unit_colors,
             hide_unit_selector=hide_unit_selector,
-            include_metrics_data=include_metrics_data
+            include_metrics_data=include_metrics_data,
         )
 
         BaseWidget.__init__(self, plot_data, backend=backend, **backend_kwargs)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/quality_metrics.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/quality_metrics.py`

 * *Files 22% similar despite different names*

```diff
@@ -6,45 +6,51 @@
 class QualityMetricsWidget(MetricsBaseWidget):
     """
     Plots quality metrics distributions.
 
     Parameters
     ----------
     waveform_extractor : WaveformExtractor
-        The object to compute/get crosscorrelograms from
+        The object to compute/get quality metrics from
     unit_ids: list
-        List of unit ids.
+        List of unit ids, default None
+    include_metrics: list
+        If given, a list of quality metrics to include, default None
     skip_metrics: list or None
-        If given, a list of quality metrics to skip
+        If given, a list of quality metrics to skip, default None
     unit_colors :  dict or None
-        If given, a dictionary with unit ids as keys and colors as values
+        If given, a dictionary with unit ids as keys and colors as values, default None
     hide_unit_selector : bool
-        For sortingview backend, if True the unit selector is not displayed
+        For sortingview backend, if True the unit selector is not displayed, default False
     """
 
     possible_backends = {}
 
     def __init__(
         self,
         waveform_extractor: WaveformExtractor,
         unit_ids=None,
         include_metrics=None,
         skip_metrics=None,
         unit_colors=None,
         hide_unit_selector=False,
         backend=None,
-        **backend_kwargs
+        **backend_kwargs,
     ):
         self.check_extensions(waveform_extractor, "quality_metrics")
         qlc = waveform_extractor.load_extension("quality_metrics")
         quality_metrics = qlc.get_data()
 
         sorting = waveform_extractor.sorting
 
-        MetricsBaseWidget.__init__(self, quality_metrics, sorting,
-                                   unit_ids=unit_ids, unit_colors=unit_colors,
-                                   include_metrics=include_metrics, 
-                                   skip_metrics=skip_metrics,
-                                   hide_unit_selector=hide_unit_selector,
-                                   backend=backend, **backend_kwargs)
-
-
+        MetricsBaseWidget.__init__(
+            self,
+            quality_metrics,
+            sorting,
+            unit_ids=unit_ids,
+            unit_colors=unit_colors,
+            include_metrics=include_metrics,
+            skip_metrics=skip_metrics,
+            hide_unit_selector=hide_unit_selector,
+            backend=backend,
+            **backend_kwargs,
+        )
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/sorting_summary.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/sorting_summary.py`

 * *Files 8% similar despite different names*

```diff
@@ -11,63 +11,72 @@
 
 from ..core import WaveformExtractor, ChannelSparsity
 
 
 class SortingSummaryWidget(BaseWidget):
     """
     Plots spike sorting summary
-    
+
     Parameters
     ----------
     waveform_extractor : WaveformExtractor
         The waveform extractor object.
     sparsity : ChannelSparsity or None
-        Optional ChannelSparsity to apply.
+        Optional ChannelSparsity to apply, default None
         If WaveformExtractor is already sparse, the argument is ignored
     max_amplitudes_per_unit : int or None
         Maximum number of spikes per unit for plotting amplitudes,
         by default None (all spikes)
     curation : bool
         If True, manual curation is enabled, by default False
         (sortingview backend)
     unit_table_properties : list or None
         List of properties to be added to the unit table, by default None
         (sortingview backend)
     """
+
     possible_backends = {}
 
-    
-    def __init__(self, waveform_extractor: WaveformExtractor, unit_ids=None,
-                 sparsity=None, max_amplitudes_per_unit=None, curation=False,
-                 unit_table_properties=None, label_choices=None, backend=None, **backend_kwargs):
-        self.check_extensions(waveform_extractor, ['correlograms', 'spike_amplitudes',
-                                                   'unit_locations', 'similarity'])
+    def __init__(
+        self,
+        waveform_extractor: WaveformExtractor,
+        unit_ids=None,
+        sparsity=None,
+        max_amplitudes_per_unit=None,
+        curation=False,
+        unit_table_properties=None,
+        label_choices=None,
+        backend=None,
+        **backend_kwargs,
+    ):
+        self.check_extensions(waveform_extractor, ["correlograms", "spike_amplitudes", "unit_locations", "similarity"])
         we = waveform_extractor
         sorting = we.sorting
 
         if unit_ids is None:
             unit_ids = sorting.get_unit_ids()
-    
+
         # use other widgets to generate data (except for similarity)
-        template_plot_data = UnitTemplatesWidget(we, unit_ids=unit_ids, sparsity=sparsity,
-                                                 hide_unit_selector=True).plot_data
+        template_plot_data = UnitTemplatesWidget(
+            we, unit_ids=unit_ids, sparsity=sparsity, hide_unit_selector=True
+        ).plot_data
         ccg_plot_data = CrossCorrelogramsWidget(we, unit_ids=unit_ids, hide_unit_selector=True).plot_data
-        amps_plot_data = AmplitudesWidget(we, unit_ids=unit_ids, max_spikes_per_unit=max_amplitudes_per_unit, 
-                                          hide_unit_selector=True).plot_data
+        amps_plot_data = AmplitudesWidget(
+            we, unit_ids=unit_ids, max_spikes_per_unit=max_amplitudes_per_unit, hide_unit_selector=True
+        ).plot_data
         locs_plot_data = UnitLocationsWidget(we, unit_ids=unit_ids, hide_unit_selector=True).plot_data
         sim_plot_data = TemplateSimilarityWidget(we, unit_ids=unit_ids).plot_data
 
         plot_data = dict(
             waveform_extractor=waveform_extractor,
             unit_ids=unit_ids,
             templates=template_plot_data,
             correlograms=ccg_plot_data,
             amplitudes=amps_plot_data,
             similarity=sim_plot_data,
             unit_locations=locs_plot_data,
             unit_table_properties=unit_table_properties,
             curation=curation,
-            label_choices=label_choices
+            label_choices=label_choices,
         )
 
         BaseWidget.__init__(self, plot_data, backend=backend, **backend_kwargs)
-
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/sortingview/__init__.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/sortingview/__init__.py`

 * *Files identical despite different names*

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/sortingview/amplitudes.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/sortingview/amplitudes.py`

 * *Files 8% similar despite different names*

```diff
@@ -6,33 +6,31 @@
 
 
 class AmplitudesPlotter(SortingviewPlotter):
     default_label = "SpikeInterface - Amplitudes"
 
     def do_plot(self, data_plot, **backend_kwargs):
         import sortingview.views as vv
+
         backend_kwargs = self.update_backend_kwargs(**backend_kwargs)
         dp = to_attr(data_plot)
-        
+
         unit_ids = self.make_serializable(dp.unit_ids)
 
         sa_items = [
             vv.SpikeAmplitudesItem(
                 unit_id=u,
                 spike_times_sec=dp.spiketrains[u].astype("float32"),
-                spike_amplitudes=dp.amplitudes[u].astype("float32")
+                spike_amplitudes=dp.amplitudes[u].astype("float32"),
             )
             for u in unit_ids
         ]
 
         v_spike_amplitudes = vv.SpikeAmplitudes(
-            start_time_sec=0,
-            end_time_sec=dp.total_duration,
-            plots=sa_items,
-            hide_unit_selector=dp.hide_unit_selector
+            start_time_sec=0, end_time_sec=dp.total_duration, plots=sa_items, hide_unit_selector=dp.hide_unit_selector
         )
 
         self.handle_display_and_url(v_spike_amplitudes, **backend_kwargs)
         return v_spike_amplitudes
-        
+
 
 AmplitudesPlotter.register(AmplitudesWidget)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/sortingview/autocorrelograms.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/sortingview/autocorrelograms.py`

 * *Files 10% similar despite different names*

```diff
@@ -4,33 +4,31 @@
 
 
 class AutoCorrelogramsPlotter(SortingviewPlotter):
     default_label = "SpikeInterface - Auto Correlograms"
 
     def do_plot(self, data_plot, **backend_kwargs):
         import sortingview.views as vv
-        
+
         backend_kwargs = self.update_backend_kwargs(**backend_kwargs)
         dp = to_attr(data_plot)
         unit_ids = self.make_serializable(dp.unit_ids)
 
         ac_items = []
         for i in range(len(unit_ids)):
             for j in range(i, len(unit_ids)):
                 if i == j:
                     ac_items.append(
                         vv.AutocorrelogramItem(
                             unit_id=unit_ids[i],
-                            bin_edges_sec=(dp.bins/1000.).astype("float32"),
-                            bin_counts=dp.correlograms[i, j].astype("int32")
+                            bin_edges_sec=(dp.bins / 1000.0).astype("float32"),
+                            bin_counts=dp.correlograms[i, j].astype("int32"),
                         )
                     )
 
-        v_autocorrelograms = vv.Autocorrelograms(
-            autocorrelograms=ac_items
-        )
+        v_autocorrelograms = vv.Autocorrelograms(autocorrelograms=ac_items)
 
         self.handle_display_and_url(v_autocorrelograms, **backend_kwargs)
         return v_autocorrelograms
 
 
 AutoCorrelogramsPlotter.register(AutoCorrelogramsWidget)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/sortingview/base_sortingview.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/sortingview/base_sortingview.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,50 +1,44 @@
 import numpy as np
 
 from ...core.core_tools import check_json
 from spikeinterface.widgets.base import BackendPlotter
 
 
 class SortingviewPlotter(BackendPlotter):
-    backend = 'sortingview'
+    backend = "sortingview"
     backend_kwargs_desc = {
         "generate_url": "If True, the figurl URL is generated and printed. Default True",
         "display": "If True and in jupyter notebook/lab, the widget is displayed in the cell. Default True.",
         "figlabel": "The figurl figure label. Default None",
-        "height": "The height of the sortingview View in jupyter. Default None"
+        "height": "The height of the sortingview View in jupyter. Default None",
     }
-    default_backend_kwargs = {
-        "generate_url": True,
-        "display": True,
-        "figlabel": None,
-        "height": None
-    }
-    
+    default_backend_kwargs = {"generate_url": True, "display": True, "figlabel": None, "height": None}
+
     def __init__(self):
         self.view = None
         self.url = None
 
     def make_serializable(*args):
         dict_to_serialize = {int(i): a for i, a in enumerate(args[1:])}
         serializable_dict = check_json(dict_to_serialize)
         returns = ()
         for i in range(len(args) - 1):
-            returns += (serializable_dict[i],)
+            returns += (serializable_dict[str(i)],)
         if len(returns) == 1:
             returns = returns[0]
         return returns
 
-
     @staticmethod
     def is_notebook() -> bool:
         try:
             shell = get_ipython().__class__.__name__
-            if shell == 'ZMQInteractiveShell':
-                return True   # Jupyter notebook or qtconsole
-            elif shell == 'TerminalInteractiveShell':
+            if shell == "ZMQInteractiveShell":
+                return True  # Jupyter notebook or qtconsole
+            elif shell == "TerminalInteractiveShell":
                 return False  # Terminal running IPython
             else:
                 return False  # Other type (?)
         except NameError:
             return False
 
     def handle_display_and_url(self, view, **backend_kwargs):
@@ -53,32 +47,30 @@
             display(self.view.jupyter(height=backend_kwargs["height"]))
         if backend_kwargs["generate_url"]:
             figlabel = backend_kwargs.get("figlabel")
             if figlabel is None:
                 figlabel = self.default_label
             url = view.url(label=figlabel)
             self.set_url(url)
-            print(url)            
+            print(url)
 
     # make view and url accessible by the plotter
     def set_view(self, view):
         self.view = view
 
     def set_url(self, url):
         self.url = url
 
 
 def generate_unit_table_view(sorting, unit_properties=None, similarity_scores=None):
     import sortingview.views as vv
+
     if unit_properties is None:
         ut_columns = []
-        ut_rows = [
-            vv.UnitsTableRow(unit_id=u, values={})
-            for u in sorting.unit_ids
-        ]
+        ut_rows = [vv.UnitsTableRow(unit_id=u, values={}) for u in sorting.unit_ids]
     else:
         ut_columns = []
         ut_rows = []
         values = {}
         valid_unit_properties = []
         for prop_name in unit_properties:
             property_values = sorting.get_property(prop_name)
@@ -91,23 +83,21 @@
             elif val0.dtype.kind == "f":
                 dtype = "float"
             elif val0.dtype.kind == "b":
                 dtype = "bool"
             else:
                 print(f"Unsupported dtype {val0.dtype} for property {prop_name}. Skipping")
                 continue
-            ut_columns.append(
-                vv.UnitsTableColumn(key=prop_name, label=prop_name, dtype=dtype)
-            )
+            ut_columns.append(vv.UnitsTableColumn(key=prop_name, label=prop_name, dtype=dtype))
             valid_unit_properties.append(prop_name)
-        
+
         for ui, unit in enumerate(sorting.unit_ids):
             for prop_name in valid_unit_properties:
                 property_values = sorting.get_property(prop_name)
                 val0 = property_values[0]
                 if np.isnan(property_values[ui]):
                     continue
                 values[prop_name] = property_values[ui]
             ut_rows.append(vv.UnitsTableRow(unit_id=unit, values=check_json(values)))
-            
+
     v_units_table = vv.UnitsTable(rows=ut_rows, columns=ut_columns, similarity_scores=similarity_scores)
     return v_units_table
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/sortingview/crosscorrelograms.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/sortingview/crosscorrelograms.py`

 * *Files 8% similar despite different names*

```diff
@@ -4,35 +4,34 @@
 
 
 class CrossCorrelogramsPlotter(SortingviewPlotter):
     default_label = "SpikeInterface - Cross Correlograms"
 
     def do_plot(self, data_plot, **backend_kwargs):
         import sortingview.views as vv
-        
+
         backend_kwargs = self.update_backend_kwargs(**backend_kwargs)
         dp = to_attr(data_plot)
-        
+
         unit_ids = self.make_serializable(dp.unit_ids)
 
         cc_items = []
         for i in range(len(unit_ids)):
             for j in range(i, len(unit_ids)):
                 cc_items.append(
                     vv.CrossCorrelogramItem(
                         unit_id1=unit_ids[i],
                         unit_id2=unit_ids[j],
-                        bin_edges_sec=(dp.bins/1000.).astype("float32"),
-                        bin_counts=dp.correlograms[i, j].astype("int32")
+                        bin_edges_sec=(dp.bins / 1000.0).astype("float32"),
+                        bin_counts=dp.correlograms[i, j].astype("int32"),
                     )
                 )
 
         v_cross_correlograms = vv.CrossCorrelograms(
-            cross_correlograms=cc_items,
-            hide_unit_selector=dp.hide_unit_selector
+            cross_correlograms=cc_items, hide_unit_selector=dp.hide_unit_selector
         )
 
         self.handle_display_and_url(v_cross_correlograms, **backend_kwargs)
         return v_cross_correlograms
 
 
 CrossCorrelogramsPlotter.register(CrossCorrelogramsWidget)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/sortingview/metrics.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/sortingview/metrics.py`

 * *Files 6% similar despite different names*

```diff
@@ -2,15 +2,14 @@
 
 from ...core.core_tools import check_json
 from ..base import to_attr
 from .base_sortingview import SortingviewPlotter, generate_unit_table_view
 
 
 class MetricsPlotter(SortingviewPlotter):
-
     def do_plot(self, data_plot, **backend_kwargs):
         import sortingview.views as vv
 
         dp = to_attr(data_plot)
         backend_kwargs = self.update_backend_kwargs(**backend_kwargs)
 
         metrics = dp.metrics
@@ -21,56 +20,42 @@
         else:
             unit_ids = dp.unit_ids
         unit_ids = self.make_serializable(unit_ids)
 
         metrics_sv = []
         for col in metric_names:
             dtype = metrics.iloc[0][col].dtype
-            metric = vv.UnitMetricsGraphMetric(
-                            key=col,
-                            label=col,
-                            dtype=dtype.str
-                        )
+            metric = vv.UnitMetricsGraphMetric(key=col, label=col, dtype=dtype.str)
             metrics_sv.append(metric)
 
         units_m = []
         for unit_id in unit_ids:
             values = check_json(metrics.loc[unit_id].to_dict())
             values_skip_nans = {}
             for k, v in values.items():
                 if np.isnan(v):
                     continue
                 values_skip_nans[k] = v
-            
-            units_m.append(
-                vv.UnitMetricsGraphUnit(
-                    unit_id=unit_id,
-                    values=values_skip_nans
-                )
-            )
-        v_metrics = vv.UnitMetricsGraph(
-                units=units_m,
-                metrics=metrics_sv
-            )
+
+            units_m.append(vv.UnitMetricsGraphUnit(unit_id=unit_id, values=values_skip_nans))
+        v_metrics = vv.UnitMetricsGraph(units=units_m, metrics=metrics_sv)
 
         if not dp.hide_unit_selector:
             if dp.include_metrics_data:
                 # make a view of the sorting to add tmp properties
                 sorting_copy = dp.sorting.select_units(unit_ids=dp.sorting.unit_ids)
                 for col in metric_names:
                     if col not in sorting_copy.get_property_keys():
                         sorting_copy.set_property(col, metrics[col].values)
                 # generate table with properties
                 v_units_table = generate_unit_table_view(sorting_copy, unit_properties=metric_names)
             else:
                 v_units_table = generate_unit_table_view(dp.sorting)
 
             view = vv.Splitter(
-                direction="horizontal",
-                item1=vv.LayoutItem(v_units_table),
-                item2=vv.LayoutItem(v_metrics)
+                direction="horizontal", item1=vv.LayoutItem(v_units_table), item2=vv.LayoutItem(v_metrics)
             )
         else:
             view = v_metrics
 
         self.handle_display_and_url(view, **backend_kwargs)
         return view
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/sortingview/sorting_summary.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/sortingview/sorting_summary.py`

 * *Files 18% similar despite different names*

```diff
@@ -11,79 +11,76 @@
 
 
 class SortingSummaryPlotter(SortingviewPlotter):
     default_label = "SpikeInterface - Sorting Summary"
 
     def do_plot(self, data_plot, **backend_kwargs):
         import sortingview.views as vv
+
         dp = to_attr(data_plot)
 
         unit_ids = self.make_serializable(dp.unit_ids)
 
         backend_kwargs = self.update_backend_kwargs(**backend_kwargs)
 
         amplitudes_plotter = AmplitudesPlotter()
-        v_spike_amplitudes = amplitudes_plotter.do_plot(dp.amplitudes, generate_url=False, 
-                                                        display=False, backend="sortingview")
+        v_spike_amplitudes = amplitudes_plotter.do_plot(
+            dp.amplitudes, generate_url=False, display=False, backend="sortingview"
+        )
         template_plotter = UnitTemplatesPlotter()
-        v_average_waveforms = template_plotter.do_plot(dp.templates, generate_url=False, 
-                                                       display=False, backend="sortingview")
+        v_average_waveforms = template_plotter.do_plot(
+            dp.templates, generate_url=False, display=False, backend="sortingview"
+        )
         xcorrelograms_plotter = CrossCorrelogramsPlotter()
-        v_cross_correlograms = xcorrelograms_plotter.do_plot(dp.correlograms, generate_url=False, 
-                                                             display=False, backend="sortingview")
+        v_cross_correlograms = xcorrelograms_plotter.do_plot(
+            dp.correlograms, generate_url=False, display=False, backend="sortingview"
+        )
         unitlocation_plotter = UnitLocationsPlotter()
-        v_unit_locations = unitlocation_plotter.do_plot(dp.unit_locations, generate_url=False, 
-                                                        display=False, backend="sortingview")
+        v_unit_locations = unitlocation_plotter.do_plot(
+            dp.unit_locations, generate_url=False, display=False, backend="sortingview"
+        )
         # similarity
         similarity_scores = []
         for i1, u1 in enumerate(unit_ids):
             for i2, u2 in enumerate(unit_ids):
-                similarity_scores.append(vv.UnitSimilarityScore(
-                    unit_id1=u1,
-                    unit_id2=u2,
-                    similarity=dp.similarity['similarity'][i1, i2].astype("float32")
-                    ))
+                similarity_scores.append(
+                    vv.UnitSimilarityScore(
+                        unit_id1=u1, unit_id2=u2, similarity=dp.similarity["similarity"][i1, i2].astype("float32")
+                    )
+                )
 
         # unit ids
-        v_units_table = generate_unit_table_view(dp.waveform_extractor.sorting, 
-                                                 dp.unit_table_properties,
-                                                 similarity_scores=similarity_scores)
+        v_units_table = generate_unit_table_view(
+            dp.waveform_extractor.sorting, dp.unit_table_properties, similarity_scores=similarity_scores
+        )
 
         if dp.curation:
             v_curation = vv.SortingCuration2(label_choices=dp.label_choices)
-            v1 = vv.Splitter(
-                direction='vertical',
-                item1=vv.LayoutItem(v_units_table),
-                item2=vv.LayoutItem(v_curation)
-            )
+            v1 = vv.Splitter(direction="vertical", item1=vv.LayoutItem(v_units_table), item2=vv.LayoutItem(v_curation))
         else:
             v1 = v_units_table
         v2 = vv.Splitter(
-                    direction='horizontal',
-                    item1=vv.LayoutItem(v_unit_locations, stretch=0.2),
+            direction="horizontal",
+            item1=vv.LayoutItem(v_unit_locations, stretch=0.2),
+            item2=vv.LayoutItem(
+                vv.Splitter(
+                    direction="horizontal",
+                    item1=vv.LayoutItem(v_average_waveforms),
                     item2=vv.LayoutItem(
                         vv.Splitter(
-                            direction='horizontal',
-                            item1=vv.LayoutItem(v_average_waveforms),
-                            item2=vv.LayoutItem(
-                                vv.Splitter(
-                                    direction='vertical',
-                                    item1=vv.LayoutItem(v_spike_amplitudes),
-                                    item2=vv.LayoutItem(v_cross_correlograms),
-                                        )
-                                    )
-                                )
-                            )
+                            direction="vertical",
+                            item1=vv.LayoutItem(v_spike_amplitudes),
+                            item2=vv.LayoutItem(v_cross_correlograms),
                         )
+                    ),
+                )
+            ),
+        )
 
         # assemble layout
-        v_summary = vv.Splitter(
-            direction='horizontal',
-            item1=vv.LayoutItem(v1),
-            item2=vv.LayoutItem(v2)
-        )
+        v_summary = vv.Splitter(direction="horizontal", item1=vv.LayoutItem(v1), item2=vv.LayoutItem(v2))
 
         self.handle_display_and_url(v_summary, **backend_kwargs)
         return v_summary
 
 
 SortingSummaryPlotter.register(SortingSummaryWidget)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/sortingview/spike_locations.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/sortingview/spike_locations.py`

 * *Files 1% similar despite different names*

```diff
@@ -12,18 +12,15 @@
         backend_kwargs = self.update_backend_kwargs(**backend_kwargs)
         dp = to_attr(data_plot)
         spike_locations = dp.spike_locations
 
         # ensure serializable for sortingview
         unit_ids, channel_ids = self.make_serializable(dp.unit_ids, dp.channel_ids)
 
-        locations = {
-            str(ch): dp.channel_locations[i_ch].astype("float32")
-            for i_ch, ch in enumerate(channel_ids)
-        }
+        locations = {str(ch): dp.channel_locations[i_ch].astype("float32") for i_ch, ch in enumerate(channel_ids)}
         xlims, ylims = estimate_axis_lims(spike_locations)
 
         unit_items = []
         for unit in unit_ids:
             spike_times_sec = dp.sorting.get_unit_spike_train(
                 unit_id=unit, segment_index=dp.segment_index, return_times=True
             )
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/sortingview/template_similarity.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/sortingview/template_similarity.py`

 * *Files 15% similar despite different names*

```diff
@@ -15,23 +15,18 @@
         # ensure serializable for sortingview
         unit_ids = self.make_serializable(dp.unit_ids)
 
         # similarity
         ss_items = []
         for i1, u1 in enumerate(unit_ids):
             for i2, u2 in enumerate(unit_ids):
-                ss_items.append(vv.UnitSimilarityScore(
-                    unit_id1=u1,
-                    unit_id2=u2,
-                    similarity=dp.similarity[i1, i2].astype("float32")
-                ))
+                ss_items.append(
+                    vv.UnitSimilarityScore(unit_id1=u1, unit_id2=u2, similarity=dp.similarity[i1, i2].astype("float32"))
+                )
 
-        view = vv.UnitSimilarityMatrix(
-            unit_ids=list(unit_ids),
-            similarity_scores=ss_items
-        )
+        view = vv.UnitSimilarityMatrix(unit_ids=list(unit_ids), similarity_scores=ss_items)
 
         self.handle_display_and_url(view, **backend_kwargs)
         return view
 
 
 TemplateSimilarityPlotter.register(TemplateSimilarityWidget)
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/sortingview/timeseries.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/sortingview/timeseries.py`

 * *Files 1% similar despite different names*

```diff
@@ -8,28 +8,28 @@
 
 
 class TimeseriesPlotter(SortingviewPlotter):
     default_label = "SpikeInterface - Timeseries"
 
     def do_plot(self, data_plot, **backend_kwargs):
         import sortingview.views as vv
+
         try:
             import pyvips
         except ImportError:
             raise ImportError("To use the timeseries in sorting view you need the pyvips package.")
 
         backend_kwargs = self.update_backend_kwargs(**backend_kwargs)
         dp = to_attr(data_plot)
 
         assert dp.mode == "map", 'sortingview plot_timeseries is only mode="map"'
 
         if not dp.order_channel_by_depth:
             warnings.warn(
-                "It is recommended to set 'order_channel_by_depth' to True "
-                "when using the sortingview backend"
+                "It is recommended to set 'order_channel_by_depth' to True " "when using the sortingview backend"
             )
 
         tiled_layers = []
         for layer_key, traces in zip(dp.layer_keys, dp.list_traces):
             img = array_to_image(
                 traces,
                 clim=dp.clims[layer_key],
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/sortingview/unit_locations.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/sortingview/unit_locations.py`

 * *Files 7% similar despite different names*

```diff
@@ -11,40 +11,33 @@
 
         backend_kwargs = self.update_backend_kwargs(**backend_kwargs)
         dp = to_attr(data_plot)
 
         # ensure serializable for sortingview
         unit_ids, channel_ids = self.make_serializable(dp.unit_ids, dp.channel_ids)
 
-        locations = {str(ch): dp.channel_locations[i_ch].astype("float32")
-                     for i_ch, ch in enumerate(channel_ids)}
+        locations = {str(ch): dp.channel_locations[i_ch].astype("float32") for i_ch, ch in enumerate(channel_ids)}
 
         unit_items = []
         for unit_id in unit_ids:
-            unit_items.append(vv.UnitLocationsItem(
-                unit_id=unit_id,
-                x=float(dp.unit_locations[unit_id][0]),
-                y=float(dp.unit_locations[unit_id][1])
-            ))
-
-        v_unit_locations = vv.UnitLocations(
-            units=unit_items,
-            channel_locations=locations,
-            disable_auto_rotate=True
-        )
+            unit_items.append(
+                vv.UnitLocationsItem(
+                    unit_id=unit_id, x=float(dp.unit_locations[unit_id][0]), y=float(dp.unit_locations[unit_id][1])
+                )
+            )
+
+        v_unit_locations = vv.UnitLocations(units=unit_items, channel_locations=locations, disable_auto_rotate=True)
 
         if not dp.hide_unit_selector:
             v_units_table = generate_unit_table_view(dp.sorting)
 
-            view = vv.Box(direction='horizontal',
-                        items=[
-                            vv.LayoutItem(v_units_table, max_size=150),
-                            vv.LayoutItem(v_unit_locations)
-                        ]
-                    )
+            view = vv.Box(
+                direction="horizontal",
+                items=[vv.LayoutItem(v_units_table, max_size=150), vv.LayoutItem(v_unit_locations)],
+            )
         else:
             view = v_unit_locations
 
         self.handle_display_and_url(view, **backend_kwargs)
         return view
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/sortingview/unit_templates.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/sortingview/unit_templates.py`

 * *Files 23% similar despite different names*

```diff
@@ -7,54 +7,47 @@
     default_label = "SpikeInterface - Unit Templates"
 
     def do_plot(self, data_plot, **backend_kwargs):
         import sortingview.views as vv
 
         dp = to_attr(data_plot)
         backend_kwargs = self.update_backend_kwargs(**backend_kwargs)
-        
+
         # ensure serializable for sortingview
         unit_id_to_channel_ids = dp.sparsity.unit_id_to_channel_ids
         unit_id_to_channel_indices = dp.sparsity.unit_id_to_channel_indices
-        
-        unit_ids, channel_ids, unit_id_to_channel_ids = \
-            self.make_serializable(dp.unit_ids, dp.channel_ids, unit_id_to_channel_ids)
+
+        unit_ids, channel_ids = self.make_serializable(dp.unit_ids, dp.channel_ids)
 
         templates_dict = {}
         for u_i, unit in enumerate(unit_ids):
             templates_dict[unit] = {}
             templates_dict[unit]["mean"] = dp.templates[u_i].T.astype("float32")[unit_id_to_channel_indices[unit]]
             templates_dict[unit]["std"] = dp.template_stds[u_i].T.astype("float32")[unit_id_to_channel_indices[unit]]
 
         aw_items = [
             vv.AverageWaveformItem(
                 unit_id=u,
                 channel_ids=list(unit_id_to_channel_ids[u]),
-                waveform=t['mean'].astype('float32'),
-                waveform_std_dev=t['std'].astype('float32')
+                waveform=t["mean"].astype("float32"),
+                waveform_std_dev=t["std"].astype("float32"),
             )
             for u, t in templates_dict.items()
         ]
 
-        locations = {str(ch): dp.channel_locations[i_ch].astype("float32")
-                     for i_ch, ch in enumerate(channel_ids)}
-        v_average_waveforms = vv.AverageWaveforms(
-            average_waveforms=aw_items,
-            channel_locations=locations
-        )
+        locations = {str(ch): dp.channel_locations[i_ch].astype("float32") for i_ch, ch in enumerate(channel_ids)}
+        v_average_waveforms = vv.AverageWaveforms(average_waveforms=aw_items, channel_locations=locations)
 
         if not dp.hide_unit_selector:
             v_units_table = generate_unit_table_view(dp.waveform_extractor.sorting)
 
-            view = vv.Box(direction='horizontal',
-                        items=[
-                            vv.LayoutItem(v_units_table, max_size=150),
-                            vv.LayoutItem(v_average_waveforms)
-                        ]
-                    )
+            view = vv.Box(
+                direction="horizontal",
+                items=[vv.LayoutItem(v_units_table, max_size=150), vv.LayoutItem(v_average_waveforms)],
+            )
         else:
             view = v_average_waveforms
 
         self.handle_display_and_url(view, **backend_kwargs)
         return view
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/spike_locations.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/spike_locations.py`

 * *Files 4% similar despite different names*

```diff
@@ -10,25 +10,27 @@
     """
     Plots spike locations.
 
     Parameters
     ----------
     waveform_extractor : WaveformExtractor
         The object to compute/get spike locations from
-    unit_ids: list
-        List of unit ids.
-    max_spikes_per_unit: int
+    unit_ids : list
+        List of unit ids, default None
+    segment_index : int or None
+        The segment index (or None if mono-segment), default None
+    max_spikes_per_unit : int
         Number of max spikes per unit to display. Use None for all spikes.
         Default 500.
-    with_channel_ids: bool False default
-        Add channel ids text on the probe
+    with_channel_ids : bool
+        Add channel ids text on the probe, default False
     unit_colors :  dict or None
-        If given, a dictionary with unit ids as keys and colors as values
+        If given, a dictionary with unit ids as keys and colors as values, default None
     hide_unit_selector : bool
-        For sortingview backend, if True the unit selector is not displayed
+        For sortingview backend, if True the unit selector is not displayed, default False
     plot_all_units : bool
         If True, all units are plotted. The unselected ones (not in unit_ids),
         are plotted in grey. Default True (matplotlib backend)
     plot_legend : bool
         If True, the legend is plotted. Default False (matplotlib backend)
     hide_axis : bool
         If True, the axis is set to off. Default False (matplotlib backend)
@@ -45,30 +47,28 @@
         with_channel_ids=False,
         unit_colors=None,
         hide_unit_selector=False,
         plot_all_units=True,
         plot_legend=False,
         hide_axis=False,
         backend=None,
-        **backend_kwargs
+        **backend_kwargs,
     ):
         self.check_extensions(waveform_extractor, "spike_locations")
         slc = waveform_extractor.load_extension("spike_locations")
         spike_locations = slc.get_data(outputs="by_unit")
 
         sorting = waveform_extractor.sorting
 
         channel_ids = waveform_extractor.channel_ids
         channel_locations = waveform_extractor.get_channel_locations()
         probegroup = waveform_extractor.get_probegroup()
 
         if sorting.get_num_segments() > 1:
-            assert (
-                segment_index is not None
-            ), "Specify segment index for multi-segment object"
+            assert segment_index is not None, "Specify segment index for multi-segment object"
         else:
             segment_index = 0
 
         if unit_colors is None:
             unit_colors = get_unit_colors(sorting)
 
         if unit_ids is None:
@@ -96,22 +96,20 @@
             unit_colors=unit_colors,
             channel_locations=channel_locations,
             probegroup_dict=probegroup.to_dict(),
             with_channel_ids=with_channel_ids,
             hide_unit_selector=hide_unit_selector,
             plot_all_units=plot_all_units,
             plot_legend=plot_legend,
-            hide_axis=hide_axis
+            hide_axis=hide_axis,
         )
 
         BaseWidget.__init__(self, plot_data, backend=backend, **backend_kwargs)
 
 
 def estimate_axis_lims(spike_locations, quantile=0.02):
     # set proper axis limits
     all_locs = np.concatenate(list(spike_locations.values()))
     xlims = np.quantile(all_locs["x"], [quantile, 1 - quantile])
     ylims = np.quantile(all_locs["y"], [quantile, 1 - quantile])
 
     return xlims, ylims
-
-
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/spikes_on_traces.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/spikes_on_traces.py`

 * *Files 25% similar despite different names*

```diff
@@ -13,89 +13,123 @@
 
 class SpikesOnTracesWidget(BaseWidget):
     """
     Plots unit spikes/waveforms over traces.
 
     Parameters
     ----------
-    waveform_extractor: WaveformExtractor
+    waveform_extractor : WaveformExtractor
         The waveform extractor
-    channel_ids: list
-        The channel ids to display
-    unit_ids: list
-        List of unit ids.
-    plot_templates: bool
-        If True, templates are plotted over the waveforms
+    channel_ids : list
+        The channel ids to display, default None
+    unit_ids : list
+        List of unit ids, default None
+    order_channel_by_depth : bool
+        If true orders channel by depth, default False
+    time_range: list
+        List with start time and end time, default None
     sparsity : ChannelSparsity or None
         Optional ChannelSparsity to apply.
-        If WaveformExtractor is already sparse, the argument is ignored
-    set_title: bool
-        Create a plot title with the unit number if True.
-    plot_channels: bool
-        Plot channel locations below traces.
-    unit_selected_waveforms: None or dict
-        A dict key is unit_id and value is the subset of waveforms indices that should be 
-        be displayed (matplotlib backend)
-    max_spikes_per_unit: int or None
-        If given and unit_selected_waveforms is None, only max_spikes_per_unit random units are
-        displayed per waveform, default 50 (matplotlib backend)
-    axis_equal: bool
-        Equal aspect ratio for x and y axis, to visualize the array geometry to scale.
-    lw_waveforms: float
-        Line width for the waveforms, default 1 (matplotlib backend)
-    lw_templates: float
-        Line width for the templates, default 2 (matplotlib backend)
-    unit_colors: None or dict
-        A dict key is unit_id and value is any color format handled by matplotlib.
+        If WaveformExtractor is already sparse, the argument is ignored, default None
+       unit_colors :  dict or None
+        If given, a dictionary with unit ids as keys and colors as values, default None
         If None, then the get_unit_colors() is internally used. (matplotlib backend)
-    alpha_waveforms: float
-        Alpha value for waveforms, default 0.5 (matplotlib backend)
-    alpha_templates: float
-        Alpha value for templates, default 1 (matplotlib backend)
-    same_axis: bool
-        If True, waveforms and templates are diplayed on the same axis, default False (matplotlib backend)
-    x_offset_units: bool
-        In case same_axis is True, this parameter allow to x-offset the waveforms for different units 
-        (recommended for a few units), default False (matlotlib backend)
+    mode : str
+        Three possible modes, default 'auto':
+        * 'line': classical for low channel count
+        * 'map': for high channel count use color heat map
+        * 'auto': auto switch depending on the channel count ('line' if less than 64 channels, 'map' otherwise)
+    return_scaled : bool
+        If True and the recording has scaled traces, it plots the scaled traces, default False
+    cmap : str
+        matplotlib colormap used in mode 'map', default 'RdBu'
+    show_channel_ids : bool
+        Set yticks with channel ids, default False
+    color_groups : bool
+        If True groups are plotted with different colors, default False
+    color : str
+        The color used to draw the traces, default None
+    clim : None, tuple or dict
+        When mode is 'map', this argument controls color limits.
+        If dict, keys should be the same as recording keys
+        Default None
+    with_colorbar : bool
+        When mode is 'map', a colorbar is added, by default True
+    tile_size : int
+        For sortingview backend, the size of each tile in the rendered image, default 512
+    seconds_per_row : float
+        For 'map' mode and sortingview backend, seconds to render in each row, default 0.2
     """
+
     possible_backends = {}
 
-    def __init__(self, waveform_extractor: WaveformExtractor, 
-                 segment_index=None, channel_ids=None, unit_ids=None, order_channel_by_depth=False,
-                 time_range=None, unit_colors=None, sparsity=None, 
-                 mode='auto', return_scaled=False, cmap='RdBu', show_channel_ids=False,
-                 color_groups=False, color=None, clim=None, tile_size=512, seconds_per_row=0.2, 
-                 with_colorbar=True, backend=None, **backend_kwargs):
+    def __init__(
+        self,
+        waveform_extractor: WaveformExtractor,
+        segment_index=None,
+        channel_ids=None,
+        unit_ids=None,
+        order_channel_by_depth=False,
+        time_range=None,
+        unit_colors=None,
+        sparsity=None,
+        mode="auto",
+        return_scaled=False,
+        cmap="RdBu",
+        show_channel_ids=False,
+        color_groups=False,
+        color=None,
+        clim=None,
+        tile_size=512,
+        seconds_per_row=0.2,
+        with_colorbar=True,
+        backend=None,
+        **backend_kwargs,
+    ):
         we = waveform_extractor
         recording: BaseRecording = we.recording
         sorting: BaseSorting = we.sorting
-        
-        ts_widget = TimeseriesWidget(recording, segment_index, channel_ids, order_channel_by_depth,
-                                     time_range, mode, return_scaled, cmap, show_channel_ids, color_groups, color, clim, 
-                                     tile_size, seconds_per_row, with_colorbar, backend, **backend_kwargs)
+
+        ts_widget = TimeseriesWidget(
+            recording,
+            segment_index,
+            channel_ids,
+            order_channel_by_depth,
+            time_range,
+            mode,
+            return_scaled,
+            cmap,
+            show_channel_ids,
+            color_groups,
+            color,
+            clim,
+            tile_size,
+            seconds_per_row,
+            with_colorbar,
+            backend,
+            **backend_kwargs,
+        )
 
         if unit_ids is None:
             unit_ids = sorting.get_unit_ids()
         unit_ids = unit_ids
-        
+
         if unit_colors is None:
             unit_colors = get_unit_colors(sorting)
 
         # sparsity is done on all the units even if unit_ids is a few ones because some backend need then all
         if waveform_extractor.is_sparse():
             sparsity = waveform_extractor.sparsity
         else:
             if sparsity is None:
                 # in this case, we construct a sparsity dictionary only with the best channel
                 extremum_channel_ids = get_template_extremum_channel(we)
                 unit_id_to_channel_ids = {u: [ch] for u, ch in extremum_channel_ids.items()}
                 sparsity = ChannelSparsity.from_unit_id_to_channel_ids(
-                    unit_id_to_channel_ids=unit_id_to_channel_ids,
-                    unit_ids=we.unit_ids,
-                    channel_ids=we.channel_ids
+                    unit_id_to_channel_ids=unit_id_to_channel_ids, unit_ids=we.unit_ids, channel_ids=we.channel_ids
                 )
             else:
                 assert isinstance(sparsity, ChannelSparsity)
 
         # get templates
         unit_locations = compute_unit_locations(we, outputs="by_unit")
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/template_metrics.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/template_metrics.py`

 * *Files 6% similar despite different names*

```diff
@@ -5,47 +5,51 @@
 class TemplateMetricsWidget(MetricsBaseWidget):
     """
     Plots template metrics distributions.
 
     Parameters
     ----------
     waveform_extractor : WaveformExtractor
-        The object to compute/get crosscorrelograms from
-    unit_ids: list
-        List of unit ids.
-    skip_metrics: list or None
-        If given, a list of quality metrics to skip
-    compute_kwargs : dict or None
-        If given, dictionary with keyword arguments for "compute_template_metrics" function
+        The object to compute/get template metrics from
+    unit_ids : list
+        List of unit ids, default None
+    include_metrics : list
+        If given list of quality metrics to include, default None
+    skip_metrics : list or None
+        If given, a list of quality metrics to skip, default None
     unit_colors :  dict or None
-        If given, a dictionary with unit ids as keys and colors as values
+        If given, a dictionary with unit ids as keys and colors as values, default None
     hide_unit_selector : bool
-        For sortingview backend, if True the unit selector is not displayed
+        For sortingview backend, if True the unit selector is not displayed, default False
     """
 
     possible_backends = {}
 
     def __init__(
         self,
         waveform_extractor: WaveformExtractor,
         unit_ids=None,
         include_metrics=None,
         skip_metrics=None,
         unit_colors=None,
         hide_unit_selector=False,
         backend=None,
-        **backend_kwargs
+        **backend_kwargs,
     ):
         self.check_extensions(waveform_extractor, "template_metrics")
         tmc = waveform_extractor.load_extension("template_metrics")
         template_metrics = tmc.get_data()
 
         sorting = waveform_extractor.sorting
 
-        MetricsBaseWidget.__init__(self, template_metrics, sorting,
-                                   unit_ids=unit_ids, unit_colors=unit_colors,
-                                   include_metrics=include_metrics, 
-                                   skip_metrics=skip_metrics,
-                                   hide_unit_selector=hide_unit_selector,
-                                   backend=backend, **backend_kwargs)
-
-
+        MetricsBaseWidget.__init__(
+            self,
+            template_metrics,
+            sorting,
+            unit_ids=unit_ids,
+            unit_colors=unit_colors,
+            include_metrics=include_metrics,
+            skip_metrics=skip_metrics,
+            hide_unit_selector=hide_unit_selector,
+            backend=backend,
+            **backend_kwargs,
+        )
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/template_similarity.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/template_similarity.py`

 * *Files 17% similar despite different names*

```diff
@@ -4,38 +4,46 @@
 from .base import BaseWidget
 from ..core.waveform_extractor import WaveformExtractor
 from ..core.basesorting import BaseSorting
 
 
 class TemplateSimilarityWidget(BaseWidget):
     """
-    Plots unit cross correlograms.
+    Plots unit template similarity.
 
     Parameters
     ----------
     waveform_extractor : WaveformExtractor
         The object to compute/get template similarity from
     unit_ids : list
-        List of unit ids.
+        List of unit ids default None
     display_diagonal_values : bool
         If False, the diagonal is displayed as zeros.
-        If True, the similarity values (all 1s) are displayed. Default False
-    cmap : Matplotlib colormap
-        The matplotlib colormap. Default 'viridis'. (matplotlib backend)
+        If True, the similarity values (all 1s) are displayed, default False
+    cmap : str
+        The matplotlib colormap. Default 'viridis'.
     show_unit_ticks : bool
-        If True, ticks display unit ids. Default False. (matplotlib backend)
+        If True, ticks display unit ids, default False.
     show_colorbar : bool
-        If True, color bar is displayed. Default True. (matplotlib backend)
+        If True, color bar is displayed, default True.
     """
+
     possible_backends = {}
 
-    def __init__(self, waveform_extractor: WaveformExtractor,
-                 unit_ids=None, cmap='viridis', display_diagonal_values=False,
-                 show_unit_ticks=False, show_colorbar=True,
-                 backend=None, **backend_kwargs):
+    def __init__(
+        self,
+        waveform_extractor: WaveformExtractor,
+        unit_ids=None,
+        cmap="viridis",
+        display_diagonal_values=False,
+        show_unit_ticks=False,
+        show_colorbar=True,
+        backend=None,
+        **backend_kwargs,
+    ):
         self.check_extensions(waveform_extractor, "similarity")
         tsc = waveform_extractor.load_extension("similarity")
         similarity = tsc.get_data().copy()
 
         sorting = waveform_extractor.sorting
         if unit_ids is None:
             unit_ids = sorting.unit_ids
@@ -47,14 +55,11 @@
             np.fill_diagonal(similarity, 0)
 
         plot_data = dict(
             similarity=similarity,
             unit_ids=unit_ids,
             cmap=cmap,
             show_unit_ticks=show_unit_ticks,
-            show_colorbar=show_colorbar
+            show_colorbar=show_colorbar,
         )
 
         BaseWidget.__init__(self, plot_data, backend=backend, **backend_kwargs)
-
-
-
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/timeseries.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/timeseries.py`

 * *Files 9% similar despite different names*

```diff
@@ -8,80 +8,99 @@
 class TimeseriesWidget(BaseWidget):
     """
     Plots recording timeseries.
 
     Parameters
     ----------
     recording: RecordingExtractor, dict, or list
-        The recording extractor object. If dict (or list) then it is a multi-layer display to compare, for example, 
+        The recording extractor object. If dict (or list) then it is a multi-layer display to compare, for example,
         different processing steps
     segment_index: None or int
-        The segment index (required for multi-segment recordings)
+        The segment index (required for multi-segment recordings), default None
     channel_ids: list
-        The channel ids to display.
+        The channel ids to display, default None
     order_channel_by_depth: bool
-        Reorder channel by depth.
+        Reorder channel by depth, default False
     time_range: list
-        List with start time and end time
+        List with start time and end time, default None
     mode: str
-        Three possible modes:
-
+        Three possible modes, default 'auto':
         * 'line': classical for low channel count
         * 'map': for high channel count use color heat map
-        * 'auto': auto switch depending the channel count ('line' if less than 64 channels, 'map' otherwise)
+        * 'auto': auto switch depending on the channel count ('line' if less than 64 channels, 'map' otherwise)
     return_scaled: bool
-        If True and the recording has scaled traces, it plots the scaled traces, by default False
+        If True and the recording has scaled traces, it plots the scaled traces, default False
     cmap: str
-        matplotlib colormap used in mode 'map', by default 'RdBu'
+        matplotlib colormap used in mode 'map', default 'RdBu'
     show_channel_ids: bool
-        Set yticks with channel ids
+        Set yticks with channel ids, default False
     color_groups: bool
-        If True groups are plotted with different colors, by default False
+        If True groups are plotted with different colors, default False
     color: str
-        The color used to draw the traces, by default None
+        The color used to draw the traces, default None
     clim: None, tuple or dict
         When mode is 'map', this argument controls color limits.
         If dict, keys should be the same as recording keys
+        Default None
     with_colorbar: bool
         When mode is 'map', a colorbar is added, by default True
     tile_size: int
-        For sortingview backend, the size of each tile in the rendered image
+        For sortingview backend, the size of each tile in the rendered image, default 1500
     seconds_per_row: float
-        For 'map' mode and sortingview backend, seconds to reder in each row
+        For 'map' mode and sortingview backend, seconds to render in each row, default 0.2
+    add_legend : bool
+        If True adds legend to figures, default True
 
     Returns
     -------
     W: TimeseriesWidget
         The output widget
     """
+
     possible_backends = {}
-    
 
-    def __init__(self, recording, segment_index=None, channel_ids=None, order_channel_by_depth=False,
-                 time_range=None, mode='auto', return_scaled=False, cmap='RdBu_r', show_channel_ids=False,
-                 color_groups=False, color=None, clim=None, tile_size=1500, seconds_per_row=0.2,
-                 with_colorbar=True, add_legend=True, backend=None, **backend_kwargs):
+    def __init__(
+        self,
+        recording,
+        segment_index=None,
+        channel_ids=None,
+        order_channel_by_depth=False,
+        time_range=None,
+        mode="auto",
+        return_scaled=False,
+        cmap="RdBu_r",
+        show_channel_ids=False,
+        color_groups=False,
+        color=None,
+        clim=None,
+        tile_size=1500,
+        seconds_per_row=0.2,
+        with_colorbar=True,
+        add_legend=True,
+        backend=None,
+        **backend_kwargs,
+    ):
         if isinstance(recording, BaseRecording):
-            recordings = {'rec': recording}
+            recordings = {"rec": recording}
             rec0 = recording
         elif isinstance(recording, dict):
             recordings = recording
             k0 = list(recordings.keys())[0]
             rec0 = recordings[k0]
         elif isinstance(recording, list):
-            recordings = {f'rec{i}': rec for i, rec in enumerate(recording)}
-            rec0= recordings[0]
+            recordings = {f"rec{i}": rec for i, rec in enumerate(recording)}
+            rec0 = recordings[0]
         else:
-            raise ValueError('plot_timeseries recording must be recording or dict or list')
+            raise ValueError("plot_timeseries recording must be recording or dict or list")
 
         layer_keys = list(recordings.keys())
 
         if segment_index is None:
             if rec0.get_num_segments() != 1:
-                raise ValueError('You must provide segment_index=...')
+                raise ValueError("You must provide segment_index=...")
             segment_index = 0
 
         if channel_ids is None:
             channel_ids = rec0.channel_ids
 
         if "location" in rec0.get_property_keys():
             channel_locations = rec0.get_channel_locations()
@@ -92,49 +111,50 @@
             if channel_locations is not None:
                 order, _ = order_channels_by_depth(rec0, channel_ids)
         else:
             order = None
 
         fs = rec0.get_sampling_frequency()
         if time_range is None:
-            time_range = (0, 1.)
+            time_range = (0, 1.0)
         time_range = np.array(time_range)
 
-        assert mode in ('auto', 'line', 'map'), 'Mode must be in auto/line/map'
-        if mode == 'auto':
+        assert mode in ("auto", "line", "map"), "Mode must be in auto/line/map"
+        if mode == "auto":
             if len(channel_ids) <= 64:
-                mode = 'line'
+                mode = "line"
             else:
-                mode = 'map'
+                mode = "map"
         mode = mode
         cmap = cmap
-        
-        times, list_traces, frame_range, channel_ids = _get_trace_list(recordings, channel_ids, time_range, 
-                                                                       segment_index, order, return_scaled)
-        
+
+        times, list_traces, frame_range, channel_ids = _get_trace_list(
+            recordings, channel_ids, time_range, segment_index, order, return_scaled
+        )
+
         # stat for auto scaling done on the first layer
         traces0 = list_traces[0]
         mean_channel_std = np.mean(np.std(traces0, axis=0))
         max_channel_amp = np.max(np.max(np.abs(traces0), axis=0))
         vspacing = max_channel_amp * 1.5
 
         if rec0.get_channel_groups() is None:
             color_groups = False
 
         # colors is a nested dict by layer and channels
         # lets first create black for all channels and layer
         colors = {}
         for k in layer_keys:
-            colors[k] = {chan_id: 'k' for chan_id in channel_ids}
+            colors[k] = {chan_id: "k" for chan_id in channel_ids}
 
         if color_groups:
             channel_groups = rec0.get_channel_groups(channel_ids=channel_ids)
             groups = np.unique(channel_groups)
 
-            group_colors = get_some_colors(groups, color_engine='auto')
+            group_colors = get_some_colors(groups, color_engine="auto")
 
             channel_colors = {}
             for i, chan_id in enumerate(channel_ids):
                 group = channel_groups[i]
                 channel_colors[chan_id] = group_colors[group]
 
             # first layer is colored then black
@@ -148,15 +168,15 @@
             # several layer
             layer_colors = get_some_colors(layer_keys)
             for k in layer_keys:
                 colors[k] = {chan_id: layer_colors[k] for chan_id in channel_ids}
         else:
             # color is None unique layer : all channels black
             pass
-        
+
         if clim is None:
             clims = {layer_key: [-200, 200] for layer_key in layer_keys}
         else:
             if isinstance(clim, tuple):
                 clims = {layer_key: clim for layer_key in layer_keys}
             elif isinstance(clim, dict):
                 assert all(layer_key in clim for layer_key in layer_keys), ""
@@ -184,48 +204,48 @@
             colors=colors,
             show_channel_ids=show_channel_ids,
             add_legend=add_legend,
             order_channel_by_depth=order_channel_by_depth,
             order=order,
             tile_size=tile_size,
             num_timepoints_per_row=int(seconds_per_row * fs),
-            return_scaled=return_scaled
+            return_scaled=return_scaled,
         )
 
         BaseWidget.__init__(self, plot_data, backend=backend, **backend_kwargs)
 
 
-def _get_trace_list(recordings, channel_ids, time_range, segment_index, order=None,
-                    return_scaled=False):
+def _get_trace_list(recordings, channel_ids, time_range, segment_index, order=None, return_scaled=False):
     # function also used in ipywidgets plotter
     k0 = list(recordings.keys())[0]
     rec0 = recordings[k0]
 
     fs = rec0.get_sampling_frequency()
-    
+
     if return_scaled:
-        assert all(rec.has_scaled() for rec in recordings.values()), \
-            ("Some recording layers do not have scaled traces. Use `return_scaled=False`")
-    frame_range = (time_range * fs).astype('int64')
+        assert all(
+            rec.has_scaled() for rec in recordings.values()
+        ), "Some recording layers do not have scaled traces. Use `return_scaled=False`"
+    frame_range = (time_range * fs).astype("int64")
     a_max = rec0.get_num_frames(segment_index=segment_index)
     frame_range = np.clip(frame_range, 0, a_max)
     time_range = frame_range / fs
     times = np.arange(frame_range[0], frame_range[1]) / fs
 
     list_traces = []
     for rec_name, rec in recordings.items():
         traces = rec.get_traces(
             segment_index=segment_index,
             channel_ids=channel_ids,
             start_frame=frame_range[0],
             end_frame=frame_range[1],
-            return_scaled=return_scaled
+            return_scaled=return_scaled,
         )
 
         if order is not None:
             traces = traces[:, order]
         list_traces.append(traces)
 
     if order is not None:
         channel_ids = np.array(channel_ids)[order]
 
-    return times, list_traces, frame_range, channel_ids
+    return times, list_traces, frame_range, channel_ids
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/unit_depths.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/unit_depths.py`

 * *Files 10% similar despite different names*

```diff
@@ -4,59 +4,55 @@
 from .base import BaseWidget
 from .utils import get_unit_colors
 
 
 from ..core.template_tools import get_template_extremum_amplitude
 
 
-
 class UnitDepthsWidget(BaseWidget):
     """
     Plot unit depths
 
     Parameters
     ----------
-    waveform_extractor: WaveformExtractor
+    waveform_extractor : WaveformExtractor
         The input waveform extractor
     unit_colors :  dict or None
-        If given, a dictionary with unit ids as keys and colors as values
-    depth_axis: int default 1
-        Which dimension of unit_locations is depth. 1 by defaults
+        If given, a dictionary with unit ids as keys and colors as values, default None
+    depth_axis : int
+        The dimension of unit_locations that is depth, default 1
     peak_sign: str (neg/pos/both)
-        Sign of peak for amplitudes.
+        Sign of peak for amplitudes, default 'neg'
     """
-    possible_backends = {}
 
-    
-    def __init__(self, waveform_extractor, unit_colors=None, depth_axis=1,
-                peak_sign='neg',
-                 backend=None, **backend_kwargs):
+    possible_backends = {}
 
+    def __init__(
+        self, waveform_extractor, unit_colors=None, depth_axis=1, peak_sign="neg", backend=None, **backend_kwargs
+    ):
         we = waveform_extractor
         unit_ids = we.sorting.unit_ids
 
         if unit_colors is None:
             unit_colors = get_unit_colors(we.sorting)
 
         colors = [unit_colors[unit_id] for unit_id in unit_ids]
 
-
         self.check_extensions(waveform_extractor, "unit_locations")
         ulc = waveform_extractor.load_extension("unit_locations")
         unit_locations = ulc.get_data(outputs="numpy")
         unit_depths = unit_locations[:, depth_axis]
 
         unit_amplitudes = get_template_extremum_amplitude(we, peak_sign=peak_sign)
         unit_amplitudes = np.abs([unit_amplitudes[unit_id] for unit_id in unit_ids])
 
         num_spikes = np.array(list(we.sorting.get_total_num_spikes().values()))
 
         plot_data = dict(
-                unit_depths=unit_depths,
-                unit_amplitudes=unit_amplitudes,
-                num_spikes=num_spikes,
-                unit_colors=unit_colors,
-                colors=colors,
+            unit_depths=unit_depths,
+            unit_amplitudes=unit_amplitudes,
+            num_spikes=num_spikes,
+            unit_colors=unit_colors,
+            colors=colors,
         )
 
         BaseWidget.__init__(self, plot_data, backend=backend, **backend_kwargs)
-
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/unit_locations.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/unit_locations.py`

 * *Files 9% similar despite different names*

```diff
@@ -2,60 +2,67 @@
 from typing import Union
 
 from .base import BaseWidget
 from .utils import get_unit_colors
 from ..core.waveform_extractor import WaveformExtractor
 
 
-
 class UnitLocationsWidget(BaseWidget):
     """
-    Plots unit locations.
+    Plots each unit's location.
 
     Parameters
     ----------
     waveform_extractor : WaveformExtractor
         The object to compute/get unit locations from
-    unit_ids: list
-        List of unit ids.
-    with_channel_ids: bool False default
-        Add channel ids text on the probe
+    unit_ids : list
+        List of unit ids default None
+    with_channel_ids : bool
+        Add channel ids text on the probe, default False
     unit_colors :  dict or None
-        If given, a dictionary with unit ids as keys and colors as values
+        If given, a dictionary with unit ids as keys and colors as values, default None
     hide_unit_selector : bool
-        If True, the unit selector is not displayed.
-        Default False (sortingview backend)
+        If True, the unit selector is not displayed, default False (sortingview backend)
     plot_all_units : bool
         If True, all units are plotted. The unselected ones (not in unit_ids),
-        are plotted in grey. Default True (matplotlib backend)
+        are plotted in grey, default True (matplotlib backend)
     plot_legend : bool
-        If True, the legend is plotted. Default False (matplotlib backend)
+        If True, the legend is plotted, default False (matplotlib backend)
     hide_axis : bool
-        If True, the axis is set to off. Default False (matplotlib backend)
+        If True, the axis is set to off, default False (matplotlib backend)
     """
+
     possible_backends = {}
 
-    def __init__(self, waveform_extractor: WaveformExtractor, 
-                 unit_ids=None, with_channel_ids=False,
-                 unit_colors=None, hide_unit_selector=False,
-                 plot_all_units=True, plot_legend=False, hide_axis=False,
-                 backend=None, **backend_kwargs):
+    def __init__(
+        self,
+        waveform_extractor: WaveformExtractor,
+        unit_ids=None,
+        with_channel_ids=False,
+        unit_colors=None,
+        hide_unit_selector=False,
+        plot_all_units=True,
+        plot_legend=False,
+        hide_axis=False,
+        backend=None,
+        **backend_kwargs,
+    ):
         self.check_extensions(waveform_extractor, "unit_locations")
         ulc = waveform_extractor.load_extension("unit_locations")
         unit_locations = ulc.get_data(outputs="by_unit")
 
         sorting = waveform_extractor.sorting
-        
+
         channel_ids = waveform_extractor.channel_ids
         channel_locations = waveform_extractor.get_channel_locations()
         probegroup = waveform_extractor.get_probegroup()
-        
+
         if unit_colors is None:
             unit_colors = get_unit_colors(sorting)
-        
+
         if unit_ids is None:
             unit_ids = sorting.unit_ids
 
         plot_data = dict(
             all_unit_ids=sorting.unit_ids,
             unit_locations=unit_locations,
             sorting=sorting,
@@ -64,14 +71,11 @@
             unit_colors=unit_colors,
             channel_locations=channel_locations,
             probegroup_dict=probegroup.to_dict(),
             with_channel_ids=with_channel_ids,
             hide_unit_selector=hide_unit_selector,
             plot_all_units=plot_all_units,
             plot_legend=plot_legend,
-            hide_axis=hide_axis
+            hide_axis=hide_axis,
         )
 
         BaseWidget.__init__(self, plot_data, backend=backend, **backend_kwargs)
-
-
-
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/unit_summary.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/unit_summary.py`

 * *Files 12% similar despite different names*

```diff
@@ -11,77 +11,92 @@
 from .autocorrelograms import AutoCorrelogramsWidget
 from .amplitudes import AmplitudesWidget
 
 
 class UnitSummaryWidget(BaseWidget):
     """
     Plot a unit summary.
-    
+
     If amplitudes are alreday computed they are displayed.
-    
+
     Parameters
     ----------
-    waveform_extractor: WaveformExtractor
+    waveform_extractor : WaveformExtractor
         The waveform extractor object
-    unit_id: into or str
+    unit_id : int or str
         The unit id to plot the summary of
     unit_colors :  dict or None
-        If given, a dictionary with unit ids as keys and colors as values
+        If given, a dictionary with unit ids as keys and colors as values, default None
     sparsity : ChannelSparsity or None
-        Optional ChannelSparsity to apply.
+        Optional ChannelSparsity to apply, default None
         If WaveformExtractor is already sparse, the argument is ignored
     """
+
     possible_backends = {}
 
-    def __init__(self, waveform_extractor, unit_id, unit_colors=None,
-                 sparsity=None, radius_um=100, backend=None, **backend_kwargs):
-        
+    def __init__(
+        self,
+        waveform_extractor,
+        unit_id,
+        unit_colors=None,
+        sparsity=None,
+        radius_um=100,
+        backend=None,
+        **backend_kwargs,
+    ):
         we = waveform_extractor
 
         if unit_colors is None:
             unit_colors = get_unit_colors(we.sorting)
-                
-        if we.is_extension('unit_locations'):
-            plot_data_unit_locations = UnitLocationsWidget(we, unit_ids=[unit_id], 
-                                                           unit_colors=unit_colors, plot_legend=False).plot_data
+
+        if we.is_extension("unit_locations"):
+            plot_data_unit_locations = UnitLocationsWidget(
+                we, unit_ids=[unit_id], unit_colors=unit_colors, plot_legend=False
+            ).plot_data
             unit_locations = waveform_extractor.load_extension("unit_locations").get_data(outputs="by_unit")
             unit_location = unit_locations[unit_id]
         else:
             plot_data_unit_locations = None
             unit_location = None
 
-        plot_data_waveforms = UnitWaveformsWidget(we, unit_ids=[unit_id], unit_colors=unit_colors,
-                                                  plot_templates=True, same_axis=True, plot_legend=False,
-                                                  sparsity=sparsity).plot_data
-        
-        plot_data_waveform_density = UnitWaveformDensityMapWidget(we, unit_ids=[unit_id], unit_colors=unit_colors,
-                                                                  use_max_channel=True, plot_templates=True,
-                                                                  same_axis=False).plot_data
-        
-        if we.is_extension('correlograms'):
-            plot_data_acc = AutoCorrelogramsWidget(we, unit_ids=[unit_id], unit_colors=unit_colors,).plot_data
+        plot_data_waveforms = UnitWaveformsWidget(
+            we,
+            unit_ids=[unit_id],
+            unit_colors=unit_colors,
+            plot_templates=True,
+            same_axis=True,
+            plot_legend=False,
+            sparsity=sparsity,
+        ).plot_data
+
+        plot_data_waveform_density = UnitWaveformDensityMapWidget(
+            we, unit_ids=[unit_id], unit_colors=unit_colors, use_max_channel=True, plot_templates=True, same_axis=False
+        ).plot_data
+
+        if we.is_extension("correlograms"):
+            plot_data_acc = AutoCorrelogramsWidget(
+                we,
+                unit_ids=[unit_id],
+                unit_colors=unit_colors,
+            ).plot_data
         else:
             plot_data_acc = None
 
         # use other widget to plot data
-        if we.is_extension('spike_amplitudes'):
-            plot_data_amplitudes = AmplitudesWidget(we, unit_ids=[unit_id], unit_colors=unit_colors,
-                                                    plot_legend=False, plot_histograms=True).plot_data
+        if we.is_extension("spike_amplitudes"):
+            plot_data_amplitudes = AmplitudesWidget(
+                we, unit_ids=[unit_id], unit_colors=unit_colors, plot_legend=False, plot_histograms=True
+            ).plot_data
         else:
             plot_data_amplitudes = None
 
-        
         plot_data = dict(
             unit_id=unit_id,
             unit_location=unit_location,
             plot_data_unit_locations=plot_data_unit_locations,
             plot_data_waveforms=plot_data_waveforms,
             plot_data_waveform_density=plot_data_waveform_density,
             plot_data_acc=plot_data_acc,
             plot_data_amplitudes=plot_data_amplitudes,
-
         )
 
         BaseWidget.__init__(self, plot_data, backend=backend, **backend_kwargs)
-
-
-
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/unit_waveforms.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/unit_waveforms.py`

 * *Files 8% similar despite different names*

```diff
@@ -10,65 +10,87 @@
 
 class UnitWaveformsWidget(BaseWidget):
     """
     Plots unit waveforms.
 
     Parameters
     ----------
-    waveform_extractor: WaveformExtractor
-    channel_ids: list
-        The channel ids to display
-    unit_ids: list
-        List of unit ids.
-    plot_templates: bool
-        If True, templates are plotted over the waveforms
+    waveform_extractor : WaveformExtractor
+        The input waveform extractor
+    channel_ids:  list
+        The channel ids to display, default None
+    unit_ids : list
+        List of unit ids, default None
+    plot_templates : bool
+        If True, templates are plotted over the waveforms, default True
     sparsity : ChannelSparsity or None
-        Optional ChannelSparsity to apply.
+        Optional ChannelSparsity to apply, default None
         If WaveformExtractor is already sparse, the argument is ignored
-    set_title: bool
-        Create a plot title with the unit number if True.
-    plot_channels: bool
-        Plot channel locations below traces.
-    unit_selected_waveforms: None or dict
-        A dict key is unit_id and value is the subset of waveforms indices that should be 
-        be displayed (matplotlib backend)
-    max_spikes_per_unit: int or None
+    set_title : bool
+        Create a plot title with the unit number if True, default True
+    plot_channels : bool
+        Plot channel locations below traces, default False
+    unit_selected_waveforms : None or dict
+        A dict key is unit_id and value is the subset of waveforms indices that should be
+        be displayed (matplotlib backend), default None
+    max_spikes_per_unit : int or None
         If given and unit_selected_waveforms is None, only max_spikes_per_unit random units are
         displayed per waveform, default 50 (matplotlib backend)
-    axis_equal: bool
-        Equal aspect ratio for x and y axis, to visualize the array geometry to scale.
-    lw_waveforms: float
+    axis_equal : bool
+        Equal aspect ratio for x and y axis, to visualize the array geometry to scale, default False
+    lw_waveforms : float
         Line width for the waveforms, default 1 (matplotlib backend)
-    lw_templates: float
+    lw_templates : float
         Line width for the templates, default 2 (matplotlib backend)
-    unit_colors: None or dict
-        A dict key is unit_id and value is any color format handled by matplotlib.
+    unit_colors : None or dict
+        A dict key is unit_id and value is any color format handled by matplotlib, default None
         If None, then the get_unit_colors() is internally used. (matplotlib backend)
-    alpha_waveforms: float
+    alpha_waveforms : float
         Alpha value for waveforms, default 0.5 (matplotlib backend)
-    alpha_templates: float
+    alpha_templates : float
         Alpha value for templates, default 1 (matplotlib backend)
     hide_unit_selector : bool
-        For sortingview backend, if True the unit selector is not displayed
-    same_axis: bool
-        If True, waveforms and templates are diplayed on the same axis, default False (matplotlib backend)
-    x_offset_units: bool
-        In case same_axis is True, this parameter allow to x-offset the waveforms for different units 
+        For sortingview backend, if True the unit selector is not displayed, default False
+    same_axis : bool
+        If True, waveforms and templates are displayed on the same axis, default False (matplotlib backend)
+    x_offset_units : bool
+        In case same_axis is True, this parameter allow to x-offset the waveforms for different units
         (recommended for a few units), default False (matlotlib backend)
-    plot_legend: bool (default True)
-        Display legend.
+    plot_legend : bool
+        Display legend, default True
     """
+
     possible_backends = {}
 
-    def __init__(self, waveform_extractor: WaveformExtractor, channel_ids=None, unit_ids=None,
-                 plot_waveforms=True, plot_templates=True, plot_channels=False,
-                 unit_colors=None, sparsity=None, ncols=5, lw_waveforms=1, lw_templates=2, axis_equal=False,
-                 unit_selected_waveforms=None, max_spikes_per_unit=50, set_title=True, same_axis=False,
-                 x_offset_units=False, alpha_waveforms=0.5, alpha_templates=1, hide_unit_selector=False,
-                 plot_legend=True, backend=None, **backend_kwargs):
+    def __init__(
+        self,
+        waveform_extractor: WaveformExtractor,
+        channel_ids=None,
+        unit_ids=None,
+        plot_waveforms=True,
+        plot_templates=True,
+        plot_channels=False,
+        unit_colors=None,
+        sparsity=None,
+        ncols=5,
+        lw_waveforms=1,
+        lw_templates=2,
+        axis_equal=False,
+        unit_selected_waveforms=None,
+        max_spikes_per_unit=50,
+        set_title=True,
+        same_axis=False,
+        x_offset_units=False,
+        alpha_waveforms=0.5,
+        alpha_templates=1,
+        hide_unit_selector=False,
+        plot_legend=True,
+        backend=None,
+        **backend_kwargs,
+    ):
         we = waveform_extractor
         sorting: BaseSorting = we.sorting
 
         if unit_ids is None:
             unit_ids = sorting.get_unit_ids()
         unit_ids = unit_ids
         if channel_ids is None:
@@ -82,27 +104,26 @@
         if waveform_extractor.is_sparse():
             sparsity = waveform_extractor.sparsity
         else:
             if sparsity is None:
                 # in this case, we construct a dense sparsity
                 unit_id_to_channel_ids = {u: we.channel_ids for u in we.unit_ids}
                 sparsity = ChannelSparsity.from_unit_id_to_channel_ids(
-                    unit_id_to_channel_ids=unit_id_to_channel_ids,
-                    unit_ids=we.unit_ids,
-                    channel_ids=we.channel_ids
+                    unit_id_to_channel_ids=unit_id_to_channel_ids, unit_ids=we.unit_ids, channel_ids=we.channel_ids
                 )
             else:
                 assert isinstance(sparsity, ChannelSparsity), "'sparsity' should be a ChannelSparsity object!"
 
         # get templates
         templates = we.get_all_templates(unit_ids=unit_ids)
         template_stds = we.get_all_templates(unit_ids=unit_ids, mode="std")
 
         xvectors, y_scale, y_offset, delta_x = get_waveforms_scales(
-            waveform_extractor, templates, channel_locations, x_offset_units)
+            waveform_extractor, templates, channel_locations, x_offset_units
+        )
 
         wfs_by_ids = {}
         if plot_waveforms:
             for unit_id in unit_ids:
                 if waveform_extractor.is_sparse():
                     wfs = we.get_waveforms(unit_id)
                 else:
@@ -141,33 +162,32 @@
             hide_unit_selector=hide_unit_selector,
             plot_legend=plot_legend,
         )
 
         BaseWidget.__init__(self, plot_data, backend=backend, **backend_kwargs)
 
 
-def get_waveforms_scales(we, templates, channel_locations,
-                         x_offset_units=False):
+def get_waveforms_scales(we, templates, channel_locations, x_offset_units=False):
     """
     Return scales and x_vector for templates plotting
     """
     wf_max = np.max(templates)
     wf_min = np.min(templates)
 
     x_chans = np.unique(channel_locations[:, 0])
     if x_chans.size > 1:
         delta_x = np.min(np.diff(x_chans))
     else:
-        delta_x = 40.
+        delta_x = 40.0
 
     y_chans = np.unique(channel_locations[:, 1])
     if y_chans.size > 1:
         delta_y = np.min(np.diff(y_chans))
     else:
-        delta_y = 40.
+        delta_y = 40.0
 
     m = max(np.abs(wf_max), np.abs(wf_min))
     y_scale = delta_y / m * 0.7
 
     y_offset = channel_locations[:, 1][None, :]
 
     xvect = delta_x * (np.arange(we.nsamples) - we.nbefore) / we.nsamples * 0.7
@@ -179,8 +199,7 @@
         ch_locs = channel_locations
 
     xvectors = ch_locs[:, 0][None, :] + xvect[:, None]
     # put nan for discontinuity
     xvectors[-1, :] = np.nan
 
     return xvectors, y_scale, y_offset, delta_x
-
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/unit_waveforms_density_map.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/unit_waveforms_density_map.py`

 * *Files 4% similar despite different names*

```diff
@@ -8,64 +8,68 @@
 
 class UnitWaveformDensityMapWidget(BaseWidget):
     """
     Plots unit waveforms using heat map density.
 
     Parameters
     ----------
-    waveform_extractor: WaveformExtractor
-    channel_ids: list
-        The channel ids to display
-    unit_ids: list
-        List of unit ids.
-    plot_templates: bool
-        If True, templates are plotted over the waveforms
+    waveform_extractor : WaveformExtractor
+        The waveformextractor for calculating waveforms
+    channel_ids : list
+        The channel ids to display, default None
+    unit_ids : list
+        List of unit ids, default None
     sparsity : ChannelSparsity or None
-        Optional ChannelSparsity to apply.
+        Optional ChannelSparsity to apply, default None
         If WaveformExtractor is already sparse, the argument is ignored
-    use_max_channel: bool default False
-        Use only the max channel
-    peak_sign: str "neg"
-        Used to detect max channel only when use_max_channel=True 
-    unit_colors: None or dict
+    use_max_channel : bool
+        Use only the max channel, default False
+    peak_sign : str (neg/pos/both)
+        Used to detect max channel only when use_max_channel=True, default 'neg'
+    unit_colors : None or dict
         A dict key is unit_id and value is any color format handled by matplotlib.
-        If None, then the get_unit_colors() is internally used.
-    same_axis: bool
+        If None, then the get_unit_colors() is internally used, default None
+    same_axis : bool
         If True then all density are plot on the same axis and then channels is the union
-        all channel per units.
-    set_title: bool
-        Create a plot title with the unit number if True.
-    plot_channels: bool
-        Plot channel locations below traces, only used if channel_locs is True
+        all channel per units, default False
     """
+
     possible_backends = {}
 
-    
-    def __init__(self, waveform_extractor, channel_ids=None, unit_ids=None,
-                 sparsity=None, same_axis=False, use_max_channel=False, peak_sign="neg", unit_colors=None,
-                 backend=None, **backend_kwargs):
+    def __init__(
+        self,
+        waveform_extractor,
+        channel_ids=None,
+        unit_ids=None,
+        sparsity=None,
+        same_axis=False,
+        use_max_channel=False,
+        peak_sign="neg",
+        unit_colors=None,
+        backend=None,
+        **backend_kwargs,
+    ):
         we = waveform_extractor
 
         if channel_ids is None:
             channel_ids = we.channel_ids
 
         if unit_ids is None:
             unit_ids = we.unit_ids
 
         if unit_colors is None:
             unit_colors = get_unit_colors(we.sorting)
 
         if use_max_channel:
-            assert len(unit_ids) == 1, ' UnitWaveformDensity : use_max_channel=True works only with one unit'
-            max_channels = get_template_extremum_channel(we,  mode="extremum", peak_sign=peak_sign, outputs="index")
-
+            assert len(unit_ids) == 1, " UnitWaveformDensity : use_max_channel=True works only with one unit"
+            max_channels = get_template_extremum_channel(we, mode="extremum", peak_sign=peak_sign, outputs="index")
 
-        # sparsity is done on all the units even if unit_ids is a few ones because some backend need then all
+        # sparsity is done on all the units even if unit_ids is a few ones because some backends need them all
         if waveform_extractor.is_sparse():
-            assert sparsity is None, 'UnitWaveformDensity WaveformExtractor is already sparse'
+            assert sparsity is None, "UnitWaveformDensity WaveformExtractor is already sparse"
             used_sparsity = waveform_extractor.sparsity
         elif sparsity is not None:
             assert isinstance(sparsity, ChannelSparsity), "'sparsity' should be a ChannelSparsity object!"
             used_sparsity = sparsity
         else:
             # in this case, we construct a dense sparsity
             used_sparsity = ChannelSparsity.create_dense(we)
@@ -80,42 +84,42 @@
         bins = np.arange(bin_min, bin_max, bin_size)
 
         # 2d histograms
         if same_axis:
             all_hist2d = None
             # channel union across units
             unit_inds = we.sorting.ids_to_indices(unit_ids)
-            shared_chan_inds, = np.nonzero(np.sum(used_sparsity.mask[unit_inds, :], axis=0))
+            (shared_chan_inds,) = np.nonzero(np.sum(used_sparsity.mask[unit_inds, :], axis=0))
         else:
             all_hist2d = {}
 
         for unit_index, unit_id in enumerate(unit_ids):
             chan_inds = channel_inds[unit_id]
-            
+
             # this have already the sparsity
             wfs = we.get_waveforms(unit_id, sparsity=sparsity)
 
             if use_max_channel:
                 chan_ind = max_channels[unit_id]
                 wfs = wfs[:, :, chan_inds == chan_ind]
-            
+
             if same_axis and not np.array_equal(chan_inds, shared_chan_inds):
                 # add more channels if necessary
                 wfs_ = np.zeros((wfs.shape[0], wfs.shape[1], shared_chan_inds.size), dtype=float)
                 mask = np.in1d(shared_chan_inds, chan_inds)
                 wfs_[:, :, mask] = wfs
                 wfs_[:, :, ~mask] = np.nan
                 wfs = wfs_
 
             # make histogram density
             wfs_flat = wfs.swapaxes(1, 2).reshape(wfs.shape[0], -1)
             hist2d = np.zeros((wfs_flat.shape[1], bins.size))
             indexes0 = np.arange(wfs_flat.shape[1])
 
-            wf_bined = np.floor((wfs_flat - bin_min) / bin_size).astype('int32')
+            wf_bined = np.floor((wfs_flat - bin_min) / bin_size).astype("int32")
             wf_bined = wf_bined.clip(0, bins.size - 1)
             for d in wf_bined:
                 hist2d[indexes0, d] += 1
 
             if same_axis:
                 if all_hist2d is None:
                     all_hist2d = hist2d
@@ -126,32 +130,29 @@
 
         # update final channel_inds
         if same_axis:
             channel_inds = {unit_id: shared_chan_inds for unit_id in unit_ids}
         if use_max_channel:
             channel_inds = {unit_id: [max_channels[unit_id]] for unit_id in unit_ids}
 
-
         # plot median
         templates_flat = {}
         for unit_index, unit_id in enumerate(unit_ids):
             chan_inds = channel_inds[unit_id]
             template = templates[unit_index, :, chan_inds]
             template_flat = template.flatten()
             templates_flat[unit_id] = template_flat
 
-
         plot_data = dict(
             unit_ids=unit_ids,
             unit_colors=unit_colors,
             channel_ids=we.channel_ids,
             channel_inds=channel_inds,
             same_axis=same_axis,
             bin_min=bin_min,
             bin_max=bin_max,
             all_hist2d=all_hist2d,
             templates_flat=templates_flat,
-            template_width=wfs.shape[1]
+            template_width=wfs.shape[1],
         )
 
         BaseWidget.__init__(self, plot_data, backend=backend, **backend_kwargs)
-
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/utils.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/utils.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,132 +1,137 @@
 import numpy as np
 import random
 
 from ..core import ChannelSparsity
 
 try:
     import distinctipy
+
     HAVE_DISTINCTIPY = True
 except ImportError:
     HAVE_DISTINCTIPY = False
 
 try:
     import matplotlib.pyplot as plt
+
     HAVE_MPL = True
 except ImportError:
     HAVE_MPL = False
 
 
-def get_some_colors(keys, color_engine='auto', map_name='gist_ncar', format='RGBA', shuffle=None, seed=None):
+def get_some_colors(keys, color_engine="auto", map_name="gist_ncar", format="RGBA", shuffle=None, seed=None):
     """
     Return a dict of colors for given keys
-    
-    Params
-    ------
-    color_engine: str 'auto' / 'matplotlib' / 'colorsys' / 'distinctipy'
+
+    Parameters
+    ----------
+    color_engine : str 'auto' / 'matplotlib' / 'colorsys' / 'distinctipy'
         The engine to generate colors
-    map_name: str
+    map_name : str
         Used for matplotlib
-    format: 'RGBA'
-        The output fomrats
-    shuffle: bool or None
-        Shuffle or not.
+    format: str
+        The output formats, default 'RGBA'
+    shuffle : bool or None
+        Shuffle or not, default None
         If None then:
-
         * set to True for matplotlib and colorsys
         * set to False for distinctipy
     seed: int or None
-        Eventually a seed
+        Set the seed, default None
 
     Returns
     -------
     dict_colors: dict
         A dict of colors for given keys.
-    
+
     """
-    assert color_engine in ('auto', 'distinctipy', 'matplotlib', 'colorsys')
+    assert color_engine in ("auto", "distinctipy", "matplotlib", "colorsys")
 
-    possible_formats = ('RGBA',)
-    assert format in possible_formats, f'format must be {possible_formats}'
+    possible_formats = ("RGBA",)
+    assert format in possible_formats, f"format must be {possible_formats}"
 
     # select the colormap engine
-    if color_engine == 'auto':
+    if color_engine == "auto":
         if HAVE_MPL:
-            color_engine = 'matplotlib'
+            color_engine = "matplotlib"
         elif HAVE_DISTINCTIPY:
             # this is the third choice because this is very slow
-            color_engine = 'distinctipy'
+            color_engine = "distinctipy"
         else:
-            color_engine = 'colorsys'
+            color_engine = "colorsys"
 
     if shuffle is None:
         # distinctipy then False
-        shuffle = color_engine != 'distinctipy'
+        shuffle = color_engine != "distinctipy"
         seed = 91
 
     N = len(keys)
 
-    if color_engine == 'distinctipy':
+    if color_engine == "distinctipy":
         colors = distinctipy.get_colors(N)
         # add the alpha
-        colors = [color + (1., ) for color in colors]
+        colors = [color + (1.0,) for color in colors]
 
-    elif color_engine == 'matplotlib':
+    elif color_engine == "matplotlib":
         # some map have black or white at border so +10
         margin = max(4, int(N * 0.08))
         cmap = plt.get_cmap(map_name, N + 2 * margin)
-        colors = [cmap(i+margin) for i, key in enumerate(keys)]
+        colors = [cmap(i + margin) for i, key in enumerate(keys)]
 
-    elif color_engine == 'colorsys':
+    elif color_engine == "colorsys":
         import colorsys
-        colors = [colorsys.hsv_to_rgb(x * 1.0 / N, 0.5, 0.5) + (1., ) for x in range(N)]
+
+        colors = [colorsys.hsv_to_rgb(x * 1.0 / N, 0.5, 0.5) + (1.0,) for x in range(N)]
 
     if shuffle:
         rng = np.random.RandomState(seed=seed)
         inds = np.arange(N)
         rng.shuffle(inds)
         colors = [colors[i] for i in inds]
 
     dict_colors = dict(zip(keys, colors))
 
     return dict_colors
 
 
-def get_unit_colors(sorting, color_engine='auto', map_name='gist_ncar', format='RGBA', shuffle=None, seed=None):
+def get_unit_colors(sorting, color_engine="auto", map_name="gist_ncar", format="RGBA", shuffle=None, seed=None):
     """
     Return a dict colors per units.
     """
-    colors = get_some_colors(sorting.unit_ids, color_engine=color_engine, map_name=map_name, format=format, 
-                             shuffle=shuffle, seed=seed)
+    colors = get_some_colors(
+        sorting.unit_ids, color_engine=color_engine, map_name=map_name, format=format, shuffle=shuffle, seed=seed
+    )
     return colors
 
 
-def array_to_image(data,
-                   colormap='RdGy',
-                   clim=None,
-                   spatial_zoom=(0.75, 1.25),
-                   num_timepoints_per_row=30000,
-                   row_spacing=0.25,
-                   scalebar=False,
-                   sampling_frequency=None):
+def array_to_image(
+    data,
+    colormap="RdGy",
+    clim=None,
+    spatial_zoom=(0.75, 1.25),
+    num_timepoints_per_row=30000,
+    row_spacing=0.25,
+    scalebar=False,
+    sampling_frequency=None,
+):
     """
-    Converts a 2D numpy array (width x height) to a 
+    Converts a 2D numpy array (width x height) to a
     3D image array (width x height x RGB color).
 
     Useful for visualizing data before/after preprocessing
 
-    Params
-    ------
+    Parameters
+    ----------
     data : np.array
         2D numpy array
-    colormap : str 
+    colormap : str
         Identifier for a Matplotlib colormap
     clim : tuple or None
         The color limits. If None, the clim is the range of the traces
-    spatial_zoom : tuple 
+    spatial_zoom : tuple
         Tuple specifying width & height scaling
     num_timepoints_per_row : int
         Max number of samples before wrapping
     row_spacing : float
         Ratio of row spacing to overall channel height
 
     Returns
@@ -140,73 +145,88 @@
     if clim is not None:
         assert len(clim) == 2, "'clim' should have a minimum and maximum value"
     else:
         clim = [np.min(data), np.max(data)]
 
     num_timepoints = data.shape[0]
     num_channels = data.shape[1]
-    num_channels_after_scaling = int(num_channels * spatial_zoom[1])
     spacing = int(num_channels * spatial_zoom[1] * row_spacing)
 
-    num_timepoints_after_scaling = int(num_timepoints * spatial_zoom[0])
-    num_timepoints_per_row_after_scaling = int(np.min([num_timepoints_per_row, num_timepoints]) * spatial_zoom[0])
-
     cmap = plt.get_cmap(colormap)
     zoomed_data = zoom(data, spatial_zoom)
+    num_timepoints_after_scaling, num_channels_after_scaling = zoomed_data.shape
+    num_timepoints_per_row_after_scaling = int(np.min([num_timepoints_per_row, num_timepoints]) * spatial_zoom[0])
 
     scaled_data = zoomed_data
     scaled_data[scaled_data < clim[0]] = clim[0]
     scaled_data[scaled_data > clim[1]] = clim[1]
     scaled_data = (scaled_data - clim[0]) / np.ptp(clim)
-    a = np.flip((cmap(scaled_data.T)[:, :, :3]*255).astype(np.uint8), axis=0)  # colorize and convert to uint8
+    a = np.flip((cmap(scaled_data.T)[:, :, :3] * 255).astype(np.uint8), axis=0)  # colorize and convert to uint8
 
     num_rows = int(np.ceil(num_timepoints / num_timepoints_per_row))
 
-    output_image = np.ones(
-        (num_rows * (num_channels_after_scaling + spacing),
-         num_timepoints_per_row_after_scaling, 3), dtype=np.uint8
-    ) * 255
+    output_image = (
+        np.ones(
+            (num_rows * (num_channels_after_scaling + spacing), num_timepoints_per_row_after_scaling, 3), dtype=np.uint8
+        )
+        * 255
+    )
 
     for ir in range(num_rows):
         i1 = ir * num_timepoints_per_row_after_scaling
         i2 = min(i1 + num_timepoints_per_row_after_scaling, num_timepoints_after_scaling)
-        output_image[ir * (num_channels_after_scaling + spacing):ir * (num_channels_after_scaling + spacing) +
-                     num_channels_after_scaling, :i2-i1, :] = a[:, i1:i2, :]
+        output_image[
+            ir * (num_channels_after_scaling + spacing) : ir * (num_channels_after_scaling + spacing)
+            + num_channels_after_scaling,
+            : i2 - i1,
+            :,
+        ] = a[:, i1:i2, :]
 
     if scalebar:
         assert sampling_frequency is not None
 
         try:
             from PIL import Image, ImageFont, ImageDraw
         except ImportError:
             raise ImportError("To add a scalebar, you need pillow: >>> pip install pillow")
         import platform
 
         y_scalebar = output_image.shape[0] - 10
         fontsize = int(0.8 * spacing)
-        row_ms = (num_timepoints_per_row / sampling_frequency) * 1000
+        num_time_points = np.min([num_timepoints_per_row, num_timepoints])
+        row_ms = (num_time_points / sampling_frequency) * 1000
 
         try:
             if platform.system() == "Linux":
                 font = ImageFont.truetype("/usr/share/fonts/truetype/freefont/FreeMono.ttf", fontsize)
             else:
                 font = ImageFont.truetype("arial.ttf", fontsize)
         except:
             print(f"Could not load font to use in scalebar. Scalebar will not be drawn.")
             font = None
 
         if font is not None:
             image = Image.fromarray(output_image)
             image_editable = ImageDraw.Draw(image)
 
-            # bar should be around 1/5 of row and ultiple of 5
-            bar_ms = int((row_ms / 5) // 5 * 5)
-            bar_px = bar_ms * num_timepoints_per_row / row_ms
+            # bar should be around 1/5 of row and ultiple of 5ms
+            if row_ms / 5 > 5:
+                bar_ms = int(np.ceil((row_ms / 5) // 5 * 5))
+                text_offset = 0.3
+            else:
+                bar_ms = int(np.ceil(row_ms / 5))
+                text_offset = -0.1
+            bar_px = bar_ms * num_time_points / row_ms
+
+            x_offset = int(0.1 * num_time_points)
 
-            x_offset = int(0.1 * num_timepoints_per_row)
             image_editable.line((x_offset, y_scalebar, x_offset + bar_px, y_scalebar), fill=(0, 0, 0), width=10)
-            image_editable.text((x_offset + 0.3 * (bar_px), y_scalebar - 1.1 * fontsize),
-                                text="10 ms", font=font, fill=(0, 0, 0))
+            image_editable.text(
+                (x_offset + text_offset * (bar_px), y_scalebar - 1.1 * fontsize),
+                text=f"{bar_ms}ms",
+                font=font,
+                fill=(0, 0, 0),
+            )
 
             output_image = np.frombuffer(image.tobytes(), dtype=np.uint8).reshape(output_image.shape)
 
     return output_image
```

### Comparing `spikeinterface-0.97.1/spikeinterface/widgets/widget_list.py` & `spikeinterface-0.98.0/src/spikeinterface/widgets/widget_list.py`

 * *Files 14% similar despite different names*

```diff
@@ -36,14 +36,18 @@
 from .amplitudes import AmplitudesWidget
 from .all_amplitudes_distributions import AllAmplitudesDistributionsWidget
 
 # metrics
 from .quality_metrics import QualityMetricsWidget
 from .template_metrics import TemplateMetricsWidget
 
+
+# motion/drift
+from .motion import MotionWidget
+
 # similarity
 from .template_similarity import TemplateSimilarityWidget
 
 
 from .unit_depths import UnitDepthsWidget
 
 # summary
@@ -53,37 +57,37 @@
 
 widget_list = [
     AmplitudesWidget,
     AllAmplitudesDistributionsWidget,
     AutoCorrelogramsWidget,
     CrossCorrelogramsWidget,
     QualityMetricsWidget,
-    SpikeLocationsWidget, 
+    SpikeLocationsWidget,
     SpikesOnTracesWidget,
     TemplateMetricsWidget,
+    MotionWidget,
     TemplateSimilarityWidget,
     TimeseriesWidget,
     UnitLocationsWidget,
     UnitTemplatesWidget,
     UnitWaveformsWidget,
     UnitWaveformDensityMapWidget,
     UnitDepthsWidget,
-    
     # summary
     UnitSummaryWidget,
     SortingSummaryWidget,
 ]
 
 
 # add backends and kwargs to doc
 for wcls in widget_list:
     wcls_doc = wcls.__doc__
-    
+
     wcls_doc += """
-    
+
     backend: str
     {backends}
     **backend_kwargs: kwargs
     {backend_kwargs}
     """
     backend_str = f"    {list(wcls.possible_backends.keys())}"
     backend_kwargs_str = ""
@@ -93,24 +97,29 @@
             backend_kwargs_str += f"\n        {backend}:\n\n"
             for bk, bk_dsc in backend_kwargs_desc.items():
                 backend_kwargs_str += f"        * {bk}: {bk_dsc}\n"
     wcls.__doc__ = wcls_doc.format(backends=backend_str, backend_kwargs=backend_kwargs_str)
 
 
 # make function for all widgets
-plot_amplitudes = define_widget_function_from_class(AmplitudesWidget, 'plot_amplitudes')
-plot_all_amplitudes_distributions = define_widget_function_from_class(AllAmplitudesDistributionsWidget, 'plot_all_amplitudes_distributions')
-plot_autocorrelograms = define_widget_function_from_class(AutoCorrelogramsWidget, 'plot_autocorrelograms')
-plot_crosscorrelograms = define_widget_function_from_class(CrossCorrelogramsWidget, 'plot_crosscorrelograms')
+plot_amplitudes = define_widget_function_from_class(AmplitudesWidget, "plot_amplitudes")
+plot_all_amplitudes_distributions = define_widget_function_from_class(
+    AllAmplitudesDistributionsWidget, "plot_all_amplitudes_distributions"
+)
+plot_autocorrelograms = define_widget_function_from_class(AutoCorrelogramsWidget, "plot_autocorrelograms")
+plot_crosscorrelograms = define_widget_function_from_class(CrossCorrelogramsWidget, "plot_crosscorrelograms")
 plot_quality_metrics = define_widget_function_from_class(QualityMetricsWidget, "plot_quality_metrics")
 plot_spike_locations = define_widget_function_from_class(SpikeLocationsWidget, "plot_spike_locations")
-plot_spikes_on_traces = define_widget_function_from_class(SpikesOnTracesWidget, 'plot_spikes_on_traces')
+plot_spikes_on_traces = define_widget_function_from_class(SpikesOnTracesWidget, "plot_spikes_on_traces")
 plot_template_metrics = define_widget_function_from_class(TemplateMetricsWidget, "plot_template_metrics")
-plot_template_similarity = define_widget_function_from_class(TemplateSimilarityWidget, 'plot_template_similarity')
-plot_timeseries = define_widget_function_from_class(TimeseriesWidget, 'plot_timeseries')
-plot_unit_locations = define_widget_function_from_class(UnitLocationsWidget, 'plot_unit_locations')
-plot_unit_templates = define_widget_function_from_class(UnitTemplatesWidget, 'plot_unit_templates')
-plot_unit_waveforms = define_widget_function_from_class(UnitWaveformsWidget, 'plot_unit_waveforms')
-plot_unit_waveforms_density_map = define_widget_function_from_class(UnitWaveformDensityMapWidget, 'plot_unit_waveforms_density_map')
-plot_unit_depths = define_widget_function_from_class(UnitDepthsWidget, 'plot_unit_depths')
+plot_motion = define_widget_function_from_class(MotionWidget, "plot_motion")
+plot_template_similarity = define_widget_function_from_class(TemplateSimilarityWidget, "plot_template_similarity")
+plot_timeseries = define_widget_function_from_class(TimeseriesWidget, "plot_timeseries")
+plot_unit_locations = define_widget_function_from_class(UnitLocationsWidget, "plot_unit_locations")
+plot_unit_templates = define_widget_function_from_class(UnitTemplatesWidget, "plot_unit_templates")
+plot_unit_waveforms = define_widget_function_from_class(UnitWaveformsWidget, "plot_unit_waveforms")
+plot_unit_waveforms_density_map = define_widget_function_from_class(
+    UnitWaveformDensityMapWidget, "plot_unit_waveforms_density_map"
+)
+plot_unit_depths = define_widget_function_from_class(UnitDepthsWidget, "plot_unit_depths")
 plot_unit_summary = define_widget_function_from_class(UnitSummaryWidget, "plot_unit_summary")
 plot_sorting_summary = define_widget_function_from_class(SortingSummaryWidget, "plot_sorting_summary")
```

### Comparing `spikeinterface-0.97.1/spikeinterface.egg-info/PKG-INFO` & `spikeinterface-0.98.0/src/spikeinterface.egg-info/PKG-INFO`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: spikeinterface
-Version: 0.97.1
+Version: 0.98.0
 Summary: Python toolkit for analysis, visualization, and comparison of spike sorting output
 Author-email: Alessio Buccino <alessiop.buccino@gmail.com>, Samuel Garcia <sam.garcia.die@gmail.com>
 Project-URL: homepage, https://github.com/SpikeInterface/spikeinterface
 Project-URL: repository, https://github.com/SpikeInterface/spikeinterface
 Project-URL: documentation, https://spikeinterface.readthedocs.io/
 Project-URL: changelog, https://spikeinterface.readthedocs.io/en/latest/whatisnew.html
 Classifier: Programming Language :: Python :: 3 :: Only
@@ -13,16 +13,18 @@
 Classifier: Operating System :: POSIX :: Linux
 Classifier: Operating System :: Microsoft :: Windows
 Classifier: Operating System :: MacOS
 Classifier: Operating System :: OS Independent
 Requires-Python: <4.0,>=3.8
 Description-Content-Type: text/markdown
 Provides-Extra: extractors
+Provides-Extra: streaming_extractors
 Provides-Extra: full
 Provides-Extra: widgets
+Provides-Extra: test_core
 Provides-Extra: test
 License-File: LICENSE
 
 # SpikeInterface: a unified framework for spike sorting
 
 <table>
 <tr>
@@ -48,16 +50,16 @@
     <img src="https://img.shields.io/pypi/l/spikeinterface.svg" alt="license" />
     </a>
 </td>
 </tr>
 <tr>
   <td>Build Status</td>
   <td>
-    <a href="https://github.com/SpikeInterface/spikeinterface/actions/workflows/full-test.yml/badge.svg">
-    <img src="https://github.com/SpikeInterface/spikeinterface/actions/workflows/full-test.yml/badge.svg" alt="travis build status" />
+    <a href="https://github.com/SpikeInterface/spikeinterface/actions/workflows/full-test-with-codecov.yml/badge.svg">
+    <img src="https://github.com/SpikeInterface/spikeinterface/actions/workflows/full-test-with-codecov.yml/badge.svg" alt="CI build status" />
     </a>
   </td>
 </tr>
 <tr>
 	<td>Codecov</td>
 	<td>
 		<a href="https://codecov.io/github/spikeinterface/spikeinterface">
@@ -91,29 +93,29 @@
 
 ## Documentation
 
 Detailed documentation for spikeinterface can be found [here](https://spikeinterface.readthedocs.io/en/latest).
 
 Several tutorials to get started can be found in [spiketutorials](https://github.com/SpikeInterface/spiketutorials).
 
-There are also some useful notebooks [on our blog](https://spikeinterface.github.io) that cover advanced benchmarking 
+There are also some useful notebooks [on our blog](https://spikeinterface.github.io) that cover advanced benchmarking
 and sorting components.
 
 You can also have a look at the [spikeinterface-gui](https://github.com/SpikeInterface/spikeinterface-gui).
 
 
 ## How to install spikeinteface
 
 You can install the new `spikeinterface` version with pip:
 
 ```bash
 pip install spikeinterface[full]
 ```
 
-The `[full]` option installs all the extra dependencies for all the different sub-modules. 
+The `[full]` option installs all the extra dependencies for all the different sub-modules.
 
 To install all interactive widget backends, you can use:
 
 ```bash
  pip install spikeinterface[full,widgets]
 ```
```

#### html2text {}

```diff
@@ -1,28 +1,28 @@
-Metadata-Version: 2.1 Name: spikeinterface Version: 0.97.1 Summary: Python
+Metadata-Version: 2.1 Name: spikeinterface Version: 0.98.0 Summary: Python
 toolkit for analysis, visualization, and comparison of spike sorting output
 Author-email: Alessio Buccino
 buccino@gmail.com>, Samuel Garcia
 garcia.die@gmail.com> Project-URL: homepage, https://github.com/SpikeInterface/
 spikeinterface Project-URL: repository, https://github.com/SpikeInterface/
 spikeinterface Project-URL: documentation, https://
 spikeinterface.readthedocs.io/ Project-URL: changelog, https://
 spikeinterface.readthedocs.io/en/latest/whatisnew.html Classifier: Programming
 Language :: Python :: 3 :: Only Classifier: License :: OSI Approved :: MIT
 License Classifier: Intended Audience :: Science/Research Classifier: Operating
 System :: POSIX :: Linux Classifier: Operating System :: Microsoft :: Windows
 Classifier: Operating System :: MacOS Classifier: Operating System :: OS
 Independent Requires-Python: <4.0,>=3.8 Description-Content-Type: text/markdown
-Provides-Extra: extractors Provides-Extra: full Provides-Extra: widgets
-Provides-Extra: test License-File: LICENSE # SpikeInterface: a unified
-framework for spike sorting
+Provides-Extra: extractors Provides-Extra: streaming_extractors Provides-Extra:
+full Provides-Extra: widgets Provides-Extra: test_core Provides-Extra: test
+License-File: LICENSE # SpikeInterface: a unified framework for spike sorting
 Latest Release [latest_release]
 Documentation  [latest_documentation]
 License        [license]
-Build Status   [travis_build_status]
+Build Status   [CI_build_status]
 Codecov        [codecov]
 [![Twitter](https://img.shields.io/badge/@spikeinterface-
 %231DA1F2.svg?style=for-the-badge&logo=Twitter&logoColor=white)](https://
 twitter.com/spikeinterface) [![Mastodon](https://img.shields.io/badge/-
 @spikeinterface-%232B90D9?style=for-the-badge&logo=mastodon&logoColor=white)]
 (https://fosstodon.org/@spikeinterface) SpikeInterface is a Python framework
 designed to unify preexisting spike sorting technologies into a single code
```

### Comparing `spikeinterface-0.97.1/spikeinterface.egg-info/SOURCES.txt` & `spikeinterface-0.98.0/src/spikeinterface.egg-info/SOURCES.txt`

 * *Files 20% similar despite different names*

```diff
@@ -1,338 +1,353 @@
 LICENSE
 README.md
 pyproject.toml
 setup.py
-spikeinterface/__init__.py
-spikeinterface/full.py
-spikeinterface.egg-info/PKG-INFO
-spikeinterface.egg-info/SOURCES.txt
-spikeinterface.egg-info/dependency_links.txt
-spikeinterface.egg-info/requires.txt
-spikeinterface.egg-info/top_level.txt
-spikeinterface/comparison/__init__.py
-spikeinterface/comparison/basecomparison.py
-spikeinterface/comparison/collisioncomparison.py
-spikeinterface/comparison/collisionstudy.py
-spikeinterface/comparison/comparisontools.py
-spikeinterface/comparison/correlogramcomparison.py
-spikeinterface/comparison/correlogramstudy.py
-spikeinterface/comparison/groundtruthstudy.py
-spikeinterface/comparison/hybrid.py
-spikeinterface/comparison/multicomparisons.py
-spikeinterface/comparison/paircomparisons.py
-spikeinterface/comparison/studytools.py
-spikeinterface/core/__init__.py
-spikeinterface/core/base.py
-spikeinterface/core/baseevent.py
-spikeinterface/core/baserecording.py
-spikeinterface/core/baserecordingsnippets.py
-spikeinterface/core/basesnippets.py
-spikeinterface/core/basesorting.py
-spikeinterface/core/binaryfolder.py
-spikeinterface/core/binaryrecordingextractor.py
-spikeinterface/core/channelsaggregationrecording.py
-spikeinterface/core/channelslice.py
-spikeinterface/core/core_tools.py
-spikeinterface/core/datasets.py
-spikeinterface/core/frameslicerecording.py
-spikeinterface/core/frameslicesorting.py
-spikeinterface/core/generate.py
-spikeinterface/core/globals.py
-spikeinterface/core/injecttemplates.py
-spikeinterface/core/job_tools.py
-spikeinterface/core/npyfoldersnippets.py
-spikeinterface/core/npysnippetsextractor.py
-spikeinterface/core/npzfolder.py
-spikeinterface/core/npzsortingextractor.py
-spikeinterface/core/numpyextractors.py
-spikeinterface/core/old_api_utils.py
-spikeinterface/core/recording_tools.py
-spikeinterface/core/segmentutils.py
-spikeinterface/core/snippets_tools.py
-spikeinterface/core/sparsity.py
-spikeinterface/core/template_tools.py
-spikeinterface/core/testing.py
-spikeinterface/core/testing_tools.py
-spikeinterface/core/unitsaggregationsorting.py
-spikeinterface/core/unitsselectionsorting.py
-spikeinterface/core/waveform_extractor.py
-spikeinterface/core/waveform_tools.py
-spikeinterface/core/zarrrecordingextractor.py
-spikeinterface/curation/__init__.py
-spikeinterface/curation/auto_merge.py
-spikeinterface/curation/curation_tools.py
-spikeinterface/curation/curationsorting.py
-spikeinterface/curation/mergeunitssorting.py
-spikeinterface/curation/remove_duplicated_spikes.py
-spikeinterface/curation/remove_excess_spikes.py
-spikeinterface/curation/remove_redundant.py
-spikeinterface/curation/sortingview_curation.py
-spikeinterface/curation/splitunitsorting.py
-spikeinterface/exporters/__init__.py
-spikeinterface/exporters/report.py
-spikeinterface/exporters/to_phy.py
-spikeinterface/extractors/__init__.py
-spikeinterface/extractors/alfsortingextractor.py
-spikeinterface/extractors/bids.py
-spikeinterface/extractors/cbin_ibl.py
-spikeinterface/extractors/cellexplorersortingextractor.py
-spikeinterface/extractors/combinatoextractors.py
-spikeinterface/extractors/extractorlist.py
-spikeinterface/extractors/hdsortextractors.py
-spikeinterface/extractors/herdingspikesextractors.py
-spikeinterface/extractors/iblstreamingrecording.py
-spikeinterface/extractors/klustaextractors.py
-spikeinterface/extractors/matlabhelpers.py
-spikeinterface/extractors/mclustextractors.py
-spikeinterface/extractors/mcsh5extractors.py
-spikeinterface/extractors/mdaextractors.py
-spikeinterface/extractors/neuropixels_utils.py
-spikeinterface/extractors/nwbextractors.py
-spikeinterface/extractors/phykilosortextractors.py
-spikeinterface/extractors/shybridextractors.py
-spikeinterface/extractors/spykingcircusextractors.py
-spikeinterface/extractors/toy_example.py
-spikeinterface/extractors/tridesclousextractors.py
-spikeinterface/extractors/waveclussnippetstextractors.py
-spikeinterface/extractors/waveclustextractors.py
-spikeinterface/extractors/yassextractors.py
-spikeinterface/extractors/neoextractors/__init__.py
-spikeinterface/extractors/neoextractors/alphaomega.py
-spikeinterface/extractors/neoextractors/axona.py
-spikeinterface/extractors/neoextractors/biocam.py
-spikeinterface/extractors/neoextractors/blackrock.py
-spikeinterface/extractors/neoextractors/ced.py
-spikeinterface/extractors/neoextractors/edf.py
-spikeinterface/extractors/neoextractors/intan.py
-spikeinterface/extractors/neoextractors/maxwell.py
-spikeinterface/extractors/neoextractors/mcsraw.py
-spikeinterface/extractors/neoextractors/mearec.py
-spikeinterface/extractors/neoextractors/neo_utils.py
-spikeinterface/extractors/neoextractors/neobaseextractor.py
-spikeinterface/extractors/neoextractors/neuralynx.py
-spikeinterface/extractors/neoextractors/neuroscope.py
-spikeinterface/extractors/neoextractors/nix.py
-spikeinterface/extractors/neoextractors/openephys.py
-spikeinterface/extractors/neoextractors/plexon.py
-spikeinterface/extractors/neoextractors/spike2.py
-spikeinterface/extractors/neoextractors/spikegadgets.py
-spikeinterface/extractors/neoextractors/spikeglx.py
-spikeinterface/extractors/neoextractors/tdt.py
-spikeinterface/postprocessing/__init__.py
-spikeinterface/postprocessing/alignsorting.py
-spikeinterface/postprocessing/correlograms.py
-spikeinterface/postprocessing/isi.py
-spikeinterface/postprocessing/noise_level.py
-spikeinterface/postprocessing/principal_component.py
-spikeinterface/postprocessing/spike_amplitudes.py
-spikeinterface/postprocessing/spike_locations.py
-spikeinterface/postprocessing/template_metrics.py
-spikeinterface/postprocessing/template_similarity.py
-spikeinterface/postprocessing/template_tools.py
-spikeinterface/postprocessing/unit_localization.py
-spikeinterface/preprocessing/__init__.py
-spikeinterface/preprocessing/align_snippets.py
-spikeinterface/preprocessing/basepreprocessor.py
-spikeinterface/preprocessing/clip.py
-spikeinterface/preprocessing/common_reference.py
-spikeinterface/preprocessing/correct_lsb.py
-spikeinterface/preprocessing/detect_bad_channels.py
-spikeinterface/preprocessing/filter.py
-spikeinterface/preprocessing/filter_opencl.py
-spikeinterface/preprocessing/highpass_spatial_filter.py
-spikeinterface/preprocessing/interpolate_bad_channels.py
-spikeinterface/preprocessing/normalize_scale.py
-spikeinterface/preprocessing/phase_shift.py
-spikeinterface/preprocessing/preprocessing_tools.py
-spikeinterface/preprocessing/preprocessinglist.py
-spikeinterface/preprocessing/rectify.py
-spikeinterface/preprocessing/remove_artifacts.py
-spikeinterface/preprocessing/resample.py
-spikeinterface/preprocessing/whiten.py
-spikeinterface/preprocessing/zero_channel_pad.py
-spikeinterface/preprocessing/deepinterpolation/__init__.py
-spikeinterface/preprocessing/deepinterpolation/deepinterpolation.py
-spikeinterface/qualitymetrics/__init__.py
-spikeinterface/qualitymetrics/misc_metrics.py
-spikeinterface/qualitymetrics/pca_metrics.py
-spikeinterface/qualitymetrics/quality_metric_calculator.py
-spikeinterface/qualitymetrics/quality_metric_list.py
-spikeinterface/qualitymetrics/utils.py
-spikeinterface/sorters/__init__.py
-spikeinterface/sorters/basesorter.py
-spikeinterface/sorters/launcher.py
-spikeinterface/sorters/runsorter.py
-spikeinterface/sorters/sorterlist.py
-spikeinterface/sorters/external/__init__.py
-spikeinterface/sorters/external/combinato.py
-spikeinterface/sorters/external/hdsort.py
-spikeinterface/sorters/external/hdsort_master.m
-spikeinterface/sorters/external/herdingspikes.py
-spikeinterface/sorters/external/ironclust.py
-spikeinterface/sorters/external/kilosort.py
-spikeinterface/sorters/external/kilosort2.py
-spikeinterface/sorters/external/kilosort2_5.py
-spikeinterface/sorters/external/kilosort2_5_master.m
-spikeinterface/sorters/external/kilosort2_master.m
-spikeinterface/sorters/external/kilosort3.py
-spikeinterface/sorters/external/kilosort3_master.m
-spikeinterface/sorters/external/kilosort_master.m
-spikeinterface/sorters/external/kilosortbase.py
-spikeinterface/sorters/external/klusta.py
-spikeinterface/sorters/external/klusta_config_default.prm
-spikeinterface/sorters/external/mountainsort4.py
-spikeinterface/sorters/external/pykilosort.py
-spikeinterface/sorters/external/sc_config_default.params
-spikeinterface/sorters/external/spyking_circus.py
-spikeinterface/sorters/external/tridesclous.py
-spikeinterface/sorters/external/waveclus.py
-spikeinterface/sorters/external/waveclus_master.m
-spikeinterface/sorters/external/waveclus_snippets.py
-spikeinterface/sorters/external/waveclus_snippets_master.m
-spikeinterface/sorters/external/yass.py
-spikeinterface/sorters/external/yass_config_default.yaml
-spikeinterface/sorters/internal/__init__.py
-spikeinterface/sorters/internal/si_based.py
-spikeinterface/sorters/internal/spyking_circus2.py
-spikeinterface/sorters/internal/tridesclous2.py
-spikeinterface/sorters/utils/__init__.py
-spikeinterface/sorters/utils/constructNPYheader.m
-spikeinterface/sorters/utils/misc.py
-spikeinterface/sorters/utils/shellscript.py
-spikeinterface/sorters/utils/writeNPY.m
-spikeinterface/sortingcomponents/__init__.py
-spikeinterface/sortingcomponents/features_from_peaks.py
-spikeinterface/sortingcomponents/motion_correction.py
-spikeinterface/sortingcomponents/motion_estimation.py
-spikeinterface/sortingcomponents/peak_detection.py
-spikeinterface/sortingcomponents/peak_localization.py
-spikeinterface/sortingcomponents/peak_pipeline.py
-spikeinterface/sortingcomponents/peak_selection.py
-spikeinterface/sortingcomponents/tools.py
-spikeinterface/sortingcomponents/benchmark/__init__.py
-spikeinterface/sortingcomponents/benchmark/benchmark_clustering.py
-spikeinterface/sortingcomponents/benchmark/benchmark_matching.py
-spikeinterface/sortingcomponents/benchmark/benchmark_motion_correction.py
-spikeinterface/sortingcomponents/benchmark/benchmark_motion_estimation.py
-spikeinterface/sortingcomponents/benchmark/benchmark_peak_selection.py
-spikeinterface/sortingcomponents/benchmark/benchmark_tools.py
-spikeinterface/sortingcomponents/clustering/__init__.py
-spikeinterface/sortingcomponents/clustering/circus.py
-spikeinterface/sortingcomponents/clustering/clustering_tools.py
-spikeinterface/sortingcomponents/clustering/dummy.py
-spikeinterface/sortingcomponents/clustering/isocut5.py
-spikeinterface/sortingcomponents/clustering/main.py
-spikeinterface/sortingcomponents/clustering/method_list.py
-spikeinterface/sortingcomponents/clustering/position.py
-spikeinterface/sortingcomponents/clustering/position_and_features.py
-spikeinterface/sortingcomponents/clustering/position_and_pca.py
-spikeinterface/sortingcomponents/clustering/position_ptp_scaled.py
-spikeinterface/sortingcomponents/clustering/random_projections.py
-spikeinterface/sortingcomponents/clustering/sliding_hdbscan.py
-spikeinterface/sortingcomponents/clustering/sliding_nn.py
-spikeinterface/sortingcomponents/clustering/triage.py
-spikeinterface/sortingcomponents/matching/__init__.py
-spikeinterface/sortingcomponents/matching/circus.py
-spikeinterface/sortingcomponents/matching/main.py
-spikeinterface/sortingcomponents/matching/method_list.py
-spikeinterface/sortingcomponents/matching/naive.py
-spikeinterface/sortingcomponents/matching/tdc.py
-spikeinterface/sortingcomponents/waveforms/__init__.py
-spikeinterface/sortingcomponents/waveforms/neural_network_denoiser.py
-spikeinterface/sortingcomponents/waveforms/temporal_pca.py
-spikeinterface/sortingcomponents/waveforms/waveform_utils.py
-spikeinterface/widgets/__init__.py
-spikeinterface/widgets/all_amplitudes_distributions.py
-spikeinterface/widgets/amplitudes.py
-spikeinterface/widgets/autocorrelograms.py
-spikeinterface/widgets/base.py
-spikeinterface/widgets/crosscorrelograms.py
-spikeinterface/widgets/metrics.py
-spikeinterface/widgets/quality_metrics.py
-spikeinterface/widgets/sorting_summary.py
-spikeinterface/widgets/spike_locations.py
-spikeinterface/widgets/spikes_on_traces.py
-spikeinterface/widgets/template_metrics.py
-spikeinterface/widgets/template_similarity.py
-spikeinterface/widgets/timeseries.py
-spikeinterface/widgets/unit_depths.py
-spikeinterface/widgets/unit_locations.py
-spikeinterface/widgets/unit_summary.py
-spikeinterface/widgets/unit_templates.py
-spikeinterface/widgets/unit_waveforms.py
-spikeinterface/widgets/unit_waveforms_density_map.py
-spikeinterface/widgets/utils.py
-spikeinterface/widgets/widget_list.py
-spikeinterface/widgets/_legacy_mpl_widgets/__init__.py
-spikeinterface/widgets/_legacy_mpl_widgets/activity.py
-spikeinterface/widgets/_legacy_mpl_widgets/agreementmatrix.py
-spikeinterface/widgets/_legacy_mpl_widgets/amplitudes.py
-spikeinterface/widgets/_legacy_mpl_widgets/basewidget.py
-spikeinterface/widgets/_legacy_mpl_widgets/collisioncomp.py
-spikeinterface/widgets/_legacy_mpl_widgets/confusionmatrix.py
-spikeinterface/widgets/_legacy_mpl_widgets/correlogramcomp.py
-spikeinterface/widgets/_legacy_mpl_widgets/correlograms_.py
-spikeinterface/widgets/_legacy_mpl_widgets/depthamplitude.py
-spikeinterface/widgets/_legacy_mpl_widgets/drift.py
-spikeinterface/widgets/_legacy_mpl_widgets/gtcomparison.py
-spikeinterface/widgets/_legacy_mpl_widgets/gtstudy.py
-spikeinterface/widgets/_legacy_mpl_widgets/isidistribution.py
-spikeinterface/widgets/_legacy_mpl_widgets/multicompgraph.py
-spikeinterface/widgets/_legacy_mpl_widgets/presence.py
-spikeinterface/widgets/_legacy_mpl_widgets/principalcomponent.py
-spikeinterface/widgets/_legacy_mpl_widgets/probemap.py
-spikeinterface/widgets/_legacy_mpl_widgets/rasters.py
-spikeinterface/widgets/_legacy_mpl_widgets/sortingperformance.py
-spikeinterface/widgets/_legacy_mpl_widgets/timeseries_.py
-spikeinterface/widgets/_legacy_mpl_widgets/unitlocalization_.py
-spikeinterface/widgets/_legacy_mpl_widgets/unitprobemap.py
-spikeinterface/widgets/_legacy_mpl_widgets/unitsummary.py
-spikeinterface/widgets/_legacy_mpl_widgets/unitwaveformdensitymap_.py
-spikeinterface/widgets/_legacy_mpl_widgets/unitwaveforms_.py
-spikeinterface/widgets/_legacy_mpl_widgets/utils.py
-spikeinterface/widgets/ipywidgets/__init__.py
-spikeinterface/widgets/ipywidgets/amplitudes.py
-spikeinterface/widgets/ipywidgets/base_ipywidgets.py
-spikeinterface/widgets/ipywidgets/metrics.py
-spikeinterface/widgets/ipywidgets/quality_metrics.py
-spikeinterface/widgets/ipywidgets/spike_locations.py
-spikeinterface/widgets/ipywidgets/spikes_on_traces.py
-spikeinterface/widgets/ipywidgets/template_metrics.py
-spikeinterface/widgets/ipywidgets/timeseries.py
-spikeinterface/widgets/ipywidgets/unit_locations.py
-spikeinterface/widgets/ipywidgets/unit_templates.py
-spikeinterface/widgets/ipywidgets/unit_waveforms.py
-spikeinterface/widgets/ipywidgets/utils.py
-spikeinterface/widgets/matplotlib/__init__.py
-spikeinterface/widgets/matplotlib/all_amplitudes_distributions.py
-spikeinterface/widgets/matplotlib/amplitudes.py
-spikeinterface/widgets/matplotlib/autocorrelograms.py
-spikeinterface/widgets/matplotlib/base_mpl.py
-spikeinterface/widgets/matplotlib/crosscorrelograms.py
-spikeinterface/widgets/matplotlib/metrics.py
-spikeinterface/widgets/matplotlib/quality_metrics.py
-spikeinterface/widgets/matplotlib/spike_locations.py
-spikeinterface/widgets/matplotlib/spikes_on_traces.py
-spikeinterface/widgets/matplotlib/template_metrics.py
-spikeinterface/widgets/matplotlib/template_similarity.py
-spikeinterface/widgets/matplotlib/timeseries.py
-spikeinterface/widgets/matplotlib/unit_depths.py
-spikeinterface/widgets/matplotlib/unit_locations.py
-spikeinterface/widgets/matplotlib/unit_summary.py
-spikeinterface/widgets/matplotlib/unit_templates.py
-spikeinterface/widgets/matplotlib/unit_waveforms.py
-spikeinterface/widgets/matplotlib/unit_waveforms_density_map.py
-spikeinterface/widgets/sortingview/__init__.py
-spikeinterface/widgets/sortingview/amplitudes.py
-spikeinterface/widgets/sortingview/autocorrelograms.py
-spikeinterface/widgets/sortingview/base_sortingview.py
-spikeinterface/widgets/sortingview/crosscorrelograms.py
-spikeinterface/widgets/sortingview/metrics.py
-spikeinterface/widgets/sortingview/quality_metrics.py
-spikeinterface/widgets/sortingview/sorting_summary.py
-spikeinterface/widgets/sortingview/spike_locations.py
-spikeinterface/widgets/sortingview/template_metrics.py
-spikeinterface/widgets/sortingview/template_similarity.py
-spikeinterface/widgets/sortingview/timeseries.py
-spikeinterface/widgets/sortingview/unit_locations.py
-spikeinterface/widgets/sortingview/unit_templates.py
+src/spikeinterface/__init__.py
+src/spikeinterface/full.py
+src/spikeinterface.egg-info/PKG-INFO
+src/spikeinterface.egg-info/SOURCES.txt
+src/spikeinterface.egg-info/dependency_links.txt
+src/spikeinterface.egg-info/requires.txt
+src/spikeinterface.egg-info/top_level.txt
+src/spikeinterface/comparison/__init__.py
+src/spikeinterface/comparison/basecomparison.py
+src/spikeinterface/comparison/collisioncomparison.py
+src/spikeinterface/comparison/collisionstudy.py
+src/spikeinterface/comparison/comparisontools.py
+src/spikeinterface/comparison/correlogramcomparison.py
+src/spikeinterface/comparison/correlogramstudy.py
+src/spikeinterface/comparison/groundtruthstudy.py
+src/spikeinterface/comparison/hybrid.py
+src/spikeinterface/comparison/multicomparisons.py
+src/spikeinterface/comparison/paircomparisons.py
+src/spikeinterface/comparison/studytools.py
+src/spikeinterface/core/__init__.py
+src/spikeinterface/core/base.py
+src/spikeinterface/core/baseevent.py
+src/spikeinterface/core/baserecording.py
+src/spikeinterface/core/baserecordingsnippets.py
+src/spikeinterface/core/basesnippets.py
+src/spikeinterface/core/basesorting.py
+src/spikeinterface/core/binaryfolder.py
+src/spikeinterface/core/binaryrecordingextractor.py
+src/spikeinterface/core/channelsaggregationrecording.py
+src/spikeinterface/core/channelslice.py
+src/spikeinterface/core/core_tools.py
+src/spikeinterface/core/datasets.py
+src/spikeinterface/core/frameslicerecording.py
+src/spikeinterface/core/frameslicesorting.py
+src/spikeinterface/core/generate.py
+src/spikeinterface/core/globals.py
+src/spikeinterface/core/injecttemplates.py
+src/spikeinterface/core/job_tools.py
+src/spikeinterface/core/npyfoldersnippets.py
+src/spikeinterface/core/npysnippetsextractor.py
+src/spikeinterface/core/npzfolder.py
+src/spikeinterface/core/npzsortingextractor.py
+src/spikeinterface/core/numpyextractors.py
+src/spikeinterface/core/old_api_utils.py
+src/spikeinterface/core/recording_tools.py
+src/spikeinterface/core/segmentutils.py
+src/spikeinterface/core/snippets_tools.py
+src/spikeinterface/core/sparsity.py
+src/spikeinterface/core/template_tools.py
+src/spikeinterface/core/testing.py
+src/spikeinterface/core/testing_tools.py
+src/spikeinterface/core/unitsaggregationsorting.py
+src/spikeinterface/core/unitsselectionsorting.py
+src/spikeinterface/core/waveform_extractor.py
+src/spikeinterface/core/waveform_tools.py
+src/spikeinterface/core/zarrrecordingextractor.py
+src/spikeinterface/curation/__init__.py
+src/spikeinterface/curation/auto_merge.py
+src/spikeinterface/curation/curation_tools.py
+src/spikeinterface/curation/curationsorting.py
+src/spikeinterface/curation/mergeunitssorting.py
+src/spikeinterface/curation/remove_duplicated_spikes.py
+src/spikeinterface/curation/remove_excess_spikes.py
+src/spikeinterface/curation/remove_redundant.py
+src/spikeinterface/curation/sortingview_curation.py
+src/spikeinterface/curation/splitunitsorting.py
+src/spikeinterface/exporters/__init__.py
+src/spikeinterface/exporters/report.py
+src/spikeinterface/exporters/to_phy.py
+src/spikeinterface/extractors/__init__.py
+src/spikeinterface/extractors/alfsortingextractor.py
+src/spikeinterface/extractors/bids.py
+src/spikeinterface/extractors/cbin_ibl.py
+src/spikeinterface/extractors/cellexplorersortingextractor.py
+src/spikeinterface/extractors/combinatoextractors.py
+src/spikeinterface/extractors/extractorlist.py
+src/spikeinterface/extractors/hdsortextractors.py
+src/spikeinterface/extractors/herdingspikesextractors.py
+src/spikeinterface/extractors/iblstreamingrecording.py
+src/spikeinterface/extractors/klustaextractors.py
+src/spikeinterface/extractors/matlabhelpers.py
+src/spikeinterface/extractors/mclustextractors.py
+src/spikeinterface/extractors/mcsh5extractors.py
+src/spikeinterface/extractors/mdaextractors.py
+src/spikeinterface/extractors/neuropixels_utils.py
+src/spikeinterface/extractors/nwbextractors.py
+src/spikeinterface/extractors/phykilosortextractors.py
+src/spikeinterface/extractors/shybridextractors.py
+src/spikeinterface/extractors/spykingcircusextractors.py
+src/spikeinterface/extractors/toy_example.py
+src/spikeinterface/extractors/tridesclousextractors.py
+src/spikeinterface/extractors/waveclussnippetstextractors.py
+src/spikeinterface/extractors/waveclustextractors.py
+src/spikeinterface/extractors/yassextractors.py
+src/spikeinterface/extractors/neoextractors/__init__.py
+src/spikeinterface/extractors/neoextractors/alphaomega.py
+src/spikeinterface/extractors/neoextractors/axona.py
+src/spikeinterface/extractors/neoextractors/biocam.py
+src/spikeinterface/extractors/neoextractors/blackrock.py
+src/spikeinterface/extractors/neoextractors/ced.py
+src/spikeinterface/extractors/neoextractors/edf.py
+src/spikeinterface/extractors/neoextractors/intan.py
+src/spikeinterface/extractors/neoextractors/maxwell.py
+src/spikeinterface/extractors/neoextractors/mcsraw.py
+src/spikeinterface/extractors/neoextractors/mearec.py
+src/spikeinterface/extractors/neoextractors/neo_utils.py
+src/spikeinterface/extractors/neoextractors/neobaseextractor.py
+src/spikeinterface/extractors/neoextractors/neuralynx.py
+src/spikeinterface/extractors/neoextractors/neuroscope.py
+src/spikeinterface/extractors/neoextractors/nix.py
+src/spikeinterface/extractors/neoextractors/openephys.py
+src/spikeinterface/extractors/neoextractors/plexon.py
+src/spikeinterface/extractors/neoextractors/spike2.py
+src/spikeinterface/extractors/neoextractors/spikegadgets.py
+src/spikeinterface/extractors/neoextractors/spikeglx.py
+src/spikeinterface/extractors/neoextractors/tdt.py
+src/spikeinterface/postprocessing/__init__.py
+src/spikeinterface/postprocessing/alignsorting.py
+src/spikeinterface/postprocessing/amplitude_scalings.py
+src/spikeinterface/postprocessing/correlograms.py
+src/spikeinterface/postprocessing/isi.py
+src/spikeinterface/postprocessing/noise_level.py
+src/spikeinterface/postprocessing/principal_component.py
+src/spikeinterface/postprocessing/spike_amplitudes.py
+src/spikeinterface/postprocessing/spike_locations.py
+src/spikeinterface/postprocessing/template_metrics.py
+src/spikeinterface/postprocessing/template_similarity.py
+src/spikeinterface/postprocessing/template_tools.py
+src/spikeinterface/postprocessing/unit_localization.py
+src/spikeinterface/preprocessing/__init__.py
+src/spikeinterface/preprocessing/align_snippets.py
+src/spikeinterface/preprocessing/astype.py
+src/spikeinterface/preprocessing/average_across_direction.py
+src/spikeinterface/preprocessing/basepreprocessor.py
+src/spikeinterface/preprocessing/clip.py
+src/spikeinterface/preprocessing/common_reference.py
+src/spikeinterface/preprocessing/correct_lsb.py
+src/spikeinterface/preprocessing/depth_order.py
+src/spikeinterface/preprocessing/detect_bad_channels.py
+src/spikeinterface/preprocessing/directional_derivative.py
+src/spikeinterface/preprocessing/filter.py
+src/spikeinterface/preprocessing/filter_gaussian.py
+src/spikeinterface/preprocessing/filter_opencl.py
+src/spikeinterface/preprocessing/highpass_spatial_filter.py
+src/spikeinterface/preprocessing/interpolate_bad_channels.py
+src/spikeinterface/preprocessing/motion.py
+src/spikeinterface/preprocessing/normalize_scale.py
+src/spikeinterface/preprocessing/phase_shift.py
+src/spikeinterface/preprocessing/preprocessing_tools.py
+src/spikeinterface/preprocessing/preprocessinglist.py
+src/spikeinterface/preprocessing/rectify.py
+src/spikeinterface/preprocessing/remove_artifacts.py
+src/spikeinterface/preprocessing/resample.py
+src/spikeinterface/preprocessing/silence_periods.py
+src/spikeinterface/preprocessing/unsigned_to_signed.py
+src/spikeinterface/preprocessing/whiten.py
+src/spikeinterface/preprocessing/zero_channel_pad.py
+src/spikeinterface/preprocessing/deepinterpolation/__init__.py
+src/spikeinterface/preprocessing/deepinterpolation/deepinterpolation.py
+src/spikeinterface/qualitymetrics/__init__.py
+src/spikeinterface/qualitymetrics/misc_metrics.py
+src/spikeinterface/qualitymetrics/pca_metrics.py
+src/spikeinterface/qualitymetrics/quality_metric_calculator.py
+src/spikeinterface/qualitymetrics/quality_metric_list.py
+src/spikeinterface/qualitymetrics/utils.py
+src/spikeinterface/sorters/__init__.py
+src/spikeinterface/sorters/basesorter.py
+src/spikeinterface/sorters/launcher.py
+src/spikeinterface/sorters/runsorter.py
+src/spikeinterface/sorters/sorterlist.py
+src/spikeinterface/sorters/external/__init__.py
+src/spikeinterface/sorters/external/combinato.py
+src/spikeinterface/sorters/external/hdsort.py
+src/spikeinterface/sorters/external/hdsort_master.m
+src/spikeinterface/sorters/external/herdingspikes.py
+src/spikeinterface/sorters/external/ironclust.py
+src/spikeinterface/sorters/external/kilosort.py
+src/spikeinterface/sorters/external/kilosort2.py
+src/spikeinterface/sorters/external/kilosort2_5.py
+src/spikeinterface/sorters/external/kilosort2_5_master.m
+src/spikeinterface/sorters/external/kilosort2_master.m
+src/spikeinterface/sorters/external/kilosort3.py
+src/spikeinterface/sorters/external/kilosort3_master.m
+src/spikeinterface/sorters/external/kilosort_master.m
+src/spikeinterface/sorters/external/kilosortbase.py
+src/spikeinterface/sorters/external/klusta.py
+src/spikeinterface/sorters/external/klusta_config_default.prm
+src/spikeinterface/sorters/external/mountainsort4.py
+src/spikeinterface/sorters/external/mountainsort5.py
+src/spikeinterface/sorters/external/pykilosort.py
+src/spikeinterface/sorters/external/sc_config_default.params
+src/spikeinterface/sorters/external/spyking_circus.py
+src/spikeinterface/sorters/external/tridesclous.py
+src/spikeinterface/sorters/external/waveclus.py
+src/spikeinterface/sorters/external/waveclus_master.m
+src/spikeinterface/sorters/external/waveclus_snippets.py
+src/spikeinterface/sorters/external/waveclus_snippets_master.m
+src/spikeinterface/sorters/external/yass.py
+src/spikeinterface/sorters/external/yass_config_default.yaml
+src/spikeinterface/sorters/internal/__init__.py
+src/spikeinterface/sorters/internal/si_based.py
+src/spikeinterface/sorters/internal/spyking_circus2.py
+src/spikeinterface/sorters/internal/tridesclous2.py
+src/spikeinterface/sorters/utils/__init__.py
+src/spikeinterface/sorters/utils/constructNPYheader.m
+src/spikeinterface/sorters/utils/misc.py
+src/spikeinterface/sorters/utils/shellscript.py
+src/spikeinterface/sorters/utils/writeNPY.m
+src/spikeinterface/sortingcomponents/__init__.py
+src/spikeinterface/sortingcomponents/features_from_peaks.py
+src/spikeinterface/sortingcomponents/motion_estimation.py
+src/spikeinterface/sortingcomponents/motion_interpolation.py
+src/spikeinterface/sortingcomponents/peak_detection.py
+src/spikeinterface/sortingcomponents/peak_localization.py
+src/spikeinterface/sortingcomponents/peak_pipeline.py
+src/spikeinterface/sortingcomponents/peak_selection.py
+src/spikeinterface/sortingcomponents/tools.py
+src/spikeinterface/sortingcomponents/benchmark/__init__.py
+src/spikeinterface/sortingcomponents/benchmark/benchmark_clustering.py
+src/spikeinterface/sortingcomponents/benchmark/benchmark_matching.py
+src/spikeinterface/sortingcomponents/benchmark/benchmark_motion_estimation.py
+src/spikeinterface/sortingcomponents/benchmark/benchmark_motion_interpolation.py
+src/spikeinterface/sortingcomponents/benchmark/benchmark_peak_localization.py
+src/spikeinterface/sortingcomponents/benchmark/benchmark_peak_selection.py
+src/spikeinterface/sortingcomponents/benchmark/benchmark_tools.py
+src/spikeinterface/sortingcomponents/clustering/__init__.py
+src/spikeinterface/sortingcomponents/clustering/circus.py
+src/spikeinterface/sortingcomponents/clustering/clustering_tools.py
+src/spikeinterface/sortingcomponents/clustering/dummy.py
+src/spikeinterface/sortingcomponents/clustering/isocut5.py
+src/spikeinterface/sortingcomponents/clustering/main.py
+src/spikeinterface/sortingcomponents/clustering/method_list.py
+src/spikeinterface/sortingcomponents/clustering/position.py
+src/spikeinterface/sortingcomponents/clustering/position_and_features.py
+src/spikeinterface/sortingcomponents/clustering/position_and_pca.py
+src/spikeinterface/sortingcomponents/clustering/position_ptp_scaled.py
+src/spikeinterface/sortingcomponents/clustering/random_projections.py
+src/spikeinterface/sortingcomponents/clustering/sliding_hdbscan.py
+src/spikeinterface/sortingcomponents/clustering/sliding_nn.py
+src/spikeinterface/sortingcomponents/clustering/triage.py
+src/spikeinterface/sortingcomponents/matching/__init__.py
+src/spikeinterface/sortingcomponents/matching/circus.py
+src/spikeinterface/sortingcomponents/matching/main.py
+src/spikeinterface/sortingcomponents/matching/method_list.py
+src/spikeinterface/sortingcomponents/matching/naive.py
+src/spikeinterface/sortingcomponents/matching/tdc.py
+src/spikeinterface/sortingcomponents/matching/wobble.py
+src/spikeinterface/sortingcomponents/waveforms/__init__.py
+src/spikeinterface/sortingcomponents/waveforms/neural_network_denoiser.py
+src/spikeinterface/sortingcomponents/waveforms/savgol_denoiser.py
+src/spikeinterface/sortingcomponents/waveforms/temporal_pca.py
+src/spikeinterface/sortingcomponents/waveforms/waveform_thresholder.py
+src/spikeinterface/sortingcomponents/waveforms/waveform_utils.py
+src/spikeinterface/widgets/__init__.py
+src/spikeinterface/widgets/all_amplitudes_distributions.py
+src/spikeinterface/widgets/amplitudes.py
+src/spikeinterface/widgets/autocorrelograms.py
+src/spikeinterface/widgets/base.py
+src/spikeinterface/widgets/crosscorrelograms.py
+src/spikeinterface/widgets/metrics.py
+src/spikeinterface/widgets/motion.py
+src/spikeinterface/widgets/quality_metrics.py
+src/spikeinterface/widgets/sorting_summary.py
+src/spikeinterface/widgets/spike_locations.py
+src/spikeinterface/widgets/spikes_on_traces.py
+src/spikeinterface/widgets/template_metrics.py
+src/spikeinterface/widgets/template_similarity.py
+src/spikeinterface/widgets/timeseries.py
+src/spikeinterface/widgets/unit_depths.py
+src/spikeinterface/widgets/unit_locations.py
+src/spikeinterface/widgets/unit_summary.py
+src/spikeinterface/widgets/unit_templates.py
+src/spikeinterface/widgets/unit_waveforms.py
+src/spikeinterface/widgets/unit_waveforms_density_map.py
+src/spikeinterface/widgets/utils.py
+src/spikeinterface/widgets/widget_list.py
+src/spikeinterface/widgets/_legacy_mpl_widgets/__init__.py
+src/spikeinterface/widgets/_legacy_mpl_widgets/activity.py
+src/spikeinterface/widgets/_legacy_mpl_widgets/agreementmatrix.py
+src/spikeinterface/widgets/_legacy_mpl_widgets/amplitudes.py
+src/spikeinterface/widgets/_legacy_mpl_widgets/basewidget.py
+src/spikeinterface/widgets/_legacy_mpl_widgets/collisioncomp.py
+src/spikeinterface/widgets/_legacy_mpl_widgets/confusionmatrix.py
+src/spikeinterface/widgets/_legacy_mpl_widgets/correlogramcomp.py
+src/spikeinterface/widgets/_legacy_mpl_widgets/correlograms_.py
+src/spikeinterface/widgets/_legacy_mpl_widgets/depthamplitude.py
+src/spikeinterface/widgets/_legacy_mpl_widgets/gtcomparison.py
+src/spikeinterface/widgets/_legacy_mpl_widgets/gtstudy.py
+src/spikeinterface/widgets/_legacy_mpl_widgets/isidistribution.py
+src/spikeinterface/widgets/_legacy_mpl_widgets/multicompgraph.py
+src/spikeinterface/widgets/_legacy_mpl_widgets/presence.py
+src/spikeinterface/widgets/_legacy_mpl_widgets/principalcomponent.py
+src/spikeinterface/widgets/_legacy_mpl_widgets/probemap.py
+src/spikeinterface/widgets/_legacy_mpl_widgets/rasters.py
+src/spikeinterface/widgets/_legacy_mpl_widgets/sortingperformance.py
+src/spikeinterface/widgets/_legacy_mpl_widgets/timeseries_.py
+src/spikeinterface/widgets/_legacy_mpl_widgets/unitlocalization_.py
+src/spikeinterface/widgets/_legacy_mpl_widgets/unitprobemap.py
+src/spikeinterface/widgets/_legacy_mpl_widgets/unitsummary.py
+src/spikeinterface/widgets/_legacy_mpl_widgets/unitwaveformdensitymap_.py
+src/spikeinterface/widgets/_legacy_mpl_widgets/unitwaveforms_.py
+src/spikeinterface/widgets/_legacy_mpl_widgets/utils.py
+src/spikeinterface/widgets/ipywidgets/__init__.py
+src/spikeinterface/widgets/ipywidgets/amplitudes.py
+src/spikeinterface/widgets/ipywidgets/base_ipywidgets.py
+src/spikeinterface/widgets/ipywidgets/metrics.py
+src/spikeinterface/widgets/ipywidgets/quality_metrics.py
+src/spikeinterface/widgets/ipywidgets/spike_locations.py
+src/spikeinterface/widgets/ipywidgets/spikes_on_traces.py
+src/spikeinterface/widgets/ipywidgets/template_metrics.py
+src/spikeinterface/widgets/ipywidgets/timeseries.py
+src/spikeinterface/widgets/ipywidgets/unit_locations.py
+src/spikeinterface/widgets/ipywidgets/unit_templates.py
+src/spikeinterface/widgets/ipywidgets/unit_waveforms.py
+src/spikeinterface/widgets/ipywidgets/utils.py
+src/spikeinterface/widgets/matplotlib/__init__.py
+src/spikeinterface/widgets/matplotlib/all_amplitudes_distributions.py
+src/spikeinterface/widgets/matplotlib/amplitudes.py
+src/spikeinterface/widgets/matplotlib/autocorrelograms.py
+src/spikeinterface/widgets/matplotlib/base_mpl.py
+src/spikeinterface/widgets/matplotlib/crosscorrelograms.py
+src/spikeinterface/widgets/matplotlib/metrics.py
+src/spikeinterface/widgets/matplotlib/motion.py
+src/spikeinterface/widgets/matplotlib/quality_metrics.py
+src/spikeinterface/widgets/matplotlib/spike_locations.py
+src/spikeinterface/widgets/matplotlib/spikes_on_traces.py
+src/spikeinterface/widgets/matplotlib/template_metrics.py
+src/spikeinterface/widgets/matplotlib/template_similarity.py
+src/spikeinterface/widgets/matplotlib/timeseries.py
+src/spikeinterface/widgets/matplotlib/unit_depths.py
+src/spikeinterface/widgets/matplotlib/unit_locations.py
+src/spikeinterface/widgets/matplotlib/unit_summary.py
+src/spikeinterface/widgets/matplotlib/unit_templates.py
+src/spikeinterface/widgets/matplotlib/unit_waveforms.py
+src/spikeinterface/widgets/matplotlib/unit_waveforms_density_map.py
+src/spikeinterface/widgets/sortingview/__init__.py
+src/spikeinterface/widgets/sortingview/amplitudes.py
+src/spikeinterface/widgets/sortingview/autocorrelograms.py
+src/spikeinterface/widgets/sortingview/base_sortingview.py
+src/spikeinterface/widgets/sortingview/crosscorrelograms.py
+src/spikeinterface/widgets/sortingview/metrics.py
+src/spikeinterface/widgets/sortingview/quality_metrics.py
+src/spikeinterface/widgets/sortingview/sorting_summary.py
+src/spikeinterface/widgets/sortingview/spike_locations.py
+src/spikeinterface/widgets/sortingview/template_metrics.py
+src/spikeinterface/widgets/sortingview/template_similarity.py
+src/spikeinterface/widgets/sortingview/timeseries.py
+src/spikeinterface/widgets/sortingview/unit_locations.py
+src/spikeinterface/widgets/sortingview/unit_templates.py
```

