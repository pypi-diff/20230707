# Comparing `tmp/pygmtools-0.3.8.tar.gz` & `tmp/pygmtools-0.3.8a0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "pygmtools-0.3.8.tar", last modified: Wed May  3 10:13:43 2023, max compression
+gzip compressed data, was "pygmtools-0.3.8a0.tar", last modified: Fri Jul  7 09:24:04 2023, max compression
```

## Comparing `pygmtools-0.3.8.tar` & `pygmtools-0.3.8a0.tar`

### file list

```diff
@@ -1,41 +1,43 @@
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 10:13:43.109770 pygmtools-0.3.8/
--rwxr-xr-x   0 runner    (1001) docker     (123)      508 2023-05-03 10:13:27.000000 pygmtools-0.3.8/LICENSE
--rw-r--r--   0 runner    (1001) docker     (123)       18 2023-05-03 10:13:27.000000 pygmtools-0.3.8/MANIFEST.in
--rw-r--r--   0 runner    (1001) docker     (123)    13602 2023-05-03 10:13:43.109770 pygmtools-0.3.8/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)    12718 2023-05-03 10:13:27.000000 pygmtools-0.3.8/README.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 10:13:43.105770 pygmtools-0.3.8/pygmtools/
--rw-r--r--   0 runner    (1001) docker     (123)     2038 2023-05-03 10:13:28.000000 pygmtools-0.3.8/pygmtools/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    26415 2023-05-03 10:13:28.000000 pygmtools-0.3.8/pygmtools/benchmark.py
--rw-r--r--   0 runner    (1001) docker     (123)    47379 2023-05-03 10:13:28.000000 pygmtools-0.3.8/pygmtools/classic_solvers.py
--rw-r--r--   0 runner    (1001) docker     (123)    57869 2023-05-03 10:13:28.000000 pygmtools-0.3.8/pygmtools/dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     3033 2023-05-03 10:13:28.000000 pygmtools-0.3.8/pygmtools/dataset_config.py
--rw-r--r--   0 runner    (1001) docker     (123)    57459 2023-05-03 10:13:28.000000 pygmtools-0.3.8/pygmtools/jittor_backend.py
--rw-r--r--   0 runner    (1001) docker     (123)    12082 2023-05-03 10:13:28.000000 pygmtools-0.3.8/pygmtools/jittor_modules.py
--rw-r--r--   0 runner    (1001) docker     (123)    75786 2023-05-03 10:13:28.000000 pygmtools-0.3.8/pygmtools/linear_solvers.py
--rw-r--r--   0 runner    (1001) docker     (123)    21183 2023-05-03 10:13:28.000000 pygmtools-0.3.8/pygmtools/mindspore_backend.py
--rw-r--r--   0 runner    (1001) docker     (123)    43089 2023-05-03 10:13:28.000000 pygmtools-0.3.8/pygmtools/multi_graph_solvers.py
--rw-r--r--   0 runner    (1001) docker     (123)    69204 2023-05-03 10:13:28.000000 pygmtools-0.3.8/pygmtools/neural_solvers.py
--rw-r--r--   0 runner    (1001) docker     (123)    59271 2023-05-03 10:13:28.000000 pygmtools-0.3.8/pygmtools/numpy_backend.py
--rw-r--r--   0 runner    (1001) docker     (123)    15282 2023-05-03 10:13:28.000000 pygmtools-0.3.8/pygmtools/numpy_modules.py
--rw-r--r--   0 runner    (1001) docker     (123)    56491 2023-05-03 10:13:28.000000 pygmtools-0.3.8/pygmtools/paddle_backend.py
--rw-r--r--   0 runner    (1001) docker     (123)    11632 2023-05-03 10:13:28.000000 pygmtools-0.3.8/pygmtools/paddle_modules.py
--rw-r--r--   0 runner    (1001) docker     (123)    57454 2023-05-03 10:13:28.000000 pygmtools-0.3.8/pygmtools/pytorch_backend.py
--rw-r--r--   0 runner    (1001) docker     (123)    12042 2023-05-03 10:13:28.000000 pygmtools-0.3.8/pygmtools/pytorch_modules.py
--rw-r--r--   0 runner    (1001) docker     (123)    23046 2023-05-03 10:13:28.000000 pygmtools-0.3.8/pygmtools/tensorflow_backend.py
--rw-r--r--   0 runner    (1001) docker     (123)      499 2023-05-03 10:13:28.000000 pygmtools-0.3.8/pygmtools/tensorflow_modules.py
--rw-r--r--   0 runner    (1001) docker     (123)    50450 2023-05-03 10:13:28.000000 pygmtools-0.3.8/pygmtools/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 10:13:43.109770 pygmtools-0.3.8/pygmtools.egg-info/
--rw-r--r--   0 runner    (1001) docker     (123)    13602 2023-05-03 10:13:43.000000 pygmtools-0.3.8/pygmtools.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)      906 2023-05-03 10:13:43.000000 pygmtools-0.3.8/pygmtools.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-05-03 10:13:43.000000 pygmtools-0.3.8/pygmtools.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (123)      110 2023-05-03 10:13:43.000000 pygmtools-0.3.8/pygmtools.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (123)       10 2023-05-03 10:13:43.000000 pygmtools-0.3.8/pygmtools.egg-info/top_level.txt
--rw-r--r--   0 runner    (1001) docker     (123)       38 2023-05-03 10:13:43.109770 pygmtools-0.3.8/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (123)     3634 2023-05-03 10:13:28.000000 pygmtools-0.3.8/setup.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 10:13:43.109770 pygmtools-0.3.8/tests/
--rw-r--r--   0 runner    (1001) docker     (123)    16074 2023-05-03 10:13:28.000000 pygmtools-0.3.8/tests/test_classic_solvers.py
--rw-r--r--   0 runner    (1001) docker     (123)     5561 2023-05-03 10:13:28.000000 pygmtools-0.3.8/tests/test_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     3800 2023-05-03 10:13:28.000000 pygmtools-0.3.8/tests/test_misc.py
--rw-r--r--   0 runner    (1001) docker     (123)    13302 2023-05-03 10:13:28.000000 pygmtools-0.3.8/tests/test_multi_graph_solvers.py
--rw-r--r--   0 runner    (1001) docker     (123)     9288 2023-05-03 10:13:28.000000 pygmtools-0.3.8/tests/test_neural_solvers.py
--rw-r--r--   0 runner    (1001) docker     (123)     1007 2023-05-03 10:13:28.000000 pygmtools-0.3.8/tests/test_utils.py
+drwxrwxrwx   0        0        0        0 2023-07-07 09:24:04.001748 pygmtools-0.3.8a0/
+-rw-rw-rw-   0        0        0      508 2023-05-03 08:07:01.000000 pygmtools-0.3.8a0/LICENSE
+-rw-rw-rw-   0        0        0       18 2023-05-03 08:07:01.000000 pygmtools-0.3.8a0/MANIFEST.in
+-rw-rw-rw-   0        0        0    13827 2023-07-07 09:24:04.001748 pygmtools-0.3.8a0/PKG-INFO
+-rw-rw-rw-   0        0        0    12718 2023-05-03 08:07:01.000000 pygmtools-0.3.8a0/README.md
+drwxrwxrwx   0        0        0        0 2023-07-07 09:24:03.995376 pygmtools-0.3.8a0/pygmtools/
+-rw-rw-rw-   0        0        0     2119 2023-07-07 09:23:15.000000 pygmtools-0.3.8a0/pygmtools/__init__.py
+-rw-rw-rw-   0        0        0    58368 2023-07-07 09:20:08.000000 pygmtools-0.3.8a0/pygmtools/a_star.cp310-win_amd64.pyd
+-rw-rw-rw-   0        0        0    27000 2023-06-19 13:47:03.000000 pygmtools-0.3.8a0/pygmtools/benchmark.py
+-rw-rw-rw-   0        0        0    58667 2023-06-24 14:15:49.000000 pygmtools-0.3.8a0/pygmtools/classic_solvers.py
+-rw-rw-rw-   0        0        0    59231 2023-06-19 13:47:03.000000 pygmtools-0.3.8a0/pygmtools/dataset.py
+-rw-rw-rw-   0        0        0     3112 2023-06-19 13:47:03.000000 pygmtools-0.3.8a0/pygmtools/dataset_config.py
+-rw-rw-rw-   0        0        0    58908 2023-06-19 13:47:03.000000 pygmtools-0.3.8a0/pygmtools/jittor_backend.py
+-rw-rw-rw-   0        0        0    12371 2023-06-19 13:47:03.000000 pygmtools-0.3.8a0/pygmtools/jittor_modules.py
+-rw-rw-rw-   0        0        0    77128 2023-06-19 13:47:03.000000 pygmtools-0.3.8a0/pygmtools/linear_solvers.py
+-rw-rw-rw-   0        0        0    21736 2023-06-19 13:47:03.000000 pygmtools-0.3.8a0/pygmtools/mindspore_backend.py
+-rw-rw-rw-   0        0        0    44038 2023-06-19 13:47:03.000000 pygmtools-0.3.8a0/pygmtools/multi_graph_solvers.py
+-rw-rw-rw-   0        0        0    70477 2023-06-19 13:47:03.000000 pygmtools-0.3.8a0/pygmtools/neural_solvers.py
+-rw-rw-rw-   0        0        0    60669 2023-06-19 13:47:03.000000 pygmtools-0.3.8a0/pygmtools/numpy_backend.py
+-rw-rw-rw-   0        0        0    15282 2023-06-19 13:47:03.000000 pygmtools-0.3.8a0/pygmtools/numpy_modules.py
+-rw-rw-rw-   0        0        0    57860 2023-06-19 13:47:03.000000 pygmtools-0.3.8a0/pygmtools/paddle_backend.py
+-rw-rw-rw-   0        0        0    11899 2023-06-19 13:47:03.000000 pygmtools-0.3.8a0/pygmtools/paddle_modules.py
+-rw-rw-rw-   0        0        0    21475 2023-07-02 09:29:01.000000 pygmtools-0.3.8a0/pygmtools/pytorch_astar_modules.py
+-rw-rw-rw-   0        0        0    68324 2023-07-02 06:13:41.000000 pygmtools-0.3.8a0/pygmtools/pytorch_backend.py
+-rw-rw-rw-   0        0        0    12329 2023-06-19 13:47:03.000000 pygmtools-0.3.8a0/pygmtools/pytorch_modules.py
+-rw-rw-rw-   0        0        0    23046 2023-06-19 13:47:03.000000 pygmtools-0.3.8a0/pygmtools/tensorflow_backend.py
+-rw-rw-rw-   0        0        0      508 2023-06-19 13:47:03.000000 pygmtools-0.3.8a0/pygmtools/tensorflow_modules.py
+-rw-rw-rw-   0        0        0    51707 2023-06-19 13:47:03.000000 pygmtools-0.3.8a0/pygmtools/utils.py
+drwxrwxrwx   0        0        0        0 2023-07-07 09:24:03.997585 pygmtools-0.3.8a0/pygmtools.egg-info/
+-rw-rw-rw-   0        0        0    13827 2023-07-07 09:24:03.000000 pygmtools-0.3.8a0/pygmtools.egg-info/PKG-INFO
+-rw-rw-rw-   0        0        0      978 2023-07-07 09:24:03.000000 pygmtools-0.3.8a0/pygmtools.egg-info/SOURCES.txt
+-rw-rw-rw-   0        0        0        1 2023-07-07 09:24:03.000000 pygmtools-0.3.8a0/pygmtools.egg-info/dependency_links.txt
+-rw-rw-rw-   0        0        0      110 2023-07-07 09:24:03.000000 pygmtools-0.3.8a0/pygmtools.egg-info/requires.txt
+-rw-rw-rw-   0        0        0       10 2023-07-07 09:24:03.000000 pygmtools-0.3.8a0/pygmtools.egg-info/top_level.txt
+-rw-rw-rw-   0        0        0       42 2023-07-07 09:24:04.002797 pygmtools-0.3.8a0/setup.cfg
+-rw-rw-rw-   0        0        0     5818 2023-07-07 09:10:43.000000 pygmtools-0.3.8a0/setup.py
+drwxrwxrwx   0        0        0        0 2023-07-07 09:24:04.001748 pygmtools-0.3.8a0/tests/
+-rw-rw-rw-   0        0        0    16089 2023-07-02 10:34:16.000000 pygmtools-0.3.8a0/tests/test_classic_solvers.py
+-rw-rw-rw-   0        0        0     5561 2023-05-03 08:07:01.000000 pygmtools-0.3.8a0/tests/test_dataset.py
+-rw-rw-rw-   0        0        0     3800 2023-05-03 08:07:01.000000 pygmtools-0.3.8a0/tests/test_misc.py
+-rw-rw-rw-   0        0        0    13302 2023-05-03 08:07:01.000000 pygmtools-0.3.8a0/tests/test_multi_graph_solvers.py
+-rw-rw-rw-   0        0        0     9288 2023-05-03 08:07:01.000000 pygmtools-0.3.8a0/tests/test_neural_solvers.py
+-rw-rw-rw-   0        0        0     1007 2023-05-03 08:07:01.000000 pygmtools-0.3.8a0/tests/test_utils.py
```

### Comparing `pygmtools-0.3.8/PKG-INFO` & `pygmtools-0.3.8a0/PKG-INFO`

 * *Files 5% similar despite different names*

```diff
@@ -1,223 +1,223 @@
-Metadata-Version: 2.1
-Name: pygmtools
-Version: 0.3.8
-Summary: pygmtools provides graph matching solvers in Python API and supports numpy and pytorch backends. pygmtools also provides dataset API for standard graph matching benchmarks.
-Home-page: https://pygmtools.readthedocs.io/
-Author: ThinkLab at SJTU
-License: Mulan PSL v2
-Classifier: License :: OSI Approved :: Mulan Permissive Software License v2 (MulanPSL-2.0)
-Classifier: Programming Language :: Python :: 3 :: Only
-Classifier: Operating System :: OS Independent
-Classifier: Environment :: GPU :: NVIDIA CUDA
-Classifier: Environment :: Console
-Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
-Classifier: Topic :: Scientific/Engineering :: Image Recognition
-Classifier: Topic :: Scientific/Engineering :: Mathematics
-Requires-Python: >=3.7
-Description-Content-Type: text/markdown
-License-File: LICENSE
-
-
-<img src="https://pygmtools.readthedocs.io/en/latest/_static/images/pygmtools_logo.svg" alt="pygmtools: Python Graph Matching Tools" width="800"/>
-
-[![PyPi version](https://badgen.net/pypi/v/pygmtools/)](https://pypi.org/pypi/pygmtools/)
-[![PyPI pyversions](https://img.shields.io/badge/dynamic/json?color=blue&label=python&query=info.requires_python&url=https%3A%2F%2Fpypi.org%2Fpypi%2Fpygmtools%2Fjson)](https://pypi.python.org/pypi/pygmtools/)
-[![Downloads](https://pepy.tech/badge/pygmtools)](https://pepy.tech/project/pygmtools)
-[![Documentation Status](https://readthedocs.org/projects/pygmtools/badge/?version=latest)](https://pygmtools.readthedocs.io/en/latest/?badge=latest)
-[![codecov](https://codecov.io/gh/Thinklab-SJTU/pygmtools/branch/main/graph/badge.svg?token=Q68XTY0N0C)](https://codecov.io/gh/Thinklab-SJTU/pygmtools)
-[![discord channel](https://img.shields.io/discord/1028701206526304317.svg?&color=blueviolet&label=discord)](https://discord.gg/8m6n7rRz9T)
-[![QQ group](https://img.shields.io/badge/QQ%20group-696401889-blue)](https://qm.qq.com/cgi-bin/qm/qr?k=QolXYJn_M5ilDEM9e2jEjlPnJ02Ktabd&jump_from=webapi&authKey=6zG6D/Js4YF5h5zj778aO5MDKOXBwPFi8gQ4LsXJN8Hn1V8uCVGV81iT4J/FjPGT)
-[![GitHub stars](https://img.shields.io/github/stars/Thinklab-SJTU/pygmtools.svg?style=social&label=Star&maxAge=8640)](https://GitHub.com/Thinklab-SJTU/pygmtools/stargazers/) 
-
------------------------------------------
-
-``pygmtools`` (Python Graph Matching Tools) provides graph matching solvers in Python and is easily accessible via:
-
-```bash
-$ pip install pygmtools
-```
-
-Official documentation: https://pygmtools.readthedocs.io
-
-Source code: https://github.com/Thinklab-SJTU/pygmtools
-
-Graph matching is a fundamental yet challenging problem in pattern recognition, data mining, and others.
-Graph matching aims to find node-to-node correspondence among multiple graphs, by solving an NP-hard combinatorial
-optimization problem.
-
-Doing graph matching in Python used to be difficult, and this library wants to make researchers' lives easier. 
-To highlight, ``pygmtools`` has the following features:
-
-* *Support various solvers*, including traditional combinatorial solvers (including linear, quadratic, and multi-graph) 
-  and novel deep learning-based solvers;
-* *Support various backends*, including ``numpy`` which is universally accessible, and some state-of-the-art deep 
-  learning architectures with GPU support: 
-  ``pytorch``, ``paddle``, ``jittor``, ``Tensorflow``; 
-* *Deep learning friendly*, the operations are designed to best preserve the gradient during computation and batched 
-  operations support for the best performance.
-  
-## Installation
-
-You can install the stable release on PyPI:
-
-```bash
-$ pip install pygmtools
-```
-
-or get the latest version by running:
-
-```bash
-$ pip install -U https://github.com/Thinklab-SJTU/pygmtools/archive/master.zip # with --user for user install (no root)
-```
-
-Now the pygmtools is available with the ``numpy`` backend.
-
-The following packages are required, and shall be automatically installed by ``pip``:
-
-```
-Python >= 3.7
-requests >= 2.25.1
-scipy >= 1.4.1
-Pillow >= 7.2.0
-numpy >= 1.18.5
-easydict >= 1.7
-appdirs >= 1.4.4
-tqdm >= 4.64.1
-wget >= 3.2
-```
-  
-## Available Graph Matching Solvers
-This library offers user-friendly API for the following solvers:
-
-* [Two-Graph Matching Solvers](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.classic_solvers.html)
-    * Linear assignment solvers including the differentiable soft 
-      [Sinkhorn algorithm](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.linear_solvers.sinkhorn.html) [1], 
-      and the exact solver [Hungarian](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.linear_solvers.hungarian.html) [2].
-    * Soft and differentiable quadratic assignment solvers, including [spectral graph matching](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.classic_solvers.sm.html) [3] 
-      and [random-walk-based graph matching](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.classic_solvers.rrwm.html) [4].
-    * Discrete (non-differentiable) quadratic assignment solver 
-      [integer projected fixed point method](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.classic_solvers.ipfp.html) [5]. 
-* [Multi-Graph Matching Solvers](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.multi_graph_solvers.html)
-    * [Composition based Affinity Optimization (CAO) solver](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.multi_graph_solvers.cao.html) [6] 
-      by optimizing the affinity score, meanwhile gradually infusing the consistency.
-    * Multi-Graph Matching based on 
-      [Floyd shortest path algorithm](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.multi_graph_solvers.mgm_floyd.html) [7].
-    * [Graduated-assignment based multi-graph matching solver](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.multi_graph_solvers.gamgm.html) [8][9]
-      by graduated annealing of Sinkhorn‚Äôs temperature.
-* [Neural Graph Matching Solvers](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.neural_solvers.html)
-    * Intra-graph and cross-graph embedding based neural graph matching solvers 
-      [PCA-GM](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.neural_solvers.pca_gm.html) 
-      and [IPCA-GM](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.neural_solvers.ipca_gm.html) [10]
-      for matching individual graphs.
-    * [Channel independent embedding (CIE)](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.neural_solvers.cie.html) [11]
-      based neural graph matching solver for matching individual graphs.
-    * [Neural graph matching solver (NGM)](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.neural_solvers.ngm.html) [12]
-      for the general quadratic assignment formulation.
-
-## Available Backends
-This library is designed to support multiple backends with the same set of API. 
-Please follow the official instructions to install your backend.
-
-The following backends are available:
-
-* [Numpy](https://numpy.org/) (**default** backend, CPU only)
-
-<img src="https://pygmtools.readthedocs.io/en/latest/_images/numpy_logo.png" alt="numpy logo" width="200"/>
-
-* [PyTorch](https://pytorch.org/) (GPU friendly, deep learning friendly)
-
-<img src="https://pygmtools.readthedocs.io/en/latest/_images/pytorch_logo.png" alt="pytorch logo" width="200"/>
-
-* [Jittor](https://github.com/Jittor/Jittor) (GPU friendly, JIT support, deep learning friendly)
-
-<img src="https://pygmtools.readthedocs.io/en/latest/_images/jittor_logo.png" alt="jittor logo" width="200"/>
-
-* [PaddlePaddle](https://www.paddlepaddle.org.cn/en) (GPU friendly, deep learning friendly)
-
-<img src="https://pygmtools.readthedocs.io/en/latest/_images/paddle_logo.png" alt="paddle logo" width="200"/>
-
-* [Tensorflow](https://tensorflow.google.cn/) (GPU friendly, deep learning friendly)
-
-<img src="https://pygmtools.readthedocs.io/en/latest/_images/tensorflow_logo.png" alt="tensorflow logo" width="200"/>
-
-### Development status (0.3.8)
-
-|                     | Numpy | PyTorch | Jittor | PaddlePaddle | Tensorflow | MindSpore |
-| ------------------- | ----- | ------- | ------ | ------------ | ---------- | --------- |
-| Linear Solvers      | ‚úî     | ‚úî       | ‚úî      | ‚úî            | ‚úî         | ‚úî        |
-| Classic Solvers     | ‚úî     | ‚úî       | ‚úî      | ‚úî            | ‚úî         | ‚úî        |
-| Multi-Graph Solvers | ‚úî    | ‚úî       | ‚úî      | ‚úî            | üìÜ         | üìÜ        |
-| Neural Solvers      | ‚úî    | ‚úî       | ‚úî      | ‚úî           | üìÜ         | üìÜ        |
-| Examples Gallery    | ‚úî    | ‚úî       | ‚úî      | ‚úî           | üìÜ         | üìÜ        |
-
-‚úî: Supported; üìÜ: Planned for future versions (contributions welcomed!).
-
-For more details, please [read the documentation](https://pygmtools.readthedocs.io/en/latest/guide/get_started.html#install-other-backends).
-
-## Pretrained Models
-
-The library includes several neural network solvers. The pretrained models shall be automatically downloaded upon 
-needed from Google Drive. If you are experiencing issues accessing Google Drive, please download the pretrained models
-manually and put them at ``~/.cache/pygmtools`` (for Linux).
-
-Available at:
-[[google drive]](https://drive.google.com/drive/folders/1O7vkIW8QXBJsNsHUIRiSw91HJ_0FAzu_?usp=sharing)
-[[baidu drive]](https://pan.baidu.com/s/1MvzfM52NJeLWx2JXbbc6HA?pwd=x8bv)
-
-## The Deep Graph Matching Benchmark
-
-``pygmtools`` is also featured with a standard data interface of several graph matching benchmarks. Please read 
-[the corresponding documentation](https://pygmtools.readthedocs.io/en/latest/guide/benchmark.html) for details.
-
-We also maintain a repository containing non-trivial implementation of deep graph matching models, please check out
-[ThinkMatch](https://thinkmatch.readthedocs.io/) if you are interested!
-
-## Chat with the Community
-
-If you have any questions, or if you are experiencing any issues, feel free to raise an issue on GitHub. 
-
-We also offer the following chat rooms if you are more comfortable with them:
-
-* Discord (for English speakers): 
-  
-  [![discord](https://discordapp.com/api/guilds/1028701206526304317/widget.png?style=banner2)](https://discord.gg/8m6n7rRz9T)
-
-* QQ Group (for Chinese speakers)/QQÁæ§(‰∏≠ÊñáÁî®Êà∑): 696401889
-  
-  [![ThinkMatch/pygmtools‰∫§ÊµÅÁæ§](http://pub.idqqimg.com/wpa/images/group.png)](https://qm.qq.com/cgi-bin/qm/qr?k=NlPuwwvaFaHzEWD8w7jSOTzoqSLIM80V&jump_from=webapi&authKey=chI2htrWDujQed6VtVid3V1NXEoJvwz3MVwruax6x5lQIvLsC8BmpmzBJOCzhtQd)
-
-## Contributing
-Any contributions/ideas/suggestions from the community is welcomed! Before starting your contribution, please read the
-[Contributing Guide](https://github.com/Thinklab-SJTU/pygmtools/blob/main/CONTRIBUTING.md).
-
-## Developers and Maintainers
-
-``pygmtools`` is currently developed and maintained by members from [ThinkLab](http://thinklab.sjtu.edu.cn) at 
-Shanghai Jiao Tong University. 
-
-## References
-<!--MLA style references-->
-
-[1] Sinkhorn, Richard, and Paul Knopp. "Concerning nonnegative matrices and doubly stochastic matrices." Pacific Journal of Mathematics 21.2 (1967): 343-348.
-
-[2] Munkres, James. "Algorithms for the assignment and transportation problems." Journal of the society for industrial and applied mathematics 5.1 (1957): 32-38.
-
-[3] Leordeanu, Marius, and Martial Hebert. "A spectral technique for correspondence problems using pairwise constraints." International Conference on Computer Vision (2005).
-
-[4] Cho, Minsu, Jungmin Lee, and Kyoung Mu Lee. "Reweighted random walks for graph matching." European conference on Computer vision. Springer, Berlin, Heidelberg, 2010.
-
-[5] Leordeanu, Marius, Martial Hebert, and Rahul Sukthankar. "An integer projected fixed point method for graph matching and map inference." Advances in neural information processing systems 22 (2009).
-
-[6] Yan, Junchi, et al. "Multi-graph matching via affinity optimization with graduated consistency regularization." IEEE transactions on pattern analysis and machine intelligence 38.6 (2015): 1228-1242.
-
-[7] Jiang, Zetian, Tianzhe Wang, and Junchi Yan. "Unifying offline and online multi-graph matching via finding shortest paths on supergraph." IEEE transactions on pattern analysis and machine intelligence 43.10 (2020): 3648-3663.
-
-[8] Sol√©-Ribalta, Albert, and Francesc Serratosa. "Graduated assignment algorithm for multiple graph matching based on a common labeling." International Journal of Pattern Recognition and Artificial Intelligence 27.01 (2013): 1350001.
-
-[9] Wang, Runzhong, Junchi Yan, and Xiaokang Yang. "Graduated assignment for joint multi-graph matching and clustering with application to unsupervised graph matching network learning." Advances in Neural Information Processing Systems 33 (2020): 19908-19919.
-
-[10] Wang, Runzhong, Junchi Yan, and Xiaokang Yang. "Combinatorial learning of robust deep graph matching: an embedding based approach." IEEE Transactions on Pattern Analysis and Machine Intelligence (2020).
-
-[11] Yu, Tianshu, et al. "Learning deep graph matching with channel-independent embedding and hungarian attention." International conference on learning representations. 2019.
-
-[12] Wang, Runzhong, Junchi Yan, and Xiaokang Yang. "Neural graph matching network: Learning lawler‚Äôs quadratic assignment problem with extension to hypergraph and multiple-graph matching." IEEE Transactions on Pattern Analysis and Machine Intelligence (2021).
+Metadata-Version: 2.1
+Name: pygmtools
+Version: 0.3.8a0
+Summary: pygmtools provides graph matching solvers in Python API and supports numpy and pytorch backends. pygmtools also provides dataset API for standard graph matching benchmarks.
+Home-page: https://pygmtools.readthedocs.io/
+Author: ThinkLab at SJTU
+License: Mulan PSL v2
+Classifier: License :: OSI Approved :: Mulan Permissive Software License v2 (MulanPSL-2.0)
+Classifier: Programming Language :: Python :: 3 :: Only
+Classifier: Operating System :: OS Independent
+Classifier: Environment :: GPU :: NVIDIA CUDA
+Classifier: Environment :: Console
+Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
+Classifier: Topic :: Scientific/Engineering :: Image Recognition
+Classifier: Topic :: Scientific/Engineering :: Mathematics
+Requires-Python: >=3.7
+Description-Content-Type: text/markdown
+License-File: LICENSE
+
+
+<img src="https://pygmtools.readthedocs.io/en/latest/_static/images/pygmtools_logo.svg" alt="pygmtools: Python Graph Matching Tools" width="800"/>
+
+[![PyPi version](https://badgen.net/pypi/v/pygmtools/)](https://pypi.org/pypi/pygmtools/)
+[![PyPI pyversions](https://img.shields.io/badge/dynamic/json?color=blue&label=python&query=info.requires_python&url=https%3A%2F%2Fpypi.org%2Fpypi%2Fpygmtools%2Fjson)](https://pypi.python.org/pypi/pygmtools/)
+[![Downloads](https://pepy.tech/badge/pygmtools)](https://pepy.tech/project/pygmtools)
+[![Documentation Status](https://readthedocs.org/projects/pygmtools/badge/?version=latest)](https://pygmtools.readthedocs.io/en/latest/?badge=latest)
+[![codecov](https://codecov.io/gh/Thinklab-SJTU/pygmtools/branch/main/graph/badge.svg?token=Q68XTY0N0C)](https://codecov.io/gh/Thinklab-SJTU/pygmtools)
+[![discord channel](https://img.shields.io/discord/1028701206526304317.svg?&color=blueviolet&label=discord)](https://discord.gg/8m6n7rRz9T)
+[![QQ group](https://img.shields.io/badge/QQ%20group-696401889-blue)](https://qm.qq.com/cgi-bin/qm/qr?k=QolXYJn_M5ilDEM9e2jEjlPnJ02Ktabd&jump_from=webapi&authKey=6zG6D/Js4YF5h5zj778aO5MDKOXBwPFi8gQ4LsXJN8Hn1V8uCVGV81iT4J/FjPGT)
+[![GitHub stars](https://img.shields.io/github/stars/Thinklab-SJTU/pygmtools.svg?style=social&label=Star&maxAge=8640)](https://GitHub.com/Thinklab-SJTU/pygmtools/stargazers/) 
+
+-----------------------------------------
+
+``pygmtools`` (Python Graph Matching Tools) provides graph matching solvers in Python and is easily accessible via:
+
+```bash
+$ pip install pygmtools
+```
+
+Official documentation: https://pygmtools.readthedocs.io
+
+Source code: https://github.com/Thinklab-SJTU/pygmtools
+
+Graph matching is a fundamental yet challenging problem in pattern recognition, data mining, and others.
+Graph matching aims to find node-to-node correspondence among multiple graphs, by solving an NP-hard combinatorial
+optimization problem.
+
+Doing graph matching in Python used to be difficult, and this library wants to make researchers' lives easier. 
+To highlight, ``pygmtools`` has the following features:
+
+* *Support various solvers*, including traditional combinatorial solvers (including linear, quadratic, and multi-graph) 
+  and novel deep learning-based solvers;
+* *Support various backends*, including ``numpy`` which is universally accessible, and some state-of-the-art deep 
+  learning architectures with GPU support: 
+  ``pytorch``, ``paddle``, ``jittor``, ``Tensorflow``; 
+* *Deep learning friendly*, the operations are designed to best preserve the gradient during computation and batched 
+  operations support for the best performance.
+  
+## Installation
+
+You can install the stable release on PyPI:
+
+```bash
+$ pip install pygmtools
+```
+
+or get the latest version by running:
+
+```bash
+$ pip install -U https://github.com/Thinklab-SJTU/pygmtools/archive/master.zip # with --user for user install (no root)
+```
+
+Now the pygmtools is available with the ``numpy`` backend.
+
+The following packages are required, and shall be automatically installed by ``pip``:
+
+```
+Python >= 3.7
+requests >= 2.25.1
+scipy >= 1.4.1
+Pillow >= 7.2.0
+numpy >= 1.18.5
+easydict >= 1.7
+appdirs >= 1.4.4
+tqdm >= 4.64.1
+wget >= 3.2
+```
+  
+## Available Graph Matching Solvers
+This library offers user-friendly API for the following solvers:
+
+* [Two-Graph Matching Solvers](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.classic_solvers.html)
+    * Linear assignment solvers including the differentiable soft 
+      [Sinkhorn algorithm](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.linear_solvers.sinkhorn.html) [1], 
+      and the exact solver [Hungarian](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.linear_solvers.hungarian.html) [2].
+    * Soft and differentiable quadratic assignment solvers, including [spectral graph matching](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.classic_solvers.sm.html) [3] 
+      and [random-walk-based graph matching](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.classic_solvers.rrwm.html) [4].
+    * Discrete (non-differentiable) quadratic assignment solver 
+      [integer projected fixed point method](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.classic_solvers.ipfp.html) [5]. 
+* [Multi-Graph Matching Solvers](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.multi_graph_solvers.html)
+    * [Composition based Affinity Optimization (CAO) solver](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.multi_graph_solvers.cao.html) [6] 
+      by optimizing the affinity score, meanwhile gradually infusing the consistency.
+    * Multi-Graph Matching based on 
+      [Floyd shortest path algorithm](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.multi_graph_solvers.mgm_floyd.html) [7].
+    * [Graduated-assignment based multi-graph matching solver](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.multi_graph_solvers.gamgm.html) [8][9]
+      by graduated annealing of Sinkhorn‚Äôs temperature.
+* [Neural Graph Matching Solvers](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.neural_solvers.html)
+    * Intra-graph and cross-graph embedding based neural graph matching solvers 
+      [PCA-GM](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.neural_solvers.pca_gm.html) 
+      and [IPCA-GM](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.neural_solvers.ipca_gm.html) [10]
+      for matching individual graphs.
+    * [Channel independent embedding (CIE)](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.neural_solvers.cie.html) [11]
+      based neural graph matching solver for matching individual graphs.
+    * [Neural graph matching solver (NGM)](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.neural_solvers.ngm.html) [12]
+      for the general quadratic assignment formulation.
+
+## Available Backends
+This library is designed to support multiple backends with the same set of API. 
+Please follow the official instructions to install your backend.
+
+The following backends are available:
+
+* [Numpy](https://numpy.org/) (**default** backend, CPU only)
+
+<img src="https://pygmtools.readthedocs.io/en/latest/_images/numpy_logo.png" alt="numpy logo" width="200"/>
+
+* [PyTorch](https://pytorch.org/) (GPU friendly, deep learning friendly)
+
+<img src="https://pygmtools.readthedocs.io/en/latest/_images/pytorch_logo.png" alt="pytorch logo" width="200"/>
+
+* [Jittor](https://github.com/Jittor/Jittor) (GPU friendly, JIT support, deep learning friendly)
+
+<img src="https://pygmtools.readthedocs.io/en/latest/_images/jittor_logo.png" alt="jittor logo" width="200"/>
+
+* [PaddlePaddle](https://www.paddlepaddle.org.cn/en) (GPU friendly, deep learning friendly)
+
+<img src="https://pygmtools.readthedocs.io/en/latest/_images/paddle_logo.png" alt="paddle logo" width="200"/>
+
+* [Tensorflow](https://tensorflow.google.cn/) (GPU friendly, deep learning friendly)
+
+<img src="https://pygmtools.readthedocs.io/en/latest/_images/tensorflow_logo.png" alt="tensorflow logo" width="200"/>
+
+### Development status (0.3.8)
+
+|                     | Numpy | PyTorch | Jittor | PaddlePaddle | Tensorflow | MindSpore |
+| ------------------- | ----- | ------- | ------ | ------------ | ---------- | --------- |
+| Linear Solvers      | ‚úî     | ‚úî       | ‚úî      | ‚úî            | ‚úî         | ‚úî        |
+| Classic Solvers     | ‚úî     | ‚úî       | ‚úî      | ‚úî            | ‚úî         | ‚úî        |
+| Multi-Graph Solvers | ‚úî    | ‚úî       | ‚úî      | ‚úî            | üìÜ         | üìÜ        |
+| Neural Solvers      | ‚úî    | ‚úî       | ‚úî      | ‚úî           | üìÜ         | üìÜ        |
+| Examples Gallery    | ‚úî    | ‚úî       | ‚úî      | ‚úî           | üìÜ         | üìÜ        |
+
+‚úî: Supported; üìÜ: Planned for future versions (contributions welcomed!).
+
+For more details, please [read the documentation](https://pygmtools.readthedocs.io/en/latest/guide/get_started.html#install-other-backends).
+
+## Pretrained Models
+
+The library includes several neural network solvers. The pretrained models shall be automatically downloaded upon 
+needed from Google Drive. If you are experiencing issues accessing Google Drive, please download the pretrained models
+manually and put them at ``~/.cache/pygmtools`` (for Linux).
+
+Available at:
+[[google drive]](https://drive.google.com/drive/folders/1O7vkIW8QXBJsNsHUIRiSw91HJ_0FAzu_?usp=sharing)
+[[baidu drive]](https://pan.baidu.com/s/1MvzfM52NJeLWx2JXbbc6HA?pwd=x8bv)
+
+## The Deep Graph Matching Benchmark
+
+``pygmtools`` is also featured with a standard data interface of several graph matching benchmarks. Please read 
+[the corresponding documentation](https://pygmtools.readthedocs.io/en/latest/guide/benchmark.html) for details.
+
+We also maintain a repository containing non-trivial implementation of deep graph matching models, please check out
+[ThinkMatch](https://thinkmatch.readthedocs.io/) if you are interested!
+
+## Chat with the Community
+
+If you have any questions, or if you are experiencing any issues, feel free to raise an issue on GitHub. 
+
+We also offer the following chat rooms if you are more comfortable with them:
+
+* Discord (for English speakers): 
+  
+  [![discord](https://discordapp.com/api/guilds/1028701206526304317/widget.png?style=banner2)](https://discord.gg/8m6n7rRz9T)
+
+* QQ Group (for Chinese speakers)/QQÁæ§(‰∏≠ÊñáÁî®Êà∑): 696401889
+  
+  [![ThinkMatch/pygmtools‰∫§ÊµÅÁæ§](http://pub.idqqimg.com/wpa/images/group.png)](https://qm.qq.com/cgi-bin/qm/qr?k=NlPuwwvaFaHzEWD8w7jSOTzoqSLIM80V&jump_from=webapi&authKey=chI2htrWDujQed6VtVid3V1NXEoJvwz3MVwruax6x5lQIvLsC8BmpmzBJOCzhtQd)
+
+## Contributing
+Any contributions/ideas/suggestions from the community is welcomed! Before starting your contribution, please read the
+[Contributing Guide](https://github.com/Thinklab-SJTU/pygmtools/blob/main/CONTRIBUTING.md).
+
+## Developers and Maintainers
+
+``pygmtools`` is currently developed and maintained by members from [ThinkLab](http://thinklab.sjtu.edu.cn) at 
+Shanghai Jiao Tong University. 
+
+## References
+<!--MLA style references-->
+
+[1] Sinkhorn, Richard, and Paul Knopp. "Concerning nonnegative matrices and doubly stochastic matrices." Pacific Journal of Mathematics 21.2 (1967): 343-348.
+
+[2] Munkres, James. "Algorithms for the assignment and transportation problems." Journal of the society for industrial and applied mathematics 5.1 (1957): 32-38.
+
+[3] Leordeanu, Marius, and Martial Hebert. "A spectral technique for correspondence problems using pairwise constraints." International Conference on Computer Vision (2005).
+
+[4] Cho, Minsu, Jungmin Lee, and Kyoung Mu Lee. "Reweighted random walks for graph matching." European conference on Computer vision. Springer, Berlin, Heidelberg, 2010.
+
+[5] Leordeanu, Marius, Martial Hebert, and Rahul Sukthankar. "An integer projected fixed point method for graph matching and map inference." Advances in neural information processing systems 22 (2009).
+
+[6] Yan, Junchi, et al. "Multi-graph matching via affinity optimization with graduated consistency regularization." IEEE transactions on pattern analysis and machine intelligence 38.6 (2015): 1228-1242.
+
+[7] Jiang, Zetian, Tianzhe Wang, and Junchi Yan. "Unifying offline and online multi-graph matching via finding shortest paths on supergraph." IEEE transactions on pattern analysis and machine intelligence 43.10 (2020): 3648-3663.
+
+[8] Sol√©-Ribalta, Albert, and Francesc Serratosa. "Graduated assignment algorithm for multiple graph matching based on a common labeling." International Journal of Pattern Recognition and Artificial Intelligence 27.01 (2013): 1350001.
+
+[9] Wang, Runzhong, Junchi Yan, and Xiaokang Yang. "Graduated assignment for joint multi-graph matching and clustering with application to unsupervised graph matching network learning." Advances in Neural Information Processing Systems 33 (2020): 19908-19919.
+
+[10] Wang, Runzhong, Junchi Yan, and Xiaokang Yang. "Combinatorial learning of robust deep graph matching: an embedding based approach." IEEE Transactions on Pattern Analysis and Machine Intelligence (2020).
+
+[11] Yu, Tianshu, et al. "Learning deep graph matching with channel-independent embedding and hungarian attention." International conference on learning representations. 2019.
+
+[12] Wang, Runzhong, Junchi Yan, and Xiaokang Yang. "Neural graph matching network: Learning lawler‚Äôs quadratic assignment problem with extension to hypergraph and multiple-graph matching." IEEE Transactions on Pattern Analysis and Machine Intelligence (2021).
```

### Comparing `pygmtools-0.3.8/README.md` & `pygmtools-0.3.8a0/README.md`

 * *Files identical despite different names*

### Comparing `pygmtools-0.3.8/pygmtools/benchmark.py` & `pygmtools-0.3.8a0/pygmtools/benchmark.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,585 +1,585 @@
-"""
-The Benchmark module with a unified data interface to evaluate graph matching methods.
-
-If you are interested in the performance and the deep learning framework, please refer to our `ThinkMatch project <https://github.com/Thinklab-SJTU/ThinkMatch>`_.
-"""
-
-# Copyright (c) 2022 Thinklab@SJTU
-# pygmtools is licensed under Mulan PSL v2.
-# You can use this software according to the terms and conditions of the Mulan PSL v2.
-# You may obtain a copy of Mulan PSL v2 at:
-# http://license.coscl.org.cn/MulanPSL2
-# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND,
-# EITHER EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT,
-# MERCHANTABILITY OR FIT FOR A PARTICULAR PURPOSE.
-# See the Mulan PSL v2 for more details.
-
-import tempfile
-import shutil
-import itertools
-from scipy.sparse import coo_matrix
-from pygmtools.dataset import *
-
-
-class Benchmark:
-    r"""
-    The `Benchmark` module provides a unified data interface and an evaluating platform for different datasets.
-
-    :param name: str, dataset name, currently support ``'PascalVOC'``, ``'WillowObject'``, ``'IMC_PT_SparseGM'``, ``'CUB2011'``, ``'SPair71k'``
-    :param sets: str, problem set, ``'train'`` for training set and ``'test'`` for test set
-    :param obj_resize: tuple, (default: ``(256, 256)``) resized object size
-    :param problem: str, (default: ``'2GM'``) problem type, ``'2GM'`` for 2-graph matching and ``'MGM'`` for multi-graph matching
-    :param filter: str, (default: ``'intersection'``) filter of nodes, ``'intersection'`` refers to retaining only common nodes;
-       ``'inclusion'`` is only for 2GM and refers to filtering only one graph to make its nodes a subset of the other graph,
-       and ``'unfiltered'`` refers to retaining all nodes in all graphs
-    :param args: keyword settings for specific dataset
-
-    .. note::
-            Ground truth cache is saved only when the parameter ``sets`` is ``'test'``, so
-            the functions ``eval()`` and ``eval_cls()`` are only for ``'test'`` set.
-
-    """
-
-    def __init__(self, name, sets, obj_resize=(256, 256), problem='2GM', filter='intersection', **args):
-        assert name == 'PascalVOC' or name == 'SPair71k' or name == 'WillowObject' or name == 'IMC_PT_SparseGM' or name == 'CUB2011', 'No match found for dataset {}'.format(
-            name)
-        assert problem == '2GM' or problem == 'MGM' or problem == 'MGM3', 'No match found for problem {}'.format(
-            problem)
-        assert filter == 'intersection' or filter == 'inclusion' or filter == 'unfiltered', 'No match found for filter {}'.format(
-            filter)
-        assert not ((
-                            problem == 'MGM' or problem == 'MGM3') and filter == 'inclusion'), 'The filter inclusion only matches 2GM'
-
-        self.name = name
-        self.problem = problem
-        self.filter = filter
-        self.sets = sets
-        self.obj_resize = obj_resize
-
-        data_set = eval(self.name)(self.sets, self.obj_resize, **args)
-        suffix = data_set.suffix
-        self.data_path = os.path.join(data_set.dataset_dir, 'data-' + str(self.obj_resize) + '-' + suffix + '.json')
-        self.data_list_path = os.path.join(data_set.dataset_dir, sets + '.json')
-        self.classes = data_set.classes
-
-        with open(self.data_path) as f:
-            self.data_dict = json.load(f)
-
-        if self.sets == 'test':
-            tmpfile = tempfile.gettempdir()
-            pid_num = os.getpid()
-            cache_dir = str(pid_num) + '_gt_cache'
-            self.gt_cache_path = os.path.join(tmpfile, cache_dir)
-
-            if not os.path.exists(self.gt_cache_path):
-                os.mkdir(self.gt_cache_path)
-                print('gt perm mat cache built')
-
-    def get_data(self, ids, test=False, shuffle=True):
-        r"""
-        Fetch a data pair or pairs of data by image ID for training or test.
-
-        :param ids: list of image ID, usually in ``train.json`` or ``test.json``
-        :param test: bool, whether the fetched data is used for test; if true, this function will not return ground truth
-        :param shuffle: bool, whether to shuffle the order of keypoints
-        :return:
-                    **data_list**: list of data, like ``[{'img': np.array, 'kpts': coordinates of kpts}, ...]``
-
-                    **perm_mat_dict**: ground truth, like ``{(0,1):scipy.sparse, (0,2):scipy.sparse, ...}``, ``(0,1)`` refers to data pair ``(ids[0],ids[1])``
-
-                    **ids**: list of image ID
-        """
-        assert (self.problem == '2GM' and len(ids) == 2) or ((self.problem == 'MGM' or self.problem == 'MGM3') and len(
-            ids) > 2), '{} problem cannot get {} data'.format(self.problem, len(ids))
-
-        ids.sort()
-        data_list = []
-        for keys in ids:
-            obj_dict = dict()
-            boundbox = self.data_dict[keys]['bounds']
-            img_file = self.data_dict[keys]['path']
-            with Image.open(str(img_file)) as img:
-                obj = img.resize(self.obj_resize, resample=Image.BICUBIC,
-                                 box=(boundbox[0], boundbox[1], boundbox[2], boundbox[3]))
-                if self.name == 'CUB2011':
-                    if not obj.mode == 'RGB':
-                        obj = obj.convert('RGB')
-            obj_dict['img'] = np.array(obj)
-            obj_dict['kpts'] = self.data_dict[keys]['kpts']
-            obj_dict['cls'] = self.data_dict[keys]['cls']
-            obj_dict['univ_size'] = self.data_dict[keys]['univ_size']
-            if shuffle:
-                random.shuffle(obj_dict['kpts'])
-            data_list.append(obj_dict)
-
-        perm_mat_dict = dict()
-        id_combination = list(itertools.combinations(list(range(len(ids))), 2))
-        for id_tuple in id_combination:
-            perm_mat = np.zeros([len(data_list[_]['kpts']) for _ in id_tuple], dtype=np.float32)
-            row_list = []
-            col_list = []
-
-            for i, keypoint in enumerate(data_list[id_tuple[0]]['kpts']):
-                for j, _keypoint in enumerate(data_list[id_tuple[1]]['kpts']):
-                    if keypoint['labels'] == _keypoint['labels']:
-                        if keypoint['labels'] != 'outlier':
-                            perm_mat[i, j] = 1
-            for i, keypoint in enumerate(data_list[id_tuple[0]]['kpts']):
-                for j, _keypoint in enumerate(data_list[id_tuple[1]]['kpts']):
-                    if keypoint['labels'] == _keypoint['labels']:
-                        row_list.append(i)
-                        break
-            for i, keypoint in enumerate(data_list[id_tuple[1]]['kpts']):
-                for j, _keypoint in enumerate(data_list[id_tuple[0]]['kpts']):
-                    if keypoint['labels'] == _keypoint['labels']:
-                        col_list.append(i)
-                        break
-            row_list.sort()
-            col_list.sort()
-            if self.filter == 'intersection':
-                perm_mat = perm_mat[row_list, :]
-                perm_mat = perm_mat[:, col_list]
-                data_list[id_tuple[0]]['kpts'] = [data_list[id_tuple[0]]['kpts'][i] for i in row_list]
-                data_list[id_tuple[1]]['kpts'] = [data_list[id_tuple[1]]['kpts'][i] for i in col_list]
-            elif self.filter == 'inclusion':
-                perm_mat = perm_mat[row_list, :]
-                data_list[id_tuple[0]]['kpts'] = [data_list[id_tuple[0]]['kpts'][i] for i in row_list]
-            if not (len(ids) > 2 and self.filter == 'intersection'):
-                sparse_perm_mat = coo_matrix(perm_mat)
-                perm_mat_dict[id_tuple] = sparse_perm_mat
-
-        if len(ids) > 2 and self.filter == 'intersection':
-            for p in range(len(ids) - 1):
-                perm_mat_list = [np.zeros([len(data_list[p]['kpts']), len(x['kpts'])], dtype=np.float32) for x in
-                                 data_list[p + 1: len(ids)]]
-                row_list = []
-                col_lists = []
-                for i in range(len(ids) - p - 1):
-                    col_lists.append([])
-
-                for i, keypoint in enumerate(data_list[p]['kpts']):
-                    kpt_idx = []
-                    for anno_dict in data_list[p + 1: len(ids)]:
-                        kpt_name_list = [x['labels'] for x in anno_dict['kpts']]
-                        if keypoint['labels'] in kpt_name_list:
-                            kpt_idx.append(kpt_name_list.index(keypoint['labels']))
-                        else:
-                            kpt_idx.append(-1)
-                    row_list.append(i)
-                    for k in range(len(ids) - p - 1):
-                        j = kpt_idx[k]
-                        if j != -1:
-                            col_lists[k].append(j)
-                            if keypoint['labels'] != 'outlier':
-                                perm_mat_list[k][i, j] = 1
-
-                row_list.sort()
-                for col_list in col_lists:
-                    col_list.sort()
-
-                for k in range(len(ids) - p - 1):
-                    perm_mat_list[k] = perm_mat_list[k][row_list, :]
-                    perm_mat_list[k] = perm_mat_list[k][:, col_lists[k]]
-                    id_tuple = (p, k + p + 1)
-                    perm_mat_dict[id_tuple] = coo_matrix(perm_mat_list[k])
-
-        if self.sets == 'test':
-            for pair in id_combination:
-                id_pair = (ids[pair[0]], ids[pair[1]])
-                gt_path = os.path.join(self.gt_cache_path, str(id_pair) + '.npy')
-                if not os.path.exists(gt_path):
-                    np.save(gt_path, perm_mat_dict[pair])
-
-        if not test:
-            return data_list, perm_mat_dict, ids
-        else:
-            return data_list, ids
-
-    def rand_get_data(self, cls=None, num=2, test=False, shuffle=True):
-        r"""
-        Randomly fetch data for training or test. Implemented by calling ``get_data`` function.
-
-        :param cls: int or str, class of expected data. None for random class
-        :param num: int, number of images; for example, 2 for 2GM
-        :param test: bool, whether the fetched data is used for test; if true, this function will not return ground truth
-        :param shuffle: bool, whether to shuffle the order of keypoints
-        :return:
-                    **data_list**: list of data, like ``[{'img': np.array, 'kpts': coordinates of kpts}, ...]``
-
-                    **perm_mat_dict**: ground truth, like ``{(0,1):scipy.sparse, (0,2):scipy.sparse, ...}``, ``(0,1)`` refers to data pair ``(ids[0],ids[1])``
-
-                    **ids**: list of image ID
-        """
-        if cls == None:
-            cls = random.randrange(0, len(self.classes))
-            clss = self.classes[cls]
-        elif type(cls) == str:
-            clss = cls
-
-        with open(self.data_list_path) as f1:
-            data_id = json.load(f1)
-
-        data_list = []
-        ids = []
-        if self.name != 'SPair71k':
-            for id in data_id:
-                if self.data_dict[id]['cls'] == clss:
-                    data_list.append(id)
-
-            for objID in random.sample(data_list, num):
-                ids.append(objID)
-        else:
-            for id in data_id:
-                if self.data_dict[id[0]]['cls'] == clss:
-                    data_list.append(id)
-            ids = random.sample(data_list, 1)[0]
-
-        return self.get_data(ids, test, shuffle)
-
-    def get_id_combination(self, cls=None, num=2):
-        r"""
-        Get the combination of images and length of combinations in specified class.
-
-        :param cls: int or str, class of expected data. None for all classes
-        :param num: int, number of images in each image ID list; for example, 2 for 2GM
-        :return:
-                **id_combination_list**: list of combinations of image ids
-
-                **length**: length of combinations
-        """
-        if cls == None:
-            clss = None
-        elif type(cls) == str:
-            clss = cls
-        else:
-            raise ValueError(f'Expect cls argument to be NoneType or str, got {type(cls)}!')
-
-        with open(self.data_list_path) as f1:
-            data_id = json.load(f1)
-
-        length = 0
-        id_combination_list = []
-        if clss != None:
-            data_list = []
-            if self.name != 'SPair71k':
-                for id in data_id:
-                    if self.data_dict[id]['cls'] == clss:
-                        data_list.append(id)
-                id_combination = list(itertools.combinations(data_list, num))
-                length += len(id_combination)
-                id_combination_list.append(id_combination)
-            else:
-                for id_pair in data_id:
-                    if self.data_dict[id_pair[0]]['cls'] == clss:
-                        data_list.append(id_pair)
-                length += len(data_list)
-                id_combination_list.append(data_list)
-        else:
-            for clss in self.classes:
-                data_list = []
-                if self.name != 'SPair71k':
-                    for id in data_id:
-                        if self.data_dict[id]['cls'] == clss:
-                            data_list.append(id)
-                    id_combination = list(itertools.combinations(data_list, num))
-                    length += len(id_combination)
-                    id_combination_list.append(id_combination)
-                else:
-                    for id_pair in data_id:
-                        if self.data_dict[id_pair[0]]['cls'] == clss:
-                            data_list.append(id_pair)
-                    length += len(data_list)
-                    id_combination_list.append(data_list)
-
-        return id_combination_list, length
-
-    def compute_length(self, cls=None, num=2):
-        r"""
-        Compute the length of image combinations in specified class.
-
-        :param cls: int or str, class of expected data. None for all classes
-        :param num: int, number of images in each image ID list; for example, 2 for two-graph matching problem
-        :return: length of combinations
-        """
-        if cls == None:
-            clss = None
-        elif type(cls) == str:
-            clss = cls
-
-        with open(self.data_list_path) as f1:
-            data_id = json.load(f1)
-
-        length = 0
-
-        if clss != None:
-            if self.name != 'SPair71k':
-                data_list = []
-                for id in data_id:
-                    if self.data_dict[id]['cls'] == clss:
-                        data_list.append(id)
-                id_combination = list(itertools.combinations(data_list, num))
-                length += len(id_combination)
-            else:
-                for id_pair in data_id:
-                    if self.data_dict[id_pair[0]]['cls'] == clss:
-                        length += 1
-
-        else:
-            for clss in self.classes:
-                if self.name != 'SPair71k':
-                    data_list = []
-                    for id in data_id:
-                        if self.data_dict[id]['cls'] == clss:
-                            data_list.append(id)
-                    id_combination = list(itertools.combinations(data_list, num))
-                    length += len(id_combination)
-                else:
-                    for id_pair in data_id:
-                        if self.data_dict[id_pair[0]]['cls'] == clss:
-                            length += 1
-        return length
-
-    def compute_img_num(self, classes):
-        r"""
-        Compute number of images in specified classes.
-
-        :param classes: list of dataset classes
-        :return: list of numbers of images in each class
-        """
-        with open(self.data_list_path) as f1:
-            data_id = json.load(f1)
-        num_list = []
-        for clss in classes:
-            cls_img_num = 0
-            if self.name != 'SPair71k':
-                for id in data_id:
-                    if self.data_dict[id]['cls'] == clss:
-                        cls_img_num += 1
-                num_list.append(cls_img_num)
-            else:
-                img_cache = []
-                for id_pair in data_id:
-                    if self.data_dict[id_pair[0]]['cls'] == clss:
-                        if id_pair[0] not in img_cache:
-                            img_cache.append(id_pair[0])
-                            cls_img_num += 1
-                        if id_pair[1] not in img_cache:
-                            img_cache.append(id_pair[1])
-                            cls_img_num += 1
-                num_list.append(cls_img_num)
-
-        return num_list
-
-    def eval(self, prediction, classes, verbose=False, rm_gt_cache=True):
-        r"""
-        Evaluate test results and compute matching accuracy and coverage.
-
-        :param prediction: list, prediction result, like ``[{'ids': (id1, id2), 'cls': cls, 'permmat': np.array or scipy.sparse}, ...]``
-        :param classes: list of evaluated classes
-        :param verbose: bool, whether to print the result
-        :param rm_gt_cache: bool, whether to remove ground truth cache
-        :return: evaluation result in each class and their averages, including p, r, f1 and their standard deviation and coverage
-
-        .. note::
-            If there are duplicate data pair in ``prediction``, this function will only evaluate the first pair and
-            expect that this pair is also the first fetched pair. Therefore, it is recommended that ``prediction`` is
-            built in an ordered manner, and not shuffled.
-
-        .. note::
-            Ground truth cache is saved when data pairs are fetched, and should be removed after evaluation. Make sure
-            all data pairs are evaluated at once, i.e., ``prediction`` should contain all fetched data pairs.
-        """
-
-        with open(self.data_list_path) as f1:
-            data_id = json.load(f1)
-
-        cls_dict = dict()
-        pred_cls_dict = dict()
-        result = dict()
-        id_cache = []
-        cls_precision = dict()
-        cls_recall = dict()
-        cls_f1 = dict()
-
-        for cls in classes:
-            cls_dict[cls] = 0
-            pred_cls_dict[cls] = 0
-            result[cls] = dict()
-            cls_precision[cls] = []
-            cls_recall[cls] = []
-            cls_f1[cls] = []
-
-        if self.name != 'SPair71k':
-            for key, obj in self.data_dict.items():
-                if (key in data_id) and (obj['cls'] in classes):
-                    cls_dict[obj['cls']] += 1
-        else:
-            for cls in classes:
-                cls_dict[cls] = self.compute_img_num([cls])[0]
-
-        for pair_dict in prediction:
-            ids = (pair_dict['ids'][0], pair_dict['ids'][1])
-            if ids not in id_cache:
-                id_cache.append(ids)
-                pred_cls_dict[pair_dict['cls']] += 1
-                perm_mat = pair_dict['perm_mat']
-                gt_path = os.path.join(self.gt_cache_path, str(ids) + '.npy')
-                gt = np.load(gt_path, allow_pickle=True).item()
-                gt_array = gt.toarray()
-                assert type(perm_mat) == type(gt_array)
-
-                assert gt_array.sum() != 0, 'ground truth permutation matrix should not be all zeros'
-                if perm_mat.sum() == 0:
-                    precision = 0
-                    recall = 0
-                else:
-                    precision = (perm_mat * gt_array).sum() / perm_mat.sum()
-                    recall = (perm_mat * gt_array).sum() / gt_array.sum()
-                if precision == 0 or recall == 0:
-                    f1_score = 0
-                else:
-                    f1_score = (2 * precision * recall) / (precision + recall)
-
-                cls_precision[pair_dict['cls']].append(precision)
-                cls_recall[pair_dict['cls']].append(recall)
-                cls_f1[pair_dict['cls']].append(f1_score)
-
-        p_sum = 0
-        r_sum = 0
-        f1_sum = 0
-        p_std_sum = 0
-        r_std_sum = 0
-        f1_std_sum = 0
-
-        for cls in classes:
-            result[cls]['precision'] = np.mean(cls_precision[cls])
-            result[cls]['recall'] = np.mean(cls_recall[cls])
-            result[cls]['f1'] = np.mean(cls_f1[cls])
-            result[cls]['precision_std'] = np.std(cls_precision[cls])
-            result[cls]['recall_std'] = np.std(cls_recall[cls])
-            result[cls]['f1_std'] = np.std(cls_f1[cls])
-            result[cls]['coverage'] = 2 * pred_cls_dict[cls] / (cls_dict[cls] * (cls_dict[cls] - 1))
-            p_sum += result[cls]['precision']
-            r_sum += result[cls]['recall']
-            f1_sum += result[cls]['f1']
-            p_std_sum += result[cls]['precision_std']
-            r_std_sum += result[cls]['recall_std']
-            f1_std_sum += result[cls]['f1_std']
-
-        result['mean'] = dict()
-        result['mean']['precision'] = p_sum / len(classes)
-        result['mean']['recall'] = r_sum / len(classes)
-        result['mean']['f1'] = f1_sum / len(classes)
-        result['mean']['precision_std'] = p_std_sum / len(classes)
-        result['mean']['recall_std'] = r_std_sum / len(classes)
-        result['mean']['f1_std'] = f1_std_sum / len(classes)
-
-        if verbose:
-            print('Matching accuracy')
-            for cls in classes:
-                print('{}: {}'.format(cls, 'p = {:.4f}¬±{:.4f}, r = {:.4f}¬±{:.4f}, f1 = {:.4f}¬±{:.4f}, cvg = {:.4f}' \
-                                      .format(result[cls]['precision'], result[cls]['precision_std'],
-                                              result[cls]['recall'], result[cls]['recall_std'], result[cls]['f1'],
-                                              result[cls]['f1_std'], result[cls]['coverage']
-                                              )))
-            print('average accuracy: {}'.format('p = {:.4f}¬±{:.4f}, r = {:.4f}¬±{:.4f}, f1 = {:.4f}¬±{:.4f}' \
-                                                .format(result['mean']['precision'], result['mean']['precision_std'],
-                                                        result['mean']['recall'], result['mean']['recall_std'],
-                                                        result['mean']['f1'], result['mean']['f1_std']
-                                                        )))
-        if rm_gt_cache:
-            self.rm_gt_cache(last_epoch=False)
-        return result
-
-    def eval_cls(self, prediction, cls, verbose=False):
-        r"""
-        Evaluate test results and compute matching accuracy and coverage on one specified class.
-
-        :param prediction: list, prediction result on one class, like ``[{'ids': (id1, id2), 'cls': cls, 'permmat': np.array or scipy.sparse}, ...]``
-        :param cls: str, evaluated class
-        :param verbose: bool, whether to print the result
-        :return: evaluation result on the specified class, including p, r, f1 and their standard deviation and coverage
-
-        .. note::
-            If there are duplicate data pair in ``prediction``, this function will only evaluate the first pair and
-            expect that this pair is also the first fetched pair. Therefore, it is recommended that ``prediction`` is
-            built in an ordered manner, and not shuffled. Same as the function ``eval``.
-
-        .. note::
-            This function will not automatically remove ground truth cache. However, you can still mannually call the
-            class function ``rm_gt_cache`` to remove groud truth cache after evaluation.
-        """
-
-        with open(self.data_list_path) as f1:
-            data_id = json.load(f1)
-
-        result = dict()
-        id_cache = []
-        cls_precision = []
-        cls_recall = []
-        cls_f1 = []
-
-        cls_dict = 0
-        pred_cls_dict = 0
-
-        if self.name != 'SPair71k':
-            for key, obj in self.data_dict.items():
-                if (key in data_id) and (obj['cls'] == cls):
-                    cls_dict += 1
-        else:
-            cls_dict = self.compute_img_num([cls])[0]
-
-        for pair_dict in prediction:
-            ids = (pair_dict['ids'][0], pair_dict['ids'][1])
-            if ids not in id_cache:
-                id_cache.append(ids)
-                pred_cls_dict += 1
-                perm_mat = pair_dict['perm_mat']
-                gt_path = os.path.join(self.gt_cache_path, str(ids) + '.npy')
-                gt = np.load(gt_path, allow_pickle=True).item()
-                gt_array = gt.toarray()
-                assert type(perm_mat) == type(gt_array)
-
-                assert gt_array.sum() != 0, 'ground truth permutation matrix should not be all zeros'
-                if perm_mat.sum() == 0:
-                    precision = 0
-                    recall = 0
-                else:
-                    precision = (perm_mat * gt_array).sum() / perm_mat.sum()
-                    recall = (perm_mat * gt_array).sum() / gt_array.sum()
-                if precision == 0 or recall == 0:
-                    f1_score = 0
-                else:
-                    f1_score = (2 * precision * recall) / (precision + recall)
-
-                cls_precision.append(precision)
-                cls_recall.append(recall)
-                cls_f1.append(f1_score)
-
-        result['precision'] = np.mean(cls_precision)
-        result['recall'] = np.mean(cls_recall)
-        result['f1'] = np.mean(cls_f1)
-        result['precision_std'] = np.std(cls_precision)
-        result['recall_std'] = np.std(cls_recall)
-        result['f1_std'] = np.std(cls_f1)
-        result['coverage'] = 2 * pred_cls_dict / (cls_dict * (cls_dict - 1))
-
-        if verbose:
-            print('Class {}: {}'.format(cls, 'p = {:.4f}¬±{:.4f}, r = {:.4f}¬±{:.4f}, f1 = {:.4f}¬±{:.4f}, cvg = {:.4f}' \
-                                        .format(result['precision'], result['precision_std'], result['recall'],
-                                                result['recall_std'], result['f1'], result['f1_std'], result['coverage']
-                                                )))
-        return result
-
-    def rm_gt_cache(self, last_epoch=False):
-        r"""
-        Remove ground truth cache. It is recommended to call this function after evaluation.
-
-        :param last_epoch: bool, whether this epoch is last epoch; if true, the directory of cache will also be removed, and no more data should be evaluated
-        """
-        if os.path.exists(self.gt_cache_path):
-            shutil.rmtree(self.gt_cache_path)
-            print('gt perm mat cache deleted')
-
-            if not last_epoch:
-                os.mkdir(self.gt_cache_path)
+"""
+The Benchmark module with a unified data interface to evaluate graph matching methods.
+
+If you are interested in the performance and the deep learning framework, please refer to our `ThinkMatch project <https://github.com/Thinklab-SJTU/ThinkMatch>`_.
+"""
+
+# Copyright (c) 2022 Thinklab@SJTU
+# pygmtools is licensed under Mulan PSL v2.
+# You can use this software according to the terms and conditions of the Mulan PSL v2.
+# You may obtain a copy of Mulan PSL v2 at:
+# http://license.coscl.org.cn/MulanPSL2
+# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND,
+# EITHER EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT,
+# MERCHANTABILITY OR FIT FOR A PARTICULAR PURPOSE.
+# See the Mulan PSL v2 for more details.
+
+import tempfile
+import shutil
+import itertools
+from scipy.sparse import coo_matrix
+from pygmtools.dataset import *
+
+
+class Benchmark:
+    r"""
+    The `Benchmark` module provides a unified data interface and an evaluating platform for different datasets.
+
+    :param name: str, dataset name, currently support ``'PascalVOC'``, ``'WillowObject'``, ``'IMC_PT_SparseGM'``, ``'CUB2011'``, ``'SPair71k'``
+    :param sets: str, problem set, ``'train'`` for training set and ``'test'`` for test set
+    :param obj_resize: tuple, (default: ``(256, 256)``) resized object size
+    :param problem: str, (default: ``'2GM'``) problem type, ``'2GM'`` for 2-graph matching and ``'MGM'`` for multi-graph matching
+    :param filter: str, (default: ``'intersection'``) filter of nodes, ``'intersection'`` refers to retaining only common nodes;
+       ``'inclusion'`` is only for 2GM and refers to filtering only one graph to make its nodes a subset of the other graph,
+       and ``'unfiltered'`` refers to retaining all nodes in all graphs
+    :param args: keyword settings for specific dataset
+
+    .. note::
+            Ground truth cache is saved only when the parameter ``sets`` is ``'test'``, so
+            the functions ``eval()`` and ``eval_cls()`` are only for ``'test'`` set.
+
+    """
+
+    def __init__(self, name, sets, obj_resize=(256, 256), problem='2GM', filter='intersection', **args):
+        assert name == 'PascalVOC' or name == 'SPair71k' or name == 'WillowObject' or name == 'IMC_PT_SparseGM' or name == 'CUB2011', 'No match found for dataset {}'.format(
+            name)
+        assert problem == '2GM' or problem == 'MGM' or problem == 'MGM3', 'No match found for problem {}'.format(
+            problem)
+        assert filter == 'intersection' or filter == 'inclusion' or filter == 'unfiltered', 'No match found for filter {}'.format(
+            filter)
+        assert not ((
+                            problem == 'MGM' or problem == 'MGM3') and filter == 'inclusion'), 'The filter inclusion only matches 2GM'
+
+        self.name = name
+        self.problem = problem
+        self.filter = filter
+        self.sets = sets
+        self.obj_resize = obj_resize
+
+        data_set = eval(self.name)(self.sets, self.obj_resize, **args)
+        suffix = data_set.suffix
+        self.data_path = os.path.join(data_set.dataset_dir, 'data-' + str(self.obj_resize) + '-' + suffix + '.json')
+        self.data_list_path = os.path.join(data_set.dataset_dir, sets + '.json')
+        self.classes = data_set.classes
+
+        with open(self.data_path) as f:
+            self.data_dict = json.load(f)
+
+        if self.sets == 'test':
+            tmpfile = tempfile.gettempdir()
+            pid_num = os.getpid()
+            cache_dir = str(pid_num) + '_gt_cache'
+            self.gt_cache_path = os.path.join(tmpfile, cache_dir)
+
+            if not os.path.exists(self.gt_cache_path):
+                os.mkdir(self.gt_cache_path)
+                print('gt perm mat cache built')
+
+    def get_data(self, ids, test=False, shuffle=True):
+        r"""
+        Fetch a data pair or pairs of data by image ID for training or test.
+
+        :param ids: list of image ID, usually in ``train.json`` or ``test.json``
+        :param test: bool, whether the fetched data is used for test; if true, this function will not return ground truth
+        :param shuffle: bool, whether to shuffle the order of keypoints
+        :return:
+                    **data_list**: list of data, like ``[{'img': np.array, 'kpts': coordinates of kpts}, ...]``
+
+                    **perm_mat_dict**: ground truth, like ``{(0,1):scipy.sparse, (0,2):scipy.sparse, ...}``, ``(0,1)`` refers to data pair ``(ids[0],ids[1])``
+
+                    **ids**: list of image ID
+        """
+        assert (self.problem == '2GM' and len(ids) == 2) or ((self.problem == 'MGM' or self.problem == 'MGM3') and len(
+            ids) > 2), '{} problem cannot get {} data'.format(self.problem, len(ids))
+
+        ids.sort()
+        data_list = []
+        for keys in ids:
+            obj_dict = dict()
+            boundbox = self.data_dict[keys]['bounds']
+            img_file = self.data_dict[keys]['path']
+            with Image.open(str(img_file)) as img:
+                obj = img.resize(self.obj_resize, resample=Image.BICUBIC,
+                                 box=(boundbox[0], boundbox[1], boundbox[2], boundbox[3]))
+                if self.name == 'CUB2011':
+                    if not obj.mode == 'RGB':
+                        obj = obj.convert('RGB')
+            obj_dict['img'] = np.array(obj)
+            obj_dict['kpts'] = self.data_dict[keys]['kpts']
+            obj_dict['cls'] = self.data_dict[keys]['cls']
+            obj_dict['univ_size'] = self.data_dict[keys]['univ_size']
+            if shuffle:
+                random.shuffle(obj_dict['kpts'])
+            data_list.append(obj_dict)
+
+        perm_mat_dict = dict()
+        id_combination = list(itertools.combinations(list(range(len(ids))), 2))
+        for id_tuple in id_combination:
+            perm_mat = np.zeros([len(data_list[_]['kpts']) for _ in id_tuple], dtype=np.float32)
+            row_list = []
+            col_list = []
+
+            for i, keypoint in enumerate(data_list[id_tuple[0]]['kpts']):
+                for j, _keypoint in enumerate(data_list[id_tuple[1]]['kpts']):
+                    if keypoint['labels'] == _keypoint['labels']:
+                        if keypoint['labels'] != 'outlier':
+                            perm_mat[i, j] = 1
+            for i, keypoint in enumerate(data_list[id_tuple[0]]['kpts']):
+                for j, _keypoint in enumerate(data_list[id_tuple[1]]['kpts']):
+                    if keypoint['labels'] == _keypoint['labels']:
+                        row_list.append(i)
+                        break
+            for i, keypoint in enumerate(data_list[id_tuple[1]]['kpts']):
+                for j, _keypoint in enumerate(data_list[id_tuple[0]]['kpts']):
+                    if keypoint['labels'] == _keypoint['labels']:
+                        col_list.append(i)
+                        break
+            row_list.sort()
+            col_list.sort()
+            if self.filter == 'intersection':
+                perm_mat = perm_mat[row_list, :]
+                perm_mat = perm_mat[:, col_list]
+                data_list[id_tuple[0]]['kpts'] = [data_list[id_tuple[0]]['kpts'][i] for i in row_list]
+                data_list[id_tuple[1]]['kpts'] = [data_list[id_tuple[1]]['kpts'][i] for i in col_list]
+            elif self.filter == 'inclusion':
+                perm_mat = perm_mat[row_list, :]
+                data_list[id_tuple[0]]['kpts'] = [data_list[id_tuple[0]]['kpts'][i] for i in row_list]
+            if not (len(ids) > 2 and self.filter == 'intersection'):
+                sparse_perm_mat = coo_matrix(perm_mat)
+                perm_mat_dict[id_tuple] = sparse_perm_mat
+
+        if len(ids) > 2 and self.filter == 'intersection':
+            for p in range(len(ids) - 1):
+                perm_mat_list = [np.zeros([len(data_list[p]['kpts']), len(x['kpts'])], dtype=np.float32) for x in
+                                 data_list[p + 1: len(ids)]]
+                row_list = []
+                col_lists = []
+                for i in range(len(ids) - p - 1):
+                    col_lists.append([])
+
+                for i, keypoint in enumerate(data_list[p]['kpts']):
+                    kpt_idx = []
+                    for anno_dict in data_list[p + 1: len(ids)]:
+                        kpt_name_list = [x['labels'] for x in anno_dict['kpts']]
+                        if keypoint['labels'] in kpt_name_list:
+                            kpt_idx.append(kpt_name_list.index(keypoint['labels']))
+                        else:
+                            kpt_idx.append(-1)
+                    row_list.append(i)
+                    for k in range(len(ids) - p - 1):
+                        j = kpt_idx[k]
+                        if j != -1:
+                            col_lists[k].append(j)
+                            if keypoint['labels'] != 'outlier':
+                                perm_mat_list[k][i, j] = 1
+
+                row_list.sort()
+                for col_list in col_lists:
+                    col_list.sort()
+
+                for k in range(len(ids) - p - 1):
+                    perm_mat_list[k] = perm_mat_list[k][row_list, :]
+                    perm_mat_list[k] = perm_mat_list[k][:, col_lists[k]]
+                    id_tuple = (p, k + p + 1)
+                    perm_mat_dict[id_tuple] = coo_matrix(perm_mat_list[k])
+
+        if self.sets == 'test':
+            for pair in id_combination:
+                id_pair = (ids[pair[0]], ids[pair[1]])
+                gt_path = os.path.join(self.gt_cache_path, str(id_pair) + '.npy')
+                if not os.path.exists(gt_path):
+                    np.save(gt_path, perm_mat_dict[pair])
+
+        if not test:
+            return data_list, perm_mat_dict, ids
+        else:
+            return data_list, ids
+
+    def rand_get_data(self, cls=None, num=2, test=False, shuffle=True):
+        r"""
+        Randomly fetch data for training or test. Implemented by calling ``get_data`` function.
+
+        :param cls: int or str, class of expected data. None for random class
+        :param num: int, number of images; for example, 2 for 2GM
+        :param test: bool, whether the fetched data is used for test; if true, this function will not return ground truth
+        :param shuffle: bool, whether to shuffle the order of keypoints
+        :return:
+                    **data_list**: list of data, like ``[{'img': np.array, 'kpts': coordinates of kpts}, ...]``
+
+                    **perm_mat_dict**: ground truth, like ``{(0,1):scipy.sparse, (0,2):scipy.sparse, ...}``, ``(0,1)`` refers to data pair ``(ids[0],ids[1])``
+
+                    **ids**: list of image ID
+        """
+        if cls == None:
+            cls = random.randrange(0, len(self.classes))
+            clss = self.classes[cls]
+        elif type(cls) == str:
+            clss = cls
+
+        with open(self.data_list_path) as f1:
+            data_id = json.load(f1)
+
+        data_list = []
+        ids = []
+        if self.name != 'SPair71k':
+            for id in data_id:
+                if self.data_dict[id]['cls'] == clss:
+                    data_list.append(id)
+
+            for objID in random.sample(data_list, num):
+                ids.append(objID)
+        else:
+            for id in data_id:
+                if self.data_dict[id[0]]['cls'] == clss:
+                    data_list.append(id)
+            ids = random.sample(data_list, 1)[0]
+
+        return self.get_data(ids, test, shuffle)
+
+    def get_id_combination(self, cls=None, num=2):
+        r"""
+        Get the combination of images and length of combinations in specified class.
+
+        :param cls: int or str, class of expected data. None for all classes
+        :param num: int, number of images in each image ID list; for example, 2 for 2GM
+        :return:
+                **id_combination_list**: list of combinations of image ids
+
+                **length**: length of combinations
+        """
+        if cls == None:
+            clss = None
+        elif type(cls) == str:
+            clss = cls
+        else:
+            raise ValueError(f'Expect cls argument to be NoneType or str, got {type(cls)}!')
+
+        with open(self.data_list_path) as f1:
+            data_id = json.load(f1)
+
+        length = 0
+        id_combination_list = []
+        if clss != None:
+            data_list = []
+            if self.name != 'SPair71k':
+                for id in data_id:
+                    if self.data_dict[id]['cls'] == clss:
+                        data_list.append(id)
+                id_combination = list(itertools.combinations(data_list, num))
+                length += len(id_combination)
+                id_combination_list.append(id_combination)
+            else:
+                for id_pair in data_id:
+                    if self.data_dict[id_pair[0]]['cls'] == clss:
+                        data_list.append(id_pair)
+                length += len(data_list)
+                id_combination_list.append(data_list)
+        else:
+            for clss in self.classes:
+                data_list = []
+                if self.name != 'SPair71k':
+                    for id in data_id:
+                        if self.data_dict[id]['cls'] == clss:
+                            data_list.append(id)
+                    id_combination = list(itertools.combinations(data_list, num))
+                    length += len(id_combination)
+                    id_combination_list.append(id_combination)
+                else:
+                    for id_pair in data_id:
+                        if self.data_dict[id_pair[0]]['cls'] == clss:
+                            data_list.append(id_pair)
+                    length += len(data_list)
+                    id_combination_list.append(data_list)
+
+        return id_combination_list, length
+
+    def compute_length(self, cls=None, num=2):
+        r"""
+        Compute the length of image combinations in specified class.
+
+        :param cls: int or str, class of expected data. None for all classes
+        :param num: int, number of images in each image ID list; for example, 2 for two-graph matching problem
+        :return: length of combinations
+        """
+        if cls == None:
+            clss = None
+        elif type(cls) == str:
+            clss = cls
+
+        with open(self.data_list_path) as f1:
+            data_id = json.load(f1)
+
+        length = 0
+
+        if clss != None:
+            if self.name != 'SPair71k':
+                data_list = []
+                for id in data_id:
+                    if self.data_dict[id]['cls'] == clss:
+                        data_list.append(id)
+                id_combination = list(itertools.combinations(data_list, num))
+                length += len(id_combination)
+            else:
+                for id_pair in data_id:
+                    if self.data_dict[id_pair[0]]['cls'] == clss:
+                        length += 1
+
+        else:
+            for clss in self.classes:
+                if self.name != 'SPair71k':
+                    data_list = []
+                    for id in data_id:
+                        if self.data_dict[id]['cls'] == clss:
+                            data_list.append(id)
+                    id_combination = list(itertools.combinations(data_list, num))
+                    length += len(id_combination)
+                else:
+                    for id_pair in data_id:
+                        if self.data_dict[id_pair[0]]['cls'] == clss:
+                            length += 1
+        return length
+
+    def compute_img_num(self, classes):
+        r"""
+        Compute number of images in specified classes.
+
+        :param classes: list of dataset classes
+        :return: list of numbers of images in each class
+        """
+        with open(self.data_list_path) as f1:
+            data_id = json.load(f1)
+        num_list = []
+        for clss in classes:
+            cls_img_num = 0
+            if self.name != 'SPair71k':
+                for id in data_id:
+                    if self.data_dict[id]['cls'] == clss:
+                        cls_img_num += 1
+                num_list.append(cls_img_num)
+            else:
+                img_cache = []
+                for id_pair in data_id:
+                    if self.data_dict[id_pair[0]]['cls'] == clss:
+                        if id_pair[0] not in img_cache:
+                            img_cache.append(id_pair[0])
+                            cls_img_num += 1
+                        if id_pair[1] not in img_cache:
+                            img_cache.append(id_pair[1])
+                            cls_img_num += 1
+                num_list.append(cls_img_num)
+
+        return num_list
+
+    def eval(self, prediction, classes, verbose=False, rm_gt_cache=True):
+        r"""
+        Evaluate test results and compute matching accuracy and coverage.
+
+        :param prediction: list, prediction result, like ``[{'ids': (id1, id2), 'cls': cls, 'permmat': np.array or scipy.sparse}, ...]``
+        :param classes: list of evaluated classes
+        :param verbose: bool, whether to print the result
+        :param rm_gt_cache: bool, whether to remove ground truth cache
+        :return: evaluation result in each class and their averages, including p, r, f1 and their standard deviation and coverage
+
+        .. note::
+            If there are duplicate data pair in ``prediction``, this function will only evaluate the first pair and
+            expect that this pair is also the first fetched pair. Therefore, it is recommended that ``prediction`` is
+            built in an ordered manner, and not shuffled.
+
+        .. note::
+            Ground truth cache is saved when data pairs are fetched, and should be removed after evaluation. Make sure
+            all data pairs are evaluated at once, i.e., ``prediction`` should contain all fetched data pairs.
+        """
+
+        with open(self.data_list_path) as f1:
+            data_id = json.load(f1)
+
+        cls_dict = dict()
+        pred_cls_dict = dict()
+        result = dict()
+        id_cache = []
+        cls_precision = dict()
+        cls_recall = dict()
+        cls_f1 = dict()
+
+        for cls in classes:
+            cls_dict[cls] = 0
+            pred_cls_dict[cls] = 0
+            result[cls] = dict()
+            cls_precision[cls] = []
+            cls_recall[cls] = []
+            cls_f1[cls] = []
+
+        if self.name != 'SPair71k':
+            for key, obj in self.data_dict.items():
+                if (key in data_id) and (obj['cls'] in classes):
+                    cls_dict[obj['cls']] += 1
+        else:
+            for cls in classes:
+                cls_dict[cls] = self.compute_img_num([cls])[0]
+
+        for pair_dict in prediction:
+            ids = (pair_dict['ids'][0], pair_dict['ids'][1])
+            if ids not in id_cache:
+                id_cache.append(ids)
+                pred_cls_dict[pair_dict['cls']] += 1
+                perm_mat = pair_dict['perm_mat']
+                gt_path = os.path.join(self.gt_cache_path, str(ids) + '.npy')
+                gt = np.load(gt_path, allow_pickle=True).item()
+                gt_array = gt.toarray()
+                assert type(perm_mat) == type(gt_array)
+
+                assert gt_array.sum() != 0, 'ground truth permutation matrix should not be all zeros'
+                if perm_mat.sum() == 0:
+                    precision = 0
+                    recall = 0
+                else:
+                    precision = (perm_mat * gt_array).sum() / perm_mat.sum()
+                    recall = (perm_mat * gt_array).sum() / gt_array.sum()
+                if precision == 0 or recall == 0:
+                    f1_score = 0
+                else:
+                    f1_score = (2 * precision * recall) / (precision + recall)
+
+                cls_precision[pair_dict['cls']].append(precision)
+                cls_recall[pair_dict['cls']].append(recall)
+                cls_f1[pair_dict['cls']].append(f1_score)
+
+        p_sum = 0
+        r_sum = 0
+        f1_sum = 0
+        p_std_sum = 0
+        r_std_sum = 0
+        f1_std_sum = 0
+
+        for cls in classes:
+            result[cls]['precision'] = np.mean(cls_precision[cls])
+            result[cls]['recall'] = np.mean(cls_recall[cls])
+            result[cls]['f1'] = np.mean(cls_f1[cls])
+            result[cls]['precision_std'] = np.std(cls_precision[cls])
+            result[cls]['recall_std'] = np.std(cls_recall[cls])
+            result[cls]['f1_std'] = np.std(cls_f1[cls])
+            result[cls]['coverage'] = 2 * pred_cls_dict[cls] / (cls_dict[cls] * (cls_dict[cls] - 1))
+            p_sum += result[cls]['precision']
+            r_sum += result[cls]['recall']
+            f1_sum += result[cls]['f1']
+            p_std_sum += result[cls]['precision_std']
+            r_std_sum += result[cls]['recall_std']
+            f1_std_sum += result[cls]['f1_std']
+
+        result['mean'] = dict()
+        result['mean']['precision'] = p_sum / len(classes)
+        result['mean']['recall'] = r_sum / len(classes)
+        result['mean']['f1'] = f1_sum / len(classes)
+        result['mean']['precision_std'] = p_std_sum / len(classes)
+        result['mean']['recall_std'] = r_std_sum / len(classes)
+        result['mean']['f1_std'] = f1_std_sum / len(classes)
+
+        if verbose:
+            print('Matching accuracy')
+            for cls in classes:
+                print('{}: {}'.format(cls, 'p = {:.4f}¬±{:.4f}, r = {:.4f}¬±{:.4f}, f1 = {:.4f}¬±{:.4f}, cvg = {:.4f}' \
+                                      .format(result[cls]['precision'], result[cls]['precision_std'],
+                                              result[cls]['recall'], result[cls]['recall_std'], result[cls]['f1'],
+                                              result[cls]['f1_std'], result[cls]['coverage']
+                                              )))
+            print('average accuracy: {}'.format('p = {:.4f}¬±{:.4f}, r = {:.4f}¬±{:.4f}, f1 = {:.4f}¬±{:.4f}' \
+                                                .format(result['mean']['precision'], result['mean']['precision_std'],
+                                                        result['mean']['recall'], result['mean']['recall_std'],
+                                                        result['mean']['f1'], result['mean']['f1_std']
+                                                        )))
+        if rm_gt_cache:
+            self.rm_gt_cache(last_epoch=False)
+        return result
+
+    def eval_cls(self, prediction, cls, verbose=False):
+        r"""
+        Evaluate test results and compute matching accuracy and coverage on one specified class.
+
+        :param prediction: list, prediction result on one class, like ``[{'ids': (id1, id2), 'cls': cls, 'permmat': np.array or scipy.sparse}, ...]``
+        :param cls: str, evaluated class
+        :param verbose: bool, whether to print the result
+        :return: evaluation result on the specified class, including p, r, f1 and their standard deviation and coverage
+
+        .. note::
+            If there are duplicate data pair in ``prediction``, this function will only evaluate the first pair and
+            expect that this pair is also the first fetched pair. Therefore, it is recommended that ``prediction`` is
+            built in an ordered manner, and not shuffled. Same as the function ``eval``.
+
+        .. note::
+            This function will not automatically remove ground truth cache. However, you can still mannually call the
+            class function ``rm_gt_cache`` to remove groud truth cache after evaluation.
+        """
+
+        with open(self.data_list_path) as f1:
+            data_id = json.load(f1)
+
+        result = dict()
+        id_cache = []
+        cls_precision = []
+        cls_recall = []
+        cls_f1 = []
+
+        cls_dict = 0
+        pred_cls_dict = 0
+
+        if self.name != 'SPair71k':
+            for key, obj in self.data_dict.items():
+                if (key in data_id) and (obj['cls'] == cls):
+                    cls_dict += 1
+        else:
+            cls_dict = self.compute_img_num([cls])[0]
+
+        for pair_dict in prediction:
+            ids = (pair_dict['ids'][0], pair_dict['ids'][1])
+            if ids not in id_cache:
+                id_cache.append(ids)
+                pred_cls_dict += 1
+                perm_mat = pair_dict['perm_mat']
+                gt_path = os.path.join(self.gt_cache_path, str(ids) + '.npy')
+                gt = np.load(gt_path, allow_pickle=True).item()
+                gt_array = gt.toarray()
+                assert type(perm_mat) == type(gt_array)
+
+                assert gt_array.sum() != 0, 'ground truth permutation matrix should not be all zeros'
+                if perm_mat.sum() == 0:
+                    precision = 0
+                    recall = 0
+                else:
+                    precision = (perm_mat * gt_array).sum() / perm_mat.sum()
+                    recall = (perm_mat * gt_array).sum() / gt_array.sum()
+                if precision == 0 or recall == 0:
+                    f1_score = 0
+                else:
+                    f1_score = (2 * precision * recall) / (precision + recall)
+
+                cls_precision.append(precision)
+                cls_recall.append(recall)
+                cls_f1.append(f1_score)
+
+        result['precision'] = np.mean(cls_precision)
+        result['recall'] = np.mean(cls_recall)
+        result['f1'] = np.mean(cls_f1)
+        result['precision_std'] = np.std(cls_precision)
+        result['recall_std'] = np.std(cls_recall)
+        result['f1_std'] = np.std(cls_f1)
+        result['coverage'] = 2 * pred_cls_dict / (cls_dict * (cls_dict - 1))
+
+        if verbose:
+            print('Class {}: {}'.format(cls, 'p = {:.4f}¬±{:.4f}, r = {:.4f}¬±{:.4f}, f1 = {:.4f}¬±{:.4f}, cvg = {:.4f}' \
+                                        .format(result['precision'], result['precision_std'], result['recall'],
+                                                result['recall_std'], result['f1'], result['f1_std'], result['coverage']
+                                                )))
+        return result
+
+    def rm_gt_cache(self, last_epoch=False):
+        r"""
+        Remove ground truth cache. It is recommended to call this function after evaluation.
+
+        :param last_epoch: bool, whether this epoch is last epoch; if true, the directory of cache will also be removed, and no more data should be evaluated
+        """
+        if os.path.exists(self.gt_cache_path):
+            shutil.rmtree(self.gt_cache_path)
+            print('gt perm mat cache deleted')
+
+            if not last_epoch:
+                os.mkdir(self.gt_cache_path)
```

### Comparing `pygmtools-0.3.8/pygmtools/classic_solvers.py` & `pygmtools-0.3.8a0/pygmtools/classic_solvers.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,1068 +1,1251 @@
-r"""
-Classic (learning-free) **two-graph matching** solvers. These two-graph matching solvers are recommended to solve
-matching problems with two explicit graphs, or problems formulated as Quadratic Assignment Problem (QAP).
-
-The two-graph matching problem considers both nodes and edges, formulated as a QAP (Lawler's):
-
-.. math::
-
-    &\max_{\mathbf{X}} \ \texttt{vec}(\mathbf{X})^\top \mathbf{K} \texttt{vec}(\mathbf{X})\\
-    s.t. \quad &\mathbf{X} \in \{0, 1\}^{n_1\times n_2}, \ \mathbf{X}\mathbf{1} = \mathbf{1}, \ \mathbf{X}^\top\mathbf{1} \leq \mathbf{1}
-"""
-
-# Copyright (c) 2022 Thinklab@SJTU
-# pygmtools is licensed under Mulan PSL v2.
-# You can use this software according to the terms and conditions of the Mulan PSL v2.
-# You may obtain a copy of Mulan PSL v2 at:
-# http://license.coscl.org.cn/MulanPSL2
-# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND,
-# EITHER EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT,
-# MERCHANTABILITY OR FIT FOR A PARTICULAR PURPOSE.
-# See the Mulan PSL v2 for more details.
-
-import importlib
-import pygmtools
-from pygmtools.utils import NOT_IMPLEMENTED_MSG, _check_shape, _get_shape, _unsqueeze, _squeeze, _check_data_type
-
-
-def sm(K, n1=None, n2=None, n1max=None, n2max=None, x0=None,
-       max_iter: int=50,
-       backend=None):
-    r"""
-    Spectral Graph Matching solver for graph matching (Lawler's QAP).
-    This algorithm is also known as Power Iteration method, because it works by computing the leading
-    eigenvector of the input affinity matrix by power iteration.
-
-    For each iteration,
-
-    .. math::
-
-        \mathbf{v}_{k+1} = \mathbf{K} \mathbf{v}_k / ||\mathbf{K} \mathbf{v}_k||_2
-
-    :param K: :math:`(b\times n_1n_2 \times n_1n_2)` the input affinity matrix, :math:`b`: batch size.
-    :param n1: :math:`(b)` number of nodes in graph1 (optional if n1max is given, and all n1=n1max).
-    :param n2: :math:`(b)` number of nodes in graph2 (optional if n2max is given, and all n2=n2max).
-    :param n1max: :math:`(b)` max number of nodes in graph1 (optional if n1 is given, and n1max=max(n1)).
-    :param n2max: :math:`(b)` max number of nodes in graph2 (optional if n2 is given, and n2max=max(n2)).
-    :param x0: :math:`(b\times n_1 \times n_2)` an initial matching solution for warm-start.
-               If not given, x0 will be randomly generated.
-    :param max_iter: (default: 50) max number of iterations. More iterations will help the solver to converge better,
-                     at the cost of increased inference time.
-    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
-    :return: :math:`(b\times n_1 \times n_2)` the solved doubly-stochastic matrix
-
-    .. note::
-        Either ``n1`` or ``n1max`` should be specified because it cannot be inferred from the input tensor size.
-        Same for ``n2`` or ``n2max``.
-
-    .. note::
-        We support batched instances with different number of nodes, therefore ``n1`` and ``n2`` are
-        required to specify the exact number of objects of each dimension in the batch. If not specified, we assume
-        the batched matrices are not padded and all elements in ``n1`` are equal, all in ``n2`` are equal.
-
-    .. note::
-        This function also supports non-batched input, by ignoring all batch dimensions in the input tensors.
-
-    .. note::
-        This solver is differentiable and supports gradient back-propagation.
-
-    .. warning::
-        The solver's output is normalized with a squared sum of 1, which is in line with the original implementation. If
-        a doubly-stochastic matrix is required, please call :func:`~pygmtools.classic_solvers.sinkhorn` after this. If a
-        discrete permutation matrix is required, please call :func:`~pygmtools.classic_solvers.hungarian`. Note that the
-        Hungarian algorithm will truncate the gradient and the Sinkhorn algorithm will not.
-
-    .. dropdown:: Numpy Example
-
-        ::
-        
-            >>> import numpy as np
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'numpy'
-            >>> np.random.seed(1)
-    
-            # Generate a batch of isomorphic graphs
-            >>> batch_size = 10
-            >>> X_gt = np.zeros((batch_size, 4, 4))
-            >>> X_gt[:, np.arange(0, 4, dtype=np.int64), np.random.permutation(4)] = 1
-            >>> A1 = np.random.rand(batch_size, 4, 4)
-            >>> A2 = np.matmul(np.matmul(X_gt.transpose((0, 2, 1)), A1), X_gt)
-            >>> n1 = n2 = np.repeat([4], batch_size)
-    
-            # Build affinity matrix
-            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
-            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
-            >>> import functools
-            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
-            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
-    
-            # Solve by SM. Note that X is normalized with a squared sum of 1
-            >>> X = pygm.sm(K, n1, n2)
-            >>> (X ** 2).sum(axis=(1, 2))
-            array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])
-    
-            # Accuracy
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum()
-            1.0
-
-    .. dropdown:: Pytorch Example
-
-        ::
-
-            >>> import torch
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'pytorch'
-            >>> _ = torch.manual_seed(1)
-    
-            # Generate a batch of isomorphic graphs
-            >>> batch_size = 10
-            >>> X_gt = torch.zeros(batch_size, 4, 4)
-            >>> X_gt[:, torch.arange(0, 4, dtype=torch.int64), torch.randperm(4)] = 1
-            >>> A1 = torch.rand(batch_size, 4, 4)
-            >>> A2 = torch.bmm(torch.bmm(X_gt.transpose(1, 2), A1), X_gt)
-            >>> n1 = n2 = torch.tensor([4] * batch_size)
-    
-            # Build affinity matrix
-            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
-            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
-            >>> import functools
-            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
-            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
-    
-            # Solve by SM. Note that X is normalized with a squared sum of 1
-            >>> X = pygm.sm(K, n1, n2)
-            >>> (X ** 2).sum(dim=(1, 2))
-            tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
-                    1.0000])
-    
-            # Accuracy
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum()
-            tensor(1.)
-    
-            # This solver supports gradient back-propogation
-            >>> K = K.requires_grad_(True)
-            >>> pygm.sm(K, n1, n2).sum().backward()
-            >>> len(torch.nonzero(K.grad))
-            2560
-
-    .. dropdown:: Paddle Example
-
-        ::
-
-            >>> import paddle
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'paddle'
-            >>> _ = paddle.seed(1)
-
-            # Generate a batch of isomorphic graphs
-            >>> batch_size = 10
-            >>> X_gt = paddle.zeros((batch_size, 4, 4))
-            >>> X_gt[:, paddle.arange(0, 4, dtype=paddle.int64), paddle.randperm(4)] = 1
-            >>> A1 = paddle.rand((batch_size, 4, 4))
-            >>> A2 = paddle.bmm(paddle.bmm(X_gt.transpose((0, 2, 1)), A1), X_gt)
-            >>> n1 = n2 = paddle.to_tensor([4] * batch_size)
-
-            # Build affinity matrix
-            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
-            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
-            >>> import functools
-            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
-            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
-
-            # Solve by SM. Note that X is normalized with a squared sum of 1
-            >>> X = pygm.sm(K, n1, n2)
-            >>> (X ** 2).sum(axis=(1, 2))
-            Tensor(shape=[10], dtype=float32, place=Place(cpu), stop_gradient=True,
-                    [1.        , 1.        , 0.99999994, 0.99999994, 1.00000012, 
-                     1.        , 1.00000012, 1.        , 1.        , 0.99999994])
-
-            # Accuracy
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum()
-            Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True, [1.])
-
-            # This solver supports gradient back-propogation
-            >>> K.stop_gradient = False
-            >>> pygm.sm(K, n1, n2).sum().backward()
-            >>> len(paddle.nonzero(K.grad))
-            2560
-
-    .. dropdown:: Jittor Example
-
-        ::
-
-            >>> import jittor as jt
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'jittor'
-            >>> _ = jt.seed(1)
-
-            # Generate a batch of isomorphic graphs
-            >>> batch_size = 10
-            >>> X_gt = jt.zeros((batch_size, 4, 4))
-            >>> X_gt[:, jt.arange(0, 4, dtype=jt.int64), jt.randperm(4)] = 1
-            >>> A1 = jt.rand(batch_size, 4, 4)
-            >>> A2 = jt.bmm(jt.bmm(X_gt.transpose(1, 2), A1), X_gt)
-            >>> n1 = n2 = jt.Var([4] * batch_size)
-
-            # Build affinity matrix
-            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
-            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
-            >>> import functools
-            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
-            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
-
-            # Solve by SM. Note that X is normalized with a squared sum of 1
-            >>> X = pygm.sm(K, n1, n2)
-            >>> (X ** 2).sum(dim=1).sum(dim=1)
-            jt.Var([0.9999998  1.         0.9999999  1.0000001  1.         1.
-                    0.9999999  0.99999994 1.0000001  1.        ], dtype=float32)
-
-            # Accuracy
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum()
-            jt.Var([1.], dtype=float32)
-            
-            # This solver supports gradient back-propogation
-            >>> from jittor import nn
-            >>> class Model(nn.Module):
-            ...     def __init__(self, K):
-            ...         self.K = K
-            ...     def execute(self, K, n1, n2):
-            ...         X = pygm.sm(K, n1, n2)
-            ...         return X
-
-            >>> model = Model(K)
-            >>> optim = nn.SGD(model.parameters(), lr=0.1)
-            >>> X = model(K, n1, n2)
-            >>> loss = X.sum()
-            >>> optim.step(loss)
-            >>> len(jt.nonzero(K.opt_grad(optim)))
-            2560
-
-    .. dropdown:: MindSpore Example
-
-        ::
-
-            >>> import mindspore
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'mindspore'
-            >>> _ = mindspore.set_seed(1)
-            >>> mindspore.set_context(mode=mindspore.PYNATIVE_MODE)
-
-            # Generate a batch of isomorphic graphs
-            >>> batch_size = 10
-            >>> X_gt = mindspore.numpy.zeros((batch_size, 4, 4))
-            >>> X_gt[:, mindspore.numpy.arange(0, 4, dtype=mindspore.int64), mindspore.ops.Randperm(4)(mindspore.Tensor([4], dtype=mindspore.int32))] = 1
-            >>> A1 = mindspore.numpy.rand((batch_size, 4, 4))
-            >>> A2 = mindspore.ops.BatchMatMul()(mindspore.ops.BatchMatMul()(X_gt.swapaxes(1, 2), A1), X_gt)
-            >>> n1 = n2 = mindspore.Tensor([4] * batch_size)
-
-            # Build affinity matrix
-            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
-            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
-            >>> import functools
-            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
-            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
-
-            # Solve by SM. Note that X is normalized with a squared sum of 1
-            >>> X = pygm.sm(K, n1, n2)
-            >>> (X ** 2).sum(axis=(1, 2))
-            [1.0000002  0.9999998  1.0000002  0.99999964 1.         1.0000001
-            1.         1.         1.         0.99999994]
-
-            # Accuracy
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum()
-            1.0
-
-            # This solver supports gradient back-propogation
-            >>> def fn(K, n1, n2):
-            >>>     res = pygm.sm(K, n1, n2).sum()
-            >>>     return res
-
-            >>> g = mindspore.ops.grad(fn)(K, n1, n2)
-            >>> mindspore.ops.count_nonzero(g)
-            
-            # This solver supports gradient back-propogation
-            >>> from jittor import nn
-            >>> class Model(nn.Module):
-            ...     def __init__(self, K):
-            ...         self.K = K
-            ...     def execute(self, K, n1, n2):
-            ...         X = pygm.sm(K, n1, n2)
-            ...         return X
-
-            >>> model = Model(K)
-            >>> optim = nn.SGD(model.parameters(), lr=0.1)
-            >>> X = model(K, n1, n2)
-            >>> loss = X.sum()
-            >>> optim.step(loss)
-            >>> len(jt.nonzero(K.opt_grad(optim)))
-            2560
-
-    .. dropdown:: Tensorflow Example
-
-        ::
-
-            >>> import tensorflow as tf
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'tensorflow'
-            >>> _ = tf.random.set_seed(1)
-
-            # Generate a batch of isomorphic graphs
-            >>> batch_size = 10
-            >>> X_gt = tf.Variable(tf.zeros([batch_size, 4, 4]))
-            >>> indices = tf.stack([tf.range(4),tf.random.shuffle(tf.range(4))], axis=1)
-            >>> updates = tf.ones([4])
-            >>> for i in range(batch_size):
-            ...     _ = X_gt[i].assign(tf.tensor_scatter_nd_update(X_gt[i], indices, updates))
-            >>> A1 = tf.random.uniform([batch_size, 4, 4])
-            >>> A2 = tf.matmul(tf.matmul(tf.transpose(X_gt, perm=[0, 2, 1]), A1), X_gt)
-            >>> n1 = n2 = tf.constant([4] * batch_size)
-
-            # Build affinity matrix
-            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
-            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
-            >>> import functools
-            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
-            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
-
-            # Solve by SM. Note that X is normalized with a squared sum of 1
-            >>> X = pygm.sm(K, n1, n2)
-            >>> (X ** 2).sum(axis=(1, 2))
-            [1.         0.9999998  0.99999976 1.         0.99999976 1.
-            1.         1.0000001  1.0000001  1.        ]
-
-            # Accuracy
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum()
-            1.0
-            >>> tf.reduce_sum((X ** 2), axis=[1, 2])
-            <tf.Tensor: shape=(10,), dtype=float32, numpy=
-            array([1.        , 1.0000001 , 1.        , 0.9999999 , 1.        ,
-                   1.        , 1.0000001 , 0.99999994, 1.        , 0.9999998 ],
-                  dtype=float32)>
-
-            # Accuracy
-            >>> tf.reduce_sum((pygm.hungarian(X) * X_gt))/ tf.reduce_sum(X_gt)
-            <tf.Tensor: shape=(), dtype=float32, numpy=1.0>
-
-            This solver supports gradient back-propogation
-            >>> K = tf.Variable(K)
-            >>> with tf.GradientTape() as tape:
-            ...     y = tf.reduce_sum(pygm.sm(K, n1, n2))
-            ...     len(tf.where(tape.gradient(y, K)))
-            2560
-
-    .. note::
-        If you find this graph matching solver useful for your research, please cite:
-
-        ::
-
-            @inproceedings{sm,
-              title={A spectral technique for correspondence problems using pairwise constraints},
-              author={Leordeanu, Marius and Hebert, Martial},
-              year={2005},
-              pages={1482-1489},
-              booktitle={International Conference on Computer Vision},
-              publisher={IEEE}
-            }
-    """
-    if backend is None:
-        backend = pygmtools.BACKEND
-    _check_data_type(K, 'K', backend)
-    if _check_shape(K, 2, backend):
-        K = _unsqueeze(K, 0, backend)
-        non_batched_input = True
-        if type(n1) is int and n1max is None:
-            n1max = n1
-            n1 = None
-        if type(n2) is int and n2max is None:
-            n2max = n2
-            n2 = None
-    elif _check_shape(K, 3, backend):
-        non_batched_input = False
-    else:
-        raise ValueError(f'the input argument K is expected to be 2-dimensional or 3-dimensional, got '
-                         f'K:{len(_get_shape(K, backend))}dims!')
-    __check_gm_arguments(n1, n2, n1max, n2max)
-    args = (K, n1, n2, n1max, n2max, x0, max_iter)
-    try:
-        mod = importlib.import_module(f'pygmtools.{backend}_backend')
-        fn = mod.sm
-    except (ModuleNotFoundError, AttributeError):
-        raise NotImplementedError(
-            NOT_IMPLEMENTED_MSG.format(backend)
-        )
-    result = fn(*args)
-    if non_batched_input:
-        return _squeeze(result, 0, backend)
-    else:
-        return result
-
-
-def rrwm(K, n1=None, n2=None, n1max=None, n2max=None, x0=None,
-         max_iter: int=50, sk_iter: int=20, alpha: float=0.2, beta: float=30,
-         backend=None):
-    r"""
-    Reweighted Random Walk Matching (RRWM) solver for graph matching (Lawler's QAP). This algorithm is implemented by
-    power iteration with Sinkhorn reweighted jumps.
-
-    The official matlab implementation is available at https://cv.snu.ac.kr/research/~RRWM/
-
-    :param K: :math:`(b\times n_1n_2 \times n_1n_2)` the input affinity matrix, :math:`b`: batch size.
-    :param n1: :math:`(b)` number of nodes in graph1 (optional if n1max is given, and all n1=n1max).
-    :param n2: :math:`(b)` number of nodes in graph2 (optional if n2max is given, and all n2=n2max).
-    :param n1max: :math:`(b)` max number of nodes in graph1 (optional if n1 is given, and n1max=max(n1)).
-    :param n2max: :math:`(b)` max number of nodes in graph2 (optional if n2 is given, and n2max=max(n2)).
-    :param x0: :math:`(b\times n_1 \times n_2)` an initial matching solution for warm-start.
-               If not given, x0 will filled with :math:`\frac{1}{n_1 n_2})`.
-    :param max_iter: (default: 50) max number of iterations (i.e. number of random walk steps) in RRWM.
-                     More iterations will be lead to more accurate result, at the cost of increased inference time.
-    :param sk_iter: (default: 20) max number of Sinkhorn iterations. More iterations will be lead to more accurate
-                    result, at the cost of increased inference time.
-    :param alpha: (default: 0.2) the parameter controlling the importance of the reweighted jump. alpha should lie
-                  between 0 and 1. If ``alpha=0``, it means no reweighted jump;
-                  if alpha=1, the reweighted jump provides all information.
-    :param beta: (default: 30) the temperature parameter of exponential function before the Sinkhorn operator.
-                 ``beta`` should be larger than 0. A larger ``beta`` means more confidence in the jump. A larger
-                 ``beta`` will usually require a larger ``sk_iter``.
-    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
-    :return: :math:`(b\times n_1 \times n_2)` the solved matching matrix
-
-    .. note::
-        Either ``n1`` or ``n1max`` should be specified because it cannot be inferred from the input tensor size.
-        Same for ``n2`` or ``n2max``.
-
-    .. note::
-        We support batched instances with different number of nodes, therefore ``n1`` and ``n2`` are
-        required to specify the exact number of objects of each dimension in the batch. If not specified, we assume
-        the batched matrices are not padded and all elements in ``n1`` are equal, all in ``n2`` are equal.
-
-    .. note::
-        This function also supports non-batched input, by ignoring all batch dimensions in the input tensors.
-
-    .. note::
-        This solver is differentiable and supports gradient back-propagation.
-
-    .. warning::
-        The solver's output is normalized with a sum of 1, which is in line with the original implementation. If a doubly-
-        stochastic matrix is required, please call :func:`~pygmtools.classic_solvers.sinkhorn` after this. If a discrete
-        permutation matrix is required, please call :func:`~pygmtools.classic_solvers.hungarian`. Note that the
-        Hungarian algorithm will truncate the gradient and the Sinkhorn algorithm will not.
-
-    .. dropdown:: Numpy Example
-
-        ::
-
-            >>> import numpy as np
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'numpy'
-            >>> np.random.seed(1)
-    
-            # Generate a batch of isomorphic graphs
-            >>> batch_size = 10
-            >>> X_gt = np.zeros((batch_size, 4, 4))
-            >>> X_gt[:, np.arange(0, 4, dtype=np.int64), np.random.permutation(4)] = 1
-            >>> A1 = np.random.rand(batch_size, 4, 4)
-            >>> A2 = np.matmul(np.matmul(X_gt.transpose((0, 2, 1)), A1), X_gt)
-            >>> n1 = n2 = np.repeat([4], batch_size)
-    
-            # Build affinity matrix
-            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
-            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
-            >>> import functools
-            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
-            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
-    
-            # Solve by RRWM. Note that X is normalized with a sum of 1
-            >>> X = pygm.rrwm(K, n1, n2, beta=100)
-            >>> X.sum(axis=(1, 2))
-            array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])
-    
-            # Accuracy
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum()
-            1.0
-
-    .. dropdown:: Pytorch Example
-
-        ::
-
-            >>> import torch
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'pytorch'
-            >>> _ = torch.manual_seed(1)
-    
-            # Generate a batch of isomorphic graphs
-            >>> batch_size = 10
-            >>> X_gt = torch.zeros(batch_size, 4, 4)
-            >>> X_gt[:, torch.arange(0, 4, dtype=torch.int64), torch.randperm(4)] = 1
-            >>> A1 = torch.rand(batch_size, 4, 4)
-            >>> A2 = torch.bmm(torch.bmm(X_gt.transpose(1, 2), A1), X_gt)
-            >>> n1 = n2 = torch.tensor([4] * batch_size)
-    
-            # Build affinity matrix
-            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
-            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
-            >>> import functools
-            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
-            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
-    
-            # Solve by RRWM. Note that X is normalized with a sum of 1
-            >>> X = pygm.rrwm(K, n1, n2, beta=100)
-            >>> X.sum(dim=(1, 2))
-            tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
-                    1.0000])
-    
-            # Accuracy
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum()
-            tensor(1.)
-    
-            # This solver supports gradient back-propogation
-            >>> K = K.requires_grad_(True)
-            >>> pygm.rrwm(K, n1, n2, beta=100).sum().backward()
-            >>> len(torch.nonzero(K.grad))
-            272
-
-    .. dropdown:: Paddle Example
-
-        ::
-
-            >>> import paddle
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'paddle'
-            >>> _ = paddle.seed(1)
-
-            # Generate a batch of isomorphic graphs
-            >>> batch_size = 10
-            >>> X_gt = paddle.zeros((batch_size, 4, 4))
-            >>> X_gt[:, paddle.arange(0, 4, dtype=paddle.int64), paddle.randperm(4)] = 1
-            >>> A1 = paddle.rand((batch_size, 4, 4))
-            >>> A2 = paddle.bmm(paddle.bmm(X_gt.transpose((0, 2, 1)), A1), X_gt)
-            >>> n1 = n2 = paddle.to_tensor([4] * batch_size)
-
-            # Build affinity matrix
-            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
-            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
-            >>> import functools
-            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
-            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
-
-            # Solve by RRWM. Note that X is normalized with a sum of 1
-            >>> X = pygm.rrwm(K, n1, n2, beta=100)
-            >>> X.sum(axis=(1, 2))
-            Tensor(shape=[10], dtype=float32, place=Place(cpu), stop_gradient=True,
-                    [0.99999988, 0.99999988, 0.99999994, 0.99999994, 1.        , 
-                     1.        , 1.        , 1.00000012, 1.00000012, 1.        ])
-
-            # Accuracy
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum()
-            Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True, [1.])
-
-            # This solver supports gradient back-propogation
-            >>> K.stop_gradient = False
-            >>> pygm.rrwm(K, n1, n2, beta=100).sum().backward()
-            >>> len(paddle.nonzero(K.grad))
-            544
-
-    .. dropdown:: Jittor Example
-
-        ::
-
-            >>> import jittor as jt
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'jittor'
-            >>> _ = jt.seed(1)
-
-
-            # Generate a batch of isomorphic graphs
-            >>> batch_size = 10
-            >>> X_gt = jt.zeros((batch_size, 4, 4))
-            >>> X_gt[:, jt.arange(0, 4, dtype=jt.int64), jt.randperm(4)] = 1
-            >>> A1 = jt.rand(batch_size, 4, 4)
-            >>> A2 = jt.bmm(jt.bmm(X_gt.transpose(1, 2), A1), X_gt)
-            >>> n1 = n2 = jt.Var([4] * batch_size)
-
-            # Build affinity matrix
-            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
-            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
-            >>> import functools
-            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
-            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
-
-            # Solve by RRWM. Note that X is normalized with a sum of 1
-            >>> X = pygm.rrwm(K, n1, n2, beta=100)
-            >>> X.sum(dims=(1, 2))
-            jt.Var([1.         1.0000001  1.         0.99999976 1.         
-                    1.         1.         1.0000001  0.99999994 1.        ], dtype=float32)
-
-            # Accuracy
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum()
-            jt.Var([1.], dtype=float32)
-            
-            # This solver supports gradient back-propogation
-            >>> from jittor import nn
-            >>> class Model(nn.Module):
-            ...     def __init__(self, K):
-            ...         self.K = K
-            ...     def execute(self, K, n1, n2, beta):
-            ...         X = pygm.rrwm(K, n1, n2, beta=beta)
-            ...         return X
-
-            >>> model = Model(K)
-            >>> optim = nn.SGD(model.parameters(), lr=0.1)
-            >>> X = model(K, n1, n2, beta=100)
-            >>> loss = X.sum()
-            >>> optim.step(loss)
-            >>> len(jt.nonzero(K.opt_grad(optim)))
-            1536
-
-    .. dropdown:: MindSpore Example
-
-        ::
-
-            >>> import mindspore
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'mindspore'
-            >>> _ = mindspore.set_seed(1)
-            >>> mindspore.set_context(mode=mindspore.PYNATIVE_MODE)
-
-            # Generate a batch of isomorphic graphs
-            >>> batch_size = 10
-            >>> X_gt = mindspore.numpy.zeros((batch_size, 4, 4))
-            >>> X_gt[:, mindspore.numpy.arange(0, 4, dtype=mindspore.int64), mindspore.ops.Randperm(4)(mindspore.Tensor([4], dtype=mindspore.int32))] = 1
-            >>> A1 = mindspore.numpy.rand((batch_size, 4, 4))
-            >>> A2 = mindspore.ops.BatchMatMul()(mindspore.ops.BatchMatMul()(X_gt.swapaxes(1, 2), A1), X_gt)
-            >>> n1 = n2 = mindspore.Tensor([4] * batch_size)
-
-            # Build affinity matrix
-            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
-            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
-            >>> import functools
-            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
-            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
-
-            # Solve by RRWM. Note that X is normalized with a sum of 1
-            >>> X = pygm.rrwm(K, n1, n2, beta=100)
-            >>> X.sum(axis=(1, 2))
-            [1.         0.99999994 0.99999994 1.         1.0000002  1.
-            1.         1.         1.0000001  1.0000001 ]
-
-            # Accuracy
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum()
-            1.0
-
-            # This solver supports gradient back-propogation
-            >>> def fn(K, n1, n2, beta):
-            >>>     X = pygm.rrwm(K, n1, n2, beta=beta)
-            >>>     X_gt = mindspore.numpy.zeros((batch_size, 4, 4))
-            >>>     X_gt[:, mindspore.numpy.arange(0, 4, dtype=mindspore.int64),mindspore.ops.Randperm(4)(mindspore.Tensor([4], dtype=mindspore.int32))] = 1
-            >>>     res = pygm.utils.permutation_loss(X, X_gt)
-            >>>     return res
-
-            >>> g = mindspore.ops.grad(fn)(K, n1, n2, beta=100)
-            >>> mindspore.ops.count_nonzero(g)
-            2560
-
-    .. dropdown:: Tensorflow Example
-
-        ::
-
-            >>> import tensorflow as tf
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'tensorflow'
-            >>> _ = tf.random.set_seed(1)
-
-            # Generate a batch of isomorphic graphs
-            >>> batch_size = 10
-            >>> X_gt = tf.Variable(tf.zeros([batch_size, 4, 4]))
-            >>> indices = tf.stack([tf.range(4),tf.random.shuffle(tf.range(4))], axis=1)
-            >>> updates = tf.ones([4])
-            >>> for i in range(batch_size):
-            ...     _ = X_gt[i].assign(tf.tensor_scatter_nd_update(X_gt[i], indices, updates))
-            >>> A1 = tf.random.uniform([batch_size, 4, 4])
-            >>> A2 = tf.matmul(tf.matmul(tf.transpose(X_gt, perm=[0, 2, 1]), A1), X_gt)
-            >>> n1 = n2 = tf.constant([4] * batch_size)
-
-            # Build affinity matrix
-            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
-            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
-            >>> import functools
-            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
-            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
-
-            # Solve by RRWM. Note that X is normalized with a sum of 1
-            >>> X = pygm.rrwm(K, n1, n2, beta=100)
-            >>> tf.reduce_sum(X, axis=[1, 2])
-            <tf.Tensor: shape=(10,), dtype=float32, numpy=
-            array([1.        , 1.        , 1.        , 0.99999994, 1.        ,
-                   1.        , 1.        , 0.99999994, 0.99999994, 1.0000001 ],
-                  dtype=float32)>
-
-            # Accuracy
-            >>> tf.reduce_sum((pygm.hungarian(X) * X_gt)) / tf.reduce_sum(X_gt)
-            <tf.Tensor: shape=(), dtype=float32, numpy=1.0>
-
-            # This solver supports gradient back-propogation
-            >>> K = tf.Variable(K)
-            >>> with tf.GradientTape(persistent=True) as tape:
-            ...     y = tf.reduce_sum(pygm.rrwm(K, n1, n2, beta=100))
-            ...     len(tf.where(tape.gradient(y, K)))
-            768
-
-    .. note::
-        If you find this graph matching solver useful in your research, please cite:
-
-        ::
-
-            @inproceedings{rrwm,
-              title={Reweighted random walks for graph matching},
-              author={Cho, Minsu and Lee, Jungmin and Lee, Kyoung Mu},
-              booktitle={European conference on Computer vision},
-              pages={492--505},
-              year={2010},
-              organization={Springer}
-            }
-    """
-    if backend is None:
-        backend = pygmtools.BACKEND
-    _check_data_type(K, 'K', backend)
-    if _check_shape(K, 2, backend):
-        K = _unsqueeze(K, 0, backend)
-        non_batched_input = True
-        if type(n1) is int and n1max is None:
-            n1max = n1
-            n1 = None
-        if type(n2) is int and n2max is None:
-            n2max = n2
-            n2 = None
-    elif _check_shape(K, 3, backend):
-        non_batched_input = False
-    else:
-        raise ValueError(f'the input argument K is expected to be 2-dimensional or 3-dimensional, got '
-                         f'K:{len(_get_shape(K, backend))}dims!')
-    __check_gm_arguments(n1, n2, n1max, n2max)
-    assert 0 <= alpha <= 1, f'illegal value of alpha, it should lie between 0 and 1, got alpha={alpha}!.'
-    assert beta > 0, f'illegal value of beta, it should be larger than 0, got beta={beta}!'
-
-    args = (K, n1, n2, n1max, n2max, x0, max_iter, sk_iter, alpha, beta)
-    try:
-        mod = importlib.import_module(f'pygmtools.{backend}_backend')
-        fn = mod.rrwm
-    except (ModuleNotFoundError, AttributeError):
-        raise NotImplementedError(
-            NOT_IMPLEMENTED_MSG.format(backend)
-        )
-    result = fn(*args)
-    if non_batched_input:
-        return _squeeze(result, 0, backend)
-    else:
-        return result
-
-
-def ipfp(K, n1=None, n2=None, n1max=None, n2max=None, x0=None,
-         max_iter: int=50,
-         backend=None):
-    r"""
-    Integer Projected Fixed Point (IPFP) method for graph matching (Lawler's QAP).
-
-    :param K: :math:`(b\times n_1n_2 \times n_1n_2)` the input affinity matrix, :math:`b`: batch size.
-    :param n1: :math:`(b)` number of nodes in graph1 (optional if n1max is given, and all n1=n1max).
-    :param n2: :math:`(b)` number of nodes in graph2 (optional if n2max is given, and all n2=n2max).
-    :param n1max: :math:`(b)` max number of nodes in graph1 (optional if n1 is given, and n1max=max(n1)).
-    :param n2max: :math:`(b)` max number of nodes in graph2 (optional if n2 is given, and n2max=max(n2)).
-    :param x0: :math:`(b\times n_1 \times n_2)` an initial matching solution for warm-start.
-               If not given, x0 will filled with :math:`\frac{1}{n_1 n_2})`.
-    :param max_iter: (default: 50) max number of iterations in IPFP.
-                     More iterations will be lead to more accurate result, at the cost of increased inference time.
-    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
-    :return: :math:`(b\times n_1 \times n_2)` the solved matching matrix
-
-    .. note::
-        Either ``n1`` or ``n1max`` should be specified because it cannot be inferred from the input tensor size.
-        Same for ``n2`` or ``n2max``.
-
-    .. note::
-        We support batched instances with different number of nodes, therefore ``n1`` and ``n2`` are
-        required to specify the exact number of objects of each dimension in the batch. If not specified, we assume
-        the batched matrices are not padded and all elements in ``n1`` are equal, all in ``n2`` are equal.
-
-    .. note::
-        This function also supports non-batched input, by ignoring all batch dimensions in the input tensors.
-
-    .. note::
-        This solver is non-differentiable. The output is a discrete matching matrix (i.e. permutation matrix).
-
-    .. dropdown:: Numpy Example
-
-        ::
-
-            >>> import numpy as np
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'numpy'
-            >>> np.random.seed(1)
-    
-            # Generate a batch of isomorphic graphs
-            >>> batch_size = 10
-            >>> X_gt = np.zeros((batch_size, 4, 4))
-            >>> X_gt[:, np.arange(0, 4, dtype=np.int64), np.random.permutation(4)] = 1
-            >>> A1 = np.random.rand(batch_size, 4, 4)
-            >>> A2 = np.matmul(np.matmul(X_gt.transpose((0, 2, 1)), A1), X_gt)
-            >>> n1 = n2 = np.repeat([4], batch_size)
-    
-            # Build affinity matrix
-            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
-            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
-            >>> import functools
-            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
-            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
-    
-            # Solve by IPFP
-            >>> X = pygm.ipfp(K, n1, n2)
-            >>> X[0]
-            array([[0., 0., 0., 1.],
-                   [0., 0., 1., 0.],
-                   [1., 0., 0., 0.],
-                   [0., 1., 0., 0.]])
-    
-            # Accuracy
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum()
-            1.0
-
-    .. dropdown:: Pytorch Example
-
-        ::
-
-            >>> import torch
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'pytorch'
-            >>> _ = torch.manual_seed(1)
-    
-            # Generate a batch of isomorphic graphs
-            >>> batch_size = 10
-            >>> X_gt = torch.zeros(batch_size, 4, 4)
-            >>> X_gt[:, torch.arange(0, 4, dtype=torch.int64), torch.randperm(4)] = 1
-            >>> A1 = torch.rand(batch_size, 4, 4)
-            >>> A2 = torch.bmm(torch.bmm(X_gt.transpose(1, 2), A1), X_gt)
-            >>> n1 = torch.tensor([4] * batch_size)
-            >>> n2 = torch.tensor([4] * batch_size)
-    
-            # Build affinity matrix
-            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
-            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
-            >>> import functools
-            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
-            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
-    
-            # Solve by IPFP
-            >>> X = pygm.ipfp(K, n1, n2)
-            >>> X[0]
-            tensor([[0., 1., 0., 0.],
-                    [0., 0., 0., 1.],
-                    [0., 0., 1., 0.],
-                    [1., 0., 0., 0.]])
-    
-            # Accuracy
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum()
-            tensor(1.)
-
-    .. dropdown:: Paddle Example
-
-        ::
-
-            >>> import paddle
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'paddle'
-            >>> _ = paddle.seed(1)
-
-            # Generate a batch of isomorphic graphs
-            >>> batch_size = 10
-            >>> X_gt = paddle.zeros((batch_size, 4, 4))
-            >>> X_gt[:, paddle.arange(0, 4, dtype=paddle.int64), paddle.randperm(4)] = 1
-            >>> A1 = paddle.rand((batch_size, 4, 4))
-            >>> A2 = paddle.bmm(paddle.bmm(X_gt.transpose((0, 2, 1)), A1), X_gt)
-            >>> n1 = paddle.to_tensor([4] * batch_size)
-            >>> n2 = paddle.to_tensor([4] * batch_size)
-
-            # Build affinity matrix
-            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
-            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
-            >>> import functools
-            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
-            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
-
-            # Solve by IPFP
-            >>> X = pygm.ipfp(K, n1, n2)
-            >>> X[0]
-            Tensor(shape=[4, 4], dtype=float32, place=Place(cpu), stop_gradient=True,
-                   [[0., 1., 0., 0.],
-                    [0., 0., 0., 1.],
-                    [0., 0., 1., 0.],
-                    [1., 0., 0., 0.]])
-
-            # Accuracy
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum()
-            Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True, [1.])
-
-    .. dropdown:: Jittor Example
-
-        ::
-
-            >>> import jittor as jt
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'jittor'
-            >>> _ = jt.seed(1)
-
-            # Generate a batch of isomorphic graphs
-            >>> batch_size = 10
-            >>> X_gt = jt.zeros((batch_size, 4, 4))
-            >>> X_gt[:, jt.arange(0, 4, dtype=jt.int64), jt.randperm(4)] = 1
-            >>> A1 = jt.rand(batch_size, 4, 4)
-            >>> A2 = jt.bmm(jt.bmm(X_gt.transpose(1, 2), A1), X_gt)
-            >>> n1 = jt.Var([4] * batch_size)
-            >>> n2 = jt.Var([4] * batch_size)
-
-            # Build affinity matrix
-            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
-            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
-            >>> import functools
-            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
-            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
-
-            # Solve by IPFP
-            >>> X = pygm.ipfp(K, n1, n2)
-            >>> X[0]
-            jt.Var([[1. 0. 0. 0.]
-                    [0. 0. 1. 0.]
-                    [0. 0. 0. 1.]
-                    [0. 1. 0. 0.]], dtype=float32)
-
-            # Accuracy
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum()
-            jt.Var([1.], dtype=float32)
-
-    .. dropdown:: MindSpore Example
-
-        ::
-
-            >>> import mindspore
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'mindspore'
-            >>> _ = mindspore.set_seed(1)
-
-            # Generate a batch of isomorphic graphs
-            >>> batch_size = 10
-            >>> X_gt = mindspore.numpy.zeros((batch_size, 4, 4))
-            >>> X_gt[:, mindspore.numpy.arange(0, 4, dtype=mindspore.int64), mindspore.ops.Randperm(4)(mindspore.Tensor([4], dtype=mindspore.int32))] = 1
-            >>> A1 = mindspore.numpy.rand((batch_size, 4, 4))
-            >>> A2 = mindspore.ops.BatchMatMul()(mindspore.ops.BatchMatMul()(X_gt.swapaxes(1, 2), A1), X_gt)
-            >>> n1 = mindspore.Tensor([4] * batch_size)
-            >>> n2 = mindspore.Tensor([4] * batch_size)
-
-            # Build affinity matrix
-            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
-            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
-            >>> import functools
-            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
-            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
-
-            # Solve by IPFP
-            >>> X = pygm.ipfp(K, n1, n2)
-            >>> X[0]
-            [[1. 0. 0. 0.]
-             [0. 0. 0. 1.]
-             [0. 0. 1. 0.]
-             [0. 1. 0. 0.]]
-
-            # Accuracy
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum()
-            1.0
-
-    .. dropdown:: Tensorflow Example
-
-        ::
-
-            >>> import tensorflow as tf
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'tensorflow'
-            >>> _ = tf.random.set_seed(1)
-
-            # Generate a batch of isomorphic graphs
-            >>> batch_size = 10
-            >>> X_gt = tf.Variable(tf.zeros([batch_size, 4, 4]))
-            >>> indices = tf.stack([tf.range(4),tf.random.shuffle(tf.range(4))], axis=1)
-            >>> updates = tf.ones([4])
-            >>> for i in range(batch_size):
-            ...     _ = X_gt[i].assign(tf.tensor_scatter_nd_update(X_gt[i], indices, updates))
-            >>> A1 = tf.random.uniform([batch_size, 4, 4])
-            >>> A2 = tf.matmul(tf.matmul(tf.transpose(X_gt, perm=[0, 2, 1]), A1), X_gt)
-            >>> n1 = n2 = tf.constant([4] * batch_size)
-
-            # Build affinity matrix
-            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
-            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
-            >>> import functools
-            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
-            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
-
-            # Solve by IPFP
-            >>> X = pygm.ipfp(K, n1, n2)
-            >>> X[0]
-            <tf.Tensor: shape=(4, 4), dtype=float32, numpy=
-            array([[0., 0., 1., 0.],
-                   [0., 1., 0., 0.],
-                   [0., 0., 0., 1.],
-                   [1., 0., 0., 0.]], dtype=float32)>
-
-            # Accuracy
-            >>> tf.reduce_sum((pygm.hungarian(X) * X_gt)) / tf.reduce_sum(X_gt)
-            <tf.Tensor: shape=(), dtype=float32, numpy=1.0>
-
-    .. note::
-        If you find this graph matching solver useful in your research, please cite:
-
-        ::
-
-            @article{ipfp,
-              title={An integer projected fixed point method for graph matching and map inference},
-              author={Leordeanu, Marius and Hebert, Martial and Sukthankar, Rahul},
-              journal={Advances in neural information processing systems},
-              volume={22},
-              year={2009}
-            }
-    """
-    if backend is None:
-        backend = pygmtools.BACKEND
-    _check_data_type(K, 'K', backend)
-    if _check_shape(K, 2, backend):
-        K = _unsqueeze(K, 0, backend)
-        non_batched_input = True
-        if type(n1) is int and n1max is None:
-            n1max = n1
-            n1 = None
-        if type(n2) is int and n2max is None:
-            n2max = n2
-            n2 = None
-    elif _check_shape(K, 3, backend):
-        non_batched_input = False
-    else:
-        raise ValueError(f'the input argument K is expected to be 2-dimensional or 3-dimensional, got '
-                         f'K:{len(_get_shape(K, backend))}dims!')
-    __check_gm_arguments(n1, n2, n1max, n2max)
-
-    args = (K, n1, n2, n1max, n2max, x0, max_iter)
-    try:
-        mod = importlib.import_module(f'pygmtools.{backend}_backend')
-        fn = mod.ipfp
-    except (ModuleNotFoundError, AttributeError):
-        raise NotImplementedError(
-            NOT_IMPLEMENTED_MSG.format(backend)
-        )
-    result = fn(*args)
-    if non_batched_input:
-        return _squeeze(result, 0, backend)
-    else:
-        return result
-
-
-def __check_gm_arguments(n1, n2, n1max, n2max):
-    if n1 is None and n1max is None:
-        raise ValueError('at least one of the following arguments are required: n1 and n1max.')
-    if n2 is None and n2max is None:
-        raise ValueError('at least one of the following arguments are required: n2 and n2max.')
+r"""
+Classic (learning-free) **two-graph matching** solvers. These two-graph matching solvers are recommended to solve
+matching problems with two explicit graphs, or problems formulated as Quadratic Assignment Problem (QAP).
+
+The two-graph matching problem considers both nodes and edges, formulated as a QAP (Lawler's):
+
+.. math::
+
+    &\max_{\mathbf{X}} \ \texttt{vec}(\mathbf{X})^\top \mathbf{K} \texttt{vec}(\mathbf{X})\\
+    s.t. \quad &\mathbf{X} \in \{0, 1\}^{n_1\times n_2}, \ \mathbf{X}\mathbf{1} = \mathbf{1}, \ \mathbf{X}^\top\mathbf{1} \leq \mathbf{1}
+"""
+
+# Copyright (c) 2022 Thinklab@SJTU
+# pygmtools is licensed under Mulan PSL v2.
+# You can use this software according to the terms and conditions of the Mulan PSL v2.
+# You may obtain a copy of Mulan PSL v2 at:
+# http://license.coscl.org.cn/MulanPSL2
+# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND,
+# EITHER EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT,
+# MERCHANTABILITY OR FIT FOR A PARTICULAR PURPOSE.
+# See the Mulan PSL v2 for more details.
+
+import importlib
+import pygmtools
+from pygmtools.utils import NOT_IMPLEMENTED_MSG, _check_shape, _get_shape,\
+    _unsqueeze, _squeeze, _check_data_type,from_numpy
+import numpy as np
+
+def sm(K, n1=None, n2=None, n1max=None, n2max=None, x0=None,
+       max_iter: int=50,
+       backend=None):
+    r"""
+    Spectral Graph Matching solver for graph matching (Lawler's QAP).
+    This algorithm is also known as Power Iteration method, because it works by computing the leading
+    eigenvector of the input affinity matrix by power iteration.
+
+    For each iteration,
+
+    .. math::
+
+        \mathbf{v}_{k+1} = \mathbf{K} \mathbf{v}_k / ||\mathbf{K} \mathbf{v}_k||_2
+
+    :param K: :math:`(b\times n_1n_2 \times n_1n_2)` the input affinity matrix, :math:`b`: batch size.
+    :param n1: :math:`(b)` number of nodes in graph1 (optional if n1max is given, and all n1=n1max).
+    :param n2: :math:`(b)` number of nodes in graph2 (optional if n2max is given, and all n2=n2max).
+    :param n1max: :math:`(b)` max number of nodes in graph1 (optional if n1 is given, and n1max=max(n1)).
+    :param n2max: :math:`(b)` max number of nodes in graph2 (optional if n2 is given, and n2max=max(n2)).
+    :param x0: :math:`(b\times n_1 \times n_2)` an initial matching solution for warm-start.
+               If not given, x0 will be randomly generated.
+    :param max_iter: (default: 50) max number of iterations. More iterations will help the solver to converge better,
+                     at the cost of increased inference time.
+    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
+    :return: :math:`(b\times n_1 \times n_2)` the solved doubly-stochastic matrix
+
+    .. note::
+        Either ``n1`` or ``n1max`` should be specified because it cannot be inferred from the input tensor size.
+        Same for ``n2`` or ``n2max``.
+
+    .. note::
+        We support batched instances with different number of nodes, therefore ``n1`` and ``n2`` are
+        required to specify the exact number of objects of each dimension in the batch. If not specified, we assume
+        the batched matrices are not padded and all elements in ``n1`` are equal, all in ``n2`` are equal.
+
+    .. note::
+        This function also supports non-batched input, by ignoring all batch dimensions in the input tensors.
+
+    .. note::
+        This solver is differentiable and supports gradient back-propagation.
+
+    .. warning::
+        The solver's output is normalized with a squared sum of 1, which is in line with the original implementation. If
+        a doubly-stochastic matrix is required, please call :func:`~pygmtools.classic_solvers.sinkhorn` after this. If a
+        discrete permutation matrix is required, please call :func:`~pygmtools.classic_solvers.hungarian`. Note that the
+        Hungarian algorithm will truncate the gradient and the Sinkhorn algorithm will not.
+
+    .. dropdown:: Numpy Example
+
+        ::
+        
+            >>> import numpy as np
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'numpy'
+            >>> np.random.seed(1)
+    
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> X_gt = np.zeros((batch_size, 4, 4))
+            >>> X_gt[:, np.arange(0, 4, dtype=np.int64), np.random.permutation(4)] = 1
+            >>> A1 = np.random.rand(batch_size, 4, 4)
+            >>> A2 = np.matmul(np.matmul(X_gt.transpose((0, 2, 1)), A1), X_gt)
+            >>> n1 = n2 = np.repeat([4], batch_size)
+    
+            # Build affinity matrix
+            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
+            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
+            >>> import functools
+            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
+            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
+    
+            # Solve by SM. Note that X is normalized with a squared sum of 1
+            >>> X = pygm.sm(K, n1, n2)
+            >>> (X ** 2).sum(axis=(1, 2))
+            array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])
+    
+            # Accuracy
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum()
+            1.0
+
+    .. dropdown:: Pytorch Example
+
+        ::
+
+            >>> import torch
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'pytorch'
+            >>> _ = torch.manual_seed(1)
+    
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> X_gt = torch.zeros(batch_size, 4, 4)
+            >>> X_gt[:, torch.arange(0, 4, dtype=torch.int64), torch.randperm(4)] = 1
+            >>> A1 = torch.rand(batch_size, 4, 4)
+            >>> A2 = torch.bmm(torch.bmm(X_gt.transpose(1, 2), A1), X_gt)
+            >>> n1 = n2 = torch.tensor([4] * batch_size)
+    
+            # Build affinity matrix
+            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
+            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
+            >>> import functools
+            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
+            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
+    
+            # Solve by SM. Note that X is normalized with a squared sum of 1
+            >>> X = pygm.sm(K, n1, n2)
+            >>> (X ** 2).sum(dim=(1, 2))
+            tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
+                    1.0000])
+    
+            # Accuracy
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum()
+            tensor(1.)
+    
+            # This solver supports gradient back-propogation
+            >>> K = K.requires_grad_(True)
+            >>> pygm.sm(K, n1, n2).sum().backward()
+            >>> len(torch.nonzero(K.grad))
+            2560
+
+    .. dropdown:: Paddle Example
+
+        ::
+
+            >>> import paddle
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'paddle'
+            >>> _ = paddle.seed(1)
+
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> X_gt = paddle.zeros((batch_size, 4, 4))
+            >>> X_gt[:, paddle.arange(0, 4, dtype=paddle.int64), paddle.randperm(4)] = 1
+            >>> A1 = paddle.rand((batch_size, 4, 4))
+            >>> A2 = paddle.bmm(paddle.bmm(X_gt.transpose((0, 2, 1)), A1), X_gt)
+            >>> n1 = n2 = paddle.to_tensor([4] * batch_size)
+
+            # Build affinity matrix
+            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
+            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
+            >>> import functools
+            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
+            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
+
+            # Solve by SM. Note that X is normalized with a squared sum of 1
+            >>> X = pygm.sm(K, n1, n2)
+            >>> (X ** 2).sum(axis=(1, 2))
+            Tensor(shape=[10], dtype=float32, place=Place(cpu), stop_gradient=True,
+                    [1.        , 1.        , 0.99999994, 0.99999994, 1.00000012, 
+                     1.        , 1.00000012, 1.        , 1.        , 0.99999994])
+
+            # Accuracy
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum()
+            Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True, [1.])
+
+            # This solver supports gradient back-propogation
+            >>> K.stop_gradient = False
+            >>> pygm.sm(K, n1, n2).sum().backward()
+            >>> len(paddle.nonzero(K.grad))
+            2560
+
+    .. dropdown:: Jittor Example
+
+        ::
+
+            >>> import jittor as jt
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'jittor'
+            >>> _ = jt.seed(1)
+
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> X_gt = jt.zeros((batch_size, 4, 4))
+            >>> X_gt[:, jt.arange(0, 4, dtype=jt.int64), jt.randperm(4)] = 1
+            >>> A1 = jt.rand(batch_size, 4, 4)
+            >>> A2 = jt.bmm(jt.bmm(X_gt.transpose(1, 2), A1), X_gt)
+            >>> n1 = n2 = jt.Var([4] * batch_size)
+
+            # Build affinity matrix
+            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
+            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
+            >>> import functools
+            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
+            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
+
+            # Solve by SM. Note that X is normalized with a squared sum of 1
+            >>> X = pygm.sm(K, n1, n2)
+            >>> (X ** 2).sum(dim=1).sum(dim=1)
+            jt.Var([0.9999998  1.         0.9999999  1.0000001  1.         1.
+                    0.9999999  0.99999994 1.0000001  1.        ], dtype=float32)
+
+            # Accuracy
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum()
+            jt.Var([1.], dtype=float32)
+            
+            # This solver supports gradient back-propogation
+            >>> from jittor import nn
+            >>> class Model(nn.Module):
+            ...     def __init__(self, K):
+            ...         self.K = K
+            ...     def execute(self, K, n1, n2):
+            ...         X = pygm.sm(K, n1, n2)
+            ...         return X
+
+            >>> model = Model(K)
+            >>> optim = nn.SGD(model.parameters(), lr=0.1)
+            >>> X = model(K, n1, n2)
+            >>> loss = X.sum()
+            >>> optim.step(loss)
+            >>> len(jt.nonzero(K.opt_grad(optim)))
+            2560
+
+    .. dropdown:: MindSpore Example
+
+        ::
+
+            >>> import mindspore
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'mindspore'
+            >>> _ = mindspore.set_seed(1)
+            >>> mindspore.set_context(mode=mindspore.PYNATIVE_MODE)
+
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> X_gt = mindspore.numpy.zeros((batch_size, 4, 4))
+            >>> X_gt[:, mindspore.numpy.arange(0, 4, dtype=mindspore.int64), mindspore.ops.Randperm(4)(mindspore.Tensor([4], dtype=mindspore.int32))] = 1
+            >>> A1 = mindspore.numpy.rand((batch_size, 4, 4))
+            >>> A2 = mindspore.ops.BatchMatMul()(mindspore.ops.BatchMatMul()(X_gt.swapaxes(1, 2), A1), X_gt)
+            >>> n1 = n2 = mindspore.Tensor([4] * batch_size)
+
+            # Build affinity matrix
+            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
+            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
+            >>> import functools
+            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
+            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
+
+            # Solve by SM. Note that X is normalized with a squared sum of 1
+            >>> X = pygm.sm(K, n1, n2)
+            >>> (X ** 2).sum(axis=(1, 2))
+            [1.0000002  0.9999998  1.0000002  0.99999964 1.         1.0000001
+            1.         1.         1.         0.99999994]
+
+            # Accuracy
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum()
+            1.0
+
+            # This solver supports gradient back-propogation
+            >>> def fn(K, n1, n2):
+            >>>     res = pygm.sm(K, n1, n2).sum()
+            >>>     return res
+
+            >>> g = mindspore.ops.grad(fn)(K, n1, n2)
+            >>> mindspore.ops.count_nonzero(g)
+            
+            # This solver supports gradient back-propogation
+            >>> from jittor import nn
+            >>> class Model(nn.Module):
+            ...     def __init__(self, K):
+            ...         self.K = K
+            ...     def execute(self, K, n1, n2):
+            ...         X = pygm.sm(K, n1, n2)
+            ...         return X
+
+            >>> model = Model(K)
+            >>> optim = nn.SGD(model.parameters(), lr=0.1)
+            >>> X = model(K, n1, n2)
+            >>> loss = X.sum()
+            >>> optim.step(loss)
+            >>> len(jt.nonzero(K.opt_grad(optim)))
+            2560
+
+    .. dropdown:: Tensorflow Example
+
+        ::
+
+            >>> import tensorflow as tf
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'tensorflow'
+            >>> _ = tf.random.set_seed(1)
+
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> X_gt = tf.Variable(tf.zeros([batch_size, 4, 4]))
+            >>> indices = tf.stack([tf.range(4),tf.random.shuffle(tf.range(4))], axis=1)
+            >>> updates = tf.ones([4])
+            >>> for i in range(batch_size):
+            ...     _ = X_gt[i].assign(tf.tensor_scatter_nd_update(X_gt[i], indices, updates))
+            >>> A1 = tf.random.uniform([batch_size, 4, 4])
+            >>> A2 = tf.matmul(tf.matmul(tf.transpose(X_gt, perm=[0, 2, 1]), A1), X_gt)
+            >>> n1 = n2 = tf.constant([4] * batch_size)
+
+            # Build affinity matrix
+            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
+            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
+            >>> import functools
+            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
+            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
+
+            # Solve by SM. Note that X is normalized with a squared sum of 1
+            >>> X = pygm.sm(K, n1, n2)
+            >>> (X ** 2).sum(axis=(1, 2))
+            [1.         0.9999998  0.99999976 1.         0.99999976 1.
+            1.         1.0000001  1.0000001  1.        ]
+
+            # Accuracy
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum()
+            1.0
+            >>> tf.reduce_sum((X ** 2), axis=[1, 2])
+            <tf.Tensor: shape=(10,), dtype=float32, numpy=
+            array([1.        , 1.0000001 , 1.        , 0.9999999 , 1.        ,
+                   1.        , 1.0000001 , 0.99999994, 1.        , 0.9999998 ],
+                  dtype=float32)>
+
+            # Accuracy
+            >>> tf.reduce_sum((pygm.hungarian(X) * X_gt))/ tf.reduce_sum(X_gt)
+            <tf.Tensor: shape=(), dtype=float32, numpy=1.0>
+
+            This solver supports gradient back-propogation
+            >>> K = tf.Variable(K)
+            >>> with tf.GradientTape() as tape:
+            ...     y = tf.reduce_sum(pygm.sm(K, n1, n2))
+            ...     len(tf.where(tape.gradient(y, K)))
+            2560
+
+    .. note::
+        If you find this graph matching solver useful for your research, please cite:
+
+        ::
+
+            @inproceedings{sm,
+              title={A spectral technique for correspondence problems using pairwise constraints},
+              author={Leordeanu, Marius and Hebert, Martial},
+              year={2005},
+              pages={1482-1489},
+              booktitle={International Conference on Computer Vision},
+              publisher={IEEE}
+            }
+    """
+    if backend is None:
+        backend = pygmtools.BACKEND
+    _check_data_type(K, 'K', backend)
+    if _check_shape(K, 2, backend):
+        K = _unsqueeze(K, 0, backend)
+        non_batched_input = True
+        if type(n1) is int and n1max is None:
+            n1max = n1
+            n1 = None
+        if type(n2) is int and n2max is None:
+            n2max = n2
+            n2 = None
+    elif _check_shape(K, 3, backend):
+        non_batched_input = False
+    else:
+        raise ValueError(f'the input argument K is expected to be 2-dimensional or 3-dimensional, got '
+                         f'K:{len(_get_shape(K, backend))}dims!')
+    __check_gm_arguments(n1, n2, n1max, n2max)
+    args = (K, n1, n2, n1max, n2max, x0, max_iter)
+    try:
+        mod = importlib.import_module(f'pygmtools.{backend}_backend')
+        fn = mod.sm
+    except (ModuleNotFoundError, AttributeError):
+        raise NotImplementedError(
+            NOT_IMPLEMENTED_MSG.format(backend)
+        )
+    result = fn(*args)
+    if non_batched_input:
+        return _squeeze(result, 0, backend)
+    else:
+        return result
+
+
+def rrwm(K, n1=None, n2=None, n1max=None, n2max=None, x0=None,
+         max_iter: int=50, sk_iter: int=20, alpha: float=0.2, beta: float=30,
+         backend=None):
+    r"""
+    Reweighted Random Walk Matching (RRWM) solver for graph matching (Lawler's QAP). This algorithm is implemented by
+    power iteration with Sinkhorn reweighted jumps.
+
+    The official matlab implementation is available at https://cv.snu.ac.kr/research/~RRWM/
+
+    :param K: :math:`(b\times n_1n_2 \times n_1n_2)` the input affinity matrix, :math:`b`: batch size.
+    :param n1: :math:`(b)` number of nodes in graph1 (optional if n1max is given, and all n1=n1max).
+    :param n2: :math:`(b)` number of nodes in graph2 (optional if n2max is given, and all n2=n2max).
+    :param n1max: :math:`(b)` max number of nodes in graph1 (optional if n1 is given, and n1max=max(n1)).
+    :param n2max: :math:`(b)` max number of nodes in graph2 (optional if n2 is given, and n2max=max(n2)).
+    :param x0: :math:`(b\times n_1 \times n_2)` an initial matching solution for warm-start.
+               If not given, x0 will filled with :math:`\frac{1}{n_1 n_2})`.
+    :param max_iter: (default: 50) max number of iterations (i.e. number of random walk steps) in RRWM.
+                     More iterations will be lead to more accurate result, at the cost of increased inference time.
+    :param sk_iter: (default: 20) max number of Sinkhorn iterations. More iterations will be lead to more accurate
+                    result, at the cost of increased inference time.
+    :param alpha: (default: 0.2) the parameter controlling the importance of the reweighted jump. alpha should lie
+                  between 0 and 1. If ``alpha=0``, it means no reweighted jump;
+                  if alpha=1, the reweighted jump provides all information.
+    :param beta: (default: 30) the temperature parameter of exponential function before the Sinkhorn operator.
+                 ``beta`` should be larger than 0. A larger ``beta`` means more confidence in the jump. A larger
+                 ``beta`` will usually require a larger ``sk_iter``.
+    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
+    :return: :math:`(b\times n_1 \times n_2)` the solved matching matrix
+
+    .. note::
+        Either ``n1`` or ``n1max`` should be specified because it cannot be inferred from the input tensor size.
+        Same for ``n2`` or ``n2max``.
+
+    .. note::
+        We support batched instances with different number of nodes, therefore ``n1`` and ``n2`` are
+        required to specify the exact number of objects of each dimension in the batch. If not specified, we assume
+        the batched matrices are not padded and all elements in ``n1`` are equal, all in ``n2`` are equal.
+
+    .. note::
+        This function also supports non-batched input, by ignoring all batch dimensions in the input tensors.
+
+    .. note::
+        This solver is differentiable and supports gradient back-propagation.
+
+    .. warning::
+        The solver's output is normalized with a sum of 1, which is in line with the original implementation. If a doubly-
+        stochastic matrix is required, please call :func:`~pygmtools.classic_solvers.sinkhorn` after this. If a discrete
+        permutation matrix is required, please call :func:`~pygmtools.classic_solvers.hungarian`. Note that the
+        Hungarian algorithm will truncate the gradient and the Sinkhorn algorithm will not.
+
+    .. dropdown:: Numpy Example
+
+        ::
+
+            >>> import numpy as np
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'numpy'
+            >>> np.random.seed(1)
+    
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> X_gt = np.zeros((batch_size, 4, 4))
+            >>> X_gt[:, np.arange(0, 4, dtype=np.int64), np.random.permutation(4)] = 1
+            >>> A1 = np.random.rand(batch_size, 4, 4)
+            >>> A2 = np.matmul(np.matmul(X_gt.transpose((0, 2, 1)), A1), X_gt)
+            >>> n1 = n2 = np.repeat([4], batch_size)
+    
+            # Build affinity matrix
+            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
+            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
+            >>> import functools
+            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
+            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
+    
+            # Solve by RRWM. Note that X is normalized with a sum of 1
+            >>> X = pygm.rrwm(K, n1, n2, beta=100)
+            >>> X.sum(axis=(1, 2))
+            array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])
+    
+            # Accuracy
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum()
+            1.0
+
+    .. dropdown:: Pytorch Example
+
+        ::
+
+            >>> import torch
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'pytorch'
+            >>> _ = torch.manual_seed(1)
+    
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> X_gt = torch.zeros(batch_size, 4, 4)
+            >>> X_gt[:, torch.arange(0, 4, dtype=torch.int64), torch.randperm(4)] = 1
+            >>> A1 = torch.rand(batch_size, 4, 4)
+            >>> A2 = torch.bmm(torch.bmm(X_gt.transpose(1, 2), A1), X_gt)
+            >>> n1 = n2 = torch.tensor([4] * batch_size)
+    
+            # Build affinity matrix
+            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
+            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
+            >>> import functools
+            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
+            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
+    
+            # Solve by RRWM. Note that X is normalized with a sum of 1
+            >>> X = pygm.rrwm(K, n1, n2, beta=100)
+            >>> X.sum(dim=(1, 2))
+            tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
+                    1.0000])
+    
+            # Accuracy
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum()
+            tensor(1.)
+    
+            # This solver supports gradient back-propogation
+            >>> K = K.requires_grad_(True)
+            >>> pygm.rrwm(K, n1, n2, beta=100).sum().backward()
+            >>> len(torch.nonzero(K.grad))
+            272
+
+    .. dropdown:: Paddle Example
+
+        ::
+
+            >>> import paddle
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'paddle'
+            >>> _ = paddle.seed(1)
+
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> X_gt = paddle.zeros((batch_size, 4, 4))
+            >>> X_gt[:, paddle.arange(0, 4, dtype=paddle.int64), paddle.randperm(4)] = 1
+            >>> A1 = paddle.rand((batch_size, 4, 4))
+            >>> A2 = paddle.bmm(paddle.bmm(X_gt.transpose((0, 2, 1)), A1), X_gt)
+            >>> n1 = n2 = paddle.to_tensor([4] * batch_size)
+
+            # Build affinity matrix
+            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
+            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
+            >>> import functools
+            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
+            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
+
+            # Solve by RRWM. Note that X is normalized with a sum of 1
+            >>> X = pygm.rrwm(K, n1, n2, beta=100)
+            >>> X.sum(axis=(1, 2))
+            Tensor(shape=[10], dtype=float32, place=Place(cpu), stop_gradient=True,
+                    [0.99999988, 0.99999988, 0.99999994, 0.99999994, 1.        , 
+                     1.        , 1.        , 1.00000012, 1.00000012, 1.        ])
+
+            # Accuracy
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum()
+            Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True, [1.])
+
+            # This solver supports gradient back-propogation
+            >>> K.stop_gradient = False
+            >>> pygm.rrwm(K, n1, n2, beta=100).sum().backward()
+            >>> len(paddle.nonzero(K.grad))
+            544
+
+    .. dropdown:: Jittor Example
+
+        ::
+
+            >>> import jittor as jt
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'jittor'
+            >>> _ = jt.seed(1)
+
+
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> X_gt = jt.zeros((batch_size, 4, 4))
+            >>> X_gt[:, jt.arange(0, 4, dtype=jt.int64), jt.randperm(4)] = 1
+            >>> A1 = jt.rand(batch_size, 4, 4)
+            >>> A2 = jt.bmm(jt.bmm(X_gt.transpose(1, 2), A1), X_gt)
+            >>> n1 = n2 = jt.Var([4] * batch_size)
+
+            # Build affinity matrix
+            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
+            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
+            >>> import functools
+            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
+            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
+
+            # Solve by RRWM. Note that X is normalized with a sum of 1
+            >>> X = pygm.rrwm(K, n1, n2, beta=100)
+            >>> X.sum(dims=(1, 2))
+            jt.Var([1.         1.0000001  1.         0.99999976 1.         
+                    1.         1.         1.0000001  0.99999994 1.        ], dtype=float32)
+
+            # Accuracy
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum()
+            jt.Var([1.], dtype=float32)
+            
+            # This solver supports gradient back-propogation
+            >>> from jittor import nn
+            >>> class Model(nn.Module):
+            ...     def __init__(self, K):
+            ...         self.K = K
+            ...     def execute(self, K, n1, n2, beta):
+            ...         X = pygm.rrwm(K, n1, n2, beta=beta)
+            ...         return X
+
+            >>> model = Model(K)
+            >>> optim = nn.SGD(model.parameters(), lr=0.1)
+            >>> X = model(K, n1, n2, beta=100)
+            >>> loss = X.sum()
+            >>> optim.step(loss)
+            >>> len(jt.nonzero(K.opt_grad(optim)))
+            1536
+
+    .. dropdown:: MindSpore Example
+
+        ::
+
+            >>> import mindspore
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'mindspore'
+            >>> _ = mindspore.set_seed(1)
+            >>> mindspore.set_context(mode=mindspore.PYNATIVE_MODE)
+
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> X_gt = mindspore.numpy.zeros((batch_size, 4, 4))
+            >>> X_gt[:, mindspore.numpy.arange(0, 4, dtype=mindspore.int64), mindspore.ops.Randperm(4)(mindspore.Tensor([4], dtype=mindspore.int32))] = 1
+            >>> A1 = mindspore.numpy.rand((batch_size, 4, 4))
+            >>> A2 = mindspore.ops.BatchMatMul()(mindspore.ops.BatchMatMul()(X_gt.swapaxes(1, 2), A1), X_gt)
+            >>> n1 = n2 = mindspore.Tensor([4] * batch_size)
+
+            # Build affinity matrix
+            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
+            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
+            >>> import functools
+            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
+            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
+
+            # Solve by RRWM. Note that X is normalized with a sum of 1
+            >>> X = pygm.rrwm(K, n1, n2, beta=100)
+            >>> X.sum(axis=(1, 2))
+            [1.         0.99999994 0.99999994 1.         1.0000002  1.
+            1.         1.         1.0000001  1.0000001 ]
+
+            # Accuracy
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum()
+            1.0
+
+            # This solver supports gradient back-propogation
+            >>> def fn(K, n1, n2, beta):
+            >>>     X = pygm.rrwm(K, n1, n2, beta=beta)
+            >>>     X_gt = mindspore.numpy.zeros((batch_size, 4, 4))
+            >>>     X_gt[:, mindspore.numpy.arange(0, 4, dtype=mindspore.int64),mindspore.ops.Randperm(4)(mindspore.Tensor([4], dtype=mindspore.int32))] = 1
+            >>>     res = pygm.utils.permutation_loss(X, X_gt)
+            >>>     return res
+
+            >>> g = mindspore.ops.grad(fn)(K, n1, n2, beta=100)
+            >>> mindspore.ops.count_nonzero(g)
+            2560
+
+    .. dropdown:: Tensorflow Example
+
+        ::
+
+            >>> import tensorflow as tf
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'tensorflow'
+            >>> _ = tf.random.set_seed(1)
+
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> X_gt = tf.Variable(tf.zeros([batch_size, 4, 4]))
+            >>> indices = tf.stack([tf.range(4),tf.random.shuffle(tf.range(4))], axis=1)
+            >>> updates = tf.ones([4])
+            >>> for i in range(batch_size):
+            ...     _ = X_gt[i].assign(tf.tensor_scatter_nd_update(X_gt[i], indices, updates))
+            >>> A1 = tf.random.uniform([batch_size, 4, 4])
+            >>> A2 = tf.matmul(tf.matmul(tf.transpose(X_gt, perm=[0, 2, 1]), A1), X_gt)
+            >>> n1 = n2 = tf.constant([4] * batch_size)
+
+            # Build affinity matrix
+            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
+            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
+            >>> import functools
+            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
+            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
+
+            # Solve by RRWM. Note that X is normalized with a sum of 1
+            >>> X = pygm.rrwm(K, n1, n2, beta=100)
+            >>> tf.reduce_sum(X, axis=[1, 2])
+            <tf.Tensor: shape=(10,), dtype=float32, numpy=
+            array([1.        , 1.        , 1.        , 0.99999994, 1.        ,
+                   1.        , 1.        , 0.99999994, 0.99999994, 1.0000001 ],
+                  dtype=float32)>
+
+            # Accuracy
+            >>> tf.reduce_sum((pygm.hungarian(X) * X_gt)) / tf.reduce_sum(X_gt)
+            <tf.Tensor: shape=(), dtype=float32, numpy=1.0>
+
+            # This solver supports gradient back-propogation
+            >>> K = tf.Variable(K)
+            >>> with tf.GradientTape(persistent=True) as tape:
+            ...     y = tf.reduce_sum(pygm.rrwm(K, n1, n2, beta=100))
+            ...     len(tf.where(tape.gradient(y, K)))
+            768
+
+    .. note::
+        If you find this graph matching solver useful in your research, please cite:
+
+        ::
+
+            @inproceedings{rrwm,
+              title={Reweighted random walks for graph matching},
+              author={Cho, Minsu and Lee, Jungmin and Lee, Kyoung Mu},
+              booktitle={European conference on Computer vision},
+              pages={492--505},
+              year={2010},
+              organization={Springer}
+            }
+    """
+    if backend is None:
+        backend = pygmtools.BACKEND
+    _check_data_type(K, 'K', backend)
+    if _check_shape(K, 2, backend):
+        K = _unsqueeze(K, 0, backend)
+        non_batched_input = True
+        if type(n1) is int and n1max is None:
+            n1max = n1
+            n1 = None
+        if type(n2) is int and n2max is None:
+            n2max = n2
+            n2 = None
+    elif _check_shape(K, 3, backend):
+        non_batched_input = False
+    else:
+        raise ValueError(f'the input argument K is expected to be 2-dimensional or 3-dimensional, got '
+                         f'K:{len(_get_shape(K, backend))}dims!')
+    __check_gm_arguments(n1, n2, n1max, n2max)
+    assert 0 <= alpha <= 1, f'illegal value of alpha, it should lie between 0 and 1, got alpha={alpha}!.'
+    assert beta > 0, f'illegal value of beta, it should be larger than 0, got beta={beta}!'
+
+    args = (K, n1, n2, n1max, n2max, x0, max_iter, sk_iter, alpha, beta)
+    try:
+        mod = importlib.import_module(f'pygmtools.{backend}_backend')
+        fn = mod.rrwm
+    except (ModuleNotFoundError, AttributeError):
+        raise NotImplementedError(
+            NOT_IMPLEMENTED_MSG.format(backend)
+        )
+    result = fn(*args)
+    if non_batched_input:
+        return _squeeze(result, 0, backend)
+    else:
+        return result
+
+
+def ipfp(K, n1=None, n2=None, n1max=None, n2max=None, x0=None,
+         max_iter: int=50,
+         backend=None):
+    r"""
+    Integer Projected Fixed Point (IPFP) method for graph matching (Lawler's QAP).
+
+    :param K: :math:`(b\times n_1n_2 \times n_1n_2)` the input affinity matrix, :math:`b`: batch size.
+    :param n1: :math:`(b)` number of nodes in graph1 (optional if n1max is given, and all n1=n1max).
+    :param n2: :math:`(b)` number of nodes in graph2 (optional if n2max is given, and all n2=n2max).
+    :param n1max: :math:`(b)` max number of nodes in graph1 (optional if n1 is given, and n1max=max(n1)).
+    :param n2max: :math:`(b)` max number of nodes in graph2 (optional if n2 is given, and n2max=max(n2)).
+    :param x0: :math:`(b\times n_1 \times n_2)` an initial matching solution for warm-start.
+               If not given, x0 will filled with :math:`\frac{1}{n_1 n_2})`.
+    :param max_iter: (default: 50) max number of iterations in IPFP.
+                     More iterations will be lead to more accurate result, at the cost of increased inference time.
+    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
+    :return: :math:`(b\times n_1 \times n_2)` the solved matching matrix
+
+    .. note::
+        Either ``n1`` or ``n1max`` should be specified because it cannot be inferred from the input tensor size.
+        Same for ``n2`` or ``n2max``.
+
+    .. note::
+        We support batched instances with different number of nodes, therefore ``n1`` and ``n2`` are
+        required to specify the exact number of objects of each dimension in the batch. If not specified, we assume
+        the batched matrices are not padded and all elements in ``n1`` are equal, all in ``n2`` are equal.
+
+    .. note::
+        This function also supports non-batched input, by ignoring all batch dimensions in the input tensors.
+
+    .. note::
+        This solver is non-differentiable. The output is a discrete matching matrix (i.e. permutation matrix).
+
+    .. dropdown:: Numpy Example
+
+        ::
+
+            >>> import numpy as np
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'numpy'
+            >>> np.random.seed(1)
+    
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> X_gt = np.zeros((batch_size, 4, 4))
+            >>> X_gt[:, np.arange(0, 4, dtype=np.int64), np.random.permutation(4)] = 1
+            >>> A1 = np.random.rand(batch_size, 4, 4)
+            >>> A2 = np.matmul(np.matmul(X_gt.transpose((0, 2, 1)), A1), X_gt)
+            >>> n1 = n2 = np.repeat([4], batch_size)
+    
+            # Build affinity matrix
+            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
+            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
+            >>> import functools
+            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
+            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
+    
+            # Solve by IPFP
+            >>> X = pygm.ipfp(K, n1, n2)
+            >>> X[0]
+            array([[0., 0., 0., 1.],
+                   [0., 0., 1., 0.],
+                   [1., 0., 0., 0.],
+                   [0., 1., 0., 0.]])
+    
+            # Accuracy
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum()
+            1.0
+
+    .. dropdown:: Pytorch Example
+
+        ::
+
+            >>> import torch
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'pytorch'
+            >>> _ = torch.manual_seed(1)
+    
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> X_gt = torch.zeros(batch_size, 4, 4)
+            >>> X_gt[:, torch.arange(0, 4, dtype=torch.int64), torch.randperm(4)] = 1
+            >>> A1 = torch.rand(batch_size, 4, 4)
+            >>> A2 = torch.bmm(torch.bmm(X_gt.transpose(1, 2), A1), X_gt)
+            >>> n1 = torch.tensor([4] * batch_size)
+            >>> n2 = torch.tensor([4] * batch_size)
+    
+            # Build affinity matrix
+            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
+            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
+            >>> import functools
+            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
+            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
+    
+            # Solve by IPFP
+            >>> X = pygm.ipfp(K, n1, n2)
+            >>> X[0]
+            tensor([[0., 1., 0., 0.],
+                    [0., 0., 0., 1.],
+                    [0., 0., 1., 0.],
+                    [1., 0., 0., 0.]])
+    
+            # Accuracy
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum()
+            tensor(1.)
+
+    .. dropdown:: Paddle Example
+
+        ::
+
+            >>> import paddle
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'paddle'
+            >>> _ = paddle.seed(1)
+
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> X_gt = paddle.zeros((batch_size, 4, 4))
+            >>> X_gt[:, paddle.arange(0, 4, dtype=paddle.int64), paddle.randperm(4)] = 1
+            >>> A1 = paddle.rand((batch_size, 4, 4))
+            >>> A2 = paddle.bmm(paddle.bmm(X_gt.transpose((0, 2, 1)), A1), X_gt)
+            >>> n1 = paddle.to_tensor([4] * batch_size)
+            >>> n2 = paddle.to_tensor([4] * batch_size)
+
+            # Build affinity matrix
+            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
+            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
+            >>> import functools
+            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
+            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
+
+            # Solve by IPFP
+            >>> X = pygm.ipfp(K, n1, n2)
+            >>> X[0]
+            Tensor(shape=[4, 4], dtype=float32, place=Place(cpu), stop_gradient=True,
+                   [[0., 1., 0., 0.],
+                    [0., 0., 0., 1.],
+                    [0., 0., 1., 0.],
+                    [1., 0., 0., 0.]])
+
+            # Accuracy
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum()
+            Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True, [1.])
+
+    .. dropdown:: Jittor Example
+
+        ::
+
+            >>> import jittor as jt
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'jittor'
+            >>> _ = jt.seed(1)
+
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> X_gt = jt.zeros((batch_size, 4, 4))
+            >>> X_gt[:, jt.arange(0, 4, dtype=jt.int64), jt.randperm(4)] = 1
+            >>> A1 = jt.rand(batch_size, 4, 4)
+            >>> A2 = jt.bmm(jt.bmm(X_gt.transpose(1, 2), A1), X_gt)
+            >>> n1 = jt.Var([4] * batch_size)
+            >>> n2 = jt.Var([4] * batch_size)
+
+            # Build affinity matrix
+            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
+            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
+            >>> import functools
+            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
+            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
+
+            # Solve by IPFP
+            >>> X = pygm.ipfp(K, n1, n2)
+            >>> X[0]
+            jt.Var([[1. 0. 0. 0.]
+                    [0. 0. 1. 0.]
+                    [0. 0. 0. 1.]
+                    [0. 1. 0. 0.]], dtype=float32)
+
+            # Accuracy
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum()
+            jt.Var([1.], dtype=float32)
+
+    .. dropdown:: MindSpore Example
+
+        ::
+
+            >>> import mindspore
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'mindspore'
+            >>> _ = mindspore.set_seed(1)
+
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> X_gt = mindspore.numpy.zeros((batch_size, 4, 4))
+            >>> X_gt[:, mindspore.numpy.arange(0, 4, dtype=mindspore.int64), mindspore.ops.Randperm(4)(mindspore.Tensor([4], dtype=mindspore.int32))] = 1
+            >>> A1 = mindspore.numpy.rand((batch_size, 4, 4))
+            >>> A2 = mindspore.ops.BatchMatMul()(mindspore.ops.BatchMatMul()(X_gt.swapaxes(1, 2), A1), X_gt)
+            >>> n1 = mindspore.Tensor([4] * batch_size)
+            >>> n2 = mindspore.Tensor([4] * batch_size)
+
+            # Build affinity matrix
+            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
+            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
+            >>> import functools
+            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
+            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
+
+            # Solve by IPFP
+            >>> X = pygm.ipfp(K, n1, n2)
+            >>> X[0]
+            [[1. 0. 0. 0.]
+             [0. 0. 0. 1.]
+             [0. 0. 1. 0.]
+             [0. 1. 0. 0.]]
+
+            # Accuracy
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum()
+            1.0
+
+    .. dropdown:: Tensorflow Example
+
+        ::
+
+            >>> import tensorflow as tf
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'tensorflow'
+            >>> _ = tf.random.set_seed(1)
+
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> X_gt = tf.Variable(tf.zeros([batch_size, 4, 4]))
+            >>> indices = tf.stack([tf.range(4),tf.random.shuffle(tf.range(4))], axis=1)
+            >>> updates = tf.ones([4])
+            >>> for i in range(batch_size):
+            ...     _ = X_gt[i].assign(tf.tensor_scatter_nd_update(X_gt[i], indices, updates))
+            >>> A1 = tf.random.uniform([batch_size, 4, 4])
+            >>> A2 = tf.matmul(tf.matmul(tf.transpose(X_gt, perm=[0, 2, 1]), A1), X_gt)
+            >>> n1 = n2 = tf.constant([4] * batch_size)
+
+            # Build affinity matrix
+            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
+            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
+            >>> import functools
+            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
+            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
+
+            # Solve by IPFP
+            >>> X = pygm.ipfp(K, n1, n2)
+            >>> X[0]
+            <tf.Tensor: shape=(4, 4), dtype=float32, numpy=
+            array([[0., 0., 1., 0.],
+                   [0., 1., 0., 0.],
+                   [0., 0., 0., 1.],
+                   [1., 0., 0., 0.]], dtype=float32)>
+
+            # Accuracy
+            >>> tf.reduce_sum((pygm.hungarian(X) * X_gt)) / tf.reduce_sum(X_gt)
+            <tf.Tensor: shape=(), dtype=float32, numpy=1.0>
+
+    .. note::
+        If you find this graph matching solver useful in your research, please cite:
+
+        ::
+
+            @article{ipfp,
+              title={An integer projected fixed point method for graph matching and map inference},
+              author={Leordeanu, Marius and Hebert, Martial and Sukthankar, Rahul},
+              journal={Advances in neural information processing systems},
+              volume={22},
+              year={2009}
+            }
+    """
+    if backend is None:
+        backend = pygmtools.BACKEND
+    _check_data_type(K, 'K', backend)
+    if _check_shape(K, 2, backend):
+        K = _unsqueeze(K, 0, backend)
+        non_batched_input = True
+        if type(n1) is int and n1max is None:
+            n1max = n1
+            n1 = None
+        if type(n2) is int and n2max is None:
+            n2max = n2
+            n2 = None
+    elif _check_shape(K, 3, backend):
+        non_batched_input = False
+    else:
+        raise ValueError(f'the input argument K is expected to be 2-dimensional or 3-dimensional, got '
+                         f'K:{len(_get_shape(K, backend))}dims!')
+    __check_gm_arguments(n1, n2, n1max, n2max)
+
+    args = (K, n1, n2, n1max, n2max, x0, max_iter)
+    try:
+        mod = importlib.import_module(f'pygmtools.{backend}_backend')
+        fn = mod.ipfp
+    except (ModuleNotFoundError, AttributeError):
+        raise NotImplementedError(
+            NOT_IMPLEMENTED_MSG.format(backend)
+        )
+    result = fn(*args)
+    if non_batched_input:
+        return _squeeze(result, 0, backend)
+    else:
+        return result
+
+
+def a_star(feat1, feat2, A1, A2, n1=None, n2=None,network=None, 
+           return_network=False, pretrain='AIDS700nef',backend=None,**kwargs):
+
+    r"""
+    GENN-A* solver for graph matching  based on Graph Neural Network.
+    
+    :param feat1: :math:`(b\times n_1 \times d)` input feature of graph1
+    :param feat2: :math:`(b\times n_2 \times d)` input feature of graph2
+    :param A1: :math:`(b\times n_1 \times n_1)` input adjacency matrix of graph1
+    :param A2: :math:`(b\times n_2 \times n_2)` input adjacency matrix of graph2
+    :param n1: :math:`(b)` number of nodes in graph1. Optional if all equal to :math:`n_1`
+    :param n2: :math:`(b)` number of nodes in graph2. Optional if all equal to :math:`n_2`
+    :param return_network: (default: False) Return the network object (saving model construction time if calling the
+        model multiple times).
+    :param pretrain: (default: 'AIDS700nef') If ``network==None``, the pretrained model weights to be loaded. Available
+        pretrained weights: ``AIDS700nef`` (feature_num=36), ``LINUX`` (feature_num=8),
+        or ``False`` (no pretraining).
+    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
+    :param feature_num: (default: 36) The feature dimension of the node, if feat1 is not entered.
+        defaults to 36. If feat1 is given, it will be obtained by feat1. If specified by the user,
+        it needs to be determined whether the feature dimension of feat1 is equal to the user input.
+    :param filters_1: (default: 64) Filters (neurons) in 1st convolution.
+    :param filters_2: (default: 32) Filters (neurons) in 2nd convolution.
+    :param filters_3: (default: 16) Filters (neurons) in 2nd convolution.
+    :param tensor_neurons: (default: 16) Neurons in tensor network layer.
+    :param bottle_neck_neurons: (default: 16) Bottle neck layer neurons.
+    :param bins: (default: 16) Similarity score bins.
+    :param dropout: (default: 0) Dropout probability
+    :param histogram: (default: False) 
+    :param diffpool: (default: False) 
+    :param use_net: (default: True) Whether to use neural networks to obtain heuristic prediction
+    :return: if ``return_network==False``, :math:`(b\times n_1 \times n_2)` the doubly-stochastic matching matrix
+
+        if ``return_network==True``, :math:`(b\times n_1 \times n_2)` the doubly-stochastic matching matrix,
+        the network object
+
+    .. note::
+        You may need a proxy to load the pretrained weights if Google drive is not accessible in your contry/region.
+        You may also download the pretrained models manually and put them at ``~/.cache/pygmtools`` (for Linux).
+
+        `[google drive] <https://drive.google.com/drive/folders/1mUpwHeW1RbMHaNxX_PZvD5HrWvyCQG8y>`_
+
+    .. note::
+        This function also supports non-batched input, by ignoring all batch dimensions in the input tensors.
+
+    .. dropdown:: PyTorch Example
+
+        ::
+
+            >>> import torch
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'pytorch'
+            >>> _ = torch.manual_seed(1)
+            
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> nodes_num = 4
+            >>> feature_num = 36
+
+            >>> X_gt = torch.zeros(batch_size, nodes_num, nodes_num)
+            >>> X_gt[:, torch.arange(0, nodes_num, dtype=torch.int64), torch.randperm(nodes_num)] = 1
+            >>> A1 = 1. * (torch.rand(batch_size, nodes_num, nodes_num) > 0.5)
+            >>> torch.diagonal(A1, dim1=1, dim2=2)[:] = 0 # discard self-loop edges
+            >>> A2 = torch.bmm(torch.bmm(X_gt.transpose(1, 2), A1), X_gt)
+            >>> feat1 = torch.rand(batch_size, nodes_num, feature_num) - 0.5
+            >>> feat2 = torch.bmm(X_gt.transpose(1, 2), feat1)
+            >>> n1 = n2 = torch.tensor([nodes_num] * batch_size)
+
+            # Match by A_STAR (load pretrained model)
+            >>> X, net = pygm.a_star(feat1, feat2, A1, A2, n1, n2, return_network=True)
+            Downloading to ~/.cache/pygmtools/best_genn_AIDS700nef_gcn_astar.pt...
+            >>> (X * X_gt).sum() / X_gt.sum()# accuracy
+            tensor(1.)
+
+            # Pass the net object to avoid rebuilding the model agian
+            >>> X = pygm.a_star(feat1, feat2, A1, A2, n1, n2, network=net)
+            
+            # This function also supports non-batched input, by ignoring all batch dimensions in the input tensors.
+            >>> part_f1 = feat1[0]
+            >>> part_f2 = feat2[0]
+            >>> part_A1 = A1[0]
+            >>> part_A2 = A2[0]
+            >>> part_X_gt = X_gt[0]
+            >>> part_X = pygm.a_star(part_f1, part_f2, part_A1, part_A2, return_network=False)
+            
+            >>> part_X.shape
+            torch.Size([4, 4])
+            
+            >>> (part_X * part_X_gt).sum() / part_X_gt.sum()# accuracy
+            tensor(1.)
+            
+            # You can also use traditional heuristic methods to solve without using neural networks
+            >>> X = pygm.a_star(feat1, feat2, A1, A2, n1, n2, use_net=False)
+            >>> (X * X_gt).sum() / X_gt.sum()# accuracy
+            tensor(1.)            
+            
+            # You may also load other pretrained weights
+            # However, it should be noted that each pretrained set supports different node feature dimensions
+            # AIDS700nef(Default): feature_num = 36
+            # LINUX: feature_num = 8
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> nodes_num = 4
+            >>> feature_num = 8
+
+            >>> X_gt = torch.zeros(batch_size, nodes_num, nodes_num)
+            >>> X_gt[:, torch.arange(0, nodes_num, dtype=torch.int64), torch.randperm(nodes_num)] = 1
+            >>> A1 = 1. * (torch.rand(batch_size, nodes_num, nodes_num) > 0.5)
+            >>> torch.diagonal(A1, dim1=1, dim2=2)[:] = 0 # discard self-loop edges
+            >>> A2 = torch.bmm(torch.bmm(X_gt.transpose(1, 2), A1), X_gt)
+            >>> feat1 = torch.rand(batch_size, nodes_num, feature_num) - 0.5
+            >>> feat2 = torch.bmm(X_gt.transpose(1, 2), feat1)
+            >>> n1 = n2 = torch.tensor([nodes_num] * batch_size)
+            
+            >>> X, net = pygm.a_star(feat1, feat2, A1, A2, n1, n2, pretrain='LINUX',return_network=True)
+            Downloading to ~/.cache/pygmtools/best_genn_LINUX_gcn_astar.pt...
+
+            >>> (X * X_gt).sum() / X_gt.sum()# accuracy
+            tensor(1.)
+            
+            # When the input node feature dimension is different from the one supported by pre training, 
+            # you can still use the solver, but the solver will provide a warning
+            >>> X, net = pygm.a_star(feat1, feat2, A1, A2, n1, n2, return_network=True, pretrain='AIDS700nef')
+            UserWarning: Pretrain AIDS700nef does not support the feature_num = 8 you entered
+            
+            # You may configure your own model and integrate the model into a deep learning pipeline. For example:
+            >>> net = pygm.utils.get_network(pygm.a_star, feature_num = 1000, filters_1 = 1024,filters_2 = 256,filters_3 = 128,pretrain=False)
+            >>> optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
+            # feat1/feat2 may be outputs by other neural networks
+            >>> X = pygm.a_star(feat1, feat2, A1, A2, n1, n2, network=net)
+            >>> loss = pygm.utils.permutation_loss(X, X_gt)
+            >>> loss.backward()
+            >>> optimizer.step()
+    """
+    
+    if backend is None:
+        backend = pygmtools.BACKEND
+    non_batched_input = False
+    if feat1 is not None: # if feat1 is None, this function skips the forward pass and only returns a network object
+        for _ in (feat1, feat2, A1, A2):
+            _check_data_type(_, backend)
+
+        if all([_check_shape(_, 2, backend) for _ in (feat1, feat2, A1, A2)]):
+            feat1, feat2, A1, A2 = [_unsqueeze(_, 0, backend) for _ in (feat1, feat2, A1, A2)]
+            if type(n1) is int: n1 = from_numpy(np.array([n1]), backend=backend)
+            if type(n2) is int: n2 = from_numpy(np.array([n2]), backend=backend)
+            non_batched_input = True
+        elif all([_check_shape(_, 3, backend) for _ in (feat1, feat2, A1, A2)]):
+            non_batched_input = False
+        else:
+            raise ValueError(
+                f'the input arguments feat1, feat2, A1, A2 are expected to be all 2-dimensional or 3-dimensional, got '
+                f'feat1:{len(_get_shape(feat1, backend))}dims, feat2:{len(_get_shape(feat2, backend))}dims, '
+                f'A1:{len(_get_shape(A1, backend))}dims, A2:{len(_get_shape(A2, backend))}dims!')
+
+        if not (_get_shape(feat1, backend)[0] == _get_shape(feat2, backend)[0] == _get_shape(A1, backend)[0] == _get_shape(A2, backend)[0])\
+                or not (_get_shape(feat1, backend)[1] == _get_shape(A1, backend)[1] == _get_shape(A1, backend)[2])\
+                or not (_get_shape(feat2, backend)[1] == _get_shape(A2, backend)[1] == _get_shape(A2, backend)[2])\
+                or not (_get_shape(feat1, backend)[2] == _get_shape(feat2, backend)[2]):
+            raise ValueError(
+                f'the input dimensions do not match. Got feat1:{_get_shape(feat1, backend)}, '
+                f'feat2:{_get_shape(feat2, backend)}, A1:{_get_shape(A1, backend)}, A2:{_get_shape(A2, backend)}!')
+    if n1 is not None: _check_data_type(n1, 'n1', backend)
+    if n2 is not None: _check_data_type(n2, 'n2', backend)
+
+    args = (feat1, feat2, A1, A2, n1, n2, network, pretrain)
+    try:
+        mod = importlib.import_module(f'pygmtools.{backend}_backend')
+        fn = mod.a_star
+    except (ModuleNotFoundError, AttributeError):
+        raise NotImplementedError(
+            NOT_IMPLEMENTED_MSG.format(backend)
+        )
+
+    result = fn(*args,**kwargs)
+    match_mat = _squeeze(result[0], 0, backend) if non_batched_input else result[0]
+    if return_network:
+        return match_mat, result[1]
+    else:
+        return match_mat
+
+
+def __check_gm_arguments(n1, n2, n1max, n2max):
+    if n1 is None and n1max is None:
+        raise ValueError('at least one of the following arguments are required: n1 and n1max.')
+    if n2 is None and n2max is None:
+        raise ValueError('at least one of the following arguments are required: n2 and n2max.')
```

### Comparing `pygmtools-0.3.8/pygmtools/dataset.py` & `pygmtools-0.3.8a0/pygmtools/dataset.py`

 * *Ordering differences only*

 * *Files 24% similar despite different names*

```diff
@@ -1,1362 +1,1362 @@
-r"""
-The implementations of data loading and data processing.
-"""
-
-# Copyright (c) 2022 Thinklab@SJTU
-# pygmtools is licensed under Mulan PSL v2.
-# You can use this software according to the terms and conditions of the Mulan PSL v2.
-# You may obtain a copy of Mulan PSL v2 at:
-# http://license.coscl.org.cn/MulanPSL2
-# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND,
-# EITHER EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT,
-# MERCHANTABILITY OR FIT FOR A PARTICULAR PURPOSE.
-# See the Mulan PSL v2 for more details.
-
-import requests
-import os
-import zipfile
-import tarfile
-from pygmtools.dataset_config import dataset_cfg
-from pathlib import Path
-from xml.etree.ElementTree import Element
-from PIL import Image
-from tqdm.auto import tqdm
-from time import sleep
-import shutil
-import numpy as np
-import xml.etree.ElementTree as ET
-import pickle
-import json
-import scipy.io as sio
-import glob
-import random
-from pygmtools.utils import download
-
-
-VOC2011_KPT_NAMES = {
-    'cat': ['L_B_Elbow', 'L_B_Paw', 'L_EarBase', 'L_Eye', 'L_F_Elbow',
-            'L_F_Paw', 'Nose', 'R_B_Elbow', 'R_B_Paw', 'R_EarBase', 'R_Eye',
-            'R_F_Elbow', 'R_F_Paw', 'TailBase', 'Throat', 'Withers'],
-    'bottle': ['L_Base', 'L_Neck', 'L_Shoulder', 'L_Top', 'R_Base', 'R_Neck',
-               'R_Shoulder', 'R_Top'],
-    'horse': ['L_B_Elbow', 'L_B_Paw', 'L_EarBase', 'L_Eye', 'L_F_Elbow',
-              'L_F_Paw', 'Nose', 'R_B_Elbow', 'R_B_Paw', 'R_EarBase', 'R_Eye',
-              'R_F_Elbow', 'R_F_Paw', 'TailBase', 'Throat', 'Withers'],
-    'motorbike': ['B_WheelCenter', 'B_WheelEnd', 'ExhaustPipeEnd',
-                  'F_WheelCenter', 'F_WheelEnd', 'HandleCenter', 'L_HandleTip',
-                  'R_HandleTip', 'SeatBase', 'TailLight'],
-    'boat': ['Hull_Back_Bot', 'Hull_Back_Top', 'Hull_Front_Bot',
-             'Hull_Front_Top', 'Hull_Mid_Left_Bot', 'Hull_Mid_Left_Top',
-             'Hull_Mid_Right_Bot', 'Hull_Mid_Right_Top', 'Mast_Top', 'Sail_Left',
-             'Sail_Right'],
-    'tvmonitor': ['B_Bottom_Left', 'B_Bottom_Right', 'B_Top_Left',
-                  'B_Top_Right', 'F_Bottom_Left', 'F_Bottom_Right', 'F_Top_Left',
-                  'F_Top_Right'],
-    'cow': ['L_B_Elbow', 'L_B_Paw', 'L_EarBase', 'L_Eye', 'L_F_Elbow',
-            'L_F_Paw', 'Nose', 'R_B_Elbow', 'R_B_Paw', 'R_EarBase', 'R_Eye',
-            'R_F_Elbow', 'R_F_Paw', 'TailBase', 'Throat', 'Withers'],
-    'chair': ['BackRest_Top_Left', 'BackRest_Top_Right', 'Leg_Left_Back',
-              'Leg_Left_Front', 'Leg_Right_Back', 'Leg_Right_Front',
-              'Seat_Left_Back', 'Seat_Left_Front', 'Seat_Right_Back',
-              'Seat_Right_Front'],
-    'car': ['L_B_RoofTop', 'L_B_WheelCenter', 'L_F_RoofTop', 'L_F_WheelCenter',
-            'L_HeadLight', 'L_SideviewMirror', 'L_TailLight', 'R_B_RoofTop',
-            'R_B_WheelCenter', 'R_F_RoofTop', 'R_F_WheelCenter', 'R_HeadLight',
-            'R_SideviewMirror', 'R_TailLight'],
-    'person': ['B_Head', 'HeadBack', 'L_Ankle', 'L_Ear', 'L_Elbow', 'L_Eye',
-               'L_Foot', 'L_Hip', 'L_Knee', 'L_Shoulder', 'L_Toes', 'L_Wrist', 'Nose',
-               'R_Ankle', 'R_Ear', 'R_Elbow', 'R_Eye', 'R_Foot', 'R_Hip', 'R_Knee',
-               'R_Shoulder', 'R_Toes', 'R_Wrist'],
-    'diningtable': ['Bot_Left_Back', 'Bot_Left_Front', 'Bot_Right_Back',
-                    'Bot_Right_Front', 'Top_Left_Back', 'Top_Left_Front', 'Top_Right_Back',
-                    'Top_Right_Front'],
-    'dog': ['L_B_Elbow', 'L_B_Paw', 'L_EarBase', 'L_Eye', 'L_F_Elbow',
-            'L_F_Paw', 'Nose', 'R_B_Elbow', 'R_B_Paw', 'R_EarBase', 'R_Eye',
-            'R_F_Elbow', 'R_F_Paw', 'TailBase', 'Throat', 'Withers'],
-    'bird': ['Beak_Base', 'Beak_Tip', 'Left_Eye', 'Left_Wing_Base',
-             'Left_Wing_Tip', 'Leg_Center', 'Lower_Neck_Base', 'Right_Eye',
-             'Right_Wing_Base', 'Right_Wing_Tip', 'Tail_Tip', 'Upper_Neck_Base'],
-    'bicycle': ['B_WheelCenter', 'B_WheelEnd', 'B_WheelIntersection',
-                'CranksetCenter', 'F_WheelCenter', 'F_WheelEnd', 'F_WheelIntersection',
-                'HandleCenter', 'L_HandleTip', 'R_HandleTip', 'SeatBase'],
-    'train': ['Base_Back_Left', 'Base_Back_Right', 'Base_Front_Left',
-              'Base_Front_Right', 'Roof_Back_Left', 'Roof_Back_Right',
-              'Roof_Front_Middle'],
-    'sheep': ['L_B_Elbow', 'L_B_Paw', 'L_EarBase', 'L_Eye', 'L_F_Elbow',
-              'L_F_Paw', 'Nose', 'R_B_Elbow', 'R_B_Paw', 'R_EarBase', 'R_Eye',
-              'R_F_Elbow', 'R_F_Paw', 'TailBase', 'Throat', 'Withers'],
-    'aeroplane': ['Bot_Rudder', 'Bot_Rudder_Front', 'L_Stabilizer',
-                  'L_WingTip', 'Left_Engine_Back', 'Left_Engine_Front',
-                  'Left_Wing_Base', 'NoseTip', 'Nose_Bottom', 'Nose_Top',
-                  'R_Stabilizer', 'R_WingTip', 'Right_Engine_Back',
-                  'Right_Engine_Front', 'Right_Wing_Base', 'Top_Rudder'],
-    'sofa': ['Back_Base_Left', 'Back_Base_Right', 'Back_Top_Left',
-             'Back_Top_Right', 'Front_Base_Left', 'Front_Base_Right',
-             'Handle_Front_Left', 'Handle_Front_Right', 'Handle_Left_Junction',
-             'Handle_Right_Junction', 'Left_Junction', 'Right_Junction'],
-    'pottedplant': ['Bottom_Left', 'Bottom_Right', 'Top_Back_Middle',
-                    'Top_Front_Middle', 'Top_Left', 'Top_Right'],
-    'bus': ['L_B_Base', 'L_B_RoofTop', 'L_F_Base', 'L_F_RoofTop', 'R_B_Base',
-            'R_B_RoofTop', 'R_F_Base', 'R_F_RoofTop']
-}
-
-
-class PascalVOC:
-    r"""
-    Download and preprocess **PascalVOC Keypoint** dataset.
-
-    :param sets: str, problem set, ``'train'`` for training set and ``'test'`` for testing set
-    :param obj_resize: tuple, resized image size
-    :param ds_dict: settings of dataset, containing at most 5 params(keys) for PascalVOC:
-
-            * **KPT_ANNO_DIR**: str, directory of keypoint annotations
-
-            * **ROOT_DIR**: str, directory of data
-
-            * **SET_SPLIT**: str, set split path
-
-            * **CLASSES**: list, data classes
-
-            * **CACHE_PATH**: str, directory of data cache
-    """
-    def __init__(self, sets, obj_resize, **ds_dict):
-        KPT_ANNO_DIR = dataset_cfg.PascalVOC.KPT_ANNO_DIR
-        ROOT_DIR = dataset_cfg.PascalVOC.ROOT_DIR
-        SET_SPLIT = dataset_cfg.PascalVOC.SET_SPLIT
-        CLASSES = dataset_cfg.PascalVOC.CLASSES
-        CACHE_PATH = dataset_cfg.CACHE_PATH
-        if len(ds_dict.keys()) > 0:
-            if 'CLASSES' in ds_dict.keys():
-                CLASSES = ds_dict['CLASSES']
-            if 'ROOT_DIR' in ds_dict.keys():
-                ROOT_DIR = ds_dict['ROOT_DIR']
-            if 'SET_SPLIT' in ds_dict.keys():
-                SET_SPLIT = ds_dict['SET_SPLIT']
-            if 'KPT_ANNO_DIR' in ds_dict.keys():
-                KPT_ANNO_DIR = ds_dict['KPT_ANNO_DIR']
-            if 'CACHE_PATH' in ds_dict.keys():
-                CACHE_PATH = ds_dict['CACHE_PATH']
-
-        VOC2011_anno_path = KPT_ANNO_DIR
-        VOC2011_img_path = ROOT_DIR + 'JPEGImages'
-        VOC2011_ori_anno_path = ROOT_DIR + 'Annotations'
-        VOC2011_cache_path = CACHE_PATH
-
-        self.VOC2011_set_path = SET_SPLIT
-        self.dataset_dir = 'data/PascalVOC'
-        if not os.path.exists(ROOT_DIR):
-            assert ROOT_DIR == dataset_cfg.PascalVOC.ROOT_DIR, 'you should not change ROOT_DIR unless the data have been manually downloaded'
-            self.download(url='http://host.robots.ox.ac.uk/pascal/VOC/voc2011/VOCtrainval_25-May-2011.tar',
-                          name='PascalVOC')
-
-        if not os.path.exists(KPT_ANNO_DIR):
-            assert KPT_ANNO_DIR == dataset_cfg.PascalVOC.KPT_ANNO_DIR, 'you should not change KPT_ANNO_DIR unless the annotations have been manually downloaded'
-            self.download(
-                url='https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/shape/poselets/voc2011_keypoints_Feb2012.tgz',
-                name='PascalVOC_anno')
-
-        if not os.path.exists(self.dataset_dir):
-            os.makedirs(self.dataset_dir)
-
-        self.sets = sets
-        self.obj_resize = obj_resize
-        self.suffix = 'pca'
-
-        self.classes = CLASSES
-        self.kpt_len = [len(VOC2011_KPT_NAMES[_]) for _ in self.classes]
-        self.classes_kpts = {cls: len(VOC2011_KPT_NAMES[cls]) for cls in self.classes}
-        self.anno_path = Path(VOC2011_anno_path)
-        self.img_path = Path(VOC2011_img_path)
-        self.ori_anno_path = Path(VOC2011_ori_anno_path)
-
-        assert sets == 'train' or sets == 'test', 'No match found for dataset {}'.format(sets)
-        cache_name = 'voc_db_' + sets + '.pkl'
-        self.cache_path = Path(VOC2011_cache_path)
-        self.cache_file = self.cache_path / cache_name
-        if self.cache_file.exists():
-            with self.cache_file.open(mode='rb') as f:
-                self.xml_list = pickle.load(f)
-            print('xml list loaded from {}'.format(self.cache_file))
-
-        else:
-            if self.sets != 'test':
-                print('Caching xml list to {}...'.format(self.cache_file))
-            self.cache_path.mkdir(exist_ok=True, parents=True)
-            with np.load(self.VOC2011_set_path, allow_pickle=True) as f:
-                self.xml_list = f[sets]
-            before_filter = sum([len(k) for k in self.xml_list])
-            self.__filter_list(self.xml_list)
-            after_filter = sum([len(k) for k in self.xml_list])
-            with self.cache_file.open(mode='wb') as f:
-                pickle.dump(self.xml_list, f)
-            print('Filtered {} images to {}. Annotation saved.'.format(before_filter, after_filter))
-
-        self.process()
-
-    def download(self, url=None, name=None, retries=5):
-        r"""
-        Automatically download PascalVOC dataset.
-
-        :param url: str, web url of PascalVOC and PascalVOC annotation
-        :param name: str, ``"PascalVOC"`` to download PascalVOC and ``"PascalVOC_anno"`` to download PascalVOC annotation
-        """
-        if retries <= 0:
-            raise RuntimeError('Max Retries exceeded!')
-
-        dirs = 'data/'
-        if not os.path.exists(dirs):
-            os.makedirs(dirs)
-
-        if name == "PascalVOC_anno":
-            print('Downloading dataset annotation...')
-            filename = "data/PascalVOC.tgz"
-            download(filename=filename, url=url, to_cache=False)
-            try:
-                tar = tarfile.open(filename, "r")
-            except tarfile.ReadError as err:
-                print('Warning: Content error. Retrying...\n', err)
-                os.remove(filename)
-                return self.download(url, name, retries - 1)
-
-            file_names = tar.getnames()
-            print('Unzipping files...')
-            sleep(0.5)
-            for file_name in tqdm(file_names):
-                tar.extract(file_name, "data/PascalVOC/")
-            tar.close()
-            try:
-                os.remove(filename)
-            except PermissionError:
-                pass
-
-        if name == "PascalVOC":
-            print('Downloading dataset PascalVOC...')
-            filename = "data/PascalVOC.tar"
-            download(filename=filename, url=url, to_cache=False)
-            try:
-                tar = tarfile.open(filename, "r")
-            except tarfile.ReadError as err:
-                print('Warning: Content error. Retrying...\n', err)
-                os.remove(filename)
-                return self.download(url, name, retries - 1)
-
-            file_names = tar.getnames()
-            print('Unzipping files...')
-            sleep(0.5)
-            for file_name in tqdm(file_names):
-                tar.extract(file_name, "data/PascalVOC/")
-            tar.close()
-            try:
-                os.remove(filename)
-            except PermissionError:
-                pass
-        return filename
-
-    def __filter_list(self, a_xml_list):
-        """
-        Filter out ``'truncated'``, ``'occluded'`` and ``'difficult'`` images following the practice of previous works.
-        In addition, this dataset has uncleaned label (in person category). They are omitted as suggested by README.
-        """
-        for cls_id in range(len(self.classes)):
-            to_del = []
-            for xml_name in a_xml_list[cls_id]:
-                xml_comps = xml_name.split('/')[-1].strip('.xml').split('_')
-                ori_xml_name = '_'.join(xml_comps[:-1]) + '.xml'
-                voc_idx = int(xml_comps[-1])
-                xml_file = self.ori_anno_path / ori_xml_name
-                assert xml_file.exists(), '{} does not exist.'.format(xml_file)
-                tree = ET.parse(xml_file.open())
-                root = tree.getroot()
-                obj: Element = root.findall('object')[voc_idx - 1]
-
-                difficult = obj.find('difficult')
-                if difficult is not None:
-                    difficult = int(difficult.text)
-                occluded = obj.find('occluded')
-                if occluded is not None:
-                    occluded = int(occluded.text)
-                truncated = obj.find('truncated')
-                if truncated is not None:
-                    truncated = int(truncated.text)
-                if difficult or occluded or truncated:
-                    to_del.append(xml_name)
-                    continue
-
-                    # Exclude uncleaned images
-                if self.classes[cls_id] == 'person' and int(xml_comps[0]) > 2008:
-                    to_del.append(xml_name)
-                    continue
-
-                    # Exclude overlapping images in Willow
-                    # if self.sets == 'train' and (self.classes[cls_id] == 'motorbike' or self.classes[cls_id] == 'car') \
-                    #        and int(xml_comps[0]) == 2007:
-                    #    to_del.append(xml_name)
-                    #    continue
-
-            for x in to_del:
-                a_xml_list[cls_id].remove(x)
-
-    def process(self):
-        r"""
-        Process the dataset and generate ``data-(size, size).json`` for preprocessed dataset, ``train.json`` for
-        training set, and ``test.json`` for testing set.
-        """
-        train_file = os.path.join(self.dataset_dir, 'train.json')
-        test_file = os.path.join(self.dataset_dir, 'test.json')
-        img_file = os.path.join(self.dataset_dir, 'data-' + str(self.obj_resize) + '-' + self.suffix + '.json')
-        if not (os.path.exists(train_file) and os.path.exists(test_file) and os.path.exists(img_file)):
-            if not (os.path.exists(train_file) and os.path.exists(test_file)):
-                list1 = []
-                for x in range(len(self.xml_list)):
-                    for xml_name in self.xml_list[x]:
-                        tmp = xml_name.split('/')
-                        tmp2 = tmp[1].split('.')
-                        objID = tmp2[0] + '_' + tmp[0]
-                        list1.append(objID)
-
-                list2 = []
-                if self.sets == 'train':
-                    with np.load(self.VOC2011_set_path, allow_pickle=True) as f:
-                        a_list = f['test']
-                    self.__filter_list(a_list)
-                    cache_name = 'voc_db_test.pkl'
-                    cache_file = self.cache_path / cache_name
-                    if not cache_file.exists():
-                        with cache_file.open(mode='wb') as f:
-                            pickle.dump(a_list, f)
-
-                    for x in range(len(a_list)):
-                        for xml_name in a_list[x]:
-                            tmp = xml_name.split('/')
-                            tmp2 = tmp[1].split('.')
-                            objID = tmp2[0] + '_' + tmp[0]
-                            list2.append(objID)
-                    str1 = json.dumps(list1)
-                    f1 = open(train_file, 'w')
-                    f1.write(str1)
-                    f1.close()
-                    str2 = json.dumps(list2)
-                    f2 = open(test_file, 'w')
-                    f2.write(str2)
-                    f2.close()
-                else:
-                    with np.load(self.VOC2011_set_path, allow_pickle=True) as f:
-                        a_list = f['train']
-                    self.__filter_list(a_list)
-                    cache_name = 'voc_db_train.pkl'
-                    cache_file = self.cache_path / cache_name
-                    if not cache_file.exists():
-                        with cache_file.open(mode='wb') as f:
-                            pickle.dump(a_list, f)
-
-                    for x in range(len(a_list)):
-                        for xml_name in a_list[x]:
-                            tmp = xml_name.split('/')
-                            tmp2 = tmp[1].split('.')
-                            objID = tmp2[0] + '_' + tmp[0]
-                            list2.append(objID)
-                    str1 = json.dumps(list1)
-                    f1 = open(test_file, 'w')
-                    f1.write(str1)
-                    f1.close()
-                    str2 = json.dumps(list2)
-                    f2 = open(train_file, 'w')
-                    f2.write(str2)
-                    f2.close()
-            else:
-                if self.sets == 'train':
-                    with np.load(self.VOC2011_set_path, allow_pickle=True) as f:
-                        a_list = f['test']
-                    self.__filter_list(a_list)
-                    cache_name = 'voc_db_test.pkl'
-                    cache_file = self.cache_path / cache_name
-                    if not cache_file.exists():
-                        with cache_file.open(mode='wb') as f:
-                            pickle.dump(a_list, f)
-
-                else:
-                    with np.load(self.VOC2011_set_path, allow_pickle=True) as f:
-                        a_list = f['train']
-                    self.__filter_list(a_list)
-                    cache_name = 'voc_db_train.pkl'
-                    cache_file = self.cache_path / cache_name
-                    if not cache_file.exists():
-                        with cache_file.open(mode='wb') as f:
-                            pickle.dump(a_list, f)
-
-            if not os.path.exists(img_file):
-                data_list = self.xml_list + a_list
-                data_dict = dict()
-                for x in range(len(data_list)):
-                    for xml_name in data_list[x]:
-                        tmp = xml_name.split('/')
-                        tmp2 = tmp[1].split('.')
-                        objID = tmp2[0] + '_' + tmp[0]
-                        annotations = self.__get_anno_dict(xml_name)
-                        data_dict[objID] = annotations
-
-                data_str = json.dumps(data_dict)
-                f3 = open(img_file, 'w')
-                f3.write(data_str)
-                f3.close()
-
-    def __get_anno_dict(self, xml_name):
-        """
-        Get an annotation dict from xml file
-        """
-        xml_file = self.anno_path / xml_name
-        assert xml_file.exists(), '{} does not exist.'.format(xml_file)
-
-        tree = ET.parse(xml_file.open())
-        root = tree.getroot()
-
-        img_name = root.find('./image').text + '.jpg'
-        img_file = self.img_path / img_name
-        bounds = root.find('./visible_bounds').attrib
-        cls = root.find('./category').text
-
-        xmin = float(bounds['xmin'])
-        ymin = float(bounds['ymin'])
-        h = float(bounds['height'])
-        w = float(bounds['width'])
-        xmax = float(xmin) + float(w)
-        ymax = float(ymin) + float(h)
-
-        keypoint_list = []
-        for keypoint in root.findall('./keypoints/keypoint'):
-            attr = keypoint.attrib
-            attr['x'] = (float(attr['x']) - xmin) * self.obj_resize[0] / w
-            attr['y'] = (float(attr['y']) - ymin) * self.obj_resize[1] / h
-            kpts_anno = dict()
-            kpts_anno['labels'] = attr['name']
-            kpts_anno['x'] = attr['x']
-            kpts_anno['y'] = attr['y']
-            keypoint_list.append(kpts_anno)
-
-        anno_dict = dict()
-        anno_dict['kpts'] = keypoint_list
-        anno_dict['path'] = str(img_file)
-        anno_dict['cls'] = cls
-        anno_dict['bounds'] = [xmin, ymin, xmax, ymax]
-        anno_dict['univ_size'] = len(VOC2011_KPT_NAMES[cls])
-
-        return anno_dict
-
-
-class WillowObject:
-    r"""
-        Download and preprocess **Willow Object Class** dataset.
-
-        :param sets: str, problem set, ``'train'`` for training set and ``'test'`` for testing set
-        :param obj_resize: tuple, resized image size
-        :param ds_dict: settings of dataset, containing at most 6 params(keys) for WillowObject:
-
-            * **ROOT_DIR**: str, directory of data
-
-            * **CLASSES**: list, data classes
-
-            * **TRAIN_NUM**: int, number of images for train in each class
-
-            * **SPLIT_OFFSET**: int, offset when split train and testing set
-
-            * **TRAIN_SAME_AS_TEST**: bool, whether to use same images for training and test
-
-            * **RAND_OUTLIER**: int, number of added outliers in one image
-    """
-    def __init__(self, sets, obj_resize, **ds_dict):
-        CLASSES = dataset_cfg.WillowObject.CLASSES
-        KPT_LEN = dataset_cfg.WillowObject.KPT_LEN
-        ROOT_DIR = dataset_cfg.WillowObject.ROOT_DIR
-        TRAIN_NUM = dataset_cfg.WillowObject.TRAIN_NUM
-        SPLIT_OFFSET = dataset_cfg.WillowObject.SPLIT_OFFSET
-        TRAIN_SAME_AS_TEST = dataset_cfg.WillowObject.TRAIN_SAME_AS_TEST
-        RAND_OUTLIER = dataset_cfg.WillowObject.RAND_OUTLIER
-        URL = 'http://www.di.ens.fr/willow/research/graphlearning/WILLOW-ObjectClass_dataset.zip'
-        if len(ds_dict.keys()) > 0:
-            if 'CLASSES' in ds_dict.keys():
-                CLASSES = ds_dict['CLASSES']
-            if 'ROOT_DIR' in ds_dict.keys():
-                ROOT_DIR = ds_dict['ROOT_DIR']
-            if 'TRAIN_NUM' in ds_dict.keys():
-                TRAIN_NUM = ds_dict['TRAIN_NUM']
-            if 'SPLIT_OFFSET' in ds_dict.keys():
-                SPLIT_OFFSET = ds_dict['SPLIT_OFFSET']
-            if 'TRAIN_SAME_AS_TEST' in ds_dict.keys():
-                TRAIN_SAME_AS_TEST = ds_dict['TRAIN_SAME_AS_TEST']
-            if 'RAND_OUTLIER' in ds_dict.keys():
-                RAND_OUTLIER = ds_dict['RAND_OUTLIER']
-            if 'URL' in ds_dict.keys():
-                URL = ds_dict['URL']
-
-        self.dataset_dir = 'data/WillowObject'
-        if not os.path.exists(ROOT_DIR):
-            assert ROOT_DIR == dataset_cfg.WillowObject.ROOT_DIR, 'you should not change ROOT_DIR unless the data have been manually downloaded'
-            self.download(url=URL)
-
-        if not os.path.exists(self.dataset_dir):
-            os.makedirs(self.dataset_dir)
-
-        self.sets = sets
-        self.obj_resize = obj_resize
-        self.suffix = 'willow-' + str(RAND_OUTLIER)
-
-        self.classes = CLASSES
-        self.kpt_len = [KPT_LEN for _ in self.classes]
-
-        self.root_path = Path(ROOT_DIR)
-        self.obj_resize = obj_resize
-
-        assert sets == 'train' or 'test', 'No match found for dataset {}'.format(sets)
-        self.split_offset = SPLIT_OFFSET
-        self.train_len = TRAIN_NUM
-        self.train_same_as_test = TRAIN_SAME_AS_TEST
-        self.rand_outlier = RAND_OUTLIER
-
-        self.mat_list = []
-
-        self.process()
-
-    def download(self, url=None, retries=5):
-        r"""
-         Automatically download WillowObject dataset.
-
-         :param url: str, web url of WillowObject
-         """
-        if retries <= 0:
-            raise RuntimeError('Max Retries exceeded!')
-
-        dirs = 'data/'
-        if not os.path.exists(dirs):
-            os.makedirs(dirs)
-
-        print('Downloading dataset WillowObject...')
-        filename = "data/WILLOW.zip"
-        download(filename=filename, url=url, to_cache=False)
-        try:
-            fz = zipfile.ZipFile(filename, "r")
-        except zipfile.BadZipFile as err:
-            print('Warning: Content error. Retrying...\n', err)
-            os.remove(filename)
-            return self.download(url, retries - 1)
-
-        print('Unzipping files...')
-        sleep(0.5)
-        for file in tqdm(fz.namelist()):
-            fz.extract(file, "data/WillowObject/")
-        try:
-            os.remove(filename)
-        except PermissionError:
-            pass
-        return filename
-
-    def process(self):
-        r"""
-        Process the dataset and generate ``data-(size, size).json`` for preprocessed dataset, ``train.json`` for
-        training set, and ``test.json`` for testing set.
-        """
-        train_file = os.path.join(self.dataset_dir, 'train.json')
-        test_file = os.path.join(self.dataset_dir, 'test.json')
-        img_file = os.path.join(self.dataset_dir, 'data-' + str(self.obj_resize) + '-' + self.suffix + '.json')
-
-        data_list = []
-        mat_list_ = []
-        for cls_name in self.classes:
-            assert type(cls_name) is str
-            cls_mat_list = [p for p in (self.root_path / cls_name).glob('*.mat')]
-            cls_mat_list.sort()
-            if cls_name == 'Face':
-                cls_mat_list.remove(self.root_path / cls_name / 'image_0160.mat')
-                assert not self.root_path / cls_name / 'image_0160.mat' in cls_mat_list
-            ori_len = len(cls_mat_list)
-            assert ori_len > 0, 'No data found for WillowObject Class. Is the dataset installed correctly?'
-            data_list.append(cls_mat_list)
-            if self.split_offset % ori_len + self.train_len <= ori_len:
-                if self.sets == 'train' and not self.train_same_as_test:
-                    self.mat_list.append(
-                        cls_mat_list[self.split_offset % ori_len: (self.split_offset + self.train_len) % ori_len]
-                    )
-
-                    mat_list_.append(
-                        cls_mat_list[:self.split_offset % ori_len] +
-                        cls_mat_list[(self.split_offset + self.train_len) % ori_len:]
-                    )
-                elif self.train_same_as_test:
-                    self.mat_list.append(
-                        cls_mat_list[:self.split_offset % ori_len] +
-                        cls_mat_list[(self.split_offset + self.train_len) % ori_len:]
-                    )
-
-                    mat_list_.append(
-                        cls_mat_list[:self.split_offset % ori_len] +
-                        cls_mat_list[(self.split_offset + self.train_len) % ori_len:]
-                    )
-                else:
-                    self.mat_list.append(
-                        cls_mat_list[:self.split_offset % ori_len] +
-                        cls_mat_list[(self.split_offset + self.train_len) % ori_len:]
-                    )
-
-                    mat_list_.append(
-                        cls_mat_list[self.split_offset % ori_len: (self.split_offset + self.train_len) % ori_len]
-                    )
-            else:
-                if self.sets == 'train' and not self.train_same_as_test:
-                    self.mat_list.append(
-                        cls_mat_list[:(self.split_offset + self.train_len) % ori_len - ori_len] +
-                        cls_mat_list[self.split_offset % ori_len:]
-                    )
-
-                    mat_list_.append(
-                        cls_mat_list[(self.split_offset + self.train_len) % ori_len - ori_len: self.split_offset % ori_len]
-                    )
-                elif self.train_same_as_test:
-                    self.mat_list.append(
-                        cls_mat_list[(self.split_offset + self.train_len) % ori_len - ori_len: self.split_offset % ori_len]
-                    )
-
-                    mat_list_.append(
-                        cls_mat_list[(self.split_offset + self.train_len) % ori_len - ori_len: self.split_offset % ori_len]
-                    )
-                else:
-                    self.mat_list.append(
-                        cls_mat_list[(self.split_offset + self.train_len) % ori_len - ori_len: self.split_offset % ori_len]
-                    )
-
-                    mat_list_.append(
-                        cls_mat_list[:(self.split_offset + self.train_len) % ori_len - ori_len] +
-                        cls_mat_list[self.split_offset % ori_len:]
-                    )
-
-        train_list = []
-        test_list = []
-        if self.sets == 'train':
-            for x in range(len(self.mat_list)):
-                for name in self.mat_list[x]:
-                    tmp = os.path.split(str(name))
-                    objID = tmp[-1].split('.')[0]
-                    train_list.append(objID)
-            for x in range(len(mat_list_)):
-                for name in mat_list_[x]:
-                    tmp = os.path.split(str(name))
-                    objID = tmp[-1].split('.')[0]
-                    test_list.append(objID)
-        else:
-            for x in range(len(self.mat_list)):
-                for name in self.mat_list[x]:
-                    tmp = os.path.split(str(name))
-                    objID = tmp[-1].split('.')[0]
-                    test_list.append(objID)
-            for x in range(len(mat_list_)):
-                for name in mat_list_[x]:
-                    tmp = os.path.split(str(name))
-                    objID = tmp[-1].split('.')[0]
-                    train_list.append(objID)
-        str1 = json.dumps(train_list)
-        f1 = open(train_file, 'w')
-        f1.write(str1)
-        f1.close()
-        str2 = json.dumps(test_list)
-        f2 = open(test_file, 'w')
-        f2.write(str2)
-        f2.close()
-
-        if not os.path.exists(img_file):
-            data_dict = dict()
-
-            for x in range(len(data_list)):
-                for name in data_list[x]:
-                    tmp = os.path.split(str(name))
-                    objID = tmp[-1].split('.')[0]
-                    cls = os.path.split(tmp[0])[-1]
-                    annotations = self.__get_anno_dict(name, cls)
-                    data_dict[objID] = annotations
-
-            data_str = json.dumps(data_dict)
-            f3 = open(img_file, 'w')
-            f3.write(data_str)
-            f3.close()
-
-    def __get_anno_dict(self, mat_file, cls):
-        """
-        Get an annotation dict from .mat annotation
-        """
-        assert mat_file.exists(), '{} does not exist.'.format(mat_file)
-
-        img_name = mat_file.stem + '.png'
-        img_file = mat_file.parent / img_name
-
-        struct = sio.loadmat(mat_file.open('rb'))
-        kpts = struct['pts_coord']
-
-        with Image.open(str(img_file)) as img:
-            ori_sizes = img.size
-            xmin = 0
-            ymin = 0
-            w = ori_sizes[0]
-            h = ori_sizes[1]
-
-        keypoint_list = []
-        for idx, keypoint in enumerate(np.split(kpts, kpts.shape[1], axis=1)):
-            attr = {'labels': idx}
-            attr['x'] = float(keypoint[0]) * self.obj_resize[0] / w
-            attr['y'] = float(keypoint[1]) * self.obj_resize[1] / h
-            keypoint_list.append(attr)
-
-        for idx in range(self.rand_outlier):
-            attr = {
-                'labels': 'outlier',
-                'x': random.uniform(0, self.obj_resize[0]),
-                'y': random.uniform(0, self.obj_resize[1])
-            }
-            keypoint_list.append(attr)
-
-        anno_dict = dict()
-        anno_dict['path'] = str(img_file)
-        anno_dict['kpts'] = keypoint_list
-        anno_dict['bounds'] = [xmin, ymin, w, h]
-        anno_dict['cls'] = cls
-        anno_dict['univ_size'] = 10
-
-        return anno_dict
-
-
-class SPair71k:
-    r"""
-    Download and preprocess **SPair71k** dataset.
-
-    :param sets: str, problem set, ``'train'`` for training set and ``'test'`` for testing set
-    :param obj_resize: tuple, resized image size
-    :param problem: str, problem type, only ``'2GM'`` is supported in SPair71k
-    :param ds_dict: settings of dataset, containing at most 5 params(keys) for SPair71k:
-
-        * **ROOT_DIR**: str, directory of data
-
-        * **TRAIN_DIFF_PARAMS**: list of images that should be dumped in train set
-
-        * **EVAL_DIFF_PARAMS**: list of images that should be dumped in testing set
-
-        * **COMB_CLS**: bool, whether to combine images in different classes
-
-        * **SIZE**: str, ``'large'`` for SPair71k-large and ``'small'`` for SPair71k-small
-    """
-    def __init__(self, sets, obj_resize, problem='2GM', **ds_dict):
-        TRAIN_DIFF_PARAMS = dataset_cfg.SPair.TRAIN_DIFF_PARAMS
-        EVAL_DIFF_PARAMS = dataset_cfg.SPair.EVAL_DIFF_PARAMS
-        COMB_CLS = dataset_cfg.SPair.COMB_CLS
-        SIZE = dataset_cfg.SPair.SIZE
-        ROOT_DIR = dataset_cfg.SPair.ROOT_DIR
-        if len(ds_dict.keys()) > 0:
-            if 'TRAIN_DIFF_PARAMS' in ds_dict.keys():
-                TRAIN_DIFF_PARAMS = ds_dict['TRAIN_DIFF_PARAMS']
-            if 'EVAL_DIFF_PARAMS' in ds_dict.keys():
-                EVAL_DIFF_PARAMS = ds_dict['EVAL_DIFF_PARAMS']
-            if 'COMB_CLS' in ds_dict.keys():
-                COMB_CLS = ds_dict['COMB_CLS']
-            if 'SIZE' in ds_dict.keys():
-                SIZE = ds_dict['SIZE']
-            if 'ROOT_DIR' in ds_dict.keys():
-                ROOT_DIR = ds_dict['ROOT_DIR']
-
-        SPair71k_pair_ann_path = ROOT_DIR + "/PairAnnotation"
-        SPair71k_image_path = ROOT_DIR + "/JPEGImages"
-        SPair71k_image_annotation = ROOT_DIR + "/ImageAnnotation"
-        self.SPair71k_layout_path = ROOT_DIR + "/Layout"
-        self.SPair71k_dataset_size = SIZE
-        self.suffix = 'spair-' + SIZE
-
-        sets_translation_dict = dict(train="trn", test="test")
-        difficulty_params_dict = dict(
-            trn=TRAIN_DIFF_PARAMS, val=EVAL_DIFF_PARAMS, test=EVAL_DIFF_PARAMS
-        )
-
-        assert not problem == 'MGM', 'No match found for problem {} in SPair-71k'.format(problem)
-        self.dataset_dir = 'data/SPair-71k'
-        if not os.path.exists(SPair71k_image_path):
-            assert ROOT_DIR == dataset_cfg.SPair.ROOT_DIR, 'you should not change ROOT_DIR unless the data have been manually downloaded'
-            self.download(url='http://cvlab.postech.ac.kr/research/SPair-71k/data/SPair-71k.tar.gz')
-
-        if not os.path.exists(self.dataset_dir):
-            os.makedirs(self.dataset_dir)
-
-        self.obj_resize = obj_resize
-        self.sets = sets_translation_dict[sets]
-        self.ann_files = open(os.path.join(self.SPair71k_layout_path, self.SPair71k_dataset_size, self.sets + ".txt"), "r").read().split("\n")
-        self.ann_files = self.ann_files[: len(self.ann_files) - 1]
-        self.difficulty_params = difficulty_params_dict[self.sets]
-        self.pair_ann_path = SPair71k_pair_ann_path
-        self.image_path = SPair71k_image_path
-        self.image_annoation = Path(SPair71k_image_annotation)
-        self.classes = list(map(lambda x: os.path.basename(x), glob.glob("%s/*" % SPair71k_image_path)))
-        self.classes.sort()
-        self.combine_classes = COMB_CLS
-        self.ann_files_filtered, self.ann_files_filtered_cls_dict, _ = self.__filter_annotations(
-            self.ann_files, self.difficulty_params
-        )
-        self.total_size = len(self.ann_files_filtered)
-        self.size_by_cls = {cls: len(ann_list) for cls, ann_list in self.ann_files_filtered_cls_dict.items()}
-
-        self.process()
-
-    def download(self, url=None, retries=5):
-        r"""
-         Automatically download SPair71k dataset.
-
-         :param url: str, web url of SPair71k
-         """
-        if retries <= 0:
-            raise RuntimeError('Max Retries exceeded!')
-
-        dirs = 'data/'
-        if not os.path.exists(dirs):
-            os.makedirs(dirs)
-        print('Downloading dataset SPair-71k...')
-        filename = "data/SPair-71k.tgz"
-        download(filename=filename, url=url, to_cache=False)
-        try:
-            tar = tarfile.open(filename, "r")
-        except tarfile.ReadError as err:
-            print('Warning: Content error. Retrying...\n', err)
-            os.remove(filename)
-            return self.download(url, retries - 1)
-
-        file_names = tar.getnames()
-        print('Unzipping files...')
-        sleep(0.5)
-        for file_name in tqdm(file_names):
-            tar.extract(file_name, "data/")
-        tar.close()
-        try:
-            os.remove(filename)
-        except PermissionError:
-            pass
-        return filename
-
-    def process(self):
-        r"""
-        Process the dataset and generate ``data-(size, size).json`` for preprocessed dataset, ``train.json`` for
-        training set, and ``test.json`` for testing set.
-        """
-        train_file = os.path.join(self.dataset_dir, 'train.json')
-        test_file = os.path.join(self.dataset_dir, 'test.json')
-        img_file = os.path.join(self.dataset_dir, 'data-' + str(self.obj_resize) + '-' + self.suffix + '.json')
-        if (not os.path.exists(train_file)) or (not os.path.exists(test_file)):
-            train_list = []
-            test_list = []
-            if self.sets == 'trn':
-                for x in self.ann_files_filtered:
-                    tmp = x.split('-')
-                    tmp2 = tmp[2].split(':')
-                    id1 = tmp[1] + '_' + tmp2[1]
-                    id2 = tmp2[0] + '_' + tmp2[1]
-                    pair_tuple = (id1, id2)
-                    train_list.append(pair_tuple)
-
-                str1 = json.dumps(train_list)
-                f1 = open(train_file, 'w')
-                f1.write(str1)
-                f1.close()
-            else:
-                for x in self.ann_files_filtered:
-                    tmp = x.split('-')
-                    tmp2 = tmp[2].split(':')
-                    id1 = tmp[1] + '_' + tmp2[1]
-                    id2 = tmp2[0] + '_' + tmp2[1]
-                    pair_tuple = (id1, id2)
-                    test_list.append(pair_tuple)
-
-                str2 = json.dumps(test_list)
-                f2 = open(test_file, 'w')
-                f2.write(str2)
-                f2.close()
-
-            data_list = []
-            data_dict = dict()
-            for cls_name in self.classes:
-                cls_json_list = [p for p in (self.image_annoation / cls_name).glob('*.json')]
-                ori_len = len(cls_json_list)
-                assert ori_len > 0, 'No data found for SPair-71k. Is the dataset installed correctly?'
-                data_list.append(cls_json_list)
-
-            for x in range(len(data_list)):
-                for name in data_list[x]:
-                    tmp = os.path.split(str(name))
-                    objID = tmp[-1].split('.')[0]
-                    cls = os.path.split(tmp[0])[-1]
-                    annotations = self.__get_anno_dict(name, cls)
-                    ID = objID + '_' + cls
-                    data_dict[ID] = annotations
-
-            data_str = json.dumps(data_dict)
-            f3 = open(img_file, 'w')
-            f3.write(data_str)
-            f3.close()
-
-    def __get_anno_dict(self, anno_file, cls):
-        assert anno_file.exists(), '{} does not exist.'.format(anno_file)
-
-        img_file = self.image_path + '/' + cls + '/' + anno_file.stem + '.jpg'
-
-        with open(anno_file) as f:
-            annotations = json.load(f)
-
-        h = float(annotations['image_height'])
-        w = float(annotations['image_width'])
-        boundbox = annotations['bndbox']
-
-        keypoint_list = []
-        for key, value in annotations['kps'].items():
-            if not value == None:
-                x = (value[0] - boundbox[0]) * self.obj_resize[0] / (boundbox[2] - boundbox[0])
-                y = (value[1] - boundbox[1]) * self.obj_resize[1] / (boundbox[3] - boundbox[1])
-                kpts_anno = dict()
-                kpts_anno['labels'] = key
-                kpts_anno['x'] = x
-                kpts_anno['y'] = y
-                keypoint_list.append(kpts_anno)
-
-        anno_dict = dict()
-        anno_dict['kpts'] = keypoint_list
-        anno_dict['path'] = img_file
-        anno_dict['cls'] = cls
-        anno_dict['bounds'] = boundbox
-        anno_dict['univ_size'] = len(VOC2011_KPT_NAMES[cls])
-
-        return anno_dict
-
-    def __filter_annotations(self, ann_files, difficulty_params):
-        if len(difficulty_params) > 0:
-            basepath = os.path.join(self.pair_ann_path, "pickled", self.sets)
-            if not os.path.exists(basepath):
-                os.makedirs(basepath)
-            difficulty_paramas_str = self.__diff_dict_to_str(difficulty_params)
-            try:
-                filepath = os.path.join(basepath, difficulty_paramas_str + ".pickle")
-                ann_files_filtered = pickle.load(open(filepath, "rb"))
-                print(
-                    f"Found filtered annotations for difficulty parameters {difficulty_params} and {self.sets}-set at {filepath}"
-                )
-            except (OSError, IOError) as e:
-                print(
-                    f"No pickled annotations found for difficulty parameters {difficulty_params} and {self.sets}-set. Filtering..."
-                )
-                ann_files_filtered_dict = {}
-
-                for ann_file in ann_files:
-                    with open(os.path.join(self.pair_ann_path, self.sets, ann_file + ".json")) as f:
-                        annotation = json.load(f)
-                    diff = {key: annotation[key] for key in self.difficulty_params.keys()}
-                    diff_str = self.__diff_dict_to_str(diff)
-                    if diff_str in ann_files_filtered_dict:
-                        ann_files_filtered_dict[diff_str].append(ann_file)
-                    else:
-                        ann_files_filtered_dict[diff_str] = [ann_file]
-                total_l = 0
-                for diff_str, file_list in ann_files_filtered_dict.items():
-                    total_l += len(file_list)
-                    filepath = os.path.join(basepath, diff_str + ".pickle")
-                    pickle.dump(file_list, open(filepath, "wb"))
-                assert total_l == len(ann_files)
-                print(f"Done filtering. Saved filtered annotations to {basepath}.")
-                ann_files_filtered = ann_files_filtered_dict[difficulty_paramas_str]
-        else:
-            print(f"No difficulty parameters for {self.sets}-set. Using all available data.")
-            ann_files_filtered = ann_files
-
-        ann_files_filtered_cls_dict = {
-            cls: list(filter(lambda x: cls in x, ann_files_filtered)) for cls in self.classes
-        }
-        class_len = {cls: len(ann_list) for cls, ann_list in ann_files_filtered_cls_dict.items()}
-        print(f"Number of annotation pairs matching the difficulty params in {self.sets}-set: {class_len}")
-        if self.combine_classes:
-            cls_name = "combined"
-            ann_files_filtered_cls_dict = {cls_name: ann_files_filtered}
-            filtered_classes = [cls_name]
-            print(f"Combining {self.sets}-set classes. Total of {len(ann_files_filtered)} image pairs used.")
-        else:
-            filtered_classes = []
-            for cls, ann_f in ann_files_filtered_cls_dict.items():
-                if len(ann_f) > 0:
-                    filtered_classes.append(cls)
-                else:
-                    print(f"Excluding class {cls} from {self.sets}-set.")
-        return ann_files_filtered, ann_files_filtered_cls_dict, filtered_classes
-
-    def __diff_dict_to_str(self, diff):
-        diff_str = ""
-        keys = ["mirror", "viewpoint_variation", "scale_variation", "truncation", "occlusion"]
-        for key in keys:
-            if key in diff.keys():
-                diff_str += key
-                diff_str += str(diff[key])
-        return diff_str
-
-
-class IMC_PT_SparseGM:
-    r"""
-    Download and preprocess **IMC_PT_SparseGM** dataset.
-
-    :param sets: str, problem set, ``'train'`` for training set and ``'test'`` for testing set
-    :param obj_resize: tuple, resized image size
-    :param ds_dict: settings of dataset, containing at most 4 params(keys) for IMC_PT_SparseGM:
-
-        * **ROOT_DIR_IMG**: str, directory of images
-
-        * **ROOT_DIR_NPZ**: str, directory of annotations
-
-        * **CLASSES**: dict, classes of training and test data, keys: ``'train'`` for training and ``'test'`` for test
-
-        * **MAX_KPT_NUM**: int, maximum kpt_num in an image
-    """
-    def __init__(self, sets, obj_resize, **ds_dict):
-        assert sets in ('train', 'test'), 'No match found for dataset {}'.format(sets)
-        MAX_KPT_NUM = dataset_cfg.IMC_PT_SparseGM.MAX_KPT_NUM
-        CLASSES = dataset_cfg.IMC_PT_SparseGM.CLASSES
-        ROOT_DIR_NPZ = dataset_cfg.IMC_PT_SparseGM.ROOT_DIR_NPZ
-        ROOT_DIR_IMG = dataset_cfg.IMC_PT_SparseGM.ROOT_DIR_IMG
-        URL = 'https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=1Po9pRMWXTqKK2ABPpVmkcsOq-6K_2v-B'
-        if len(ds_dict.keys()) > 0:
-            if 'MAX_KPT_NUM' in ds_dict.keys():
-                MAX_KPT_NUM = ds_dict['MAX_KPT_NUM']
-            if 'CLASSES' in ds_dict.keys():
-                CLASSES = ds_dict['CLASSES']
-            if 'ROOT_DIR_NPZ' in ds_dict.keys():
-                ROOT_DIR_NPZ = ds_dict['ROOT_DIR_NPZ']
-            if 'ROOT_DIR_IMG' in ds_dict.keys():
-                ROOT_DIR_IMG = ds_dict['ROOT_DIR_IMG']
-            if 'URL' in ds_dict.keys():
-                URL = ds_dict['URL']
-
-        self.dataset_dir = 'data/IMC-PT-SparseGM'
-        if not os.path.exists(ROOT_DIR_IMG):
-            assert ROOT_DIR_IMG == dataset_cfg.IMC_PT_SparseGM.ROOT_DIR_IMG, 'you should not change ROOT_DIR_IMG or ROOT_DIR_NPZ unless the data have been manually downloaded'
-            assert ROOT_DIR_NPZ == dataset_cfg.IMC_PT_SparseGM.ROOT_DIR_NPZ, 'you should not change ROOT_DIR_IMG or ROOT_DIR_NPZ unless the data have been manually downloaded'
-            self.download(url=URL)
-
-        if not os.path.exists(self.dataset_dir):
-            os.makedirs(self.dataset_dir)
-        self.sets = sets
-        self.classes = CLASSES[sets]
-        self.class_dict = CLASSES
-        self.max_kpt_num = MAX_KPT_NUM
-        self.suffix = 'imcpt-' + str(MAX_KPT_NUM)
-
-        self.root_path_npz = Path(ROOT_DIR_NPZ)
-        self.root_path_img = Path(ROOT_DIR_IMG)
-        self.obj_resize = obj_resize
-
-        self.img_lists = [np.load(self.root_path_npz / cls / 'img_info.npz')['img_name'].tolist()
-                          for cls in self.classes]
-
-        self.process()
-
-    def download(self, url=None, retries=15):
-        r"""
-         Automatically download IMC_PT_SparseGM dataset.
-
-         :param url: str, web url of IMC_PT_SparseGM
-         """
-        if retries <= 0:
-            raise RuntimeError('Max Retries exceeded!')
-
-        dirs = 'data/'
-        if not os.path.exists(dirs):
-            os.makedirs(dirs)
-        print('Downloading dataset IMC-PT-SparseGM...')
-        filename = 'data/IMC-PT-SparseGM.tar.gz'
-        download(filename=filename, url=url, to_cache=False)
-        try:
-            tar = tarfile.open(filename, "r")
-        except tarfile.ReadError as err:
-            print('Warning: Content error. Retrying...\n', err)
-            os.remove(filename)
-            return self.download(url, retries - 1)
-
-        file_names = tar.getnames()
-        print('Unzipping files...')
-        sleep(0.5)
-        for file_name in tqdm(file_names):
-            tar.extract(file_name, "data/")
-        tar.close()
-        try:
-            os.remove(filename)
-        except PermissionError:
-            pass
-        return filename
-
-    def process(self):
-        r"""
-        Process the dataset and generate ``data-(size, size).json`` for preprocessed dataset, ``train.json`` for
-        training set, and ``test.json`` for testing set.
-        """
-        set_file = os.path.join(self.dataset_dir, self.sets + '.json')
-        img_file = os.path.join(self.dataset_dir, 'data-' + str(self.obj_resize) + '-' + self.suffix + '.json')
-
-        if not os.path.exists(set_file):
-            set_list = []
-            for _list in self.img_lists:
-                for img_name in _list:
-                    set_list.append(img_name.split('.')[0])
-            str1 = json.dumps(set_list)
-            f1 = open(set_file, 'w')
-            f1.write(str1)
-            f1.close()
-
-        if not os.path.exists(img_file):
-            total_cls = []
-            for cls in self.class_dict['train']:
-                total_cls.append(cls)
-            for cls in self.class_dict['test']:
-                total_cls.append(cls)
-
-            total_img_lists = [np.load(self.root_path_npz / cls / 'img_info.npz')['img_name'].tolist()
-                               for cls in total_cls]
-            data_dict = dict()
-            for i, _list in enumerate(total_img_lists):
-                cls = total_cls[i]
-                for img_name in _list:
-                    img_id = img_name.split('.')[0]
-                    anno_dict = self.__get_anno_dict(img_name, cls)
-                    data_dict[img_id] = anno_dict
-
-            str2 = json.dumps(data_dict)
-            f2 = open(img_file, 'w')
-            f2.write(str2)
-            f2.close()
-
-    def __get_anno_dict(self, img_name, cls):
-        """
-        Get an annotation dict from ``.npz`` annotation
-        """
-        img_file = self.root_path_img / cls / img_name
-        npz_file = self.root_path_npz / cls / (img_name.split('.')[0] + '.npz')
-
-        assert img_file.exists(), '{} does not exist.'.format(img_file)
-        assert npz_file.exists(), '{} does not exist.'.format(npz_file)
-
-        with Image.open(str(img_file)) as img:
-            ori_sizes = img.size
-            xmin = 0
-            ymin = 0
-            w = ori_sizes[0]
-            h = ori_sizes[1]
-
-        with np.load(str(npz_file)) as npz_anno:
-            kpts = npz_anno['points']
-            if len(kpts.shape) != 2:
-                ValueError('{} contains no keypoints.'.format(img_file))
-
-        keypoint_list = []
-        for i in range(kpts.shape[1]):
-            kpt_index = int(kpts[0, i])
-            assert kpt_index < self.max_kpt_num
-            attr = {
-                'labels': kpt_index,
-                'x': kpts[1, i] * self.obj_resize[0] / w,
-                'y': kpts[2, i] * self.obj_resize[1] / h
-            }
-            keypoint_list.append(attr)
-
-        anno_dict = dict()
-        anno_dict['path'] = str(img_file)
-        anno_dict['kpts'] = keypoint_list
-        anno_dict['bounds'] = [xmin, ymin, w, h]
-        anno_dict['cls'] = cls
-        anno_dict['univ_size'] = self.max_kpt_num
-
-        return anno_dict
-
-    def __len(self, cls):
-        if type(cls) == int:
-            cls = self.classes[cls]
-        assert cls in self.classes
-        return len(self.img_lists[self.classes.index(cls)])
-
-
-class CUB2011:
-    r"""
-    Download and preprocess **CUB2011** dataset.
-
-    :param sets: str, problem set, ``'train'`` for training set and ``'test'`` for testing set
-    :param obj_resize: tuple, resized image size
-    :param ds_dict: settings of dataset, containing at most 1 params(key) for CUB2011:
-
-        * **ROOT_DIR**: str, directory of data
-    """
-    def __init__(self, sets, obj_resize, **ds_dict):
-        CLS_SPLIT = dataset_cfg.CUB2011.CLASS_SPLIT
-        ROOT_DIR = dataset_cfg.CUB2011.ROOT_DIR
-        URL = 'https://drive.google.com/u/0/uc?export=download&confirm=B8eu&id=1hbzc_P1FuxMkcabkgn9ZKinBwW683j45'
-        if len(ds_dict.keys()) > 0:
-            if 'ROOT_DIR' in ds_dict.keys():
-                ROOT_DIR = ds_dict['ROOT_DIR']
-            if 'URL' in ds_dict.keys():
-                URL = ds_dict['URL']
-
-        self.set_data = {'train': [], 'test': []}
-        self.classes = []
-
-        self._set_pairs = {}
-        self._set_mask = {}
-        self.cls_split = CLS_SPLIT
-        self.suffix = 'cub2011'
-
-        self.rootpath = ROOT_DIR
-
-        self.dataset_dir = 'data/CUB_200_2011'
-        if not os.path.exists(ROOT_DIR):
-            assert ROOT_DIR == dataset_cfg.CUB2011.ROOT_DIR, 'you should not change ROOT_DIR unless the data have been manually downloaded'
-            self.download(url=URL)
-
-        if not os.path.exists(self.dataset_dir):
-            os.makedirs(self.dataset_dir)
-
-        with open(os.path.join(self.rootpath, 'images.txt')) as f:
-            self.im2fn = dict(l.rstrip('\n').split() for l in f.readlines())
-        with open(os.path.join(self.rootpath, 'train_test_split.txt')) as f:
-            train_split = dict(l.rstrip('\n').split() for l in f.readlines())
-        with open(os.path.join(self.rootpath, 'classes.txt')) as f:
-            classes = dict(l.rstrip('\n').split() for l in f.readlines())
-        with open(os.path.join(self.rootpath, 'image_class_labels.txt')) as f:
-            img2class = [l.rstrip('\n').split() for l in f.readlines()]
-            img_idxs, class_idxs = map(list, zip(*img2class))
-            class2img = self.__lists2dict_for_cub(class_idxs, img_idxs)
-        with open(os.path.join(self.rootpath, 'parts', 'part_locs.txt')) as f:
-            part_locs = [l.rstrip('\n').split() for l in f.readlines()]
-            fi, pi, x, y, v = map(list, zip(*part_locs))
-            self.im2kpts = self.__lists2dict_for_cub(fi, zip(pi, x, y, v))
-        with open(os.path.join(self.rootpath, 'bounding_boxes.txt')) as f:
-            bboxes = [l.rstrip('\n').split() for l in f.readlines()]
-            ii, x, y, w, h = map(list, zip(*bboxes))
-            self.im2bbox = dict(zip(ii, zip(x, y, w, h)))
-        if self.cls_split == 'ori':
-            for class_idx in sorted(classes):
-                self.classes.append(classes[class_idx])
-                train_set = []
-                test_set = []
-                for img_idx in class2img[class_idx]:
-                    if train_split[img_idx] == '1':
-                        train_set.append(img_idx)
-                    else:
-                        test_set.append(img_idx)
-                self.set_data['train'].append(train_set)
-                self.set_data['test'].append(test_set)
-        self.sets = sets
-        self.obj_resize = obj_resize
-
-        self.process()
-
-    def download(self, url=None, retries=50):
-        r"""
-         Automatically download CUB2011 dataset.
-
-         :param url: str, web url of CUB2011
-         """
-        if retries <= 0:
-            raise RuntimeError('Max Retries exceeded!')
-
-        dirs = 'data/'
-        if not os.path.exists(dirs):
-            os.makedirs(dirs)
-        print('Downloading dataset CUB2011...')
-        filename = 'data/CUB_200_2011.tgz'
-        download(filename=filename, url=url, to_cache=False)
-        try:
-            tar = tarfile.open(filename, "r")
-        except tarfile.ReadError as err:
-            print('Warning: Content error. Retrying...\n', err)
-            os.remove(filename)
-            return self.download(url, retries - 1)
-
-        file_names = tar.getnames()
-        print('Unzipping files...')
-        sleep(0.5)
-        for file_name in tqdm(file_names):
-            tar.extract(file_name, "data/")
-        tar.close()
-        try:
-            os.remove(filename)
-        except PermissionError:
-            pass
-        return filename
-
-    def process(self):
-        r"""
-        Process the dataset and generate ``data-(size, size).json`` for preprocessed dataset, ``train.json`` for
-        training set, and ``test.json`` for testing set.
-        """
-        set_file = os.path.join(self.dataset_dir, self.sets + '.json')
-        img_file = os.path.join(self.dataset_dir, 'data-' + str(self.obj_resize) + '-' + self.suffix + '.json')
-
-        if not os.path.exists(set_file):
-            set_list = []
-            set_img_idx_list = self.set_data[self.sets]
-            for cls_img_idx_list in set_img_idx_list:
-                for img_idx in cls_img_idx_list:
-                    img_name = self.im2fn[img_idx].split('/')[-1].split('.')[0]
-                    set_list.append(img_name)
-
-            str1 = json.dumps(set_list)
-            f1 = open(set_file, 'w')
-            f1.write(str1)
-            f1.close()
-
-        if not os.path.exists(img_file):
-            data_dict = dict()
-            for img_idx, img_name in self.im2fn.items():
-                cls = img_name.split('/')[0]
-                obj_id = img_name.split('/')[-1].split('.')[0]
-                obj_dict = self.__get_anno_dict(img_idx, cls)
-                data_dict[obj_id] = obj_dict
-
-            str2 = json.dumps(data_dict)
-            f2 = open(img_file, 'w')
-            f2.write(str2)
-            f2.close()
-
-    def __get_imgname(self, data):
-        return os.path.join(self.rootpath, 'images', self.im2fn[data])
-
-    def __get_meta(self, data):
-        pi, x, y, v = map(list, zip(*self.im2kpts[data]))
-        order = np.argsort(np.array(pi).astype(int))
-        keypts = np.array([np.array(x).astype('float')[order],
-                           np.array(y).astype('float')[order]])
-        visible = np.array(v).astype('uint8')[order]
-        bbox = np.array(self.im2bbox[data]).astype(float)
-        return keypts, visible, bbox
-
-    def __get_anno_dict(self, img_name, cls):
-        keypts, visible, bbox = self.__get_meta(img_name)
-        xmin, ymin, w, h = bbox
-        img_file = self.__get_imgname(img_name)
-        with Image.open(str(img_file)) as img:
-            xmin, xmax = np.clip((xmin, xmin + w), 0, img.size[0])
-            ymin, ymax = np.clip((ymin, ymin + h), 0, img.size[1])
-
-        keypoint_list = []
-        for keypt_idx in range(keypts.shape[1]):
-            if visible[keypt_idx]:
-                attr = dict()
-                attr['labels'] = keypt_idx
-                attr['x'] = (keypts[0, keypt_idx] - xmin) * self.obj_resize[0] / w
-                attr['y'] = (keypts[1, keypt_idx] - ymin) * self.obj_resize[1] / h
-                keypoint_list.append(attr)
-
-        anno_dict = dict()
-        anno_dict['path'] = img_file
-        anno_dict['kpts'] = keypoint_list
-        anno_dict['bounds'] = [xmin, ymin, xmax, ymax]
-        anno_dict['cls'] = cls
-        anno_dict['univ_size'] = 15
-
-        return anno_dict
-
-    @staticmethod
-    def __lists2dict_for_cub(keys, vals):
-        ans = {}
-        for idx, val_i in enumerate(vals):
-            if keys[idx] in ans:
-                ans[keys[idx]].append(val_i)
-            else:
-                ans[keys[idx]] = [val_i]
-        return ans
+r"""
+The implementations of data loading and data processing.
+"""
+
+# Copyright (c) 2022 Thinklab@SJTU
+# pygmtools is licensed under Mulan PSL v2.
+# You can use this software according to the terms and conditions of the Mulan PSL v2.
+# You may obtain a copy of Mulan PSL v2 at:
+# http://license.coscl.org.cn/MulanPSL2
+# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND,
+# EITHER EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT,
+# MERCHANTABILITY OR FIT FOR A PARTICULAR PURPOSE.
+# See the Mulan PSL v2 for more details.
+
+import requests
+import os
+import zipfile
+import tarfile
+from pygmtools.dataset_config import dataset_cfg
+from pathlib import Path
+from xml.etree.ElementTree import Element
+from PIL import Image
+from tqdm.auto import tqdm
+from time import sleep
+import shutil
+import numpy as np
+import xml.etree.ElementTree as ET
+import pickle
+import json
+import scipy.io as sio
+import glob
+import random
+from pygmtools.utils import download
+
+
+VOC2011_KPT_NAMES = {
+    'cat': ['L_B_Elbow', 'L_B_Paw', 'L_EarBase', 'L_Eye', 'L_F_Elbow',
+            'L_F_Paw', 'Nose', 'R_B_Elbow', 'R_B_Paw', 'R_EarBase', 'R_Eye',
+            'R_F_Elbow', 'R_F_Paw', 'TailBase', 'Throat', 'Withers'],
+    'bottle': ['L_Base', 'L_Neck', 'L_Shoulder', 'L_Top', 'R_Base', 'R_Neck',
+               'R_Shoulder', 'R_Top'],
+    'horse': ['L_B_Elbow', 'L_B_Paw', 'L_EarBase', 'L_Eye', 'L_F_Elbow',
+              'L_F_Paw', 'Nose', 'R_B_Elbow', 'R_B_Paw', 'R_EarBase', 'R_Eye',
+              'R_F_Elbow', 'R_F_Paw', 'TailBase', 'Throat', 'Withers'],
+    'motorbike': ['B_WheelCenter', 'B_WheelEnd', 'ExhaustPipeEnd',
+                  'F_WheelCenter', 'F_WheelEnd', 'HandleCenter', 'L_HandleTip',
+                  'R_HandleTip', 'SeatBase', 'TailLight'],
+    'boat': ['Hull_Back_Bot', 'Hull_Back_Top', 'Hull_Front_Bot',
+             'Hull_Front_Top', 'Hull_Mid_Left_Bot', 'Hull_Mid_Left_Top',
+             'Hull_Mid_Right_Bot', 'Hull_Mid_Right_Top', 'Mast_Top', 'Sail_Left',
+             'Sail_Right'],
+    'tvmonitor': ['B_Bottom_Left', 'B_Bottom_Right', 'B_Top_Left',
+                  'B_Top_Right', 'F_Bottom_Left', 'F_Bottom_Right', 'F_Top_Left',
+                  'F_Top_Right'],
+    'cow': ['L_B_Elbow', 'L_B_Paw', 'L_EarBase', 'L_Eye', 'L_F_Elbow',
+            'L_F_Paw', 'Nose', 'R_B_Elbow', 'R_B_Paw', 'R_EarBase', 'R_Eye',
+            'R_F_Elbow', 'R_F_Paw', 'TailBase', 'Throat', 'Withers'],
+    'chair': ['BackRest_Top_Left', 'BackRest_Top_Right', 'Leg_Left_Back',
+              'Leg_Left_Front', 'Leg_Right_Back', 'Leg_Right_Front',
+              'Seat_Left_Back', 'Seat_Left_Front', 'Seat_Right_Back',
+              'Seat_Right_Front'],
+    'car': ['L_B_RoofTop', 'L_B_WheelCenter', 'L_F_RoofTop', 'L_F_WheelCenter',
+            'L_HeadLight', 'L_SideviewMirror', 'L_TailLight', 'R_B_RoofTop',
+            'R_B_WheelCenter', 'R_F_RoofTop', 'R_F_WheelCenter', 'R_HeadLight',
+            'R_SideviewMirror', 'R_TailLight'],
+    'person': ['B_Head', 'HeadBack', 'L_Ankle', 'L_Ear', 'L_Elbow', 'L_Eye',
+               'L_Foot', 'L_Hip', 'L_Knee', 'L_Shoulder', 'L_Toes', 'L_Wrist', 'Nose',
+               'R_Ankle', 'R_Ear', 'R_Elbow', 'R_Eye', 'R_Foot', 'R_Hip', 'R_Knee',
+               'R_Shoulder', 'R_Toes', 'R_Wrist'],
+    'diningtable': ['Bot_Left_Back', 'Bot_Left_Front', 'Bot_Right_Back',
+                    'Bot_Right_Front', 'Top_Left_Back', 'Top_Left_Front', 'Top_Right_Back',
+                    'Top_Right_Front'],
+    'dog': ['L_B_Elbow', 'L_B_Paw', 'L_EarBase', 'L_Eye', 'L_F_Elbow',
+            'L_F_Paw', 'Nose', 'R_B_Elbow', 'R_B_Paw', 'R_EarBase', 'R_Eye',
+            'R_F_Elbow', 'R_F_Paw', 'TailBase', 'Throat', 'Withers'],
+    'bird': ['Beak_Base', 'Beak_Tip', 'Left_Eye', 'Left_Wing_Base',
+             'Left_Wing_Tip', 'Leg_Center', 'Lower_Neck_Base', 'Right_Eye',
+             'Right_Wing_Base', 'Right_Wing_Tip', 'Tail_Tip', 'Upper_Neck_Base'],
+    'bicycle': ['B_WheelCenter', 'B_WheelEnd', 'B_WheelIntersection',
+                'CranksetCenter', 'F_WheelCenter', 'F_WheelEnd', 'F_WheelIntersection',
+                'HandleCenter', 'L_HandleTip', 'R_HandleTip', 'SeatBase'],
+    'train': ['Base_Back_Left', 'Base_Back_Right', 'Base_Front_Left',
+              'Base_Front_Right', 'Roof_Back_Left', 'Roof_Back_Right',
+              'Roof_Front_Middle'],
+    'sheep': ['L_B_Elbow', 'L_B_Paw', 'L_EarBase', 'L_Eye', 'L_F_Elbow',
+              'L_F_Paw', 'Nose', 'R_B_Elbow', 'R_B_Paw', 'R_EarBase', 'R_Eye',
+              'R_F_Elbow', 'R_F_Paw', 'TailBase', 'Throat', 'Withers'],
+    'aeroplane': ['Bot_Rudder', 'Bot_Rudder_Front', 'L_Stabilizer',
+                  'L_WingTip', 'Left_Engine_Back', 'Left_Engine_Front',
+                  'Left_Wing_Base', 'NoseTip', 'Nose_Bottom', 'Nose_Top',
+                  'R_Stabilizer', 'R_WingTip', 'Right_Engine_Back',
+                  'Right_Engine_Front', 'Right_Wing_Base', 'Top_Rudder'],
+    'sofa': ['Back_Base_Left', 'Back_Base_Right', 'Back_Top_Left',
+             'Back_Top_Right', 'Front_Base_Left', 'Front_Base_Right',
+             'Handle_Front_Left', 'Handle_Front_Right', 'Handle_Left_Junction',
+             'Handle_Right_Junction', 'Left_Junction', 'Right_Junction'],
+    'pottedplant': ['Bottom_Left', 'Bottom_Right', 'Top_Back_Middle',
+                    'Top_Front_Middle', 'Top_Left', 'Top_Right'],
+    'bus': ['L_B_Base', 'L_B_RoofTop', 'L_F_Base', 'L_F_RoofTop', 'R_B_Base',
+            'R_B_RoofTop', 'R_F_Base', 'R_F_RoofTop']
+}
+
+
+class PascalVOC:
+    r"""
+    Download and preprocess **PascalVOC Keypoint** dataset.
+
+    :param sets: str, problem set, ``'train'`` for training set and ``'test'`` for testing set
+    :param obj_resize: tuple, resized image size
+    :param ds_dict: settings of dataset, containing at most 5 params(keys) for PascalVOC:
+
+            * **KPT_ANNO_DIR**: str, directory of keypoint annotations
+
+            * **ROOT_DIR**: str, directory of data
+
+            * **SET_SPLIT**: str, set split path
+
+            * **CLASSES**: list, data classes
+
+            * **CACHE_PATH**: str, directory of data cache
+    """
+    def __init__(self, sets, obj_resize, **ds_dict):
+        KPT_ANNO_DIR = dataset_cfg.PascalVOC.KPT_ANNO_DIR
+        ROOT_DIR = dataset_cfg.PascalVOC.ROOT_DIR
+        SET_SPLIT = dataset_cfg.PascalVOC.SET_SPLIT
+        CLASSES = dataset_cfg.PascalVOC.CLASSES
+        CACHE_PATH = dataset_cfg.CACHE_PATH
+        if len(ds_dict.keys()) > 0:
+            if 'CLASSES' in ds_dict.keys():
+                CLASSES = ds_dict['CLASSES']
+            if 'ROOT_DIR' in ds_dict.keys():
+                ROOT_DIR = ds_dict['ROOT_DIR']
+            if 'SET_SPLIT' in ds_dict.keys():
+                SET_SPLIT = ds_dict['SET_SPLIT']
+            if 'KPT_ANNO_DIR' in ds_dict.keys():
+                KPT_ANNO_DIR = ds_dict['KPT_ANNO_DIR']
+            if 'CACHE_PATH' in ds_dict.keys():
+                CACHE_PATH = ds_dict['CACHE_PATH']
+
+        VOC2011_anno_path = KPT_ANNO_DIR
+        VOC2011_img_path = ROOT_DIR + 'JPEGImages'
+        VOC2011_ori_anno_path = ROOT_DIR + 'Annotations'
+        VOC2011_cache_path = CACHE_PATH
+
+        self.VOC2011_set_path = SET_SPLIT
+        self.dataset_dir = 'data/PascalVOC'
+        if not os.path.exists(ROOT_DIR):
+            assert ROOT_DIR == dataset_cfg.PascalVOC.ROOT_DIR, 'you should not change ROOT_DIR unless the data have been manually downloaded'
+            self.download(url='http://host.robots.ox.ac.uk/pascal/VOC/voc2011/VOCtrainval_25-May-2011.tar',
+                          name='PascalVOC')
+
+        if not os.path.exists(KPT_ANNO_DIR):
+            assert KPT_ANNO_DIR == dataset_cfg.PascalVOC.KPT_ANNO_DIR, 'you should not change KPT_ANNO_DIR unless the annotations have been manually downloaded'
+            self.download(
+                url='https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/shape/poselets/voc2011_keypoints_Feb2012.tgz',
+                name='PascalVOC_anno')
+
+        if not os.path.exists(self.dataset_dir):
+            os.makedirs(self.dataset_dir)
+
+        self.sets = sets
+        self.obj_resize = obj_resize
+        self.suffix = 'pca'
+
+        self.classes = CLASSES
+        self.kpt_len = [len(VOC2011_KPT_NAMES[_]) for _ in self.classes]
+        self.classes_kpts = {cls: len(VOC2011_KPT_NAMES[cls]) for cls in self.classes}
+        self.anno_path = Path(VOC2011_anno_path)
+        self.img_path = Path(VOC2011_img_path)
+        self.ori_anno_path = Path(VOC2011_ori_anno_path)
+
+        assert sets == 'train' or sets == 'test', 'No match found for dataset {}'.format(sets)
+        cache_name = 'voc_db_' + sets + '.pkl'
+        self.cache_path = Path(VOC2011_cache_path)
+        self.cache_file = self.cache_path / cache_name
+        if self.cache_file.exists():
+            with self.cache_file.open(mode='rb') as f:
+                self.xml_list = pickle.load(f)
+            print('xml list loaded from {}'.format(self.cache_file))
+
+        else:
+            if self.sets != 'test':
+                print('Caching xml list to {}...'.format(self.cache_file))
+            self.cache_path.mkdir(exist_ok=True, parents=True)
+            with np.load(self.VOC2011_set_path, allow_pickle=True) as f:
+                self.xml_list = f[sets]
+            before_filter = sum([len(k) for k in self.xml_list])
+            self.__filter_list(self.xml_list)
+            after_filter = sum([len(k) for k in self.xml_list])
+            with self.cache_file.open(mode='wb') as f:
+                pickle.dump(self.xml_list, f)
+            print('Filtered {} images to {}. Annotation saved.'.format(before_filter, after_filter))
+
+        self.process()
+
+    def download(self, url=None, name=None, retries=5):
+        r"""
+        Automatically download PascalVOC dataset.
+
+        :param url: str, web url of PascalVOC and PascalVOC annotation
+        :param name: str, ``"PascalVOC"`` to download PascalVOC and ``"PascalVOC_anno"`` to download PascalVOC annotation
+        """
+        if retries <= 0:
+            raise RuntimeError('Max Retries exceeded!')
+
+        dirs = 'data/'
+        if not os.path.exists(dirs):
+            os.makedirs(dirs)
+
+        if name == "PascalVOC_anno":
+            print('Downloading dataset annotation...')
+            filename = "data/PascalVOC.tgz"
+            download(filename=filename, url=url, to_cache=False)
+            try:
+                tar = tarfile.open(filename, "r")
+            except tarfile.ReadError as err:
+                print('Warning: Content error. Retrying...\n', err)
+                os.remove(filename)
+                return self.download(url, name, retries - 1)
+
+            file_names = tar.getnames()
+            print('Unzipping files...')
+            sleep(0.5)
+            for file_name in tqdm(file_names):
+                tar.extract(file_name, "data/PascalVOC/")
+            tar.close()
+            try:
+                os.remove(filename)
+            except PermissionError:
+                pass
+
+        if name == "PascalVOC":
+            print('Downloading dataset PascalVOC...')
+            filename = "data/PascalVOC.tar"
+            download(filename=filename, url=url, to_cache=False)
+            try:
+                tar = tarfile.open(filename, "r")
+            except tarfile.ReadError as err:
+                print('Warning: Content error. Retrying...\n', err)
+                os.remove(filename)
+                return self.download(url, name, retries - 1)
+
+            file_names = tar.getnames()
+            print('Unzipping files...')
+            sleep(0.5)
+            for file_name in tqdm(file_names):
+                tar.extract(file_name, "data/PascalVOC/")
+            tar.close()
+            try:
+                os.remove(filename)
+            except PermissionError:
+                pass
+        return filename
+
+    def __filter_list(self, a_xml_list):
+        """
+        Filter out ``'truncated'``, ``'occluded'`` and ``'difficult'`` images following the practice of previous works.
+        In addition, this dataset has uncleaned label (in person category). They are omitted as suggested by README.
+        """
+        for cls_id in range(len(self.classes)):
+            to_del = []
+            for xml_name in a_xml_list[cls_id]:
+                xml_comps = xml_name.split('/')[-1].strip('.xml').split('_')
+                ori_xml_name = '_'.join(xml_comps[:-1]) + '.xml'
+                voc_idx = int(xml_comps[-1])
+                xml_file = self.ori_anno_path / ori_xml_name
+                assert xml_file.exists(), '{} does not exist.'.format(xml_file)
+                tree = ET.parse(xml_file.open())
+                root = tree.getroot()
+                obj: Element = root.findall('object')[voc_idx - 1]
+
+                difficult = obj.find('difficult')
+                if difficult is not None:
+                    difficult = int(difficult.text)
+                occluded = obj.find('occluded')
+                if occluded is not None:
+                    occluded = int(occluded.text)
+                truncated = obj.find('truncated')
+                if truncated is not None:
+                    truncated = int(truncated.text)
+                if difficult or occluded or truncated:
+                    to_del.append(xml_name)
+                    continue
+
+                    # Exclude uncleaned images
+                if self.classes[cls_id] == 'person' and int(xml_comps[0]) > 2008:
+                    to_del.append(xml_name)
+                    continue
+
+                    # Exclude overlapping images in Willow
+                    # if self.sets == 'train' and (self.classes[cls_id] == 'motorbike' or self.classes[cls_id] == 'car') \
+                    #        and int(xml_comps[0]) == 2007:
+                    #    to_del.append(xml_name)
+                    #    continue
+
+            for x in to_del:
+                a_xml_list[cls_id].remove(x)
+
+    def process(self):
+        r"""
+        Process the dataset and generate ``data-(size, size).json`` for preprocessed dataset, ``train.json`` for
+        training set, and ``test.json`` for testing set.
+        """
+        train_file = os.path.join(self.dataset_dir, 'train.json')
+        test_file = os.path.join(self.dataset_dir, 'test.json')
+        img_file = os.path.join(self.dataset_dir, 'data-' + str(self.obj_resize) + '-' + self.suffix + '.json')
+        if not (os.path.exists(train_file) and os.path.exists(test_file) and os.path.exists(img_file)):
+            if not (os.path.exists(train_file) and os.path.exists(test_file)):
+                list1 = []
+                for x in range(len(self.xml_list)):
+                    for xml_name in self.xml_list[x]:
+                        tmp = xml_name.split('/')
+                        tmp2 = tmp[1].split('.')
+                        objID = tmp2[0] + '_' + tmp[0]
+                        list1.append(objID)
+
+                list2 = []
+                if self.sets == 'train':
+                    with np.load(self.VOC2011_set_path, allow_pickle=True) as f:
+                        a_list = f['test']
+                    self.__filter_list(a_list)
+                    cache_name = 'voc_db_test.pkl'
+                    cache_file = self.cache_path / cache_name
+                    if not cache_file.exists():
+                        with cache_file.open(mode='wb') as f:
+                            pickle.dump(a_list, f)
+
+                    for x in range(len(a_list)):
+                        for xml_name in a_list[x]:
+                            tmp = xml_name.split('/')
+                            tmp2 = tmp[1].split('.')
+                            objID = tmp2[0] + '_' + tmp[0]
+                            list2.append(objID)
+                    str1 = json.dumps(list1)
+                    f1 = open(train_file, 'w')
+                    f1.write(str1)
+                    f1.close()
+                    str2 = json.dumps(list2)
+                    f2 = open(test_file, 'w')
+                    f2.write(str2)
+                    f2.close()
+                else:
+                    with np.load(self.VOC2011_set_path, allow_pickle=True) as f:
+                        a_list = f['train']
+                    self.__filter_list(a_list)
+                    cache_name = 'voc_db_train.pkl'
+                    cache_file = self.cache_path / cache_name
+                    if not cache_file.exists():
+                        with cache_file.open(mode='wb') as f:
+                            pickle.dump(a_list, f)
+
+                    for x in range(len(a_list)):
+                        for xml_name in a_list[x]:
+                            tmp = xml_name.split('/')
+                            tmp2 = tmp[1].split('.')
+                            objID = tmp2[0] + '_' + tmp[0]
+                            list2.append(objID)
+                    str1 = json.dumps(list1)
+                    f1 = open(test_file, 'w')
+                    f1.write(str1)
+                    f1.close()
+                    str2 = json.dumps(list2)
+                    f2 = open(train_file, 'w')
+                    f2.write(str2)
+                    f2.close()
+            else:
+                if self.sets == 'train':
+                    with np.load(self.VOC2011_set_path, allow_pickle=True) as f:
+                        a_list = f['test']
+                    self.__filter_list(a_list)
+                    cache_name = 'voc_db_test.pkl'
+                    cache_file = self.cache_path / cache_name
+                    if not cache_file.exists():
+                        with cache_file.open(mode='wb') as f:
+                            pickle.dump(a_list, f)
+
+                else:
+                    with np.load(self.VOC2011_set_path, allow_pickle=True) as f:
+                        a_list = f['train']
+                    self.__filter_list(a_list)
+                    cache_name = 'voc_db_train.pkl'
+                    cache_file = self.cache_path / cache_name
+                    if not cache_file.exists():
+                        with cache_file.open(mode='wb') as f:
+                            pickle.dump(a_list, f)
+
+            if not os.path.exists(img_file):
+                data_list = self.xml_list + a_list
+                data_dict = dict()
+                for x in range(len(data_list)):
+                    for xml_name in data_list[x]:
+                        tmp = xml_name.split('/')
+                        tmp2 = tmp[1].split('.')
+                        objID = tmp2[0] + '_' + tmp[0]
+                        annotations = self.__get_anno_dict(xml_name)
+                        data_dict[objID] = annotations
+
+                data_str = json.dumps(data_dict)
+                f3 = open(img_file, 'w')
+                f3.write(data_str)
+                f3.close()
+
+    def __get_anno_dict(self, xml_name):
+        """
+        Get an annotation dict from xml file
+        """
+        xml_file = self.anno_path / xml_name
+        assert xml_file.exists(), '{} does not exist.'.format(xml_file)
+
+        tree = ET.parse(xml_file.open())
+        root = tree.getroot()
+
+        img_name = root.find('./image').text + '.jpg'
+        img_file = self.img_path / img_name
+        bounds = root.find('./visible_bounds').attrib
+        cls = root.find('./category').text
+
+        xmin = float(bounds['xmin'])
+        ymin = float(bounds['ymin'])
+        h = float(bounds['height'])
+        w = float(bounds['width'])
+        xmax = float(xmin) + float(w)
+        ymax = float(ymin) + float(h)
+
+        keypoint_list = []
+        for keypoint in root.findall('./keypoints/keypoint'):
+            attr = keypoint.attrib
+            attr['x'] = (float(attr['x']) - xmin) * self.obj_resize[0] / w
+            attr['y'] = (float(attr['y']) - ymin) * self.obj_resize[1] / h
+            kpts_anno = dict()
+            kpts_anno['labels'] = attr['name']
+            kpts_anno['x'] = attr['x']
+            kpts_anno['y'] = attr['y']
+            keypoint_list.append(kpts_anno)
+
+        anno_dict = dict()
+        anno_dict['kpts'] = keypoint_list
+        anno_dict['path'] = str(img_file)
+        anno_dict['cls'] = cls
+        anno_dict['bounds'] = [xmin, ymin, xmax, ymax]
+        anno_dict['univ_size'] = len(VOC2011_KPT_NAMES[cls])
+
+        return anno_dict
+
+
+class WillowObject:
+    r"""
+        Download and preprocess **Willow Object Class** dataset.
+
+        :param sets: str, problem set, ``'train'`` for training set and ``'test'`` for testing set
+        :param obj_resize: tuple, resized image size
+        :param ds_dict: settings of dataset, containing at most 6 params(keys) for WillowObject:
+
+            * **ROOT_DIR**: str, directory of data
+
+            * **CLASSES**: list, data classes
+
+            * **TRAIN_NUM**: int, number of images for train in each class
+
+            * **SPLIT_OFFSET**: int, offset when split train and testing set
+
+            * **TRAIN_SAME_AS_TEST**: bool, whether to use same images for training and test
+
+            * **RAND_OUTLIER**: int, number of added outliers in one image
+    """
+    def __init__(self, sets, obj_resize, **ds_dict):
+        CLASSES = dataset_cfg.WillowObject.CLASSES
+        KPT_LEN = dataset_cfg.WillowObject.KPT_LEN
+        ROOT_DIR = dataset_cfg.WillowObject.ROOT_DIR
+        TRAIN_NUM = dataset_cfg.WillowObject.TRAIN_NUM
+        SPLIT_OFFSET = dataset_cfg.WillowObject.SPLIT_OFFSET
+        TRAIN_SAME_AS_TEST = dataset_cfg.WillowObject.TRAIN_SAME_AS_TEST
+        RAND_OUTLIER = dataset_cfg.WillowObject.RAND_OUTLIER
+        URL = 'http://www.di.ens.fr/willow/research/graphlearning/WILLOW-ObjectClass_dataset.zip'
+        if len(ds_dict.keys()) > 0:
+            if 'CLASSES' in ds_dict.keys():
+                CLASSES = ds_dict['CLASSES']
+            if 'ROOT_DIR' in ds_dict.keys():
+                ROOT_DIR = ds_dict['ROOT_DIR']
+            if 'TRAIN_NUM' in ds_dict.keys():
+                TRAIN_NUM = ds_dict['TRAIN_NUM']
+            if 'SPLIT_OFFSET' in ds_dict.keys():
+                SPLIT_OFFSET = ds_dict['SPLIT_OFFSET']
+            if 'TRAIN_SAME_AS_TEST' in ds_dict.keys():
+                TRAIN_SAME_AS_TEST = ds_dict['TRAIN_SAME_AS_TEST']
+            if 'RAND_OUTLIER' in ds_dict.keys():
+                RAND_OUTLIER = ds_dict['RAND_OUTLIER']
+            if 'URL' in ds_dict.keys():
+                URL = ds_dict['URL']
+
+        self.dataset_dir = 'data/WillowObject'
+        if not os.path.exists(ROOT_DIR):
+            assert ROOT_DIR == dataset_cfg.WillowObject.ROOT_DIR, 'you should not change ROOT_DIR unless the data have been manually downloaded'
+            self.download(url=URL)
+
+        if not os.path.exists(self.dataset_dir):
+            os.makedirs(self.dataset_dir)
+
+        self.sets = sets
+        self.obj_resize = obj_resize
+        self.suffix = 'willow-' + str(RAND_OUTLIER)
+
+        self.classes = CLASSES
+        self.kpt_len = [KPT_LEN for _ in self.classes]
+
+        self.root_path = Path(ROOT_DIR)
+        self.obj_resize = obj_resize
+
+        assert sets == 'train' or 'test', 'No match found for dataset {}'.format(sets)
+        self.split_offset = SPLIT_OFFSET
+        self.train_len = TRAIN_NUM
+        self.train_same_as_test = TRAIN_SAME_AS_TEST
+        self.rand_outlier = RAND_OUTLIER
+
+        self.mat_list = []
+
+        self.process()
+
+    def download(self, url=None, retries=5):
+        r"""
+         Automatically download WillowObject dataset.
+
+         :param url: str, web url of WillowObject
+         """
+        if retries <= 0:
+            raise RuntimeError('Max Retries exceeded!')
+
+        dirs = 'data/'
+        if not os.path.exists(dirs):
+            os.makedirs(dirs)
+
+        print('Downloading dataset WillowObject...')
+        filename = "data/WILLOW.zip"
+        download(filename=filename, url=url, to_cache=False)
+        try:
+            fz = zipfile.ZipFile(filename, "r")
+        except zipfile.BadZipFile as err:
+            print('Warning: Content error. Retrying...\n', err)
+            os.remove(filename)
+            return self.download(url, retries - 1)
+
+        print('Unzipping files...')
+        sleep(0.5)
+        for file in tqdm(fz.namelist()):
+            fz.extract(file, "data/WillowObject/")
+        try:
+            os.remove(filename)
+        except PermissionError:
+            pass
+        return filename
+
+    def process(self):
+        r"""
+        Process the dataset and generate ``data-(size, size).json`` for preprocessed dataset, ``train.json`` for
+        training set, and ``test.json`` for testing set.
+        """
+        train_file = os.path.join(self.dataset_dir, 'train.json')
+        test_file = os.path.join(self.dataset_dir, 'test.json')
+        img_file = os.path.join(self.dataset_dir, 'data-' + str(self.obj_resize) + '-' + self.suffix + '.json')
+
+        data_list = []
+        mat_list_ = []
+        for cls_name in self.classes:
+            assert type(cls_name) is str
+            cls_mat_list = [p for p in (self.root_path / cls_name).glob('*.mat')]
+            cls_mat_list.sort()
+            if cls_name == 'Face':
+                cls_mat_list.remove(self.root_path / cls_name / 'image_0160.mat')
+                assert not self.root_path / cls_name / 'image_0160.mat' in cls_mat_list
+            ori_len = len(cls_mat_list)
+            assert ori_len > 0, 'No data found for WillowObject Class. Is the dataset installed correctly?'
+            data_list.append(cls_mat_list)
+            if self.split_offset % ori_len + self.train_len <= ori_len:
+                if self.sets == 'train' and not self.train_same_as_test:
+                    self.mat_list.append(
+                        cls_mat_list[self.split_offset % ori_len: (self.split_offset + self.train_len) % ori_len]
+                    )
+
+                    mat_list_.append(
+                        cls_mat_list[:self.split_offset % ori_len] +
+                        cls_mat_list[(self.split_offset + self.train_len) % ori_len:]
+                    )
+                elif self.train_same_as_test:
+                    self.mat_list.append(
+                        cls_mat_list[:self.split_offset % ori_len] +
+                        cls_mat_list[(self.split_offset + self.train_len) % ori_len:]
+                    )
+
+                    mat_list_.append(
+                        cls_mat_list[:self.split_offset % ori_len] +
+                        cls_mat_list[(self.split_offset + self.train_len) % ori_len:]
+                    )
+                else:
+                    self.mat_list.append(
+                        cls_mat_list[:self.split_offset % ori_len] +
+                        cls_mat_list[(self.split_offset + self.train_len) % ori_len:]
+                    )
+
+                    mat_list_.append(
+                        cls_mat_list[self.split_offset % ori_len: (self.split_offset + self.train_len) % ori_len]
+                    )
+            else:
+                if self.sets == 'train' and not self.train_same_as_test:
+                    self.mat_list.append(
+                        cls_mat_list[:(self.split_offset + self.train_len) % ori_len - ori_len] +
+                        cls_mat_list[self.split_offset % ori_len:]
+                    )
+
+                    mat_list_.append(
+                        cls_mat_list[(self.split_offset + self.train_len) % ori_len - ori_len: self.split_offset % ori_len]
+                    )
+                elif self.train_same_as_test:
+                    self.mat_list.append(
+                        cls_mat_list[(self.split_offset + self.train_len) % ori_len - ori_len: self.split_offset % ori_len]
+                    )
+
+                    mat_list_.append(
+                        cls_mat_list[(self.split_offset + self.train_len) % ori_len - ori_len: self.split_offset % ori_len]
+                    )
+                else:
+                    self.mat_list.append(
+                        cls_mat_list[(self.split_offset + self.train_len) % ori_len - ori_len: self.split_offset % ori_len]
+                    )
+
+                    mat_list_.append(
+                        cls_mat_list[:(self.split_offset + self.train_len) % ori_len - ori_len] +
+                        cls_mat_list[self.split_offset % ori_len:]
+                    )
+
+        train_list = []
+        test_list = []
+        if self.sets == 'train':
+            for x in range(len(self.mat_list)):
+                for name in self.mat_list[x]:
+                    tmp = os.path.split(str(name))
+                    objID = tmp[-1].split('.')[0]
+                    train_list.append(objID)
+            for x in range(len(mat_list_)):
+                for name in mat_list_[x]:
+                    tmp = os.path.split(str(name))
+                    objID = tmp[-1].split('.')[0]
+                    test_list.append(objID)
+        else:
+            for x in range(len(self.mat_list)):
+                for name in self.mat_list[x]:
+                    tmp = os.path.split(str(name))
+                    objID = tmp[-1].split('.')[0]
+                    test_list.append(objID)
+            for x in range(len(mat_list_)):
+                for name in mat_list_[x]:
+                    tmp = os.path.split(str(name))
+                    objID = tmp[-1].split('.')[0]
+                    train_list.append(objID)
+        str1 = json.dumps(train_list)
+        f1 = open(train_file, 'w')
+        f1.write(str1)
+        f1.close()
+        str2 = json.dumps(test_list)
+        f2 = open(test_file, 'w')
+        f2.write(str2)
+        f2.close()
+
+        if not os.path.exists(img_file):
+            data_dict = dict()
+
+            for x in range(len(data_list)):
+                for name in data_list[x]:
+                    tmp = os.path.split(str(name))
+                    objID = tmp[-1].split('.')[0]
+                    cls = os.path.split(tmp[0])[-1]
+                    annotations = self.__get_anno_dict(name, cls)
+                    data_dict[objID] = annotations
+
+            data_str = json.dumps(data_dict)
+            f3 = open(img_file, 'w')
+            f3.write(data_str)
+            f3.close()
+
+    def __get_anno_dict(self, mat_file, cls):
+        """
+        Get an annotation dict from .mat annotation
+        """
+        assert mat_file.exists(), '{} does not exist.'.format(mat_file)
+
+        img_name = mat_file.stem + '.png'
+        img_file = mat_file.parent / img_name
+
+        struct = sio.loadmat(mat_file.open('rb'))
+        kpts = struct['pts_coord']
+
+        with Image.open(str(img_file)) as img:
+            ori_sizes = img.size
+            xmin = 0
+            ymin = 0
+            w = ori_sizes[0]
+            h = ori_sizes[1]
+
+        keypoint_list = []
+        for idx, keypoint in enumerate(np.split(kpts, kpts.shape[1], axis=1)):
+            attr = {'labels': idx}
+            attr['x'] = float(keypoint[0]) * self.obj_resize[0] / w
+            attr['y'] = float(keypoint[1]) * self.obj_resize[1] / h
+            keypoint_list.append(attr)
+
+        for idx in range(self.rand_outlier):
+            attr = {
+                'labels': 'outlier',
+                'x': random.uniform(0, self.obj_resize[0]),
+                'y': random.uniform(0, self.obj_resize[1])
+            }
+            keypoint_list.append(attr)
+
+        anno_dict = dict()
+        anno_dict['path'] = str(img_file)
+        anno_dict['kpts'] = keypoint_list
+        anno_dict['bounds'] = [xmin, ymin, w, h]
+        anno_dict['cls'] = cls
+        anno_dict['univ_size'] = 10
+
+        return anno_dict
+
+
+class SPair71k:
+    r"""
+    Download and preprocess **SPair71k** dataset.
+
+    :param sets: str, problem set, ``'train'`` for training set and ``'test'`` for testing set
+    :param obj_resize: tuple, resized image size
+    :param problem: str, problem type, only ``'2GM'`` is supported in SPair71k
+    :param ds_dict: settings of dataset, containing at most 5 params(keys) for SPair71k:
+
+        * **ROOT_DIR**: str, directory of data
+
+        * **TRAIN_DIFF_PARAMS**: list of images that should be dumped in train set
+
+        * **EVAL_DIFF_PARAMS**: list of images that should be dumped in testing set
+
+        * **COMB_CLS**: bool, whether to combine images in different classes
+
+        * **SIZE**: str, ``'large'`` for SPair71k-large and ``'small'`` for SPair71k-small
+    """
+    def __init__(self, sets, obj_resize, problem='2GM', **ds_dict):
+        TRAIN_DIFF_PARAMS = dataset_cfg.SPair.TRAIN_DIFF_PARAMS
+        EVAL_DIFF_PARAMS = dataset_cfg.SPair.EVAL_DIFF_PARAMS
+        COMB_CLS = dataset_cfg.SPair.COMB_CLS
+        SIZE = dataset_cfg.SPair.SIZE
+        ROOT_DIR = dataset_cfg.SPair.ROOT_DIR
+        if len(ds_dict.keys()) > 0:
+            if 'TRAIN_DIFF_PARAMS' in ds_dict.keys():
+                TRAIN_DIFF_PARAMS = ds_dict['TRAIN_DIFF_PARAMS']
+            if 'EVAL_DIFF_PARAMS' in ds_dict.keys():
+                EVAL_DIFF_PARAMS = ds_dict['EVAL_DIFF_PARAMS']
+            if 'COMB_CLS' in ds_dict.keys():
+                COMB_CLS = ds_dict['COMB_CLS']
+            if 'SIZE' in ds_dict.keys():
+                SIZE = ds_dict['SIZE']
+            if 'ROOT_DIR' in ds_dict.keys():
+                ROOT_DIR = ds_dict['ROOT_DIR']
+
+        SPair71k_pair_ann_path = ROOT_DIR + "/PairAnnotation"
+        SPair71k_image_path = ROOT_DIR + "/JPEGImages"
+        SPair71k_image_annotation = ROOT_DIR + "/ImageAnnotation"
+        self.SPair71k_layout_path = ROOT_DIR + "/Layout"
+        self.SPair71k_dataset_size = SIZE
+        self.suffix = 'spair-' + SIZE
+
+        sets_translation_dict = dict(train="trn", test="test")
+        difficulty_params_dict = dict(
+            trn=TRAIN_DIFF_PARAMS, val=EVAL_DIFF_PARAMS, test=EVAL_DIFF_PARAMS
+        )
+
+        assert not problem == 'MGM', 'No match found for problem {} in SPair-71k'.format(problem)
+        self.dataset_dir = 'data/SPair-71k'
+        if not os.path.exists(SPair71k_image_path):
+            assert ROOT_DIR == dataset_cfg.SPair.ROOT_DIR, 'you should not change ROOT_DIR unless the data have been manually downloaded'
+            self.download(url='http://cvlab.postech.ac.kr/research/SPair-71k/data/SPair-71k.tar.gz')
+
+        if not os.path.exists(self.dataset_dir):
+            os.makedirs(self.dataset_dir)
+
+        self.obj_resize = obj_resize
+        self.sets = sets_translation_dict[sets]
+        self.ann_files = open(os.path.join(self.SPair71k_layout_path, self.SPair71k_dataset_size, self.sets + ".txt"), "r").read().split("\n")
+        self.ann_files = self.ann_files[: len(self.ann_files) - 1]
+        self.difficulty_params = difficulty_params_dict[self.sets]
+        self.pair_ann_path = SPair71k_pair_ann_path
+        self.image_path = SPair71k_image_path
+        self.image_annoation = Path(SPair71k_image_annotation)
+        self.classes = list(map(lambda x: os.path.basename(x), glob.glob("%s/*" % SPair71k_image_path)))
+        self.classes.sort()
+        self.combine_classes = COMB_CLS
+        self.ann_files_filtered, self.ann_files_filtered_cls_dict, _ = self.__filter_annotations(
+            self.ann_files, self.difficulty_params
+        )
+        self.total_size = len(self.ann_files_filtered)
+        self.size_by_cls = {cls: len(ann_list) for cls, ann_list in self.ann_files_filtered_cls_dict.items()}
+
+        self.process()
+
+    def download(self, url=None, retries=5):
+        r"""
+         Automatically download SPair71k dataset.
+
+         :param url: str, web url of SPair71k
+         """
+        if retries <= 0:
+            raise RuntimeError('Max Retries exceeded!')
+
+        dirs = 'data/'
+        if not os.path.exists(dirs):
+            os.makedirs(dirs)
+        print('Downloading dataset SPair-71k...')
+        filename = "data/SPair-71k.tgz"
+        download(filename=filename, url=url, to_cache=False)
+        try:
+            tar = tarfile.open(filename, "r")
+        except tarfile.ReadError as err:
+            print('Warning: Content error. Retrying...\n', err)
+            os.remove(filename)
+            return self.download(url, retries - 1)
+
+        file_names = tar.getnames()
+        print('Unzipping files...')
+        sleep(0.5)
+        for file_name in tqdm(file_names):
+            tar.extract(file_name, "data/")
+        tar.close()
+        try:
+            os.remove(filename)
+        except PermissionError:
+            pass
+        return filename
+
+    def process(self):
+        r"""
+        Process the dataset and generate ``data-(size, size).json`` for preprocessed dataset, ``train.json`` for
+        training set, and ``test.json`` for testing set.
+        """
+        train_file = os.path.join(self.dataset_dir, 'train.json')
+        test_file = os.path.join(self.dataset_dir, 'test.json')
+        img_file = os.path.join(self.dataset_dir, 'data-' + str(self.obj_resize) + '-' + self.suffix + '.json')
+        if (not os.path.exists(train_file)) or (not os.path.exists(test_file)):
+            train_list = []
+            test_list = []
+            if self.sets == 'trn':
+                for x in self.ann_files_filtered:
+                    tmp = x.split('-')
+                    tmp2 = tmp[2].split(':')
+                    id1 = tmp[1] + '_' + tmp2[1]
+                    id2 = tmp2[0] + '_' + tmp2[1]
+                    pair_tuple = (id1, id2)
+                    train_list.append(pair_tuple)
+
+                str1 = json.dumps(train_list)
+                f1 = open(train_file, 'w')
+                f1.write(str1)
+                f1.close()
+            else:
+                for x in self.ann_files_filtered:
+                    tmp = x.split('-')
+                    tmp2 = tmp[2].split(':')
+                    id1 = tmp[1] + '_' + tmp2[1]
+                    id2 = tmp2[0] + '_' + tmp2[1]
+                    pair_tuple = (id1, id2)
+                    test_list.append(pair_tuple)
+
+                str2 = json.dumps(test_list)
+                f2 = open(test_file, 'w')
+                f2.write(str2)
+                f2.close()
+
+            data_list = []
+            data_dict = dict()
+            for cls_name in self.classes:
+                cls_json_list = [p for p in (self.image_annoation / cls_name).glob('*.json')]
+                ori_len = len(cls_json_list)
+                assert ori_len > 0, 'No data found for SPair-71k. Is the dataset installed correctly?'
+                data_list.append(cls_json_list)
+
+            for x in range(len(data_list)):
+                for name in data_list[x]:
+                    tmp = os.path.split(str(name))
+                    objID = tmp[-1].split('.')[0]
+                    cls = os.path.split(tmp[0])[-1]
+                    annotations = self.__get_anno_dict(name, cls)
+                    ID = objID + '_' + cls
+                    data_dict[ID] = annotations
+
+            data_str = json.dumps(data_dict)
+            f3 = open(img_file, 'w')
+            f3.write(data_str)
+            f3.close()
+
+    def __get_anno_dict(self, anno_file, cls):
+        assert anno_file.exists(), '{} does not exist.'.format(anno_file)
+
+        img_file = self.image_path + '/' + cls + '/' + anno_file.stem + '.jpg'
+
+        with open(anno_file) as f:
+            annotations = json.load(f)
+
+        h = float(annotations['image_height'])
+        w = float(annotations['image_width'])
+        boundbox = annotations['bndbox']
+
+        keypoint_list = []
+        for key, value in annotations['kps'].items():
+            if not value == None:
+                x = (value[0] - boundbox[0]) * self.obj_resize[0] / (boundbox[2] - boundbox[0])
+                y = (value[1] - boundbox[1]) * self.obj_resize[1] / (boundbox[3] - boundbox[1])
+                kpts_anno = dict()
+                kpts_anno['labels'] = key
+                kpts_anno['x'] = x
+                kpts_anno['y'] = y
+                keypoint_list.append(kpts_anno)
+
+        anno_dict = dict()
+        anno_dict['kpts'] = keypoint_list
+        anno_dict['path'] = img_file
+        anno_dict['cls'] = cls
+        anno_dict['bounds'] = boundbox
+        anno_dict['univ_size'] = len(VOC2011_KPT_NAMES[cls])
+
+        return anno_dict
+
+    def __filter_annotations(self, ann_files, difficulty_params):
+        if len(difficulty_params) > 0:
+            basepath = os.path.join(self.pair_ann_path, "pickled", self.sets)
+            if not os.path.exists(basepath):
+                os.makedirs(basepath)
+            difficulty_paramas_str = self.__diff_dict_to_str(difficulty_params)
+            try:
+                filepath = os.path.join(basepath, difficulty_paramas_str + ".pickle")
+                ann_files_filtered = pickle.load(open(filepath, "rb"))
+                print(
+                    f"Found filtered annotations for difficulty parameters {difficulty_params} and {self.sets}-set at {filepath}"
+                )
+            except (OSError, IOError) as e:
+                print(
+                    f"No pickled annotations found for difficulty parameters {difficulty_params} and {self.sets}-set. Filtering..."
+                )
+                ann_files_filtered_dict = {}
+
+                for ann_file in ann_files:
+                    with open(os.path.join(self.pair_ann_path, self.sets, ann_file + ".json")) as f:
+                        annotation = json.load(f)
+                    diff = {key: annotation[key] for key in self.difficulty_params.keys()}
+                    diff_str = self.__diff_dict_to_str(diff)
+                    if diff_str in ann_files_filtered_dict:
+                        ann_files_filtered_dict[diff_str].append(ann_file)
+                    else:
+                        ann_files_filtered_dict[diff_str] = [ann_file]
+                total_l = 0
+                for diff_str, file_list in ann_files_filtered_dict.items():
+                    total_l += len(file_list)
+                    filepath = os.path.join(basepath, diff_str + ".pickle")
+                    pickle.dump(file_list, open(filepath, "wb"))
+                assert total_l == len(ann_files)
+                print(f"Done filtering. Saved filtered annotations to {basepath}.")
+                ann_files_filtered = ann_files_filtered_dict[difficulty_paramas_str]
+        else:
+            print(f"No difficulty parameters for {self.sets}-set. Using all available data.")
+            ann_files_filtered = ann_files
+
+        ann_files_filtered_cls_dict = {
+            cls: list(filter(lambda x: cls in x, ann_files_filtered)) for cls in self.classes
+        }
+        class_len = {cls: len(ann_list) for cls, ann_list in ann_files_filtered_cls_dict.items()}
+        print(f"Number of annotation pairs matching the difficulty params in {self.sets}-set: {class_len}")
+        if self.combine_classes:
+            cls_name = "combined"
+            ann_files_filtered_cls_dict = {cls_name: ann_files_filtered}
+            filtered_classes = [cls_name]
+            print(f"Combining {self.sets}-set classes. Total of {len(ann_files_filtered)} image pairs used.")
+        else:
+            filtered_classes = []
+            for cls, ann_f in ann_files_filtered_cls_dict.items():
+                if len(ann_f) > 0:
+                    filtered_classes.append(cls)
+                else:
+                    print(f"Excluding class {cls} from {self.sets}-set.")
+        return ann_files_filtered, ann_files_filtered_cls_dict, filtered_classes
+
+    def __diff_dict_to_str(self, diff):
+        diff_str = ""
+        keys = ["mirror", "viewpoint_variation", "scale_variation", "truncation", "occlusion"]
+        for key in keys:
+            if key in diff.keys():
+                diff_str += key
+                diff_str += str(diff[key])
+        return diff_str
+
+
+class IMC_PT_SparseGM:
+    r"""
+    Download and preprocess **IMC_PT_SparseGM** dataset.
+
+    :param sets: str, problem set, ``'train'`` for training set and ``'test'`` for testing set
+    :param obj_resize: tuple, resized image size
+    :param ds_dict: settings of dataset, containing at most 4 params(keys) for IMC_PT_SparseGM:
+
+        * **ROOT_DIR_IMG**: str, directory of images
+
+        * **ROOT_DIR_NPZ**: str, directory of annotations
+
+        * **CLASSES**: dict, classes of training and test data, keys: ``'train'`` for training and ``'test'`` for test
+
+        * **MAX_KPT_NUM**: int, maximum kpt_num in an image
+    """
+    def __init__(self, sets, obj_resize, **ds_dict):
+        assert sets in ('train', 'test'), 'No match found for dataset {}'.format(sets)
+        MAX_KPT_NUM = dataset_cfg.IMC_PT_SparseGM.MAX_KPT_NUM
+        CLASSES = dataset_cfg.IMC_PT_SparseGM.CLASSES
+        ROOT_DIR_NPZ = dataset_cfg.IMC_PT_SparseGM.ROOT_DIR_NPZ
+        ROOT_DIR_IMG = dataset_cfg.IMC_PT_SparseGM.ROOT_DIR_IMG
+        URL = 'https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=1Po9pRMWXTqKK2ABPpVmkcsOq-6K_2v-B'
+        if len(ds_dict.keys()) > 0:
+            if 'MAX_KPT_NUM' in ds_dict.keys():
+                MAX_KPT_NUM = ds_dict['MAX_KPT_NUM']
+            if 'CLASSES' in ds_dict.keys():
+                CLASSES = ds_dict['CLASSES']
+            if 'ROOT_DIR_NPZ' in ds_dict.keys():
+                ROOT_DIR_NPZ = ds_dict['ROOT_DIR_NPZ']
+            if 'ROOT_DIR_IMG' in ds_dict.keys():
+                ROOT_DIR_IMG = ds_dict['ROOT_DIR_IMG']
+            if 'URL' in ds_dict.keys():
+                URL = ds_dict['URL']
+
+        self.dataset_dir = 'data/IMC-PT-SparseGM'
+        if not os.path.exists(ROOT_DIR_IMG):
+            assert ROOT_DIR_IMG == dataset_cfg.IMC_PT_SparseGM.ROOT_DIR_IMG, 'you should not change ROOT_DIR_IMG or ROOT_DIR_NPZ unless the data have been manually downloaded'
+            assert ROOT_DIR_NPZ == dataset_cfg.IMC_PT_SparseGM.ROOT_DIR_NPZ, 'you should not change ROOT_DIR_IMG or ROOT_DIR_NPZ unless the data have been manually downloaded'
+            self.download(url=URL)
+
+        if not os.path.exists(self.dataset_dir):
+            os.makedirs(self.dataset_dir)
+        self.sets = sets
+        self.classes = CLASSES[sets]
+        self.class_dict = CLASSES
+        self.max_kpt_num = MAX_KPT_NUM
+        self.suffix = 'imcpt-' + str(MAX_KPT_NUM)
+
+        self.root_path_npz = Path(ROOT_DIR_NPZ)
+        self.root_path_img = Path(ROOT_DIR_IMG)
+        self.obj_resize = obj_resize
+
+        self.img_lists = [np.load(self.root_path_npz / cls / 'img_info.npz')['img_name'].tolist()
+                          for cls in self.classes]
+
+        self.process()
+
+    def download(self, url=None, retries=15):
+        r"""
+         Automatically download IMC_PT_SparseGM dataset.
+
+         :param url: str, web url of IMC_PT_SparseGM
+         """
+        if retries <= 0:
+            raise RuntimeError('Max Retries exceeded!')
+
+        dirs = 'data/'
+        if not os.path.exists(dirs):
+            os.makedirs(dirs)
+        print('Downloading dataset IMC-PT-SparseGM...')
+        filename = 'data/IMC-PT-SparseGM.tar.gz'
+        download(filename=filename, url=url, to_cache=False)
+        try:
+            tar = tarfile.open(filename, "r")
+        except tarfile.ReadError as err:
+            print('Warning: Content error. Retrying...\n', err)
+            os.remove(filename)
+            return self.download(url, retries - 1)
+
+        file_names = tar.getnames()
+        print('Unzipping files...')
+        sleep(0.5)
+        for file_name in tqdm(file_names):
+            tar.extract(file_name, "data/")
+        tar.close()
+        try:
+            os.remove(filename)
+        except PermissionError:
+            pass
+        return filename
+
+    def process(self):
+        r"""
+        Process the dataset and generate ``data-(size, size).json`` for preprocessed dataset, ``train.json`` for
+        training set, and ``test.json`` for testing set.
+        """
+        set_file = os.path.join(self.dataset_dir, self.sets + '.json')
+        img_file = os.path.join(self.dataset_dir, 'data-' + str(self.obj_resize) + '-' + self.suffix + '.json')
+
+        if not os.path.exists(set_file):
+            set_list = []
+            for _list in self.img_lists:
+                for img_name in _list:
+                    set_list.append(img_name.split('.')[0])
+            str1 = json.dumps(set_list)
+            f1 = open(set_file, 'w')
+            f1.write(str1)
+            f1.close()
+
+        if not os.path.exists(img_file):
+            total_cls = []
+            for cls in self.class_dict['train']:
+                total_cls.append(cls)
+            for cls in self.class_dict['test']:
+                total_cls.append(cls)
+
+            total_img_lists = [np.load(self.root_path_npz / cls / 'img_info.npz')['img_name'].tolist()
+                               for cls in total_cls]
+            data_dict = dict()
+            for i, _list in enumerate(total_img_lists):
+                cls = total_cls[i]
+                for img_name in _list:
+                    img_id = img_name.split('.')[0]
+                    anno_dict = self.__get_anno_dict(img_name, cls)
+                    data_dict[img_id] = anno_dict
+
+            str2 = json.dumps(data_dict)
+            f2 = open(img_file, 'w')
+            f2.write(str2)
+            f2.close()
+
+    def __get_anno_dict(self, img_name, cls):
+        """
+        Get an annotation dict from ``.npz`` annotation
+        """
+        img_file = self.root_path_img / cls / img_name
+        npz_file = self.root_path_npz / cls / (img_name.split('.')[0] + '.npz')
+
+        assert img_file.exists(), '{} does not exist.'.format(img_file)
+        assert npz_file.exists(), '{} does not exist.'.format(npz_file)
+
+        with Image.open(str(img_file)) as img:
+            ori_sizes = img.size
+            xmin = 0
+            ymin = 0
+            w = ori_sizes[0]
+            h = ori_sizes[1]
+
+        with np.load(str(npz_file)) as npz_anno:
+            kpts = npz_anno['points']
+            if len(kpts.shape) != 2:
+                ValueError('{} contains no keypoints.'.format(img_file))
+
+        keypoint_list = []
+        for i in range(kpts.shape[1]):
+            kpt_index = int(kpts[0, i])
+            assert kpt_index < self.max_kpt_num
+            attr = {
+                'labels': kpt_index,
+                'x': kpts[1, i] * self.obj_resize[0] / w,
+                'y': kpts[2, i] * self.obj_resize[1] / h
+            }
+            keypoint_list.append(attr)
+
+        anno_dict = dict()
+        anno_dict['path'] = str(img_file)
+        anno_dict['kpts'] = keypoint_list
+        anno_dict['bounds'] = [xmin, ymin, w, h]
+        anno_dict['cls'] = cls
+        anno_dict['univ_size'] = self.max_kpt_num
+
+        return anno_dict
+
+    def __len(self, cls):
+        if type(cls) == int:
+            cls = self.classes[cls]
+        assert cls in self.classes
+        return len(self.img_lists[self.classes.index(cls)])
+
+
+class CUB2011:
+    r"""
+    Download and preprocess **CUB2011** dataset.
+
+    :param sets: str, problem set, ``'train'`` for training set and ``'test'`` for testing set
+    :param obj_resize: tuple, resized image size
+    :param ds_dict: settings of dataset, containing at most 1 params(key) for CUB2011:
+
+        * **ROOT_DIR**: str, directory of data
+    """
+    def __init__(self, sets, obj_resize, **ds_dict):
+        CLS_SPLIT = dataset_cfg.CUB2011.CLASS_SPLIT
+        ROOT_DIR = dataset_cfg.CUB2011.ROOT_DIR
+        URL = 'https://drive.google.com/u/0/uc?export=download&confirm=B8eu&id=1hbzc_P1FuxMkcabkgn9ZKinBwW683j45'
+        if len(ds_dict.keys()) > 0:
+            if 'ROOT_DIR' in ds_dict.keys():
+                ROOT_DIR = ds_dict['ROOT_DIR']
+            if 'URL' in ds_dict.keys():
+                URL = ds_dict['URL']
+
+        self.set_data = {'train': [], 'test': []}
+        self.classes = []
+
+        self._set_pairs = {}
+        self._set_mask = {}
+        self.cls_split = CLS_SPLIT
+        self.suffix = 'cub2011'
+
+        self.rootpath = ROOT_DIR
+
+        self.dataset_dir = 'data/CUB_200_2011'
+        if not os.path.exists(ROOT_DIR):
+            assert ROOT_DIR == dataset_cfg.CUB2011.ROOT_DIR, 'you should not change ROOT_DIR unless the data have been manually downloaded'
+            self.download(url=URL)
+
+        if not os.path.exists(self.dataset_dir):
+            os.makedirs(self.dataset_dir)
+
+        with open(os.path.join(self.rootpath, 'images.txt')) as f:
+            self.im2fn = dict(l.rstrip('\n').split() for l in f.readlines())
+        with open(os.path.join(self.rootpath, 'train_test_split.txt')) as f:
+            train_split = dict(l.rstrip('\n').split() for l in f.readlines())
+        with open(os.path.join(self.rootpath, 'classes.txt')) as f:
+            classes = dict(l.rstrip('\n').split() for l in f.readlines())
+        with open(os.path.join(self.rootpath, 'image_class_labels.txt')) as f:
+            img2class = [l.rstrip('\n').split() for l in f.readlines()]
+            img_idxs, class_idxs = map(list, zip(*img2class))
+            class2img = self.__lists2dict_for_cub(class_idxs, img_idxs)
+        with open(os.path.join(self.rootpath, 'parts', 'part_locs.txt')) as f:
+            part_locs = [l.rstrip('\n').split() for l in f.readlines()]
+            fi, pi, x, y, v = map(list, zip(*part_locs))
+            self.im2kpts = self.__lists2dict_for_cub(fi, zip(pi, x, y, v))
+        with open(os.path.join(self.rootpath, 'bounding_boxes.txt')) as f:
+            bboxes = [l.rstrip('\n').split() for l in f.readlines()]
+            ii, x, y, w, h = map(list, zip(*bboxes))
+            self.im2bbox = dict(zip(ii, zip(x, y, w, h)))
+        if self.cls_split == 'ori':
+            for class_idx in sorted(classes):
+                self.classes.append(classes[class_idx])
+                train_set = []
+                test_set = []
+                for img_idx in class2img[class_idx]:
+                    if train_split[img_idx] == '1':
+                        train_set.append(img_idx)
+                    else:
+                        test_set.append(img_idx)
+                self.set_data['train'].append(train_set)
+                self.set_data['test'].append(test_set)
+        self.sets = sets
+        self.obj_resize = obj_resize
+
+        self.process()
+
+    def download(self, url=None, retries=50):
+        r"""
+         Automatically download CUB2011 dataset.
+
+         :param url: str, web url of CUB2011
+         """
+        if retries <= 0:
+            raise RuntimeError('Max Retries exceeded!')
+
+        dirs = 'data/'
+        if not os.path.exists(dirs):
+            os.makedirs(dirs)
+        print('Downloading dataset CUB2011...')
+        filename = 'data/CUB_200_2011.tgz'
+        download(filename=filename, url=url, to_cache=False)
+        try:
+            tar = tarfile.open(filename, "r")
+        except tarfile.ReadError as err:
+            print('Warning: Content error. Retrying...\n', err)
+            os.remove(filename)
+            return self.download(url, retries - 1)
+
+        file_names = tar.getnames()
+        print('Unzipping files...')
+        sleep(0.5)
+        for file_name in tqdm(file_names):
+            tar.extract(file_name, "data/")
+        tar.close()
+        try:
+            os.remove(filename)
+        except PermissionError:
+            pass
+        return filename
+
+    def process(self):
+        r"""
+        Process the dataset and generate ``data-(size, size).json`` for preprocessed dataset, ``train.json`` for
+        training set, and ``test.json`` for testing set.
+        """
+        set_file = os.path.join(self.dataset_dir, self.sets + '.json')
+        img_file = os.path.join(self.dataset_dir, 'data-' + str(self.obj_resize) + '-' + self.suffix + '.json')
+
+        if not os.path.exists(set_file):
+            set_list = []
+            set_img_idx_list = self.set_data[self.sets]
+            for cls_img_idx_list in set_img_idx_list:
+                for img_idx in cls_img_idx_list:
+                    img_name = self.im2fn[img_idx].split('/')[-1].split('.')[0]
+                    set_list.append(img_name)
+
+            str1 = json.dumps(set_list)
+            f1 = open(set_file, 'w')
+            f1.write(str1)
+            f1.close()
+
+        if not os.path.exists(img_file):
+            data_dict = dict()
+            for img_idx, img_name in self.im2fn.items():
+                cls = img_name.split('/')[0]
+                obj_id = img_name.split('/')[-1].split('.')[0]
+                obj_dict = self.__get_anno_dict(img_idx, cls)
+                data_dict[obj_id] = obj_dict
+
+            str2 = json.dumps(data_dict)
+            f2 = open(img_file, 'w')
+            f2.write(str2)
+            f2.close()
+
+    def __get_imgname(self, data):
+        return os.path.join(self.rootpath, 'images', self.im2fn[data])
+
+    def __get_meta(self, data):
+        pi, x, y, v = map(list, zip(*self.im2kpts[data]))
+        order = np.argsort(np.array(pi).astype(int))
+        keypts = np.array([np.array(x).astype('float')[order],
+                           np.array(y).astype('float')[order]])
+        visible = np.array(v).astype('uint8')[order]
+        bbox = np.array(self.im2bbox[data]).astype(float)
+        return keypts, visible, bbox
+
+    def __get_anno_dict(self, img_name, cls):
+        keypts, visible, bbox = self.__get_meta(img_name)
+        xmin, ymin, w, h = bbox
+        img_file = self.__get_imgname(img_name)
+        with Image.open(str(img_file)) as img:
+            xmin, xmax = np.clip((xmin, xmin + w), 0, img.size[0])
+            ymin, ymax = np.clip((ymin, ymin + h), 0, img.size[1])
+
+        keypoint_list = []
+        for keypt_idx in range(keypts.shape[1]):
+            if visible[keypt_idx]:
+                attr = dict()
+                attr['labels'] = keypt_idx
+                attr['x'] = (keypts[0, keypt_idx] - xmin) * self.obj_resize[0] / w
+                attr['y'] = (keypts[1, keypt_idx] - ymin) * self.obj_resize[1] / h
+                keypoint_list.append(attr)
+
+        anno_dict = dict()
+        anno_dict['path'] = img_file
+        anno_dict['kpts'] = keypoint_list
+        anno_dict['bounds'] = [xmin, ymin, xmax, ymax]
+        anno_dict['cls'] = cls
+        anno_dict['univ_size'] = 15
+
+        return anno_dict
+
+    @staticmethod
+    def __lists2dict_for_cub(keys, vals):
+        ans = {}
+        for idx, val_i in enumerate(vals):
+            if keys[idx] in ans:
+                ans[keys[idx]].append(val_i)
+            else:
+                ans[keys[idx]] = [val_i]
+        return ans
```

### Comparing `pygmtools-0.3.8/pygmtools/dataset_config.py` & `pygmtools-0.3.8a0/pygmtools/dataset_config.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,79 +1,79 @@
-# Copyright (c) 2022 Thinklab@SJTU
-# pygmtools is licensed under Mulan PSL v2.
-# You can use this software according to the terms and conditions of the Mulan PSL v2.
-# You may obtain a copy of Mulan PSL v2 at:
-# http://license.coscl.org.cn/MulanPSL2
-# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND,
-# EITHER EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT,
-# MERCHANTABILITY OR FIT FOR A PARTICULAR PURPOSE.
-# See the Mulan PSL v2 for more details.
-
-from easydict import EasyDict as edict
-
-__C = edict()
-
-dataset_cfg = __C
-# Pascal VOC 2011 dataset with keypoint annotations
-__C.PascalVOC = edict()
-__C.PascalVOC.KPT_ANNO_DIR = 'data/PascalVOC/annotations/'  # keypoint annotation
-__C.PascalVOC.ROOT_DIR = 'data/PascalVOC/TrainVal/VOCdevkit/VOC2011/'  # original VOC2011 dataset
-__C.PascalVOC.SET_SPLIT = 'data/PascalVOC/voc2011_pairs.npz'  # set split path
-__C.PascalVOC.CLASSES = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow',
-                         'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train',
-                         'tvmonitor']
-
-# Willow-Object Class dataset
-__C.WillowObject = edict()
-__C.WillowObject.ROOT_DIR = 'data/WillowObject/WILLOW-ObjectClass'
-__C.WillowObject.CLASSES = ['Car', 'Duck', 'Face', 'Motorbike', 'Winebottle']
-__C.WillowObject.KPT_LEN = 10
-__C.WillowObject.TRAIN_NUM = 20
-__C.WillowObject.SPLIT_OFFSET = 0
-__C.WillowObject.TRAIN_SAME_AS_TEST = False
-__C.WillowObject.RAND_OUTLIER = 0
-
-# CUB2011 dataset
-__C.CUB2011 = edict()
-__C.CUB2011.ROOT_DIR = 'data/CUB_200_2011'
-__C.CUB2011.CLASS_SPLIT = 'ori' # choose from 'ori' (original split), 'sup' (super class) or 'all' (all birds as one class), only support 'ori'
-
-# SWPair-71 Dataset
-__C.SPair = edict()
-__C.SPair.ROOT_DIR = "data/SPair-71k"
-__C.SPair.SIZE = "large"
-__C.SPair.CLASSES = [
-    "aeroplane",
-    "bicycle",
-    "bird",
-    "boat",
-    "bottle",
-    "bus",
-    "car",
-    "cat",
-    "chair",
-    "cow",
-    "dog",
-    "horse",
-    "motorbike",
-    "person",
-    "pottedplant",
-    "sheep",
-    "train",
-    "tvmonitor",
-]
-__C.SPair.TRAIN_DIFF_PARAMS = {}
-__C.SPair.EVAL_DIFF_PARAMS = {}
-__C.SPair.COMB_CLS = False
-
-# IMC_PT_SparseGM dataset
-__C.IMC_PT_SparseGM = edict()
-__C.IMC_PT_SparseGM.CLASSES = {'train': ['brandenburg_gate', 'buckingham_palace', 'colosseum_exterior',
-                                      'grand_place_brussels', 'hagia_sophia_interior', 'notre_dame_front_facade',
-                                      'palace_of_westminster', 'pantheon_exterior', 'prague_old_town_square',
-                                      'taj_mahal', 'temple_nara_japan', 'trevi_fountain', 'westminster_abbey'],
-                            'test': ['reichstag', 'sacre_coeur', 'st_peters_square']}
-__C.IMC_PT_SparseGM.ROOT_DIR_NPZ = 'data/IMC-PT-SparseGM/annotations'
-__C.IMC_PT_SparseGM.ROOT_DIR_IMG = 'data/IMC-PT-SparseGM/images'
-__C.IMC_PT_SparseGM.MAX_KPT_NUM = 50
-
-__C.CACHE_PATH = 'data/cache'
+# Copyright (c) 2022 Thinklab@SJTU
+# pygmtools is licensed under Mulan PSL v2.
+# You can use this software according to the terms and conditions of the Mulan PSL v2.
+# You may obtain a copy of Mulan PSL v2 at:
+# http://license.coscl.org.cn/MulanPSL2
+# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND,
+# EITHER EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT,
+# MERCHANTABILITY OR FIT FOR A PARTICULAR PURPOSE.
+# See the Mulan PSL v2 for more details.
+
+from easydict import EasyDict as edict
+
+__C = edict()
+
+dataset_cfg = __C
+# Pascal VOC 2011 dataset with keypoint annotations
+__C.PascalVOC = edict()
+__C.PascalVOC.KPT_ANNO_DIR = 'data/PascalVOC/annotations/'  # keypoint annotation
+__C.PascalVOC.ROOT_DIR = 'data/PascalVOC/TrainVal/VOCdevkit/VOC2011/'  # original VOC2011 dataset
+__C.PascalVOC.SET_SPLIT = 'data/PascalVOC/voc2011_pairs.npz'  # set split path
+__C.PascalVOC.CLASSES = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow',
+                         'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train',
+                         'tvmonitor']
+
+# Willow-Object Class dataset
+__C.WillowObject = edict()
+__C.WillowObject.ROOT_DIR = 'data/WillowObject/WILLOW-ObjectClass'
+__C.WillowObject.CLASSES = ['Car', 'Duck', 'Face', 'Motorbike', 'Winebottle']
+__C.WillowObject.KPT_LEN = 10
+__C.WillowObject.TRAIN_NUM = 20
+__C.WillowObject.SPLIT_OFFSET = 0
+__C.WillowObject.TRAIN_SAME_AS_TEST = False
+__C.WillowObject.RAND_OUTLIER = 0
+
+# CUB2011 dataset
+__C.CUB2011 = edict()
+__C.CUB2011.ROOT_DIR = 'data/CUB_200_2011'
+__C.CUB2011.CLASS_SPLIT = 'ori' # choose from 'ori' (original split), 'sup' (super class) or 'all' (all birds as one class), only support 'ori'
+
+# SWPair-71 Dataset
+__C.SPair = edict()
+__C.SPair.ROOT_DIR = "data/SPair-71k"
+__C.SPair.SIZE = "large"
+__C.SPair.CLASSES = [
+    "aeroplane",
+    "bicycle",
+    "bird",
+    "boat",
+    "bottle",
+    "bus",
+    "car",
+    "cat",
+    "chair",
+    "cow",
+    "dog",
+    "horse",
+    "motorbike",
+    "person",
+    "pottedplant",
+    "sheep",
+    "train",
+    "tvmonitor",
+]
+__C.SPair.TRAIN_DIFF_PARAMS = {}
+__C.SPair.EVAL_DIFF_PARAMS = {}
+__C.SPair.COMB_CLS = False
+
+# IMC_PT_SparseGM dataset
+__C.IMC_PT_SparseGM = edict()
+__C.IMC_PT_SparseGM.CLASSES = {'train': ['brandenburg_gate', 'buckingham_palace', 'colosseum_exterior',
+                                      'grand_place_brussels', 'hagia_sophia_interior', 'notre_dame_front_facade',
+                                      'palace_of_westminster', 'pantheon_exterior', 'prague_old_town_square',
+                                      'taj_mahal', 'temple_nara_japan', 'trevi_fountain', 'westminster_abbey'],
+                            'test': ['reichstag', 'sacre_coeur', 'st_peters_square']}
+__C.IMC_PT_SparseGM.ROOT_DIR_NPZ = 'data/IMC-PT-SparseGM/annotations'
+__C.IMC_PT_SparseGM.ROOT_DIR_IMG = 'data/IMC-PT-SparseGM/images'
+__C.IMC_PT_SparseGM.MAX_KPT_NUM = 50
+
+__C.CACHE_PATH = 'data/cache'
```

### Comparing `pygmtools-0.3.8/pygmtools/jittor_backend.py` & `pygmtools-0.3.8a0/pygmtools/jittor_backend.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,1449 +1,1449 @@
-# Copyright (c) 2022 Thinklab@SJTU
-# pygmtools is licensed under Mulan PSL v2.
-# You can use this software according to the terms and conditions of the Mulan PSL v2.
-# You may obtain a copy of Mulan PSL v2 at:
-# http://license.coscl.org.cn/MulanPSL2
-# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND,
-# EITHER EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT,
-# MERCHANTABILITY OR FIT FOR A PARTICULAR PURPOSE.
-# See the Mulan PSL v2 for more details.
-
-import numpy as np
-import jittor as jt
-from jittor import Var
-import functools
-import itertools
-import pygmtools.utils
-from multiprocessing import Pool
-import os
-from pygmtools.numpy_backend import _hung_kernel
-
-#############################################
-#     Linear Assignment Problem Solvers     #
-#############################################
-
-def hungarian(s: Var, n1: Var=None, n2: Var=None,
-              unmatch1: Var=None, unmatch2: Var=None,
-              nproc: int=1) -> Var:
-    """
-    Jittor implementation of Hungarian algorithm
-    """
-    batch_num = s.shape[0]
-
-    perm_mat = s.detach().numpy() * -1
-    if n1 is not None:
-        n1 = n1.numpy()
-    else:
-        n1 = [None] * batch_num
-    if n2 is not None:
-        n2 = n2.numpy()
-    else:
-        n2 = [None] * batch_num
-    if unmatch1 is not None:
-        unmatch1 = -unmatch1.numpy()
-    else:
-        unmatch1 = [None] * batch_num
-    if unmatch2 is not None:
-        unmatch2 = -unmatch2.numpy()
-    else:
-        unmatch2 = [None] * batch_num
-
-    if nproc > 1:
-        with Pool(processes=nproc) as pool:
-            mapresult = pool.starmap_async(_hung_kernel, zip(perm_mat, n1, n2, unmatch1, unmatch2))
-            perm_mat = np.stack(mapresult.get())
-    else:
-        perm_mat = np.stack([_hung_kernel(perm_mat[b], n1[b], n2[b], unmatch1[b], unmatch2[b]) for b in range(batch_num)])
-
-    perm_mat = jt.Var(perm_mat)
-
-    return perm_mat
-
-def sinkhorn(s: Var, nrows: Var=None, ncols: Var=None,
-             unmatchrows: Var=None, unmatchcols: Var=None,
-             dummy_row: bool=False, max_iter: int=10, tau: float=1., batched_operation: bool=False) -> Var:
-    """
-    Jittor implementation of Sinkhorn algorithm
-    """
-    batch_size = s.shape[0]
-
-    if s.shape[2] >= s.shape[1]:
-        transposed = False
-    else:
-        s = s.transpose(1, 2)
-        nrows, ncols = ncols, nrows
-        unmatchrows, unmatchcols = unmatchcols, unmatchrows
-        transposed = True
-
-    if nrows is None:
-        nrows = jt.Var([s.shape[1] for _ in range(batch_size)])
-    if ncols is None:
-        ncols = jt.Var([s.shape[2] for _ in range(batch_size)])
-
-    # ensure that in each dimension we have nrow < ncol
-    transposed_batch = nrows > ncols
-    if jt.any(transposed_batch):
-        s_t = s.transpose(1, 2)
-        s_t = jt.concat((
-            s_t[:, :s.shape[1], :],
-            jt.full((batch_size, s.shape[1], s.shape[2]-s.shape[1]), -float('inf'))), dim=2)
-        cond = transposed_batch.view(batch_size, 1, 1)
-        if cond.shape != s.shape:
-            cond = cond.expand(s.shape)
-        s = jt.where(cond, s_t, s)
-
-        new_nrows = jt.where(transposed_batch, ncols, nrows)
-        new_ncols = jt.where(transposed_batch, nrows, ncols)
-        nrows = new_nrows
-        ncols = new_ncols
-
-        if unmatchrows is not None and unmatchcols is not None:
-            unmatchrows_pad = jt.concat((
-                unmatchrows,
-                jt.full((batch_size, unmatchcols.shape[1]-unmatchrows.shape[1]), -float('inf'))), dim=1)
-            new_unmatchrows = jt.where(transposed_batch.view(batch_size, 1).expand(unmatchcols.shape), unmatchcols, unmatchrows_pad)[:, :unmatchrows.shape[1]]
-            new_unmatchcols = jt.where(transposed_batch.view(batch_size, 1).expand(unmatchcols.shape), unmatchrows_pad, unmatchcols)
-            unmatchrows = new_unmatchrows
-            unmatchcols = new_unmatchcols
-
-    # operations are performed on log_s
-    log_s = s / tau
-    if unmatchrows is not None and unmatchcols is not None:
-        unmatchrows = unmatchrows / tau
-        unmatchcols = unmatchcols / tau
-
-    if dummy_row:
-        assert log_s.shape[2] >= log_s.shape[1]
-        dummy_shape = list(log_s.shape)
-        dummy_shape[1] = log_s.shape[2] - log_s.shape[1]
-        ori_nrows = nrows
-        nrows = ncols.clone()
-        log_s = jt.concat((log_s, jt.full(dummy_shape, -float('inf'))), dim=1)
-        if unmatchrows is not None:
-            unmatchrows = jt.concat((unmatchrows, jt.full((dummy_shape[0], dummy_shape[1]), -float('inf'))), dim=1)
-        for b in range(batch_size):
-            log_s[b, int(ori_nrows[b]):int(nrows[b]), :int(ncols[b])] = -100
-
-    # assign the unmatch weights
-    if unmatchrows is not None and unmatchcols is not None:
-        new_log_s = jt.full((log_s.shape[0], log_s.shape[1]+1, log_s.shape[2]+1), -float('inf'))
-        new_log_s[:, :-1, :-1] = log_s
-        log_s = new_log_s
-        for b in range(batch_size):
-            r, c = int(nrows[b]), int(ncols[b])
-            log_s[b, 0:r, c] = unmatchrows[b, 0:r]
-            log_s[b, r, 0:c] = unmatchcols[b, 0:c]
-    row_mask = jt.zeros((batch_size, log_s.shape[1], 1), dtype=jt.bool)
-    col_mask = jt.zeros((batch_size, 1, log_s.shape[2]), dtype=jt.bool)
-    for b in range(batch_size):
-        r, c = int(nrows[b]), int(ncols[b])
-        row_mask[b, 0:r, 0] = 1
-        col_mask[b, 0, 0:c] = 1
-    if unmatchrows is not None and unmatchcols is not None:
-        ncols += 1
-        nrows += 1
-
-    if batched_operation:
-        for b in range(batch_size):
-            log_s[b, int(nrows[b]):, :] = -float('inf')
-            log_s[b, :, int(ncols[b]):] = -float('inf')
-
-        for i in range(max_iter):
-            if i % 2 == 0:
-                m = log_s.max(2, keepdims=True)  #optimized logsumexp
-                log_sum = jt.nn.logsumexp(log_s - m, 2, keepdim=True) + m
-                log_s = log_s - jt.where(row_mask, log_sum, jt.zeros_like(log_sum))
-                if jt.flags.use_cuda == 0:
-                    assert not jt.any(jt.isnan(log_s))
-            else:
-                m = log_s.max(1, keepdims=True)
-                log_sum = jt.nn.logsumexp(log_s - m, 1, keepdim=True) + m                
-                log_s = log_s - jt.where(col_mask, log_sum, jt.zeros_like(log_sum))
-                if jt.flags.use_cuda == 0:
-                    assert not jt.any(jt.isnan(log_s))
-
-        ret_log_s = log_s
-    else:
-        ret_log_s = jt.full((batch_size, log_s.shape[1], log_s.shape[2]), -float('inf'), dtype=log_s.dtype)
-        for b in range(batch_size):
-            r,c = nrows[b],ncols[b]
-            if not isinstance(nrows[b],int):
-                r = int(nrows[b].item())
-            if not isinstance(ncols[b],int):
-                c = int(ncols[b].item())
-            log_s_b = log_s[b, 0:r, 0:c]
-            row_mask_b = row_mask[b, 0:r, :]
-            col_mask_b = col_mask[b, :, 0:c]
-            for i in range(max_iter):
-                if i % 2 == 0:
-                    m = log_s_b.max(1, keepdims=True)
-                    log_sum = jt.nn.logsumexp(log_s_b - m, 1, keepdim=True) + m
-                    log_s_b = log_s_b - jt.where(row_mask_b, log_sum, jt.zeros_like(log_sum))
-                else:
-                    m = log_s_b.max(0, keepdims=True)
-                    log_sum = jt.nn.logsumexp(log_s_b - m, 0, keepdim=True) + m
-                    log_s_b = log_s_b - jt.where(col_mask_b, log_sum, jt.zeros_like(log_sum))
-            ret_log_s[b, 0:r, 0:c] = log_s_b
-
-    if unmatchrows is not None and unmatchcols is not None:
-        ncols -= 1
-        nrows -= 1
-        for b in range(batch_size):
-            r, c = int(nrows[b]), int(ncols[b])
-            ret_log_s[b, 0:r+1, c] = -float('inf')
-            ret_log_s[b, r, 0:c] = -float('inf')
-        ret_log_s = ret_log_s[:, :-1, :-1]
-
-    if dummy_row:
-        if dummy_shape[1] > 0:
-            ret_log_s = ret_log_s[:, :-dummy_shape[1]]
-        for b in range(batch_size):
-            ret_log_s[int(b), int(ori_nrows[b]):int(nrows[b]), :int(ncols[b])] = -float('inf')
-
-    if jt.any(transposed_batch):
-        s_t = ret_log_s.transpose(1, 2)
-        s_t = jt.concat((
-            s_t[:, :ret_log_s.shape[1], :],
-            jt.full((batch_size, ret_log_s.shape[1], ret_log_s.shape[2]-ret_log_s.shape[1]), -float('inf'))), dim=2)
-        cond = transposed_batch.view(batch_size, 1, 1)
-        if cond.shape != s_t.shape:
-            cond = cond.expand(s_t.shape)
-        ret_log_s = jt.where(cond, s_t, ret_log_s)    
-
-    if transposed:
-        ret_log_s = ret_log_s.transpose(1, 2)
-
-    return jt.exp(ret_log_s)
-
-#############################################
-#    Quadratic Assignment Problem Solvers   #
-#############################################
-
-def rrwm(K: Var, n1: Var, n2: Var, n1max, n2max, x0: Var,
-         max_iter: int, sk_iter: int, alpha: float, beta: float) -> Var:
-    """
-    Jittor implementation of RRWM algorithm.
-    """
-    batch_num, n1, n2, n1max, n2max, n1n2, v0 = _check_and_init_gm(K, n1, n2, n1max, n2max, x0)
-    # rescale the values in K
-    d = K.sum(dim=2, keepdims=True)
-    dmax = d.max(dim=1, keepdims=True) 
-    K = K / (dmax + d.min() * 1e-5)
-    v = v0
-    for i in range(max_iter):
-        # try fixing memory error caused by growing scale of
-        # computation graph when testing multi-graph solvers
-        if jt.number_of_lived_ops() > 100000:
-            jt.clean_graph() 
-            
-        # random walk
-        v = jt.bmm(K, v)
-        last_v = v
-        n = jt.norm(v, p=1, dim=1, keepdim=True)
-        v = v / n
-
-        # reweighted jump
-        s = v.view((batch_num, int(n2max), int(n1max))).transpose(1, 2)
-        s = beta * s / s.max(dim=1, keepdims=True).max(dim=2, keepdims=True)
-        v = alpha * sinkhorn(s, n1, n2, max_iter=sk_iter).transpose(1, 2).reshape(batch_num, n1n2, 1) + \
-            (1 - alpha) * v
-        n = jt.norm(v, p=1, dim=1, keepdim=True)
-        v = jt.matmul(v, 1 / n)
-        if (v - last_v).sum().sqrt() < 1e-5:
-            break
-
-    return v.view((batch_num, int(n2max), int(n1max))).transpose(1, 2)
-
-def sm(K: Var, n1: Var, n2: Var, n1max, n2max, x0: Var,
-        max_iter: int) -> Var:
-    """
-    Jittor implementation of SM algorithm.
-    """
-    batch_num, n1, n2, n1max, n2max, n1n2, v0 = _check_and_init_gm(K, n1, n2, n1max, n2max, x0)
-    v = vlast = v0
-    for i in range(max_iter):
-        v = jt.bmm(K, v)
-        n = jt.norm(v, p=2, dim=1)
-        v = jt.matmul(v, (1 / n).reshape(batch_num, 1, 1))
-        if (v - vlast).sum().sqrt() < 1e-5:
-            break        
-        vlast = v
-
-    x = v.reshape(batch_num, n2max, n1max).transpose(1,2)
-    return x
-
-def ipfp(K: Var, n1: Var, n2: Var, n1max, n2max, x0: Var,
-         max_iter) -> Var:
-    """
-    Jittor implementation of IPFP algorithm
-    """
-    batch_num, n1, n2, n1max, n2max, n1n2, v0 = _check_and_init_gm(K, n1, n2, n1max, n2max, x0)
-    v = v0
-    last_v = v
-
-    def comp_obj_score(v1, K, v2):
-        return jt.bmm(jt.bmm(v1.view(batch_num, 1, -1), K), v2)
-
-    for i in range(max_iter):
-        cost = jt.bmm(K, v).reshape((batch_num, int(n2max), int(n1max))).transpose(1, 2)
-        binary_sol = hungarian(cost, n1, n2)
-        binary_v = binary_sol.transpose(1, 2).view(batch_num, -1, 1)
-        alpha = comp_obj_score(v, K, binary_v - v)
-        beta = comp_obj_score(binary_v - v, K, binary_v - v)
-        t0 = alpha / beta
-        cond = jt.logical_or(beta <= 0, t0 >= 1)
-        if cond.shape != binary_v.shape:
-            cond = cond.expand(binary_v.shape)
-        v = jt.where(cond, binary_v, v + t0 * (binary_v - v))
-        last_v_sol = comp_obj_score(last_v, K, last_v)
-        if jt.max(jt.abs(
-                last_v_sol - jt.bmm(cost.reshape(batch_num, 1, -1), binary_sol.reshape(batch_num, -1, 1))
-        ) / last_v_sol) < 1e-3:
-            break
-        last_v = v
-
-    pred_x = binary_sol
-    return pred_x
-
-
-############################################
-#      Multi-Graph Matching Solvers        #
-############################################
-
-
-def cao_solver(K, X, num_graph, num_node, max_iter, lambda_init, lambda_step, lambda_max, iter_boost):
-    r"""
-    Jittor implementation of CAO solver (mode="c")
-
-    :param K: affinity matrix, (m, m, n*n, n*n)
-    :param X: initial matching, (m, m, n, n)
-    :param num_graph: number of graphs, int
-    :param num_node: number of nodes, int
-    :return: X, (m, m, n, n)
-    """
-    m, n = num_graph, num_node
-    param_lambda = lambda_init
-
-    def _comp_aff_score(x, k):
-        return pygmtools.utils.compute_affinity_score(x, k, backend='jittor').unsqueeze(-1).unsqueeze(-1)
-
-    for iter in range(max_iter):
-        if iter >= iter_boost:
-            param_lambda = np.min([param_lambda * lambda_step, lambda_max])
-        # pair_con = get_batch_pc_opt(X)
-        pair_aff = _comp_aff_score(X.reshape(-1, n, n), K.reshape(-1, n * n, n * n)).reshape(m, m)
-        pair_aff = pair_aff - jt.init.eye(m) * pair_aff
-        norm = jt.max(pair_aff)
-        for i in range(m):
-            for j in range(m):
-                if i >= j:
-                    continue
-                aff_ori = _comp_aff_score(X[i, j], K[i, j]) / norm
-                con_ori = _get_single_pc_opt(X, i, j)
-                # con_ori = jt.sqrt(pair_con[i, j])
-                if iter < iter_boost:
-                    score_ori = aff_ori
-                else:
-                    score_ori = aff_ori * (1 - param_lambda) + con_ori * param_lambda
-                X_upt = X[i, j]
-                for k in range(m):
-                    X_combo = jt.matmul(X[i, k], X[k, j])
-                    aff_combo = _comp_aff_score(X_combo, K[i, j]) / norm
-                    con_combo = _get_single_pc_opt(X, i, j, X_combo)
-                    # con_combo = jt.sqrt(pair_con[i, k] * pair_con[k, j])
-                    if iter < iter_boost:
-                        score_combo = aff_combo
-                    else:
-                        score_combo = aff_combo * (1 - param_lambda) + con_combo * param_lambda
-                    if score_combo > score_ori:
-                        X_upt = X_combo
-                X[i, j] = X_upt
-                X[j, i] = X_upt.transpose(0, 1)
-    return X
-
-
-def cao_fast_solver(K, X, num_graph, num_node, max_iter, lambda_init, lambda_step, lambda_max, iter_boost):
-    r"""
-    Jittor implementation of CAO solver in fast config (mode="pc")
-
-    :param K: affinity matrix, (m, m, n*n, n*n)
-    :param X: initial matching, (m, m, n, n)
-    :param num_graph: number of graphs, int
-    :param num_node: number of nodes, int
-    :return: X, (m, m, n, n)
-    """
-    m, n = num_graph, num_node
-    param_lambda = lambda_init
-
-    def _comp_aff_score(x, k):
-        return pygmtools.utils.compute_affinity_score(x, k, backend='jittor').unsqueeze(-1).unsqueeze(-1)
-
-    mask1 = jt.arange(m).reshape(m, 1).repeat(1, m)
-    mask2 = jt.arange(m).reshape(1, m).repeat(m, 1)
-    mask = (mask1 < mask2).float()
-    X_mask = mask.reshape(m, m, 1, 1)
-
-    for iter in range(max_iter):
-        if iter >= iter_boost:
-            param_lambda = np.min([param_lambda * lambda_step, lambda_max])
-
-        pair_aff = _comp_aff_score(X.reshape(-1, n, n), K.reshape(-1, n * n, n * n)).reshape(m, m)
-        pair_aff = pair_aff - jt.init.eye(m) * pair_aff
-        norm = jt.max(pair_aff)
-
-        X1 = X.reshape(m, 1, m, n, n).repeat(1, m, 1, 1, 1).reshape(-1, n, n)  # X1[i,j,k] = X[i,k]
-        X2 = X.reshape(1, m, m, n, n).repeat(m, 1, 1, 1, 1).transpose(1, 2).reshape(-1, n, n)  # X2[i,j,k] = X[k,j]
-        X_combo = jt.bmm(X1, X2).reshape(m, m, m, n, n) # X_combo[i,j,k] = X[i, k] * X[k, j]
-
-        aff_ori = (_comp_aff_score(X.reshape(-1, n, n), K.reshape(-1, n * n, n * n)) / norm).reshape(m, m)
-        pair_con = _get_batch_pc_opt(X)
-        con_ori = jt.sqrt(pair_con)
-
-        K_repeat = K.reshape(m, m, 1, n * n, n * n).repeat(1, 1, m, 1, 1).reshape(-1, n * n, n * n)
-        aff_combo = (_comp_aff_score(X_combo.reshape(-1, n, n), K_repeat) / norm).reshape(m, m, m)
-        con1 = pair_con.reshape(m, 1, m).repeat(1, m, 1)  # con1[i,j,k] = pair_con[i,k]
-        con2 = pair_con.reshape(1, m, m).repeat(m, 1, 1).transpose(1, 2)  # con2[i,j,k] = pair_con[j,k]
-        con_combo = jt.sqrt(con1 * con2)
-
-        if iter < iter_boost:
-            score_ori = aff_ori
-            score_combo = aff_combo
-        else:
-            score_ori = aff_ori * (1 - param_lambda) + con_ori * param_lambda
-            score_combo = aff_combo * (1 - param_lambda) + con_combo * param_lambda
-
-        score_combo_max = jt.max(score_combo, dim=-1)
-        idx = []
-        for i in range(score_combo.shape[0]):
-            idx.append([])
-            for j in range(score_combo.shape[1]):
-                ix = jt.where(score_combo[i][j]==score_combo_max[i][j])[0]
-                idx[i].append(ix[0].item() if ix.shape[0]>1 else ix.item())
-        idx = jt.Var(idx)
-
-        assert jt.all(score_combo_max + 1e-4 >= score_ori), jt.min(score_combo_max - score_ori)
-        X_upt = X_combo[mask1, mask2, idx, :, :]
-        X = X_upt * X_mask + X_upt.transpose(0, 1).transpose(2, 3) * X_mask.transpose(0, 1) + X * (1 - X_mask - X_mask.transpose(0, 1))
-        assert jt.all(X.transpose(0, 1).transpose(2, 3) == X)
-    return X
-
-
-def mgm_floyd_solver(K, X, num_graph, num_node, param_lambda):
-    m, n = num_graph, num_node
-
-    def _comp_aff_score(x, k):
-        return pygmtools.utils.compute_affinity_score(x, k, backend='jittor').unsqueeze(-1).unsqueeze(-1)
-
-    for k in range(m):
-        pair_aff = _comp_aff_score(X.reshape(-1, n, n), K.reshape(-1, n * n, n * n)).reshape(m, m)
-        pair_aff = pair_aff - jt.init.eye(m) * pair_aff
-        norm = jt.max(pair_aff)
-
-        # print("iter:{} aff:{:.4f} con:{:.4f}".format(
-        #     k, jt.mean(pair_aff).item(), jt.mean(get_batch_pc_opt(X)).item()
-        # ))
-
-        for i in range(m):
-            for j in range(m):
-                if i >= j:
-                    continue
-                score_ori = _comp_aff_score(X[i, j], K[i, j]) / norm
-                X_combo = jt.matmul(X[i, k], X[k, j])
-                score_combo = _comp_aff_score(X_combo, K[i, j]) / norm
-
-                if score_combo > score_ori:
-                    X[i, j] = X_combo
-                    X[j, i] = X_combo.transpose(0, 1)
-
-    for k in range(m):
-        pair_aff = _comp_aff_score(X.reshape(-1, n, n), K.reshape(-1, n * n, n * n)).reshape(m, m)
-        pair_aff = pair_aff - jt.init.eye(m) * pair_aff
-        norm = jt.max(pair_aff)
-
-        pair_con = _get_batch_pc_opt(X)
-        for i in range(m):
-            for j in range(m):
-                if i >= j:
-                    continue
-                aff_ori = _comp_aff_score(X[i, j], K[i, j]) / norm
-                con_ori = _get_single_pc_opt(X, i, j)
-                # con_ori = jt.sqrt(pair_con[i, j])
-                score_ori = aff_ori * (1 - param_lambda) + con_ori * param_lambda
-
-                X_combo = jt.matmul(X[i, k], X[k, j])
-                aff_combo = _comp_aff_score(X_combo, K[i, j]) / norm
-                con_combo = _get_single_pc_opt(X, i, j, X_combo)
-                # con_combo = jt.sqrt(pair_con[i, k] * pair_con[k, j])
-                score_combo = aff_combo * (1 - param_lambda) + con_combo * param_lambda
-
-                if score_combo > score_ori:
-                    X[i, j] = X_combo
-                    X[j, i] = X_combo.transpose(0, 1)
-    return X
-
-
-def mgm_floyd_fast_solver(K, X, num_graph, num_node, param_lambda):
-    m, n = num_graph, num_node
-
-    def _comp_aff_score(x, k):
-        return pygmtools.utils.compute_affinity_score(x, k, backend='jittor').unsqueeze(-1).unsqueeze(-1)
-
-    mask1 = jt.arange(m).reshape(m, 1).repeat(1, m)
-    mask2 = jt.arange(m).reshape(1, m).repeat(m, 1)
-    mask = (mask1 < mask2).float()
-    X_mask = mask.reshape(m, m, 1, 1)
-
-    for k in range(m):
-        pair_aff = _comp_aff_score(X.reshape(-1, n, n), K.reshape(-1, n * n, n * n)).reshape(m, m)
-        pair_aff = pair_aff - jt.init.eye(m) * pair_aff
-        norm = jt.max(pair_aff)
-
-        # print("iter:{} aff:{:.4f} con:{:.4f}".format(
-        #     k, jt.mean(pair_aff).item(), jt.mean(get_batch_pc_opt(X)).item()
-        # ))
-
-        X1 = X[:, k].reshape(m, 1, n, n).repeat(1, m, 1, 1).reshape(-1, n, n)  # X[i, j] = X[i, k]
-        X2 = X[k, :].reshape(1, m, n, n).repeat(m, 1, 1, 1).reshape(-1, n, n)  # X[i, j] = X[j, k]
-        X_combo = jt.bmm(X1, X2).reshape(m, m, n, n)
-
-        aff_ori = (_comp_aff_score(X.reshape(-1, n, n), K.reshape(-1, n * n, n * n)) / norm).reshape(m, m)
-        aff_combo = (_comp_aff_score(X_combo.reshape(-1, n, n), K.reshape(-1, n * n, n * n)) / norm).reshape(m, m)
-
-        score_ori = aff_ori
-        score_combo = aff_combo
-
-        upt = (score_ori < score_combo).float()
-        upt = (upt * mask).reshape(m, m, 1, 1)
-        X = X * (1.0 - upt) + X_combo * upt
-        X = X * X_mask + X.transpose(0, 1).transpose(2, 3) * (1 - X_mask)
-
-    for k in range(m):
-        pair_aff = _comp_aff_score(X.reshape(-1, n, n), K.reshape(-1, n * n, n * n)).reshape(m, m)
-        pair_aff = pair_aff - jt.init.eye(m) * pair_aff
-        norm = jt.max(pair_aff)
-
-        pair_con = _get_batch_pc_opt(X)
-
-        X1 = X[:, k].reshape(m, 1, n, n).repeat(1, m, 1, 1).reshape(-1, n, n)  # X[i, j] = X[i, k]
-        X2 = X[k, :].reshape(1, m, n, n).repeat(m, 1, 1, 1).reshape(-1, n, n)  # X[i, j] = X[j, k]
-        X_combo = jt.bmm(X1, X2).reshape(m, m, n, n)
-
-        aff_ori = (_comp_aff_score(X.reshape(-1, n, n), K.reshape(-1, n * n, n * n)) / norm).reshape(m, m)
-        aff_combo = (_comp_aff_score(X_combo.reshape(-1, n, n), K.reshape(-1, n * n, n * n)) / norm).reshape(m, m)
-
-        con_ori = jt.sqrt(pair_con)
-        con1 = pair_con[:, k].reshape(m, 1).repeat(1, m)
-        con2 = pair_con[k, :].reshape(1, m).repeat(m, 1)
-        con_combo = jt.sqrt(con1 * con2)
-
-        score_ori = aff_ori * (1 - param_lambda) + con_ori * param_lambda
-        score_combo = aff_combo * (1 - param_lambda) + con_combo * param_lambda
-
-        upt = (score_ori < score_combo).float()
-        upt = (upt * mask).reshape(m, m, 1, 1)
-        X = X * (1.0 - upt) + X_combo * upt
-        X = X * X_mask + X.transpose(0, 1).transpose(2, 3) * (1 - X_mask)
-    return X
-
-
-def _get_single_pc_opt(X, i, j, Xij=None):
-    """
-    CAO/Floyd helper function (compute consistency)
-    :param X: (m, m, n, n) all the matching results
-    :param i: index
-    :param j: index
-    :return: the consistency of X_ij
-    """
-    m, _, n, _ = X.size()
-    if Xij is None:
-        Xij = X[i, j]
-    X1 = X[i, :].reshape(-1, n, n)
-    X2 = X[:, j].reshape(-1, n, n)
-    X_combo = jt.bmm(X1, X2)
-    pair_con = 1 - jt.sum(jt.abs(Xij - X_combo)) / (2 * n * m)
-    return pair_con
-
-
-def _get_batch_pc_opt(X):
-    """
-    CAO/Floyd-fast helper function (compute consistency in batch)
-    :param X: (m, m, n, n) all the matching results
-    :return: (m, m) the consistency of X
-    """
-    m, _, n, _ = X.size()
-    X1 = X.reshape(m, 1, m, n, n).repeat(1, m, 1, 1, 1).reshape(-1, n, n)  # X1[i, j, k] = X[i, k]
-    X2 = X.reshape(1, m, m, n, n).repeat(m, 1, 1, 1, 1).transpose(1, 2).reshape(-1, n, n)  # X2[i, j, k] = X[k, j]
-    X_combo = jt.bmm(X1, X2).reshape(m, m, m, n, n)
-    X_ori = X.reshape(m, m, 1, n, n).repeat(1, 1, m, 1, 1)
-    pair_con = 1 - jt.sum(jt.abs(X_combo - X_ori), dims=(2, 3, 4)) / (2 * n * m)
-    return pair_con
-
-
-def gamgm(
-        A, W, ns, n_univ, U0,
-        init_tau, min_tau, sk_gamma,
-        sk_iter, max_iter, quad_weight,
-        converge_thresh, outlier_thresh, bb_smooth,
-        verbose,
-        cluster_M=None, projector='sinkhorn', hung_iter=True # these arguments are reserved for clustering
-):
-    """
-    Jittor implementation of Graduated Assignment for Multi-Graph Matching (with compatibility for 2GM and clustering)
-    """
-
-    num_graphs = A.shape[0]
-    if ns is None:
-        ns = jt.full((num_graphs,), A.shape[1], dtype=jt.int)
-    n_indices = jt.cumsum(ns, dim=0)
-
-    # build a super adjacency matrix A
-    supA = jt.zeros((n_indices[-1].item(), n_indices[-1].item()))
-    for i in range(num_graphs):
-        start_n = (n_indices[i] - ns[i]).item()
-        end_n = n_indices[i].item()
-        supA[start_n:end_n, start_n:end_n] = A[i, :ns[i].item(), :ns[i].item()]
-
-    # handle the type of n_univ
-    if type(n_univ) is jt.Var:
-        n_univ = n_univ.item()
-
-    # randomly init U
-    if U0 is None:
-        U0 = jt.full((n_indices[-1].item(), n_univ), 1 / n_univ)
-        U0 += jt.randn_like(U0) / 1000
-
-    # init cluster_M if not given
-    if cluster_M is None:
-        cluster_M = jt.ones((num_graphs, num_graphs))
-
-    # reshape W into supW
-    supW = jt.zeros((n_indices[-1].item(), n_indices[-1].item()))
-    for i, j in itertools.product(range(num_graphs), repeat=2):
-        start_x = (n_indices[i] - ns[i]).item()
-        end_x = n_indices[i].item()
-        start_y = (n_indices[j] - ns[j]).item()
-        end_y = n_indices[j].item()
-        supW[start_x:end_x, start_y:end_y] = W[i, j, :ns[i].item(), :ns[j].item()]
-
-    U = GAMGMJittorFunc.apply(
-        bb_smooth,
-        supA, supW, ns, n_indices, n_univ, num_graphs, U0,
-        init_tau, min_tau, sk_gamma,
-        sk_iter, max_iter, quad_weight,
-        converge_thresh, outlier_thresh,
-        verbose,
-        cluster_M, projector, hung_iter
-    )
-
-    # build MultiMatchingResult
-    result = pygmtools.utils.MultiMatchingResult(True, 'jittor')
-
-    for i in range(num_graphs):
-        start_n = n_indices[i] - ns[i]
-        end_n = n_indices[i]
-        result[i] = U[start_n.item():end_n.item()]
-
-    return result
-
-
-class GAMGMJittorFunc(jt.Function):
-    """
-    Jittor wrapper to support forward and backward pass (by black-box differentiation)
-    """
-
-    def execute(self, bb_smooth, supA, supW, ns, n_indices, n_univ, num_graphs, U0, *args):
-        # save parameters
-        self.bb_smooth = bb_smooth
-        self.named_args = supA, supW, ns, n_indices, n_univ, num_graphs, U0
-        self.list_args = args
-
-        # real solver function
-        U = gamgm_real(supA, supW, ns, n_indices, n_univ, num_graphs, U0, *args)
-
-        # save result
-        self.U = U
-        return U
-
-    def grad(self, dU):
-        epsilon = 1e-8
-        bb_smooth = self.bb_smooth
-        supA, supW, ns, n_indices, n_univ, num_graphs, U0 = self.named_args
-        args = self.list_args
-        U = self.U
-
-        for i, j in itertools.product(range(num_graphs), repeat=2):
-            start_x = (n_indices[i] - ns[i]).item()
-            end_x = n_indices[i].item()
-            start_y = (n_indices[j] - ns[j]).item()
-            end_y = n_indices[j].item()
-            supW[start_x:end_x, start_y:end_y] += bb_smooth * jt.matmul(dU[start_x:end_x], dU[start_y:end_y].transpose(0, 1))
-
-        U_prime = gamgm_real(supA, supW, ns, n_indices, n_univ, num_graphs, U0, *args)
-
-        grad_supW = jt.zeros((n_indices[-1].item(), n_indices[-1].item()))
-        for i, j in itertools.product(range(num_graphs), repeat=2):
-            start_x = (n_indices[i] - ns[i]).item()
-            end_x = n_indices[i].item()
-            start_y = (n_indices[j] - ns[j]).item()
-            end_y = n_indices[j].item()
-            X = jt.matmul(U[start_x:end_x], U[start_y:end_y].transpose(0, 1))
-            X_prime = jt.matmul(U_prime[start_x:end_x], U_prime[start_y:end_y].transpose(0, 1))
-            grad_supW[start_x:end_x, start_y:end_y] = -(X - X_prime) / (bb_smooth + epsilon)
-
-        return_list = [None, None, grad_supW] + [None] * (len(args) + 8 - 3)
-        return tuple(return_list)
-
-
-def gamgm_real(
-        supA, supW, ns, n_indices, n_univ, num_graphs, U0,
-        init_tau, min_tau, sk_gamma,
-        sk_iter, max_iter, quad_weight,
-        converge_thresh, outlier_thresh,
-        verbose,
-        cluster_M, projector, hung_iter # these arguments are reserved for clustering
-        ):
-    """
-    The real forward function of GAMGM
-    """
-    U = U0
-    sinkhorn_tau = init_tau
-    iter_flag = True
-
-    while iter_flag:
-        with jt.enable_grad():
-            for i in range(max_iter):
-                # compact matrix form update of V
-                UUt = jt.matmul(U, U.t())
-                lastUUt = UUt
-                # jittor does not accept array as the second parameter of repeat_interleave and jittor Var is based on numpy array
-                import numpy as np
-                cluster_weight = jt.Var(np.repeat(cluster_M, ns.long().data, axis=0))
-                cluster_weight = jt.Var(np.repeat(cluster_weight, ns.long().data, axis=1))
-                quad, chains = supA, [UUt * cluster_weight, supA, U]
-                for matrix in chains:
-                    quad = jt.matmul(quad, matrix)
-                quad *= (quad_weight * 2)
-                # quad = jt.chain_matmul(supA, UUt * cluster_weight, supA, U) * quad_weight * 2
-                unary = jt.matmul(supW * cluster_weight, U)
-                if verbose:
-                    if projector == 'sinkhorn':
-                        print_str = f'tau={sinkhorn_tau:.3e}'
-                    else:
-                        print_str = 'hungarian'
-                    print(print_str + f' #iter={i}/{max_iter} '
-                        f'quad score: {(quad * U).sum():.3e}, unary score: {(unary * U).sum():.3e}')
-                V = (quad + unary) / num_graphs
-
-                U_list = []
-                if projector == 'hungarian':
-                    n_start = 0
-                    for n_end in n_indices:
-                        if isinstance(n_start, Var):
-                            n_start = n_start.item()
-                        U_list.append(pygmtools.hungarian(V[n_start:n_end.item(), :n_univ], backend='jittor'))
-                        n_start = n_end
-                elif projector == 'sinkhorn':
-                    if jt.all(ns == ns[0]):
-                        if ns[0] <= n_univ:
-                            U_list.append(
-                                sinkhorn(
-                                    V.reshape(num_graphs, -1, n_univ),
-                                    max_iter=sk_iter, tau=sinkhorn_tau, batched_operation=True, dummy_row=True
-                                ).reshape(-1, n_univ))
-                        else:
-                            U_list.append(
-                                sinkhorn(
-                                    V.reshape(num_graphs, -1, n_univ).transpose(1, 2),
-                                    max_iter=sk_iter, tau=sinkhorn_tau, batched_operation=True, dummy_row=True
-                                ).transpose(1, 2).reshape(-1, n_univ))
-                    else:
-                        V_list = []
-                        n1 = []
-                        n_start = 0
-                        for n_end in n_indices:
-                            if isinstance(n_start, Var):
-                                n_start = n_start.item()
-                            V_list.append(V[n_start:n_end.item(), :n_univ])
-                            n1.append(n_end.item() - n_start)
-                            n_start = n_end
-                        V_batch = build_batch(V_list)
-                        n1 = jt.Var(n1)
-                        U = sinkhorn(V_batch, n1,
-                                    max_iter=sk_iter, tau=sinkhorn_tau, batched_operation=True, dummy_row=True)
-                        n_start = 0
-                        for idx, n_end in enumerate(n_indices):
-                            U_list.append(U[idx, :n_end.item() - n_start, :])
-                            n_start = n_end.item()
-                else:
-                    raise NameError('Unknown projecter name: {}'.format(projector))
-
-                U = jt.concat(U_list, dim=0)
-                if num_graphs == 2:
-                    U[:ns[0], :] = jt.init.eye(ns[0], n_univ)
-
-                # calculate gap to discrete
-                if projector == 'sinkhorn' and verbose:
-                    U_list_hung = []
-                    n_start = 0
-                    for n_end in n_indices:
-                        n_end = n_end.item()
-                        U_list_hung.append(pygmtools.hungarian(V[n_start:n_end, :n_univ], backend='jittor'))
-                        n_start = n_end
-                    U_hung = jt.concat(U_list_hung, dim=0)
-                    diff = jt.norm(jt.matmul(U, U.t()) - lastUUt).sum()
-                    print(f'tau={sinkhorn_tau:.3e} #iter={i}/{max_iter} '
-                          f'gap to discrete: {jt.mean(jt.abs(U - U_hung)).item():.3e}, '
-                          f'iter diff: {diff.item():.3e}')
-
-                if projector == 'hungarian' and outlier_thresh > 0:
-                    U_hung = U
-                    UUt = jt.matmul(U_hung, U_hung.t())
-                    cluster_weight = jt.Var(np.repeat(cluster_M, ns.long().data, axis=0))
-                    cluster_weight = jt.Var(np.repeat(cluster_weight, ns.long().data, axis=1))
-                    quad, chains = supA, [UUt * cluster_weight, supA, U_hung]
-                    for matrix in chains:
-                        quad = jt.matmul(quad, matrix)
-                    quad *= (quad_weight * 2)
-                    unary = jt.matmul(supW * cluster_weight, U_hung)
-                    max_vals = (unary + quad).max(dim=1)
-                    U = U * (unary + quad > outlier_thresh)
-                    if verbose:
-                        print(f'hungarian #iter={i}/{max_iter} '
-                            f'unary+quad score thresh={outlier_thresh:.3f}, #>thresh={jt.sum(max_vals > outlier_thresh)}/{max_vals.shape[0]}'
-                            f' min:{max_vals.min():.4f}, mean:{max_vals.mean():.4f}, median:{max_vals.median():.4f}, max:{max_vals.max():.4f}')
-
-                if (jt.matmul(U, U.t()) - lastUUt).pow(2).sum().sqrt() < converge_thresh:
-                    break
-
-        if verbose: print('-' * 20)
-
-        if i == max_iter - 1: # not converged
-            if hung_iter:
-                pass
-            else:
-                U_list = [pygmtools.hungarian(_, backend='jittor') for _ in U_list]
-                U = jt.concat(U_list, dim=0)
-                break
-
-        # projection control
-        if projector == 'hungarian':
-            break
-        elif sinkhorn_tau > min_tau:
-            sinkhorn_tau *= sk_gamma
-        else:
-            if hung_iter:
-                projector = 'hungarian'
-            else:
-                U_list = [pygmtools.hungarian(_, backend='jittor') for _ in U_list]
-                U = jt.concat(U_list, dim=0)
-                break
-
-    return U
-
-
-############################################
-#          Neural Network Solvers          #
-############################################
-
-from pygmtools.jittor_modules import *
-
-
-class PCA_GM_Net(Sequential):
-    """
-    Jittor implementation of PCA-GM and IPCA-GM network
-    """
-    def __init__(self, in_channel, hidden_channel, out_channel, num_layers, cross_iter_num=-1):
-        super(PCA_GM_Net, self).__init__()
-        self.gnn_layer = num_layers
-        for i in range(self.gnn_layer):
-            if i == 0:
-                gnn_layer = Siamese_Gconv(in_channel, hidden_channel)
-            elif 0 < i < self.gnn_layer - 1:
-                gnn_layer = Siamese_Gconv(hidden_channel, hidden_channel)
-            else:
-                gnn_layer = Siamese_Gconv(hidden_channel, out_channel)
-                self.add_module('affinity_{}'.format(i), WeightedInnerProdAffinity(out_channel))
-            self.add_module('gnn_layer_{}'.format(i), gnn_layer)
-            if i == self.gnn_layer - 2:  # only the second last layer will have cross-graph module
-                self.add_module('cross_graph_{}'.format(i), jt.nn.Linear(hidden_channel * 2, hidden_channel))
-                if cross_iter_num <= 0:
-                    self.add_module('affinity_{}'.format(i), WeightedInnerProdAffinity(hidden_channel))
-
-
-    def execute(self, feat1, feat2, A1, A2, n1, n2, cross_iter_num, sk_max_iter, sk_tau):
-        _sinkhorn_func = functools.partial(sinkhorn,
-                                           dummy_row=False, max_iter=sk_max_iter, tau=sk_tau, batched_operation=False)
-        emb1, emb2 = feat1, feat2
-        if cross_iter_num <= 0:
-            # Vanilla PCA-GM
-            for i in range(self.gnn_layer):
-                gnn_layer = self.layers[f'gnn_layer_{i}']
-                emb1, emb2 = gnn_layer([A1, emb1], [A2, emb2])
-
-                if i == self.gnn_layer - 2:
-                    affinity = self.layers[f'affinity_{i}']
-                    s = affinity(emb1, emb2)
-                    s = _sinkhorn_func(s, n1, n2)
-
-                    cross_graph = self.layers[f'cross_graph_{i}']
-                    new_emb1 = cross_graph(jt.concat((emb1, jt.bmm(s, emb2)), dim=-1))
-                    new_emb2 = cross_graph(jt.concat((emb2, jt.bmm(s.transpose(1, 2), emb1)), dim=-1))
-                    emb1 = new_emb1
-                    emb2 = new_emb2
-
-            affinity = self.layers[f'affinity_{self.gnn_layer - 1}']
-            s = affinity(emb1, emb2)
-            s = _sinkhorn_func(s, n1, n2)
-
-        else:
-            # IPCA-GM
-            for i in range(self.gnn_layer - 1):
-                gnn_layer = self.layers[f'gnn_layer_{i}']
-                emb1, emb2 = gnn_layer([A1, emb1], [A2, emb2])
-
-            emb1_0, emb2_0 = emb1, emb2
-            s = jt.zeros((emb1.shape[0], emb1.shape[1], emb2.shape[1]))
-
-            for x in range(cross_iter_num):
-                # cross-graph convolution in second last layer
-                i = self.gnn_layer - 2
-                cross_graph = self.layers[f'cross_graph_{i}']
-                emb1 = cross_graph(jt.concat((emb1_0, jt.bmm(s, emb2_0)), dim=-1))
-                emb2 = cross_graph(jt.concat((emb2_0, jt.bmm(s.transpose(1, 2), emb1_0)), dim=-1))
-
-                # last layer
-                i = self.gnn_layer - 1
-                gnn_layer = self.layers[f'gnn_layer_{i}']
-                emb1, emb2 = gnn_layer([A1, emb1], [A2, emb2])
-                affinity = self.layers[f'affinity_{i}']
-                s = affinity(emb1, emb2)
-                s = _sinkhorn_func(s, n1, n2)
-
-        return s
-
-
-pca_gm_pretrain_path = {
-    'voc': ('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=1k4eBJ869uX7sN9TVTe67-8ZKRffpeBu8',
-            '112bb91bd0ccc573c3a936c49416d79e'),
-    'willow': ('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=15R3mdOR99g1LuSyv2IikRmlvy06ub7GQ',
-               '72f4decf48eb5e00933699518563035a'),
-    'voc-all': ('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=17QvlZRAFcPBslaMCax9BVmQpoFMUWv5I',
-                '65cdf9ab437fa37c18eac147cb490c8f')
-}
-
-
-def pca_gm(feat1, feat2, A1, A2, n1, n2,
-           in_channel, hidden_channel, out_channel, num_layers, sk_max_iter, sk_tau,
-           network, pretrain):
-    """
-    Jittor implementation of PCA-GM
-    """
-    if feat1 is None:
-        forward_pass = False
-    else:
-        forward_pass = True
-    if network is None:
-        network = PCA_GM_Net(in_channel, hidden_channel, out_channel, num_layers)
-        if pretrain:
-            if pretrain in pca_gm_pretrain_path:
-                url, md5 = pca_gm_pretrain_path[pretrain]
-                filename = pygmtools.utils.download(f'pca_gm_{pretrain}_jittor.pt', url, md5)
-                _load_model(network, filename) 
-            else:
-                raise ValueError(f'Unknown pretrain tag. Available tags: {pca_gm_pretrain_path.keys()}')
-
-    if forward_pass:
-        batch_size = feat1.shape[0]
-        if n1 is None:
-            n1 = jt.Var([feat1.shape[1]] * batch_size)
-        if n2 is None:
-            n2 = jt.Var([feat2.shape[1]] * batch_size)
-        result = network(feat1, feat2, A1, A2, n1, n2, -1, sk_max_iter, sk_tau)
-    else:
-        result = None
-    return result, network
-
-
-ipca_gm_pretrain_path = {
-    'voc': ('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=1B5W83efRL50C1D348xPJHaHoEXpAfKTL',
-            '3a6dc7948c75d2e31781847941b5f2f6'),
-    'willow': ('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=1iHSAY0d7Ufw9slYQjD_dEMkUB8SQM0kO',
-               '5a1a5b783b9e7ba51579b724a26dccb4'),
-}
-
-
-def ipca_gm(feat1, feat2, A1, A2, n1, n2,
-           in_channel, hidden_channel, out_channel, num_layers, cross_iter, sk_max_iter, sk_tau,
-           network, pretrain):
-    """
-    Jittor implementation of IPCA-GM
-    """
-    if feat1 is None:
-        forward_pass = False
-    else:
-        forward_pass = True
-    if network is None:
-        network = PCA_GM_Net(in_channel, hidden_channel, out_channel, num_layers, cross_iter)
-        if pretrain:
-            if pretrain in ipca_gm_pretrain_path:
-                url, md5 = ipca_gm_pretrain_path[pretrain]
-                filename = pygmtools.utils.download(f'ipca_gm_{pretrain}_jittor.pt', url, md5)
-                _load_model(network, filename)
-            else:
-                raise ValueError(f'Unknown pretrain tag. Available tags: {ipca_gm_pretrain_path.keys()}')
-    
-    if forward_pass:
-        batch_size = feat1.shape[0]
-        if n1 is None:
-            n1 = jt.Var([feat1.shape[1]] * batch_size)
-        if n2 is None:
-            n2 = jt.Var([feat2.shape[1]] * batch_size)
-        result = network(feat1, feat2, A1, A2, n1, n2, cross_iter, sk_max_iter, sk_tau)
-    else:
-        result = None
-    return result, network
-
-
-class CIE_Net(Sequential):
-    """
-    Jittor implementation of CIE graph matching network
-    """
-
-    def __init__(self, in_node_channel, in_edge_channel, hidden_channel, out_channel, num_layers):
-        super(CIE_Net, self).__init__()
-
-        self.gnn_layer = num_layers
-        for i in range(self.gnn_layer):
-            if i == 0:
-                gnn_layer = Siamese_ChannelIndependentConv(in_node_channel, hidden_channel, in_edge_channel)
-            elif 0 < i < self.gnn_layer - 1:
-                gnn_layer = Siamese_ChannelIndependentConv(hidden_channel, hidden_channel, hidden_channel)
-            else:
-                gnn_layer = Siamese_ChannelIndependentConv(hidden_channel, out_channel, hidden_channel)
-                self.add_module('affinity_{}'.format(i), WeightedInnerProdAffinity(out_channel))
-            self.add_module('gnn_layer_{}'.format(i), gnn_layer)
-            if i == self.gnn_layer - 2:  # only the second last layer will have cross-graph module
-                self.add_module('cross_graph_{}'.format(i), jt.nn.Linear(hidden_channel * 2, hidden_channel))
-                self.add_module('affinity_{}'.format(i), WeightedInnerProdAffinity(hidden_channel))
-
-    def execute(self, feat_node1, feat_node2, A1, A2, feat_edge1, feat_edge2, n1, n2, sk_max_iter, sk_tau):
-        _sinkhorn_func = functools.partial(sinkhorn,
-                                           dummy_row=False, max_iter=sk_max_iter, tau=sk_tau, batched_operation=False)
-        emb1, emb2 = feat_node1, feat_node2
-        emb_edge1, emb_edge2 = feat_edge1, feat_edge2
-        for i in range(self.gnn_layer):
-            gnn_layer = self.layers[f'gnn_layer_{i}']
-            # during forward process, the network structure will not change
-            emb1, emb2, emb_edge1, emb_edge2 = gnn_layer([A1, emb1, emb_edge1], [A2, emb2, emb_edge2])
-
-            if i == self.gnn_layer - 2:
-                affinity = self.layers[f'affinity_{i}']
-                s = affinity(emb1, emb2)
-                s = _sinkhorn_func(s, n1, n2)
-
-                cross_graph = self.layers[f'cross_graph_{i}']
-                new_emb1 = cross_graph(jt.concat((emb1, jt.bmm(s, emb2)), dim=-1))
-                new_emb2 = cross_graph(jt.concat((emb2, jt.bmm(s.transpose(1, 2), emb1)), dim=-1))
-                emb1 = new_emb1
-                emb2 = new_emb2
-
-        affinity = self.layers[f'affinity_{self.gnn_layer - 1}']
-        s = affinity(emb1, emb2)
-        s = _sinkhorn_func(s, n1, n2)
-        return s
-
-
-cie_pretrain_path = {
-    'voc': ('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=1jjzbtXne_ppdg7M2jWEpye8piURDVidY',
-            'dc398a5885c5d5894ed6667103d2ff18'),
-    'willow': ('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=11ftNCYBGnjGpFM3__oTCpBhOBabSU1Rv',
-               'bef2c341f605669ed4211e8ff7b1fe0b'),
-}
-
-
-def cie(feat_node1, feat_node2, A1, A2, feat_edge1, feat_edge2, n1, n2,
-        in_node_channel, in_edge_channel, hidden_channel, out_channel, num_layers, sk_max_iter, sk_tau,
-        network, pretrain):
-    """
-    Jittor implementation of CIE
-    """
-    if feat_node1 is None:
-        forward_pass = False
-    else:
-        forward_pass = True
-    if network is None:
-        network = CIE_Net(in_node_channel, in_edge_channel, hidden_channel, out_channel, num_layers)
-        if pretrain:
-            if pretrain in cie_pretrain_path:
-                url, md5 = cie_pretrain_path[pretrain]
-                filename = pygmtools.utils.download(f'cie_{pretrain}_jittor.pt', url, md5)
-                _load_model(network, filename)
-            else:
-                raise ValueError(f'Unknown pretrain tag. Available tags: {cie_pretrain_path.keys()}')
-
-    if forward_pass:
-        batch_size = feat_node1.shape[0]
-        if n1 is None:
-            n1 = jt.Var([feat_node1.shape[1]] * batch_size)
-        if n2 is None:
-            n2 = jt.Var([feat_node1.shape[1]] * batch_size)
-        result = network(feat_node1, feat_node2, A1, A2, feat_edge1, feat_edge2, n1, n2, sk_max_iter, sk_tau)
-    else:
-        result = None
-    return result, network
-
-
-class NGM_Net(Sequential):
-    """
-    Jittor implementation of NGM network
-    """
-    def __init__(self, gnn_channels, sk_emb):
-        super(NGM_Net, self).__init__()
-        self.gnn_layer = len(gnn_channels)
-        for i in range(self.gnn_layer):
-            if i == 0:
-                gnn_layer = NGMConvLayer(1, 1,
-                                         gnn_channels[i] + sk_emb, gnn_channels[i],
-                                         sk_channel=sk_emb)
-            else:
-                gnn_layer = NGMConvLayer(gnn_channels[i - 1] + sk_emb, gnn_channels[i - 1],
-                                         gnn_channels[i] + sk_emb, gnn_channels[i],
-                                         sk_channel=sk_emb)
-            self.add_module('gnn_layer_{}'.format(i), gnn_layer)
-        # self.classifier = nn.Linear(gnn_channels[-1] + sk_emb, 1)
-        self.add_module('classifier', nn.Linear(gnn_channels[-1] + sk_emb, 1))
-
-    def execute(self, K, n1, n2, n1max, n2max, v0, sk_max_iter, sk_tau):
-        _sinkhorn_func = functools.partial(sinkhorn,
-                                           dummy_row=False, max_iter=sk_max_iter, tau=sk_tau, batched_operation=False)
-        emb = v0
-        A = (K != 0)
-        emb_K = K.unsqueeze(-1)
-
-        # NGM qap solver
-        for i in range(self.gnn_layer):
-            gnn_layer = self.layers[f'gnn_layer_{i}']
-            emb_K, emb = gnn_layer(A, emb_K, emb, n1, n2, sk_func=_sinkhorn_func)
-
-        classifier = self.layers['classifier']
-        v = classifier(emb)
-        s = v.view(v.shape[0], n2max, -1).transpose(1, 2)
-
-        return _sinkhorn_func(s, n1, n2, dummy_row=True)
-
-
-ngm_pretrain_path = {
-    'voc': ('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=1_KZQPR6msYsMXupfrAgGgXT-zUXaGtmL',
-            '1c01a48ee2095b70da270da9d862a8c0'),
-    'willow': ('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=1sLI7iC9kUyWm3xeByHvAMx_Hux8VAuP7',
-               'c23821751c895f79bbd038fa426ce259'),
-}
-
-
-def ngm(K, n1, n2, n1max, n2max, x0, gnn_channels, sk_emb, sk_max_iter, sk_tau, network, return_network, pretrain):
-    """
-    Jittor implementation of NGM
-    """
-    if K is None:
-        forward_pass = False
-    else:
-        forward_pass = True
-    if network is None:
-        network = NGM_Net(gnn_channels, sk_emb)
-        if pretrain:
-            if pretrain in ngm_pretrain_path:
-                url, md5 = ngm_pretrain_path[pretrain]
-                try:
-                    filename = pygmtools.utils.download(f'ngm_{pretrain}_jittor.pt', url, md5)
-                except:
-                    filename = os.path.dirname(__file__) + f'/temp/ngm_{pretrain}_jittor.pt'
-                _load_model(network, filename)
-            else:
-                raise ValueError(f'Unknown pretrain tag. Available tags: {ngm_pretrain_path.keys()}')
-
-    if forward_pass:
-        batch_num, n1, n2, n1max, n2max, n1n2, v0 = _check_and_init_gm(K, n1, n2, n1max, n2max, x0)
-        v0 = v0 / jt.mean(v0)
-        result = network(K, n1, n2, n1max, n2max, v0, sk_max_iter, sk_tau)
-    else:
-        result = None
-    return result, network
-
-
-#############################################
-#              Utils Functions              #
-#############################################
-
-def inner_prod_aff_fn(feat1, feat2):
-    """
-    Jittor implementation of inner product affinity function
-    """
-    return jt.matmul(feat1, feat2.transpose(1, 2))
-
-
-def gaussian_aff_fn(feat1, feat2, sigma):
-    """
-    Jittor implementation of Gaussian affinity function
-    """
-    feat1 = feat1.unsqueeze(2)
-    feat2 = feat2.unsqueeze(1)
-    return jt.exp(-((feat1 - feat2) ** 2).sum(dim=-1) / sigma)
-
-
-def build_batch(input, return_ori_dim=False):
-    """
-    Jittor implementation of building a batched Var
-    """
-    assert type(input[0]) == jt.Var
-
-    it = iter(input)
-    t = next(it)
-    max_shape = list(t.shape)
-    ori_shape = [[_] for _ in max_shape]
-    while True:
-        try:
-            t = next(it)
-            for i in range(len(max_shape)):
-                max_shape[i] = int(max(max_shape[i], t.shape[i]))
-                ori_shape[i].append(t.shape[i])
-        except StopIteration:
-            break
-    max_shape = np.array(max_shape)
-
-    padded_ts = []
-    for t in input:
-        pad_pattern = np.zeros(2 * len(max_shape), dtype=np.int64)
-        pad_pattern[::-2] = max_shape - np.array(t.shape)
-        pad_pattern = tuple(pad_pattern.tolist())
-        padded_ts.append(jt.nn.pad(t, pad_pattern, 'constant', 0))
-
-    if return_ori_dim:
-        return jt.stack(padded_ts, dim=0), tuple([jt.int64(_) for _ in ori_shape])
-    else:
-        return jt.stack(padded_ts, dim=0)
-
-
-def dense_to_sparse(dense_adj):
-    """
-    Jittor implementation of converting a dense adjacency matrix to a sparse matrix
-    """
-    batch_size = dense_adj.shape[0]
-    conn, ori_shape = build_batch([jt.nonzero(a) for a in dense_adj], return_ori_dim=True)
-    nedges = ori_shape[0]
-    edge_weight = build_batch([dense_adj[b][(conn[b, :, 0], conn[b, :, 1])] for b in range(batch_size)])
-    return conn, edge_weight.unsqueeze(-1), nedges
-
-
-def compute_affinity_score(X, K):
-    """
-    Jittor implementation of computing affinity score
-    """
-    b, n, _ = X.size()
-    vx = X.transpose(1, 2).reshape(b, -1, 1)  # (b, n*n, 1)
-    vxt = vx.transpose(1, 2)  # (b, 1, n*n)
-    affinity = jt.bmm(jt.bmm(vxt, K), vx)
-    return affinity
-
-
-def to_numpy(input):
-    """
-    Jittor function to_numpy
-    """
-    return input.detach().numpy()
-
-def from_numpy(input, device=None):
-    """
-    Jittor function from_numpy
-    """
-    return jt.Var(input)
-
-
-def generate_isomorphic_graphs(node_num, graph_num, node_feat_dim):
-    """
-    Jittor implementation of generate_isomorphic_graphs
-    """
-    X_gt = jt.zeros((graph_num, node_num, node_num))
-    X_gt[0, jt.arange(0, node_num, dtype=jt.int64), jt.arange(0, node_num, dtype=jt.int64)] = 1
-    for i in range(graph_num):
-        if i > 0:
-            X_gt[i, jt.arange(0, node_num, dtype=jt.int64), jt.randperm(node_num)] = 1
-    joint_X = X_gt.reshape(graph_num * node_num, node_num)
-    X_gt = jt.matmul(joint_X, joint_X.t())
-    X_gt = X_gt.reshape(graph_num, node_num, graph_num, node_num).permute(0, 2, 1, 3)
-    A0 = jt.rand(node_num, node_num)
-    A0[jt.arange(node_num), jt.arange(node_num)] = 0
-    As = [A0]
-    for i in range(graph_num):
-        if i > 0:
-            As.append(jt.matmul(jt.matmul(X_gt[i, 0], A0), X_gt[0, i]))
-    if node_feat_dim > 0:
-        F0 = jt.rand(node_num, node_feat_dim)
-        Fs = [F0]
-        for i in range(graph_num):
-            if i > 0:
-                Fs.append(jt.matmul(X_gt[i, 0], F0))
-        return jt.stack(As, dim=0), X_gt, jt.stack(Fs, dim=0)
-    else:
-        return jt.stack(As, dim=0), X_gt
-
-def permutation_loss(pred_dsmat: Var, gt_perm: Var, n1: Var, n2: Var) -> Var:
-    """
-    Jittor implementation of permutation_loss
-    """
-    batch_num = pred_dsmat.shape[0]
-
-    pred_dsmat = pred_dsmat.float32()
-
-    if not jt.all((pred_dsmat >= 0) * (pred_dsmat <= 1)):
-        raise ValueError("pred_dsmat contains invalid numerical entries.")
-    if not jt.all((gt_perm >= 0) * (gt_perm <= 1)):
-        raise ValueError("gt_perm contains invalid numerical entries.")
-
-    if n1 is None:
-        n1 = jt.Var([pred_dsmat.shape[1] for _ in range(batch_num)])
-    if n2 is None:
-        n2 = jt.Var([pred_dsmat.shape[2] for _ in range(batch_num)])
-
-    loss = jt.Var(0.)
-    n_sum = jt.zeros_like(loss)
-    for b in range(batch_num):
-        loss += jt.nn.bce_loss(
-            pred_dsmat[b, 0:n1[b].item(), 0:n2[b].item()],
-            gt_perm[b, 0:n1[b].item(), 0:n2[b].item()],
-            size_average=False).sum()
-        n_sum += n1[b]
-
-    return loss / n_sum
-
-def _get_shape(input):
-    """
-    Jittor implementation of _get_shape
-    """
-    return input.shape
-
-def _check_and_init_gm(K, n1, n2, n1max, n2max, x0):
-    # get batch number
-    batch_num = K.shape[0]
-    n1n2 = K.shape[1]
-
-    # get values of n1, n2, n1max, n2max and check
-    if n1 is None:
-        n1 = jt.full((batch_num,), n1max, dtype=jt.int)
-    if n2 is None:
-        n2 = jt.full((batch_num,), n2max, dtype=jt.int)
-    if n1max is None:
-        n1max = jt.max(n1).item()
-    if n2max is None:
-        n2max = jt.max(n2).item()
-
-    assert n1max * n2max == n1n2, 'the input size of K does not match with n1max * n2max!'
-
-    # initialize x0 (also v0)
-    if x0 is None:
-        x0 = jt.zeros((batch_num, int(n1max), int(n2max)), dtype=K.dtype)
-        for b in range(batch_num):
-            x0[b, 0:int(n1[b].item()), 0:int(n2[b].item())] = jt.Var(1.) / (n1[b] * n2[b])
-    v0 = x0.transpose(1, 2).reshape(batch_num, n1n2, 1)
-
-    return batch_num, n1, n2, n1max, n2max, n1n2, v0
-
-
-def _check_data_type(input: Var, var_name, raise_err):
-    """
-    Jittor implementation of _check_data_type
-    """
-    if raise_err and type(input) is not Var:
-        raise ValueError(f'Expected Jittor Var{f" for variable {var_name}" if var_name is not None else ""}, '
-                         f'but got {type(input)}. Perhaps the wrong backend?')
-    return type(input) is Var
-
-def _check_shape(input, dim_num):
-    """
-    Jittor implementation of _check_shape
-    """
-    return len(input.shape) == dim_num
-
-def _aff_mat_from_node_edge_aff(node_aff: Var, edge_aff: Var, connectivity1: Var, connectivity2: Var,
-                                n1, n2, ne1, ne2):
-    """
-    Jittor implementation of _aff_mat_from_node_edge_aff
-    """
-    if edge_aff is not None:
-        dtype = edge_aff.dtype
-        batch_size = edge_aff.shape[0]
-        if n1 is None:
-            n1 = jt.max(jt.max(connectivity1, dim=-1), dim=-1) + 1
-        if n2 is None:
-            n2 = jt.max(jt.max(connectivity2, dim=-1), dim=-1) + 1
-        if ne1 is None:
-            ne1 = jt.Var([edge_aff.shape[1]] * batch_size)
-        if ne2 is None:
-            ne2 = jt.Var([edge_aff.shape[2]] * batch_size)
-    else:
-        dtype = node_aff.dtype
-        batch_size = node_aff.shape[0]
-        if n1 is None:
-            n1 = jt.Var([node_aff.shape[1]] * batch_size)
-        if n2 is None:
-            n2 = jt.Var([node_aff.shape[2]] * batch_size)
-
-    n1max = int(max(n1).item())
-    n2max = int(max(n2).item())
-    ks = []
-    for b in range(batch_size):
-        k = jt.zeros((n2max, n1max, n2max, n1max), dtype=dtype)
-        # edge-wise affinity
-        if edge_aff is not None:
-            conn1 = connectivity1[b][:int(ne1[b])]
-            conn2 = connectivity2[b][:int(ne2[b])]
-
-            edge_indices = jt.concat([conn1.repeat_interleave(int(ne2[b]), dim=0), conn2.repeat(int(ne1[b]), 1)], dim=1) # indices: start_g1, end_g1, start_g2, end_g2
-            edge_indices = (edge_indices[:, 2], edge_indices[:, 0], edge_indices[:, 3], edge_indices[:, 1]) # indices: start_g2, start_g1, end_g2, end_g1
-            k[edge_indices] = edge_aff[b, :int(ne1[b]), :int(ne2[b])].reshape(-1)
-        k = k.reshape(n2max * n1max, n2max * n1max)
-        # node-wise affinity
-        if node_aff is not None:
-            k[jt.arange(n2max * n1max), jt.arange(n2max * n1max)] = node_aff[b].transpose(0, 1).reshape(-1)
-        ks.append(k)
-
-    return jt.stack(ks, dim=0)
-
-def _squeeze(input, dim):
-    """
-    Jittor implementation of _squeeze
-    """
-    return input.squeeze(dim)
-
-def _unsqueeze(input, dim):
-    """
-    Jittor implementation of _unsqueeze
-    """
-    return input.unsqueeze(dim)
-
-def _transpose(input, dim1, dim2):
-    """
-    Jittor implementaiton of _transpose
-    """
-    return input.transpose(dim1, dim2)
-
-def _mm(input1, input2):
-    """
-    Jittor implementation of _mm
-    """
-    return jt.matmul(input1, input2)
-
-def _save_model(model, path):
-    """
-    Save Jittor model to a given path
-    """
-    if isinstance(model, jt.nn.DataParallel):
-        model = model.module
-
-    jt.save(model.state_dict(), path)
-
-def _load_model(model, path):
-    """
-    Load Jittor model from a given path. Unmatched keys shall be shown in jittor warning.
-    """
-    module = model
-    module.load_state_dict(jt.load(path))
+# Copyright (c) 2022 Thinklab@SJTU
+# pygmtools is licensed under Mulan PSL v2.
+# You can use this software according to the terms and conditions of the Mulan PSL v2.
+# You may obtain a copy of Mulan PSL v2 at:
+# http://license.coscl.org.cn/MulanPSL2
+# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND,
+# EITHER EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT,
+# MERCHANTABILITY OR FIT FOR A PARTICULAR PURPOSE.
+# See the Mulan PSL v2 for more details.
+
+import numpy as np
+import jittor as jt
+from jittor import Var
+import functools
+import itertools
+import pygmtools.utils
+from multiprocessing import Pool
+import os
+from pygmtools.numpy_backend import _hung_kernel
+
+#############################################
+#     Linear Assignment Problem Solvers     #
+#############################################
+
+def hungarian(s: Var, n1: Var=None, n2: Var=None,
+              unmatch1: Var=None, unmatch2: Var=None,
+              nproc: int=1) -> Var:
+    """
+    Jittor implementation of Hungarian algorithm
+    """
+    batch_num = s.shape[0]
+
+    perm_mat = s.detach().numpy() * -1
+    if n1 is not None:
+        n1 = n1.numpy()
+    else:
+        n1 = [None] * batch_num
+    if n2 is not None:
+        n2 = n2.numpy()
+    else:
+        n2 = [None] * batch_num
+    if unmatch1 is not None:
+        unmatch1 = -unmatch1.numpy()
+    else:
+        unmatch1 = [None] * batch_num
+    if unmatch2 is not None:
+        unmatch2 = -unmatch2.numpy()
+    else:
+        unmatch2 = [None] * batch_num
+
+    if nproc > 1:
+        with Pool(processes=nproc) as pool:
+            mapresult = pool.starmap_async(_hung_kernel, zip(perm_mat, n1, n2, unmatch1, unmatch2))
+            perm_mat = np.stack(mapresult.get())
+    else:
+        perm_mat = np.stack([_hung_kernel(perm_mat[b], n1[b], n2[b], unmatch1[b], unmatch2[b]) for b in range(batch_num)])
+
+    perm_mat = jt.Var(perm_mat)
+
+    return perm_mat
+
+def sinkhorn(s: Var, nrows: Var=None, ncols: Var=None,
+             unmatchrows: Var=None, unmatchcols: Var=None,
+             dummy_row: bool=False, max_iter: int=10, tau: float=1., batched_operation: bool=False) -> Var:
+    """
+    Jittor implementation of Sinkhorn algorithm
+    """
+    batch_size = s.shape[0]
+
+    if s.shape[2] >= s.shape[1]:
+        transposed = False
+    else:
+        s = s.transpose(1, 2)
+        nrows, ncols = ncols, nrows
+        unmatchrows, unmatchcols = unmatchcols, unmatchrows
+        transposed = True
+
+    if nrows is None:
+        nrows = jt.Var([s.shape[1] for _ in range(batch_size)])
+    if ncols is None:
+        ncols = jt.Var([s.shape[2] for _ in range(batch_size)])
+
+    # ensure that in each dimension we have nrow < ncol
+    transposed_batch = nrows > ncols
+    if jt.any(transposed_batch):
+        s_t = s.transpose(1, 2)
+        s_t = jt.concat((
+            s_t[:, :s.shape[1], :],
+            jt.full((batch_size, s.shape[1], s.shape[2]-s.shape[1]), -float('inf'))), dim=2)
+        cond = transposed_batch.view(batch_size, 1, 1)
+        if cond.shape != s.shape:
+            cond = cond.expand(s.shape)
+        s = jt.where(cond, s_t, s)
+
+        new_nrows = jt.where(transposed_batch, ncols, nrows)
+        new_ncols = jt.where(transposed_batch, nrows, ncols)
+        nrows = new_nrows
+        ncols = new_ncols
+
+        if unmatchrows is not None and unmatchcols is not None:
+            unmatchrows_pad = jt.concat((
+                unmatchrows,
+                jt.full((batch_size, unmatchcols.shape[1]-unmatchrows.shape[1]), -float('inf'))), dim=1)
+            new_unmatchrows = jt.where(transposed_batch.view(batch_size, 1).expand(unmatchcols.shape), unmatchcols, unmatchrows_pad)[:, :unmatchrows.shape[1]]
+            new_unmatchcols = jt.where(transposed_batch.view(batch_size, 1).expand(unmatchcols.shape), unmatchrows_pad, unmatchcols)
+            unmatchrows = new_unmatchrows
+            unmatchcols = new_unmatchcols
+
+    # operations are performed on log_s
+    log_s = s / tau
+    if unmatchrows is not None and unmatchcols is not None:
+        unmatchrows = unmatchrows / tau
+        unmatchcols = unmatchcols / tau
+
+    if dummy_row:
+        assert log_s.shape[2] >= log_s.shape[1]
+        dummy_shape = list(log_s.shape)
+        dummy_shape[1] = log_s.shape[2] - log_s.shape[1]
+        ori_nrows = nrows
+        nrows = ncols.clone()
+        log_s = jt.concat((log_s, jt.full(dummy_shape, -float('inf'))), dim=1)
+        if unmatchrows is not None:
+            unmatchrows = jt.concat((unmatchrows, jt.full((dummy_shape[0], dummy_shape[1]), -float('inf'))), dim=1)
+        for b in range(batch_size):
+            log_s[b, int(ori_nrows[b]):int(nrows[b]), :int(ncols[b])] = -100
+
+    # assign the unmatch weights
+    if unmatchrows is not None and unmatchcols is not None:
+        new_log_s = jt.full((log_s.shape[0], log_s.shape[1]+1, log_s.shape[2]+1), -float('inf'))
+        new_log_s[:, :-1, :-1] = log_s
+        log_s = new_log_s
+        for b in range(batch_size):
+            r, c = int(nrows[b]), int(ncols[b])
+            log_s[b, 0:r, c] = unmatchrows[b, 0:r]
+            log_s[b, r, 0:c] = unmatchcols[b, 0:c]
+    row_mask = jt.zeros((batch_size, log_s.shape[1], 1), dtype=jt.bool)
+    col_mask = jt.zeros((batch_size, 1, log_s.shape[2]), dtype=jt.bool)
+    for b in range(batch_size):
+        r, c = int(nrows[b]), int(ncols[b])
+        row_mask[b, 0:r, 0] = 1
+        col_mask[b, 0, 0:c] = 1
+    if unmatchrows is not None and unmatchcols is not None:
+        ncols += 1
+        nrows += 1
+
+    if batched_operation:
+        for b in range(batch_size):
+            log_s[b, int(nrows[b]):, :] = -float('inf')
+            log_s[b, :, int(ncols[b]):] = -float('inf')
+
+        for i in range(max_iter):
+            if i % 2 == 0:
+                m = log_s.max(2, keepdims=True)  #optimized logsumexp
+                log_sum = jt.nn.logsumexp(log_s - m, 2, keepdim=True) + m
+                log_s = log_s - jt.where(row_mask, log_sum, jt.zeros_like(log_sum))
+                if jt.flags.use_cuda == 0:
+                    assert not jt.any(jt.isnan(log_s))
+            else:
+                m = log_s.max(1, keepdims=True)
+                log_sum = jt.nn.logsumexp(log_s - m, 1, keepdim=True) + m                
+                log_s = log_s - jt.where(col_mask, log_sum, jt.zeros_like(log_sum))
+                if jt.flags.use_cuda == 0:
+                    assert not jt.any(jt.isnan(log_s))
+
+        ret_log_s = log_s
+    else:
+        ret_log_s = jt.full((batch_size, log_s.shape[1], log_s.shape[2]), -float('inf'), dtype=log_s.dtype)
+        for b in range(batch_size):
+            r,c = nrows[b],ncols[b]
+            if not isinstance(nrows[b],int):
+                r = int(nrows[b].item())
+            if not isinstance(ncols[b],int):
+                c = int(ncols[b].item())
+            log_s_b = log_s[b, 0:r, 0:c]
+            row_mask_b = row_mask[b, 0:r, :]
+            col_mask_b = col_mask[b, :, 0:c]
+            for i in range(max_iter):
+                if i % 2 == 0:
+                    m = log_s_b.max(1, keepdims=True)
+                    log_sum = jt.nn.logsumexp(log_s_b - m, 1, keepdim=True) + m
+                    log_s_b = log_s_b - jt.where(row_mask_b, log_sum, jt.zeros_like(log_sum))
+                else:
+                    m = log_s_b.max(0, keepdims=True)
+                    log_sum = jt.nn.logsumexp(log_s_b - m, 0, keepdim=True) + m
+                    log_s_b = log_s_b - jt.where(col_mask_b, log_sum, jt.zeros_like(log_sum))
+            ret_log_s[b, 0:r, 0:c] = log_s_b
+
+    if unmatchrows is not None and unmatchcols is not None:
+        ncols -= 1
+        nrows -= 1
+        for b in range(batch_size):
+            r, c = int(nrows[b]), int(ncols[b])
+            ret_log_s[b, 0:r+1, c] = -float('inf')
+            ret_log_s[b, r, 0:c] = -float('inf')
+        ret_log_s = ret_log_s[:, :-1, :-1]
+
+    if dummy_row:
+        if dummy_shape[1] > 0:
+            ret_log_s = ret_log_s[:, :-dummy_shape[1]]
+        for b in range(batch_size):
+            ret_log_s[int(b), int(ori_nrows[b]):int(nrows[b]), :int(ncols[b])] = -float('inf')
+
+    if jt.any(transposed_batch):
+        s_t = ret_log_s.transpose(1, 2)
+        s_t = jt.concat((
+            s_t[:, :ret_log_s.shape[1], :],
+            jt.full((batch_size, ret_log_s.shape[1], ret_log_s.shape[2]-ret_log_s.shape[1]), -float('inf'))), dim=2)
+        cond = transposed_batch.view(batch_size, 1, 1)
+        if cond.shape != s_t.shape:
+            cond = cond.expand(s_t.shape)
+        ret_log_s = jt.where(cond, s_t, ret_log_s)    
+
+    if transposed:
+        ret_log_s = ret_log_s.transpose(1, 2)
+
+    return jt.exp(ret_log_s)
+
+#############################################
+#    Quadratic Assignment Problem Solvers   #
+#############################################
+
+def rrwm(K: Var, n1: Var, n2: Var, n1max, n2max, x0: Var,
+         max_iter: int, sk_iter: int, alpha: float, beta: float) -> Var:
+    """
+    Jittor implementation of RRWM algorithm.
+    """
+    batch_num, n1, n2, n1max, n2max, n1n2, v0 = _check_and_init_gm(K, n1, n2, n1max, n2max, x0)
+    # rescale the values in K
+    d = K.sum(dim=2, keepdims=True)
+    dmax = d.max(dim=1, keepdims=True) 
+    K = K / (dmax + d.min() * 1e-5)
+    v = v0
+    for i in range(max_iter):
+        # try fixing memory error caused by growing scale of
+        # computation graph when testing multi-graph solvers
+        if jt.number_of_lived_ops() > 100000:
+            jt.clean_graph() 
+            
+        # random walk
+        v = jt.bmm(K, v)
+        last_v = v
+        n = jt.norm(v, p=1, dim=1, keepdim=True)
+        v = v / n
+
+        # reweighted jump
+        s = v.view((batch_num, int(n2max), int(n1max))).transpose(1, 2)
+        s = beta * s / s.max(dim=1, keepdims=True).max(dim=2, keepdims=True)
+        v = alpha * sinkhorn(s, n1, n2, max_iter=sk_iter).transpose(1, 2).reshape(batch_num, n1n2, 1) + \
+            (1 - alpha) * v
+        n = jt.norm(v, p=1, dim=1, keepdim=True)
+        v = jt.matmul(v, 1 / n)
+        if (v - last_v).sum().sqrt() < 1e-5:
+            break
+
+    return v.view((batch_num, int(n2max), int(n1max))).transpose(1, 2)
+
+def sm(K: Var, n1: Var, n2: Var, n1max, n2max, x0: Var,
+        max_iter: int) -> Var:
+    """
+    Jittor implementation of SM algorithm.
+    """
+    batch_num, n1, n2, n1max, n2max, n1n2, v0 = _check_and_init_gm(K, n1, n2, n1max, n2max, x0)
+    v = vlast = v0
+    for i in range(max_iter):
+        v = jt.bmm(K, v)
+        n = jt.norm(v, p=2, dim=1)
+        v = jt.matmul(v, (1 / n).reshape(batch_num, 1, 1))
+        if (v - vlast).sum().sqrt() < 1e-5:
+            break        
+        vlast = v
+
+    x = v.reshape(batch_num, n2max, n1max).transpose(1,2)
+    return x
+
+def ipfp(K: Var, n1: Var, n2: Var, n1max, n2max, x0: Var,
+         max_iter) -> Var:
+    """
+    Jittor implementation of IPFP algorithm
+    """
+    batch_num, n1, n2, n1max, n2max, n1n2, v0 = _check_and_init_gm(K, n1, n2, n1max, n2max, x0)
+    v = v0
+    last_v = v
+
+    def comp_obj_score(v1, K, v2):
+        return jt.bmm(jt.bmm(v1.view(batch_num, 1, -1), K), v2)
+
+    for i in range(max_iter):
+        cost = jt.bmm(K, v).reshape((batch_num, int(n2max), int(n1max))).transpose(1, 2)
+        binary_sol = hungarian(cost, n1, n2)
+        binary_v = binary_sol.transpose(1, 2).view(batch_num, -1, 1)
+        alpha = comp_obj_score(v, K, binary_v - v)
+        beta = comp_obj_score(binary_v - v, K, binary_v - v)
+        t0 = alpha / beta
+        cond = jt.logical_or(beta <= 0, t0 >= 1)
+        if cond.shape != binary_v.shape:
+            cond = cond.expand(binary_v.shape)
+        v = jt.where(cond, binary_v, v + t0 * (binary_v - v))
+        last_v_sol = comp_obj_score(last_v, K, last_v)
+        if jt.max(jt.abs(
+                last_v_sol - jt.bmm(cost.reshape(batch_num, 1, -1), binary_sol.reshape(batch_num, -1, 1))
+        ) / last_v_sol) < 1e-3:
+            break
+        last_v = v
+
+    pred_x = binary_sol
+    return pred_x
+
+
+############################################
+#      Multi-Graph Matching Solvers        #
+############################################
+
+
+def cao_solver(K, X, num_graph, num_node, max_iter, lambda_init, lambda_step, lambda_max, iter_boost):
+    r"""
+    Jittor implementation of CAO solver (mode="c")
+
+    :param K: affinity matrix, (m, m, n*n, n*n)
+    :param X: initial matching, (m, m, n, n)
+    :param num_graph: number of graphs, int
+    :param num_node: number of nodes, int
+    :return: X, (m, m, n, n)
+    """
+    m, n = num_graph, num_node
+    param_lambda = lambda_init
+
+    def _comp_aff_score(x, k):
+        return pygmtools.utils.compute_affinity_score(x, k, backend='jittor').unsqueeze(-1).unsqueeze(-1)
+
+    for iter in range(max_iter):
+        if iter >= iter_boost:
+            param_lambda = np.min([param_lambda * lambda_step, lambda_max])
+        # pair_con = get_batch_pc_opt(X)
+        pair_aff = _comp_aff_score(X.reshape(-1, n, n), K.reshape(-1, n * n, n * n)).reshape(m, m)
+        pair_aff = pair_aff - jt.init.eye(m) * pair_aff
+        norm = jt.max(pair_aff)
+        for i in range(m):
+            for j in range(m):
+                if i >= j:
+                    continue
+                aff_ori = _comp_aff_score(X[i, j], K[i, j]) / norm
+                con_ori = _get_single_pc_opt(X, i, j)
+                # con_ori = jt.sqrt(pair_con[i, j])
+                if iter < iter_boost:
+                    score_ori = aff_ori
+                else:
+                    score_ori = aff_ori * (1 - param_lambda) + con_ori * param_lambda
+                X_upt = X[i, j]
+                for k in range(m):
+                    X_combo = jt.matmul(X[i, k], X[k, j])
+                    aff_combo = _comp_aff_score(X_combo, K[i, j]) / norm
+                    con_combo = _get_single_pc_opt(X, i, j, X_combo)
+                    # con_combo = jt.sqrt(pair_con[i, k] * pair_con[k, j])
+                    if iter < iter_boost:
+                        score_combo = aff_combo
+                    else:
+                        score_combo = aff_combo * (1 - param_lambda) + con_combo * param_lambda
+                    if score_combo > score_ori:
+                        X_upt = X_combo
+                X[i, j] = X_upt
+                X[j, i] = X_upt.transpose(0, 1)
+    return X
+
+
+def cao_fast_solver(K, X, num_graph, num_node, max_iter, lambda_init, lambda_step, lambda_max, iter_boost):
+    r"""
+    Jittor implementation of CAO solver in fast config (mode="pc")
+
+    :param K: affinity matrix, (m, m, n*n, n*n)
+    :param X: initial matching, (m, m, n, n)
+    :param num_graph: number of graphs, int
+    :param num_node: number of nodes, int
+    :return: X, (m, m, n, n)
+    """
+    m, n = num_graph, num_node
+    param_lambda = lambda_init
+
+    def _comp_aff_score(x, k):
+        return pygmtools.utils.compute_affinity_score(x, k, backend='jittor').unsqueeze(-1).unsqueeze(-1)
+
+    mask1 = jt.arange(m).reshape(m, 1).repeat(1, m)
+    mask2 = jt.arange(m).reshape(1, m).repeat(m, 1)
+    mask = (mask1 < mask2).float()
+    X_mask = mask.reshape(m, m, 1, 1)
+
+    for iter in range(max_iter):
+        if iter >= iter_boost:
+            param_lambda = np.min([param_lambda * lambda_step, lambda_max])
+
+        pair_aff = _comp_aff_score(X.reshape(-1, n, n), K.reshape(-1, n * n, n * n)).reshape(m, m)
+        pair_aff = pair_aff - jt.init.eye(m) * pair_aff
+        norm = jt.max(pair_aff)
+
+        X1 = X.reshape(m, 1, m, n, n).repeat(1, m, 1, 1, 1).reshape(-1, n, n)  # X1[i,j,k] = X[i,k]
+        X2 = X.reshape(1, m, m, n, n).repeat(m, 1, 1, 1, 1).transpose(1, 2).reshape(-1, n, n)  # X2[i,j,k] = X[k,j]
+        X_combo = jt.bmm(X1, X2).reshape(m, m, m, n, n) # X_combo[i,j,k] = X[i, k] * X[k, j]
+
+        aff_ori = (_comp_aff_score(X.reshape(-1, n, n), K.reshape(-1, n * n, n * n)) / norm).reshape(m, m)
+        pair_con = _get_batch_pc_opt(X)
+        con_ori = jt.sqrt(pair_con)
+
+        K_repeat = K.reshape(m, m, 1, n * n, n * n).repeat(1, 1, m, 1, 1).reshape(-1, n * n, n * n)
+        aff_combo = (_comp_aff_score(X_combo.reshape(-1, n, n), K_repeat) / norm).reshape(m, m, m)
+        con1 = pair_con.reshape(m, 1, m).repeat(1, m, 1)  # con1[i,j,k] = pair_con[i,k]
+        con2 = pair_con.reshape(1, m, m).repeat(m, 1, 1).transpose(1, 2)  # con2[i,j,k] = pair_con[j,k]
+        con_combo = jt.sqrt(con1 * con2)
+
+        if iter < iter_boost:
+            score_ori = aff_ori
+            score_combo = aff_combo
+        else:
+            score_ori = aff_ori * (1 - param_lambda) + con_ori * param_lambda
+            score_combo = aff_combo * (1 - param_lambda) + con_combo * param_lambda
+
+        score_combo_max = jt.max(score_combo, dim=-1)
+        idx = []
+        for i in range(score_combo.shape[0]):
+            idx.append([])
+            for j in range(score_combo.shape[1]):
+                ix = jt.where(score_combo[i][j]==score_combo_max[i][j])[0]
+                idx[i].append(ix[0].item() if ix.shape[0]>1 else ix.item())
+        idx = jt.Var(idx)
+
+        assert jt.all(score_combo_max + 1e-4 >= score_ori), jt.min(score_combo_max - score_ori)
+        X_upt = X_combo[mask1, mask2, idx, :, :]
+        X = X_upt * X_mask + X_upt.transpose(0, 1).transpose(2, 3) * X_mask.transpose(0, 1) + X * (1 - X_mask - X_mask.transpose(0, 1))
+        assert jt.all(X.transpose(0, 1).transpose(2, 3) == X)
+    return X
+
+
+def mgm_floyd_solver(K, X, num_graph, num_node, param_lambda):
+    m, n = num_graph, num_node
+
+    def _comp_aff_score(x, k):
+        return pygmtools.utils.compute_affinity_score(x, k, backend='jittor').unsqueeze(-1).unsqueeze(-1)
+
+    for k in range(m):
+        pair_aff = _comp_aff_score(X.reshape(-1, n, n), K.reshape(-1, n * n, n * n)).reshape(m, m)
+        pair_aff = pair_aff - jt.init.eye(m) * pair_aff
+        norm = jt.max(pair_aff)
+
+        # print("iter:{} aff:{:.4f} con:{:.4f}".format(
+        #     k, jt.mean(pair_aff).item(), jt.mean(get_batch_pc_opt(X)).item()
+        # ))
+
+        for i in range(m):
+            for j in range(m):
+                if i >= j:
+                    continue
+                score_ori = _comp_aff_score(X[i, j], K[i, j]) / norm
+                X_combo = jt.matmul(X[i, k], X[k, j])
+                score_combo = _comp_aff_score(X_combo, K[i, j]) / norm
+
+                if score_combo > score_ori:
+                    X[i, j] = X_combo
+                    X[j, i] = X_combo.transpose(0, 1)
+
+    for k in range(m):
+        pair_aff = _comp_aff_score(X.reshape(-1, n, n), K.reshape(-1, n * n, n * n)).reshape(m, m)
+        pair_aff = pair_aff - jt.init.eye(m) * pair_aff
+        norm = jt.max(pair_aff)
+
+        pair_con = _get_batch_pc_opt(X)
+        for i in range(m):
+            for j in range(m):
+                if i >= j:
+                    continue
+                aff_ori = _comp_aff_score(X[i, j], K[i, j]) / norm
+                con_ori = _get_single_pc_opt(X, i, j)
+                # con_ori = jt.sqrt(pair_con[i, j])
+                score_ori = aff_ori * (1 - param_lambda) + con_ori * param_lambda
+
+                X_combo = jt.matmul(X[i, k], X[k, j])
+                aff_combo = _comp_aff_score(X_combo, K[i, j]) / norm
+                con_combo = _get_single_pc_opt(X, i, j, X_combo)
+                # con_combo = jt.sqrt(pair_con[i, k] * pair_con[k, j])
+                score_combo = aff_combo * (1 - param_lambda) + con_combo * param_lambda
+
+                if score_combo > score_ori:
+                    X[i, j] = X_combo
+                    X[j, i] = X_combo.transpose(0, 1)
+    return X
+
+
+def mgm_floyd_fast_solver(K, X, num_graph, num_node, param_lambda):
+    m, n = num_graph, num_node
+
+    def _comp_aff_score(x, k):
+        return pygmtools.utils.compute_affinity_score(x, k, backend='jittor').unsqueeze(-1).unsqueeze(-1)
+
+    mask1 = jt.arange(m).reshape(m, 1).repeat(1, m)
+    mask2 = jt.arange(m).reshape(1, m).repeat(m, 1)
+    mask = (mask1 < mask2).float()
+    X_mask = mask.reshape(m, m, 1, 1)
+
+    for k in range(m):
+        pair_aff = _comp_aff_score(X.reshape(-1, n, n), K.reshape(-1, n * n, n * n)).reshape(m, m)
+        pair_aff = pair_aff - jt.init.eye(m) * pair_aff
+        norm = jt.max(pair_aff)
+
+        # print("iter:{} aff:{:.4f} con:{:.4f}".format(
+        #     k, jt.mean(pair_aff).item(), jt.mean(get_batch_pc_opt(X)).item()
+        # ))
+
+        X1 = X[:, k].reshape(m, 1, n, n).repeat(1, m, 1, 1).reshape(-1, n, n)  # X[i, j] = X[i, k]
+        X2 = X[k, :].reshape(1, m, n, n).repeat(m, 1, 1, 1).reshape(-1, n, n)  # X[i, j] = X[j, k]
+        X_combo = jt.bmm(X1, X2).reshape(m, m, n, n)
+
+        aff_ori = (_comp_aff_score(X.reshape(-1, n, n), K.reshape(-1, n * n, n * n)) / norm).reshape(m, m)
+        aff_combo = (_comp_aff_score(X_combo.reshape(-1, n, n), K.reshape(-1, n * n, n * n)) / norm).reshape(m, m)
+
+        score_ori = aff_ori
+        score_combo = aff_combo
+
+        upt = (score_ori < score_combo).float()
+        upt = (upt * mask).reshape(m, m, 1, 1)
+        X = X * (1.0 - upt) + X_combo * upt
+        X = X * X_mask + X.transpose(0, 1).transpose(2, 3) * (1 - X_mask)
+
+    for k in range(m):
+        pair_aff = _comp_aff_score(X.reshape(-1, n, n), K.reshape(-1, n * n, n * n)).reshape(m, m)
+        pair_aff = pair_aff - jt.init.eye(m) * pair_aff
+        norm = jt.max(pair_aff)
+
+        pair_con = _get_batch_pc_opt(X)
+
+        X1 = X[:, k].reshape(m, 1, n, n).repeat(1, m, 1, 1).reshape(-1, n, n)  # X[i, j] = X[i, k]
+        X2 = X[k, :].reshape(1, m, n, n).repeat(m, 1, 1, 1).reshape(-1, n, n)  # X[i, j] = X[j, k]
+        X_combo = jt.bmm(X1, X2).reshape(m, m, n, n)
+
+        aff_ori = (_comp_aff_score(X.reshape(-1, n, n), K.reshape(-1, n * n, n * n)) / norm).reshape(m, m)
+        aff_combo = (_comp_aff_score(X_combo.reshape(-1, n, n), K.reshape(-1, n * n, n * n)) / norm).reshape(m, m)
+
+        con_ori = jt.sqrt(pair_con)
+        con1 = pair_con[:, k].reshape(m, 1).repeat(1, m)
+        con2 = pair_con[k, :].reshape(1, m).repeat(m, 1)
+        con_combo = jt.sqrt(con1 * con2)
+
+        score_ori = aff_ori * (1 - param_lambda) + con_ori * param_lambda
+        score_combo = aff_combo * (1 - param_lambda) + con_combo * param_lambda
+
+        upt = (score_ori < score_combo).float()
+        upt = (upt * mask).reshape(m, m, 1, 1)
+        X = X * (1.0 - upt) + X_combo * upt
+        X = X * X_mask + X.transpose(0, 1).transpose(2, 3) * (1 - X_mask)
+    return X
+
+
+def _get_single_pc_opt(X, i, j, Xij=None):
+    """
+    CAO/Floyd helper function (compute consistency)
+    :param X: (m, m, n, n) all the matching results
+    :param i: index
+    :param j: index
+    :return: the consistency of X_ij
+    """
+    m, _, n, _ = X.size()
+    if Xij is None:
+        Xij = X[i, j]
+    X1 = X[i, :].reshape(-1, n, n)
+    X2 = X[:, j].reshape(-1, n, n)
+    X_combo = jt.bmm(X1, X2)
+    pair_con = 1 - jt.sum(jt.abs(Xij - X_combo)) / (2 * n * m)
+    return pair_con
+
+
+def _get_batch_pc_opt(X):
+    """
+    CAO/Floyd-fast helper function (compute consistency in batch)
+    :param X: (m, m, n, n) all the matching results
+    :return: (m, m) the consistency of X
+    """
+    m, _, n, _ = X.size()
+    X1 = X.reshape(m, 1, m, n, n).repeat(1, m, 1, 1, 1).reshape(-1, n, n)  # X1[i, j, k] = X[i, k]
+    X2 = X.reshape(1, m, m, n, n).repeat(m, 1, 1, 1, 1).transpose(1, 2).reshape(-1, n, n)  # X2[i, j, k] = X[k, j]
+    X_combo = jt.bmm(X1, X2).reshape(m, m, m, n, n)
+    X_ori = X.reshape(m, m, 1, n, n).repeat(1, 1, m, 1, 1)
+    pair_con = 1 - jt.sum(jt.abs(X_combo - X_ori), dims=(2, 3, 4)) / (2 * n * m)
+    return pair_con
+
+
+def gamgm(
+        A, W, ns, n_univ, U0,
+        init_tau, min_tau, sk_gamma,
+        sk_iter, max_iter, quad_weight,
+        converge_thresh, outlier_thresh, bb_smooth,
+        verbose,
+        cluster_M=None, projector='sinkhorn', hung_iter=True # these arguments are reserved for clustering
+):
+    """
+    Jittor implementation of Graduated Assignment for Multi-Graph Matching (with compatibility for 2GM and clustering)
+    """
+
+    num_graphs = A.shape[0]
+    if ns is None:
+        ns = jt.full((num_graphs,), A.shape[1], dtype=jt.int)
+    n_indices = jt.cumsum(ns, dim=0)
+
+    # build a super adjacency matrix A
+    supA = jt.zeros((n_indices[-1].item(), n_indices[-1].item()))
+    for i in range(num_graphs):
+        start_n = (n_indices[i] - ns[i]).item()
+        end_n = n_indices[i].item()
+        supA[start_n:end_n, start_n:end_n] = A[i, :ns[i].item(), :ns[i].item()]
+
+    # handle the type of n_univ
+    if type(n_univ) is jt.Var:
+        n_univ = n_univ.item()
+
+    # randomly init U
+    if U0 is None:
+        U0 = jt.full((n_indices[-1].item(), n_univ), 1 / n_univ)
+        U0 += jt.randn_like(U0) / 1000
+
+    # init cluster_M if not given
+    if cluster_M is None:
+        cluster_M = jt.ones((num_graphs, num_graphs))
+
+    # reshape W into supW
+    supW = jt.zeros((n_indices[-1].item(), n_indices[-1].item()))
+    for i, j in itertools.product(range(num_graphs), repeat=2):
+        start_x = (n_indices[i] - ns[i]).item()
+        end_x = n_indices[i].item()
+        start_y = (n_indices[j] - ns[j]).item()
+        end_y = n_indices[j].item()
+        supW[start_x:end_x, start_y:end_y] = W[i, j, :ns[i].item(), :ns[j].item()]
+
+    U = GAMGMJittorFunc.apply(
+        bb_smooth,
+        supA, supW, ns, n_indices, n_univ, num_graphs, U0,
+        init_tau, min_tau, sk_gamma,
+        sk_iter, max_iter, quad_weight,
+        converge_thresh, outlier_thresh,
+        verbose,
+        cluster_M, projector, hung_iter
+    )
+
+    # build MultiMatchingResult
+    result = pygmtools.utils.MultiMatchingResult(True, 'jittor')
+
+    for i in range(num_graphs):
+        start_n = n_indices[i] - ns[i]
+        end_n = n_indices[i]
+        result[i] = U[start_n.item():end_n.item()]
+
+    return result
+
+
+class GAMGMJittorFunc(jt.Function):
+    """
+    Jittor wrapper to support forward and backward pass (by black-box differentiation)
+    """
+
+    def execute(self, bb_smooth, supA, supW, ns, n_indices, n_univ, num_graphs, U0, *args):
+        # save parameters
+        self.bb_smooth = bb_smooth
+        self.named_args = supA, supW, ns, n_indices, n_univ, num_graphs, U0
+        self.list_args = args
+
+        # real solver function
+        U = gamgm_real(supA, supW, ns, n_indices, n_univ, num_graphs, U0, *args)
+
+        # save result
+        self.U = U
+        return U
+
+    def grad(self, dU):
+        epsilon = 1e-8
+        bb_smooth = self.bb_smooth
+        supA, supW, ns, n_indices, n_univ, num_graphs, U0 = self.named_args
+        args = self.list_args
+        U = self.U
+
+        for i, j in itertools.product(range(num_graphs), repeat=2):
+            start_x = (n_indices[i] - ns[i]).item()
+            end_x = n_indices[i].item()
+            start_y = (n_indices[j] - ns[j]).item()
+            end_y = n_indices[j].item()
+            supW[start_x:end_x, start_y:end_y] += bb_smooth * jt.matmul(dU[start_x:end_x], dU[start_y:end_y].transpose(0, 1))
+
+        U_prime = gamgm_real(supA, supW, ns, n_indices, n_univ, num_graphs, U0, *args)
+
+        grad_supW = jt.zeros((n_indices[-1].item(), n_indices[-1].item()))
+        for i, j in itertools.product(range(num_graphs), repeat=2):
+            start_x = (n_indices[i] - ns[i]).item()
+            end_x = n_indices[i].item()
+            start_y = (n_indices[j] - ns[j]).item()
+            end_y = n_indices[j].item()
+            X = jt.matmul(U[start_x:end_x], U[start_y:end_y].transpose(0, 1))
+            X_prime = jt.matmul(U_prime[start_x:end_x], U_prime[start_y:end_y].transpose(0, 1))
+            grad_supW[start_x:end_x, start_y:end_y] = -(X - X_prime) / (bb_smooth + epsilon)
+
+        return_list = [None, None, grad_supW] + [None] * (len(args) + 8 - 3)
+        return tuple(return_list)
+
+
+def gamgm_real(
+        supA, supW, ns, n_indices, n_univ, num_graphs, U0,
+        init_tau, min_tau, sk_gamma,
+        sk_iter, max_iter, quad_weight,
+        converge_thresh, outlier_thresh,
+        verbose,
+        cluster_M, projector, hung_iter # these arguments are reserved for clustering
+        ):
+    """
+    The real forward function of GAMGM
+    """
+    U = U0
+    sinkhorn_tau = init_tau
+    iter_flag = True
+
+    while iter_flag:
+        with jt.enable_grad():
+            for i in range(max_iter):
+                # compact matrix form update of V
+                UUt = jt.matmul(U, U.t())
+                lastUUt = UUt
+                # jittor does not accept array as the second parameter of repeat_interleave and jittor Var is based on numpy array
+                import numpy as np
+                cluster_weight = jt.Var(np.repeat(cluster_M, ns.long().data, axis=0))
+                cluster_weight = jt.Var(np.repeat(cluster_weight, ns.long().data, axis=1))
+                quad, chains = supA, [UUt * cluster_weight, supA, U]
+                for matrix in chains:
+                    quad = jt.matmul(quad, matrix)
+                quad *= (quad_weight * 2)
+                # quad = jt.chain_matmul(supA, UUt * cluster_weight, supA, U) * quad_weight * 2
+                unary = jt.matmul(supW * cluster_weight, U)
+                if verbose:
+                    if projector == 'sinkhorn':
+                        print_str = f'tau={sinkhorn_tau:.3e}'
+                    else:
+                        print_str = 'hungarian'
+                    print(print_str + f' #iter={i}/{max_iter} '
+                        f'quad score: {(quad * U).sum():.3e}, unary score: {(unary * U).sum():.3e}')
+                V = (quad + unary) / num_graphs
+
+                U_list = []
+                if projector == 'hungarian':
+                    n_start = 0
+                    for n_end in n_indices:
+                        if isinstance(n_start, Var):
+                            n_start = n_start.item()
+                        U_list.append(pygmtools.hungarian(V[n_start:n_end.item(), :n_univ], backend='jittor'))
+                        n_start = n_end
+                elif projector == 'sinkhorn':
+                    if jt.all(ns == ns[0]):
+                        if ns[0] <= n_univ:
+                            U_list.append(
+                                sinkhorn(
+                                    V.reshape(num_graphs, -1, n_univ),
+                                    max_iter=sk_iter, tau=sinkhorn_tau, batched_operation=True, dummy_row=True
+                                ).reshape(-1, n_univ))
+                        else:
+                            U_list.append(
+                                sinkhorn(
+                                    V.reshape(num_graphs, -1, n_univ).transpose(1, 2),
+                                    max_iter=sk_iter, tau=sinkhorn_tau, batched_operation=True, dummy_row=True
+                                ).transpose(1, 2).reshape(-1, n_univ))
+                    else:
+                        V_list = []
+                        n1 = []
+                        n_start = 0
+                        for n_end in n_indices:
+                            if isinstance(n_start, Var):
+                                n_start = n_start.item()
+                            V_list.append(V[n_start:n_end.item(), :n_univ])
+                            n1.append(n_end.item() - n_start)
+                            n_start = n_end
+                        V_batch = build_batch(V_list)
+                        n1 = jt.Var(n1)
+                        U = sinkhorn(V_batch, n1,
+                                    max_iter=sk_iter, tau=sinkhorn_tau, batched_operation=True, dummy_row=True)
+                        n_start = 0
+                        for idx, n_end in enumerate(n_indices):
+                            U_list.append(U[idx, :n_end.item() - n_start, :])
+                            n_start = n_end.item()
+                else:
+                    raise NameError('Unknown projecter name: {}'.format(projector))
+
+                U = jt.concat(U_list, dim=0)
+                if num_graphs == 2:
+                    U[:ns[0], :] = jt.init.eye(ns[0], n_univ)
+
+                # calculate gap to discrete
+                if projector == 'sinkhorn' and verbose:
+                    U_list_hung = []
+                    n_start = 0
+                    for n_end in n_indices:
+                        n_end = n_end.item()
+                        U_list_hung.append(pygmtools.hungarian(V[n_start:n_end, :n_univ], backend='jittor'))
+                        n_start = n_end
+                    U_hung = jt.concat(U_list_hung, dim=0)
+                    diff = jt.norm(jt.matmul(U, U.t()) - lastUUt).sum()
+                    print(f'tau={sinkhorn_tau:.3e} #iter={i}/{max_iter} '
+                          f'gap to discrete: {jt.mean(jt.abs(U - U_hung)).item():.3e}, '
+                          f'iter diff: {diff.item():.3e}')
+
+                if projector == 'hungarian' and outlier_thresh > 0:
+                    U_hung = U
+                    UUt = jt.matmul(U_hung, U_hung.t())
+                    cluster_weight = jt.Var(np.repeat(cluster_M, ns.long().data, axis=0))
+                    cluster_weight = jt.Var(np.repeat(cluster_weight, ns.long().data, axis=1))
+                    quad, chains = supA, [UUt * cluster_weight, supA, U_hung]
+                    for matrix in chains:
+                        quad = jt.matmul(quad, matrix)
+                    quad *= (quad_weight * 2)
+                    unary = jt.matmul(supW * cluster_weight, U_hung)
+                    max_vals = (unary + quad).max(dim=1)
+                    U = U * (unary + quad > outlier_thresh)
+                    if verbose:
+                        print(f'hungarian #iter={i}/{max_iter} '
+                            f'unary+quad score thresh={outlier_thresh:.3f}, #>thresh={jt.sum(max_vals > outlier_thresh)}/{max_vals.shape[0]}'
+                            f' min:{max_vals.min():.4f}, mean:{max_vals.mean():.4f}, median:{max_vals.median():.4f}, max:{max_vals.max():.4f}')
+
+                if (jt.matmul(U, U.t()) - lastUUt).pow(2).sum().sqrt() < converge_thresh:
+                    break
+
+        if verbose: print('-' * 20)
+
+        if i == max_iter - 1: # not converged
+            if hung_iter:
+                pass
+            else:
+                U_list = [pygmtools.hungarian(_, backend='jittor') for _ in U_list]
+                U = jt.concat(U_list, dim=0)
+                break
+
+        # projection control
+        if projector == 'hungarian':
+            break
+        elif sinkhorn_tau > min_tau:
+            sinkhorn_tau *= sk_gamma
+        else:
+            if hung_iter:
+                projector = 'hungarian'
+            else:
+                U_list = [pygmtools.hungarian(_, backend='jittor') for _ in U_list]
+                U = jt.concat(U_list, dim=0)
+                break
+
+    return U
+
+
+############################################
+#          Neural Network Solvers          #
+############################################
+
+from pygmtools.jittor_modules import *
+
+
+class PCA_GM_Net(Sequential):
+    """
+    Jittor implementation of PCA-GM and IPCA-GM network
+    """
+    def __init__(self, in_channel, hidden_channel, out_channel, num_layers, cross_iter_num=-1):
+        super(PCA_GM_Net, self).__init__()
+        self.gnn_layer = num_layers
+        for i in range(self.gnn_layer):
+            if i == 0:
+                gnn_layer = Siamese_Gconv(in_channel, hidden_channel)
+            elif 0 < i < self.gnn_layer - 1:
+                gnn_layer = Siamese_Gconv(hidden_channel, hidden_channel)
+            else:
+                gnn_layer = Siamese_Gconv(hidden_channel, out_channel)
+                self.add_module('affinity_{}'.format(i), WeightedInnerProdAffinity(out_channel))
+            self.add_module('gnn_layer_{}'.format(i), gnn_layer)
+            if i == self.gnn_layer - 2:  # only the second last layer will have cross-graph module
+                self.add_module('cross_graph_{}'.format(i), jt.nn.Linear(hidden_channel * 2, hidden_channel))
+                if cross_iter_num <= 0:
+                    self.add_module('affinity_{}'.format(i), WeightedInnerProdAffinity(hidden_channel))
+
+
+    def execute(self, feat1, feat2, A1, A2, n1, n2, cross_iter_num, sk_max_iter, sk_tau):
+        _sinkhorn_func = functools.partial(sinkhorn,
+                                           dummy_row=False, max_iter=sk_max_iter, tau=sk_tau, batched_operation=False)
+        emb1, emb2 = feat1, feat2
+        if cross_iter_num <= 0:
+            # Vanilla PCA-GM
+            for i in range(self.gnn_layer):
+                gnn_layer = self.layers[f'gnn_layer_{i}']
+                emb1, emb2 = gnn_layer([A1, emb1], [A2, emb2])
+
+                if i == self.gnn_layer - 2:
+                    affinity = self.layers[f'affinity_{i}']
+                    s = affinity(emb1, emb2)
+                    s = _sinkhorn_func(s, n1, n2)
+
+                    cross_graph = self.layers[f'cross_graph_{i}']
+                    new_emb1 = cross_graph(jt.concat((emb1, jt.bmm(s, emb2)), dim=-1))
+                    new_emb2 = cross_graph(jt.concat((emb2, jt.bmm(s.transpose(1, 2), emb1)), dim=-1))
+                    emb1 = new_emb1
+                    emb2 = new_emb2
+
+            affinity = self.layers[f'affinity_{self.gnn_layer - 1}']
+            s = affinity(emb1, emb2)
+            s = _sinkhorn_func(s, n1, n2)
+
+        else:
+            # IPCA-GM
+            for i in range(self.gnn_layer - 1):
+                gnn_layer = self.layers[f'gnn_layer_{i}']
+                emb1, emb2 = gnn_layer([A1, emb1], [A2, emb2])
+
+            emb1_0, emb2_0 = emb1, emb2
+            s = jt.zeros((emb1.shape[0], emb1.shape[1], emb2.shape[1]))
+
+            for x in range(cross_iter_num):
+                # cross-graph convolution in second last layer
+                i = self.gnn_layer - 2
+                cross_graph = self.layers[f'cross_graph_{i}']
+                emb1 = cross_graph(jt.concat((emb1_0, jt.bmm(s, emb2_0)), dim=-1))
+                emb2 = cross_graph(jt.concat((emb2_0, jt.bmm(s.transpose(1, 2), emb1_0)), dim=-1))
+
+                # last layer
+                i = self.gnn_layer - 1
+                gnn_layer = self.layers[f'gnn_layer_{i}']
+                emb1, emb2 = gnn_layer([A1, emb1], [A2, emb2])
+                affinity = self.layers[f'affinity_{i}']
+                s = affinity(emb1, emb2)
+                s = _sinkhorn_func(s, n1, n2)
+
+        return s
+
+
+pca_gm_pretrain_path = {
+    'voc': ('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=1k4eBJ869uX7sN9TVTe67-8ZKRffpeBu8',
+            '112bb91bd0ccc573c3a936c49416d79e'),
+    'willow': ('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=15R3mdOR99g1LuSyv2IikRmlvy06ub7GQ',
+               '72f4decf48eb5e00933699518563035a'),
+    'voc-all': ('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=17QvlZRAFcPBslaMCax9BVmQpoFMUWv5I',
+                '65cdf9ab437fa37c18eac147cb490c8f')
+}
+
+
+def pca_gm(feat1, feat2, A1, A2, n1, n2,
+           in_channel, hidden_channel, out_channel, num_layers, sk_max_iter, sk_tau,
+           network, pretrain):
+    """
+    Jittor implementation of PCA-GM
+    """
+    if feat1 is None:
+        forward_pass = False
+    else:
+        forward_pass = True
+    if network is None:
+        network = PCA_GM_Net(in_channel, hidden_channel, out_channel, num_layers)
+        if pretrain:
+            if pretrain in pca_gm_pretrain_path:
+                url, md5 = pca_gm_pretrain_path[pretrain]
+                filename = pygmtools.utils.download(f'pca_gm_{pretrain}_jittor.pt', url, md5)
+                _load_model(network, filename) 
+            else:
+                raise ValueError(f'Unknown pretrain tag. Available tags: {pca_gm_pretrain_path.keys()}')
+
+    if forward_pass:
+        batch_size = feat1.shape[0]
+        if n1 is None:
+            n1 = jt.Var([feat1.shape[1]] * batch_size)
+        if n2 is None:
+            n2 = jt.Var([feat2.shape[1]] * batch_size)
+        result = network(feat1, feat2, A1, A2, n1, n2, -1, sk_max_iter, sk_tau)
+    else:
+        result = None
+    return result, network
+
+
+ipca_gm_pretrain_path = {
+    'voc': ('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=1B5W83efRL50C1D348xPJHaHoEXpAfKTL',
+            '3a6dc7948c75d2e31781847941b5f2f6'),
+    'willow': ('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=1iHSAY0d7Ufw9slYQjD_dEMkUB8SQM0kO',
+               '5a1a5b783b9e7ba51579b724a26dccb4'),
+}
+
+
+def ipca_gm(feat1, feat2, A1, A2, n1, n2,
+           in_channel, hidden_channel, out_channel, num_layers, cross_iter, sk_max_iter, sk_tau,
+           network, pretrain):
+    """
+    Jittor implementation of IPCA-GM
+    """
+    if feat1 is None:
+        forward_pass = False
+    else:
+        forward_pass = True
+    if network is None:
+        network = PCA_GM_Net(in_channel, hidden_channel, out_channel, num_layers, cross_iter)
+        if pretrain:
+            if pretrain in ipca_gm_pretrain_path:
+                url, md5 = ipca_gm_pretrain_path[pretrain]
+                filename = pygmtools.utils.download(f'ipca_gm_{pretrain}_jittor.pt', url, md5)
+                _load_model(network, filename)
+            else:
+                raise ValueError(f'Unknown pretrain tag. Available tags: {ipca_gm_pretrain_path.keys()}')
+    
+    if forward_pass:
+        batch_size = feat1.shape[0]
+        if n1 is None:
+            n1 = jt.Var([feat1.shape[1]] * batch_size)
+        if n2 is None:
+            n2 = jt.Var([feat2.shape[1]] * batch_size)
+        result = network(feat1, feat2, A1, A2, n1, n2, cross_iter, sk_max_iter, sk_tau)
+    else:
+        result = None
+    return result, network
+
+
+class CIE_Net(Sequential):
+    """
+    Jittor implementation of CIE graph matching network
+    """
+
+    def __init__(self, in_node_channel, in_edge_channel, hidden_channel, out_channel, num_layers):
+        super(CIE_Net, self).__init__()
+
+        self.gnn_layer = num_layers
+        for i in range(self.gnn_layer):
+            if i == 0:
+                gnn_layer = Siamese_ChannelIndependentConv(in_node_channel, hidden_channel, in_edge_channel)
+            elif 0 < i < self.gnn_layer - 1:
+                gnn_layer = Siamese_ChannelIndependentConv(hidden_channel, hidden_channel, hidden_channel)
+            else:
+                gnn_layer = Siamese_ChannelIndependentConv(hidden_channel, out_channel, hidden_channel)
+                self.add_module('affinity_{}'.format(i), WeightedInnerProdAffinity(out_channel))
+            self.add_module('gnn_layer_{}'.format(i), gnn_layer)
+            if i == self.gnn_layer - 2:  # only the second last layer will have cross-graph module
+                self.add_module('cross_graph_{}'.format(i), jt.nn.Linear(hidden_channel * 2, hidden_channel))
+                self.add_module('affinity_{}'.format(i), WeightedInnerProdAffinity(hidden_channel))
+
+    def execute(self, feat_node1, feat_node2, A1, A2, feat_edge1, feat_edge2, n1, n2, sk_max_iter, sk_tau):
+        _sinkhorn_func = functools.partial(sinkhorn,
+                                           dummy_row=False, max_iter=sk_max_iter, tau=sk_tau, batched_operation=False)
+        emb1, emb2 = feat_node1, feat_node2
+        emb_edge1, emb_edge2 = feat_edge1, feat_edge2
+        for i in range(self.gnn_layer):
+            gnn_layer = self.layers[f'gnn_layer_{i}']
+            # during forward process, the network structure will not change
+            emb1, emb2, emb_edge1, emb_edge2 = gnn_layer([A1, emb1, emb_edge1], [A2, emb2, emb_edge2])
+
+            if i == self.gnn_layer - 2:
+                affinity = self.layers[f'affinity_{i}']
+                s = affinity(emb1, emb2)
+                s = _sinkhorn_func(s, n1, n2)
+
+                cross_graph = self.layers[f'cross_graph_{i}']
+                new_emb1 = cross_graph(jt.concat((emb1, jt.bmm(s, emb2)), dim=-1))
+                new_emb2 = cross_graph(jt.concat((emb2, jt.bmm(s.transpose(1, 2), emb1)), dim=-1))
+                emb1 = new_emb1
+                emb2 = new_emb2
+
+        affinity = self.layers[f'affinity_{self.gnn_layer - 1}']
+        s = affinity(emb1, emb2)
+        s = _sinkhorn_func(s, n1, n2)
+        return s
+
+
+cie_pretrain_path = {
+    'voc': ('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=1jjzbtXne_ppdg7M2jWEpye8piURDVidY',
+            'dc398a5885c5d5894ed6667103d2ff18'),
+    'willow': ('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=11ftNCYBGnjGpFM3__oTCpBhOBabSU1Rv',
+               'bef2c341f605669ed4211e8ff7b1fe0b'),
+}
+
+
+def cie(feat_node1, feat_node2, A1, A2, feat_edge1, feat_edge2, n1, n2,
+        in_node_channel, in_edge_channel, hidden_channel, out_channel, num_layers, sk_max_iter, sk_tau,
+        network, pretrain):
+    """
+    Jittor implementation of CIE
+    """
+    if feat_node1 is None:
+        forward_pass = False
+    else:
+        forward_pass = True
+    if network is None:
+        network = CIE_Net(in_node_channel, in_edge_channel, hidden_channel, out_channel, num_layers)
+        if pretrain:
+            if pretrain in cie_pretrain_path:
+                url, md5 = cie_pretrain_path[pretrain]
+                filename = pygmtools.utils.download(f'cie_{pretrain}_jittor.pt', url, md5)
+                _load_model(network, filename)
+            else:
+                raise ValueError(f'Unknown pretrain tag. Available tags: {cie_pretrain_path.keys()}')
+
+    if forward_pass:
+        batch_size = feat_node1.shape[0]
+        if n1 is None:
+            n1 = jt.Var([feat_node1.shape[1]] * batch_size)
+        if n2 is None:
+            n2 = jt.Var([feat_node1.shape[1]] * batch_size)
+        result = network(feat_node1, feat_node2, A1, A2, feat_edge1, feat_edge2, n1, n2, sk_max_iter, sk_tau)
+    else:
+        result = None
+    return result, network
+
+
+class NGM_Net(Sequential):
+    """
+    Jittor implementation of NGM network
+    """
+    def __init__(self, gnn_channels, sk_emb):
+        super(NGM_Net, self).__init__()
+        self.gnn_layer = len(gnn_channels)
+        for i in range(self.gnn_layer):
+            if i == 0:
+                gnn_layer = NGMConvLayer(1, 1,
+                                         gnn_channels[i] + sk_emb, gnn_channels[i],
+                                         sk_channel=sk_emb)
+            else:
+                gnn_layer = NGMConvLayer(gnn_channels[i - 1] + sk_emb, gnn_channels[i - 1],
+                                         gnn_channels[i] + sk_emb, gnn_channels[i],
+                                         sk_channel=sk_emb)
+            self.add_module('gnn_layer_{}'.format(i), gnn_layer)
+        # self.classifier = nn.Linear(gnn_channels[-1] + sk_emb, 1)
+        self.add_module('classifier', nn.Linear(gnn_channels[-1] + sk_emb, 1))
+
+    def execute(self, K, n1, n2, n1max, n2max, v0, sk_max_iter, sk_tau):
+        _sinkhorn_func = functools.partial(sinkhorn,
+                                           dummy_row=False, max_iter=sk_max_iter, tau=sk_tau, batched_operation=False)
+        emb = v0
+        A = (K != 0)
+        emb_K = K.unsqueeze(-1)
+
+        # NGM qap solver
+        for i in range(self.gnn_layer):
+            gnn_layer = self.layers[f'gnn_layer_{i}']
+            emb_K, emb = gnn_layer(A, emb_K, emb, n1, n2, sk_func=_sinkhorn_func)
+
+        classifier = self.layers['classifier']
+        v = classifier(emb)
+        s = v.view(v.shape[0], n2max, -1).transpose(1, 2)
+
+        return _sinkhorn_func(s, n1, n2, dummy_row=True)
+
+
+ngm_pretrain_path = {
+    'voc': ('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=1_KZQPR6msYsMXupfrAgGgXT-zUXaGtmL',
+            '1c01a48ee2095b70da270da9d862a8c0'),
+    'willow': ('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=1sLI7iC9kUyWm3xeByHvAMx_Hux8VAuP7',
+               'c23821751c895f79bbd038fa426ce259'),
+}
+
+
+def ngm(K, n1, n2, n1max, n2max, x0, gnn_channels, sk_emb, sk_max_iter, sk_tau, network, return_network, pretrain):
+    """
+    Jittor implementation of NGM
+    """
+    if K is None:
+        forward_pass = False
+    else:
+        forward_pass = True
+    if network is None:
+        network = NGM_Net(gnn_channels, sk_emb)
+        if pretrain:
+            if pretrain in ngm_pretrain_path:
+                url, md5 = ngm_pretrain_path[pretrain]
+                try:
+                    filename = pygmtools.utils.download(f'ngm_{pretrain}_jittor.pt', url, md5)
+                except:
+                    filename = os.path.dirname(__file__) + f'/temp/ngm_{pretrain}_jittor.pt'
+                _load_model(network, filename)
+            else:
+                raise ValueError(f'Unknown pretrain tag. Available tags: {ngm_pretrain_path.keys()}')
+
+    if forward_pass:
+        batch_num, n1, n2, n1max, n2max, n1n2, v0 = _check_and_init_gm(K, n1, n2, n1max, n2max, x0)
+        v0 = v0 / jt.mean(v0)
+        result = network(K, n1, n2, n1max, n2max, v0, sk_max_iter, sk_tau)
+    else:
+        result = None
+    return result, network
+
+
+#############################################
+#              Utils Functions              #
+#############################################
+
+def inner_prod_aff_fn(feat1, feat2):
+    """
+    Jittor implementation of inner product affinity function
+    """
+    return jt.matmul(feat1, feat2.transpose(1, 2))
+
+
+def gaussian_aff_fn(feat1, feat2, sigma):
+    """
+    Jittor implementation of Gaussian affinity function
+    """
+    feat1 = feat1.unsqueeze(2)
+    feat2 = feat2.unsqueeze(1)
+    return jt.exp(-((feat1 - feat2) ** 2).sum(dim=-1) / sigma)
+
+
+def build_batch(input, return_ori_dim=False):
+    """
+    Jittor implementation of building a batched Var
+    """
+    assert type(input[0]) == jt.Var
+
+    it = iter(input)
+    t = next(it)
+    max_shape = list(t.shape)
+    ori_shape = [[_] for _ in max_shape]
+    while True:
+        try:
+            t = next(it)
+            for i in range(len(max_shape)):
+                max_shape[i] = int(max(max_shape[i], t.shape[i]))
+                ori_shape[i].append(t.shape[i])
+        except StopIteration:
+            break
+    max_shape = np.array(max_shape)
+
+    padded_ts = []
+    for t in input:
+        pad_pattern = np.zeros(2 * len(max_shape), dtype=np.int64)
+        pad_pattern[::-2] = max_shape - np.array(t.shape)
+        pad_pattern = tuple(pad_pattern.tolist())
+        padded_ts.append(jt.nn.pad(t, pad_pattern, 'constant', 0))
+
+    if return_ori_dim:
+        return jt.stack(padded_ts, dim=0), tuple([jt.int64(_) for _ in ori_shape])
+    else:
+        return jt.stack(padded_ts, dim=0)
+
+
+def dense_to_sparse(dense_adj):
+    """
+    Jittor implementation of converting a dense adjacency matrix to a sparse matrix
+    """
+    batch_size = dense_adj.shape[0]
+    conn, ori_shape = build_batch([jt.nonzero(a) for a in dense_adj], return_ori_dim=True)
+    nedges = ori_shape[0]
+    edge_weight = build_batch([dense_adj[b][(conn[b, :, 0], conn[b, :, 1])] for b in range(batch_size)])
+    return conn, edge_weight.unsqueeze(-1), nedges
+
+
+def compute_affinity_score(X, K):
+    """
+    Jittor implementation of computing affinity score
+    """
+    b, n, _ = X.size()
+    vx = X.transpose(1, 2).reshape(b, -1, 1)  # (b, n*n, 1)
+    vxt = vx.transpose(1, 2)  # (b, 1, n*n)
+    affinity = jt.bmm(jt.bmm(vxt, K), vx)
+    return affinity
+
+
+def to_numpy(input):
+    """
+    Jittor function to_numpy
+    """
+    return input.detach().numpy()
+
+def from_numpy(input, device=None):
+    """
+    Jittor function from_numpy
+    """
+    return jt.Var(input)
+
+
+def generate_isomorphic_graphs(node_num, graph_num, node_feat_dim):
+    """
+    Jittor implementation of generate_isomorphic_graphs
+    """
+    X_gt = jt.zeros((graph_num, node_num, node_num))
+    X_gt[0, jt.arange(0, node_num, dtype=jt.int64), jt.arange(0, node_num, dtype=jt.int64)] = 1
+    for i in range(graph_num):
+        if i > 0:
+            X_gt[i, jt.arange(0, node_num, dtype=jt.int64), jt.randperm(node_num)] = 1
+    joint_X = X_gt.reshape(graph_num * node_num, node_num)
+    X_gt = jt.matmul(joint_X, joint_X.t())
+    X_gt = X_gt.reshape(graph_num, node_num, graph_num, node_num).permute(0, 2, 1, 3)
+    A0 = jt.rand(node_num, node_num)
+    A0[jt.arange(node_num), jt.arange(node_num)] = 0
+    As = [A0]
+    for i in range(graph_num):
+        if i > 0:
+            As.append(jt.matmul(jt.matmul(X_gt[i, 0], A0), X_gt[0, i]))
+    if node_feat_dim > 0:
+        F0 = jt.rand(node_num, node_feat_dim)
+        Fs = [F0]
+        for i in range(graph_num):
+            if i > 0:
+                Fs.append(jt.matmul(X_gt[i, 0], F0))
+        return jt.stack(As, dim=0), X_gt, jt.stack(Fs, dim=0)
+    else:
+        return jt.stack(As, dim=0), X_gt
+
+def permutation_loss(pred_dsmat: Var, gt_perm: Var, n1: Var, n2: Var) -> Var:
+    """
+    Jittor implementation of permutation_loss
+    """
+    batch_num = pred_dsmat.shape[0]
+
+    pred_dsmat = pred_dsmat.float32()
+
+    if not jt.all((pred_dsmat >= 0) * (pred_dsmat <= 1)):
+        raise ValueError("pred_dsmat contains invalid numerical entries.")
+    if not jt.all((gt_perm >= 0) * (gt_perm <= 1)):
+        raise ValueError("gt_perm contains invalid numerical entries.")
+
+    if n1 is None:
+        n1 = jt.Var([pred_dsmat.shape[1] for _ in range(batch_num)])
+    if n2 is None:
+        n2 = jt.Var([pred_dsmat.shape[2] for _ in range(batch_num)])
+
+    loss = jt.Var(0.)
+    n_sum = jt.zeros_like(loss)
+    for b in range(batch_num):
+        loss += jt.nn.bce_loss(
+            pred_dsmat[b, 0:n1[b].item(), 0:n2[b].item()],
+            gt_perm[b, 0:n1[b].item(), 0:n2[b].item()],
+            size_average=False).sum()
+        n_sum += n1[b]
+
+    return loss / n_sum
+
+def _get_shape(input):
+    """
+    Jittor implementation of _get_shape
+    """
+    return input.shape
+
+def _check_and_init_gm(K, n1, n2, n1max, n2max, x0):
+    # get batch number
+    batch_num = K.shape[0]
+    n1n2 = K.shape[1]
+
+    # get values of n1, n2, n1max, n2max and check
+    if n1 is None:
+        n1 = jt.full((batch_num,), n1max, dtype=jt.int)
+    if n2 is None:
+        n2 = jt.full((batch_num,), n2max, dtype=jt.int)
+    if n1max is None:
+        n1max = jt.max(n1).item()
+    if n2max is None:
+        n2max = jt.max(n2).item()
+
+    assert n1max * n2max == n1n2, 'the input size of K does not match with n1max * n2max!'
+
+    # initialize x0 (also v0)
+    if x0 is None:
+        x0 = jt.zeros((batch_num, int(n1max), int(n2max)), dtype=K.dtype)
+        for b in range(batch_num):
+            x0[b, 0:int(n1[b].item()), 0:int(n2[b].item())] = jt.Var(1.) / (n1[b] * n2[b])
+    v0 = x0.transpose(1, 2).reshape(batch_num, n1n2, 1)
+
+    return batch_num, n1, n2, n1max, n2max, n1n2, v0
+
+
+def _check_data_type(input: Var, var_name, raise_err):
+    """
+    Jittor implementation of _check_data_type
+    """
+    if raise_err and type(input) is not Var:
+        raise ValueError(f'Expected Jittor Var{f" for variable {var_name}" if var_name is not None else ""}, '
+                         f'but got {type(input)}. Perhaps the wrong backend?')
+    return type(input) is Var
+
+def _check_shape(input, dim_num):
+    """
+    Jittor implementation of _check_shape
+    """
+    return len(input.shape) == dim_num
+
+def _aff_mat_from_node_edge_aff(node_aff: Var, edge_aff: Var, connectivity1: Var, connectivity2: Var,
+                                n1, n2, ne1, ne2):
+    """
+    Jittor implementation of _aff_mat_from_node_edge_aff
+    """
+    if edge_aff is not None:
+        dtype = edge_aff.dtype
+        batch_size = edge_aff.shape[0]
+        if n1 is None:
+            n1 = jt.max(jt.max(connectivity1, dim=-1), dim=-1) + 1
+        if n2 is None:
+            n2 = jt.max(jt.max(connectivity2, dim=-1), dim=-1) + 1
+        if ne1 is None:
+            ne1 = jt.Var([edge_aff.shape[1]] * batch_size)
+        if ne2 is None:
+            ne2 = jt.Var([edge_aff.shape[2]] * batch_size)
+    else:
+        dtype = node_aff.dtype
+        batch_size = node_aff.shape[0]
+        if n1 is None:
+            n1 = jt.Var([node_aff.shape[1]] * batch_size)
+        if n2 is None:
+            n2 = jt.Var([node_aff.shape[2]] * batch_size)
+
+    n1max = int(max(n1).item())
+    n2max = int(max(n2).item())
+    ks = []
+    for b in range(batch_size):
+        k = jt.zeros((n2max, n1max, n2max, n1max), dtype=dtype)
+        # edge-wise affinity
+        if edge_aff is not None:
+            conn1 = connectivity1[b][:int(ne1[b])]
+            conn2 = connectivity2[b][:int(ne2[b])]
+
+            edge_indices = jt.concat([conn1.repeat_interleave(int(ne2[b]), dim=0), conn2.repeat(int(ne1[b]), 1)], dim=1) # indices: start_g1, end_g1, start_g2, end_g2
+            edge_indices = (edge_indices[:, 2], edge_indices[:, 0], edge_indices[:, 3], edge_indices[:, 1]) # indices: start_g2, start_g1, end_g2, end_g1
+            k[edge_indices] = edge_aff[b, :int(ne1[b]), :int(ne2[b])].reshape(-1)
+        k = k.reshape(n2max * n1max, n2max * n1max)
+        # node-wise affinity
+        if node_aff is not None:
+            k[jt.arange(n2max * n1max), jt.arange(n2max * n1max)] = node_aff[b].transpose(0, 1).reshape(-1)
+        ks.append(k)
+
+    return jt.stack(ks, dim=0)
+
+def _squeeze(input, dim):
+    """
+    Jittor implementation of _squeeze
+    """
+    return input.squeeze(dim)
+
+def _unsqueeze(input, dim):
+    """
+    Jittor implementation of _unsqueeze
+    """
+    return input.unsqueeze(dim)
+
+def _transpose(input, dim1, dim2):
+    """
+    Jittor implementaiton of _transpose
+    """
+    return input.transpose(dim1, dim2)
+
+def _mm(input1, input2):
+    """
+    Jittor implementation of _mm
+    """
+    return jt.matmul(input1, input2)
+
+def _save_model(model, path):
+    """
+    Save Jittor model to a given path
+    """
+    if isinstance(model, jt.nn.DataParallel):
+        model = model.module
+
+    jt.save(model.state_dict(), path)
+
+def _load_model(model, path):
+    """
+    Load Jittor model from a given path. Unmatched keys shall be shown in jittor warning.
+    """
+    module = model
+    module.load_state_dict(jt.load(path))
```

### Comparing `pygmtools-0.3.8/pygmtools/jittor_modules.py` & `pygmtools-0.3.8a0/pygmtools/jittor_modules.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,289 +1,289 @@
-# Copyright (c) 2022 Thinklab@SJTU
-# pygmtools is licensed under Mulan PSL v2.
-# You can use this software according to the terms and conditions of the Mulan PSL v2.
-# You may obtain a copy of Mulan PSL v2 at:
-# http://license.coscl.org.cn/MulanPSL2
-# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND,
-# EITHER EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT,
-# MERCHANTABILITY OR FIT FOR A PARTICULAR PURPOSE.
-# See the Mulan PSL v2 for more details.
-
-import jittor as jt
-from jittor import Module, nn, Var
-from jittor.nn import Sequential
-from typing import Tuple, Optional, List, Union
-import math
-
-
-############################################
-#            Affinity Modules              #
-############################################
-
-
-class WeightedInnerProdAffinity(Module):
-    """
-    Weighted inner product affinity layer to compute the affinity matrix from feature space.
-    M = X * A * Y^T
-    Parameter: scale of weight d
-    Input: feature X, Y
-    Output: affinity matrix M
-    """
-    def __init__(self, d):
-        super(WeightedInnerProdAffinity, self).__init__()
-        self.d = d
-        self.A = jt.rand(self.d, self.d)
-        self.reset_parameters()
-
-    def reset_parameters(self):
-        stdv = 1. / math.sqrt(self.d)
-        self.A.uniform_(-stdv, stdv)
-        self.A.data += jt.init.eye(self.d)
-
-    def execute(self, X, Y):
-        assert X.shape[2] == Y.shape[2] == self.d
-        M = jt.matmul(X, self.A)
-        M = jt.matmul(M, Y.transpose(1, 2))
-        return M
-
-
-############################################
-#         Graph Convolution Modules        #
-############################################
-
-
-class Gconv(Module):
-    r"""
-    Graph Convolutional Layer which is inspired and developed based on Graph Convolutional Network (GCN).
-    Inspired by `Kipf and Welling. Semi-Supervised Classification with Graph Convolutional Networks. ICLR 2017.
-    <https://arxiv.org/abs/1609.02907>`_
-
-    :param in_features: the dimension of input node features
-    :param out_features: the dimension of output node features
-    """
-    def __init__(self, in_features: int, out_features: int):
-        super(Gconv, self).__init__()
-        self.num_inputs = in_features
-        self.num_outputs = out_features
-        self.a_fc = nn.Linear(self.num_inputs, self.num_outputs)
-        self.u_fc = nn.Linear(self.num_inputs, self.num_outputs)
-
-    def execute(self, A: Var, x: Var, norm: bool=True) -> Var:
-        r"""
-        Forward computation of graph convolution network.
-
-        :param A: :math:`(b\times n\times n)` {0,1} adjacency matrix. :math:`b`: batch size, :math:`n`: number of nodes
-        :param x: :math:`(b\times n\times d)` input node embedding. :math:`d`: feature dimension
-        :param norm: normalize connectivity matrix or not
-        :return: :math:`(b\times n\times d^\prime)` new node embedding
-        """
-        if norm is True:
-            A = _l1_normalize(A, dim=-2)
-        ax = self.a_fc(x)
-        ux = self.u_fc(x)
-        x = jt.bmm(A, nn.relu(ax)) + nn.relu(ux) # has size (bs, N, num_outputs)
-        return x
-
-
-class ChannelIndependentConv(Module):
-    r"""
-    Channel Independent Embedding Convolution.
-    Proposed by `"Yu et al. Learning deep graph matching with channel-independent embedding and Hungarian attention.
-    ICLR 2020." <https://openreview.net/forum?id=rJgBd2NYPH>`_
-
-    :param in_features: the dimension of input node features
-    :param out_features: the dimension of output node features
-    :param in_edges: the dimension of input edge features
-    :param out_edges: (optional) the dimension of output edge features. It needs to be the same as ``out_features``
-    """
-    def __init__(self, in_features: int, out_features: int, in_edges: int, out_edges: int=None):
-        super(ChannelIndependentConv, self).__init__()
-        if out_edges is None:
-            out_edges = out_features
-        self.in_features = in_features
-        self.out_features = out_features
-        self.out_edges = out_edges
-        # self.node_fc = nn.Linear(in_features, out_features // self.out_edges)
-        self.node_fc = nn.Linear(in_features, out_features)
-        self.node_sfc = nn.Linear(in_features, out_features)
-        self.edge_fc = nn.Linear(in_edges, self.out_edges)
-
-    def execute(self, A: Var, emb_node: Var, emb_edge: Var, mode: int=1) -> Tuple[Var, Var]:
-        r"""
-        :param A: :math:`(b\times n\times n)` {0,1} adjacency matrix. :math:`b`: batch size, :math:`n`: number of nodes
-        :param emb_node: :math:`(b\times n\times d_n)` input node embedding. :math:`d_n`: node feature dimension
-        :param emb_edge: :math:`(b\times n\times n\times d_e)` input edge embedding. :math:`d_e`: edge feature dimension
-        :param mode: 1 or 2, refer to the paper for details
-        :return: :math:`(b\times n\times d^\prime)` new node embedding,
-         :math:`(b\times n\times n\times d^\prime)` new edge embedding
-        """
-        if mode == 1:
-            node_x = self.node_fc(emb_node)
-            node_sx = self.node_sfc(emb_node)
-            edge_x = self.edge_fc(emb_edge)
-
-            A = A.unsqueeze(-1)
-            A = jt.multiply(A.expand_as(edge_x), edge_x)
-
-            node_x = jt.matmul(A.transpose(2, 3).transpose(1, 2),
-                                  node_x.unsqueeze(2).transpose(2, 3).transpose(1, 2))
-            node_x = node_x.squeeze(-1).transpose(1, 2)
-            node_x = nn.relu(node_x) + nn.relu(node_sx)
-            edge_x = nn.relu(edge_x)
-
-            return node_x, edge_x
-
-        # The following code lines are not called in pygmtools
-        # elif mode == 2:
-        #     node_x = self.node_fc(emb_node)
-        #     node_sx = self.node_sfc(emb_node)
-        #     edge_x = self.edge_fc(emb_edge)
-        #
-        #     d_x = node_x.unsqueeze(1) - node_x.unsqueeze(2)
-        #     d_x = jt.sum(d_x ** 2, dim=3, keepdim=False)
-        #     d_x = jt.exp(-d_x)
-        #
-        #     A = A.unsqueeze(-1)
-        #     A = jt.multiply(A.expand_as(edge_x), edge_x)
-        #
-        #     node_x = jt.matmul(A.transpose(2, 3).transpose(1, 2),
-        #                           node_x.unsqueeze(2).transpose(2, 3).transpose(1, 2))
-        #     node_x = node_x.squeeze(-1).transpose(1, 2)
-        #     node_x = nn.relu(node_x) + nn.relu(node_sx)
-        #     edge_x = nn.relu(edge_x)
-        #     return node_x, edge_x
-
-        else:
-            raise ValueError('Unknown mode {}. Possible options: 1 or 2'.format(mode))
-
-
-class Siamese_Gconv(Module):
-    r"""
-    Siamese Gconv neural network for processing arbitrary number of graphs.
-
-    :param in_features: the dimension of input node features
-    :param num_features: the dimension of output node features
-    """
-    def __init__(self, in_features, num_features):
-        super(Siamese_Gconv, self).__init__()
-        self.gconv = Gconv(in_features, num_features)
-
-    def execute(self, g1: Tuple[Var, Var, Var, int], *args) -> Union[Var, List[Var]]:
-        r"""
-        Forward computation of Siamese Gconv.
-
-        :param g1: The first graph, which is a tuple of (:math:`(b\times n\times n)` {0,1} adjacency matrix,
-         :math:`(b\times n\times d)` input node embedding, normalize connectivity matrix or not)
-        :param args: Other graphs
-        :return: A list of tensors composed of new node embeddings :math:`(b\times n\times d^\prime)`
-        """
-        # embx are tensors of size (bs, N, num_features)
-        emb1 = self.gconv(*g1)
-        if len(args) == 0:
-            return emb1
-        else:
-            returns = [emb1]
-            for g in args:
-                returns.append(self.gconv(*g))
-            return returns
-
-
-class Siamese_ChannelIndependentConv(Module):
-    r"""
-    Siamese Channel Independent Conv neural network for processing arbitrary number of graphs.
-
-    :param in_features: the dimension of input node features
-    :param num_features: the dimension of output node features
-    :param in_edges: the dimension of input edge features
-    :param out_edges: (optional) the dimension of output edge features. It needs to be the same as ``num_features``
-    """
-    def __init__(self, in_features, num_features, in_edges, out_edges=None):
-        super(Siamese_ChannelIndependentConv, self).__init__()
-        self.in_feature = in_features
-        self.gconv = ChannelIndependentConv(in_features, num_features, in_edges, out_edges)
-
-    def execute(self, g1: Tuple[Var, Var, Optional[bool]], *args) -> List[Var]:
-        r"""
-        Forward computation of Siamese Channel Independent Conv.
-
-        :param g1: The first graph, which is a tuple of (:math:`(b\times n\times n)` {0,1} adjacency matrix,
-         :math:`(b\times n\times d_n)` input node embedding, :math:`(b\times n\times n\times d_e)` input edge embedding,
-         mode (``1`` or ``2``))
-        :param args: Other graphs
-        :return: A list of tensors composed of new node embeddings :math:`(b\times n\times d^\prime)`, appended with new
-         edge embeddings :math:`(b\times n\times n\times d^\prime)`
-        """
-        emb1, emb_edge1 = self.gconv(*g1)
-        embs = [emb1]
-        emb_edges = [emb_edge1]
-        for g in args:
-            emb2, emb_edge2 = self.gconv(*g)
-            embs.append(emb2), emb_edges.append(emb_edge2)
-        return embs + emb_edges
-
-
-class NGMConvLayer(Module):
-    def __init__(self, in_node_features, in_edge_features, out_node_features, out_edge_features,
-                 sk_channel=0):
-        super(NGMConvLayer, self).__init__()
-        self.in_nfeat = in_node_features
-        self.in_efeat = in_edge_features
-        self.out_efeat = out_edge_features
-        self.sk_channel = sk_channel
-        assert out_node_features == out_edge_features + self.sk_channel
-        if self.sk_channel > 0:
-            self.out_nfeat = out_node_features - self.sk_channel
-            self.classifier = nn.Linear(self.out_nfeat, self.sk_channel)
-        else:
-            self.out_nfeat = out_node_features
-            self.classifier = None
-
-        self.n_func = nn.Sequential(
-            nn.Linear(self.in_nfeat, self.out_nfeat),
-            #nn.Linear(self.in_nfeat, self.out_nfeat // self.out_efeat),
-            nn.ReLU(),
-            nn.Linear(self.out_nfeat, self.out_nfeat),
-            #nn.Linear(self.out_nfeat // self.out_efeat, self.out_nfeat // self.out_efeat),
-            nn.ReLU(),
-        )
-
-        self.n_self_func = nn.Sequential(
-            nn.Linear(self.in_nfeat, self.out_nfeat),
-            nn.ReLU(),
-            nn.Linear(self.out_nfeat, self.out_nfeat),
-            nn.ReLU()
-        )
-
-    def execute(self, A, W, x, n1=None, n2=None, norm=True, sk_func=None):
-        """
-        :param A: adjacent matrix in 0/1 (b x n x n)
-        :param W: edge feature tensor (b x n x n x feat_dim)
-        :param x: node feature tensor (b x n x feat_dim)
-        """
-        W_new = W
-
-        if norm is True:
-            A = jt.normalize(A, p=1, dim=2)
-            A[jt.isnan(A)] = 0
-
-        x1 = self.n_func(x)
-        x2 = jt.matmul((A.unsqueeze(-1) * W_new).permute(0, 3, 1, 2), x1.unsqueeze(2).permute(0, 3, 1, 2)).squeeze(-1).transpose(1, 2)
-        x2 += self.n_self_func(x)
-
-        if self.classifier is not None:
-            assert n1.max() * n2.max() == x.shape[1]
-            assert sk_func is not None
-            x3 = self.classifier(x2)
-            n1_rep = jt.repeat_interleave(n1, self.sk_channel, dim=0)
-            n2_rep = jt.repeat_interleave(n2, self.sk_channel, dim=0)
-            x4 = x3.permute(0,2,1).reshape((x.shape[0] * self.sk_channel, n2.max().item(), n1.max().item())).transpose(1, 2)
-            x5 = sk_func(x4, n1_rep, n2_rep, dummy_row=True).transpose(2, 1) #.contiguous()
-
-            x6 = x5.reshape((x.shape[0], self.sk_channel, n1.max().item() * n2.max().item())).permute(0, 2, 1)
-            x_new = jt.concat((x2, x6), dim=-1)
-        else:
-            x_new = x2
-
-        return W_new, x_new
-
-def _l1_normalize(input: Var, dim=-1, eps=1e-12):
-    return input / input.abs().sum(dim, keepdims=True).maximum(eps)
+# Copyright (c) 2022 Thinklab@SJTU
+# pygmtools is licensed under Mulan PSL v2.
+# You can use this software according to the terms and conditions of the Mulan PSL v2.
+# You may obtain a copy of Mulan PSL v2 at:
+# http://license.coscl.org.cn/MulanPSL2
+# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND,
+# EITHER EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT,
+# MERCHANTABILITY OR FIT FOR A PARTICULAR PURPOSE.
+# See the Mulan PSL v2 for more details.
+
+import jittor as jt
+from jittor import Module, nn, Var
+from jittor.nn import Sequential
+from typing import Tuple, Optional, List, Union
+import math
+
+
+############################################
+#            Affinity Modules              #
+############################################
+
+
+class WeightedInnerProdAffinity(Module):
+    """
+    Weighted inner product affinity layer to compute the affinity matrix from feature space.
+    M = X * A * Y^T
+    Parameter: scale of weight d
+    Input: feature X, Y
+    Output: affinity matrix M
+    """
+    def __init__(self, d):
+        super(WeightedInnerProdAffinity, self).__init__()
+        self.d = d
+        self.A = jt.rand(self.d, self.d)
+        self.reset_parameters()
+
+    def reset_parameters(self):
+        stdv = 1. / math.sqrt(self.d)
+        self.A.uniform_(-stdv, stdv)
+        self.A.data += jt.init.eye(self.d)
+
+    def execute(self, X, Y):
+        assert X.shape[2] == Y.shape[2] == self.d
+        M = jt.matmul(X, self.A)
+        M = jt.matmul(M, Y.transpose(1, 2))
+        return M
+
+
+############################################
+#         Graph Convolution Modules        #
+############################################
+
+
+class Gconv(Module):
+    r"""
+    Graph Convolutional Layer which is inspired and developed based on Graph Convolutional Network (GCN).
+    Inspired by `Kipf and Welling. Semi-Supervised Classification with Graph Convolutional Networks. ICLR 2017.
+    <https://arxiv.org/abs/1609.02907>`_
+
+    :param in_features: the dimension of input node features
+    :param out_features: the dimension of output node features
+    """
+    def __init__(self, in_features: int, out_features: int):
+        super(Gconv, self).__init__()
+        self.num_inputs = in_features
+        self.num_outputs = out_features
+        self.a_fc = nn.Linear(self.num_inputs, self.num_outputs)
+        self.u_fc = nn.Linear(self.num_inputs, self.num_outputs)
+
+    def execute(self, A: Var, x: Var, norm: bool=True) -> Var:
+        r"""
+        Forward computation of graph convolution network.
+
+        :param A: :math:`(b\times n\times n)` {0,1} adjacency matrix. :math:`b`: batch size, :math:`n`: number of nodes
+        :param x: :math:`(b\times n\times d)` input node embedding. :math:`d`: feature dimension
+        :param norm: normalize connectivity matrix or not
+        :return: :math:`(b\times n\times d^\prime)` new node embedding
+        """
+        if norm is True:
+            A = _l1_normalize(A, dim=-2)
+        ax = self.a_fc(x)
+        ux = self.u_fc(x)
+        x = jt.bmm(A, nn.relu(ax)) + nn.relu(ux) # has size (bs, N, num_outputs)
+        return x
+
+
+class ChannelIndependentConv(Module):
+    r"""
+    Channel Independent Embedding Convolution.
+    Proposed by `"Yu et al. Learning deep graph matching with channel-independent embedding and Hungarian attention.
+    ICLR 2020." <https://openreview.net/forum?id=rJgBd2NYPH>`_
+
+    :param in_features: the dimension of input node features
+    :param out_features: the dimension of output node features
+    :param in_edges: the dimension of input edge features
+    :param out_edges: (optional) the dimension of output edge features. It needs to be the same as ``out_features``
+    """
+    def __init__(self, in_features: int, out_features: int, in_edges: int, out_edges: int=None):
+        super(ChannelIndependentConv, self).__init__()
+        if out_edges is None:
+            out_edges = out_features
+        self.in_features = in_features
+        self.out_features = out_features
+        self.out_edges = out_edges
+        # self.node_fc = nn.Linear(in_features, out_features // self.out_edges)
+        self.node_fc = nn.Linear(in_features, out_features)
+        self.node_sfc = nn.Linear(in_features, out_features)
+        self.edge_fc = nn.Linear(in_edges, self.out_edges)
+
+    def execute(self, A: Var, emb_node: Var, emb_edge: Var, mode: int=1) -> Tuple[Var, Var]:
+        r"""
+        :param A: :math:`(b\times n\times n)` {0,1} adjacency matrix. :math:`b`: batch size, :math:`n`: number of nodes
+        :param emb_node: :math:`(b\times n\times d_n)` input node embedding. :math:`d_n`: node feature dimension
+        :param emb_edge: :math:`(b\times n\times n\times d_e)` input edge embedding. :math:`d_e`: edge feature dimension
+        :param mode: 1 or 2, refer to the paper for details
+        :return: :math:`(b\times n\times d^\prime)` new node embedding,
+         :math:`(b\times n\times n\times d^\prime)` new edge embedding
+        """
+        if mode == 1:
+            node_x = self.node_fc(emb_node)
+            node_sx = self.node_sfc(emb_node)
+            edge_x = self.edge_fc(emb_edge)
+
+            A = A.unsqueeze(-1)
+            A = jt.multiply(A.expand_as(edge_x), edge_x)
+
+            node_x = jt.matmul(A.transpose(2, 3).transpose(1, 2),
+                                  node_x.unsqueeze(2).transpose(2, 3).transpose(1, 2))
+            node_x = node_x.squeeze(-1).transpose(1, 2)
+            node_x = nn.relu(node_x) + nn.relu(node_sx)
+            edge_x = nn.relu(edge_x)
+
+            return node_x, edge_x
+
+        # The following code lines are not called in pygmtools
+        # elif mode == 2:
+        #     node_x = self.node_fc(emb_node)
+        #     node_sx = self.node_sfc(emb_node)
+        #     edge_x = self.edge_fc(emb_edge)
+        #
+        #     d_x = node_x.unsqueeze(1) - node_x.unsqueeze(2)
+        #     d_x = jt.sum(d_x ** 2, dim=3, keepdim=False)
+        #     d_x = jt.exp(-d_x)
+        #
+        #     A = A.unsqueeze(-1)
+        #     A = jt.multiply(A.expand_as(edge_x), edge_x)
+        #
+        #     node_x = jt.matmul(A.transpose(2, 3).transpose(1, 2),
+        #                           node_x.unsqueeze(2).transpose(2, 3).transpose(1, 2))
+        #     node_x = node_x.squeeze(-1).transpose(1, 2)
+        #     node_x = nn.relu(node_x) + nn.relu(node_sx)
+        #     edge_x = nn.relu(edge_x)
+        #     return node_x, edge_x
+
+        else:
+            raise ValueError('Unknown mode {}. Possible options: 1 or 2'.format(mode))
+
+
+class Siamese_Gconv(Module):
+    r"""
+    Siamese Gconv neural network for processing arbitrary number of graphs.
+
+    :param in_features: the dimension of input node features
+    :param num_features: the dimension of output node features
+    """
+    def __init__(self, in_features, num_features):
+        super(Siamese_Gconv, self).__init__()
+        self.gconv = Gconv(in_features, num_features)
+
+    def execute(self, g1: Tuple[Var, Var, Var, int], *args) -> Union[Var, List[Var]]:
+        r"""
+        Forward computation of Siamese Gconv.
+
+        :param g1: The first graph, which is a tuple of (:math:`(b\times n\times n)` {0,1} adjacency matrix,
+         :math:`(b\times n\times d)` input node embedding, normalize connectivity matrix or not)
+        :param args: Other graphs
+        :return: A list of tensors composed of new node embeddings :math:`(b\times n\times d^\prime)`
+        """
+        # embx are tensors of size (bs, N, num_features)
+        emb1 = self.gconv(*g1)
+        if len(args) == 0:
+            return emb1
+        else:
+            returns = [emb1]
+            for g in args:
+                returns.append(self.gconv(*g))
+            return returns
+
+
+class Siamese_ChannelIndependentConv(Module):
+    r"""
+    Siamese Channel Independent Conv neural network for processing arbitrary number of graphs.
+
+    :param in_features: the dimension of input node features
+    :param num_features: the dimension of output node features
+    :param in_edges: the dimension of input edge features
+    :param out_edges: (optional) the dimension of output edge features. It needs to be the same as ``num_features``
+    """
+    def __init__(self, in_features, num_features, in_edges, out_edges=None):
+        super(Siamese_ChannelIndependentConv, self).__init__()
+        self.in_feature = in_features
+        self.gconv = ChannelIndependentConv(in_features, num_features, in_edges, out_edges)
+
+    def execute(self, g1: Tuple[Var, Var, Optional[bool]], *args) -> List[Var]:
+        r"""
+        Forward computation of Siamese Channel Independent Conv.
+
+        :param g1: The first graph, which is a tuple of (:math:`(b\times n\times n)` {0,1} adjacency matrix,
+         :math:`(b\times n\times d_n)` input node embedding, :math:`(b\times n\times n\times d_e)` input edge embedding,
+         mode (``1`` or ``2``))
+        :param args: Other graphs
+        :return: A list of tensors composed of new node embeddings :math:`(b\times n\times d^\prime)`, appended with new
+         edge embeddings :math:`(b\times n\times n\times d^\prime)`
+        """
+        emb1, emb_edge1 = self.gconv(*g1)
+        embs = [emb1]
+        emb_edges = [emb_edge1]
+        for g in args:
+            emb2, emb_edge2 = self.gconv(*g)
+            embs.append(emb2), emb_edges.append(emb_edge2)
+        return embs + emb_edges
+
+
+class NGMConvLayer(Module):
+    def __init__(self, in_node_features, in_edge_features, out_node_features, out_edge_features,
+                 sk_channel=0):
+        super(NGMConvLayer, self).__init__()
+        self.in_nfeat = in_node_features
+        self.in_efeat = in_edge_features
+        self.out_efeat = out_edge_features
+        self.sk_channel = sk_channel
+        assert out_node_features == out_edge_features + self.sk_channel
+        if self.sk_channel > 0:
+            self.out_nfeat = out_node_features - self.sk_channel
+            self.classifier = nn.Linear(self.out_nfeat, self.sk_channel)
+        else:
+            self.out_nfeat = out_node_features
+            self.classifier = None
+
+        self.n_func = nn.Sequential(
+            nn.Linear(self.in_nfeat, self.out_nfeat),
+            #nn.Linear(self.in_nfeat, self.out_nfeat // self.out_efeat),
+            nn.ReLU(),
+            nn.Linear(self.out_nfeat, self.out_nfeat),
+            #nn.Linear(self.out_nfeat // self.out_efeat, self.out_nfeat // self.out_efeat),
+            nn.ReLU(),
+        )
+
+        self.n_self_func = nn.Sequential(
+            nn.Linear(self.in_nfeat, self.out_nfeat),
+            nn.ReLU(),
+            nn.Linear(self.out_nfeat, self.out_nfeat),
+            nn.ReLU()
+        )
+
+    def execute(self, A, W, x, n1=None, n2=None, norm=True, sk_func=None):
+        """
+        :param A: adjacent matrix in 0/1 (b x n x n)
+        :param W: edge feature tensor (b x n x n x feat_dim)
+        :param x: node feature tensor (b x n x feat_dim)
+        """
+        W_new = W
+
+        if norm is True:
+            A = jt.normalize(A, p=1, dim=2)
+            A[jt.isnan(A)] = 0
+
+        x1 = self.n_func(x)
+        x2 = jt.matmul((A.unsqueeze(-1) * W_new).permute(0, 3, 1, 2), x1.unsqueeze(2).permute(0, 3, 1, 2)).squeeze(-1).transpose(1, 2)
+        x2 += self.n_self_func(x)
+
+        if self.classifier is not None:
+            assert n1.max() * n2.max() == x.shape[1]
+            assert sk_func is not None
+            x3 = self.classifier(x2)
+            n1_rep = jt.repeat_interleave(n1, self.sk_channel, dim=0)
+            n2_rep = jt.repeat_interleave(n2, self.sk_channel, dim=0)
+            x4 = x3.permute(0,2,1).reshape((x.shape[0] * self.sk_channel, n2.max().item(), n1.max().item())).transpose(1, 2)
+            x5 = sk_func(x4, n1_rep, n2_rep, dummy_row=True).transpose(2, 1) #.contiguous()
+
+            x6 = x5.reshape((x.shape[0], self.sk_channel, n1.max().item() * n2.max().item())).permute(0, 2, 1)
+            x_new = jt.concat((x2, x6), dim=-1)
+        else:
+            x_new = x2
+
+        return W_new, x_new
+
+def _l1_normalize(input: Var, dim=-1, eps=1e-12):
+    return input / input.abs().sum(dim, keepdims=True).maximum(eps)
```

### Comparing `pygmtools-0.3.8/pygmtools/mindspore_backend.py` & `pygmtools-0.3.8a0/pygmtools/mindspore_backend.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,553 +1,553 @@
-from multiprocessing import Pool
-import numpy as np
-import mindspore
-import mindspore.nn as nn
-from mindspore.ops import stop_gradient
-import math
-
-#############################################
-#     Linear Assignment Problem Solvers     #
-#############################################
-
-from pygmtools.numpy_backend import _hung_kernel
-
-
-def hungarian(s: mindspore.Tensor, n1: mindspore.Tensor = None, n2: mindspore.Tensor = None,
-              unmatch1: mindspore.Tensor = None, unmatch2: mindspore.Tensor = None,
-              nproc: int = 1) -> mindspore.Tensor:
-    """
-    mindspore implementation of Hungarian algorithm
-    """
-    # device = s.device
-    batch_num = s.shape[0]
-
-    perm_mat = stop_gradient(s).asnumpy() * -1
-    if n1 is not None:
-        n1 = n1.asnumpy()
-    else:
-        n1 = [None] * batch_num
-    if n2 is not None:
-        n2 = n2.asnumpy()
-    else:
-        n2 = [None] * batch_num
-    if unmatch1 is not None:
-        unmatch1 = -unmatch1.asnumpy()
-    else:
-        unmatch1 = [None] * batch_num
-    if unmatch2 is not None:
-        unmatch2 = -unmatch2.asnumpy()
-    else:
-        unmatch2 = [None] * batch_num
-
-    if nproc > 1:
-        with Pool(processes=nproc) as pool:
-            mapresult = pool.starmap_async(_hung_kernel, zip(perm_mat, n1, n2, unmatch1, unmatch2))
-            perm_mat = np.stack(mapresult.get())
-    else:
-        perm_mat = np.stack(
-            [_hung_kernel(perm_mat[b], n1[b], n2[b], unmatch1[b], unmatch2[b]) for b in range(batch_num)])
-
-    perm_mat = mindspore.Tensor(perm_mat)
-
-    return perm_mat
-
-
-def sinkhorn(s: mindspore.Tensor, nrows: mindspore.Tensor = None, ncols: mindspore.Tensor = None,
-             unmatchrows: mindspore.Tensor = None, unmatchcols: mindspore.Tensor = None,
-             dummy_row: bool = False, max_iter: int = 10, tau: float = 1.,
-             batched_operation: bool = False) -> mindspore.Tensor:
-    """
-    mindspore implementation of Sinkhorn algorithm
-    """
-    batch_size = s.shape[0]
-
-    if s.shape[2] >= s.shape[1]:
-        transposed = False
-    else:
-        s = s.swapaxes(1, 2)
-        nrows, ncols = ncols, nrows
-        unmatchrows, unmatchcols = unmatchcols, unmatchrows
-        transposed = True
-
-    if nrows is None:
-        nrows = mindspore.Tensor([s.shape[1] for _ in range(batch_size)])
-    if ncols is None:
-        ncols = mindspore.Tensor([s.shape[2] for _ in range(batch_size)])
-
-    # ensure that in each dimension we have nrow < ncol
-    transposed_batch = nrows > ncols
-    if transposed_batch.any():
-        s_t = s.swapaxes(1, 2)
-        s_t = mindspore.ops.concat((
-            s_t[:, :s.shape[1], :],
-            mindspore.numpy.full((batch_size, s.shape[1], s.shape[2] - s.shape[1]), -float('inf'))),
-            axis=2)
-        s = mindspore.numpy.where(transposed_batch.view(batch_size, 1, 1), s_t, s)
-
-        new_nrows = mindspore.numpy.where(transposed_batch, ncols, nrows)
-        new_ncols = mindspore.numpy.where(transposed_batch, nrows, ncols)
-        nrows = new_nrows
-        ncols = new_ncols
-
-        if unmatchrows is not None and unmatchcols is not None:
-            unmatchrows_pad = mindspore.ops.concat((
-                unmatchrows,
-                mindspore.numpy.full((batch_size, unmatchcols.shape[1] - unmatchrows.shape[1]),
-                                     -float('inf'))),
-                axis=1)
-            new_unmatchrows = mindspore.numpy.where(transposed_batch.view(batch_size, 1), unmatchcols, unmatchrows_pad)[
-                              :,
-                              :unmatchrows.shape[1]]
-            new_unmatchcols = mindspore.numpy.where(transposed_batch.view(batch_size, 1), unmatchrows_pad, unmatchcols)
-            unmatchrows = new_unmatchrows
-            unmatchcols = new_unmatchcols
-
-    # operations are performed on log_s
-    log_s = s / tau
-    if unmatchrows is not None and unmatchcols is not None:
-        unmatchrows = unmatchrows / tau
-        unmatchcols = unmatchcols / tau
-
-    if dummy_row:
-        assert log_s.shape[2] >= log_s.shape[1]
-        dummy_shape = list(log_s.shape)
-        dummy_shape[1] = log_s.shape[2] - log_s.shape[1]
-        ori_nrows = nrows
-        nrows = ncols.copy()
-        log_s = mindspore.ops.concat((log_s, mindspore.numpy.full(dummy_shape, -float('inf'), dtype=log_s.dtype)),
-                                     axis=1)
-        if unmatchrows is not None:
-            unmatchrows = mindspore.ops.concat((unmatchrows,
-                                                mindspore.numpy.full((dummy_shape[0], dummy_shape[1]),
-                                                                     -float('inf'), dtype=log_s.dtype
-                                                                     )), axis=1)
-        for b in range(batch_size):
-            log_s[b, int(ori_nrows[b]):int(nrows[b]), :int(ncols[b])] = -100
-
-    # assign the unmatch weights
-    if unmatchrows is not None and unmatchcols is not None:
-        new_log_s = mindspore.numpy.full((log_s.shape[0], log_s.shape[1] + 1, log_s.shape[2] + 1),
-                                         -float('inf'), dtype=log_s.dtype
-                                         )
-        new_log_s[:, :-1, :-1] = log_s
-        log_s = new_log_s
-        for b in range(batch_size):
-            r, c = int(nrows[b]), int(ncols[b])
-            log_s[b, 0:r, c] = unmatchrows[b, 0:r]
-            log_s[b, r, 0:c] = unmatchcols[b, 0:c]
-    row_mask = mindspore.numpy.zeros((batch_size, log_s.shape[1], 1), dtype=mindspore.bool_)
-    col_mask = mindspore.numpy.zeros((batch_size, 1, log_s.shape[2]), dtype=mindspore.bool_)
-    for b in range(batch_size):
-        r, c = int(nrows[b]), int(ncols[b])
-        row_mask[b, 0:r, 0] = 1
-        col_mask[b, 0, 0:c] = 1
-    if unmatchrows is not None and unmatchcols is not None:
-        ncols += 1
-        nrows += 1
-
-    if batched_operation:
-        for b in range(batch_size):
-            log_s[b, int(nrows[b]):, :] = -float('inf')
-            log_s[b, :, int(ncols[b]):] = -float('inf')
-
-        for i in range(max_iter):
-            if i % 2 == 0:
-                index, m = mindspore.ops.max(log_s, axis=2, keep_dims=True)
-                log_sum = mindspore.ops.logsumexp(log_s - m, 2, keep_dims=True) + m
-                log_s = log_s - mindspore.numpy.where(row_mask, log_sum, mindspore.numpy.zeros_like(log_sum))
-                assert not mindspore.ops.isnan(log_s).any()
-            else:
-                index, m = mindspore.ops.max(log_s, axis=1, keep_dims=True)
-                log_sum = mindspore.ops.logsumexp(log_s - m, 1, keep_dims=True) + m
-                log_s = log_s - mindspore.numpy.where(col_mask, log_sum, mindspore.numpy.zeros_like(log_sum))
-                assert not mindspore.ops.isnan(log_s).any()
-
-        ret_log_s = log_s
-    else:
-        ret_log_s = mindspore.numpy.full((batch_size, log_s.shape[1], log_s.shape[2]), -float('inf'), dtype=log_s.dtype)
-
-        for b in range(batch_size):
-            row_slice = slice(0, int(nrows[b]))
-            col_slice = slice(0, int(ncols[b]))
-            log_s_b = log_s[b, row_slice, col_slice]
-            row_mask_b = row_mask[b, row_slice, :]
-            col_mask_b = col_mask[b, :, col_slice]
-
-            for i in range(max_iter):
-                if i % 2 == 0:
-                    index, m = mindspore.ops.max(log_s_b, axis=1, keep_dims=True)
-                    log_sum = mindspore.ops.logsumexp(log_s_b - m, 1, keep_dims=True) + m
-                    log_s_b = log_s_b - mindspore.numpy.where(row_mask_b, log_sum, mindspore.numpy.zeros_like(log_sum))
-                else:
-                    index, m = mindspore.ops.max(log_s_b, axis=0, keep_dims=True)
-                    log_sum = mindspore.ops.logsumexp(log_s_b - m, 0, keep_dims=True) + m
-                    log_s_b = log_s_b - mindspore.numpy.where(col_mask_b, log_sum, mindspore.numpy.zeros_like(log_sum))
-
-            ret_log_s[b, row_slice, col_slice] = log_s_b
-
-    if unmatchrows is not None and unmatchcols is not None:
-        ncols -= 1
-        nrows -= 1
-        for b in range(batch_size):
-            ret_log_s[b, :nrows[b] + 1, ncols[b]] = -float('inf')
-            ret_log_s[b, nrows[b], :ncols[b]] = -float('inf')
-        ret_log_s = ret_log_s[:, :-1, :-1]
-
-    if dummy_row:
-        if dummy_shape[1] > 0:
-            ret_log_s = ret_log_s[:, :-dummy_shape[1]]
-        for b in range(batch_size):
-            ret_log_s[b, ori_nrows[b]:nrows[b], :ncols[b]] = -float('inf')
-
-    if transposed_batch.any():
-        s_t = ret_log_s.swapaxes(1, 2)
-        s_t = mindspore.ops.concat((
-            s_t[:, :ret_log_s.shape[1], :],
-            mindspore.numpy.full((batch_size, ret_log_s.shape[1], ret_log_s.shape[2] - ret_log_s.shape[1]),
-                                 -float('inf'), )), axis=2)
-        ret_log_s = mindspore.numpy.where(transposed_batch.view(batch_size, 1, 1), s_t, ret_log_s)
-
-    if transposed:
-        ret_log_s = ret_log_s.swapaxes(1, 2)
-
-    return mindspore.ops.exp(ret_log_s)
-
-
-#############################################
-#    Quadratic Assignment Problem Solvers   #
-#############################################
-
-
-def rrwm(K: mindspore.Tensor, n1: mindspore.Tensor, n2: mindspore.Tensor, n1max, n2max, x0: mindspore.Tensor,
-         max_iter: int, sk_iter: int, alpha: float, beta: float) -> mindspore.Tensor:
-    """
-    mindspore implementation of RRWM algorithm.
-    """
-    batch_num, n1, n2, n1max, n2max, n1n2, v0 = _check_and_init_gm(K, n1, n2, n1max, n2max, x0)
-    # rescale the values in K
-    d = K.sum(axis=2, keepdims=True)
-    dmax = d.max(axis=1, keepdims=True)
-    K = K / (dmax + d.min() * 1e-5)
-    v = v0
-    for i in range(max_iter):
-        # random walk
-        v = mindspore.ops.BatchMatMul()(K, v)
-        last_v = v
-        n = mindspore.ops.norm(v, axis=1, p=1, keep_dims=True)
-        v = v / n
-
-        # reweighted jump
-        s = v.view(batch_num, int(n2max), int(n1max)).swapaxes(1, 2)
-        s = beta * s / s.max(axis=1, keepdims=True).max(axis=2, keepdims=True)
-        v = alpha * sinkhorn(s, n1, n2, max_iter=sk_iter).swapaxes(1, 2).reshape(batch_num, n1n2, 1) + \
-            (1 - alpha) * v
-        n = mindspore.ops.norm(v, axis=1, p=1, keep_dims=True)
-        v = mindspore.ops.matmul(v, 1 / n)
-
-        if (v - last_v).sum().sqrt() < 1e-5:
-            break
-
-    return v.view(batch_num, int(n2max), int(n1max)).swapaxes(1, 2)
-
-
-def sm(K: mindspore.Tensor, n1: mindspore.Tensor, n2: mindspore.Tensor, n1max, n2max, x0: mindspore.Tensor,
-       max_iter: int) -> mindspore.Tensor:
-    """
-    mindspore implementation of SM algorithm.
-    """
-    batch_num, n1, n2, n1max, n2max, n1n2, v0 = _check_and_init_gm(K, n1, n2, n1max, n2max, x0)
-    v = vlast = v0
-    for i in range(max_iter):
-        v = mindspore.ops.BatchMatMul()(K, v)
-        n = mindspore.ops.norm(v, axis=1, p=2)
-        v = mindspore.ops.matmul(v, (1 / n).view(batch_num, 1, 1))
-        if (v - vlast).sum().sqrt() < 1e-5:
-            break
-        vlast = v
-
-    x = v.view(batch_num, int(n2max), int(n1max)).swapaxes(1, 2)
-    return x
-
-
-def ipfp(K: mindspore.Tensor, n1: mindspore.Tensor, n2: mindspore.Tensor, n1max, n2max, x0: mindspore.Tensor,
-         max_iter) -> mindspore.Tensor:
-    """
-    mindspore implementation of IPFP algorithm
-    """
-    batch_num, n1, n2, n1max, n2max, n1n2, v0 = _check_and_init_gm(K, n1, n2, n1max, n2max, x0)
-    v = v0
-    last_v = v
-
-    def comp_obj_score(v1, K, v2):
-        return mindspore.ops.BatchMatMul()(mindspore.ops.BatchMatMul()(v1.view(batch_num, 1, -1), K), v2)
-
-    for i in range(max_iter):
-        cost = mindspore.ops.BatchMatMul()(K, v).reshape(batch_num, int(n2max), int(n1max)).swapaxes(1, 2)
-        binary_sol = hungarian(cost, n1, n2)
-        binary_v = binary_sol.swapaxes(1, 2).view(batch_num, -1, 1)
-        alpha = comp_obj_score(v, K, binary_v - v)  # + torch.mm(k_diag.view(1, -1), (binary_sol - v).view(-1, 1))
-        beta = comp_obj_score(binary_v - v, K, binary_v - v)
-        t0 = alpha / beta
-        v = mindspore.numpy.where(mindspore.ops.logical_or(beta <= 0, t0 >= 1), binary_v, v + t0 * (binary_v - v))
-        last_v_sol = comp_obj_score(last_v, K, last_v)
-        if (mindspore.ops.max(mindspore.ops.abs(
-                last_v_sol - mindspore.ops.BatchMatMul()(cost.reshape((batch_num, 1, -1)),
-                                                         binary_sol.reshape((batch_num, -1, 1)))
-        ) / last_v_sol)[1] < 1e-3).any():
-            break
-        last_v = v
-
-    pred_x = binary_sol
-    return pred_x
-
-
-def _check_and_init_gm(K, n1, n2, n1max, n2max, x0):
-    # get batch number
-    batch_num = K.shape[0]
-    n1n2 = K.shape[1]
-
-    # get values of n1, n2, n1max, n2max and check
-    if n1 is None:
-        n1 = mindspore.numpy.full((batch_num,), n1max, dtype=mindspore.numpy.int_)
-    if n2 is None:
-        n2 = mindspore.numpy.full((batch_num,), n2max, dtype=mindspore.numpy.int_)
-    if n1max is None:
-        n1max = mindspore.ops.max(n1)[1]
-    if n2max is None:
-        n2max = mindspore.ops.max(n2)[1]
-
-    assert n1max * n2max == n1n2, 'the input size of K does not match with n1max * n2max!'
-
-    # initialize x0 (also v0)
-    if x0 is None:
-        x0 = mindspore.numpy.zeros((batch_num, int(n1max), int(n2max)), dtype=K.dtype)
-        for b in range(batch_num):
-            x0[b, 0:n1[b], 0:n2[b]] = mindspore.Tensor(1.) / (n1[b] * n2[b])
-    v0 = x0.swapaxes(1, 2).reshape((batch_num, n1n2, 1))
-
-    return batch_num, n1, n2, n1max, n2max, n1n2, v0
-
-
-#############################################
-#              Utils Functions              #
-#############################################
-
-def inner_prod_aff_fn(feat1, feat2):
-    """
-    mindspore implementation of inner product affinity function
-    """
-    return mindspore.ops.matmul(feat1, feat2.swapaxes(1, 2))
-
-
-def gaussian_aff_fn(feat1, feat2, sigma):
-    """
-    mindspore implementation of Gaussian affinity function
-    """
-    feat1 = mindspore.ops.expand_dims(feat1, axis=2)
-    feat2 = mindspore.ops.expand_dims(feat2, axis=1)
-    return mindspore.ops.exp(-((feat1 - feat2) ** 2).sum(axis=-1) / sigma)
-
-
-def build_batch(input, return_ori_dim=False):
-    """
-    mindspore implementation of building a batched tensor
-    """
-    assert type(input[0]) == mindspore.Tensor
-    # device = input[0].device
-    it = iter(input)
-    t = next(it)
-    max_shape = list(t.shape)
-    ori_shape = [[_] for _ in max_shape]
-    while True:
-        try:
-            t = next(it)
-            for i in range(len(max_shape)):
-                max_shape[i] = int(max(max_shape[i], t.shape[i]))
-                ori_shape[i].append(t.shape[i])
-        except StopIteration:
-            break
-    max_shape = np.array(max_shape)
-
-    padded_ts = []
-    for t in input:
-        pad_pattern = np.zeros(2 * len(max_shape), dtype=np.int64)
-        pad_pattern[::-2] = max_shape - np.array(t.shape)
-        pad_list = list((pad_pattern[2 * i], pad_pattern[2 * i + 1]) for i in range(int(len(pad_pattern) / 2)))
-        while len(pad_list) < t.ndim:
-            pad_list.append((0, 0))
-        pad_list.reverse()
-        pad_pattern = tuple(pad_list)
-        mindspore_pad = nn.Pad(pad_pattern, mode="CONSTANT")
-        padded_ts.append(mindspore_pad(t))
-
-    if return_ori_dim:
-        return mindspore.ops.stack(padded_ts, axis=0), tuple(
-            [mindspore.Tensor(_, dtype=mindspore.int64) for _ in ori_shape])
-    else:
-        return mindspore.ops.stack(padded_ts, axis=0)
-
-
-def dense_to_sparse(dense_adj):
-    """
-    mindspore implementation of converting a dense adjacency matrix to a sparse matrix
-    """
-    batch_size = dense_adj.shape[0]
-    conn, ori_shape = build_batch([mindspore.ops.nonzero(a) for a in dense_adj], return_ori_dim=True)
-    nedges = ori_shape[0]
-    edge_weight = build_batch([dense_adj[b][(conn[b, :, 0], conn[b, :, 1])] for b in range(batch_size)])
-    return conn, mindspore.ops.expand_dims(edge_weight, axis=-1), nedges
-
-
-def to_numpy(input):
-    """
-    mindspore function to_numpy
-    """
-    return stop_gradient(input).asnumpy()
-
-
-def from_numpy(input, device=None):
-    """
-    mindspore function from_numpy
-    """
-    return mindspore.Tensor(input)
-
-
-def permutation_loss(pred_dsmat: mindspore.Tensor, gt_perm: mindspore.Tensor, n1: mindspore.Tensor,
-                     n2: mindspore.Tensor) -> mindspore.Tensor:
-    """
-    Pytorch implementation of permutation_loss
-    """
-    batch_num = pred_dsmat.shape[0]
-
-    pred_dsmat = mindspore.Tensor(pred_dsmat, dtype=mindspore.float32)
-
-    if not mindspore.ops.logical_and(pred_dsmat >= 0, pred_dsmat <= 1).all:
-        raise ValueError("pred_dsmat contains invalid numerical entries.")
-    if not mindspore.ops.logical_and(gt_perm >= 0, gt_perm <= 1).all:
-        raise ValueError("gt_perm contains invalid numerical entries.")
-
-    if n1 is None:
-        n1 = mindspore.Tensor([pred_dsmat.shape[1] for _ in range(batch_num)])
-    if n2 is None:
-        n2 = mindspore.Tensor([pred_dsmat.shape[2] for _ in range(batch_num)])
-
-    loss = mindspore.Tensor(0.)
-    n_sum = mindspore.ops.zeros_like(loss)
-    for b in range(batch_num):
-        batch_slice = [b, slice(n1[b]), slice(n2[b])]
-        weight = mindspore.ops.ones_like(pred_dsmat[batch_slice])
-        loss += mindspore.ops.BinaryCrossEntropy(reduction='sum')(
-            pred_dsmat[batch_slice],
-            gt_perm[batch_slice], weight)
-        n1_b = mindspore.Tensor(n1[b], dtype=n_sum.dtype)
-        n_sum += n1_b
-
-    return loss / n_sum
-
-
-def _aff_mat_from_node_edge_aff(node_aff: mindspore.Tensor, edge_aff: mindspore.Tensor, connectivity1: mindspore.Tensor,
-                                connectivity2: mindspore.Tensor,
-                                n1, n2, ne1, ne2):
-    """
-    mindspore implementation of _aff_mat_from_node_edge_aff
-    """
-    if edge_aff is not None:
-        # device = edge_aff.device
-        dtype = edge_aff.dtype
-        batch_size = edge_aff.shape[0]
-        if n1 is None:
-            n1 = mindspore.Tensor([math.sqrt(connectivity1.shape[1])] * batch_size)
-        if n2 is None:
-            n2 = mindspore.Tensor([math.sqrt(connectivity2.shape[1])] * batch_size)
-        if ne1 is None:
-            ne1 = [edge_aff.shape[1]] * batch_size
-        if ne2 is None:
-            ne2 = [edge_aff.shape[2]] * batch_size
-    else:
-        # device = node_aff.device
-        dtype = node_aff.dtype
-        batch_size = node_aff.shape[0]
-        if n1 is None:
-            n1 = [node_aff.shape[1]] * batch_size
-        if n2 is None:
-            n2 = [node_aff.shape[2]] * batch_size
-
-    n1max = int(max(n1))
-    n2max = int(max(n2))
-    ks = []
-    for b in range(batch_size):
-        k = mindspore.numpy.zeros((n2max, n1max, n2max, n1max), dtype=dtype)
-        # edge-wise affinity
-        if edge_aff is not None:
-            conn1 = connectivity1[b][:int(ne1[b])]
-            conn2 = connectivity2[b][:int(ne2[b])]
-            edge_indices = mindspore.ops.concat(
-                [mindspore.ops.repeat_elements(conn1, int(ne2[b]), axis=0),
-                 mindspore.numpy.tile(conn2, (int(ne1[b]), 1))],
-                axis=1)  # indices: start_g1, end_g1, start_g2, end_g2
-            edge_indices = (edge_indices[:, 2], edge_indices[:, 0], edge_indices[:, 3],
-                            edge_indices[:, 1])  # indices: start_g2, start_g1, end_g2, end_g1
-            k[edge_indices] = edge_aff[b, :int(ne1[b]), :int(ne2[b])].reshape(-1)
-        k = k.reshape((n2max * n1max, n2max * n1max))
-        # node-wise affinity
-        if node_aff is not None:
-            k[mindspore.numpy.arange(n2max * n1max), mindspore.numpy.arange(n2max * n1max)] = node_aff[b].transpose(1,
-                                                                                                                    0).reshape(
-                -1)
-            # k_diag = mindspore.numpy.diagonal(k)
-            # k_diag[:] = node_aff[b].transpose(0, 1).reshape(-1)
-        ks.append(k)
-
-    return mindspore.ops.stack(ks, axis=0)
-
-
-def _check_data_type(input: mindspore.Tensor, var_name, raise_err):
-    """
-    mindspore implementation of _check_data_type
-    """
-    if raise_err and type(input) is not mindspore.Tensor:
-        raise ValueError(f'Expected mindspore Tensor{f" for variable {var_name}" if var_name is not None else ""}, '
-                         f'but got {type(input)}. Perhaps the wrong backend?')
-    return type(input) is mindspore.Tensor
-
-
-def _check_shape(input, dim_num):
-    """
-    mindspore implementation of _check_shape
-    """
-    return len(input.shape) == dim_num
-
-
-def _get_shape(input):
-    """
-    mindspore implementation of _get_shape
-    """
-    return input.shape
-
-
-def _squeeze(input, dim):
-    """
-    mindspore implementation of _squeeze
-    """
-    return mindspore.ops.squeeze(input, axis=dim)
-
-
-def _unsqueeze(input, dim):
-    """
-    mindspore implementation of _unsqueeze
-    """
-    return mindspore.ops.expand_dims(input, axis=dim)
-
-
-def _transpose(input, dim1, dim2):
-    """
-    mindspore implementaiton of _transpose
-    """
-    return input.swapaxes(dim1, dim2)
-
-
-def _mm(input1, input2):
-    """
-    mindspore implementation of _mm
-    """
-    return mindspore.ops.matmul(input1, input2)
+from multiprocessing import Pool
+import numpy as np
+import mindspore
+import mindspore.nn as nn
+from mindspore.ops import stop_gradient
+import math
+
+#############################################
+#     Linear Assignment Problem Solvers     #
+#############################################
+
+from pygmtools.numpy_backend import _hung_kernel
+
+
+def hungarian(s: mindspore.Tensor, n1: mindspore.Tensor = None, n2: mindspore.Tensor = None,
+              unmatch1: mindspore.Tensor = None, unmatch2: mindspore.Tensor = None,
+              nproc: int = 1) -> mindspore.Tensor:
+    """
+    mindspore implementation of Hungarian algorithm
+    """
+    # device = s.device
+    batch_num = s.shape[0]
+
+    perm_mat = stop_gradient(s).asnumpy() * -1
+    if n1 is not None:
+        n1 = n1.asnumpy()
+    else:
+        n1 = [None] * batch_num
+    if n2 is not None:
+        n2 = n2.asnumpy()
+    else:
+        n2 = [None] * batch_num
+    if unmatch1 is not None:
+        unmatch1 = -unmatch1.asnumpy()
+    else:
+        unmatch1 = [None] * batch_num
+    if unmatch2 is not None:
+        unmatch2 = -unmatch2.asnumpy()
+    else:
+        unmatch2 = [None] * batch_num
+
+    if nproc > 1:
+        with Pool(processes=nproc) as pool:
+            mapresult = pool.starmap_async(_hung_kernel, zip(perm_mat, n1, n2, unmatch1, unmatch2))
+            perm_mat = np.stack(mapresult.get())
+    else:
+        perm_mat = np.stack(
+            [_hung_kernel(perm_mat[b], n1[b], n2[b], unmatch1[b], unmatch2[b]) for b in range(batch_num)])
+
+    perm_mat = mindspore.Tensor(perm_mat)
+
+    return perm_mat
+
+
+def sinkhorn(s: mindspore.Tensor, nrows: mindspore.Tensor = None, ncols: mindspore.Tensor = None,
+             unmatchrows: mindspore.Tensor = None, unmatchcols: mindspore.Tensor = None,
+             dummy_row: bool = False, max_iter: int = 10, tau: float = 1.,
+             batched_operation: bool = False) -> mindspore.Tensor:
+    """
+    mindspore implementation of Sinkhorn algorithm
+    """
+    batch_size = s.shape[0]
+
+    if s.shape[2] >= s.shape[1]:
+        transposed = False
+    else:
+        s = s.swapaxes(1, 2)
+        nrows, ncols = ncols, nrows
+        unmatchrows, unmatchcols = unmatchcols, unmatchrows
+        transposed = True
+
+    if nrows is None:
+        nrows = mindspore.Tensor([s.shape[1] for _ in range(batch_size)])
+    if ncols is None:
+        ncols = mindspore.Tensor([s.shape[2] for _ in range(batch_size)])
+
+    # ensure that in each dimension we have nrow < ncol
+    transposed_batch = nrows > ncols
+    if transposed_batch.any():
+        s_t = s.swapaxes(1, 2)
+        s_t = mindspore.ops.concat((
+            s_t[:, :s.shape[1], :],
+            mindspore.numpy.full((batch_size, s.shape[1], s.shape[2] - s.shape[1]), -float('inf'))),
+            axis=2)
+        s = mindspore.numpy.where(transposed_batch.view(batch_size, 1, 1), s_t, s)
+
+        new_nrows = mindspore.numpy.where(transposed_batch, ncols, nrows)
+        new_ncols = mindspore.numpy.where(transposed_batch, nrows, ncols)
+        nrows = new_nrows
+        ncols = new_ncols
+
+        if unmatchrows is not None and unmatchcols is not None:
+            unmatchrows_pad = mindspore.ops.concat((
+                unmatchrows,
+                mindspore.numpy.full((batch_size, unmatchcols.shape[1] - unmatchrows.shape[1]),
+                                     -float('inf'))),
+                axis=1)
+            new_unmatchrows = mindspore.numpy.where(transposed_batch.view(batch_size, 1), unmatchcols, unmatchrows_pad)[
+                              :,
+                              :unmatchrows.shape[1]]
+            new_unmatchcols = mindspore.numpy.where(transposed_batch.view(batch_size, 1), unmatchrows_pad, unmatchcols)
+            unmatchrows = new_unmatchrows
+            unmatchcols = new_unmatchcols
+
+    # operations are performed on log_s
+    log_s = s / tau
+    if unmatchrows is not None and unmatchcols is not None:
+        unmatchrows = unmatchrows / tau
+        unmatchcols = unmatchcols / tau
+
+    if dummy_row:
+        assert log_s.shape[2] >= log_s.shape[1]
+        dummy_shape = list(log_s.shape)
+        dummy_shape[1] = log_s.shape[2] - log_s.shape[1]
+        ori_nrows = nrows
+        nrows = ncols.copy()
+        log_s = mindspore.ops.concat((log_s, mindspore.numpy.full(dummy_shape, -float('inf'), dtype=log_s.dtype)),
+                                     axis=1)
+        if unmatchrows is not None:
+            unmatchrows = mindspore.ops.concat((unmatchrows,
+                                                mindspore.numpy.full((dummy_shape[0], dummy_shape[1]),
+                                                                     -float('inf'), dtype=log_s.dtype
+                                                                     )), axis=1)
+        for b in range(batch_size):
+            log_s[b, int(ori_nrows[b]):int(nrows[b]), :int(ncols[b])] = -100
+
+    # assign the unmatch weights
+    if unmatchrows is not None and unmatchcols is not None:
+        new_log_s = mindspore.numpy.full((log_s.shape[0], log_s.shape[1] + 1, log_s.shape[2] + 1),
+                                         -float('inf'), dtype=log_s.dtype
+                                         )
+        new_log_s[:, :-1, :-1] = log_s
+        log_s = new_log_s
+        for b in range(batch_size):
+            r, c = int(nrows[b]), int(ncols[b])
+            log_s[b, 0:r, c] = unmatchrows[b, 0:r]
+            log_s[b, r, 0:c] = unmatchcols[b, 0:c]
+    row_mask = mindspore.numpy.zeros((batch_size, log_s.shape[1], 1), dtype=mindspore.bool_)
+    col_mask = mindspore.numpy.zeros((batch_size, 1, log_s.shape[2]), dtype=mindspore.bool_)
+    for b in range(batch_size):
+        r, c = int(nrows[b]), int(ncols[b])
+        row_mask[b, 0:r, 0] = 1
+        col_mask[b, 0, 0:c] = 1
+    if unmatchrows is not None and unmatchcols is not None:
+        ncols += 1
+        nrows += 1
+
+    if batched_operation:
+        for b in range(batch_size):
+            log_s[b, int(nrows[b]):, :] = -float('inf')
+            log_s[b, :, int(ncols[b]):] = -float('inf')
+
+        for i in range(max_iter):
+            if i % 2 == 0:
+                index, m = mindspore.ops.max(log_s, axis=2, keep_dims=True)
+                log_sum = mindspore.ops.logsumexp(log_s - m, 2, keep_dims=True) + m
+                log_s = log_s - mindspore.numpy.where(row_mask, log_sum, mindspore.numpy.zeros_like(log_sum))
+                assert not mindspore.ops.isnan(log_s).any()
+            else:
+                index, m = mindspore.ops.max(log_s, axis=1, keep_dims=True)
+                log_sum = mindspore.ops.logsumexp(log_s - m, 1, keep_dims=True) + m
+                log_s = log_s - mindspore.numpy.where(col_mask, log_sum, mindspore.numpy.zeros_like(log_sum))
+                assert not mindspore.ops.isnan(log_s).any()
+
+        ret_log_s = log_s
+    else:
+        ret_log_s = mindspore.numpy.full((batch_size, log_s.shape[1], log_s.shape[2]), -float('inf'), dtype=log_s.dtype)
+
+        for b in range(batch_size):
+            row_slice = slice(0, int(nrows[b]))
+            col_slice = slice(0, int(ncols[b]))
+            log_s_b = log_s[b, row_slice, col_slice]
+            row_mask_b = row_mask[b, row_slice, :]
+            col_mask_b = col_mask[b, :, col_slice]
+
+            for i in range(max_iter):
+                if i % 2 == 0:
+                    index, m = mindspore.ops.max(log_s_b, axis=1, keep_dims=True)
+                    log_sum = mindspore.ops.logsumexp(log_s_b - m, 1, keep_dims=True) + m
+                    log_s_b = log_s_b - mindspore.numpy.where(row_mask_b, log_sum, mindspore.numpy.zeros_like(log_sum))
+                else:
+                    index, m = mindspore.ops.max(log_s_b, axis=0, keep_dims=True)
+                    log_sum = mindspore.ops.logsumexp(log_s_b - m, 0, keep_dims=True) + m
+                    log_s_b = log_s_b - mindspore.numpy.where(col_mask_b, log_sum, mindspore.numpy.zeros_like(log_sum))
+
+            ret_log_s[b, row_slice, col_slice] = log_s_b
+
+    if unmatchrows is not None and unmatchcols is not None:
+        ncols -= 1
+        nrows -= 1
+        for b in range(batch_size):
+            ret_log_s[b, :nrows[b] + 1, ncols[b]] = -float('inf')
+            ret_log_s[b, nrows[b], :ncols[b]] = -float('inf')
+        ret_log_s = ret_log_s[:, :-1, :-1]
+
+    if dummy_row:
+        if dummy_shape[1] > 0:
+            ret_log_s = ret_log_s[:, :-dummy_shape[1]]
+        for b in range(batch_size):
+            ret_log_s[b, ori_nrows[b]:nrows[b], :ncols[b]] = -float('inf')
+
+    if transposed_batch.any():
+        s_t = ret_log_s.swapaxes(1, 2)
+        s_t = mindspore.ops.concat((
+            s_t[:, :ret_log_s.shape[1], :],
+            mindspore.numpy.full((batch_size, ret_log_s.shape[1], ret_log_s.shape[2] - ret_log_s.shape[1]),
+                                 -float('inf'), )), axis=2)
+        ret_log_s = mindspore.numpy.where(transposed_batch.view(batch_size, 1, 1), s_t, ret_log_s)
+
+    if transposed:
+        ret_log_s = ret_log_s.swapaxes(1, 2)
+
+    return mindspore.ops.exp(ret_log_s)
+
+
+#############################################
+#    Quadratic Assignment Problem Solvers   #
+#############################################
+
+
+def rrwm(K: mindspore.Tensor, n1: mindspore.Tensor, n2: mindspore.Tensor, n1max, n2max, x0: mindspore.Tensor,
+         max_iter: int, sk_iter: int, alpha: float, beta: float) -> mindspore.Tensor:
+    """
+    mindspore implementation of RRWM algorithm.
+    """
+    batch_num, n1, n2, n1max, n2max, n1n2, v0 = _check_and_init_gm(K, n1, n2, n1max, n2max, x0)
+    # rescale the values in K
+    d = K.sum(axis=2, keepdims=True)
+    dmax = d.max(axis=1, keepdims=True)
+    K = K / (dmax + d.min() * 1e-5)
+    v = v0
+    for i in range(max_iter):
+        # random walk
+        v = mindspore.ops.BatchMatMul()(K, v)
+        last_v = v
+        n = mindspore.ops.norm(v, axis=1, p=1, keep_dims=True)
+        v = v / n
+
+        # reweighted jump
+        s = v.view(batch_num, int(n2max), int(n1max)).swapaxes(1, 2)
+        s = beta * s / s.max(axis=1, keepdims=True).max(axis=2, keepdims=True)
+        v = alpha * sinkhorn(s, n1, n2, max_iter=sk_iter).swapaxes(1, 2).reshape(batch_num, n1n2, 1) + \
+            (1 - alpha) * v
+        n = mindspore.ops.norm(v, axis=1, p=1, keep_dims=True)
+        v = mindspore.ops.matmul(v, 1 / n)
+
+        if (v - last_v).sum().sqrt() < 1e-5:
+            break
+
+    return v.view(batch_num, int(n2max), int(n1max)).swapaxes(1, 2)
+
+
+def sm(K: mindspore.Tensor, n1: mindspore.Tensor, n2: mindspore.Tensor, n1max, n2max, x0: mindspore.Tensor,
+       max_iter: int) -> mindspore.Tensor:
+    """
+    mindspore implementation of SM algorithm.
+    """
+    batch_num, n1, n2, n1max, n2max, n1n2, v0 = _check_and_init_gm(K, n1, n2, n1max, n2max, x0)
+    v = vlast = v0
+    for i in range(max_iter):
+        v = mindspore.ops.BatchMatMul()(K, v)
+        n = mindspore.ops.norm(v, axis=1, p=2)
+        v = mindspore.ops.matmul(v, (1 / n).view(batch_num, 1, 1))
+        if (v - vlast).sum().sqrt() < 1e-5:
+            break
+        vlast = v
+
+    x = v.view(batch_num, int(n2max), int(n1max)).swapaxes(1, 2)
+    return x
+
+
+def ipfp(K: mindspore.Tensor, n1: mindspore.Tensor, n2: mindspore.Tensor, n1max, n2max, x0: mindspore.Tensor,
+         max_iter) -> mindspore.Tensor:
+    """
+    mindspore implementation of IPFP algorithm
+    """
+    batch_num, n1, n2, n1max, n2max, n1n2, v0 = _check_and_init_gm(K, n1, n2, n1max, n2max, x0)
+    v = v0
+    last_v = v
+
+    def comp_obj_score(v1, K, v2):
+        return mindspore.ops.BatchMatMul()(mindspore.ops.BatchMatMul()(v1.view(batch_num, 1, -1), K), v2)
+
+    for i in range(max_iter):
+        cost = mindspore.ops.BatchMatMul()(K, v).reshape(batch_num, int(n2max), int(n1max)).swapaxes(1, 2)
+        binary_sol = hungarian(cost, n1, n2)
+        binary_v = binary_sol.swapaxes(1, 2).view(batch_num, -1, 1)
+        alpha = comp_obj_score(v, K, binary_v - v)  # + torch.mm(k_diag.view(1, -1), (binary_sol - v).view(-1, 1))
+        beta = comp_obj_score(binary_v - v, K, binary_v - v)
+        t0 = alpha / beta
+        v = mindspore.numpy.where(mindspore.ops.logical_or(beta <= 0, t0 >= 1), binary_v, v + t0 * (binary_v - v))
+        last_v_sol = comp_obj_score(last_v, K, last_v)
+        if (mindspore.ops.max(mindspore.ops.abs(
+                last_v_sol - mindspore.ops.BatchMatMul()(cost.reshape((batch_num, 1, -1)),
+                                                         binary_sol.reshape((batch_num, -1, 1)))
+        ) / last_v_sol)[1] < 1e-3).any():
+            break
+        last_v = v
+
+    pred_x = binary_sol
+    return pred_x
+
+
+def _check_and_init_gm(K, n1, n2, n1max, n2max, x0):
+    # get batch number
+    batch_num = K.shape[0]
+    n1n2 = K.shape[1]
+
+    # get values of n1, n2, n1max, n2max and check
+    if n1 is None:
+        n1 = mindspore.numpy.full((batch_num,), n1max, dtype=mindspore.numpy.int_)
+    if n2 is None:
+        n2 = mindspore.numpy.full((batch_num,), n2max, dtype=mindspore.numpy.int_)
+    if n1max is None:
+        n1max = mindspore.ops.max(n1)[1]
+    if n2max is None:
+        n2max = mindspore.ops.max(n2)[1]
+
+    assert n1max * n2max == n1n2, 'the input size of K does not match with n1max * n2max!'
+
+    # initialize x0 (also v0)
+    if x0 is None:
+        x0 = mindspore.numpy.zeros((batch_num, int(n1max), int(n2max)), dtype=K.dtype)
+        for b in range(batch_num):
+            x0[b, 0:n1[b], 0:n2[b]] = mindspore.Tensor(1.) / (n1[b] * n2[b])
+    v0 = x0.swapaxes(1, 2).reshape((batch_num, n1n2, 1))
+
+    return batch_num, n1, n2, n1max, n2max, n1n2, v0
+
+
+#############################################
+#              Utils Functions              #
+#############################################
+
+def inner_prod_aff_fn(feat1, feat2):
+    """
+    mindspore implementation of inner product affinity function
+    """
+    return mindspore.ops.matmul(feat1, feat2.swapaxes(1, 2))
+
+
+def gaussian_aff_fn(feat1, feat2, sigma):
+    """
+    mindspore implementation of Gaussian affinity function
+    """
+    feat1 = mindspore.ops.expand_dims(feat1, axis=2)
+    feat2 = mindspore.ops.expand_dims(feat2, axis=1)
+    return mindspore.ops.exp(-((feat1 - feat2) ** 2).sum(axis=-1) / sigma)
+
+
+def build_batch(input, return_ori_dim=False):
+    """
+    mindspore implementation of building a batched tensor
+    """
+    assert type(input[0]) == mindspore.Tensor
+    # device = input[0].device
+    it = iter(input)
+    t = next(it)
+    max_shape = list(t.shape)
+    ori_shape = [[_] for _ in max_shape]
+    while True:
+        try:
+            t = next(it)
+            for i in range(len(max_shape)):
+                max_shape[i] = int(max(max_shape[i], t.shape[i]))
+                ori_shape[i].append(t.shape[i])
+        except StopIteration:
+            break
+    max_shape = np.array(max_shape)
+
+    padded_ts = []
+    for t in input:
+        pad_pattern = np.zeros(2 * len(max_shape), dtype=np.int64)
+        pad_pattern[::-2] = max_shape - np.array(t.shape)
+        pad_list = list((pad_pattern[2 * i], pad_pattern[2 * i + 1]) for i in range(int(len(pad_pattern) / 2)))
+        while len(pad_list) < t.ndim:
+            pad_list.append((0, 0))
+        pad_list.reverse()
+        pad_pattern = tuple(pad_list)
+        mindspore_pad = nn.Pad(pad_pattern, mode="CONSTANT")
+        padded_ts.append(mindspore_pad(t))
+
+    if return_ori_dim:
+        return mindspore.ops.stack(padded_ts, axis=0), tuple(
+            [mindspore.Tensor(_, dtype=mindspore.int64) for _ in ori_shape])
+    else:
+        return mindspore.ops.stack(padded_ts, axis=0)
+
+
+def dense_to_sparse(dense_adj):
+    """
+    mindspore implementation of converting a dense adjacency matrix to a sparse matrix
+    """
+    batch_size = dense_adj.shape[0]
+    conn, ori_shape = build_batch([mindspore.ops.nonzero(a) for a in dense_adj], return_ori_dim=True)
+    nedges = ori_shape[0]
+    edge_weight = build_batch([dense_adj[b][(conn[b, :, 0], conn[b, :, 1])] for b in range(batch_size)])
+    return conn, mindspore.ops.expand_dims(edge_weight, axis=-1), nedges
+
+
+def to_numpy(input):
+    """
+    mindspore function to_numpy
+    """
+    return stop_gradient(input).asnumpy()
+
+
+def from_numpy(input, device=None):
+    """
+    mindspore function from_numpy
+    """
+    return mindspore.Tensor(input)
+
+
+def permutation_loss(pred_dsmat: mindspore.Tensor, gt_perm: mindspore.Tensor, n1: mindspore.Tensor,
+                     n2: mindspore.Tensor) -> mindspore.Tensor:
+    """
+    Pytorch implementation of permutation_loss
+    """
+    batch_num = pred_dsmat.shape[0]
+
+    pred_dsmat = mindspore.Tensor(pred_dsmat, dtype=mindspore.float32)
+
+    if not mindspore.ops.logical_and(pred_dsmat >= 0, pred_dsmat <= 1).all:
+        raise ValueError("pred_dsmat contains invalid numerical entries.")
+    if not mindspore.ops.logical_and(gt_perm >= 0, gt_perm <= 1).all:
+        raise ValueError("gt_perm contains invalid numerical entries.")
+
+    if n1 is None:
+        n1 = mindspore.Tensor([pred_dsmat.shape[1] for _ in range(batch_num)])
+    if n2 is None:
+        n2 = mindspore.Tensor([pred_dsmat.shape[2] for _ in range(batch_num)])
+
+    loss = mindspore.Tensor(0.)
+    n_sum = mindspore.ops.zeros_like(loss)
+    for b in range(batch_num):
+        batch_slice = [b, slice(n1[b]), slice(n2[b])]
+        weight = mindspore.ops.ones_like(pred_dsmat[batch_slice])
+        loss += mindspore.ops.BinaryCrossEntropy(reduction='sum')(
+            pred_dsmat[batch_slice],
+            gt_perm[batch_slice], weight)
+        n1_b = mindspore.Tensor(n1[b], dtype=n_sum.dtype)
+        n_sum += n1_b
+
+    return loss / n_sum
+
+
+def _aff_mat_from_node_edge_aff(node_aff: mindspore.Tensor, edge_aff: mindspore.Tensor, connectivity1: mindspore.Tensor,
+                                connectivity2: mindspore.Tensor,
+                                n1, n2, ne1, ne2):
+    """
+    mindspore implementation of _aff_mat_from_node_edge_aff
+    """
+    if edge_aff is not None:
+        # device = edge_aff.device
+        dtype = edge_aff.dtype
+        batch_size = edge_aff.shape[0]
+        if n1 is None:
+            n1 = mindspore.Tensor([math.sqrt(connectivity1.shape[1])] * batch_size)
+        if n2 is None:
+            n2 = mindspore.Tensor([math.sqrt(connectivity2.shape[1])] * batch_size)
+        if ne1 is None:
+            ne1 = [edge_aff.shape[1]] * batch_size
+        if ne2 is None:
+            ne2 = [edge_aff.shape[2]] * batch_size
+    else:
+        # device = node_aff.device
+        dtype = node_aff.dtype
+        batch_size = node_aff.shape[0]
+        if n1 is None:
+            n1 = [node_aff.shape[1]] * batch_size
+        if n2 is None:
+            n2 = [node_aff.shape[2]] * batch_size
+
+    n1max = int(max(n1))
+    n2max = int(max(n2))
+    ks = []
+    for b in range(batch_size):
+        k = mindspore.numpy.zeros((n2max, n1max, n2max, n1max), dtype=dtype)
+        # edge-wise affinity
+        if edge_aff is not None:
+            conn1 = connectivity1[b][:int(ne1[b])]
+            conn2 = connectivity2[b][:int(ne2[b])]
+            edge_indices = mindspore.ops.concat(
+                [mindspore.ops.repeat_elements(conn1, int(ne2[b]), axis=0),
+                 mindspore.numpy.tile(conn2, (int(ne1[b]), 1))],
+                axis=1)  # indices: start_g1, end_g1, start_g2, end_g2
+            edge_indices = (edge_indices[:, 2], edge_indices[:, 0], edge_indices[:, 3],
+                            edge_indices[:, 1])  # indices: start_g2, start_g1, end_g2, end_g1
+            k[edge_indices] = edge_aff[b, :int(ne1[b]), :int(ne2[b])].reshape(-1)
+        k = k.reshape((n2max * n1max, n2max * n1max))
+        # node-wise affinity
+        if node_aff is not None:
+            k[mindspore.numpy.arange(n2max * n1max), mindspore.numpy.arange(n2max * n1max)] = node_aff[b].transpose(1,
+                                                                                                                    0).reshape(
+                -1)
+            # k_diag = mindspore.numpy.diagonal(k)
+            # k_diag[:] = node_aff[b].transpose(0, 1).reshape(-1)
+        ks.append(k)
+
+    return mindspore.ops.stack(ks, axis=0)
+
+
+def _check_data_type(input: mindspore.Tensor, var_name, raise_err):
+    """
+    mindspore implementation of _check_data_type
+    """
+    if raise_err and type(input) is not mindspore.Tensor:
+        raise ValueError(f'Expected mindspore Tensor{f" for variable {var_name}" if var_name is not None else ""}, '
+                         f'but got {type(input)}. Perhaps the wrong backend?')
+    return type(input) is mindspore.Tensor
+
+
+def _check_shape(input, dim_num):
+    """
+    mindspore implementation of _check_shape
+    """
+    return len(input.shape) == dim_num
+
+
+def _get_shape(input):
+    """
+    mindspore implementation of _get_shape
+    """
+    return input.shape
+
+
+def _squeeze(input, dim):
+    """
+    mindspore implementation of _squeeze
+    """
+    return mindspore.ops.squeeze(input, axis=dim)
+
+
+def _unsqueeze(input, dim):
+    """
+    mindspore implementation of _unsqueeze
+    """
+    return mindspore.ops.expand_dims(input, axis=dim)
+
+
+def _transpose(input, dim1, dim2):
+    """
+    mindspore implementaiton of _transpose
+    """
+    return input.swapaxes(dim1, dim2)
+
+
+def _mm(input1, input2):
+    """
+    mindspore implementation of _mm
+    """
+    return mindspore.ops.matmul(input1, input2)
```

### Comparing `pygmtools-0.3.8/pygmtools/multi_graph_solvers.py` & `pygmtools-0.3.8a0/pygmtools/multi_graph_solvers.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,949 +1,949 @@
-r"""
-Classic (learning-free) **multi-graph matching** solvers. These multi-graph matching solvers are recommended to solve
-the joint matching problem of multiple graphs.
-"""
-
-# Copyright (c) 2022 Thinklab@SJTU
-# pygmtools is licensed under Mulan PSL v2.
-# You can use this software according to the terms and conditions of the Mulan PSL v2.
-# You may obtain a copy of Mulan PSL v2 at:
-# http://license.coscl.org.cn/MulanPSL2
-# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND,
-# EITHER EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT,
-# MERCHANTABILITY OR FIT FOR A PARTICULAR PURPOSE.
-# See the Mulan PSL v2 for more details.
-
-import functools
-import importlib
-import pygmtools
-from pygmtools.utils import NOT_IMPLEMENTED_MSG, _check_shape, _get_shape, _unsqueeze, _squeeze, _check_data_type
-import math
-
-
-def cao(K, x0=None, qap_solver=None,
-        mode='time',
-        max_iter=6, lambda_init=0.3, lambda_step=1.1, lambda_max=1.0, iter_boost=2,
-        backend=None):
-    r"""
-    Composition based Affinity Optimization (CAO) solver for multi-graph matching. This solver builds a supergraph for
-    matching update to incorporate the two aspects by optimizing the affinity score, meanwhile gradually
-    infusing the consistency.
-
-    Each update step is described as follows:
-
-    .. math::
-
-        \arg \max_{k} (1-\lambda) J(\mathbf{X}_{ik} \mathbf{X}_{kj}) + \lambda C_p(\mathbf{X}_{ik} \mathbf{X}_{kj})
-
-    where :math:`J(\mathbf{X}_{ik} \mathbf{X}_{kj})` is the objective score, and
-    :math:`C_p(\mathbf{X}_{ik} \mathbf{X}_{kj})` measures a consistency score compared to other matchings. These two
-    terms are balanced by :math:`\lambda`, and :math:`\lambda` starts from a smaller number and gradually grows.
-
-    :param K: :math:`(m\times m \times n^2 \times n^2)` the input affinity matrix, where ``K[i,j]`` is the affinity
-              matrix of graph ``i`` and graph ``j`` (:math:`m`: number of nodes)
-    :param x0: (optional) :math:`(m\times m \times n \times n)` the initial two-graph matching result, where ``X[i,j]``
-               is the matching matrix result of graph ``i`` and graph ``j``. If this argument is not given,
-               ``qap_solver`` will be used to compute the two-graph matching result.
-    :param qap_solver: (default: pygm.rrwm) a function object that accepts a batched affinity matrix and returns the
-                       matching matrices. It is suggested to use ``functools.partial`` and the QAP solvers provided in
-                       the :mod:`~pygmtools.classic_solvers` module (see examples below).
-    :param mode: (default: ``'time'``) the operation mode of this algorithm. Options: ``'time', 'memory'``,
-                 where ``'time'`` is a time-efficient version and ``'memory'`` is a memory-efficient version.
-    :param max_iter: (default: 6) max number of iterations
-    :param lambda_init: (default: 0.3) initial value of :math:`\lambda`, with :math:`\lambda\in[0,1]`
-    :param lambda_step: (default: 1.1) the increase step size of :math:`\lambda`, updated by ``lambda = step * lambda``
-    :param lambda_max: (default: 1.0) the max value of lambda
-    :param iter_boost: (default: 2) to boost the convergence of the CAO algorithm, :math:`\lambda` will be forced to
-                       update every ``iter_boost`` iterations.
-    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
-    :return: :math:`(m\times m \times n \times n)` the multi-graph matching result
-
-    .. note::
-
-        The input graphs must have the same number of nodes for this algorithm to work correctly.
-
-    .. note::
-
-       Multi-graph matching methods process all graphs at once and do not support the additional batch dimension. Please
-       note that this behavior is different from two-graph matching solvers in :mod:`~pygmtools.classic_solvers`.
-    
-    .. dropdown:: Numpy Example
-
-        ::
-
-            >>> import numpy as np
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'numpy'
-            >>> np.random.seed(1)
-
-            # Generate 10 isomorphic graphs
-            >>> graph_num = 10
-            >>> As, X_gt = pygm.utils.generate_isomorphic_graphs(node_num=4, graph_num=10)
-            >>> As_1, As_2 = [], []
-            >>> for i in range(graph_num):
-            ...     for j in range(graph_num):
-            ...         As_1.append(As[i])
-            ...         As_2.append(As[j])
-            >>> As_1 = np.stack(As_1, axis=0)
-            >>> As_2 = np.stack(As_2, axis=0)
-
-            # Build affinity matrix
-            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(As_1)
-            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(As_2)
-            >>> import functools
-            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
-            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, None, None, None, None, edge_aff_fn=gaussian_aff)
-            >>> K = K.reshape(graph_num, graph_num, 4*4, 4*4)
-            >>> K.shape
-            (10, 10, 16, 16)
-
-            # Solve the multi-matching problem
-            >>> X = pygm.cao(K)
-            >>> (X * X_gt).sum() / X_gt.sum()
-            1.0
-
-            # Use the IPFP solver for two-graph matching
-            >>> ipfp_func = functools.partial(pygmtools.ipfp, n1max=4, n2max=4)
-            >>> X = pygm.cao(K, qap_solver=ipfp_func)
-            >>> (X * X_gt).sum() / X_gt.sum()
-            1.0
-
-            # Run the faster version of CAO algorithm
-            >>> X = pygm.cao(K, mode='fast')
-            >>> (X * X_gt).sum() / X_gt.sum()
-            1.0
-
-
-    .. dropdown:: Pytorch Example
-
-        ::
-
-            >>> import torch
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'pytorch'
-            >>> _ = torch.manual_seed(1)
-
-            # Generate 10 isomorphic graphs
-            >>> graph_num = 10
-            >>> As, X_gt = pygm.utils.generate_isomorphic_graphs(node_num=4, graph_num=10)
-            >>> As_1, As_2 = [], []
-            >>> for i in range(graph_num):
-            ...     for j in range(graph_num):
-            ...         As_1.append(As[i])
-            ...         As_2.append(As[j])
-            >>> As_1 = torch.stack(As_1, dim=0)
-            >>> As_2 = torch.stack(As_2, dim=0)
-
-            # Build affinity matrix
-            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(As_1)
-            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(As_2)
-            >>> import functools
-            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
-            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, None, None, None, None, edge_aff_fn=gaussian_aff)
-            >>> K = K.reshape(graph_num, graph_num, 4*4, 4*4)
-            >>> K.shape
-            torch.Size([10, 10, 16, 16])
-
-            # Solve the multi-matching problem
-            >>> X = pygm.cao(K)
-            >>> (X * X_gt).sum() / X_gt.sum()
-            tensor(1.)
-
-            # Use the IPFP solver for two-graph matching
-            >>> ipfp_func = functools.partial(pygm.ipfp, n1max=4, n2max=4)
-            >>> X = pygm.cao(K, qap_solver=ipfp_func)
-            >>> (X * X_gt).sum() / X_gt.sum()
-            tensor(1.)
-
-            # Run the faster version of CAO algorithm
-            >>> X = pygm.cao(K, mode='fast')
-            >>> (X * X_gt).sum() / X_gt.sum()
-            tensor(1.)
-
-
-    .. dropdown:: Paddle Example
-
-        ::
-
-            >>> import paddle
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'paddle'
-            >>> _ = paddle.seed(1)
-
-            # Generate 10 isomorphic graphs
-            >>> graph_num = 10
-            >>> As, X_gt = pygm.utils.generate_isomorphic_graphs(node_num=4, graph_num=10)
-            >>> As_1, As_2 = [], []
-            >>> for i in range(graph_num):
-            ...     for j in range(graph_num):
-            ...         As_1.append(As[i])
-            ...         As_2.append(As[j])
-            >>> As_1 = paddle.stack(As_1, axis=0)
-            >>> As_2 = paddle.stack(As_2, axis=0)
-
-            # Build affinity matrix
-            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(As_1)
-            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(As_2)
-            >>> import functools
-            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
-            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, None, None, None, None, edge_aff_fn=gaussian_aff)
-            >>> K = K.reshape((graph_num, graph_num, 4*4, 4*4))
-            >>> K.shape
-            [10, 10, 16, 16]
-
-            # Solve the multi-matching problem
-            >>> X = pygm.cao(K)
-            >>> (X * X_gt).sum() / X_gt.sum()
-            Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True, [1.])
-
-            # Use the IPFP solver for two-graph matching
-            >>> ipfp_func = functools.partial(pygm.ipfp, n1max=4, n2max=4)
-            >>> X = pygm.cao(K, qap_solver=ipfp_func)
-            >>> (X * X_gt).sum() / X_gt.sum()
-            Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True, [1.])
-
-            # Run the faster version of CAO algorithm
-            >>> X = pygm.cao(K, mode='fast')
-            >>> (X * X_gt).sum() / X_gt.sum()
-            Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True, [1.])
-
-
-    .. dropdown:: Jittor Example
-
-        ::
-
-            >>> import jittor as jt
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'jittor'
-            >>> _ = jt.seed(1)
-
-            # Generate 10 isomorphic graphs
-            >>> graph_num = 10
-            >>> As, X_gt = pygm.utils.generate_isomorphic_graphs(node_num=4, graph_num=10)
-            >>> As_1, As_2 = [], []
-            >>> for i in range(graph_num):
-            ...     for j in range(graph_num):
-            ...         As_1.append(As[i])
-            ...         As_2.append(As[j])
-            >>> As_1 = jt.stack(As_1, dim=0)
-            >>> As_2 = jt.stack(As_2, dim=0)
-
-            # Build affinity matrix
-            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(As_1)
-            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(As_2)
-            >>> import functools
-            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
-            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, None, None, None, None, edge_aff_fn=gaussian_aff)
-            >>> K = K.reshape(graph_num, graph_num, 4*4, 4*4)
-            >>> K.shape
-            [10,10,16,16,]
-
-            # Solve the multi-matching problem
-            >>> X = pygm.cao(K, mode='memory')
-            >>> (X * X_gt).sum() / X_gt.sum()
-            jt.Var([1.], dtype=float32)
-
-            # Use the IPFP solver for two-graph matching
-            >>> ipfp_func = functools.partial(pygm.ipfp, n1max=4, n2max=4)
-            >>> X = pygm.cao(K, qap_solver=ipfp_func, mode='memory')
-            >>> (X * X_gt).sum() / X_gt.sum()
-            jt.Var([1.], dtype=float32)
-
-            # Run the faster version of CAO algorithm
-            >>> X = pygm.cao(K, mode='time')
-            >>> (X * X_gt).sum() / X_gt.sum()
-            jt.Var([1.], dtype=float32)
-
-    .. note::
-        If you find this graph matching solver useful in your research, please cite:
-
-        ::
-
-            @article{cao,
-              title={Multi-graph matching via affinity optimization with graduated consistency regularization},
-              author={Yan, Junchi and Cho, Minsu and Zha, Hongyuan and Yang, Xiaokang and Chu, Stephen M},
-              journal={IEEE transactions on pattern analysis and machine intelligence},
-              volume={38},
-              number={6},
-              pages={1228--1242},
-              year={2015},
-              publisher={IEEE}
-            }
-    """
-    if backend is None:
-        backend = pygmtools.BACKEND
-    # check the correctness of input
-    _check_data_type(K, 'K', backend)
-    K_shape = _get_shape(K, backend)
-    if not (len(K_shape) == 4 and K_shape[0] == K_shape[1] and K_shape[2] == K_shape[3]):
-        raise ValueError(f"Unsupported input data shape: got K {K_shape}")
-    num_graph, aff_size = K_shape[0], K_shape[2]
-    num_node = int(math.sqrt(aff_size))
-    if not num_node ** 2 == aff_size:
-        raise ValueError("The input affinity matrix is not supported. Please note that this function "
-                         "does not support matching with outliers or partial matching.")
-    if not 0 <= lambda_init <= 1: raise ValueError(f"lambda_init must be in [0, 1], got lambda_init={lambda_init}")
-    if not 0 <= lambda_max <= 1: raise ValueError(f"lambda_max must be in [0, 1], got lambda_max={lambda_max}")
-    if not lambda_step > 1: raise ValueError(f"lambda_step must be >1, got lambda_step={lambda_step}")
-    if x0 is not None:
-        _check_data_type(x0, 'x0', backend)
-        x0_shape = _get_shape(x0, backend)
-        if not len(x0_shape) == 4 and num_graph == x0_shape[0] == x0_shape[1] and num_node == x0_shape[2] == x0_shape[3]:
-            raise ValueError(f"Unsupported input data shape: got K {K_shape} x0 {x0_shape}")
-    else:
-        if qap_solver is None:
-            qap_solver = functools.partial(pygmtools.rrwm, n1max=num_node, n2max=num_node, backend=backend)
-        x0 = qap_solver(K.reshape((num_graph ** 2, aff_size, aff_size)))
-        x0 = pygmtools.hungarian(x0, backend=backend)
-        x0 = x0.reshape((num_graph, num_graph, num_node, num_node))
-
-    args = (K, x0, num_graph, num_node, max_iter, lambda_init, lambda_step, lambda_max, iter_boost)
-    try:
-        mod = importlib.import_module(f'pygmtools.{backend}_backend')
-        if mode in ['time']:
-            fn = mod.cao_fast_solver
-        elif mode in ['memory']:
-            fn = mod.cao_solver
-        else:
-            raise ValueError("Unknown value of mode: supported values ['time', 'memory']")
-    except (ModuleNotFoundError, AttributeError):
-        raise NotImplementedError(
-            NOT_IMPLEMENTED_MSG.format(backend)
-        )
-
-    return fn(*args)
-
-
-def mgm_floyd(K, x0=None, qap_solver=None,
-              mode='time',
-              param_lambda=0.2,
-              backend=None):
-    r"""
-    Multi-Graph Matching based on Floyd shortest path algorithm. A supergraph is considered by regarding each input
-    graph as a node, and the matching between graphs are regraded as edges in the supergraph. Floyd algorithm is used
-    to discover a shortest path on this supergraph for matching update.
-
-    The length of edges on the supergraph is described as follows:
-
-    .. math::
-
-        \arg \max_{k} (1-\lambda) J(\mathbf{X}_{ik} \mathbf{X}_{kj}) + \lambda C_p(\mathbf{X}_{ik} \mathbf{X}_{kj})
-
-    where :math:`J(\mathbf{X}_{ik} \mathbf{X}_{kj})` is the objective score, and
-    :math:`C_p(\mathbf{X}_{ik} \mathbf{X}_{kj})` measures a consistency score compared to other matchings. These two
-    terms are balanced by :math:`\lambda`.
-
-    :param K: :math:`(m\times m \times n^2 \times n^2)` the input affinity matrix, where ``K[i,j]`` is the affinity
-              matrix of graph ``i`` and graph ``j`` (:math:`m`: number of nodes)
-    :param x0: (optional) :math:`(m\times m \times n \times n)` the initial two-graph matching result, where ``X[i,j]``
-               is the matching matrix result of graph ``i`` and graph ``j``. If this argument is not given,
-               ``qap_solver`` will be used to compute the two-graph matching result.
-    :param qap_solver: (default: pygm.rrwm) a function object that accepts a batched affinity matrix and returns the
-                       matching matrices. It is suggested to use ``functools.partial`` and the QAP solvers provided in
-                       the :mod:`~pygmtools.classic_solvers` module (see examples below).
-    :param mode: (default: ``'time'``) the operation mode of this algorithm. Options: ``'time', 'memory'``,
-                 where ``'time'`` is a time-efficient version and ``'memory'`` is a memory-efficient version.
-    :param param_lambda: (default: 0.3) value of :math:`\lambda`, with :math:`\lambda\in[0,1]`
-    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
-    :return: :math:`(m\times m \times n \times n)` the multi-graph matching result
-
-    .. dropdown:: Numpy Example
-
-        ::
-
-            >>> import numpy as np
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'numpy'
-            >>> np.random.seed(1)
-
-            # Generate 10 isomorphic graphs
-            >>> graph_num = 10
-            >>> As, X_gt = pygm.utils.generate_isomorphic_graphs(node_num=4, graph_num=10)
-            >>> As_1, As_2 = [], []
-            >>> for i in range(graph_num):
-            ...     for j in range(graph_num):
-            ...         As_1.append(As[i])
-            ...         As_2.append(As[j])
-            >>> As_1 = np.stack(As_1, axis=0)
-            >>> As_2 = np.stack(As_2, axis=0)
-
-            # Build affinity matrix
-            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(As_1)
-            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(As_2)
-            >>> import functools
-            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
-            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, None, None, None, None, edge_aff_fn=gaussian_aff)
-            >>> K = K.reshape(graph_num, graph_num, 4*4, 4*4)
-            >>> K.shape
-            (10, 10, 16, 16)
-
-            # Solve the multi-matching problem
-            >>> X = pygm.mgm_floyd(K)
-            >>> (X * X_gt).sum() / X_gt.sum()
-            1.0
-
-            # Use the IPFP solver for two-graph matching
-            >>> ipfp_func = functools.partial(pygm.ipfp, n1max=4, n2max=4)
-            >>> X = pygm.mgm_floyd(K, qap_solver=ipfp_func)
-            >>> (X * X_gt).sum() / X_gt.sum()
-            1.0
-
-            # Run the faster version of CAO algorithm
-            >>> X = pygm.mgm_floyd(K, mode='fast')
-            >>> (X * X_gt).sum() / X_gt.sum()
-            1.0
-
-
-    .. dropdown:: Pytorch Example
-
-        ::
-
-            >>> import torch
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'pytorch'
-            >>> _ = torch.manual_seed(1)
-
-            # Generate 10 isomorphic graphs
-            >>> graph_num = 10
-            >>> As, X_gt = pygm.utils.generate_isomorphic_graphs(node_num=4, graph_num=10)
-            >>> As_1, As_2 = [], []
-            >>> for i in range(graph_num):
-            ...     for j in range(graph_num):
-            ...         As_1.append(As[i])
-            ...         As_2.append(As[j])
-            >>> As_1 = torch.stack(As_1, dim=0)
-            >>> As_2 = torch.stack(As_2, dim=0)
-
-            # Build affinity matrix
-            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(As_1)
-            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(As_2)
-            >>> import functools
-            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
-            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, None, None, None, None, edge_aff_fn=gaussian_aff)
-            >>> K = K.reshape(graph_num, graph_num, 4*4, 4*4)
-            >>> K.shape
-            torch.Size([10, 10, 16, 16])
-
-            # Solve the multi-matching problem
-            >>> X = pygm.mgm_floyd(K)
-            >>> (X * X_gt).sum() / X_gt.sum()
-            tensor(1.)
-
-            # Use the IPFP solver for two-graph matching
-            >>> ipfp_func = functools.partial(pygm.ipfp, n1max=4, n2max=4)
-            >>> X = pygm.mgm_floyd(K, qap_solver=ipfp_func)
-            >>> (X * X_gt).sum() / X_gt.sum()
-            tensor(1.)
-
-            # Run the faster version of CAO algorithm
-            >>> X = pygm.mgm_floyd(K, mode='fast')
-            >>> (X * X_gt).sum() / X_gt.sum()
-            tensor(1.)
-
-
-    .. dropdown:: Paddle Example
-
-        ::
-
-            >>> import paddle
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'paddle'
-            >>> _ = paddle.seed(1)
-
-            # Generate 10 isomorphic graphs
-            >>> graph_num = 10
-            >>> As, X_gt = pygm.utils.generate_isomorphic_graphs(node_num=4, graph_num=10)
-            >>> As_1, As_2 = [], []
-            >>> for i in range(graph_num):
-            ...     for j in range(graph_num):
-            ...         As_1.append(As[i])
-            ...         As_2.append(As[j])
-            >>> As_1 = paddle.stack(As_1, axis=0)
-            >>> As_2 = paddle.stack(As_2, axis=0)
-
-            # Build affinity matrix
-            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(As_1)
-            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(As_2)
-            >>> import functools
-            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
-            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, None, None, None, None, edge_aff_fn=gaussian_aff)
-            >>> K = K.reshape((graph_num, graph_num, 4*4, 4*4))
-            >>> K.shape
-            [10, 10, 16, 16]
-
-            # Solve the multi-matching problem
-            >>> X = pygm.mgm_floyd(K)
-            >>> (X * X_gt).sum() / X_gt.sum()
-            Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True, [1.])
-
-            # Use the IPFP solver for two-graph matching
-            >>> ipfp_func = functools.partial(pygm.ipfp, n1max=4, n2max=4)
-            >>> X = pygm.mgm_floyd(K, qap_solver=ipfp_func)
-            >>> (X * X_gt).sum() / X_gt.sum()
-            Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True, [1.])
-
-            # Run the faster version of CAO algorithm
-            >>> X = pygm.mgm_floyd(K, mode='fast')
-            >>> (X * X_gt).sum() / X_gt.sum()
-            Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True, [1.])
-
-
-    .. dropdown:: Jittor Example
-
-        ::
-            
-            >>> import jittor as jt
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'jittor'
-            >>> _ = jt.seed(1)
-
-            # Generate 10 isomorphic graphs
-            >>> graph_num = 10
-            >>> As, X_gt = pygm.utils.generate_isomorphic_graphs(node_num=4, graph_num=10)
-            >>> As_1, As_2 = [], []
-            >>> for i in range(graph_num):
-            ...    for j in range(graph_num):
-            ...        As_1.append(As[i])
-            ...        As_2.append(As[j])
-            >>> As_1 = jt.stack(As_1, dim=0)
-            >>> As_2 = jt.stack(As_2, dim=0)
-
-            # Build affinity matrix
-            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(As_1)
-            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(As_2)
-            >>> import functools
-            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
-            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, None, None, None, None, edge_aff_fn=gaussian_aff)
-            >>> K = K.reshape(graph_num, graph_num, 4*4, 4*4)
-            >>> K.shape
-            [10,10,16,16,]
-
-            # Solve the multi-matching problem
-            >>> X = pygm.mgm_floyd(K, mode='memory')
-            >>> (X * X_gt).sum() / X_gt.sum()
-            jt.Var([1.], dtype=float32)
-
-            # Use the IPFP solver for two-graph matching
-            >>> ipfp_func = functools.partial(pygm.ipfp, n1max=4, n2max=4)
-            >>> X = pygm.mgm_floyd(K, qap_solver=ipfp_func, mode='memory')
-            >>> (X * X_gt).sum() / X_gt.sum()
-            jt.Var([1.], dtype=float32)
-
-            # Run the faster version of CAO algorithm
-            >>> X = pygm.mgm_floyd(K, mode='time')
-            >>> (X * X_gt).sum() / X_gt.sum()
-            jt.Var([1.], dtype=float32)
-
-
-    .. note::
-
-        If you find this graph matching solver useful in your research, please cite:
-
-        ::
-
-            @article{mgm_floyd,
-              title={Unifying offline and online multi-graph matching via finding shortest paths on supergraph},
-              author={Jiang, Zetian and Wang, Tianzhe and Yan, Junchi},
-              journal={IEEE transactions on pattern analysis and machine intelligence},
-              volume={43},
-              number={10},
-              pages={3648--3663},
-              year={2020},
-              publisher={IEEE}
-            }
-    """
-    if backend is None:
-        backend = pygmtools.BACKEND
-    # check the correctness of input
-    _check_data_type(K, 'K', backend)
-    K_shape = _get_shape(K, backend)
-    if not (len(K_shape) == 4 and K_shape[0] == K_shape[1] and K_shape[2] == K_shape[3]):
-        raise ValueError(f"Unsupported input data shape: got K {K_shape}")
-    num_graph, aff_size = K_shape[0], K_shape[2]
-    num_node = int(math.sqrt(aff_size))
-    if not num_node ** 2 == aff_size:
-        raise ValueError("The input affinity matrix is not supported. Please note that this function "
-                         "does not support matching with outliers or partial matching.")
-    if not 0 <= param_lambda <= 1: raise ValueError(f"param_lambda must be in [0, 1], got param_lambda={param_lambda}")
-    if x0 is not None:
-        _check_data_type(x0, 'x0', backend)
-        x0_shape = _get_shape(x0, backend)
-        if not len(x0_shape) == 4 and num_graph == x0_shape[0] == x0_shape[1] and num_node == x0_shape[2] == x0_shape[3]:
-            raise ValueError(f"Unsupported input data shape: got K {K_shape} x0 {x0_shape}")
-    else:
-        if qap_solver is None:
-            qap_solver = functools.partial(pygmtools.rrwm, n1max=num_node, n2max=num_node, backend=backend)
-        x0 = qap_solver(K.reshape((num_graph ** 2, aff_size, aff_size)))
-        x0 = pygmtools.hungarian(x0, backend=backend)
-        x0 = x0.reshape((num_graph, num_graph, num_node, num_node))
-
-    args = (K, x0, num_graph, num_node, param_lambda)
-    try:
-        mod = importlib.import_module(f'pygmtools.{backend}_backend')
-        if mode in ['time']:
-            fn = mod.mgm_floyd_fast_solver
-        elif mode in ['memory']:
-            fn = mod.mgm_floyd_solver
-        else:
-            raise ValueError("Unknown value of mode: supported values ['time', 'memory']")
-    except (ModuleNotFoundError, AttributeError):
-        raise NotImplementedError(
-            NOT_IMPLEMENTED_MSG.format(backend)
-        )
-
-    return fn(*args)
-
-
-def gamgm(A, W,
-          ns=None, n_univ=None, U0=None,
-          sk_init_tau=0.5, sk_min_tau=0.1, sk_gamma=0.8, sk_iter=20, max_iter=100, param_lambda=1.,
-          converge_thresh=1e-5, outlier_thresh=-1, bb_smooth=0.1,
-          verbose=False,
-          backend=None):
-    r"""
-    Graduated Assignment-based multi-graph matching solver. Graduated assignment is a classic approach for hard
-    assignment problems like graph matching, based on graduated annealing of Sinkhorn's temperature :math:`\tau` to
-    enforce the matching constraint.
-
-    The objective score is described as
-
-    .. math::
-
-        \max_{\mathbf{X}_{i,j}, i,j\in [m]} \ \sum_{i,j\in [m]} \left( \lambda \ \mathrm{tr}(\mathbf{X}_{ij}^\top \mathbf{A}_{i} \mathbf{X}_{ij} \mathbf{A}_{j}) + \mathrm{tr}(\mathbf{X}_{ij}^\top \mathbf{W}_{ij})\right)
-
-    Once the algorithm converges at a fixed :math:`\tau` value, :math:`\tau` shrinks as:
-
-    .. math::
-
-        \tau = \tau \times \gamma
-
-    and the iteration continues. At last, Hungarian algorithm is applied to ensure the result is a permutation matrix.
-
-    .. note::
-
-        This algorithm is based on the Koopmans-Beckmann's QAP formulation and you should input the adjacency matrices
-        ``A`` and node-wise similarity matrices ``W`` instead of the affinity matrices.
-
-    :param A: :math:`(m\times n \times n)` the adjacency matrix (:math:`m`: number of nodes).
-              The graphs may have different number of nodes (specified by the ``ns`` argument).
-    :param W: :math:`(m\times m \times n \times n)` the node-wise similarity matrix, where ``W[i,j]`` is the similarity
-              matrix
-    :param ns: (optional) :math:`(m)` the number of nodes. If not given, it will be inferred based on the size of ``A``.
-    :param n_univ: (optional) the size of the universe node set. If not given, it will be the largest number of nodes.
-    :param U0: (optional) the initial multi-graph matching result. If not given, it will be randomly initialized.
-    :param sk_init_tau: (default: 0.05) initial value of :math:`\tau` for Sinkhorn algorithm
-    :param sk_min_tau: (default: 1.0e-3) minimal value of :math:`\tau` for Sinkhorn algorithm
-    :param sk_gamma: (default: 0.8) the shrinking parameter of :math:`\tau`: :math:`\tau = \tau \times \gamma`
-    :param sk_iter: (default: 200) max number of iterations for Sinkhorn algorithm
-    :param max_iter: (default: 1000) max number of iterations for graduated assignment
-    :param param_lambda: (default: 1) the weight :math:`\lambda` of the quadratic term
-    :param converge_thresh: (default: 1e-5) if the Frobenius norm of the change of U is smaller than this, the iteration
-                            is stopped.
-    :param outlier_thresh: (default: -1) if > 0, pairs with node+edge similarity score smaller than this threshold will
-                           be discarded. This threshold is designed to handle outliers.
-    :param bb_smooth: (default: 0.1) the black-box differentiation smoothing parameter.
-    :param verbose: (default: False) print verbose information for parameter tuning
-    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
-    :return: the multi-graph matching result (a :mod:`~pygmtools.utils.MultiMatchingResult` object)
-
-    .. note::
-
-        In PyTorch and Jittor backends, this function is differentiable through the black-box trick.
-        See the following paper for details:
-
-        ::
-
-            Vlastelica M, Paulus A., Differentiation of Blackbox Combinatorial Solvers, ICLR 2020
-
-        If you want to disable this differentiable feature, please detach the input tensors from the computational
-        graph.
-
-    .. note::
-
-        Setting ``verbose=True`` may help you tune the parameters.
-
-    .. dropdown:: Numpy Example
-
-        ::
-
-            >>> import numpy as np
-            >>> import pygmtools as pygm
-            >>> import itertools
-            >>> import time
-            >>> pygm.BACKEND = 'numpy'
-            >>> np.random.seed(1)
-
-            # Generate 10 isomorphic graphs
-            >>> graph_num = 10
-            >>> As, X_gt, Fs = pygm.utils.generate_isomorphic_graphs(node_num=4, graph_num=10, node_feat_dim=20)
-
-            # Compute node-wise similarity by inner-product and Sinkhorn
-            >>> W = np.matmul(np.expand_dims(Fs,axis=1), np.expand_dims(Fs.swapaxes(1, 2),axis=0))
-            >>> W = pygm.sinkhorn(W.reshape(graph_num ** 2, 4, 4)).reshape(graph_num, graph_num, 4, 4)
-
-            # Solve the multi-matching problem
-            >>> X = pygm.gamgm(As, W)
-            >>> matched = 0
-            for i, j in itertools.product(range(graph_num), repeat=2):
-            ...    matched += (X[i,j] * X_gt[i,j]).sum()
-            >>> acc = matched / X_gt.sum()
-            >>> acc
-            1.0
-
-            # This function supports graphs with different nodes (also known as partial matching)
-            # In the following we ignore the last node from the last 5 graphs
-            >>> ns = np.array([4, 4, 4, 4, 4, 3, 3, 3, 3, 3], dtype='i4')
-            >>> for i in range(graph_num):
-            ...    As[i, ns[i]:, :] = 0
-            ...    As[i, :, ns[i]:] = 0
-            >>> for i, j in itertools.product(range(graph_num), repeat=2):
-            ...    X_gt[i, j, ns[i]:, :] = 0
-            ...    X_gt[i, j, :, ns[j]:] = 0
-            ...    W[i, j, ns[i]:, :] = 0
-            ...    W[i, j, :, ns[j]:] = 0
-
-            # Partial matching is challenging and the following parameters are carefully tuned
-            >>> X = pygm.gamgm(As, W, ns, n_univ=4, sk_init_tau=.1, sk_min_tau=0.01, param_lambda=0.3)
-
-            # Check the partial matching result
-            >>> matched = 0
-            >>> for i, j in itertools.product(range(graph_num), repeat=2):
-            ...    matched += (X[i,j] * X_gt[i, j, :ns[i], :ns[j]]).sum()
-            >>> matched / X_gt.sum()
-            1.0
-
-
-    .. dropdown:: Pytorch Example
-
-        ::
-
-            >>> import torch
-            >>> import pygmtools as pygm
-            >>> import itertools
-            >>> import time
-            >>> pygm.BACKEND = 'pytorch'
-            >>> _ = torch.manual_seed(1)
-
-            # Generate 10 isomorphic graphs
-            >>> graph_num = 10
-            >>> As, X_gt, Fs = pygm.utils.generate_isomorphic_graphs(node_num=4, graph_num=10, node_feat_dim=20)
-
-            # Compute node-wise similarity by inner-product and Sinkhorn
-            >>> W = torch.matmul(Fs.unsqueeze(1), Fs.transpose(1, 2).unsqueeze(0))
-            >>> W = pygm.sinkhorn(W.reshape(graph_num ** 2, 4, 4)).reshape(graph_num, graph_num, 4, 4)
-
-            # Solve the multi-matching problem
-            >>> X = pygm.gamgm(As, W)
-            >>> matched = 0
-            >>> for i, j in itertools.product(range(graph_num), repeat=2):
-            ...     matched += (X[i,j] * X_gt[i,j]).sum()
-            >>> acc = matched / X_gt.sum()
-            >>> acc
-            tensor(1.)
-
-            # This function is differentiable by the black-box trick
-            >>> W.requires_grad_(True)  # tell PyTorch to track the gradients
-            >>> X = pygm.gamgm(As, W)
-            >>> matched = 0
-            >>> for i, j in itertools.product(range(graph_num), repeat=2):
-            ...     matched += (X[i,j] * X_gt[i,j]).sum()
-            >>> acc = matched / X_gt.sum()
-
-            # Backward pass via black-box trick
-            >>> acc.backward()
-            >>> torch.sum(W.grad != 0)
-            tensor(128)
-
-            # This function supports graphs with different nodes (also known as partial matching)
-            # In the following we ignore the last node from the last 5 graphs
-            >>> ns = torch.tensor([4, 4, 4, 4, 4, 3, 3, 3, 3, 3], dtype=torch.int)
-            >>> for i in range(graph_num):
-            ...     As[i, ns[i]:, :] = 0
-            ...     As[i, :, ns[i]:] = 0
-            >>> for i, j in itertools.product(range(graph_num), repeat=2):
-            ...     X_gt[i, j, ns[i]:, :] = 0
-            ...     X_gt[i, j, :, ns[j]:] = 0
-            ...     W[i, j, ns[i]:, :] = 0
-            ...     W[i, j, :, ns[j]:] = 0
-            >>> W = W.detach() # detach tensor if gradient is not needed
-
-            # Partial matching is challenging and the following parameters are carefully tuned
-            >>> X = pygm.gamgm(As, W, ns, n_univ=4, sk_init_tau=.1, sk_min_tau=0.01, param_lambda=0.3)
-
-            # Check the partial matching result
-            >>> matched = 0
-            >>> for i, j in itertools.product(range(graph_num), repeat=2):
-            ...     matched += (X[i,j] * X_gt[i, j, :ns[i], :ns[j]]).sum()
-            >>> matched / X_gt.sum()
-            tensor(1.)
-
-
-    .. dropdown:: Paddle Example
-
-        ::
-
-            >>> import paddle
-            >>> import pygmtools as pygm
-            >>> import itertools
-            >>> import time
-            >>> pygm.BACKEND = 'paddle'
-            >>> _ = paddle.seed(1)
-
-            # Generate 10 isomorphic graphs
-            >>> graph_num = 10
-            >>> As, X_gt, Fs = pygm.utils.generate_isomorphic_graphs(node_num=4, graph_num=10, node_feat_dim=20)
-
-            # Compute node-wise similarity by inner-product and Sinkhorn
-            >>> W = paddle.matmul(Fs.unsqueeze(1), Fs.transpose((0, 2, 1)).unsqueeze(0))
-            >>> W = pygm.sinkhorn(W.reshape((graph_num ** 2, 4, 4))).reshape((graph_num, graph_num, 4, 4))
-
-            # Solve the multi-matching problem
-            >>> X = pygm.gamgm(As, W)
-            >>> matched = 0
-            >>> for i, j in itertools.product(range(graph_num), repeat=2):
-            ...     matched += (X[i,j] * X_gt[i,j]).sum()
-            >>> acc = matched / X_gt.sum()
-            >>> acc
-            Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True, [1.])
-
-            # This function supports graphs with different nodes (also known as partial matching)
-            # In the following we ignore the last node from the last 5 graphs
-            >>> ns = paddle.to_tensor([4, 4, 4, 4, 4, 3, 3, 3, 3, 3], dtype=paddle.int32)
-            >>> for i in range(graph_num):
-            ...     As[i, ns[i]:, :] = 0
-            ...     As[i, :, ns[i]:] = 0
-            >>> for i, j in itertools.product(range(graph_num), repeat=2):
-            ...     X_gt[i, j, ns[i]:, :] = 0
-            ...     X_gt[i, j, :, ns[j]:] = 0
-            ...     W[i, j, ns[i]:, :] = 0
-            ...     W[i, j, :, ns[j]:] = 0
-            >>> W = W.detach() # detach tensor if gradient is not needed
-
-            # Partial matching is challenging and the following parameters are carefully tuned
-            >>> X = pygm.gamgm(As, W, ns, n_univ=4, sk_init_tau=.1, sk_min_tau=0.01, param_lambda=0.3)
-
-            # Check the partial matching result
-            >>> matched = 0
-            >>> for i, j in itertools.product(range(graph_num), repeat=2):
-            ...     matched += (X[i,j] * X_gt[i, j, :ns[i], :ns[j]]).sum()
-            >>> matched / X_gt.sum()
-            Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True, [0.88424438])
-
-
-    .. dropdown:: Jittor Example
-
-        ::
-
-            >>> import jittor as jt
-            >>> import pygmtools as pygm
-            >>> import itertools
-            >>> import time
-            >>> pygm.BACKEND = 'jittor'
-
-            # Generate 10 isomorphic graphs
-            >>> graph_num = 10
-            >>> As, X_gt, Fs = pygm.utils.generate_isomorphic_graphs(node_num=4, graph_num=10, node_feat_dim=20)
-
-            # Compute node-wise similarity by inner-product and Sinkhorn
-            >>> W = jt.matmul(Fs.unsqueeze(1), Fs.transpose(1, 2).unsqueeze(0))
-            >>> W = pygm.sinkhorn(W.reshape(graph_num ** 2, 4, 4)).reshape(graph_num, graph_num, 4, 4)
-
-            # Solve the multi-matching problem
-            >>> X = pygm.gamgm(As, W)
-            >>> matched = 0
-            >>> for i, j in itertools.product(range(graph_num), repeat=2):
-            ...    matched += (X[i,j] * X_gt[i,j]).sum()
-            >>> acc = matched / X_gt.sum()
-            >>> acc
-            jt.Var([1.], dtype=float32)
-
-            # This function supports graphs with different nodes (also known as partial matching)
-            # In the following we ignore the last node from the last 3 graphs
-            >>> ns = [4, 4, 4, 4, 4, 3, 3, 3, 3, 3]
-            >>> for i in range(graph_num):
-            ...    As[i, ns[i]:, :] = 0
-            ...    As[i, :, ns[i]:] = 0
-            >>> for i, j in itertools.product(range(graph_num), repeat=2):
-            ...    X_gt[i, j, ns[i]:, :] = 0
-            ...    X_gt[i, j, :, ns[j]:] = 0
-            ...    W[i, j, ns[i]:, :] = 0
-            ...    W[i, j, :, ns[j]:] = 0
-            >>> ns = jt.int32(ns)
-            >>> W = W.detach() # detach tensor if gradient is not needed
-
-            # Partial matching is challenging and the following parameters are carefully tuned
-            >>> X = pygm.gamgm(As, W, ns, n_univ=4, sk_init_tau=.1, sk_min_tau=0.01, param_lambda=0.3)
-
-            # Check the partial matching result
-            >>> matched = 0
-            >>> for i, j in itertools.product(range(graph_num), repeat=2):
-            ...    matched += (X[i,j] * X_gt[i, j, :ns[i].item(), :ns[j].item()]).sum()
-            >>> matched / X_gt.sum()
-            jt.Var([1.], dtype=float32)
-
-    .. note::
-
-        If you find this graph matching solver useful in your research, please cite:
-
-        ::
-
-            @article{gamgm1,
-              title={Graduated assignment algorithm for multiple graph matching based on a common labeling},
-              author={Sol{\'e}-Ribalta, Albert and Serratosa, Francesc},
-              journal={International Journal of Pattern Recognition and Artificial Intelligence},
-              volume={27},
-              number={01},
-              pages={1350001},
-              year={2013},
-              publisher={World Scientific}
-            }
-
-            @article{gamgm2,
-              title={Graduated assignment for joint multi-graph matching and clustering with application to unsupervised graph matching network learning},
-              author={Wang, Runzhong and Yan, Junchi and Yang, Xiaokang},
-              journal={Advances in Neural Information Processing Systems},
-              volume={33},
-              pages={19908--19919},
-              year={2020}
-            }
-
-        This algorithm is originally proposed by paper ``gamgm1``, and further improved by paper ``gamgm2`` to fit
-        modern computing architectures like GPU.
-    """
-    if backend is None:
-        backend = pygmtools.BACKEND
-    # check the correctness of input
-    _check_data_type(A, 'A', backend)
-    A_shape = _get_shape(A, backend)
-    if not (len(A_shape) == 3 and A_shape[1] == A_shape[2]):
-        raise ValueError(f"Unsupported input data shape: got A {A_shape}")
-    num_graph, max_node = A_shape[0], A_shape[1]
-    _check_data_type(W, 'W', backend)
-    W_shape = _get_shape(W, backend)
-    if not (len(W_shape) == 4 and W_shape[0] == W_shape[1] == num_graph and W_shape[2] == W_shape[3] == max_node):
-        raise ValueError(f"Unsupported input data shape: got A {A_shape}, W {W_shape}")
-    if ns is not None:
-        _check_data_type(ns, 'ns', backend)
-        ns_shape = _get_shape(ns, backend)
-        if not (len(ns_shape) == 1 and ns_shape[0] == num_graph):
-            raise ValueError(f"The size of ns mismatches the sizes of A and W: got ns {ns_shape}, A {A_shape}, W {W_shape}")
-    if n_univ is None:
-        n_univ = max_node
-    if U0 is not None:
-        _check_data_type(U0, 'U0', backend)
-    if not sk_init_tau > 0: raise ValueError(f"sk_init_tau must be >0, got sk_init_tau={sk_init_tau}")
-    if not sk_min_tau > 0: raise ValueError(f"sk_min_tau must be >0, got sk_min_tau={sk_min_tau}")
-    if not 0 < sk_gamma < 1: raise ValueError(f"sk_gamma must be in (0, 1), got sk_gamma={sk_gamma}")
-    if not 0 < bb_smooth < 1: raise ValueError(f"bb_smooth must be in (0, 1), got bb_smooth={bb_smooth}")
-
-    args = (A, W, ns, n_univ, U0, sk_init_tau, sk_min_tau, sk_gamma, sk_iter, max_iter, param_lambda,
-            converge_thresh, outlier_thresh, bb_smooth, verbose)
-    try:
-        mod = importlib.import_module(f'pygmtools.{backend}_backend')
-        fn = mod.gamgm
-    except (ModuleNotFoundError, AttributeError):
-        raise NotImplementedError(
-            NOT_IMPLEMENTED_MSG.format(backend)
-        )
-
-    return fn(*args)
+r"""
+Classic (learning-free) **multi-graph matching** solvers. These multi-graph matching solvers are recommended to solve
+the joint matching problem of multiple graphs.
+"""
+
+# Copyright (c) 2022 Thinklab@SJTU
+# pygmtools is licensed under Mulan PSL v2.
+# You can use this software according to the terms and conditions of the Mulan PSL v2.
+# You may obtain a copy of Mulan PSL v2 at:
+# http://license.coscl.org.cn/MulanPSL2
+# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND,
+# EITHER EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT,
+# MERCHANTABILITY OR FIT FOR A PARTICULAR PURPOSE.
+# See the Mulan PSL v2 for more details.
+
+import functools
+import importlib
+import pygmtools
+from pygmtools.utils import NOT_IMPLEMENTED_MSG, _check_shape, _get_shape, _unsqueeze, _squeeze, _check_data_type
+import math
+
+
+def cao(K, x0=None, qap_solver=None,
+        mode='time',
+        max_iter=6, lambda_init=0.3, lambda_step=1.1, lambda_max=1.0, iter_boost=2,
+        backend=None):
+    r"""
+    Composition based Affinity Optimization (CAO) solver for multi-graph matching. This solver builds a supergraph for
+    matching update to incorporate the two aspects by optimizing the affinity score, meanwhile gradually
+    infusing the consistency.
+
+    Each update step is described as follows:
+
+    .. math::
+
+        \arg \max_{k} (1-\lambda) J(\mathbf{X}_{ik} \mathbf{X}_{kj}) + \lambda C_p(\mathbf{X}_{ik} \mathbf{X}_{kj})
+
+    where :math:`J(\mathbf{X}_{ik} \mathbf{X}_{kj})` is the objective score, and
+    :math:`C_p(\mathbf{X}_{ik} \mathbf{X}_{kj})` measures a consistency score compared to other matchings. These two
+    terms are balanced by :math:`\lambda`, and :math:`\lambda` starts from a smaller number and gradually grows.
+
+    :param K: :math:`(m\times m \times n^2 \times n^2)` the input affinity matrix, where ``K[i,j]`` is the affinity
+              matrix of graph ``i`` and graph ``j`` (:math:`m`: number of nodes)
+    :param x0: (optional) :math:`(m\times m \times n \times n)` the initial two-graph matching result, where ``X[i,j]``
+               is the matching matrix result of graph ``i`` and graph ``j``. If this argument is not given,
+               ``qap_solver`` will be used to compute the two-graph matching result.
+    :param qap_solver: (default: pygm.rrwm) a function object that accepts a batched affinity matrix and returns the
+                       matching matrices. It is suggested to use ``functools.partial`` and the QAP solvers provided in
+                       the :mod:`~pygmtools.classic_solvers` module (see examples below).
+    :param mode: (default: ``'time'``) the operation mode of this algorithm. Options: ``'time', 'memory'``,
+                 where ``'time'`` is a time-efficient version and ``'memory'`` is a memory-efficient version.
+    :param max_iter: (default: 6) max number of iterations
+    :param lambda_init: (default: 0.3) initial value of :math:`\lambda`, with :math:`\lambda\in[0,1]`
+    :param lambda_step: (default: 1.1) the increase step size of :math:`\lambda`, updated by ``lambda = step * lambda``
+    :param lambda_max: (default: 1.0) the max value of lambda
+    :param iter_boost: (default: 2) to boost the convergence of the CAO algorithm, :math:`\lambda` will be forced to
+                       update every ``iter_boost`` iterations.
+    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
+    :return: :math:`(m\times m \times n \times n)` the multi-graph matching result
+
+    .. note::
+
+        The input graphs must have the same number of nodes for this algorithm to work correctly.
+
+    .. note::
+
+       Multi-graph matching methods process all graphs at once and do not support the additional batch dimension. Please
+       note that this behavior is different from two-graph matching solvers in :mod:`~pygmtools.classic_solvers`.
+    
+    .. dropdown:: Numpy Example
+
+        ::
+
+            >>> import numpy as np
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'numpy'
+            >>> np.random.seed(1)
+
+            # Generate 10 isomorphic graphs
+            >>> graph_num = 10
+            >>> As, X_gt = pygm.utils.generate_isomorphic_graphs(node_num=4, graph_num=10)
+            >>> As_1, As_2 = [], []
+            >>> for i in range(graph_num):
+            ...     for j in range(graph_num):
+            ...         As_1.append(As[i])
+            ...         As_2.append(As[j])
+            >>> As_1 = np.stack(As_1, axis=0)
+            >>> As_2 = np.stack(As_2, axis=0)
+
+            # Build affinity matrix
+            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(As_1)
+            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(As_2)
+            >>> import functools
+            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
+            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, None, None, None, None, edge_aff_fn=gaussian_aff)
+            >>> K = K.reshape(graph_num, graph_num, 4*4, 4*4)
+            >>> K.shape
+            (10, 10, 16, 16)
+
+            # Solve the multi-matching problem
+            >>> X = pygm.cao(K)
+            >>> (X * X_gt).sum() / X_gt.sum()
+            1.0
+
+            # Use the IPFP solver for two-graph matching
+            >>> ipfp_func = functools.partial(pygmtools.ipfp, n1max=4, n2max=4)
+            >>> X = pygm.cao(K, qap_solver=ipfp_func)
+            >>> (X * X_gt).sum() / X_gt.sum()
+            1.0
+
+            # Run the faster version of CAO algorithm
+            >>> X = pygm.cao(K, mode='fast')
+            >>> (X * X_gt).sum() / X_gt.sum()
+            1.0
+
+
+    .. dropdown:: Pytorch Example
+
+        ::
+
+            >>> import torch
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'pytorch'
+            >>> _ = torch.manual_seed(1)
+
+            # Generate 10 isomorphic graphs
+            >>> graph_num = 10
+            >>> As, X_gt = pygm.utils.generate_isomorphic_graphs(node_num=4, graph_num=10)
+            >>> As_1, As_2 = [], []
+            >>> for i in range(graph_num):
+            ...     for j in range(graph_num):
+            ...         As_1.append(As[i])
+            ...         As_2.append(As[j])
+            >>> As_1 = torch.stack(As_1, dim=0)
+            >>> As_2 = torch.stack(As_2, dim=0)
+
+            # Build affinity matrix
+            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(As_1)
+            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(As_2)
+            >>> import functools
+            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
+            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, None, None, None, None, edge_aff_fn=gaussian_aff)
+            >>> K = K.reshape(graph_num, graph_num, 4*4, 4*4)
+            >>> K.shape
+            torch.Size([10, 10, 16, 16])
+
+            # Solve the multi-matching problem
+            >>> X = pygm.cao(K)
+            >>> (X * X_gt).sum() / X_gt.sum()
+            tensor(1.)
+
+            # Use the IPFP solver for two-graph matching
+            >>> ipfp_func = functools.partial(pygm.ipfp, n1max=4, n2max=4)
+            >>> X = pygm.cao(K, qap_solver=ipfp_func)
+            >>> (X * X_gt).sum() / X_gt.sum()
+            tensor(1.)
+
+            # Run the faster version of CAO algorithm
+            >>> X = pygm.cao(K, mode='fast')
+            >>> (X * X_gt).sum() / X_gt.sum()
+            tensor(1.)
+
+
+    .. dropdown:: Paddle Example
+
+        ::
+
+            >>> import paddle
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'paddle'
+            >>> _ = paddle.seed(1)
+
+            # Generate 10 isomorphic graphs
+            >>> graph_num = 10
+            >>> As, X_gt = pygm.utils.generate_isomorphic_graphs(node_num=4, graph_num=10)
+            >>> As_1, As_2 = [], []
+            >>> for i in range(graph_num):
+            ...     for j in range(graph_num):
+            ...         As_1.append(As[i])
+            ...         As_2.append(As[j])
+            >>> As_1 = paddle.stack(As_1, axis=0)
+            >>> As_2 = paddle.stack(As_2, axis=0)
+
+            # Build affinity matrix
+            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(As_1)
+            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(As_2)
+            >>> import functools
+            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
+            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, None, None, None, None, edge_aff_fn=gaussian_aff)
+            >>> K = K.reshape((graph_num, graph_num, 4*4, 4*4))
+            >>> K.shape
+            [10, 10, 16, 16]
+
+            # Solve the multi-matching problem
+            >>> X = pygm.cao(K)
+            >>> (X * X_gt).sum() / X_gt.sum()
+            Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True, [1.])
+
+            # Use the IPFP solver for two-graph matching
+            >>> ipfp_func = functools.partial(pygm.ipfp, n1max=4, n2max=4)
+            >>> X = pygm.cao(K, qap_solver=ipfp_func)
+            >>> (X * X_gt).sum() / X_gt.sum()
+            Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True, [1.])
+
+            # Run the faster version of CAO algorithm
+            >>> X = pygm.cao(K, mode='fast')
+            >>> (X * X_gt).sum() / X_gt.sum()
+            Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True, [1.])
+
+
+    .. dropdown:: Jittor Example
+
+        ::
+
+            >>> import jittor as jt
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'jittor'
+            >>> _ = jt.seed(1)
+
+            # Generate 10 isomorphic graphs
+            >>> graph_num = 10
+            >>> As, X_gt = pygm.utils.generate_isomorphic_graphs(node_num=4, graph_num=10)
+            >>> As_1, As_2 = [], []
+            >>> for i in range(graph_num):
+            ...     for j in range(graph_num):
+            ...         As_1.append(As[i])
+            ...         As_2.append(As[j])
+            >>> As_1 = jt.stack(As_1, dim=0)
+            >>> As_2 = jt.stack(As_2, dim=0)
+
+            # Build affinity matrix
+            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(As_1)
+            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(As_2)
+            >>> import functools
+            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
+            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, None, None, None, None, edge_aff_fn=gaussian_aff)
+            >>> K = K.reshape(graph_num, graph_num, 4*4, 4*4)
+            >>> K.shape
+            [10,10,16,16,]
+
+            # Solve the multi-matching problem
+            >>> X = pygm.cao(K, mode='memory')
+            >>> (X * X_gt).sum() / X_gt.sum()
+            jt.Var([1.], dtype=float32)
+
+            # Use the IPFP solver for two-graph matching
+            >>> ipfp_func = functools.partial(pygm.ipfp, n1max=4, n2max=4)
+            >>> X = pygm.cao(K, qap_solver=ipfp_func, mode='memory')
+            >>> (X * X_gt).sum() / X_gt.sum()
+            jt.Var([1.], dtype=float32)
+
+            # Run the faster version of CAO algorithm
+            >>> X = pygm.cao(K, mode='time')
+            >>> (X * X_gt).sum() / X_gt.sum()
+            jt.Var([1.], dtype=float32)
+
+    .. note::
+        If you find this graph matching solver useful in your research, please cite:
+
+        ::
+
+            @article{cao,
+              title={Multi-graph matching via affinity optimization with graduated consistency regularization},
+              author={Yan, Junchi and Cho, Minsu and Zha, Hongyuan and Yang, Xiaokang and Chu, Stephen M},
+              journal={IEEE transactions on pattern analysis and machine intelligence},
+              volume={38},
+              number={6},
+              pages={1228--1242},
+              year={2015},
+              publisher={IEEE}
+            }
+    """
+    if backend is None:
+        backend = pygmtools.BACKEND
+    # check the correctness of input
+    _check_data_type(K, 'K', backend)
+    K_shape = _get_shape(K, backend)
+    if not (len(K_shape) == 4 and K_shape[0] == K_shape[1] and K_shape[2] == K_shape[3]):
+        raise ValueError(f"Unsupported input data shape: got K {K_shape}")
+    num_graph, aff_size = K_shape[0], K_shape[2]
+    num_node = int(math.sqrt(aff_size))
+    if not num_node ** 2 == aff_size:
+        raise ValueError("The input affinity matrix is not supported. Please note that this function "
+                         "does not support matching with outliers or partial matching.")
+    if not 0 <= lambda_init <= 1: raise ValueError(f"lambda_init must be in [0, 1], got lambda_init={lambda_init}")
+    if not 0 <= lambda_max <= 1: raise ValueError(f"lambda_max must be in [0, 1], got lambda_max={lambda_max}")
+    if not lambda_step > 1: raise ValueError(f"lambda_step must be >1, got lambda_step={lambda_step}")
+    if x0 is not None:
+        _check_data_type(x0, 'x0', backend)
+        x0_shape = _get_shape(x0, backend)
+        if not len(x0_shape) == 4 and num_graph == x0_shape[0] == x0_shape[1] and num_node == x0_shape[2] == x0_shape[3]:
+            raise ValueError(f"Unsupported input data shape: got K {K_shape} x0 {x0_shape}")
+    else:
+        if qap_solver is None:
+            qap_solver = functools.partial(pygmtools.rrwm, n1max=num_node, n2max=num_node, backend=backend)
+        x0 = qap_solver(K.reshape((num_graph ** 2, aff_size, aff_size)))
+        x0 = pygmtools.hungarian(x0, backend=backend)
+        x0 = x0.reshape((num_graph, num_graph, num_node, num_node))
+
+    args = (K, x0, num_graph, num_node, max_iter, lambda_init, lambda_step, lambda_max, iter_boost)
+    try:
+        mod = importlib.import_module(f'pygmtools.{backend}_backend')
+        if mode in ['time']:
+            fn = mod.cao_fast_solver
+        elif mode in ['memory']:
+            fn = mod.cao_solver
+        else:
+            raise ValueError("Unknown value of mode: supported values ['time', 'memory']")
+    except (ModuleNotFoundError, AttributeError):
+        raise NotImplementedError(
+            NOT_IMPLEMENTED_MSG.format(backend)
+        )
+
+    return fn(*args)
+
+
+def mgm_floyd(K, x0=None, qap_solver=None,
+              mode='time',
+              param_lambda=0.2,
+              backend=None):
+    r"""
+    Multi-Graph Matching based on Floyd shortest path algorithm. A supergraph is considered by regarding each input
+    graph as a node, and the matching between graphs are regraded as edges in the supergraph. Floyd algorithm is used
+    to discover a shortest path on this supergraph for matching update.
+
+    The length of edges on the supergraph is described as follows:
+
+    .. math::
+
+        \arg \max_{k} (1-\lambda) J(\mathbf{X}_{ik} \mathbf{X}_{kj}) + \lambda C_p(\mathbf{X}_{ik} \mathbf{X}_{kj})
+
+    where :math:`J(\mathbf{X}_{ik} \mathbf{X}_{kj})` is the objective score, and
+    :math:`C_p(\mathbf{X}_{ik} \mathbf{X}_{kj})` measures a consistency score compared to other matchings. These two
+    terms are balanced by :math:`\lambda`.
+
+    :param K: :math:`(m\times m \times n^2 \times n^2)` the input affinity matrix, where ``K[i,j]`` is the affinity
+              matrix of graph ``i`` and graph ``j`` (:math:`m`: number of nodes)
+    :param x0: (optional) :math:`(m\times m \times n \times n)` the initial two-graph matching result, where ``X[i,j]``
+               is the matching matrix result of graph ``i`` and graph ``j``. If this argument is not given,
+               ``qap_solver`` will be used to compute the two-graph matching result.
+    :param qap_solver: (default: pygm.rrwm) a function object that accepts a batched affinity matrix and returns the
+                       matching matrices. It is suggested to use ``functools.partial`` and the QAP solvers provided in
+                       the :mod:`~pygmtools.classic_solvers` module (see examples below).
+    :param mode: (default: ``'time'``) the operation mode of this algorithm. Options: ``'time', 'memory'``,
+                 where ``'time'`` is a time-efficient version and ``'memory'`` is a memory-efficient version.
+    :param param_lambda: (default: 0.3) value of :math:`\lambda`, with :math:`\lambda\in[0,1]`
+    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
+    :return: :math:`(m\times m \times n \times n)` the multi-graph matching result
+
+    .. dropdown:: Numpy Example
+
+        ::
+
+            >>> import numpy as np
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'numpy'
+            >>> np.random.seed(1)
+
+            # Generate 10 isomorphic graphs
+            >>> graph_num = 10
+            >>> As, X_gt = pygm.utils.generate_isomorphic_graphs(node_num=4, graph_num=10)
+            >>> As_1, As_2 = [], []
+            >>> for i in range(graph_num):
+            ...     for j in range(graph_num):
+            ...         As_1.append(As[i])
+            ...         As_2.append(As[j])
+            >>> As_1 = np.stack(As_1, axis=0)
+            >>> As_2 = np.stack(As_2, axis=0)
+
+            # Build affinity matrix
+            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(As_1)
+            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(As_2)
+            >>> import functools
+            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
+            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, None, None, None, None, edge_aff_fn=gaussian_aff)
+            >>> K = K.reshape(graph_num, graph_num, 4*4, 4*4)
+            >>> K.shape
+            (10, 10, 16, 16)
+
+            # Solve the multi-matching problem
+            >>> X = pygm.mgm_floyd(K)
+            >>> (X * X_gt).sum() / X_gt.sum()
+            1.0
+
+            # Use the IPFP solver for two-graph matching
+            >>> ipfp_func = functools.partial(pygm.ipfp, n1max=4, n2max=4)
+            >>> X = pygm.mgm_floyd(K, qap_solver=ipfp_func)
+            >>> (X * X_gt).sum() / X_gt.sum()
+            1.0
+
+            # Run the faster version of CAO algorithm
+            >>> X = pygm.mgm_floyd(K, mode='fast')
+            >>> (X * X_gt).sum() / X_gt.sum()
+            1.0
+
+
+    .. dropdown:: Pytorch Example
+
+        ::
+
+            >>> import torch
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'pytorch'
+            >>> _ = torch.manual_seed(1)
+
+            # Generate 10 isomorphic graphs
+            >>> graph_num = 10
+            >>> As, X_gt = pygm.utils.generate_isomorphic_graphs(node_num=4, graph_num=10)
+            >>> As_1, As_2 = [], []
+            >>> for i in range(graph_num):
+            ...     for j in range(graph_num):
+            ...         As_1.append(As[i])
+            ...         As_2.append(As[j])
+            >>> As_1 = torch.stack(As_1, dim=0)
+            >>> As_2 = torch.stack(As_2, dim=0)
+
+            # Build affinity matrix
+            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(As_1)
+            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(As_2)
+            >>> import functools
+            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
+            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, None, None, None, None, edge_aff_fn=gaussian_aff)
+            >>> K = K.reshape(graph_num, graph_num, 4*4, 4*4)
+            >>> K.shape
+            torch.Size([10, 10, 16, 16])
+
+            # Solve the multi-matching problem
+            >>> X = pygm.mgm_floyd(K)
+            >>> (X * X_gt).sum() / X_gt.sum()
+            tensor(1.)
+
+            # Use the IPFP solver for two-graph matching
+            >>> ipfp_func = functools.partial(pygm.ipfp, n1max=4, n2max=4)
+            >>> X = pygm.mgm_floyd(K, qap_solver=ipfp_func)
+            >>> (X * X_gt).sum() / X_gt.sum()
+            tensor(1.)
+
+            # Run the faster version of CAO algorithm
+            >>> X = pygm.mgm_floyd(K, mode='fast')
+            >>> (X * X_gt).sum() / X_gt.sum()
+            tensor(1.)
+
+
+    .. dropdown:: Paddle Example
+
+        ::
+
+            >>> import paddle
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'paddle'
+            >>> _ = paddle.seed(1)
+
+            # Generate 10 isomorphic graphs
+            >>> graph_num = 10
+            >>> As, X_gt = pygm.utils.generate_isomorphic_graphs(node_num=4, graph_num=10)
+            >>> As_1, As_2 = [], []
+            >>> for i in range(graph_num):
+            ...     for j in range(graph_num):
+            ...         As_1.append(As[i])
+            ...         As_2.append(As[j])
+            >>> As_1 = paddle.stack(As_1, axis=0)
+            >>> As_2 = paddle.stack(As_2, axis=0)
+
+            # Build affinity matrix
+            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(As_1)
+            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(As_2)
+            >>> import functools
+            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
+            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, None, None, None, None, edge_aff_fn=gaussian_aff)
+            >>> K = K.reshape((graph_num, graph_num, 4*4, 4*4))
+            >>> K.shape
+            [10, 10, 16, 16]
+
+            # Solve the multi-matching problem
+            >>> X = pygm.mgm_floyd(K)
+            >>> (X * X_gt).sum() / X_gt.sum()
+            Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True, [1.])
+
+            # Use the IPFP solver for two-graph matching
+            >>> ipfp_func = functools.partial(pygm.ipfp, n1max=4, n2max=4)
+            >>> X = pygm.mgm_floyd(K, qap_solver=ipfp_func)
+            >>> (X * X_gt).sum() / X_gt.sum()
+            Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True, [1.])
+
+            # Run the faster version of CAO algorithm
+            >>> X = pygm.mgm_floyd(K, mode='fast')
+            >>> (X * X_gt).sum() / X_gt.sum()
+            Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True, [1.])
+
+
+    .. dropdown:: Jittor Example
+
+        ::
+            
+            >>> import jittor as jt
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'jittor'
+            >>> _ = jt.seed(1)
+
+            # Generate 10 isomorphic graphs
+            >>> graph_num = 10
+            >>> As, X_gt = pygm.utils.generate_isomorphic_graphs(node_num=4, graph_num=10)
+            >>> As_1, As_2 = [], []
+            >>> for i in range(graph_num):
+            ...    for j in range(graph_num):
+            ...        As_1.append(As[i])
+            ...        As_2.append(As[j])
+            >>> As_1 = jt.stack(As_1, dim=0)
+            >>> As_2 = jt.stack(As_2, dim=0)
+
+            # Build affinity matrix
+            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(As_1)
+            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(As_2)
+            >>> import functools
+            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
+            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, None, None, None, None, edge_aff_fn=gaussian_aff)
+            >>> K = K.reshape(graph_num, graph_num, 4*4, 4*4)
+            >>> K.shape
+            [10,10,16,16,]
+
+            # Solve the multi-matching problem
+            >>> X = pygm.mgm_floyd(K, mode='memory')
+            >>> (X * X_gt).sum() / X_gt.sum()
+            jt.Var([1.], dtype=float32)
+
+            # Use the IPFP solver for two-graph matching
+            >>> ipfp_func = functools.partial(pygm.ipfp, n1max=4, n2max=4)
+            >>> X = pygm.mgm_floyd(K, qap_solver=ipfp_func, mode='memory')
+            >>> (X * X_gt).sum() / X_gt.sum()
+            jt.Var([1.], dtype=float32)
+
+            # Run the faster version of CAO algorithm
+            >>> X = pygm.mgm_floyd(K, mode='time')
+            >>> (X * X_gt).sum() / X_gt.sum()
+            jt.Var([1.], dtype=float32)
+
+
+    .. note::
+
+        If you find this graph matching solver useful in your research, please cite:
+
+        ::
+
+            @article{mgm_floyd,
+              title={Unifying offline and online multi-graph matching via finding shortest paths on supergraph},
+              author={Jiang, Zetian and Wang, Tianzhe and Yan, Junchi},
+              journal={IEEE transactions on pattern analysis and machine intelligence},
+              volume={43},
+              number={10},
+              pages={3648--3663},
+              year={2020},
+              publisher={IEEE}
+            }
+    """
+    if backend is None:
+        backend = pygmtools.BACKEND
+    # check the correctness of input
+    _check_data_type(K, 'K', backend)
+    K_shape = _get_shape(K, backend)
+    if not (len(K_shape) == 4 and K_shape[0] == K_shape[1] and K_shape[2] == K_shape[3]):
+        raise ValueError(f"Unsupported input data shape: got K {K_shape}")
+    num_graph, aff_size = K_shape[0], K_shape[2]
+    num_node = int(math.sqrt(aff_size))
+    if not num_node ** 2 == aff_size:
+        raise ValueError("The input affinity matrix is not supported. Please note that this function "
+                         "does not support matching with outliers or partial matching.")
+    if not 0 <= param_lambda <= 1: raise ValueError(f"param_lambda must be in [0, 1], got param_lambda={param_lambda}")
+    if x0 is not None:
+        _check_data_type(x0, 'x0', backend)
+        x0_shape = _get_shape(x0, backend)
+        if not len(x0_shape) == 4 and num_graph == x0_shape[0] == x0_shape[1] and num_node == x0_shape[2] == x0_shape[3]:
+            raise ValueError(f"Unsupported input data shape: got K {K_shape} x0 {x0_shape}")
+    else:
+        if qap_solver is None:
+            qap_solver = functools.partial(pygmtools.rrwm, n1max=num_node, n2max=num_node, backend=backend)
+        x0 = qap_solver(K.reshape((num_graph ** 2, aff_size, aff_size)))
+        x0 = pygmtools.hungarian(x0, backend=backend)
+        x0 = x0.reshape((num_graph, num_graph, num_node, num_node))
+
+    args = (K, x0, num_graph, num_node, param_lambda)
+    try:
+        mod = importlib.import_module(f'pygmtools.{backend}_backend')
+        if mode in ['time']:
+            fn = mod.mgm_floyd_fast_solver
+        elif mode in ['memory']:
+            fn = mod.mgm_floyd_solver
+        else:
+            raise ValueError("Unknown value of mode: supported values ['time', 'memory']")
+    except (ModuleNotFoundError, AttributeError):
+        raise NotImplementedError(
+            NOT_IMPLEMENTED_MSG.format(backend)
+        )
+
+    return fn(*args)
+
+
+def gamgm(A, W,
+          ns=None, n_univ=None, U0=None,
+          sk_init_tau=0.5, sk_min_tau=0.1, sk_gamma=0.8, sk_iter=20, max_iter=100, param_lambda=1.,
+          converge_thresh=1e-5, outlier_thresh=-1, bb_smooth=0.1,
+          verbose=False,
+          backend=None):
+    r"""
+    Graduated Assignment-based multi-graph matching solver. Graduated assignment is a classic approach for hard
+    assignment problems like graph matching, based on graduated annealing of Sinkhorn's temperature :math:`\tau` to
+    enforce the matching constraint.
+
+    The objective score is described as
+
+    .. math::
+
+        \max_{\mathbf{X}_{i,j}, i,j\in [m]} \ \sum_{i,j\in [m]} \left( \lambda \ \mathrm{tr}(\mathbf{X}_{ij}^\top \mathbf{A}_{i} \mathbf{X}_{ij} \mathbf{A}_{j}) + \mathrm{tr}(\mathbf{X}_{ij}^\top \mathbf{W}_{ij})\right)
+
+    Once the algorithm converges at a fixed :math:`\tau` value, :math:`\tau` shrinks as:
+
+    .. math::
+
+        \tau = \tau \times \gamma
+
+    and the iteration continues. At last, Hungarian algorithm is applied to ensure the result is a permutation matrix.
+
+    .. note::
+
+        This algorithm is based on the Koopmans-Beckmann's QAP formulation and you should input the adjacency matrices
+        ``A`` and node-wise similarity matrices ``W`` instead of the affinity matrices.
+
+    :param A: :math:`(m\times n \times n)` the adjacency matrix (:math:`m`: number of nodes).
+              The graphs may have different number of nodes (specified by the ``ns`` argument).
+    :param W: :math:`(m\times m \times n \times n)` the node-wise similarity matrix, where ``W[i,j]`` is the similarity
+              matrix
+    :param ns: (optional) :math:`(m)` the number of nodes. If not given, it will be inferred based on the size of ``A``.
+    :param n_univ: (optional) the size of the universe node set. If not given, it will be the largest number of nodes.
+    :param U0: (optional) the initial multi-graph matching result. If not given, it will be randomly initialized.
+    :param sk_init_tau: (default: 0.05) initial value of :math:`\tau` for Sinkhorn algorithm
+    :param sk_min_tau: (default: 1.0e-3) minimal value of :math:`\tau` for Sinkhorn algorithm
+    :param sk_gamma: (default: 0.8) the shrinking parameter of :math:`\tau`: :math:`\tau = \tau \times \gamma`
+    :param sk_iter: (default: 200) max number of iterations for Sinkhorn algorithm
+    :param max_iter: (default: 1000) max number of iterations for graduated assignment
+    :param param_lambda: (default: 1) the weight :math:`\lambda` of the quadratic term
+    :param converge_thresh: (default: 1e-5) if the Frobenius norm of the change of U is smaller than this, the iteration
+                            is stopped.
+    :param outlier_thresh: (default: -1) if > 0, pairs with node+edge similarity score smaller than this threshold will
+                           be discarded. This threshold is designed to handle outliers.
+    :param bb_smooth: (default: 0.1) the black-box differentiation smoothing parameter.
+    :param verbose: (default: False) print verbose information for parameter tuning
+    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
+    :return: the multi-graph matching result (a :mod:`~pygmtools.utils.MultiMatchingResult` object)
+
+    .. note::
+
+        In PyTorch and Jittor backends, this function is differentiable through the black-box trick.
+        See the following paper for details:
+
+        ::
+
+            Vlastelica M, Paulus A., Differentiation of Blackbox Combinatorial Solvers, ICLR 2020
+
+        If you want to disable this differentiable feature, please detach the input tensors from the computational
+        graph.
+
+    .. note::
+
+        Setting ``verbose=True`` may help you tune the parameters.
+
+    .. dropdown:: Numpy Example
+
+        ::
+
+            >>> import numpy as np
+            >>> import pygmtools as pygm
+            >>> import itertools
+            >>> import time
+            >>> pygm.BACKEND = 'numpy'
+            >>> np.random.seed(1)
+
+            # Generate 10 isomorphic graphs
+            >>> graph_num = 10
+            >>> As, X_gt, Fs = pygm.utils.generate_isomorphic_graphs(node_num=4, graph_num=10, node_feat_dim=20)
+
+            # Compute node-wise similarity by inner-product and Sinkhorn
+            >>> W = np.matmul(np.expand_dims(Fs,axis=1), np.expand_dims(Fs.swapaxes(1, 2),axis=0))
+            >>> W = pygm.sinkhorn(W.reshape(graph_num ** 2, 4, 4)).reshape(graph_num, graph_num, 4, 4)
+
+            # Solve the multi-matching problem
+            >>> X = pygm.gamgm(As, W)
+            >>> matched = 0
+            for i, j in itertools.product(range(graph_num), repeat=2):
+            ...    matched += (X[i,j] * X_gt[i,j]).sum()
+            >>> acc = matched / X_gt.sum()
+            >>> acc
+            1.0
+
+            # This function supports graphs with different nodes (also known as partial matching)
+            # In the following we ignore the last node from the last 5 graphs
+            >>> ns = np.array([4, 4, 4, 4, 4, 3, 3, 3, 3, 3], dtype='i4')
+            >>> for i in range(graph_num):
+            ...    As[i, ns[i]:, :] = 0
+            ...    As[i, :, ns[i]:] = 0
+            >>> for i, j in itertools.product(range(graph_num), repeat=2):
+            ...    X_gt[i, j, ns[i]:, :] = 0
+            ...    X_gt[i, j, :, ns[j]:] = 0
+            ...    W[i, j, ns[i]:, :] = 0
+            ...    W[i, j, :, ns[j]:] = 0
+
+            # Partial matching is challenging and the following parameters are carefully tuned
+            >>> X = pygm.gamgm(As, W, ns, n_univ=4, sk_init_tau=.1, sk_min_tau=0.01, param_lambda=0.3)
+
+            # Check the partial matching result
+            >>> matched = 0
+            >>> for i, j in itertools.product(range(graph_num), repeat=2):
+            ...    matched += (X[i,j] * X_gt[i, j, :ns[i], :ns[j]]).sum()
+            >>> matched / X_gt.sum()
+            1.0
+
+
+    .. dropdown:: Pytorch Example
+
+        ::
+
+            >>> import torch
+            >>> import pygmtools as pygm
+            >>> import itertools
+            >>> import time
+            >>> pygm.BACKEND = 'pytorch'
+            >>> _ = torch.manual_seed(1)
+
+            # Generate 10 isomorphic graphs
+            >>> graph_num = 10
+            >>> As, X_gt, Fs = pygm.utils.generate_isomorphic_graphs(node_num=4, graph_num=10, node_feat_dim=20)
+
+            # Compute node-wise similarity by inner-product and Sinkhorn
+            >>> W = torch.matmul(Fs.unsqueeze(1), Fs.transpose(1, 2).unsqueeze(0))
+            >>> W = pygm.sinkhorn(W.reshape(graph_num ** 2, 4, 4)).reshape(graph_num, graph_num, 4, 4)
+
+            # Solve the multi-matching problem
+            >>> X = pygm.gamgm(As, W)
+            >>> matched = 0
+            >>> for i, j in itertools.product(range(graph_num), repeat=2):
+            ...     matched += (X[i,j] * X_gt[i,j]).sum()
+            >>> acc = matched / X_gt.sum()
+            >>> acc
+            tensor(1.)
+
+            # This function is differentiable by the black-box trick
+            >>> W.requires_grad_(True)  # tell PyTorch to track the gradients
+            >>> X = pygm.gamgm(As, W)
+            >>> matched = 0
+            >>> for i, j in itertools.product(range(graph_num), repeat=2):
+            ...     matched += (X[i,j] * X_gt[i,j]).sum()
+            >>> acc = matched / X_gt.sum()
+
+            # Backward pass via black-box trick
+            >>> acc.backward()
+            >>> torch.sum(W.grad != 0)
+            tensor(128)
+
+            # This function supports graphs with different nodes (also known as partial matching)
+            # In the following we ignore the last node from the last 5 graphs
+            >>> ns = torch.tensor([4, 4, 4, 4, 4, 3, 3, 3, 3, 3], dtype=torch.int)
+            >>> for i in range(graph_num):
+            ...     As[i, ns[i]:, :] = 0
+            ...     As[i, :, ns[i]:] = 0
+            >>> for i, j in itertools.product(range(graph_num), repeat=2):
+            ...     X_gt[i, j, ns[i]:, :] = 0
+            ...     X_gt[i, j, :, ns[j]:] = 0
+            ...     W[i, j, ns[i]:, :] = 0
+            ...     W[i, j, :, ns[j]:] = 0
+            >>> W = W.detach() # detach tensor if gradient is not needed
+
+            # Partial matching is challenging and the following parameters are carefully tuned
+            >>> X = pygm.gamgm(As, W, ns, n_univ=4, sk_init_tau=.1, sk_min_tau=0.01, param_lambda=0.3)
+
+            # Check the partial matching result
+            >>> matched = 0
+            >>> for i, j in itertools.product(range(graph_num), repeat=2):
+            ...     matched += (X[i,j] * X_gt[i, j, :ns[i], :ns[j]]).sum()
+            >>> matched / X_gt.sum()
+            tensor(1.)
+
+
+    .. dropdown:: Paddle Example
+
+        ::
+
+            >>> import paddle
+            >>> import pygmtools as pygm
+            >>> import itertools
+            >>> import time
+            >>> pygm.BACKEND = 'paddle'
+            >>> _ = paddle.seed(1)
+
+            # Generate 10 isomorphic graphs
+            >>> graph_num = 10
+            >>> As, X_gt, Fs = pygm.utils.generate_isomorphic_graphs(node_num=4, graph_num=10, node_feat_dim=20)
+
+            # Compute node-wise similarity by inner-product and Sinkhorn
+            >>> W = paddle.matmul(Fs.unsqueeze(1), Fs.transpose((0, 2, 1)).unsqueeze(0))
+            >>> W = pygm.sinkhorn(W.reshape((graph_num ** 2, 4, 4))).reshape((graph_num, graph_num, 4, 4))
+
+            # Solve the multi-matching problem
+            >>> X = pygm.gamgm(As, W)
+            >>> matched = 0
+            >>> for i, j in itertools.product(range(graph_num), repeat=2):
+            ...     matched += (X[i,j] * X_gt[i,j]).sum()
+            >>> acc = matched / X_gt.sum()
+            >>> acc
+            Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True, [1.])
+
+            # This function supports graphs with different nodes (also known as partial matching)
+            # In the following we ignore the last node from the last 5 graphs
+            >>> ns = paddle.to_tensor([4, 4, 4, 4, 4, 3, 3, 3, 3, 3], dtype=paddle.int32)
+            >>> for i in range(graph_num):
+            ...     As[i, ns[i]:, :] = 0
+            ...     As[i, :, ns[i]:] = 0
+            >>> for i, j in itertools.product(range(graph_num), repeat=2):
+            ...     X_gt[i, j, ns[i]:, :] = 0
+            ...     X_gt[i, j, :, ns[j]:] = 0
+            ...     W[i, j, ns[i]:, :] = 0
+            ...     W[i, j, :, ns[j]:] = 0
+            >>> W = W.detach() # detach tensor if gradient is not needed
+
+            # Partial matching is challenging and the following parameters are carefully tuned
+            >>> X = pygm.gamgm(As, W, ns, n_univ=4, sk_init_tau=.1, sk_min_tau=0.01, param_lambda=0.3)
+
+            # Check the partial matching result
+            >>> matched = 0
+            >>> for i, j in itertools.product(range(graph_num), repeat=2):
+            ...     matched += (X[i,j] * X_gt[i, j, :ns[i], :ns[j]]).sum()
+            >>> matched / X_gt.sum()
+            Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True, [0.88424438])
+
+
+    .. dropdown:: Jittor Example
+
+        ::
+
+            >>> import jittor as jt
+            >>> import pygmtools as pygm
+            >>> import itertools
+            >>> import time
+            >>> pygm.BACKEND = 'jittor'
+
+            # Generate 10 isomorphic graphs
+            >>> graph_num = 10
+            >>> As, X_gt, Fs = pygm.utils.generate_isomorphic_graphs(node_num=4, graph_num=10, node_feat_dim=20)
+
+            # Compute node-wise similarity by inner-product and Sinkhorn
+            >>> W = jt.matmul(Fs.unsqueeze(1), Fs.transpose(1, 2).unsqueeze(0))
+            >>> W = pygm.sinkhorn(W.reshape(graph_num ** 2, 4, 4)).reshape(graph_num, graph_num, 4, 4)
+
+            # Solve the multi-matching problem
+            >>> X = pygm.gamgm(As, W)
+            >>> matched = 0
+            >>> for i, j in itertools.product(range(graph_num), repeat=2):
+            ...    matched += (X[i,j] * X_gt[i,j]).sum()
+            >>> acc = matched / X_gt.sum()
+            >>> acc
+            jt.Var([1.], dtype=float32)
+
+            # This function supports graphs with different nodes (also known as partial matching)
+            # In the following we ignore the last node from the last 3 graphs
+            >>> ns = [4, 4, 4, 4, 4, 3, 3, 3, 3, 3]
+            >>> for i in range(graph_num):
+            ...    As[i, ns[i]:, :] = 0
+            ...    As[i, :, ns[i]:] = 0
+            >>> for i, j in itertools.product(range(graph_num), repeat=2):
+            ...    X_gt[i, j, ns[i]:, :] = 0
+            ...    X_gt[i, j, :, ns[j]:] = 0
+            ...    W[i, j, ns[i]:, :] = 0
+            ...    W[i, j, :, ns[j]:] = 0
+            >>> ns = jt.int32(ns)
+            >>> W = W.detach() # detach tensor if gradient is not needed
+
+            # Partial matching is challenging and the following parameters are carefully tuned
+            >>> X = pygm.gamgm(As, W, ns, n_univ=4, sk_init_tau=.1, sk_min_tau=0.01, param_lambda=0.3)
+
+            # Check the partial matching result
+            >>> matched = 0
+            >>> for i, j in itertools.product(range(graph_num), repeat=2):
+            ...    matched += (X[i,j] * X_gt[i, j, :ns[i].item(), :ns[j].item()]).sum()
+            >>> matched / X_gt.sum()
+            jt.Var([1.], dtype=float32)
+
+    .. note::
+
+        If you find this graph matching solver useful in your research, please cite:
+
+        ::
+
+            @article{gamgm1,
+              title={Graduated assignment algorithm for multiple graph matching based on a common labeling},
+              author={Sol{\'e}-Ribalta, Albert and Serratosa, Francesc},
+              journal={International Journal of Pattern Recognition and Artificial Intelligence},
+              volume={27},
+              number={01},
+              pages={1350001},
+              year={2013},
+              publisher={World Scientific}
+            }
+
+            @article{gamgm2,
+              title={Graduated assignment for joint multi-graph matching and clustering with application to unsupervised graph matching network learning},
+              author={Wang, Runzhong and Yan, Junchi and Yang, Xiaokang},
+              journal={Advances in Neural Information Processing Systems},
+              volume={33},
+              pages={19908--19919},
+              year={2020}
+            }
+
+        This algorithm is originally proposed by paper ``gamgm1``, and further improved by paper ``gamgm2`` to fit
+        modern computing architectures like GPU.
+    """
+    if backend is None:
+        backend = pygmtools.BACKEND
+    # check the correctness of input
+    _check_data_type(A, 'A', backend)
+    A_shape = _get_shape(A, backend)
+    if not (len(A_shape) == 3 and A_shape[1] == A_shape[2]):
+        raise ValueError(f"Unsupported input data shape: got A {A_shape}")
+    num_graph, max_node = A_shape[0], A_shape[1]
+    _check_data_type(W, 'W', backend)
+    W_shape = _get_shape(W, backend)
+    if not (len(W_shape) == 4 and W_shape[0] == W_shape[1] == num_graph and W_shape[2] == W_shape[3] == max_node):
+        raise ValueError(f"Unsupported input data shape: got A {A_shape}, W {W_shape}")
+    if ns is not None:
+        _check_data_type(ns, 'ns', backend)
+        ns_shape = _get_shape(ns, backend)
+        if not (len(ns_shape) == 1 and ns_shape[0] == num_graph):
+            raise ValueError(f"The size of ns mismatches the sizes of A and W: got ns {ns_shape}, A {A_shape}, W {W_shape}")
+    if n_univ is None:
+        n_univ = max_node
+    if U0 is not None:
+        _check_data_type(U0, 'U0', backend)
+    if not sk_init_tau > 0: raise ValueError(f"sk_init_tau must be >0, got sk_init_tau={sk_init_tau}")
+    if not sk_min_tau > 0: raise ValueError(f"sk_min_tau must be >0, got sk_min_tau={sk_min_tau}")
+    if not 0 < sk_gamma < 1: raise ValueError(f"sk_gamma must be in (0, 1), got sk_gamma={sk_gamma}")
+    if not 0 < bb_smooth < 1: raise ValueError(f"bb_smooth must be in (0, 1), got bb_smooth={bb_smooth}")
+
+    args = (A, W, ns, n_univ, U0, sk_init_tau, sk_min_tau, sk_gamma, sk_iter, max_iter, param_lambda,
+            converge_thresh, outlier_thresh, bb_smooth, verbose)
+    try:
+        mod = importlib.import_module(f'pygmtools.{backend}_backend')
+        fn = mod.gamgm
+    except (ModuleNotFoundError, AttributeError):
+        raise NotImplementedError(
+            NOT_IMPLEMENTED_MSG.format(backend)
+        )
+
+    return fn(*args)
```

### Comparing `pygmtools-0.3.8/pygmtools/neural_solvers.py` & `pygmtools-0.3.8a0/pygmtools/neural_solvers.py`

 * *Ordering differences only*

 * *Files 9% similar despite different names*

```diff
@@ -1,1273 +1,1273 @@
-"""
-**Neural network-based** graph matching solvers. It is recommended to integrate these networks as modules into your
-existing deep learning pipeline (either supervised, unsupervised or reinforcement learning).
-"""
-
-# Copyright (c) 2022 Thinklab@SJTU
-# pygmtools is licensed under Mulan PSL v2.
-# You can use this software according to the terms and conditions of the Mulan PSL v2.
-# You may obtain a copy of Mulan PSL v2 at:
-# http://license.coscl.org.cn/MulanPSL2
-# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND,
-# EITHER EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT,
-# MERCHANTABILITY OR FIT FOR A PARTICULAR PURPOSE.
-# See the Mulan PSL v2 for more details.
-
-import importlib
-import pygmtools
-import numpy as np
-from pygmtools.utils import NOT_IMPLEMENTED_MSG, from_numpy, \
-    _check_shape, _get_shape, _unsqueeze, _squeeze, _check_data_type
-from pygmtools.classic_solvers import __check_gm_arguments
-
-
-def pca_gm(feat1, feat2, A1, A2, n1=None, n2=None,
-           in_channel=1024, hidden_channel=2048, out_channel=2048, num_layers=2, sk_max_iter=20, sk_tau=0.05,
-           network=None, return_network=False, pretrain='voc',
-           backend=None):
-    r"""
-    The **PCA-GM** (Permutation loss and Cross-graph Affinity Graph Matching) neural network model for processing two
-    individual graphs (KB-QAP).
-    The graph matching module is composed of several intra-graph embedding layers, a cross-graph embedding layer, and
-    a Sinkhorn matching layer. Only the second last layer has a cross-graph update layer.
-
-    See the following pipeline for an example, with application to visual graph matching (layers in the gray box
-    + Affinity Metric + Sinkhorn are implemented by pygmtools):
-
-    .. image:: ../../images/pca_gm.png
-
-    See the following paper for more technical details:
-    `"Wang et al. Combinatorial Learning of Robust Deep Graph Matching: an Embedding based Approach. TPAMI 2020."
-    <https://ieeexplore.ieee.org/abstract/document/9128045/>`_
-
-    You may be also interested in the extended version IPCA-GM (see :func:`~pygmtools.neural_solvers.ipca_gm`).
-
-    :param feat1: :math:`(b\times n_1 \times d)` input feature of graph1
-    :param feat2: :math:`(b\times n_2 \times d)` input feature of graph2
-    :param A1: :math:`(b\times n_1 \times n_1)` input adjacency matrix of graph1
-    :param A2: :math:`(b\times n_2 \times n_2)` input adjacency matrix of graph2
-    :param n1: :math:`(b)` number of nodes in graph1. Optional if all equal to :math:`n_1`
-    :param n2: :math:`(b)` number of nodes in graph2. Optional if all equal to :math:`n_2`
-    :param in_channel: (default: 1024) Channel size of the input layer. It must match the feature dimension :math:`(d)`
-        of ``feat1, feat2``. Ignored if the network object is given (ignored if ``network!=None``)
-    :param hidden_channel: (default: 2048) Channel size of hidden layers. Ignored if the network object is given
-        (ignored if ``network!=None``)
-    :param out_channel: (default: 2048) Channel size of the output layer. Ignored if the network object is given
-        (ignored if ``network!=None``)
-    :param num_layers: (default: 2) Number of graph embedding layers. Must be >=2. Ignored if the network object is
-        given (ignored if ``network!=None``)
-    :param sk_max_iter: (default: 20) Max number of iterations of Sinkhorn. See
-        :func:`~pygmtools.classic_solvers.sinkhorn` for more details about this argument.
-    :param sk_tau: (default: 0.05) The temperature parameter of Sinkhorn. See
-        :func:`~pygmtools.classic_solvers.sinkhorn` for more details about this argument.
-    :param network: (default: None) The network object. If None, a new network object will be created, and load the
-        model weights specified in ``pretrain`` argument.
-    :param return_network: (default: False) Return the network object (saving model construction time if calling the
-        model multiple times).
-    :param pretrain: (default: 'voc') If ``network==None``, the pretrained model weights to be loaded. Available
-        pretrained weights: ``voc`` (on Pascal VOC Keypoint dataset), ``willow`` (on Willow Object Class dataset),
-        ``voc-all`` (on Pascal VOC Keypoint dataset, without filtering), or ``False`` (no pretraining).
-    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
-    :return: if ``return_network==False``, :math:`(b\times n_1 \times n_2)` the doubly-stochastic matching matrix
-
-        if ``return_network==True``, :math:`(b\times n_1 \times n_2)` the doubly-stochastic matching matrix,
-        the network object
-
-    .. note::
-        You may need a proxy to load the pretrained weights if Google drive is not accessible in your contry/region.
-        You may also download the pretrained models manually and put them at ``~/.cache/pygmtools`` (for Linux).
-
-        `[google drive] <https://drive.google.com/drive/folders/1O7vkIW8QXBJsNsHUIRiSw91HJ_0FAzu_?usp=sharing>`_
-        `[baidu drive] <https://pan.baidu.com/s/1MvzfM52NJeLWx2JXbbc6HA?pwd=x8bv>`_
-
-    .. note::
-        This function also supports non-batched input, by ignoring all batch dimensions in the input tensors.
-
-    .. dropdown:: Numpy Example
-
-        ::
-
-            >>> import numpy as np
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'numpy'
-            >>> np.random.seed(1)
-
-            # Generate a batch of isomorphic graphs
-            >>> batch_size = 10
-            >>> X_gt = np.zeros((batch_size, 4, 4))
-            >>> X_gt[:, np.arange(0, 4, dtype='i4'), np.random.permutation(4)] = 1
-            >>> A1 = 1. * (np.random.rand(batch_size, 4, 4) > 0.5)
-            >>> for i in np.arange(4): # discard self-loop edges
-            ...    for j in np.arange(batch_size):
-            ...        A1[j][i][i] = 0
-            >>> A2 = np.matmul(np.matmul(X_gt.swapaxes(1, 2), A1), X_gt)
-            >>> feat1 = np.random.rand(batch_size, 4, 1024) - 0.5
-            >>> feat2 = np.matmul(X_gt.swapaxes(1, 2), feat1)
-            >>> n1 = n2 = np.array([4] * batch_size)
-
-            # Match by PCA-GM (load pretrained model)
-            >>> X, net = pygm.pca_gm(feat1, feat2, A1, A2, n1, n2, return_network=True)
-            Downloading to ~/.cache/pygmtools/pca_gm_voc_numpy.npy...
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
-            1.0
-
-            # Pass the net object to avoid rebuilding the model agian
-            >>> X = pygm.pca_gm(feat1, feat2, A1, A2, n1, n2, network=net)
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
-            1.0
-
-            # You may also load other pretrained weights
-            >>> X, net = pygm.pca_gm(feat1, feat2, A1, A2, n1, n2, return_network=True, pretrain='willow')
-            Downloading to ~/.cache/pygmtools/pca_gm_willow_numpy.npy...
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
-            1.0
-
-            # You may configure your own model and integrate the model into a deep learning pipeline. For example:
-            >>> net = pygm.utils.get_network(pygm.pca_gm, in_channel=1024, hidden_channel=2048, out_channel=512, num_layers=3, pretrain=False)
-            # feat1/feat2 may be outputs by other neural networks
-            >>> X = pygm.pca_gm(feat1, feat2, A1, A2, n1, n2, network=net)
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
-            1.0
-
-
-    .. dropdown:: PyTorch Example
-
-        ::
-
-            >>> import torch
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'pytorch'
-            >>> _ = torch.manual_seed(1)
-
-            # Generate a batch of isomorphic graphs
-            >>> batch_size = 10
-            >>> X_gt = torch.zeros(batch_size, 4, 4)
-            >>> X_gt[:, torch.arange(0, 4, dtype=torch.int64), torch.randperm(4)] = 1
-            >>> A1 = 1. * (torch.rand(batch_size, 4, 4) > 0.5)
-            >>> torch.diagonal(A1, dim1=1, dim2=2)[:] = 0 # discard self-loop edges
-            >>> A2 = torch.bmm(torch.bmm(X_gt.transpose(1, 2), A1), X_gt)
-            >>> feat1 = torch.rand(batch_size, 4, 1024) - 0.5
-            >>> feat2 = torch.bmm(X_gt.transpose(1, 2), feat1)
-            >>> n1 = n2 = torch.tensor([4] * batch_size)
-
-            # Match by PCA-GM (load pretrained model)
-            >>> X, net = pygm.pca_gm(feat1, feat2, A1, A2, n1, n2, return_network=True)
-            Downloading to ~/.cache/pygmtools/pca_gm_voc_pytorch.pt...
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
-            tensor(1.)
-
-            # Pass the net object to avoid rebuilding the model agian
-            >>> X = pygm.pca_gm(feat1, feat2, A1, A2, n1, n2, network=net)
-
-            # You may also load other pretrained weights
-            >>> X, net = pygm.pca_gm(feat1, feat2, A1, A2, n1, n2, return_network=True, pretrain='willow')
-            Downloading to ~/.cache/pygmtools/pca_gm_willow_pytorch.pt...
-
-            # You may configure your own model and integrate the model into a deep learning pipeline. For example:
-            >>> net = pygm.utils.get_network(pygm.pca_gm, in_channel=1024, hidden_channel=2048, out_channel=512, num_layers=3, pretrain=False)
-            >>> optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
-            # feat1/feat2 may be outputs by other neural networks
-            >>> X = pygm.pca_gm(feat1, feat2, A1, A2, n1, n2, network=net)
-            >>> loss = pygm.utils.permutation_loss(X, X_gt)
-            >>> loss.backward()
-            >>> optimizer.step()
-
-        
-    .. dropdown:: Jittor Example
-
-        ::
-
-            >>> import jittor as jt
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'jittor'
-            >>> _ = jt.seed(1)
-
-            # Generate a batch of isomorphic graphs
-            >>> batch_size = 10
-            >>> X_gt = jt.zeros((batch_size, 4, 4))
-            >>> X_gt[:, jt.arange(0, 4, dtype=jt.int64), jt.randperm(4)] = 1
-            >>> A1 = 1. * (jt.rand(batch_size, 4, 4) > 0.5)
-            >>> for i in range(batch_size):
-            >>>     for j in range(4):
-            >>>         A1.data[i][j][j] = 0  # discard self-loop edges
-            >>> A2 = jt.bmm(jt.bmm(X_gt.transpose(1, 2), A1), X_gt)
-            >>> feat1 = jt.rand(batch_size, 4, 1024) - 0.5
-            >>> feat2 = jt.bmm(X_gt.transpose(1, 2), feat1)
-            >>> n1 = n2 = jt.Var([4] * batch_size)
-
-            # Match by PCA-GM (load pretrained model)
-            >>> X, net = pygm.pca_gm(feat1, feat2, A1, A2, n1, n2, return_network=True)
-            Downloading to ~/.cache/pygmtools/pca_gm_voc_jittor.pt...
-
-            # Pass the net object to avoid rebuilding the model agian
-            >>> X = pygm.pca_gm(feat1, feat2, A1, A2, n1, n2, network=net)
-
-            # You may also load other pretrained weights
-            >>> X, net = pygm.pca_gm(feat1, feat2, A1, A2, n1, n2, return_network=True, pretrain='willow')
-            Downloading to ~/.cache/pygmtools/pca_gm_willow_jittor.pt...
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
-            jt.Var([1.], dtype=float32)
-
-            # You may configure your own model and integrate the model into a deep learning pipeline. For example:
-            >>> net = pygm.utils.get_network(pygm.pca_gm, in_channel=1024, hidden_channel=2048, out_channel=512, num_layers=3, pretrain=False)
-            >>> optimizer = jt.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
-            # feat1/feat2 may be outputs by other neural networks
-            >>> X = pygm.pca_gm(feat1, feat2, A1, A2, n1, n2, network=net)
-            >>> loss = pygm.utils.permutation_loss(X, X_gt)
-            >>> optimizer.backward(loss)
-            >>> optimizer.step()
-
-            
-    .. dropdown:: Paddle Example
-
-        ::
-
-            >>> import paddle
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'paddle'
-            >>> _ = paddle.seed(4)
-
-            # Generate a batch of isomorphic graphs
-            >>> batch_size = 10
-            >>> X_gt = paddle.zeros((batch_size, 4, 4))
-            >>> X_gt[:, paddle.arange(0, 4, dtype=paddle.int64), paddle.randperm(4)] = 1
-            >>> A1 = 1. * (paddle.rand((batch_size, 4, 4)) > 0.5)
-            >>> paddle.diagonal(A1, axis1=1, axis2=2)[:] = 0 # discard self-loop edges
-            >>> A2 = paddle.bmm(paddle.bmm(X_gt.transpose((0, 2, 1)), A1), X_gt)
-            >>> feat1 = paddle.rand((batch_size, 4, 1024)) - 0.5
-            >>> feat2 = paddle.bmm(X_gt.transpose((0, 2, 1)), feat1)
-            >>> n1 = n2 = paddle.to_tensor([4] * batch_size)
-
-            # Match by PCA-GM (load pretrained model)
-            >>> X, net = pygm.pca_gm(feat1, feat2, A1, A2, n1, n2, return_network=True)
-            Downloading to ~/.cache/pygmtools/pca_gm_voc_paddle.pdparams...
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
-            Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True,
-                [1.])
-
-            # Pass the net object to avoid rebuilding the model agian
-            >>> X = pygm.pca_gm(feat1, feat2, A1, A2, n1, n2, network=net)
-
-            # You may also load other pretrained weights
-            >>> X, net = pygm.pca_gm(feat1, feat2, A1, A2, n1, n2, return_network=True, pretrain='willow')
-            Downloading to ~/.cache/pygmtools/pca_gm_willow_paddle.pdparams...
-
-            # You may configure your own model and integrate the model into a deep learning pipeline. For example:
-            >>> net = pygm.utils.get_network(pygm.pca_gm, in_channel=1024, hidden_channel=2048, out_channel=512, num_layers=3, pretrain=False)
-            >>> optimizer = paddle.optimizer.SGD(parameters=net.parameters(), learning_rate=0.001)
-            # feat1/feat2 may be outputs by other neural networks
-            >>> X = pygm.pca_gm(feat1, feat2, A1, A2, n1, n2, network=net)
-            >>> loss = pygm.utils.permutation_loss(X, X_gt)
-            >>> loss.backward()
-            >>> optimizer.step()
-
-    .. note::
-
-        If you find this model useful in your research, please cite:
-
-        ::
-
-            @article{WangPAMI20,
-              author = {Wang, Runzhong and Yan, Junchi and Yang, Xiaokang},
-              title = {Combinatorial Learning of Robust Deep Graph Matching: an Embedding based Approach},
-              journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
-              year = {2020}
-            }
-    """
-    if not num_layers >= 2: raise ValueError(f'num_layers must be >=2, got {num_layers}!')
-
-    if backend is None:
-        backend = pygmtools.BACKEND
-    non_batched_input = False
-    if feat1 is not None: # if feat1 is None, this function skips the forward pass and only returns a network object
-        for _ in (feat1, feat2, A1, A2):
-            _check_data_type(_, backend)
-
-        if all([_check_shape(_, 2, backend) for _ in (feat1, feat2, A1, A2)]):
-            feat1, feat2, A1, A2 = [_unsqueeze(_, 0, backend) for _ in (feat1, feat2, A1, A2)]
-            if type(n1) is int: n1 = from_numpy(np.array([n1]), backend=backend)
-            if type(n2) is int: n2 = from_numpy(np.array([n2]), backend=backend)
-            non_batched_input = True
-        elif all([_check_shape(_, 3, backend) for _ in (feat1, feat2, A1, A2)]):
-            non_batched_input = False
-        else:
-            raise ValueError(
-                f'the input arguments feat1, feat2, A1, A2 are expected to be all 2-dimensional or 3-dimensional, got '
-                f'feat1:{len(_get_shape(feat1, backend))}dims, feat2:{len(_get_shape(feat2, backend))}dims, '
-                f'A1:{len(_get_shape(A1, backend))}dims, A2:{len(_get_shape(A2, backend))}dims!')
-
-        if not (_get_shape(feat1, backend)[0] == _get_shape(feat2, backend)[0] == _get_shape(A1, backend)[0] == _get_shape(A2, backend)[0])\
-                or not (_get_shape(feat1, backend)[1] == _get_shape(A1, backend)[1] == _get_shape(A1, backend)[2])\
-                or not (_get_shape(feat2, backend)[1] == _get_shape(A2, backend)[1] == _get_shape(A2, backend)[2])\
-                or not (_get_shape(feat1, backend)[2] == _get_shape(feat2, backend)[2]):
-            raise ValueError(
-                f'the input dimensions do not match. Got feat1:{_get_shape(feat1, backend)}, '
-                f'feat2:{_get_shape(feat2, backend)}, A1:{_get_shape(A1, backend)}, A2:{_get_shape(A2, backend)}!')
-    if n1 is not None: _check_data_type(n1, 'n1', backend)
-    if n2 is not None: _check_data_type(n2, 'n2', backend)
-
-    args = (feat1, feat2, A1, A2, n1, n2, in_channel, hidden_channel, out_channel, num_layers, sk_max_iter, sk_tau,
-           network, pretrain)
-    try:
-        mod = importlib.import_module(f'pygmtools.{backend}_backend')
-        fn = mod.pca_gm
-    except (ModuleNotFoundError, AttributeError):
-        raise NotImplementedError(
-            NOT_IMPLEMENTED_MSG.format(backend)
-        )
-
-    result = fn(*args)
-    match_mat = _squeeze(result[0], 0, backend) if non_batched_input else result[0]
-    if return_network:
-        return match_mat, result[1]
-    else:
-        return match_mat
-
-
-def ipca_gm(feat1, feat2, A1, A2, n1=None, n2=None,
-            in_channel=1024, hidden_channel=2048, out_channel=2048, num_layers=2, cross_iter=3,
-            sk_max_iter=20, sk_tau=0.05,
-            network=None, return_network=False, pretrain='voc',
-            backend=None):
-    r"""
-    The **IPCA-GM** (Iterative Permutation loss and Cross-graph Affinity Graph Matching) neural network model for
-    processing two individual graphs (KB-QAP).
-    The graph matching module is composed of several intra-graph embedding layers, a cross-graph embedding layer, and
-    a Sinkhorn matching layer. The weight matrix of the cross-graph embedding layer is updated iteratively.
-    Only the second last layer has a cross-graph update layer.
-    IPCA-GM is the extended version of PCA-GM (see :func:`~pygmtools.neural_solvers.pca_gm`). The dfference is that
-    the cross-graph weight in PCA-GM is computed in one shot, and in IPCA-GM it is updated iteratively.
-
-    See the following pipeline for an example, with application to visual graph matching (layers in gray box are
-    implemented by pygmtools):
-
-    .. image:: ../../images/ipca_gm.png
-
-    See the following paper for more technical details:
-    `"Wang et al. Combinatorial Learning of Robust Deep Graph Matching: an Embedding based Approach. TPAMI 2020."
-    <https://ieeexplore.ieee.org/abstract/document/9128045/>`_
-
-    :param feat1: :math:`(b\times n_1 \times d)` input feature of graph1
-    :param feat2: :math:`(b\times n_2 \times d)` input feature of graph2
-    :param A1: :math:`(b\times n_1 \times n_1)` input adjacency matrix of graph1
-    :param A2: :math:`(b\times n_2 \times n_2)` input adjacency matrix of graph2
-    :param n1: :math:`(b)` number of nodes in graph1. Optional if all equal to :math:`n_1`
-    :param n2: :math:`(b)` number of nodes in graph2. Optional if all equal to :math:`n_2`
-    :param in_channel: (default: 1024) Channel size of the input layer. It must match the feature dimension :math:`(d)`
-        of ``feat1, feat2``. Ignored if the network object is given (ignored if ``network!=None``)
-    :param hidden_channel: (default: 2048) Channel size of hidden layers. Ignored if the network object is given
-        (ignored if ``network!=None``)
-    :param out_channel: (default: 2048) Channel size of the output layer. Ignored if the network object is given
-        (ignored if ``network!=None``)
-    :param num_layers: (default: 2) Number of graph embedding layers. Must be >=2. Ignored if the network object is
-        given (ignored if ``network!=None``)
-    :param cross_iter: (default: 3) Number of iterations for the cross-graph embedding layer.
-    :param sk_max_iter: (default: 20) Max number of iterations of Sinkhorn. See
-        :func:`~pygmtools.classic_solvers.sinkhorn` for more details about this argument.
-    :param sk_tau: (default: 0.05) The temperature parameter of Sinkhorn. See
-        :func:`~pygmtools.classic_solvers.sinkhorn` for more details about this argument.
-    :param network: (default: None) The network object. If None, a new network object will be created, and load the
-        model weights specified in ``pretrain`` argument.
-    :param return_network: (default: False) Return the network object (saving model construction time if calling the
-        model multiple times).
-    :param pretrain: (default: 'voc') If ``network==None``, the pretrained model weights to be loaded. Available
-        pretrained weights: ``voc`` (on Pascal VOC Keypoint dataset), ``willow`` (on Willow Object Class dataset),
-        or ``False`` (no pretraining).
-    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
-    :return: if ``return_network==False``, :math:`(b\times n_1 \times n_2)` the doubly-stochastic matching matrix
-
-        if ``return_network==True``, :math:`(b\times n_1 \times n_2)` the doubly-stochastic matching matrix,
-        the network object
-
-    .. note::
-        You may need a proxy to load the pretrained weights if Google drive is not accessible in your contry/region.
-        You may also download the pretrained models manually and put them at ``~/.cache/pygmtools`` (for Linux).
-
-        `[google drive] <https://drive.google.com/drive/folders/1O7vkIW8QXBJsNsHUIRiSw91HJ_0FAzu_?usp=sharing>`_
-        `[baidu drive] <https://pan.baidu.com/s/1MvzfM52NJeLWx2JXbbc6HA?pwd=x8bv>`_
-
-    .. note::
-        This function also supports non-batched input, by ignoring all batch dimensions in the input tensors.
-
-    .. dropdown:: Numpy Example
-
-        ::
-
-            >>> import numpy as np
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'numpy'
-            >>> np.random.seed(1)
-
-            # Generate a batch of isomorphic graphs
-            >>> batch_size = 10
-            >>> X_gt = np.zeros((batch_size, 4, 4))
-            >>> X_gt[:, np.arange(0, 4, dtype='i4'), np.random.permutation(4)] = 1
-            >>> A1 = 1. * (np.random.rand(batch_size, 4, 4) > 0.5)
-            >>> for i in np.arange(4): # discard self-loop edges
-            ...    for j in np.arange(batch_size):
-            ...        A1[j][i][i] = 0
-            >>> A2 = np.matmul(np.matmul(X_gt.swapaxes(1, 2), A1), X_gt)
-            >>> feat1 = np.random.rand(batch_size, 4, 1024) - 0.5
-            >>> feat2 = np.matmul(X_gt.swapaxes(1, 2), feat1)
-            >>> n1 = n2 = np.array([4] * batch_size)
-
-            # Match by IPCA-GM (load pretrained model)
-            >>> X, net = pygm.ipca_gm(feat1, feat2, A1, A2, n1, n2, return_network=True)
-            Downloading to ~/.cache/pygmtools/ipca_gm_voc_numpy.npy...
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
-            1.0
-
-            # Pass the net object to avoid rebuilding the model agian
-            >>> X = pygm.ipca_gm(feat1, feat2, A1, A2, n1, n2, network=net)
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
-            1.0
-
-            # You may also load other pretrained weights
-            >>> X, net = pygm.ipca_gm(feat1, feat2, A1, A2, n1, n2, return_network=True, pretrain='willow')
-            Downloading to ~/.cache/pygmtools/ipca_gm_willow_numpy.npy...
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
-            1.0
-
-            # You may configure your own model and integrate the model into a deep learning pipeline. For example:
-            >>> net = pygm.utils.get_network(pygm.ipca_gm, in_channel=1024, hidden_channel=2048, out_channel=512, num_layers=3, cross_iter=10, pretrain=False)
-            # feat1/feat2 may be outputs by other neural networks
-            >>> X = pygm.ipca_gm(feat1, feat2, A1, A2, n1, n2, network=net)
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
-            1.0
-
-
-    .. dropdown:: PyTorch Example
-
-        ::
-
-            >>> import torch
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'pytorch'
-            >>> _ = torch.manual_seed(1)
-
-            # Generate a batch of isomorphic graphs
-            >>> batch_size = 10
-            >>> X_gt = torch.zeros(batch_size, 4, 4)
-            >>> X_gt[:, torch.arange(0, 4, dtype=torch.int64), torch.randperm(4)] = 1
-            >>> A1 = 1. * (torch.rand(batch_size, 4, 4) > 0.5)
-            >>> torch.diagonal(A1, dim1=1, dim2=2)[:] = 0 # discard self-loop edges
-            >>> A2 = torch.bmm(torch.bmm(X_gt.transpose(1, 2), A1), X_gt)
-            >>> feat1 = torch.rand(batch_size, 4, 1024) - 0.5
-            >>> feat2 = torch.bmm(X_gt.transpose(1, 2), feat1)
-            >>> n1 = n2 = torch.tensor([4] * batch_size)
-
-            # Match by IPCA-GM (load pretrained model)
-            >>> X, net = pygm.ipca_gm(feat1, feat2, A1, A2, n1, n2, return_network=True)
-            Downloading to ~/.cache/pygmtools/ipca_gm_voc_pytorch.pt...
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
-            tensor(1.)
-
-            # Pass the net object to avoid rebuilding the model agian
-            >>> X = pygm.ipca_gm(feat1, feat2, A1, A2, n1, n2, network=net)
-
-            # You may also load other pretrained weights
-            >>> X, net = pygm.ipca_gm(feat1, feat2, A1, A2, n1, n2, return_network=True, pretrain='willow')
-            Downloading to ~/.cache/pygmtools/ipca_gm_willow_pytorch.pt...
-
-            # You may configure your own model and integrate the model into a deep learning pipeline. For example:
-            >>> net = pygm.utils.get_network(pygm.ipca_gm, in_channel=1024, hidden_channel=2048, out_channel=512, num_layers=3, cross_iter=10, pretrain=False)
-            >>> optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
-            # feat1/feat2 may be outputs by other neural networks
-            >>> X = pygm.ipca_gm(feat1, feat2, A1, A2, n1, n2, network=net)
-            >>> loss = pygm.utils.permutation_loss(X, X_gt)
-            >>> loss.backward()
-            >>> optimizer.step()
-
-
-    .. dropdown:: Jittor Example
-
-        ::
-
-            >>> import jittor as jt
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'jittor'
-            >>> _ = jt.seed(1)
-
-            # Generate a batch of isomorphic graphs
-            >>> batch_size = 10
-            >>> X_gt = jt.zeros((batch_size, 4, 4))
-            >>> X_gt[:, jt.arange(0, 4, dtype=jt.int64), jt.randperm(4)] = 1
-            >>> A1 = 1. * (jt.rand(batch_size, 4, 4) > 0.5)
-            >>> for i in range(batch_size):
-            >>>     for j in range(4):
-            >>>         A1.data[i][j][j] = 0  # discard self-loop edges
-            >>> A2 = jt.bmm(jt.bmm(X_gt.transpose(1, 2), A1), X_gt)
-            >>> feat1 = jt.rand(batch_size, 4, 1024) - 0.5
-            >>> feat2 = jt.bmm(X_gt.transpose(1, 2), feat1)
-            >>> n1 = n2 = jt.Var([4] * batch_size)
-
-            # Match by IPCA-GM (load pretrained model)
-            >>> X, net = pygm.ipca_gm(feat1, feat2, A1, A2, n1, n2, return_network=True)
-            Downloading to ~/.cache/pygmtools/ipca_gm_voc_jitttor.pt...
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
-            jt.Var([1.], dtype=float32)
-
-            # Pass the net object to avoid rebuilding the model agian
-            >>> X = pygm.ipca_gm(feat1, feat2, A1, A2, n1, n2, network=net)
-
-            # You may also load other pretrained weights
-            >>> X, net = pygm.ipca_gm(feat1, feat2, A1, A2, n1, n2, return_network=True, pretrain='willow')
-            Downloading to ~/.ca/che/pygmtools/ipca_gm_willow_jittor.pt...
-
-            # You may configure your own model and integrate the model into a deep learning pipeline. For example:
-            >>> net = pygm.utils.get_network(pygm.ipca_gm, in_channel=1024, hidden_channel=2048, out_channel=512, num_layers=3, cross_iter=10, pretrain=False)
-            >>> optimizer = jt.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
-            # feat1/feat2 may be outputs by other neural networks
-            >>> X = pygm.ipca_gm(feat1, feat2, A1, A2, n1, n2, network=net)
-            >>> loss = pygm.utils.permutation_loss(X, X_gt)
-            >>> optimizer.backward(loss)
-            >>> optimizer.step()
-    
-    
-    .. dropdown:: Paddle Example
-
-        ::
-
-            >>> import paddle
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'paddle'
-            >>> _ = paddle.seed(5)
-
-            # Generate a batch of isomorphic graphs
-            >>> batch_size = 10
-            >>> X_gt = paddle.zeros((batch_size, 4, 4))
-            >>> X_gt[:, paddle.arange(0, 4, dtype=paddle.int64), paddle.randperm(4)] = 1
-            >>> A1 = 1. * (paddle.rand((batch_size, 4, 4)) > 0.5)
-            >>> paddle.diagonal(A1, axis1=1, axis2=2)[:] = 0 # discard self-loop edges
-            >>> A2 = paddle.bmm(paddle.bmm(X_gt.transpose((0, 2, 1)), A1), X_gt)
-            >>> feat1 = paddle.rand((batch_size, 4, 1024)) - 0.5
-            >>> feat2 = paddle.bmm(X_gt.transpose((0, 2, 1)), feat1)
-            >>> n1 = n2 = paddle.to_tensor([4] * batch_size)
-
-            # Match by IPCA-GM (load pretrained model)
-            >>> X, net = pygm.ipca_gm(feat1, feat2, A1, A2, n1, n2, return_network=True)
-            Downloading to ~/.cache/pygmtools/ipca_gm_voc_paddle.pdparams...
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
-            Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True,
-                    [1.])
-
-            # Pass the net object to avoid rebuilding the model agian
-            >>> X = pygm.ipca_gm(feat1, feat2, A1, A2, n1, n2, network=net)
-
-            # You may also load other pretrained weights
-            >>> X, net = pygm.ipca_gm(feat1, feat2, A1, A2, n1, n2, return_network=True, pretrain='willow')
-            Downloading to ~/.cache/pygmtools/ipca_gm_willow_paddle.pdparams...
-
-            # You may configure your own model and integrate the model into a deep learning pipeline. For example:
-            >>> net = pygm.utils.get_network(pygm.ipca_gm, in_channel=1024, hidden_channel=2048, out_channel=512, num_layers=3, cross_iter=10, pretrain=False)
-            >>> optimizer = paddle.optimizer.SGD(parameters=net.parameters(), learning_rate=0.001)
-            # feat1/feat2 may be outputs by other neural networks
-            >>> X = pygm.ipca_gm(feat1, feat2, A1, A2, n1, n2, network=net)
-            >>> loss = pygm.utils.permutation_loss(X, X_gt)
-            >>> loss.backward()
-            >>> optimizer.step()
-
-    .. note::
-
-        If you find this model useful in your research, please cite:
-
-        ::
-
-            @article{WangPAMI20,
-              author = {Wang, Runzhong and Yan, Junchi and Yang, Xiaokang},
-              title = {Combinatorial Learning of Robust Deep Graph Matching: an Embedding based Approach},
-              journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
-              year = {2020}
-            }
-    """
-    if not num_layers >= 2: raise ValueError(f'num_layers must be >=2, got {num_layers}!')
-    if not cross_iter >= 1: raise ValueError(f'cross_iter must be >=1, got {cross_iter}!')
-
-    if backend is None:
-        backend = pygmtools.BACKEND
-    non_batched_input = False
-    if feat1 is not None:  # if feat1 is None, this function skips the forward pass and only returns a network object
-        _check_data_type(feat1, 'feat1', backend)
-        _check_data_type(feat2, 'feat2', backend)
-        _check_data_type(A1, 'A1', backend)
-        _check_data_type(A2, 'A2', backend)
-
-        if all([_check_shape(_, 2, backend) for _ in (feat1, feat2, A1, A2)]):
-            feat1, feat2, A1, A2 = [_unsqueeze(_, 0, backend) for _ in (feat1, feat2, A1, A2)]
-            if type(n1) is int: n1 = from_numpy(np.array([n1]), backend=backend)
-            if type(n2) is int: n2 = from_numpy(np.array([n2]), backend=backend)
-            non_batched_input = True
-        elif all([_check_shape(_, 3, backend) for _ in (feat1, feat2, A1, A2)]):
-            non_batched_input = False
-        else:
-            raise ValueError(
-                f'the input arguments feat1, feat2, A1, A2 are expected to be all 2-dimensional or 3-dimensional, got '
-                f'feat1:{len(_get_shape(feat1, backend))}dims, feat2:{len(_get_shape(feat2, backend))}dims, '
-                f'A1:{len(_get_shape(A1, backend))}dims, A2:{len(_get_shape(A2, backend))}dims!')
-
-        if not (_get_shape(feat1, backend)[0] == _get_shape(feat2, backend)[0] == _get_shape(A1, backend)[0] == _get_shape(A2, backend)[0])\
-                or not (_get_shape(feat1, backend)[1] == _get_shape(A1, backend)[1] == _get_shape(A1, backend)[2])\
-                or not (_get_shape(feat2, backend)[1] == _get_shape(A2, backend)[1] == _get_shape(A2, backend)[2])\
-                or not (_get_shape(feat1, backend)[2] == _get_shape(feat2, backend)[2]):
-            raise ValueError(
-                f'the input dimensions do not match. Got feat1:{_get_shape(feat1, backend)}, '
-                f'feat2:{_get_shape(feat2, backend)}, A1:{_get_shape(A1, backend)}, A2:{_get_shape(A2, backend)}!')
-    if n1 is not None: _check_data_type(n1, 'n1', backend)
-    if n2 is not None: _check_data_type(n2, 'n2', backend)
-
-    args = (feat1, feat2, A1, A2, n1, n2, in_channel, hidden_channel, out_channel, num_layers, cross_iter,
-            sk_max_iter, sk_tau, network, pretrain)
-    try:
-        mod = importlib.import_module(f'pygmtools.{backend}_backend')
-        fn = mod.ipca_gm
-    except (ModuleNotFoundError, AttributeError):
-        raise NotImplementedError(
-            NOT_IMPLEMENTED_MSG.format(backend)
-        )
-
-    result = fn(*args)
-    match_mat = _squeeze(result[0], 0, backend) if non_batched_input else result[0]
-    if return_network:
-        return match_mat, result[1]
-    else:
-        return match_mat
-
-
-def cie(feat_node1, feat_node2, A1, A2, feat_edge1, feat_edge2, n1=None, n2=None,
-        in_node_channel=1024, in_edge_channel=1, hidden_channel=2048, out_channel=2048, num_layers=2,
-        sk_max_iter=20, sk_tau=0.05,
-        network=None, return_network=False, pretrain='voc',
-        backend=None):
-    r"""
-    The **CIE** (Channel Independent Embedding) graph matching neural network model for processing two individual graphs
-    (KB-QAP).
-    The graph matching module is composed of several intra-graph embedding layers, a cross-graph embedding layer, and
-    a Sinkhorn matching layer. Only the second last layer has a cross-graph update layer. The graph embedding layers
-    are based on channel-independent embedding, under the assumption that such a message passing scheme may offer higher
-    model capacity especially with high-dimensional edge features.
-
-    See the following pipeline for an example, with application to visual graph matching:
-
-    .. image:: ../../images/cie_framework.png
-
-    The graph embedding layer (CIE layer) involves both node embeding and edge embedding:
-
-    .. image:: ../../images/cie_layer.png
-
-    See the following paper for more technical details:
-    `"Yu et al. Learning Deep Graph Matching with Channel-Independent Embedding and Hungarian Attention. ICLR 2020."
-    <https://openreview.net/pdf?id=rJgBd2NYPH>`_
-
-    :param feat_node1: :math:`(b\times n_1 \times d_n)` input node feature of graph1
-    :param feat_node2: :math:`(b\times n_2 \times d_n)` input node feature of graph2
-    :param A1: :math:`(b\times n_1 \times n_1)` input adjacency matrix of graph1
-    :param A2: :math:`(b\times n_2 \times n_2)` input adjacency matrix of graph2
-    :param feat_edge1: :math:`(b\times n_1 \times n_1 \times d_e)` input edge feature of graph1
-    :param feat_edge2: :math:`(b\times n_2 \times n_2 \times d_e)` input edge feature of graph2
-    :param n1: :math:`(b)` number of nodes in graph1. Optional if all equal to :math:`n_1`
-    :param n2: :math:`(b)` number of nodes in graph2. Optional if all equal to :math:`n_2`
-    :param in_node_channel: (default: 1024) Node channel size of the input layer. It must match the feature dimension
-        :math:`(d_n)` of ``feat_node1, feat_node2``. Ignored if the network object is given (ignored if ``network!=None``)
-    :param in_edge_channel: (default: 1) Edge channel size of the input layer. It must match the feature dimension
-        :math:`(d_e)` of ``feat_edge1, feat_edge2``. Ignored if the network object is given (ignored if ``network!=None``)
-    :param hidden_channel: (default: 2048) Channel size of hidden layers (node channel == edge channel).
-        Ignored if the network object is given (ignored if ``network!=None``)
-    :param out_channel: (default: 2048) Channel size of the output layer (node channel == edge channel).
-        Ignored if the network object is given (ignored if ``network!=None``)
-    :param num_layers: (default: 2) Number of graph embedding layers. Must be >=2. Ignored if the network object is
-        given (ignored if ``network!=None``)
-    :param sk_max_iter: (default: 20) Max number of iterations of Sinkhorn. See
-        :func:`~pygmtools.classic_solvers.sinkhorn` for more details about this argument.
-    :param sk_tau: (default: 0.05) The temperature parameter of Sinkhorn. See
-        :func:`~pygmtools.classic_solvers.sinkhorn` for more details about this argument.
-    :param network: (default: None) The network object. If None, a new network object will be created, and load the
-        model weights specified in ``pretrain`` argument.
-    :param return_network: (default: False) Return the network object (saving model construction time if calling the
-        model multiple times).
-    :param pretrain: (default: 'voc') If ``network==None``, the pretrained model weights to be loaded. Available
-        pretrained weights: ``voc`` (on Pascal VOC Keypoint dataset), ``willow`` (on Willow Object Class dataset),
-        or ``False`` (no pretraining).
-    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
-    :return: if ``return_network==False``, :math:`(b\times n_1 \times n_2)` the doubly-stochastic matching matrix
-
-        if ``return_network==True``, :math:`(b\times n_1 \times n_2)` the doubly-stochastic matching matrix,
-        the network object
-
-    .. note::
-        You may need a proxy to load the pretrained weights if Google drive is not accessible in your contry/region.
-        You may also download the pretrained models manually and put them at ``~/.cache/pygmtools`` (for Linux).
-
-        `[google drive] <https://drive.google.com/drive/folders/1O7vkIW8QXBJsNsHUIRiSw91HJ_0FAzu_?usp=sharing>`_
-        `[baidu drive] <https://pan.baidu.com/s/1MvzfM52NJeLWx2JXbbc6HA?pwd=x8bv>`_
-
-    .. note::
-        This function also supports non-batched input, by ignoring all batch dimensions in the input tensors.
-
-    .. dropdown:: Numpy Example
-
-        ::
-
-            >>> import numpy as np
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'numpy'
-            >>> np.random.seed(1)
-
-            # Generate a batch of isomorphic graphs
-            >>> batch_size = 10
-            >>> X_gt = np.zeros((batch_size, 4, 4))
-            >>> X_gt[:, np.arange(0, 4, dtype='i4'), np.random.permutation(4)] = 1
-            >>> A1 = 1. * (np.random.rand(batch_size, 4, 4) > 0.5)
-            >>> for i in np.arange(4): # discard self-loop edges
-            ...    for j in np.arange(batch_size):
-            ...        A1[j][i][i] = 0
-            >>> e_feat1 = np.expand_dims(np.random.rand(batch_size, 4, 4) * A1,axis=-1) # shape: (10, 4, 4, 1)
-            >>> A2 = np.matmul(np.matmul(X_gt.swapaxes(1, 2), A1), X_gt)
-            >>> e_feat2 = np.expand_dims(np.matmul(np.matmul(X_gt.swapaxes(1, 2),np.squeeze(e_feat1,axis=-1)), X_gt),axis=-1)
-            >>> feat1 = np.random.rand(batch_size, 4, 1024) - 0.5
-            >>> feat2 = np.matmul(X_gt.swapaxes(1, 2), feat1)
-            >>> n1 = n2 = np.array([4] * batch_size)
-
-            # Match by CIE (load pretrained model)
-            >>> X, net = pygm.cie(feat1, feat2, A1, A2, e_feat1, e_feat2, n1, n2, return_network=True)
-            Downloading to ~/.cache/pygmtools/cie_voc_numpy.npy...
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
-            1.0
-
-            # Pass the net object to avoid rebuilding the model agian
-            >>> X = pygm.cie(feat1, feat2, A1, A2, e_feat1, e_feat2, n1, n2, network=net)
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
-            1.0
-
-            # You may also load other pretrained weights
-            >>> X, net = pygm.cie(feat1, feat2, A1, A2, e_feat1, e_feat2, n1, n2, return_network=True, pretrain='willow')
-            Downloading to ~/.cache/pygmtools/cie_willow_numpy.npy...
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
-            1.0
-
-            # You may configure your own model and integrate the model into a deep learning pipeline. For example:
-            >>> net = pygm.utils.get_network(pygm.cie, in_node_channel=1024, in_edge_channel=1, hidden_channel=2048, out_channel=512, num_layers=3, pretrain=False)
-            # feat1/feat2/e_feat1/e_feat2 may be outputs by other neural networks
-            >>> X = pygm.cie(feat1, feat2, A1, A2, e_feat1, e_feat2, n1, n2, network=net)
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
-            1.0
-            
-
-    .. dropdown:: PyTorch Example
-
-        ::
-
-            >>> import torch
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'pytorch'
-            >>> _ = torch.manual_seed(1)
-
-            # Generate a batch of isomorphic graphs
-            >>> batch_size = 10
-            >>> X_gt = torch.zeros(batch_size, 4, 4)
-            >>> X_gt[:, torch.arange(0, 4, dtype=torch.int64), torch.randperm(4)] = 1
-            >>> A1 = 1. * (torch.rand(batch_size, 4, 4) > 0.5)
-            >>> torch.diagonal(A1, dim1=1, dim2=2)[:] = 0 # discard self-loop edges
-            >>> e_feat1 = (torch.rand(batch_size, 4, 4) * A1).unsqueeze(-1) # shape: (10, 4, 4, 1)
-            >>> A2 = torch.bmm(torch.bmm(X_gt.transpose(1, 2), A1), X_gt)
-            >>> e_feat2 = torch.bmm(torch.bmm(X_gt.transpose(1, 2), e_feat1.squeeze(-1)), X_gt).unsqueeze(-1)
-            >>> feat1 = torch.rand(batch_size, 4, 1024) - 0.5
-            >>> feat2 = torch.bmm(X_gt.transpose(1, 2), feat1)
-            >>> n1 = n2 = torch.tensor([4] * batch_size)
-
-            # Match by CIE (load pretrained model)
-            >>> X, net = pygm.cie(feat1, feat2, A1, A2, e_feat1, e_feat2, n1, n2, return_network=True)
-            Downloading to ~/.cache/pygmtools/cie_voc_pytorch.pt...
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
-            tensor(1.)
-
-            # Pass the net object to avoid rebuilding the model agian
-            >>> X = pygm.cie(feat1, feat2, A1, A2, e_feat1, e_feat2, n1, n2, network=net)
-
-            # You may also load other pretrained weights
-            >>> X, net = pygm.cie(feat1, feat2, A1, A2, e_feat1, e_feat2, n1, n2, return_network=True, pretrain='willow')
-            Downloading to ~/.cache/pygmtools/cie_willow_pytorch.pt...
-
-            # You may configure your own model and integrate the model into a deep learning pipeline. For example:
-            >>> net = pygm.utils.get_network(pygm.cie, in_node_channel=1024, in_edge_channel=1, hidden_channel=2048, out_channel=512, num_layers=3, pretrain=False)
-            >>> optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
-            # feat1/feat2/e_feat1/e_feat2 may be outputs by other neural networks
-            >>> X = pygm.cie(feat1, feat2, A1, A2, e_feat1, e_feat2, n1, n2, network=net)
-            >>> loss = pygm.utils.permutation_loss(X, X_gt)
-            >>> loss.backward()
-            >>> optimizer.step()
-
-    .. dropdown:: Jittor Example
-
-        ::
-
-            >>> import jittor as jt
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'jittor'
-            >>> _ = jt.seed(1)
-
-            # Generate a batch of isomorphic graphs
-            >>> batch_size = 10
-            >>> X_gt = jt.zeros((batch_size, 4, 4))
-            >>> X_gt[:, jt.arange(0, 4, dtype=jt.int64), jt.randperm(4)] = 1
-            >>> A1 = 1. * (jt.rand(batch_size, 4, 4) > 0.5)
-            >>> for i in range(batch_size):
-            >>>     for j in range(4):
-            >>>        A1.data[i][j][j] = 0  # discard self-loop edges
-            >>> e_feat1 = (jt.rand(batch_size, 4, 4) * A1).unsqueeze(-1) # shape: (10, 4, 4, 1)
-            >>> A2 = jt.bmm(jt.bmm(X_gt.transpose(1, 2), A1), X_gt)
-            >>> e_feat2 = jt.bmm(jt.bmm(X_gt.transpose(1, 2), e_feat1.squeeze(-1)), X_gt).unsqueeze(-1)
-            >>> feat1 = jt.rand(batch_size, 4, 1024) - 0.5
-            >>> feat2 = jt.bmm(X_gt.transpose(1, 2), feat1)
-            >>> n1 = n2 = jt.Var([4] * batch_size)
-
-            # Match by CIE (load pretrained model)
-            >>> X, net = pygm.cie(feat1, feat2, A1, A2, e_feat1, e_feat2, n1, n2, return_network=True)
-            Downloading to ~/.cache/pygmtools/cie_voc_jittor.pt...
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
-            jt.Var([1.], dtype=float32)
-
-            # Pass the net object to avoid rebuilding the model agian
-            >>> X = pygm.cie(feat1, feat2, A1, A2, e_feat1, e_feat2, n1, n2, network=net)
-
-            # You may also load other pretrained weights
-            >>> X, net = pygm.cie(feat1, feat2, A1, A2, e_feat1, e_feat2, n1, n2, return_network=True, pretrain='willow')
-            Downloading to ~/.cache/pygmtools/cie_willow_jittor.pt...
-
-            # You may configure your own model and integrate the model into a deep learning pipeline. For example:
-            >>> net = pygm.utils.get_network(pygm.cie, in_node_channel=1024, in_edge_channel=1, hidden_channel=2048, out_channel=512, num_layers=3, pretrain=False)
-            >>> optimizer = jt.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
-            # feat1/feat2/e_feat1/e_feat2 may be outputs by other neural networks
-            >>> X = pygm.cie(feat1, feat2, A1, A2, e_feat1, e_feat2, n1, n2, network=net)
-            >>> loss = pygm.utils.permutation_loss(X, X_gt)
-            >>> optimizer.backward(loss)
-            >>> optimizer.step()
-
-    
-    .. dropdown:: Paddle Example
-
-        ::
-
-            >>> import paddle
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'paddle'
-            >>> _ = paddle.seed(1)
-
-            # Generate a batch of isomorphic graphs
-            >>> batch_size = 10
-            >>> X_gt = paddle.zeros((batch_size, 4, 4))
-            >>> X_gt[:, paddle.arange(0, 4, dtype=paddle.int64), paddle.randperm(4)] = 1
-            >>> A1 = 1. * (paddle.rand((batch_size, 4, 4)) > 0.5)
-            >>> paddle.diagonal(A1, axis1=1, axis2=2)[:] = 0 # discard self-loop edges
-            >>> e_feat1 = (paddle.rand((batch_size, 4, 4)) * A1).unsqueeze(-1) # shape: (10, 4, 4, 1)
-            >>> A2 = paddle.bmm(paddle.bmm(X_gt.transpose((0, 2, 1)), A1), X_gt)
-            >>> e_feat2 = paddle.bmm(paddle.bmm(X_gt.transpose((0, 2, 1)), e_feat1.squeeze(-1)), X_gt).unsqueeze(-1)
-            >>> feat1 = paddle.rand((batch_size, 4, 1024)) - 0.5
-            >>> feat2 = paddle.bmm(X_gt.transpose((0, 2, 1)), feat1)
-            >>> n1 = n2 = paddle.to_tensor([4] * batch_size)
-
-            # Match by CIE (load pretrained model)
-            >>> X, net = pygm.cie(feat1, feat2, A1, A2, e_feat1, e_feat2, n1, n2, return_network=True)
-            Downloading to ~/.cache/pygmtools/cie_voc_paddle.pdparams...
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
-            Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True,
-                   [1.])
-
-            # Pass the net object to avoid rebuilding the model agian
-            >>> X = pygm.cie(feat1, feat2, A1, A2, e_feat1, e_feat2, n1, n2, network=net)
-
-            # You may also load other pretrained weights
-            >>> X, net = pygm.cie(feat1, feat2, A1, A2, e_feat1, e_feat2, n1, n2, return_network=True, pretrain='willow')
-            Downloading to ~/.cache/pygmtools/cie_willow_paddle.pdparams...
-
-            # You may configure your own model and integrate the model into a deep learning pipeline. For example:
-            >>> net = pygm.utils.get_network(pygm.cie, in_node_channel=1024, in_edge_channel=1, hidden_channel=2048, out_channel=512, num_layers=3, pretrain=False)
-            >>> optimizer = paddle.optimizer.SGD(parameters=net.parameters(), learning_rate=0.001)
-            # feat1/feat2/e_feat1/e_feat2 may be outputs by other neural networks
-            >>> X = pygm.cie(feat1, feat2, A1, A2, e_feat1, e_feat2, n1, n2, network=net)
-            >>> loss = pygm.utils.permutation_loss(X, X_gt)
-            >>> loss.backward()
-            >>> optimizer.step()
-    
-    .. note::
-
-        If you find this model useful in your research, please cite:
-
-        ::
-
-            @inproceedings{YuICLR20,
-              title={Learning deep graph matching with channel-independent embedding and Hungarian attention},
-              author={Yu, Tianshu and Wang, Runzhong and Yan, Junchi and Li, Baoxin},
-              booktitle={International Conference on Learning Representations},
-              year={2020}
-            }
-    """
-    if not num_layers >= 2: raise ValueError(f'num_layers must be >=2, got {num_layers}!')
-
-    if backend is None:
-        backend = pygmtools.BACKEND
-    non_batched_input = False
-    if feat_node1 is not None:  # if feat_node1 is None, this function skips the forward pass and only returns a network object
-        _check_data_type(feat_node1, 'feat_node1', backend)
-        _check_data_type(feat_node2, 'feat_node2', backend)
-        _check_data_type(A1, 'A1', backend)
-        _check_data_type(A2, 'A2', backend)
-        _check_data_type(feat_edge1, 'feat_edge1', backend)
-        _check_data_type(feat_edge2, 'feat_edge2', backend)
-
-        if all([_check_shape(_, 2, backend) for _ in (feat_node1, feat_node2, A1, A2)]) \
-                and all([_check_shape(_, 3, backend) for _ in (feat_edge1, feat_edge2)]):
-            feat_node1, feat_node2, A1, A2, feat_edge1, feat_edge2 =\
-                [_unsqueeze(_, 0, backend) for _ in (feat_node1, feat_node2, A1, A2, feat_edge1, feat_edge2)]
-            if type(n1) is int: n1 = from_numpy(np.array([n1]), backend=backend)
-            if type(n2) is int: n2 = from_numpy(np.array([n2]), backend=backend)
-            non_batched_input = True
-        elif all([_check_shape(_, 3, backend) for _ in (feat_node1, feat_node2, A1, A2)]) \
-                and all([_check_shape(_, 4, backend) for _ in (feat_edge1, feat_edge2)]):
-            non_batched_input = False
-        else:
-            raise ValueError(
-                f'the dimensions of the input arguments are illegal. Got '
-                f'feat_node1:{len(_get_shape(feat_node1, backend))}dims, feat_node2:{len(_get_shape(feat_node2, backend))}dims, '
-                f'A1:{len(_get_shape(A1, backend))}dims, A2:{len(_get_shape(A2, backend))}dims, '
-                f'feat_edge1:{len(_get_shape(feat_edge1, backend))}dims, feat_edge2:{len(_get_shape(feat_edge2, backend))}dims. '
-                f'Read the doc for more details!')
-
-        if not (_get_shape(feat_node1, backend)[0] == _get_shape(feat_node2, backend)[0] == _get_shape(A1, backend)[0] ==
-                _get_shape(A2, backend)[0] == _get_shape(feat_edge1, backend)[0] == _get_shape(feat_edge2, backend)[0])\
-                or not (_get_shape(feat_node1, backend)[1] == _get_shape(A1, backend)[1] == _get_shape(A1, backend)[2] ==
-                        _get_shape(feat_edge1, backend)[1] == _get_shape(feat_edge1, backend)[2])\
-                or not (_get_shape(feat_node2, backend)[1] == _get_shape(A2, backend)[1] == _get_shape(A2, backend)[2] ==
-                        _get_shape(feat_edge2, backend)[1] == _get_shape(feat_edge2, backend)[2])\
-                or not (_get_shape(feat_node1, backend)[2] == _get_shape(feat_node2, backend)[2])\
-                or not (_get_shape(feat_edge1, backend)[3] == _get_shape(feat_edge2, backend)[3]):
-            raise ValueError(
-                f'the input dimensions do not match. Got feat_node1:{_get_shape(feat_node1, backend)}, '
-                f'feat_node2:{_get_shape(feat_node2, backend)}, A1:{_get_shape(A1, backend)}, A2:{_get_shape(A2, backend)},'
-                f'feat_edge1:{_get_shape(feat_edge1, backend)}, feat_edge2:{_get_shape(feat_edge2, backend)}!')
-    if n1 is not None: _check_data_type(n1, 'n1', backend)
-    if n2 is not None: _check_data_type(n2, 'n2', backend)
-
-    args = (feat_node1, feat_node2, A1, A2, feat_edge1, feat_edge2, n1, n2,
-            in_node_channel, in_edge_channel, hidden_channel, out_channel, num_layers,
-            sk_max_iter, sk_tau, network, pretrain)
-    try:
-        mod = importlib.import_module(f'pygmtools.{backend}_backend')
-        fn = mod.cie
-    except (ModuleNotFoundError, AttributeError):
-        raise NotImplementedError(
-            NOT_IMPLEMENTED_MSG.format(backend)
-        )
-
-    result = fn(*args)
-    match_mat = _squeeze(result[0], 0, backend) if non_batched_input else result[0]
-    if return_network:
-        return match_mat, result[1]
-    else:
-        return match_mat
-
-
-def ngm(K, n1=None, n2=None, n1max=None, n2max=None, x0=None,
-        gnn_channels=(16, 16, 16), sk_emb=1,
-        sk_max_iter=20, sk_tau=0.05,
-        network=None, return_network=False, pretrain='voc',
-        backend=None):
-    r"""
-    The **NGM** (Neural Graph Matching) model for processing the affinity matrix (the most general form of Lawler's QAP).
-    The math form of graph matching (Lawler's QAP) is equivalent to a vertex classification problem on the
-    **association graph**, which is an equivalent formulation based on the affinity matrix :math:`\mathbf{K}`.
-    The graph matching module is composed of several graph convolution layers, Sinkhorn embedding layers and finally
-    a Sinkhorn layer to output a doubly-stochastic matrix.
-
-    See the following pipeline for an example:
-
-    .. image:: ../../images/ngm.png
-
-    See the following paper for more technical details:
-    `"Wang et al. Neural Graph Matching Network: Learning Lawler‚Äôs Quadratic Assignment Problem With Extension to
-    Hypergraph and Multiple-Graph Matching. TPAMI 2022."
-    <https://ieeexplore.ieee.org/abstract/document/9426408/>`_
-
-    :param K: :math:`(b\times n_1n_2 \times n_1n_2)` the input affinity matrix, :math:`b`: batch size.
-    :param n1: :math:`(b)` number of nodes in graph1 (optional if n1max is given, and all n1=n1max).
-    :param n2: :math:`(b)` number of nodes in graph2 (optional if n2max is given, and all n2=n2max).
-    :param n1max: :math:`(b)` max number of nodes in graph1 (optional if n1 is given, and n1max=max(n1)).
-    :param n2max: :math:`(b)` max number of nodes in graph2 (optional if n2 is given, and n2max=max(n2)).
-    :param x0: :math:`(b\times n_1 \times n_2)` an initial matching solution to warm-start the vertex embedding.
-        If not given, the vertex embedding is initialized as a vector of all 1s.
-    :param gnn_channels: (default: ``(16, 16, 16)``) A list/tuple of channel sizes of the GNN.
-        Ignored if the network object is given (ignored if ``network!=None``)
-    :param sk_emb: (default: 1) Number of Sinkhorn embedding channels. Sinkhorn embedding is designed to encode the
-        matching constraints inside GNN layers. How it works: a Sinkhorn embedding channel accepts the vertex feature
-        from the current layer and computes a doubly-stochastic matrix, which is then concatenated to the vertex feature.
-        Ignored if the network object is given (ignored if ``network!=None``)
-    :param sk_max_iter: (default: 20) Max number of iterations of Sinkhorn. See
-        :func:`~pygmtools.classic_solvers.sinkhorn` for more details about this argument.
-    :param sk_tau: (default: 0.05) The temperature parameter of Sinkhorn. See
-        :func:`~pygmtools.classic_solvers.sinkhorn` for more details about this argument.
-    :param network: (default: None) The network object. If None, a new network object will be created, and load the
-        model weights specified in ``pretrain`` argument.
-    :param return_network: (default: False) Return the network object (saving model construction time if calling the
-        model multiple times).
-    :param pretrain: (default: 'voc') If ``network==None``, the pretrained model weights to be loaded. Available
-        pretrained weights: ``voc`` (on Pascal VOC Keypoint dataset), ``willow`` (on Willow Object Class dataset),
-        or ``False`` (no pretraining).
-    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
-    :return: if ``return_network==False``, :math:`(b\times n_1 \times n_2)` the doubly-stochastic matching matrix
-
-        if ``return_network==True``, :math:`(b\times n_1 \times n_2)` the doubly-stochastic matching matrix,
-        the network object
-
-    .. note::
-        You may need a proxy to load the pretrained weights if Google drive is not accessible in your contry/region.
-        You may also download the pretrained models manually and put them at ``~/.cache/pygmtools`` (for Linux).
-
-        `[google drive] <https://drive.google.com/drive/folders/1O7vkIW8QXBJsNsHUIRiSw91HJ_0FAzu_?usp=sharing>`_
-        `[baidu drive] <https://pan.baidu.com/s/1MvzfM52NJeLWx2JXbbc6HA?pwd=x8bv>`_
-
-    .. note::
-        This function also supports non-batched input, by ignoring all batch dimensions in the input tensors.
-
-    .. dropdown:: Numpy Example
-
-        ::
-
-            >>> import numpy as np
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'numpy'
-            >>> np.random.seed(1)
-
-            # Generate a batch of isomorphic graphs
-            >>> batch_size = 10
-            >>> X_gt = np.zeros((batch_size, 4, 4))
-            >>> X_gt[:, np.arange(0, 4, dtype='i4'), np.random.permutation(4)] = 1
-            >>> A1 = np.random.rand(batch_size, 4, 4)
-            >>> A2 = np.matmul(np.matmul(X_gt.swapaxes(1, 2), A1), X_gt)
-            >>> n1 = n2 = np.array([4] * batch_size)
-
-            # Build affinity matrix
-            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
-            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
-            >>> import functools
-            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
-            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
-
-            # Solve by NGM
-            >>> X, net = pygm.ngm(K, n1, n2, return_network=True)
-            Downloading to ~/.cache/pygmtools/ngm_voc_numpy.npy...
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
-            1.0
-
-            # Pass the net object to avoid rebuilding the model agian
-            >>> X = pygm.ngm(K, n1, n2, network=net)
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
-            1.0
-
-            # You may also load other pretrained weights
-            >>> X, net = pygm.ngm(feat1, feat2, A1, A2, e_feat1, e_feat2, n1, n2, return_network=True, pretrain='willow')
-            Downloading to ~/.cache/pygmtools/ngm_willow_numpy.npy...
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
-            1.0
-
-            # You may configure your own model and integrate the model into a deep learning pipeline. For example:
-            >>> net = pygm.utils.get_network(pygm.ngm, gnn_channels=(32, 64, 128, 64, 32), sk_emb=8, pretrain=False)
-            # K may be outputs by other neural networks (constructed K from node/edge features by pygm.utils.build_aff_mat)
-            >>> X = pygm.ngm(K, n1, n2, network=net)
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
-            1.0
-
-
-    .. dropdown:: PyTorch Example
-
-        ::
-
-            >>> import torch
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'pytorch'
-            >>> _ = torch.manual_seed(1)
-
-            # Generate a batch of isomorphic graphs
-            >>> batch_size = 10
-            >>> X_gt = torch.zeros(batch_size, 4, 4)
-            >>> X_gt[:, torch.arange(0, 4, dtype=torch.int64), torch.randperm(4)] = 1
-            >>> A1 = torch.rand(batch_size, 4, 4)
-            >>> A2 = torch.bmm(torch.bmm(X_gt.transpose(1, 2), A1), X_gt)
-            >>> n1 = n2 = torch.tensor([4] * batch_size)
-
-            # Build affinity matrix
-            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
-            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
-            >>> import functools
-            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
-            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
-
-            # Solve by NGM
-            >>> X, net = pygm.ngm(K, n1, n2, return_network=True)
-            Downloading to ~/.cache/pygmtools/ngm_voc_pytorch.pt...
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
-            tensor(1.)
-
-            # Pass the net object to avoid rebuilding the model agian
-            >>> X = pygm.ngm(K, n1, n2, network=net)
-
-            # You may also load other pretrained weights
-            >>> X, net = pygm.ngm(K, n1, n2, return_network=True, pretrain='willow')
-            Downloading to ~/.cache/pygmtools/ngm_willow_pytorch.pt...
-
-            # You may configure your own model and integrate the model into a deep learning pipeline. For example:
-            >>> net = pygm.utils.get_network(pygm.ngm, gnn_channels=(32, 64, 128, 64, 32), sk_emb=8, pretrain=False)
-            >>> optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
-            # K may be outputs by other neural networks (constructed K from node/edge features by pygm.utils.build_aff_mat)
-            >>> X = pygm.ngm(K, n1, n2, network=net)
-            >>> loss = pygm.utils.permutation_loss(X, X_gt)
-            >>> loss.backward()
-            >>> optimizer.step()
-
-    .. dropdown:: Jittor Example
-
-        ::
-
-            >>> import jittor as jt
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'jittor'
-            >>> _ = jt.seed(1)
-
-            # Generate a batch of isomorphic graphs
-            >>> batch_size = 10
-            >>> X_gt = jt.zeros((batch_size, 4, 4))
-            >>> X_gt[:, jt.arange(0, 4, dtype=jt.int64), jt.randperm(4)] = 1
-            >>> A1 = jt.rand(batch_size, 4, 4)
-            >>> A2 = jt.bmm(jt.bmm(X_gt.transpose(1, 2), A1), X_gt)
-            >>> n1 = n2 = jt.Var([4] * batch_size)
-
-            # Build affinity matrix
-            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
-            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
-            import functools
-            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
-            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
-
-            # Solve by NGM
-            >>> X, net = pygm.ngm(K, n1, n2, return_network=True)
-            # Downloading to ~/.cache/pygmtools/ngm_voc_jittor.pt...
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
-            jt.Var([1.], dtype=float32)
-
-            # Pass the net object to avoid rebuilding the model agian
-            >>> X = pygm.ngm(K, n1, n2, network=net)
-
-            # You may also load other pretrained weights
-            >>> X, net = pygm.ngm(K, n1, n2, return_network=True, pretrain='willow')
-            # Downloading to ~/.cache/pygmtools/ngm_willow_jittor.pt...
-
-            # You may configure your own model and integrate the model into a deep learning pipeline. For example:
-            >>> net = pygm.utils.get_network(pygm.ngm, gnn_channels=(32, 64, 128, 64, 32), sk_emb=8, pretrain=False)
-            >>> optimizer = jt.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
-            # K may be outputs by other neural networks (constructed K from node/edge features by pygm.utils.build_aff_mat)
-            >>> X = pygm.ngm(K, n1, n2, network=net)
-            >>> loss = pygm.utils.permutation_loss(X, X_gt)
-            >>> optimizer.backward(loss)
-            >>> optimizer.step()        
-
-    
-    .. dropdown:: Paddle Example
-
-        ::
-
-            >>> import paddle
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'paddle'
-            >>> _ = paddle.seed(1)
-
-            # Generate a batch of isomorphic graphs
-            >>> batch_size = 10
-            >>> X_gt = paddle.zeros((batch_size, 4, 4))
-            >>> X_gt[:, paddle.arange(0, 4, dtype=paddle.int64), paddle.randperm(4)] = 1
-            >>> A1 = paddle.rand((batch_size, 4, 4))
-            >>> A2 = paddle.bmm(paddle.bmm(X_gt.transpose((0, 2, 1)), A1), X_gt)
-            >>> n1 = n2 = paddle.to_tensor([4] * batch_size)
-
-            # Build affinity matrix
-            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
-            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
-            >>> import functools
-            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
-            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
-
-            # Solve by NGM
-            >>> X, net = pygm.ngm(K, n1, n2, return_network=True)
-            Downloading to ~/.cache/pygmtools/ngm_voc_paddle.pdparams...
-            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
-            Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True,
-                   [1.])
-
-            # Pass the net object to avoid rebuilding the model agian
-            >>> X = pygm.ngm(K, n1, n2, network=net)
-
-            # You may also load other pretrained weights
-            >>> X, net = pygm.ngm(K, n1, n2, return_network=True, pretrain='willow')
-            Downloading to ~/.cache/pygmtools/ngm_willow_paddle.pdparams...
-
-            # You may configure your own model and integrate the model into a deep learning pipeline. For example:
-            >>> net = pygm.utils.get_network(pygm.ngm, gnn_channels=(32, 64, 128, 64, 32), sk_emb=8, pretrain=False)
-            >>> optimizer = paddle.optimizer.SGD(parameters=net.parameters(), learning_rate=0.001)
-            # K may be outputs by other neural networks (constructed K from node/edge features by pygm.utils.build_aff_mat)
-            >>> X = pygm.ngm(K, n1, n2, network=net)
-            >>> loss = pygm.utils.permutation_loss(X, X_gt)
-            >>> loss.backward()
-            >>> optimizer.step()
-    
-    .. note::
-
-        If you find this model useful in your research, please cite:
-
-        ::
-
-            @ARTICLE{WangPAMI22,
-              author={Wang, Runzhong and Yan, Junchi and Yang, Xiaokang},
-              journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
-              title={Neural Graph Matching Network: Learning Lawler‚Äôs Quadratic Assignment Problem With Extension to Hypergraph and Multiple-Graph Matching},
-              year={2022},
-              volume={44},
-              number={9},
-              pages={5261-5279},
-              doi={10.1109/TPAMI.2021.3078053}
-            }
-    """
-    if not len(gnn_channels) >= 1: raise ValueError(f'gnn_channels should not be empty!')
-    if not sk_emb >= 0: raise ValueError(f'sk_emb must be >=0. Got sk_emb={sk_emb}!')
-
-    if backend is None:
-        backend = pygmtools.BACKEND
-    non_batched_input = False
-    if K is not None: # if K is None, this function skips the forward pass and only returns a network object
-        _check_data_type(K, 'K', backend)
-        if _check_shape(K, 2, backend):
-            K = _unsqueeze(K, 0, backend)
-            non_batched_input = True
-            if type(n1) is int and n1max is None:
-                n1max = n1
-                n1 = None
-            if type(n2) is int and n2max is None:
-                n2max = n2
-                n2 = None
-        elif _check_shape(K, 3, backend):
-            non_batched_input = False
-        else:
-            raise ValueError(f'the input argument K is expected to be 2-dimensional or 3-dimensional, got '
-                             f'K:{len(_get_shape(K, backend))}dims!')
-        __check_gm_arguments(n1, n2, n1max, n2max)
-
-    args = (K, n1, n2, n1max, n2max, x0, gnn_channels, sk_emb, sk_max_iter, sk_tau, network, return_network, pretrain)
-    try:
-        mod = importlib.import_module(f'pygmtools.{backend}_backend')
-        fn = mod.ngm
-    except (ModuleNotFoundError, AttributeError):
-        raise NotImplementedError(
-            NOT_IMPLEMENTED_MSG.format(backend)
-        )
-    result = fn(*args)
-    match_mat = _squeeze(result[0], 0, backend) if non_batched_input else result[0]
-    if return_network:
-        return match_mat, result[1]
-    else:
-        return match_mat
+"""
+**Neural network-based** graph matching solvers. It is recommended to integrate these networks as modules into your
+existing deep learning pipeline (either supervised, unsupervised or reinforcement learning).
+"""
+
+# Copyright (c) 2022 Thinklab@SJTU
+# pygmtools is licensed under Mulan PSL v2.
+# You can use this software according to the terms and conditions of the Mulan PSL v2.
+# You may obtain a copy of Mulan PSL v2 at:
+# http://license.coscl.org.cn/MulanPSL2
+# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND,
+# EITHER EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT,
+# MERCHANTABILITY OR FIT FOR A PARTICULAR PURPOSE.
+# See the Mulan PSL v2 for more details.
+
+import importlib
+import pygmtools
+import numpy as np
+from pygmtools.utils import NOT_IMPLEMENTED_MSG, from_numpy, \
+    _check_shape, _get_shape, _unsqueeze, _squeeze, _check_data_type
+from pygmtools.classic_solvers import __check_gm_arguments
+
+
+def pca_gm(feat1, feat2, A1, A2, n1=None, n2=None,
+           in_channel=1024, hidden_channel=2048, out_channel=2048, num_layers=2, sk_max_iter=20, sk_tau=0.05,
+           network=None, return_network=False, pretrain='voc',
+           backend=None):
+    r"""
+    The **PCA-GM** (Permutation loss and Cross-graph Affinity Graph Matching) neural network model for processing two
+    individual graphs (KB-QAP).
+    The graph matching module is composed of several intra-graph embedding layers, a cross-graph embedding layer, and
+    a Sinkhorn matching layer. Only the second last layer has a cross-graph update layer.
+
+    See the following pipeline for an example, with application to visual graph matching (layers in the gray box
+    + Affinity Metric + Sinkhorn are implemented by pygmtools):
+
+    .. image:: ../../images/pca_gm.png
+
+    See the following paper for more technical details:
+    `"Wang et al. Combinatorial Learning of Robust Deep Graph Matching: an Embedding based Approach. TPAMI 2020."
+    <https://ieeexplore.ieee.org/abstract/document/9128045/>`_
+
+    You may be also interested in the extended version IPCA-GM (see :func:`~pygmtools.neural_solvers.ipca_gm`).
+
+    :param feat1: :math:`(b\times n_1 \times d)` input feature of graph1
+    :param feat2: :math:`(b\times n_2 \times d)` input feature of graph2
+    :param A1: :math:`(b\times n_1 \times n_1)` input adjacency matrix of graph1
+    :param A2: :math:`(b\times n_2 \times n_2)` input adjacency matrix of graph2
+    :param n1: :math:`(b)` number of nodes in graph1. Optional if all equal to :math:`n_1`
+    :param n2: :math:`(b)` number of nodes in graph2. Optional if all equal to :math:`n_2`
+    :param in_channel: (default: 1024) Channel size of the input layer. It must match the feature dimension :math:`(d)`
+        of ``feat1, feat2``. Ignored if the network object is given (ignored if ``network!=None``)
+    :param hidden_channel: (default: 2048) Channel size of hidden layers. Ignored if the network object is given
+        (ignored if ``network!=None``)
+    :param out_channel: (default: 2048) Channel size of the output layer. Ignored if the network object is given
+        (ignored if ``network!=None``)
+    :param num_layers: (default: 2) Number of graph embedding layers. Must be >=2. Ignored if the network object is
+        given (ignored if ``network!=None``)
+    :param sk_max_iter: (default: 20) Max number of iterations of Sinkhorn. See
+        :func:`~pygmtools.classic_solvers.sinkhorn` for more details about this argument.
+    :param sk_tau: (default: 0.05) The temperature parameter of Sinkhorn. See
+        :func:`~pygmtools.classic_solvers.sinkhorn` for more details about this argument.
+    :param network: (default: None) The network object. If None, a new network object will be created, and load the
+        model weights specified in ``pretrain`` argument.
+    :param return_network: (default: False) Return the network object (saving model construction time if calling the
+        model multiple times).
+    :param pretrain: (default: 'voc') If ``network==None``, the pretrained model weights to be loaded. Available
+        pretrained weights: ``voc`` (on Pascal VOC Keypoint dataset), ``willow`` (on Willow Object Class dataset),
+        ``voc-all`` (on Pascal VOC Keypoint dataset, without filtering), or ``False`` (no pretraining).
+    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
+    :return: if ``return_network==False``, :math:`(b\times n_1 \times n_2)` the doubly-stochastic matching matrix
+
+        if ``return_network==True``, :math:`(b\times n_1 \times n_2)` the doubly-stochastic matching matrix,
+        the network object
+
+    .. note::
+        You may need a proxy to load the pretrained weights if Google drive is not accessible in your contry/region.
+        You may also download the pretrained models manually and put them at ``~/.cache/pygmtools`` (for Linux).
+
+        `[google drive] <https://drive.google.com/drive/folders/1O7vkIW8QXBJsNsHUIRiSw91HJ_0FAzu_?usp=sharing>`_
+        `[baidu drive] <https://pan.baidu.com/s/1MvzfM52NJeLWx2JXbbc6HA?pwd=x8bv>`_
+
+    .. note::
+        This function also supports non-batched input, by ignoring all batch dimensions in the input tensors.
+
+    .. dropdown:: Numpy Example
+
+        ::
+
+            >>> import numpy as np
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'numpy'
+            >>> np.random.seed(1)
+
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> X_gt = np.zeros((batch_size, 4, 4))
+            >>> X_gt[:, np.arange(0, 4, dtype='i4'), np.random.permutation(4)] = 1
+            >>> A1 = 1. * (np.random.rand(batch_size, 4, 4) > 0.5)
+            >>> for i in np.arange(4): # discard self-loop edges
+            ...    for j in np.arange(batch_size):
+            ...        A1[j][i][i] = 0
+            >>> A2 = np.matmul(np.matmul(X_gt.swapaxes(1, 2), A1), X_gt)
+            >>> feat1 = np.random.rand(batch_size, 4, 1024) - 0.5
+            >>> feat2 = np.matmul(X_gt.swapaxes(1, 2), feat1)
+            >>> n1 = n2 = np.array([4] * batch_size)
+
+            # Match by PCA-GM (load pretrained model)
+            >>> X, net = pygm.pca_gm(feat1, feat2, A1, A2, n1, n2, return_network=True)
+            Downloading to ~/.cache/pygmtools/pca_gm_voc_numpy.npy...
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
+            1.0
+
+            # Pass the net object to avoid rebuilding the model agian
+            >>> X = pygm.pca_gm(feat1, feat2, A1, A2, n1, n2, network=net)
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
+            1.0
+
+            # You may also load other pretrained weights
+            >>> X, net = pygm.pca_gm(feat1, feat2, A1, A2, n1, n2, return_network=True, pretrain='willow')
+            Downloading to ~/.cache/pygmtools/pca_gm_willow_numpy.npy...
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
+            1.0
+
+            # You may configure your own model and integrate the model into a deep learning pipeline. For example:
+            >>> net = pygm.utils.get_network(pygm.pca_gm, in_channel=1024, hidden_channel=2048, out_channel=512, num_layers=3, pretrain=False)
+            # feat1/feat2 may be outputs by other neural networks
+            >>> X = pygm.pca_gm(feat1, feat2, A1, A2, n1, n2, network=net)
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
+            1.0
+
+
+    .. dropdown:: PyTorch Example
+
+        ::
+
+            >>> import torch
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'pytorch'
+            >>> _ = torch.manual_seed(1)
+
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> X_gt = torch.zeros(batch_size, 4, 4)
+            >>> X_gt[:, torch.arange(0, 4, dtype=torch.int64), torch.randperm(4)] = 1
+            >>> A1 = 1. * (torch.rand(batch_size, 4, 4) > 0.5)
+            >>> torch.diagonal(A1, dim1=1, dim2=2)[:] = 0 # discard self-loop edges
+            >>> A2 = torch.bmm(torch.bmm(X_gt.transpose(1, 2), A1), X_gt)
+            >>> feat1 = torch.rand(batch_size, 4, 1024) - 0.5
+            >>> feat2 = torch.bmm(X_gt.transpose(1, 2), feat1)
+            >>> n1 = n2 = torch.tensor([4] * batch_size)
+
+            # Match by PCA-GM (load pretrained model)
+            >>> X, net = pygm.pca_gm(feat1, feat2, A1, A2, n1, n2, return_network=True)
+            Downloading to ~/.cache/pygmtools/pca_gm_voc_pytorch.pt...
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
+            tensor(1.)
+
+            # Pass the net object to avoid rebuilding the model agian
+            >>> X = pygm.pca_gm(feat1, feat2, A1, A2, n1, n2, network=net)
+
+            # You may also load other pretrained weights
+            >>> X, net = pygm.pca_gm(feat1, feat2, A1, A2, n1, n2, return_network=True, pretrain='willow')
+            Downloading to ~/.cache/pygmtools/pca_gm_willow_pytorch.pt...
+
+            # You may configure your own model and integrate the model into a deep learning pipeline. For example:
+            >>> net = pygm.utils.get_network(pygm.pca_gm, in_channel=1024, hidden_channel=2048, out_channel=512, num_layers=3, pretrain=False)
+            >>> optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
+            # feat1/feat2 may be outputs by other neural networks
+            >>> X = pygm.pca_gm(feat1, feat2, A1, A2, n1, n2, network=net)
+            >>> loss = pygm.utils.permutation_loss(X, X_gt)
+            >>> loss.backward()
+            >>> optimizer.step()
+
+        
+    .. dropdown:: Jittor Example
+
+        ::
+
+            >>> import jittor as jt
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'jittor'
+            >>> _ = jt.seed(1)
+
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> X_gt = jt.zeros((batch_size, 4, 4))
+            >>> X_gt[:, jt.arange(0, 4, dtype=jt.int64), jt.randperm(4)] = 1
+            >>> A1 = 1. * (jt.rand(batch_size, 4, 4) > 0.5)
+            >>> for i in range(batch_size):
+            >>>     for j in range(4):
+            >>>         A1.data[i][j][j] = 0  # discard self-loop edges
+            >>> A2 = jt.bmm(jt.bmm(X_gt.transpose(1, 2), A1), X_gt)
+            >>> feat1 = jt.rand(batch_size, 4, 1024) - 0.5
+            >>> feat2 = jt.bmm(X_gt.transpose(1, 2), feat1)
+            >>> n1 = n2 = jt.Var([4] * batch_size)
+
+            # Match by PCA-GM (load pretrained model)
+            >>> X, net = pygm.pca_gm(feat1, feat2, A1, A2, n1, n2, return_network=True)
+            Downloading to ~/.cache/pygmtools/pca_gm_voc_jittor.pt...
+
+            # Pass the net object to avoid rebuilding the model agian
+            >>> X = pygm.pca_gm(feat1, feat2, A1, A2, n1, n2, network=net)
+
+            # You may also load other pretrained weights
+            >>> X, net = pygm.pca_gm(feat1, feat2, A1, A2, n1, n2, return_network=True, pretrain='willow')
+            Downloading to ~/.cache/pygmtools/pca_gm_willow_jittor.pt...
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
+            jt.Var([1.], dtype=float32)
+
+            # You may configure your own model and integrate the model into a deep learning pipeline. For example:
+            >>> net = pygm.utils.get_network(pygm.pca_gm, in_channel=1024, hidden_channel=2048, out_channel=512, num_layers=3, pretrain=False)
+            >>> optimizer = jt.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
+            # feat1/feat2 may be outputs by other neural networks
+            >>> X = pygm.pca_gm(feat1, feat2, A1, A2, n1, n2, network=net)
+            >>> loss = pygm.utils.permutation_loss(X, X_gt)
+            >>> optimizer.backward(loss)
+            >>> optimizer.step()
+
+            
+    .. dropdown:: Paddle Example
+
+        ::
+
+            >>> import paddle
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'paddle'
+            >>> _ = paddle.seed(4)
+
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> X_gt = paddle.zeros((batch_size, 4, 4))
+            >>> X_gt[:, paddle.arange(0, 4, dtype=paddle.int64), paddle.randperm(4)] = 1
+            >>> A1 = 1. * (paddle.rand((batch_size, 4, 4)) > 0.5)
+            >>> paddle.diagonal(A1, axis1=1, axis2=2)[:] = 0 # discard self-loop edges
+            >>> A2 = paddle.bmm(paddle.bmm(X_gt.transpose((0, 2, 1)), A1), X_gt)
+            >>> feat1 = paddle.rand((batch_size, 4, 1024)) - 0.5
+            >>> feat2 = paddle.bmm(X_gt.transpose((0, 2, 1)), feat1)
+            >>> n1 = n2 = paddle.to_tensor([4] * batch_size)
+
+            # Match by PCA-GM (load pretrained model)
+            >>> X, net = pygm.pca_gm(feat1, feat2, A1, A2, n1, n2, return_network=True)
+            Downloading to ~/.cache/pygmtools/pca_gm_voc_paddle.pdparams...
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
+            Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True,
+                [1.])
+
+            # Pass the net object to avoid rebuilding the model agian
+            >>> X = pygm.pca_gm(feat1, feat2, A1, A2, n1, n2, network=net)
+
+            # You may also load other pretrained weights
+            >>> X, net = pygm.pca_gm(feat1, feat2, A1, A2, n1, n2, return_network=True, pretrain='willow')
+            Downloading to ~/.cache/pygmtools/pca_gm_willow_paddle.pdparams...
+
+            # You may configure your own model and integrate the model into a deep learning pipeline. For example:
+            >>> net = pygm.utils.get_network(pygm.pca_gm, in_channel=1024, hidden_channel=2048, out_channel=512, num_layers=3, pretrain=False)
+            >>> optimizer = paddle.optimizer.SGD(parameters=net.parameters(), learning_rate=0.001)
+            # feat1/feat2 may be outputs by other neural networks
+            >>> X = pygm.pca_gm(feat1, feat2, A1, A2, n1, n2, network=net)
+            >>> loss = pygm.utils.permutation_loss(X, X_gt)
+            >>> loss.backward()
+            >>> optimizer.step()
+
+    .. note::
+
+        If you find this model useful in your research, please cite:
+
+        ::
+
+            @article{WangPAMI20,
+              author = {Wang, Runzhong and Yan, Junchi and Yang, Xiaokang},
+              title = {Combinatorial Learning of Robust Deep Graph Matching: an Embedding based Approach},
+              journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
+              year = {2020}
+            }
+    """
+    if not num_layers >= 2: raise ValueError(f'num_layers must be >=2, got {num_layers}!')
+
+    if backend is None:
+        backend = pygmtools.BACKEND
+    non_batched_input = False
+    if feat1 is not None: # if feat1 is None, this function skips the forward pass and only returns a network object
+        for _ in (feat1, feat2, A1, A2):
+            _check_data_type(_, backend)
+
+        if all([_check_shape(_, 2, backend) for _ in (feat1, feat2, A1, A2)]):
+            feat1, feat2, A1, A2 = [_unsqueeze(_, 0, backend) for _ in (feat1, feat2, A1, A2)]
+            if type(n1) is int: n1 = from_numpy(np.array([n1]), backend=backend)
+            if type(n2) is int: n2 = from_numpy(np.array([n2]), backend=backend)
+            non_batched_input = True
+        elif all([_check_shape(_, 3, backend) for _ in (feat1, feat2, A1, A2)]):
+            non_batched_input = False
+        else:
+            raise ValueError(
+                f'the input arguments feat1, feat2, A1, A2 are expected to be all 2-dimensional or 3-dimensional, got '
+                f'feat1:{len(_get_shape(feat1, backend))}dims, feat2:{len(_get_shape(feat2, backend))}dims, '
+                f'A1:{len(_get_shape(A1, backend))}dims, A2:{len(_get_shape(A2, backend))}dims!')
+
+        if not (_get_shape(feat1, backend)[0] == _get_shape(feat2, backend)[0] == _get_shape(A1, backend)[0] == _get_shape(A2, backend)[0])\
+                or not (_get_shape(feat1, backend)[1] == _get_shape(A1, backend)[1] == _get_shape(A1, backend)[2])\
+                or not (_get_shape(feat2, backend)[1] == _get_shape(A2, backend)[1] == _get_shape(A2, backend)[2])\
+                or not (_get_shape(feat1, backend)[2] == _get_shape(feat2, backend)[2]):
+            raise ValueError(
+                f'the input dimensions do not match. Got feat1:{_get_shape(feat1, backend)}, '
+                f'feat2:{_get_shape(feat2, backend)}, A1:{_get_shape(A1, backend)}, A2:{_get_shape(A2, backend)}!')
+    if n1 is not None: _check_data_type(n1, 'n1', backend)
+    if n2 is not None: _check_data_type(n2, 'n2', backend)
+
+    args = (feat1, feat2, A1, A2, n1, n2, in_channel, hidden_channel, out_channel, num_layers, sk_max_iter, sk_tau,
+           network, pretrain)
+    try:
+        mod = importlib.import_module(f'pygmtools.{backend}_backend')
+        fn = mod.pca_gm
+    except (ModuleNotFoundError, AttributeError):
+        raise NotImplementedError(
+            NOT_IMPLEMENTED_MSG.format(backend)
+        )
+
+    result = fn(*args)
+    match_mat = _squeeze(result[0], 0, backend) if non_batched_input else result[0]
+    if return_network:
+        return match_mat, result[1]
+    else:
+        return match_mat
+
+
+def ipca_gm(feat1, feat2, A1, A2, n1=None, n2=None,
+            in_channel=1024, hidden_channel=2048, out_channel=2048, num_layers=2, cross_iter=3,
+            sk_max_iter=20, sk_tau=0.05,
+            network=None, return_network=False, pretrain='voc',
+            backend=None):
+    r"""
+    The **IPCA-GM** (Iterative Permutation loss and Cross-graph Affinity Graph Matching) neural network model for
+    processing two individual graphs (KB-QAP).
+    The graph matching module is composed of several intra-graph embedding layers, a cross-graph embedding layer, and
+    a Sinkhorn matching layer. The weight matrix of the cross-graph embedding layer is updated iteratively.
+    Only the second last layer has a cross-graph update layer.
+    IPCA-GM is the extended version of PCA-GM (see :func:`~pygmtools.neural_solvers.pca_gm`). The dfference is that
+    the cross-graph weight in PCA-GM is computed in one shot, and in IPCA-GM it is updated iteratively.
+
+    See the following pipeline for an example, with application to visual graph matching (layers in gray box are
+    implemented by pygmtools):
+
+    .. image:: ../../images/ipca_gm.png
+
+    See the following paper for more technical details:
+    `"Wang et al. Combinatorial Learning of Robust Deep Graph Matching: an Embedding based Approach. TPAMI 2020."
+    <https://ieeexplore.ieee.org/abstract/document/9128045/>`_
+
+    :param feat1: :math:`(b\times n_1 \times d)` input feature of graph1
+    :param feat2: :math:`(b\times n_2 \times d)` input feature of graph2
+    :param A1: :math:`(b\times n_1 \times n_1)` input adjacency matrix of graph1
+    :param A2: :math:`(b\times n_2 \times n_2)` input adjacency matrix of graph2
+    :param n1: :math:`(b)` number of nodes in graph1. Optional if all equal to :math:`n_1`
+    :param n2: :math:`(b)` number of nodes in graph2. Optional if all equal to :math:`n_2`
+    :param in_channel: (default: 1024) Channel size of the input layer. It must match the feature dimension :math:`(d)`
+        of ``feat1, feat2``. Ignored if the network object is given (ignored if ``network!=None``)
+    :param hidden_channel: (default: 2048) Channel size of hidden layers. Ignored if the network object is given
+        (ignored if ``network!=None``)
+    :param out_channel: (default: 2048) Channel size of the output layer. Ignored if the network object is given
+        (ignored if ``network!=None``)
+    :param num_layers: (default: 2) Number of graph embedding layers. Must be >=2. Ignored if the network object is
+        given (ignored if ``network!=None``)
+    :param cross_iter: (default: 3) Number of iterations for the cross-graph embedding layer.
+    :param sk_max_iter: (default: 20) Max number of iterations of Sinkhorn. See
+        :func:`~pygmtools.classic_solvers.sinkhorn` for more details about this argument.
+    :param sk_tau: (default: 0.05) The temperature parameter of Sinkhorn. See
+        :func:`~pygmtools.classic_solvers.sinkhorn` for more details about this argument.
+    :param network: (default: None) The network object. If None, a new network object will be created, and load the
+        model weights specified in ``pretrain`` argument.
+    :param return_network: (default: False) Return the network object (saving model construction time if calling the
+        model multiple times).
+    :param pretrain: (default: 'voc') If ``network==None``, the pretrained model weights to be loaded. Available
+        pretrained weights: ``voc`` (on Pascal VOC Keypoint dataset), ``willow`` (on Willow Object Class dataset),
+        or ``False`` (no pretraining).
+    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
+    :return: if ``return_network==False``, :math:`(b\times n_1 \times n_2)` the doubly-stochastic matching matrix
+
+        if ``return_network==True``, :math:`(b\times n_1 \times n_2)` the doubly-stochastic matching matrix,
+        the network object
+
+    .. note::
+        You may need a proxy to load the pretrained weights if Google drive is not accessible in your contry/region.
+        You may also download the pretrained models manually and put them at ``~/.cache/pygmtools`` (for Linux).
+
+        `[google drive] <https://drive.google.com/drive/folders/1O7vkIW8QXBJsNsHUIRiSw91HJ_0FAzu_?usp=sharing>`_
+        `[baidu drive] <https://pan.baidu.com/s/1MvzfM52NJeLWx2JXbbc6HA?pwd=x8bv>`_
+
+    .. note::
+        This function also supports non-batched input, by ignoring all batch dimensions in the input tensors.
+
+    .. dropdown:: Numpy Example
+
+        ::
+
+            >>> import numpy as np
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'numpy'
+            >>> np.random.seed(1)
+
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> X_gt = np.zeros((batch_size, 4, 4))
+            >>> X_gt[:, np.arange(0, 4, dtype='i4'), np.random.permutation(4)] = 1
+            >>> A1 = 1. * (np.random.rand(batch_size, 4, 4) > 0.5)
+            >>> for i in np.arange(4): # discard self-loop edges
+            ...    for j in np.arange(batch_size):
+            ...        A1[j][i][i] = 0
+            >>> A2 = np.matmul(np.matmul(X_gt.swapaxes(1, 2), A1), X_gt)
+            >>> feat1 = np.random.rand(batch_size, 4, 1024) - 0.5
+            >>> feat2 = np.matmul(X_gt.swapaxes(1, 2), feat1)
+            >>> n1 = n2 = np.array([4] * batch_size)
+
+            # Match by IPCA-GM (load pretrained model)
+            >>> X, net = pygm.ipca_gm(feat1, feat2, A1, A2, n1, n2, return_network=True)
+            Downloading to ~/.cache/pygmtools/ipca_gm_voc_numpy.npy...
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
+            1.0
+
+            # Pass the net object to avoid rebuilding the model agian
+            >>> X = pygm.ipca_gm(feat1, feat2, A1, A2, n1, n2, network=net)
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
+            1.0
+
+            # You may also load other pretrained weights
+            >>> X, net = pygm.ipca_gm(feat1, feat2, A1, A2, n1, n2, return_network=True, pretrain='willow')
+            Downloading to ~/.cache/pygmtools/ipca_gm_willow_numpy.npy...
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
+            1.0
+
+            # You may configure your own model and integrate the model into a deep learning pipeline. For example:
+            >>> net = pygm.utils.get_network(pygm.ipca_gm, in_channel=1024, hidden_channel=2048, out_channel=512, num_layers=3, cross_iter=10, pretrain=False)
+            # feat1/feat2 may be outputs by other neural networks
+            >>> X = pygm.ipca_gm(feat1, feat2, A1, A2, n1, n2, network=net)
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
+            1.0
+
+
+    .. dropdown:: PyTorch Example
+
+        ::
+
+            >>> import torch
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'pytorch'
+            >>> _ = torch.manual_seed(1)
+
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> X_gt = torch.zeros(batch_size, 4, 4)
+            >>> X_gt[:, torch.arange(0, 4, dtype=torch.int64), torch.randperm(4)] = 1
+            >>> A1 = 1. * (torch.rand(batch_size, 4, 4) > 0.5)
+            >>> torch.diagonal(A1, dim1=1, dim2=2)[:] = 0 # discard self-loop edges
+            >>> A2 = torch.bmm(torch.bmm(X_gt.transpose(1, 2), A1), X_gt)
+            >>> feat1 = torch.rand(batch_size, 4, 1024) - 0.5
+            >>> feat2 = torch.bmm(X_gt.transpose(1, 2), feat1)
+            >>> n1 = n2 = torch.tensor([4] * batch_size)
+
+            # Match by IPCA-GM (load pretrained model)
+            >>> X, net = pygm.ipca_gm(feat1, feat2, A1, A2, n1, n2, return_network=True)
+            Downloading to ~/.cache/pygmtools/ipca_gm_voc_pytorch.pt...
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
+            tensor(1.)
+
+            # Pass the net object to avoid rebuilding the model agian
+            >>> X = pygm.ipca_gm(feat1, feat2, A1, A2, n1, n2, network=net)
+
+            # You may also load other pretrained weights
+            >>> X, net = pygm.ipca_gm(feat1, feat2, A1, A2, n1, n2, return_network=True, pretrain='willow')
+            Downloading to ~/.cache/pygmtools/ipca_gm_willow_pytorch.pt...
+
+            # You may configure your own model and integrate the model into a deep learning pipeline. For example:
+            >>> net = pygm.utils.get_network(pygm.ipca_gm, in_channel=1024, hidden_channel=2048, out_channel=512, num_layers=3, cross_iter=10, pretrain=False)
+            >>> optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
+            # feat1/feat2 may be outputs by other neural networks
+            >>> X = pygm.ipca_gm(feat1, feat2, A1, A2, n1, n2, network=net)
+            >>> loss = pygm.utils.permutation_loss(X, X_gt)
+            >>> loss.backward()
+            >>> optimizer.step()
+
+
+    .. dropdown:: Jittor Example
+
+        ::
+
+            >>> import jittor as jt
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'jittor'
+            >>> _ = jt.seed(1)
+
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> X_gt = jt.zeros((batch_size, 4, 4))
+            >>> X_gt[:, jt.arange(0, 4, dtype=jt.int64), jt.randperm(4)] = 1
+            >>> A1 = 1. * (jt.rand(batch_size, 4, 4) > 0.5)
+            >>> for i in range(batch_size):
+            >>>     for j in range(4):
+            >>>         A1.data[i][j][j] = 0  # discard self-loop edges
+            >>> A2 = jt.bmm(jt.bmm(X_gt.transpose(1, 2), A1), X_gt)
+            >>> feat1 = jt.rand(batch_size, 4, 1024) - 0.5
+            >>> feat2 = jt.bmm(X_gt.transpose(1, 2), feat1)
+            >>> n1 = n2 = jt.Var([4] * batch_size)
+
+            # Match by IPCA-GM (load pretrained model)
+            >>> X, net = pygm.ipca_gm(feat1, feat2, A1, A2, n1, n2, return_network=True)
+            Downloading to ~/.cache/pygmtools/ipca_gm_voc_jitttor.pt...
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
+            jt.Var([1.], dtype=float32)
+
+            # Pass the net object to avoid rebuilding the model agian
+            >>> X = pygm.ipca_gm(feat1, feat2, A1, A2, n1, n2, network=net)
+
+            # You may also load other pretrained weights
+            >>> X, net = pygm.ipca_gm(feat1, feat2, A1, A2, n1, n2, return_network=True, pretrain='willow')
+            Downloading to ~/.ca/che/pygmtools/ipca_gm_willow_jittor.pt...
+
+            # You may configure your own model and integrate the model into a deep learning pipeline. For example:
+            >>> net = pygm.utils.get_network(pygm.ipca_gm, in_channel=1024, hidden_channel=2048, out_channel=512, num_layers=3, cross_iter=10, pretrain=False)
+            >>> optimizer = jt.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
+            # feat1/feat2 may be outputs by other neural networks
+            >>> X = pygm.ipca_gm(feat1, feat2, A1, A2, n1, n2, network=net)
+            >>> loss = pygm.utils.permutation_loss(X, X_gt)
+            >>> optimizer.backward(loss)
+            >>> optimizer.step()
+    
+    
+    .. dropdown:: Paddle Example
+
+        ::
+
+            >>> import paddle
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'paddle'
+            >>> _ = paddle.seed(5)
+
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> X_gt = paddle.zeros((batch_size, 4, 4))
+            >>> X_gt[:, paddle.arange(0, 4, dtype=paddle.int64), paddle.randperm(4)] = 1
+            >>> A1 = 1. * (paddle.rand((batch_size, 4, 4)) > 0.5)
+            >>> paddle.diagonal(A1, axis1=1, axis2=2)[:] = 0 # discard self-loop edges
+            >>> A2 = paddle.bmm(paddle.bmm(X_gt.transpose((0, 2, 1)), A1), X_gt)
+            >>> feat1 = paddle.rand((batch_size, 4, 1024)) - 0.5
+            >>> feat2 = paddle.bmm(X_gt.transpose((0, 2, 1)), feat1)
+            >>> n1 = n2 = paddle.to_tensor([4] * batch_size)
+
+            # Match by IPCA-GM (load pretrained model)
+            >>> X, net = pygm.ipca_gm(feat1, feat2, A1, A2, n1, n2, return_network=True)
+            Downloading to ~/.cache/pygmtools/ipca_gm_voc_paddle.pdparams...
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
+            Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True,
+                    [1.])
+
+            # Pass the net object to avoid rebuilding the model agian
+            >>> X = pygm.ipca_gm(feat1, feat2, A1, A2, n1, n2, network=net)
+
+            # You may also load other pretrained weights
+            >>> X, net = pygm.ipca_gm(feat1, feat2, A1, A2, n1, n2, return_network=True, pretrain='willow')
+            Downloading to ~/.cache/pygmtools/ipca_gm_willow_paddle.pdparams...
+
+            # You may configure your own model and integrate the model into a deep learning pipeline. For example:
+            >>> net = pygm.utils.get_network(pygm.ipca_gm, in_channel=1024, hidden_channel=2048, out_channel=512, num_layers=3, cross_iter=10, pretrain=False)
+            >>> optimizer = paddle.optimizer.SGD(parameters=net.parameters(), learning_rate=0.001)
+            # feat1/feat2 may be outputs by other neural networks
+            >>> X = pygm.ipca_gm(feat1, feat2, A1, A2, n1, n2, network=net)
+            >>> loss = pygm.utils.permutation_loss(X, X_gt)
+            >>> loss.backward()
+            >>> optimizer.step()
+
+    .. note::
+
+        If you find this model useful in your research, please cite:
+
+        ::
+
+            @article{WangPAMI20,
+              author = {Wang, Runzhong and Yan, Junchi and Yang, Xiaokang},
+              title = {Combinatorial Learning of Robust Deep Graph Matching: an Embedding based Approach},
+              journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
+              year = {2020}
+            }
+    """
+    if not num_layers >= 2: raise ValueError(f'num_layers must be >=2, got {num_layers}!')
+    if not cross_iter >= 1: raise ValueError(f'cross_iter must be >=1, got {cross_iter}!')
+
+    if backend is None:
+        backend = pygmtools.BACKEND
+    non_batched_input = False
+    if feat1 is not None:  # if feat1 is None, this function skips the forward pass and only returns a network object
+        _check_data_type(feat1, 'feat1', backend)
+        _check_data_type(feat2, 'feat2', backend)
+        _check_data_type(A1, 'A1', backend)
+        _check_data_type(A2, 'A2', backend)
+
+        if all([_check_shape(_, 2, backend) for _ in (feat1, feat2, A1, A2)]):
+            feat1, feat2, A1, A2 = [_unsqueeze(_, 0, backend) for _ in (feat1, feat2, A1, A2)]
+            if type(n1) is int: n1 = from_numpy(np.array([n1]), backend=backend)
+            if type(n2) is int: n2 = from_numpy(np.array([n2]), backend=backend)
+            non_batched_input = True
+        elif all([_check_shape(_, 3, backend) for _ in (feat1, feat2, A1, A2)]):
+            non_batched_input = False
+        else:
+            raise ValueError(
+                f'the input arguments feat1, feat2, A1, A2 are expected to be all 2-dimensional or 3-dimensional, got '
+                f'feat1:{len(_get_shape(feat1, backend))}dims, feat2:{len(_get_shape(feat2, backend))}dims, '
+                f'A1:{len(_get_shape(A1, backend))}dims, A2:{len(_get_shape(A2, backend))}dims!')
+
+        if not (_get_shape(feat1, backend)[0] == _get_shape(feat2, backend)[0] == _get_shape(A1, backend)[0] == _get_shape(A2, backend)[0])\
+                or not (_get_shape(feat1, backend)[1] == _get_shape(A1, backend)[1] == _get_shape(A1, backend)[2])\
+                or not (_get_shape(feat2, backend)[1] == _get_shape(A2, backend)[1] == _get_shape(A2, backend)[2])\
+                or not (_get_shape(feat1, backend)[2] == _get_shape(feat2, backend)[2]):
+            raise ValueError(
+                f'the input dimensions do not match. Got feat1:{_get_shape(feat1, backend)}, '
+                f'feat2:{_get_shape(feat2, backend)}, A1:{_get_shape(A1, backend)}, A2:{_get_shape(A2, backend)}!')
+    if n1 is not None: _check_data_type(n1, 'n1', backend)
+    if n2 is not None: _check_data_type(n2, 'n2', backend)
+
+    args = (feat1, feat2, A1, A2, n1, n2, in_channel, hidden_channel, out_channel, num_layers, cross_iter,
+            sk_max_iter, sk_tau, network, pretrain)
+    try:
+        mod = importlib.import_module(f'pygmtools.{backend}_backend')
+        fn = mod.ipca_gm
+    except (ModuleNotFoundError, AttributeError):
+        raise NotImplementedError(
+            NOT_IMPLEMENTED_MSG.format(backend)
+        )
+
+    result = fn(*args)
+    match_mat = _squeeze(result[0], 0, backend) if non_batched_input else result[0]
+    if return_network:
+        return match_mat, result[1]
+    else:
+        return match_mat
+
+
+def cie(feat_node1, feat_node2, A1, A2, feat_edge1, feat_edge2, n1=None, n2=None,
+        in_node_channel=1024, in_edge_channel=1, hidden_channel=2048, out_channel=2048, num_layers=2,
+        sk_max_iter=20, sk_tau=0.05,
+        network=None, return_network=False, pretrain='voc',
+        backend=None):
+    r"""
+    The **CIE** (Channel Independent Embedding) graph matching neural network model for processing two individual graphs
+    (KB-QAP).
+    The graph matching module is composed of several intra-graph embedding layers, a cross-graph embedding layer, and
+    a Sinkhorn matching layer. Only the second last layer has a cross-graph update layer. The graph embedding layers
+    are based on channel-independent embedding, under the assumption that such a message passing scheme may offer higher
+    model capacity especially with high-dimensional edge features.
+
+    See the following pipeline for an example, with application to visual graph matching:
+
+    .. image:: ../../images/cie_framework.png
+
+    The graph embedding layer (CIE layer) involves both node embeding and edge embedding:
+
+    .. image:: ../../images/cie_layer.png
+
+    See the following paper for more technical details:
+    `"Yu et al. Learning Deep Graph Matching with Channel-Independent Embedding and Hungarian Attention. ICLR 2020."
+    <https://openreview.net/pdf?id=rJgBd2NYPH>`_
+
+    :param feat_node1: :math:`(b\times n_1 \times d_n)` input node feature of graph1
+    :param feat_node2: :math:`(b\times n_2 \times d_n)` input node feature of graph2
+    :param A1: :math:`(b\times n_1 \times n_1)` input adjacency matrix of graph1
+    :param A2: :math:`(b\times n_2 \times n_2)` input adjacency matrix of graph2
+    :param feat_edge1: :math:`(b\times n_1 \times n_1 \times d_e)` input edge feature of graph1
+    :param feat_edge2: :math:`(b\times n_2 \times n_2 \times d_e)` input edge feature of graph2
+    :param n1: :math:`(b)` number of nodes in graph1. Optional if all equal to :math:`n_1`
+    :param n2: :math:`(b)` number of nodes in graph2. Optional if all equal to :math:`n_2`
+    :param in_node_channel: (default: 1024) Node channel size of the input layer. It must match the feature dimension
+        :math:`(d_n)` of ``feat_node1, feat_node2``. Ignored if the network object is given (ignored if ``network!=None``)
+    :param in_edge_channel: (default: 1) Edge channel size of the input layer. It must match the feature dimension
+        :math:`(d_e)` of ``feat_edge1, feat_edge2``. Ignored if the network object is given (ignored if ``network!=None``)
+    :param hidden_channel: (default: 2048) Channel size of hidden layers (node channel == edge channel).
+        Ignored if the network object is given (ignored if ``network!=None``)
+    :param out_channel: (default: 2048) Channel size of the output layer (node channel == edge channel).
+        Ignored if the network object is given (ignored if ``network!=None``)
+    :param num_layers: (default: 2) Number of graph embedding layers. Must be >=2. Ignored if the network object is
+        given (ignored if ``network!=None``)
+    :param sk_max_iter: (default: 20) Max number of iterations of Sinkhorn. See
+        :func:`~pygmtools.classic_solvers.sinkhorn` for more details about this argument.
+    :param sk_tau: (default: 0.05) The temperature parameter of Sinkhorn. See
+        :func:`~pygmtools.classic_solvers.sinkhorn` for more details about this argument.
+    :param network: (default: None) The network object. If None, a new network object will be created, and load the
+        model weights specified in ``pretrain`` argument.
+    :param return_network: (default: False) Return the network object (saving model construction time if calling the
+        model multiple times).
+    :param pretrain: (default: 'voc') If ``network==None``, the pretrained model weights to be loaded. Available
+        pretrained weights: ``voc`` (on Pascal VOC Keypoint dataset), ``willow`` (on Willow Object Class dataset),
+        or ``False`` (no pretraining).
+    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
+    :return: if ``return_network==False``, :math:`(b\times n_1 \times n_2)` the doubly-stochastic matching matrix
+
+        if ``return_network==True``, :math:`(b\times n_1 \times n_2)` the doubly-stochastic matching matrix,
+        the network object
+
+    .. note::
+        You may need a proxy to load the pretrained weights if Google drive is not accessible in your contry/region.
+        You may also download the pretrained models manually and put them at ``~/.cache/pygmtools`` (for Linux).
+
+        `[google drive] <https://drive.google.com/drive/folders/1O7vkIW8QXBJsNsHUIRiSw91HJ_0FAzu_?usp=sharing>`_
+        `[baidu drive] <https://pan.baidu.com/s/1MvzfM52NJeLWx2JXbbc6HA?pwd=x8bv>`_
+
+    .. note::
+        This function also supports non-batched input, by ignoring all batch dimensions in the input tensors.
+
+    .. dropdown:: Numpy Example
+
+        ::
+
+            >>> import numpy as np
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'numpy'
+            >>> np.random.seed(1)
+
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> X_gt = np.zeros((batch_size, 4, 4))
+            >>> X_gt[:, np.arange(0, 4, dtype='i4'), np.random.permutation(4)] = 1
+            >>> A1 = 1. * (np.random.rand(batch_size, 4, 4) > 0.5)
+            >>> for i in np.arange(4): # discard self-loop edges
+            ...    for j in np.arange(batch_size):
+            ...        A1[j][i][i] = 0
+            >>> e_feat1 = np.expand_dims(np.random.rand(batch_size, 4, 4) * A1,axis=-1) # shape: (10, 4, 4, 1)
+            >>> A2 = np.matmul(np.matmul(X_gt.swapaxes(1, 2), A1), X_gt)
+            >>> e_feat2 = np.expand_dims(np.matmul(np.matmul(X_gt.swapaxes(1, 2),np.squeeze(e_feat1,axis=-1)), X_gt),axis=-1)
+            >>> feat1 = np.random.rand(batch_size, 4, 1024) - 0.5
+            >>> feat2 = np.matmul(X_gt.swapaxes(1, 2), feat1)
+            >>> n1 = n2 = np.array([4] * batch_size)
+
+            # Match by CIE (load pretrained model)
+            >>> X, net = pygm.cie(feat1, feat2, A1, A2, e_feat1, e_feat2, n1, n2, return_network=True)
+            Downloading to ~/.cache/pygmtools/cie_voc_numpy.npy...
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
+            1.0
+
+            # Pass the net object to avoid rebuilding the model agian
+            >>> X = pygm.cie(feat1, feat2, A1, A2, e_feat1, e_feat2, n1, n2, network=net)
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
+            1.0
+
+            # You may also load other pretrained weights
+            >>> X, net = pygm.cie(feat1, feat2, A1, A2, e_feat1, e_feat2, n1, n2, return_network=True, pretrain='willow')
+            Downloading to ~/.cache/pygmtools/cie_willow_numpy.npy...
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
+            1.0
+
+            # You may configure your own model and integrate the model into a deep learning pipeline. For example:
+            >>> net = pygm.utils.get_network(pygm.cie, in_node_channel=1024, in_edge_channel=1, hidden_channel=2048, out_channel=512, num_layers=3, pretrain=False)
+            # feat1/feat2/e_feat1/e_feat2 may be outputs by other neural networks
+            >>> X = pygm.cie(feat1, feat2, A1, A2, e_feat1, e_feat2, n1, n2, network=net)
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
+            1.0
+            
+
+    .. dropdown:: PyTorch Example
+
+        ::
+
+            >>> import torch
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'pytorch'
+            >>> _ = torch.manual_seed(1)
+
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> X_gt = torch.zeros(batch_size, 4, 4)
+            >>> X_gt[:, torch.arange(0, 4, dtype=torch.int64), torch.randperm(4)] = 1
+            >>> A1 = 1. * (torch.rand(batch_size, 4, 4) > 0.5)
+            >>> torch.diagonal(A1, dim1=1, dim2=2)[:] = 0 # discard self-loop edges
+            >>> e_feat1 = (torch.rand(batch_size, 4, 4) * A1).unsqueeze(-1) # shape: (10, 4, 4, 1)
+            >>> A2 = torch.bmm(torch.bmm(X_gt.transpose(1, 2), A1), X_gt)
+            >>> e_feat2 = torch.bmm(torch.bmm(X_gt.transpose(1, 2), e_feat1.squeeze(-1)), X_gt).unsqueeze(-1)
+            >>> feat1 = torch.rand(batch_size, 4, 1024) - 0.5
+            >>> feat2 = torch.bmm(X_gt.transpose(1, 2), feat1)
+            >>> n1 = n2 = torch.tensor([4] * batch_size)
+
+            # Match by CIE (load pretrained model)
+            >>> X, net = pygm.cie(feat1, feat2, A1, A2, e_feat1, e_feat2, n1, n2, return_network=True)
+            Downloading to ~/.cache/pygmtools/cie_voc_pytorch.pt...
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
+            tensor(1.)
+
+            # Pass the net object to avoid rebuilding the model agian
+            >>> X = pygm.cie(feat1, feat2, A1, A2, e_feat1, e_feat2, n1, n2, network=net)
+
+            # You may also load other pretrained weights
+            >>> X, net = pygm.cie(feat1, feat2, A1, A2, e_feat1, e_feat2, n1, n2, return_network=True, pretrain='willow')
+            Downloading to ~/.cache/pygmtools/cie_willow_pytorch.pt...
+
+            # You may configure your own model and integrate the model into a deep learning pipeline. For example:
+            >>> net = pygm.utils.get_network(pygm.cie, in_node_channel=1024, in_edge_channel=1, hidden_channel=2048, out_channel=512, num_layers=3, pretrain=False)
+            >>> optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
+            # feat1/feat2/e_feat1/e_feat2 may be outputs by other neural networks
+            >>> X = pygm.cie(feat1, feat2, A1, A2, e_feat1, e_feat2, n1, n2, network=net)
+            >>> loss = pygm.utils.permutation_loss(X, X_gt)
+            >>> loss.backward()
+            >>> optimizer.step()
+
+    .. dropdown:: Jittor Example
+
+        ::
+
+            >>> import jittor as jt
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'jittor'
+            >>> _ = jt.seed(1)
+
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> X_gt = jt.zeros((batch_size, 4, 4))
+            >>> X_gt[:, jt.arange(0, 4, dtype=jt.int64), jt.randperm(4)] = 1
+            >>> A1 = 1. * (jt.rand(batch_size, 4, 4) > 0.5)
+            >>> for i in range(batch_size):
+            >>>     for j in range(4):
+            >>>        A1.data[i][j][j] = 0  # discard self-loop edges
+            >>> e_feat1 = (jt.rand(batch_size, 4, 4) * A1).unsqueeze(-1) # shape: (10, 4, 4, 1)
+            >>> A2 = jt.bmm(jt.bmm(X_gt.transpose(1, 2), A1), X_gt)
+            >>> e_feat2 = jt.bmm(jt.bmm(X_gt.transpose(1, 2), e_feat1.squeeze(-1)), X_gt).unsqueeze(-1)
+            >>> feat1 = jt.rand(batch_size, 4, 1024) - 0.5
+            >>> feat2 = jt.bmm(X_gt.transpose(1, 2), feat1)
+            >>> n1 = n2 = jt.Var([4] * batch_size)
+
+            # Match by CIE (load pretrained model)
+            >>> X, net = pygm.cie(feat1, feat2, A1, A2, e_feat1, e_feat2, n1, n2, return_network=True)
+            Downloading to ~/.cache/pygmtools/cie_voc_jittor.pt...
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
+            jt.Var([1.], dtype=float32)
+
+            # Pass the net object to avoid rebuilding the model agian
+            >>> X = pygm.cie(feat1, feat2, A1, A2, e_feat1, e_feat2, n1, n2, network=net)
+
+            # You may also load other pretrained weights
+            >>> X, net = pygm.cie(feat1, feat2, A1, A2, e_feat1, e_feat2, n1, n2, return_network=True, pretrain='willow')
+            Downloading to ~/.cache/pygmtools/cie_willow_jittor.pt...
+
+            # You may configure your own model and integrate the model into a deep learning pipeline. For example:
+            >>> net = pygm.utils.get_network(pygm.cie, in_node_channel=1024, in_edge_channel=1, hidden_channel=2048, out_channel=512, num_layers=3, pretrain=False)
+            >>> optimizer = jt.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
+            # feat1/feat2/e_feat1/e_feat2 may be outputs by other neural networks
+            >>> X = pygm.cie(feat1, feat2, A1, A2, e_feat1, e_feat2, n1, n2, network=net)
+            >>> loss = pygm.utils.permutation_loss(X, X_gt)
+            >>> optimizer.backward(loss)
+            >>> optimizer.step()
+
+    
+    .. dropdown:: Paddle Example
+
+        ::
+
+            >>> import paddle
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'paddle'
+            >>> _ = paddle.seed(1)
+
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> X_gt = paddle.zeros((batch_size, 4, 4))
+            >>> X_gt[:, paddle.arange(0, 4, dtype=paddle.int64), paddle.randperm(4)] = 1
+            >>> A1 = 1. * (paddle.rand((batch_size, 4, 4)) > 0.5)
+            >>> paddle.diagonal(A1, axis1=1, axis2=2)[:] = 0 # discard self-loop edges
+            >>> e_feat1 = (paddle.rand((batch_size, 4, 4)) * A1).unsqueeze(-1) # shape: (10, 4, 4, 1)
+            >>> A2 = paddle.bmm(paddle.bmm(X_gt.transpose((0, 2, 1)), A1), X_gt)
+            >>> e_feat2 = paddle.bmm(paddle.bmm(X_gt.transpose((0, 2, 1)), e_feat1.squeeze(-1)), X_gt).unsqueeze(-1)
+            >>> feat1 = paddle.rand((batch_size, 4, 1024)) - 0.5
+            >>> feat2 = paddle.bmm(X_gt.transpose((0, 2, 1)), feat1)
+            >>> n1 = n2 = paddle.to_tensor([4] * batch_size)
+
+            # Match by CIE (load pretrained model)
+            >>> X, net = pygm.cie(feat1, feat2, A1, A2, e_feat1, e_feat2, n1, n2, return_network=True)
+            Downloading to ~/.cache/pygmtools/cie_voc_paddle.pdparams...
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
+            Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True,
+                   [1.])
+
+            # Pass the net object to avoid rebuilding the model agian
+            >>> X = pygm.cie(feat1, feat2, A1, A2, e_feat1, e_feat2, n1, n2, network=net)
+
+            # You may also load other pretrained weights
+            >>> X, net = pygm.cie(feat1, feat2, A1, A2, e_feat1, e_feat2, n1, n2, return_network=True, pretrain='willow')
+            Downloading to ~/.cache/pygmtools/cie_willow_paddle.pdparams...
+
+            # You may configure your own model and integrate the model into a deep learning pipeline. For example:
+            >>> net = pygm.utils.get_network(pygm.cie, in_node_channel=1024, in_edge_channel=1, hidden_channel=2048, out_channel=512, num_layers=3, pretrain=False)
+            >>> optimizer = paddle.optimizer.SGD(parameters=net.parameters(), learning_rate=0.001)
+            # feat1/feat2/e_feat1/e_feat2 may be outputs by other neural networks
+            >>> X = pygm.cie(feat1, feat2, A1, A2, e_feat1, e_feat2, n1, n2, network=net)
+            >>> loss = pygm.utils.permutation_loss(X, X_gt)
+            >>> loss.backward()
+            >>> optimizer.step()
+    
+    .. note::
+
+        If you find this model useful in your research, please cite:
+
+        ::
+
+            @inproceedings{YuICLR20,
+              title={Learning deep graph matching with channel-independent embedding and Hungarian attention},
+              author={Yu, Tianshu and Wang, Runzhong and Yan, Junchi and Li, Baoxin},
+              booktitle={International Conference on Learning Representations},
+              year={2020}
+            }
+    """
+    if not num_layers >= 2: raise ValueError(f'num_layers must be >=2, got {num_layers}!')
+
+    if backend is None:
+        backend = pygmtools.BACKEND
+    non_batched_input = False
+    if feat_node1 is not None:  # if feat_node1 is None, this function skips the forward pass and only returns a network object
+        _check_data_type(feat_node1, 'feat_node1', backend)
+        _check_data_type(feat_node2, 'feat_node2', backend)
+        _check_data_type(A1, 'A1', backend)
+        _check_data_type(A2, 'A2', backend)
+        _check_data_type(feat_edge1, 'feat_edge1', backend)
+        _check_data_type(feat_edge2, 'feat_edge2', backend)
+
+        if all([_check_shape(_, 2, backend) for _ in (feat_node1, feat_node2, A1, A2)]) \
+                and all([_check_shape(_, 3, backend) for _ in (feat_edge1, feat_edge2)]):
+            feat_node1, feat_node2, A1, A2, feat_edge1, feat_edge2 =\
+                [_unsqueeze(_, 0, backend) for _ in (feat_node1, feat_node2, A1, A2, feat_edge1, feat_edge2)]
+            if type(n1) is int: n1 = from_numpy(np.array([n1]), backend=backend)
+            if type(n2) is int: n2 = from_numpy(np.array([n2]), backend=backend)
+            non_batched_input = True
+        elif all([_check_shape(_, 3, backend) for _ in (feat_node1, feat_node2, A1, A2)]) \
+                and all([_check_shape(_, 4, backend) for _ in (feat_edge1, feat_edge2)]):
+            non_batched_input = False
+        else:
+            raise ValueError(
+                f'the dimensions of the input arguments are illegal. Got '
+                f'feat_node1:{len(_get_shape(feat_node1, backend))}dims, feat_node2:{len(_get_shape(feat_node2, backend))}dims, '
+                f'A1:{len(_get_shape(A1, backend))}dims, A2:{len(_get_shape(A2, backend))}dims, '
+                f'feat_edge1:{len(_get_shape(feat_edge1, backend))}dims, feat_edge2:{len(_get_shape(feat_edge2, backend))}dims. '
+                f'Read the doc for more details!')
+
+        if not (_get_shape(feat_node1, backend)[0] == _get_shape(feat_node2, backend)[0] == _get_shape(A1, backend)[0] ==
+                _get_shape(A2, backend)[0] == _get_shape(feat_edge1, backend)[0] == _get_shape(feat_edge2, backend)[0])\
+                or not (_get_shape(feat_node1, backend)[1] == _get_shape(A1, backend)[1] == _get_shape(A1, backend)[2] ==
+                        _get_shape(feat_edge1, backend)[1] == _get_shape(feat_edge1, backend)[2])\
+                or not (_get_shape(feat_node2, backend)[1] == _get_shape(A2, backend)[1] == _get_shape(A2, backend)[2] ==
+                        _get_shape(feat_edge2, backend)[1] == _get_shape(feat_edge2, backend)[2])\
+                or not (_get_shape(feat_node1, backend)[2] == _get_shape(feat_node2, backend)[2])\
+                or not (_get_shape(feat_edge1, backend)[3] == _get_shape(feat_edge2, backend)[3]):
+            raise ValueError(
+                f'the input dimensions do not match. Got feat_node1:{_get_shape(feat_node1, backend)}, '
+                f'feat_node2:{_get_shape(feat_node2, backend)}, A1:{_get_shape(A1, backend)}, A2:{_get_shape(A2, backend)},'
+                f'feat_edge1:{_get_shape(feat_edge1, backend)}, feat_edge2:{_get_shape(feat_edge2, backend)}!')
+    if n1 is not None: _check_data_type(n1, 'n1', backend)
+    if n2 is not None: _check_data_type(n2, 'n2', backend)
+
+    args = (feat_node1, feat_node2, A1, A2, feat_edge1, feat_edge2, n1, n2,
+            in_node_channel, in_edge_channel, hidden_channel, out_channel, num_layers,
+            sk_max_iter, sk_tau, network, pretrain)
+    try:
+        mod = importlib.import_module(f'pygmtools.{backend}_backend')
+        fn = mod.cie
+    except (ModuleNotFoundError, AttributeError):
+        raise NotImplementedError(
+            NOT_IMPLEMENTED_MSG.format(backend)
+        )
+
+    result = fn(*args)
+    match_mat = _squeeze(result[0], 0, backend) if non_batched_input else result[0]
+    if return_network:
+        return match_mat, result[1]
+    else:
+        return match_mat
+
+
+def ngm(K, n1=None, n2=None, n1max=None, n2max=None, x0=None,
+        gnn_channels=(16, 16, 16), sk_emb=1,
+        sk_max_iter=20, sk_tau=0.05,
+        network=None, return_network=False, pretrain='voc',
+        backend=None):
+    r"""
+    The **NGM** (Neural Graph Matching) model for processing the affinity matrix (the most general form of Lawler's QAP).
+    The math form of graph matching (Lawler's QAP) is equivalent to a vertex classification problem on the
+    **association graph**, which is an equivalent formulation based on the affinity matrix :math:`\mathbf{K}`.
+    The graph matching module is composed of several graph convolution layers, Sinkhorn embedding layers and finally
+    a Sinkhorn layer to output a doubly-stochastic matrix.
+
+    See the following pipeline for an example:
+
+    .. image:: ../../images/ngm.png
+
+    See the following paper for more technical details:
+    `"Wang et al. Neural Graph Matching Network: Learning Lawler‚Äôs Quadratic Assignment Problem With Extension to
+    Hypergraph and Multiple-Graph Matching. TPAMI 2022."
+    <https://ieeexplore.ieee.org/abstract/document/9426408/>`_
+
+    :param K: :math:`(b\times n_1n_2 \times n_1n_2)` the input affinity matrix, :math:`b`: batch size.
+    :param n1: :math:`(b)` number of nodes in graph1 (optional if n1max is given, and all n1=n1max).
+    :param n2: :math:`(b)` number of nodes in graph2 (optional if n2max is given, and all n2=n2max).
+    :param n1max: :math:`(b)` max number of nodes in graph1 (optional if n1 is given, and n1max=max(n1)).
+    :param n2max: :math:`(b)` max number of nodes in graph2 (optional if n2 is given, and n2max=max(n2)).
+    :param x0: :math:`(b\times n_1 \times n_2)` an initial matching solution to warm-start the vertex embedding.
+        If not given, the vertex embedding is initialized as a vector of all 1s.
+    :param gnn_channels: (default: ``(16, 16, 16)``) A list/tuple of channel sizes of the GNN.
+        Ignored if the network object is given (ignored if ``network!=None``)
+    :param sk_emb: (default: 1) Number of Sinkhorn embedding channels. Sinkhorn embedding is designed to encode the
+        matching constraints inside GNN layers. How it works: a Sinkhorn embedding channel accepts the vertex feature
+        from the current layer and computes a doubly-stochastic matrix, which is then concatenated to the vertex feature.
+        Ignored if the network object is given (ignored if ``network!=None``)
+    :param sk_max_iter: (default: 20) Max number of iterations of Sinkhorn. See
+        :func:`~pygmtools.classic_solvers.sinkhorn` for more details about this argument.
+    :param sk_tau: (default: 0.05) The temperature parameter of Sinkhorn. See
+        :func:`~pygmtools.classic_solvers.sinkhorn` for more details about this argument.
+    :param network: (default: None) The network object. If None, a new network object will be created, and load the
+        model weights specified in ``pretrain`` argument.
+    :param return_network: (default: False) Return the network object (saving model construction time if calling the
+        model multiple times).
+    :param pretrain: (default: 'voc') If ``network==None``, the pretrained model weights to be loaded. Available
+        pretrained weights: ``voc`` (on Pascal VOC Keypoint dataset), ``willow`` (on Willow Object Class dataset),
+        or ``False`` (no pretraining).
+    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
+    :return: if ``return_network==False``, :math:`(b\times n_1 \times n_2)` the doubly-stochastic matching matrix
+
+        if ``return_network==True``, :math:`(b\times n_1 \times n_2)` the doubly-stochastic matching matrix,
+        the network object
+
+    .. note::
+        You may need a proxy to load the pretrained weights if Google drive is not accessible in your contry/region.
+        You may also download the pretrained models manually and put them at ``~/.cache/pygmtools`` (for Linux).
+
+        `[google drive] <https://drive.google.com/drive/folders/1O7vkIW8QXBJsNsHUIRiSw91HJ_0FAzu_?usp=sharing>`_
+        `[baidu drive] <https://pan.baidu.com/s/1MvzfM52NJeLWx2JXbbc6HA?pwd=x8bv>`_
+
+    .. note::
+        This function also supports non-batched input, by ignoring all batch dimensions in the input tensors.
+
+    .. dropdown:: Numpy Example
+
+        ::
+
+            >>> import numpy as np
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'numpy'
+            >>> np.random.seed(1)
+
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> X_gt = np.zeros((batch_size, 4, 4))
+            >>> X_gt[:, np.arange(0, 4, dtype='i4'), np.random.permutation(4)] = 1
+            >>> A1 = np.random.rand(batch_size, 4, 4)
+            >>> A2 = np.matmul(np.matmul(X_gt.swapaxes(1, 2), A1), X_gt)
+            >>> n1 = n2 = np.array([4] * batch_size)
+
+            # Build affinity matrix
+            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
+            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
+            >>> import functools
+            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
+            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
+
+            # Solve by NGM
+            >>> X, net = pygm.ngm(K, n1, n2, return_network=True)
+            Downloading to ~/.cache/pygmtools/ngm_voc_numpy.npy...
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
+            1.0
+
+            # Pass the net object to avoid rebuilding the model agian
+            >>> X = pygm.ngm(K, n1, n2, network=net)
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
+            1.0
+
+            # You may also load other pretrained weights
+            >>> X, net = pygm.ngm(feat1, feat2, A1, A2, e_feat1, e_feat2, n1, n2, return_network=True, pretrain='willow')
+            Downloading to ~/.cache/pygmtools/ngm_willow_numpy.npy...
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
+            1.0
+
+            # You may configure your own model and integrate the model into a deep learning pipeline. For example:
+            >>> net = pygm.utils.get_network(pygm.ngm, gnn_channels=(32, 64, 128, 64, 32), sk_emb=8, pretrain=False)
+            # K may be outputs by other neural networks (constructed K from node/edge features by pygm.utils.build_aff_mat)
+            >>> X = pygm.ngm(K, n1, n2, network=net)
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
+            1.0
+
+
+    .. dropdown:: PyTorch Example
+
+        ::
+
+            >>> import torch
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'pytorch'
+            >>> _ = torch.manual_seed(1)
+
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> X_gt = torch.zeros(batch_size, 4, 4)
+            >>> X_gt[:, torch.arange(0, 4, dtype=torch.int64), torch.randperm(4)] = 1
+            >>> A1 = torch.rand(batch_size, 4, 4)
+            >>> A2 = torch.bmm(torch.bmm(X_gt.transpose(1, 2), A1), X_gt)
+            >>> n1 = n2 = torch.tensor([4] * batch_size)
+
+            # Build affinity matrix
+            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
+            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
+            >>> import functools
+            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
+            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
+
+            # Solve by NGM
+            >>> X, net = pygm.ngm(K, n1, n2, return_network=True)
+            Downloading to ~/.cache/pygmtools/ngm_voc_pytorch.pt...
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
+            tensor(1.)
+
+            # Pass the net object to avoid rebuilding the model agian
+            >>> X = pygm.ngm(K, n1, n2, network=net)
+
+            # You may also load other pretrained weights
+            >>> X, net = pygm.ngm(K, n1, n2, return_network=True, pretrain='willow')
+            Downloading to ~/.cache/pygmtools/ngm_willow_pytorch.pt...
+
+            # You may configure your own model and integrate the model into a deep learning pipeline. For example:
+            >>> net = pygm.utils.get_network(pygm.ngm, gnn_channels=(32, 64, 128, 64, 32), sk_emb=8, pretrain=False)
+            >>> optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
+            # K may be outputs by other neural networks (constructed K from node/edge features by pygm.utils.build_aff_mat)
+            >>> X = pygm.ngm(K, n1, n2, network=net)
+            >>> loss = pygm.utils.permutation_loss(X, X_gt)
+            >>> loss.backward()
+            >>> optimizer.step()
+
+    .. dropdown:: Jittor Example
+
+        ::
+
+            >>> import jittor as jt
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'jittor'
+            >>> _ = jt.seed(1)
+
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> X_gt = jt.zeros((batch_size, 4, 4))
+            >>> X_gt[:, jt.arange(0, 4, dtype=jt.int64), jt.randperm(4)] = 1
+            >>> A1 = jt.rand(batch_size, 4, 4)
+            >>> A2 = jt.bmm(jt.bmm(X_gt.transpose(1, 2), A1), X_gt)
+            >>> n1 = n2 = jt.Var([4] * batch_size)
+
+            # Build affinity matrix
+            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
+            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
+            import functools
+            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
+            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
+
+            # Solve by NGM
+            >>> X, net = pygm.ngm(K, n1, n2, return_network=True)
+            # Downloading to ~/.cache/pygmtools/ngm_voc_jittor.pt...
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
+            jt.Var([1.], dtype=float32)
+
+            # Pass the net object to avoid rebuilding the model agian
+            >>> X = pygm.ngm(K, n1, n2, network=net)
+
+            # You may also load other pretrained weights
+            >>> X, net = pygm.ngm(K, n1, n2, return_network=True, pretrain='willow')
+            # Downloading to ~/.cache/pygmtools/ngm_willow_jittor.pt...
+
+            # You may configure your own model and integrate the model into a deep learning pipeline. For example:
+            >>> net = pygm.utils.get_network(pygm.ngm, gnn_channels=(32, 64, 128, 64, 32), sk_emb=8, pretrain=False)
+            >>> optimizer = jt.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
+            # K may be outputs by other neural networks (constructed K from node/edge features by pygm.utils.build_aff_mat)
+            >>> X = pygm.ngm(K, n1, n2, network=net)
+            >>> loss = pygm.utils.permutation_loss(X, X_gt)
+            >>> optimizer.backward(loss)
+            >>> optimizer.step()        
+
+    
+    .. dropdown:: Paddle Example
+
+        ::
+
+            >>> import paddle
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'paddle'
+            >>> _ = paddle.seed(1)
+
+            # Generate a batch of isomorphic graphs
+            >>> batch_size = 10
+            >>> X_gt = paddle.zeros((batch_size, 4, 4))
+            >>> X_gt[:, paddle.arange(0, 4, dtype=paddle.int64), paddle.randperm(4)] = 1
+            >>> A1 = paddle.rand((batch_size, 4, 4))
+            >>> A2 = paddle.bmm(paddle.bmm(X_gt.transpose((0, 2, 1)), A1), X_gt)
+            >>> n1 = n2 = paddle.to_tensor([4] * batch_size)
+
+            # Build affinity matrix
+            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
+            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
+            >>> import functools
+            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.) # set affinity function
+            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, None, n2, None, edge_aff_fn=gaussian_aff)
+
+            # Solve by NGM
+            >>> X, net = pygm.ngm(K, n1, n2, return_network=True)
+            Downloading to ~/.cache/pygmtools/ngm_voc_paddle.pdparams...
+            >>> (pygm.hungarian(X) * X_gt).sum() / X_gt.sum() # accuracy
+            Tensor(shape=[1], dtype=float32, place=Place(cpu), stop_gradient=True,
+                   [1.])
+
+            # Pass the net object to avoid rebuilding the model agian
+            >>> X = pygm.ngm(K, n1, n2, network=net)
+
+            # You may also load other pretrained weights
+            >>> X, net = pygm.ngm(K, n1, n2, return_network=True, pretrain='willow')
+            Downloading to ~/.cache/pygmtools/ngm_willow_paddle.pdparams...
+
+            # You may configure your own model and integrate the model into a deep learning pipeline. For example:
+            >>> net = pygm.utils.get_network(pygm.ngm, gnn_channels=(32, 64, 128, 64, 32), sk_emb=8, pretrain=False)
+            >>> optimizer = paddle.optimizer.SGD(parameters=net.parameters(), learning_rate=0.001)
+            # K may be outputs by other neural networks (constructed K from node/edge features by pygm.utils.build_aff_mat)
+            >>> X = pygm.ngm(K, n1, n2, network=net)
+            >>> loss = pygm.utils.permutation_loss(X, X_gt)
+            >>> loss.backward()
+            >>> optimizer.step()
+    
+    .. note::
+
+        If you find this model useful in your research, please cite:
+
+        ::
+
+            @ARTICLE{WangPAMI22,
+              author={Wang, Runzhong and Yan, Junchi and Yang, Xiaokang},
+              journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
+              title={Neural Graph Matching Network: Learning Lawler‚Äôs Quadratic Assignment Problem With Extension to Hypergraph and Multiple-Graph Matching},
+              year={2022},
+              volume={44},
+              number={9},
+              pages={5261-5279},
+              doi={10.1109/TPAMI.2021.3078053}
+            }
+    """
+    if not len(gnn_channels) >= 1: raise ValueError(f'gnn_channels should not be empty!')
+    if not sk_emb >= 0: raise ValueError(f'sk_emb must be >=0. Got sk_emb={sk_emb}!')
+
+    if backend is None:
+        backend = pygmtools.BACKEND
+    non_batched_input = False
+    if K is not None: # if K is None, this function skips the forward pass and only returns a network object
+        _check_data_type(K, 'K', backend)
+        if _check_shape(K, 2, backend):
+            K = _unsqueeze(K, 0, backend)
+            non_batched_input = True
+            if type(n1) is int and n1max is None:
+                n1max = n1
+                n1 = None
+            if type(n2) is int and n2max is None:
+                n2max = n2
+                n2 = None
+        elif _check_shape(K, 3, backend):
+            non_batched_input = False
+        else:
+            raise ValueError(f'the input argument K is expected to be 2-dimensional or 3-dimensional, got '
+                             f'K:{len(_get_shape(K, backend))}dims!')
+        __check_gm_arguments(n1, n2, n1max, n2max)
+
+    args = (K, n1, n2, n1max, n2max, x0, gnn_channels, sk_emb, sk_max_iter, sk_tau, network, return_network, pretrain)
+    try:
+        mod = importlib.import_module(f'pygmtools.{backend}_backend')
+        fn = mod.ngm
+    except (ModuleNotFoundError, AttributeError):
+        raise NotImplementedError(
+            NOT_IMPLEMENTED_MSG.format(backend)
+        )
+    result = fn(*args)
+    match_mat = _squeeze(result[0], 0, backend) if non_batched_input else result[0]
+    if return_network:
+        return match_mat, result[1]
+    else:
+        return match_mat
```

### Comparing `pygmtools-0.3.8/pygmtools/numpy_modules.py` & `pygmtools-0.3.8a0/pygmtools/numpy_modules.py`

 * *Files identical despite different names*

### Comparing `pygmtools-0.3.8/pygmtools/paddle_backend.py` & `pygmtools-0.3.8a0/pygmtools/numpy_backend.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,1370 +1,1398 @@
-# Copyright (c) 2022 Thinklab@SJTU
-# pygmtools is licensed under Mulan PSL v2.
-# You can use this software according to the terms and conditions of the Mulan PSL v2.
-# You may obtain a copy of Mulan PSL v2 at:
-# http://license.coscl.org.cn/MulanPSL2
-# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND,
-# EITHER EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT,
-# MERCHANTABILITY OR FIT FOR A PARTICULAR PURPOSE.
-# See the Mulan PSL v2 for more details.
-
-import itertools
-import functools
-import paddle
-import numpy as np
-from multiprocessing import Pool
-import os
-
-import pygmtools.utils
-from pygmtools.numpy_backend import _hung_kernel
-
-
-#############################################
-#     Linear Assignment Problem Solvers     #
-#############################################
-
-def hungarian(s: paddle.Tensor, n1: paddle.Tensor=None, n2: paddle.Tensor=None,
-              unmatch1: paddle.Tensor=None, unmatch2: paddle.Tensor=None,
-              nproc: int=1) -> paddle.Tensor:
-    """
-    Paddle implementation of Hungarian algorithm
-    """
-    device = s.place
-    batch_num = s.shape[0]
-
-    perm_mat = s.cpu().detach().numpy() * -1
-    if n1 is not None:
-        n1 = n1.cpu().numpy()
-    else:
-        n1 = [None] * batch_num
-    if n2 is not None:
-        n2 = n2.cpu().numpy()
-    else:
-        n2 = [None] * batch_num
-    if unmatch1 is not None:
-        unmatch1 = -unmatch1.cpu().numpy()
-    else:
-        unmatch1 = [None] * batch_num
-    if unmatch2 is not None:
-        unmatch2 = -unmatch2.cpu().numpy()
-    else:
-        unmatch2 = [None] * batch_num
-
-    if nproc > 1:
-        with Pool(processes=nproc) as pool:
-            mapresult = pool.starmap_async(_hung_kernel, zip(perm_mat, n1, n2, unmatch1, unmatch2))
-            perm_mat = np.stack(mapresult.get())
-    else:
-        perm_mat = np.stack([_hung_kernel(perm_mat[b], n1[b], n2[b], unmatch1[b], unmatch2[b]) for b in range(batch_num)])
-
-    perm_mat = paddle.to_tensor(perm_mat, place=device)
-
-    return perm_mat
-
-
-def sinkhorn(s: paddle.Tensor, nrows: paddle.Tensor=None, ncols: paddle.Tensor=None,
-             unmatchrows: paddle.Tensor=None, unmatchcols: paddle.Tensor=None,
-             dummy_row: bool=False, max_iter: int=10, tau: float=1., batched_operation: bool=False) -> paddle.Tensor:
-    """
-    Paddle implementation of Sinkhorn algorithm
-    """
-    batch_size = s.shape[0]
-
-    if s.shape[2] >= s.shape[1]:
-        transposed = False
-    else:
-        s = s.transpose((0, 2, 1))
-        nrows, ncols = ncols, nrows
-        unmatchrows, unmatchcols = unmatchcols, unmatchrows
-        transposed = True
-
-    if nrows is None:
-        nrows = paddle.to_tensor([s.shape[1] for _ in range(batch_size)], place=s.place, dtype=paddle.int32)
-    if ncols is None:
-        ncols = paddle.to_tensor([s.shape[2] for _ in range(batch_size)], place=s.place, dtype=paddle.int32)
-
-
-    # ensure that in each dimension we have nrow < ncol
-    transposed_batch = nrows > ncols
-    if paddle.any(transposed_batch):
-        s_t = s.transpose((0, 2, 1))
-        s_t = paddle.concat((
-            s_t[:, :s.shape[1], :],
-            paddle.to_tensor(paddle.full((batch_size, s.shape[1], s.shape[2]-s.shape[1]), -float('inf')), place=s.place)), axis=2)
-        s = paddle.where(transposed_batch.reshape((batch_size, 1, 1)), s_t, s)
-
-        new_nrows = paddle.where(transposed_batch, ncols, nrows)
-        new_ncols = paddle.where(transposed_batch, nrows, ncols)
-        nrows = new_nrows
-        ncols = new_ncols
-
-        if unmatchrows is not None and unmatchcols is not None:
-            unmatchrows_pad = paddle.concat((
-                unmatchrows,
-                paddle.to_tensor(paddle.full((batch_size, unmatchcols.shape[1] - unmatchrows.shape[1]), -float('inf'), dtype=unmatchrows.dtype), place=unmatchrows.place)),
-            axis=1)
-            new_unmatchrows = paddle.where(transposed_batch.reshape((batch_size, 1)), unmatchcols, unmatchrows_pad)[:, :unmatchrows.shape[1]]
-            new_unmatchcols = paddle.where(transposed_batch.reshape((batch_size, 1)), unmatchrows_pad, unmatchcols)
-            unmatchrows = new_unmatchrows
-            unmatchcols = new_unmatchcols
-
-    # operations are performed on log_s
-    log_s = s / tau
-    if unmatchrows is not None and unmatchcols is not None:
-        unmatchrows = unmatchrows / tau
-        unmatchcols = unmatchcols / tau
-
-    if dummy_row:
-        assert log_s.shape[2] >= log_s.shape[1]
-        dummy_shape = list(log_s.shape)
-        dummy_shape[1] = log_s.shape[2] - log_s.shape[1]
-        ori_nrows = nrows
-        nrows = ncols.clone()
-        log_s = paddle.concat((log_s, paddle.to_tensor(paddle.full(dummy_shape, -float('inf'), dtype=log_s.dtype), place=log_s.place)), axis=1)
-        if unmatchrows is not None:
-            unmatchrows = paddle.concat((unmatchrows, paddle.to_tensor(paddle.full((dummy_shape[0], dummy_shape[1]), -float('inf'), dtype=log_s.dtype), place=log_s.place)), axis=1)
-        for b in range(batch_size):
-            log_s[b, ori_nrows[b]:nrows[b], :ncols[b]] = -100
-
-    # assign the unmatch weights
-    if unmatchrows is not None and unmatchcols is not None:
-        new_log_s = paddle.to_tensor(paddle.full((log_s.shape[0], log_s.shape[1]+1, log_s.shape[2]+1), -float('inf'), dtype=log_s.dtype), place=log_s.place)
-        new_log_s[:, :-1, :-1] = log_s
-        log_s = new_log_s
-        for b in range(batch_size):
-            log_s[b, :nrows[b], ncols[b]] = unmatchrows[b, :nrows[b]]
-            log_s[b, nrows[b], :ncols[b]] = unmatchcols[b, :ncols[b]]
-    row_mask = paddle.zeros((batch_size, log_s.shape[1], 1), dtype=paddle.bool)
-    col_mask = paddle.zeros((batch_size, 1, log_s.shape[2]), dtype=paddle.bool)
-    for b in range(batch_size):
-        row_mask[b, :nrows[b], 0] = 1
-        col_mask[b, 0, :ncols[b]] = 1
-    if unmatchrows is not None and unmatchcols is not None:
-        ncols += 1
-        nrows += 1
-
-    if batched_operation:
-        for b in range(batch_size):
-            log_s[b, nrows[b]:, :] = -float('inf')
-            log_s[b, :, ncols[b]:] = -float('inf')
-
-        for i in range(max_iter):
-            if i % 2 == 0:
-                log_sum = paddle.logsumexp(log_s, 2, keepdim=True)
-                log_s = log_s - paddle.where(row_mask, log_sum, paddle.zeros_like(log_sum))
-                nan_indices = paddle.nonzero(paddle.isnan(log_s), True)
-                assert nan_indices[0].size == 0
-            else:
-                log_sum = paddle.logsumexp(log_s, 1, keepdim=True)
-                log_s = log_s - paddle.where(col_mask, log_sum, paddle.zeros_like(log_sum))
-                nan_indices = paddle.nonzero(paddle.isnan(log_s), True)
-                assert nan_indices[0].size == 0
-
-        ret_log_s = log_s
-    else:
-        ret_log_s = paddle.to_tensor(paddle.full((batch_size, log_s.shape[1], log_s.shape[2]), -float('inf')), place=log_s.place, dtype=log_s.dtype)
-
-        for b in range(batch_size):
-            row_slice = slice(0, nrows[b])
-            col_slice = slice(0, ncols[b])
-            log_s_b = log_s[b, row_slice, col_slice]
-            row_mask_b = row_mask[b, row_slice, :]
-            col_mask_b = col_mask[b, :, col_slice]
-
-            for i in range(max_iter):
-                if i % 2 == 0:
-                    log_sum = paddle.logsumexp(log_s_b, 1, keepdim=True)
-                    log_s_b = log_s_b - paddle.where(row_mask_b, log_sum, paddle.zeros_like(log_sum))
-                else:
-                    log_sum = paddle.logsumexp(log_s_b, 0, keepdim=True)
-                    log_s_b = log_s_b - paddle.where(col_mask_b, log_sum, paddle.zeros_like(log_sum))
-
-            ret_log_s[b, row_slice, col_slice] = log_s_b
-
-    if unmatchrows is not None and unmatchcols is not None:
-        ncols -= 1
-        nrows -= 1
-        for b in range(batch_size):
-            ret_log_s[b, :nrows[b] + 1, ncols[b]] = -float('inf')
-            ret_log_s[b, nrows[b], :ncols[b]] = -float('inf')
-        ret_log_s = ret_log_s[:, :-1, :-1]
-
-    if dummy_row:
-        if dummy_shape[1] > 0:
-            ret_log_s = ret_log_s[:, :-dummy_shape[1]]
-        for b in range(batch_size):
-            ret_log_s[b, ori_nrows[b]:nrows[b], :ncols[b]] = -float('inf')
-
-    if paddle.any(transposed_batch):
-        s_t = ret_log_s.transpose((0, 2, 1))
-        s_t = paddle.concat((
-            s_t[:, :ret_log_s.shape[1], :],
-            paddle.to_tensor(paddle.full((batch_size, ret_log_s.shape[1], ret_log_s.shape[2]-ret_log_s.shape[1]), -float('inf')), place=log_s.place)), axis=2)
-        ret_log_s = paddle.where(transposed_batch.reshape((batch_size, 1, 1)), s_t, ret_log_s)
-
-    if transposed:
-        ret_log_s = ret_log_s.transpose((0, 2, 1))
-
-    return paddle.exp(ret_log_s)
-
-
-#############################################
-#    Quadratic Assignment Problem Solvers   #
-#############################################
-
-
-def rrwm(K: paddle.Tensor, n1: paddle.Tensor, n2: paddle.Tensor, n1max, n2max, x0: paddle.Tensor,
-         max_iter: int, sk_iter: int, alpha: float, beta: float) -> paddle.Tensor:
-    """
-    Paddle implementation of RRWM algorithm.
-    """
-    batch_num, n1, n2, n1max, n2max, n1n2, v0 = _check_and_init_gm(K, n1, n2, n1max, n2max, x0)
-    # rescale the values in K
-    d = paddle.sum(K, axis=2, keepdim=True)
-    dmax = paddle.max(d, axis=1, keepdim=True)
-    K = K / (dmax + paddle.min(d) * 1e-5)
-    v = v0
-    for i in range(max_iter):
-        # random walk
-        v = paddle.bmm(K, v)
-        last_v = v
-        n = paddle.norm(v, p=1, axis=1, keepdim=True)
-        v = v / n
-
-        # reweighted jump
-        s = paddle.reshape(v, (batch_num, n2max, n1max)).transpose((0, 2, 1))
-        s = beta * s / s.max(axis=1, keepdim=True).max(axis=2, keepdim=True)
-        v = alpha * paddle.reshape(sinkhorn(s, n1, n2, max_iter=sk_iter).transpose((0, 2, 1)),(batch_num, n1n2, 1)) + \
-            (1 - alpha) * v
-        n = paddle.norm(v, p=1, axis=1, keepdim=True)
-        v = paddle.matmul(v, 1 / n)
-
-        if paddle.norm(v - last_v) < 1e-5:
-            break
-
-    return v.reshape((batch_num, n2max, n1max)).transpose((0, 2, 1))
-
-
-def sm(K: paddle.Tensor, n1: paddle.Tensor, n2: paddle.Tensor, n1max, n2max, x0: paddle.Tensor,
-       max_iter: int) -> paddle.Tensor:
-    """
-    Paddle implementation of SM algorithm.
-    """
-    batch_num, n1, n2, n1max, n2max, n1n2, v0 = _check_and_init_gm(K, n1, n2, n1max, n2max, x0)
-    v = vlast = v0
-    for _ in range(max_iter):
-        v = paddle.bmm(K, v)
-        n = paddle.norm(v, p=2, axis=1)
-        v = paddle.matmul(v, paddle.reshape(1 / n, (batch_num, 1, 1)))
-        if paddle.norm(v - vlast) < 1e-5:
-            break
-        vlast = v
-
-    x = paddle.reshape(v, (batch_num, n2max, n1max)).transpose((0, 2, 1))
-    return x
-
-
-def ipfp(K: paddle.Tensor, n1: paddle.Tensor, n2: paddle.Tensor, n1max, n2max, x0: paddle.Tensor,
-         max_iter) -> paddle.Tensor:
-    """
-    Paddle implementation of IPFP algorithm
-    """
-    batch_num, n1, n2, n1max, n2max, n1n2, v0 = _check_and_init_gm(K, n1, n2, n1max, n2max, x0)
-    v = v0
-    last_v = v
-
-    def comp_obj_score(v1, K, v2):
-        return paddle.bmm(paddle.bmm(paddle.reshape(v1, (batch_num, 1, -1)), K), v2)
-
-    for i in range(max_iter):
-        cost = paddle.reshape(paddle.bmm(K, v),(batch_num, n2max, n1max)).transpose((0, 2, 1))
-        binary_sol = hungarian(cost, n1, n2)
-        binary_v = paddle.reshape(binary_sol.transpose((0, 2, 1)),(batch_num, -1, 1))
-        alpha = comp_obj_score(v, K, binary_v - v)
-        beta = comp_obj_score(binary_v - v, K, binary_v - v)
-        t0 = alpha / beta
-        v = paddle.where(paddle.logical_or(beta <= 0, t0 >= 1), binary_v, v + t0 * (binary_v - v))
-        last_v_sol = comp_obj_score(last_v, K, last_v)
-        if paddle.max(paddle.abs(
-                last_v_sol - paddle.bmm(paddle.reshape(cost,(batch_num, 1, -1)), paddle.reshape(binary_sol, (batch_num, -1, 1)))
-        ) / last_v_sol) < 1e-3:
-            break
-        last_v = v
-
-    pred_x = binary_sol
-    return pred_x
-
-
-def _check_and_init_gm(K, n1, n2, n1max, n2max, x0):
-    # get batch number
-    batch_num = K.shape[0]
-    n1n2 = K.shape[1]
-
-    # get values of n1, n2, n1max, n2max and check
-    if n1 is None:
-        n1 = paddle.to_tensor(paddle.full((batch_num,), n1max, dtype=paddle.int32), place=K.place)
-    if n2 is None:
-        n2 = paddle.to_tensor(paddle.full((batch_num,), n2max, dtype=paddle.int32), place=K.place)
-    if n1max is None:
-        n1max = paddle.max(n1)
-    if n2max is None:
-        n2max = paddle.max(n2)
-
-    assert n1max * n2max == n1n2, 'the input size of K does not match with n1max * n2max!'
-
-    # initialize x0 (also v0)
-    if x0 is None:
-        x0 = paddle.to_tensor(paddle.zeros((batch_num, n1max, n2max), dtype=K.dtype), place=K.place)
-        for b in range(batch_num):
-            x0[b, 0:n1[b], 0:n2[b]] = paddle.to_tensor(1.) / (n1[b] * n2[b])
-
-    v0 = paddle.reshape(paddle.transpose(x0, perm=(0, 2, 1)), (batch_num, n1n2, 1))
-
-    return batch_num, n1, n2, n1max, n2max, n1n2, v0
-
-
-############################################
-#      Multi-Graph Matching Solvers        #
-############################################
-
-
-def cao_solver(K, X, num_graph, num_node, max_iter, lambda_init, lambda_step, lambda_max, iter_boost):
-    r"""
-    Paddle implementation of CAO solver (mode="c")
-
-    :param K: affinity matrix, (m, m, n*n, n*n)
-    :param X: initial matching, (m, m, n, n)
-    :param num_graph: number of graphs, int
-    :param num_node: number of nodes, int
-    :return: X, (m, m, n, n)
-    """
-    m, n = num_graph, num_node
-    param_lambda = lambda_init
-    device = K.place
-
-    def _comp_aff_score(x, k):
-        return pygmtools.utils.compute_affinity_score(x, k, backend='paddle').unsqueeze(-1).unsqueeze(-1)
-
-    for iter in range(max_iter):
-        if iter >= iter_boost:
-            param_lambda = np.min([param_lambda * lambda_step, lambda_max])
-        # pair_con = get_batch_pc_opt(X)
-        pair_aff = _comp_aff_score(X.reshape((-1, n, n)), K.reshape((-1, n * n, n * n))).reshape((m, m))
-        pair_aff = pair_aff - paddle.to_tensor(paddle.eye(m) , place=device) * pair_aff
-        norm = paddle.max(pair_aff)
-        for i in range(m):
-            for j in range(m):
-                if i >= j:
-                    continue
-                aff_ori = _comp_aff_score(X[i, j], K[i, j]) / norm
-                con_ori = _get_single_pc_opt(X, i, j)
-                if iter < iter_boost:
-                    score_ori = aff_ori
-                else:
-                    score_ori = aff_ori * (1 - param_lambda) + con_ori * param_lambda
-                X_upt = X[i, j]
-                for k in range(m):
-                    X_combo = paddle.matmul(X[i, k], X[k, j])
-                    aff_combo = _comp_aff_score(X_combo, K[i, j]) / norm
-                    con_combo = _get_single_pc_opt(X, i, j, X_combo)
-                    if iter < iter_boost:
-                        score_combo = aff_combo
-                    else:
-                        score_combo = aff_combo * (1 - param_lambda) + con_combo * param_lambda
-                    if score_combo > score_ori:
-                        X_upt = X_combo
-                X[i, j] = X_upt
-                X[j, i] = X_upt.transpose((1, 0))
-    return X
-
-
-def cao_fast_solver(K, X, num_graph, num_node, max_iter, lambda_init, lambda_step, lambda_max, iter_boost):
-    r"""
-    Paddle implementation of CAO solver in fast config (mode="pc")
-
-    :param K: affinity matrix, (m, m, n*n, n*n)
-    :param X: initial matching, (m, m, n, n)
-    :param num_graph: number of graphs, int
-    :param num_node: number of nodes, int
-    :return: X, (m, m, n, n)
-    """
-    m, n = num_graph, num_node
-    param_lambda = lambda_init
-
-    def _comp_aff_score(x, k):
-        return pygmtools.utils.compute_affinity_score(x, k, backend='paddle').unsqueeze(-1).unsqueeze(-1)
-
-    device = K.place
-    mask1 = paddle.to_tensor(paddle.arange(m).reshape((m, 1)).tile((1, m)), place = device)
-    mask2 = paddle.to_tensor(paddle.arange(m).reshape((1, m)).tile((m, 1)), place = device)
-    mask = paddle.to_tensor((mask1 < mask2), dtype = 'float32')
-    X_mask = mask.reshape((m, m, 1, 1))
-
-    for iter in range(max_iter):
-        if iter >= iter_boost:
-            param_lambda = np.min([param_lambda * lambda_step, lambda_max])
-
-        pair_aff = _comp_aff_score(X.reshape((-1, n, n)), K.reshape((-1, n * n, n * n))).reshape((m, m))
-        pair_aff = pair_aff - paddle.to_tensor(paddle.eye(m), place=device) * pair_aff
-        norm = paddle.max(pair_aff)
-
-        X1 = X.reshape((m, 1, m, n, n)).tile((1, m, 1, 1, 1)).reshape((-1, n, n))  # X1[i,j,k] = X[i,k]
-        X2 = X.reshape((1, m, m, n, n)).tile((m, 1, 1, 1, 1)).transpose((0, 2, 1, 3, 4)).reshape((-1, n, n))  # X2[i,j,k] = X[k,j]
-        X_combo = paddle.bmm(X1, X2).reshape((m, m, m, n, n)) # X_combo[i,j,k] = X[i, k] * X[k, j]
-
-        aff_ori = (_comp_aff_score(X.reshape((-1, n, n)), K.reshape((-1, n * n, n * n))) / norm).reshape((m, m))
-        pair_con = _get_batch_pc_opt(X)
-        con_ori = paddle.sqrt(pair_con)
-
-        K_repeat = K.reshape((m, m, 1, n * n, n * n)).tile((1, 1, m, 1, 1)).reshape((-1, n * n, n * n))
-        aff_combo = (_comp_aff_score(X_combo.reshape((-1, n, n)), K_repeat) / norm).reshape((m, m, m))
-        con1 = pair_con.reshape((m, 1, m)).tile((1, m, 1))  # con1[i,j,k] = pair_con[i,k]
-        con2 = pair_con.reshape((1, m, m)).tile((m, 1, 1)).transpose((0, 2, 1))  # con2[i,j,k] = pair_con[j,k]
-        con_combo = paddle.sqrt(con1 * con2)
-
-        if iter < iter_boost:
-            score_ori = aff_ori
-            score_combo = aff_combo
-        else:
-            score_ori = aff_ori * (1 - param_lambda) + con_ori * param_lambda
-            score_combo = aff_combo * (1 - param_lambda) + con_combo * param_lambda
-
-        idx = paddle.argmax(score_combo, axis=-1)
-        score_combo = paddle.max(score_combo, axis=-1)
-
-        assert paddle.all(score_combo + 1e-4 >= score_ori), paddle.min(score_combo - score_ori)
-        X_upt = X_combo[mask1, mask2, idx]
-        X = X_upt * X_mask + X_upt.transpose((1, 0, 3, 2))* X_mask.transpose((1, 0, 2, 3)) + X * (1 - X_mask - X_mask.transpose((1, 0, 2, 3)))
-        assert paddle.all(X.transpose((1, 0, 3, 2)) == X)
-    return X
-
-
-def mgm_floyd_solver(K, X, num_graph, num_node, param_lambda):
-    m, n = num_graph, num_node
-    device = K.place
-
-    def _comp_aff_score(x, k):
-        return pygmtools.utils.compute_affinity_score(x, k, backend='paddle').unsqueeze(-1).unsqueeze(-1)
-
-    for k in range(m):
-        pair_aff = _comp_aff_score(X.reshape((-1, n, n)), K.reshape((-1, n * n, n * n))).reshape((m, m))
-        pair_aff = pair_aff - paddle.to_tensor(paddle.eye(m), place=device) * pair_aff
-        norm = paddle.max(pair_aff)
-
-        for i in range(m):
-            for j in range(m):
-                if i >= j:
-                    continue
-                score_ori = _comp_aff_score(X[i, j], K[i, j]) / norm
-                X_combo = paddle.matmul(X[i, k], X[k, j])
-                score_combo = _comp_aff_score(X_combo, K[i, j]) / norm
-
-                if score_combo > score_ori:
-                    X[i, j] = X_combo
-                    X[j, i] = X_combo.transpose((1, 0))
-
-    for k in range(m):
-        pair_aff = _comp_aff_score(X.reshape((-1, n, n)), K.reshape((-1, n * n, n * n))).reshape((m, m))
-        pair_aff = pair_aff - paddle.to_tensor(paddle.eye(m), place=device) * pair_aff
-        norm = paddle.max(pair_aff)
-
-        pair_con = _get_batch_pc_opt(X)
-        for i in range(m):
-            for j in range(m):
-                if i >= j:
-                    continue
-                aff_ori = _comp_aff_score(X[i, j], K[i, j]) / norm
-                con_ori = _get_single_pc_opt(X, i, j)
-                score_ori = aff_ori * (1 - param_lambda) + con_ori * param_lambda
-
-                X_combo = paddle.matmul(X[i, k], X[k, j])
-                aff_combo = _comp_aff_score(X_combo, K[i, j]) / norm
-                con_combo = _get_single_pc_opt(X, i, j, X_combo)
-                score_combo = aff_combo * (1 - param_lambda) + con_combo * param_lambda
-
-                if score_combo > score_ori:
-                    X[i, j] = X_combo
-                    X[j, i] = X_combo.transpose((1, 0))
-    return X
-
-
-def mgm_floyd_fast_solver(K, X, num_graph, num_node, param_lambda):
-    m, n = num_graph, num_node
-    device = K.place
-
-    def _comp_aff_score(x, k):
-        return pygmtools.utils.compute_affinity_score(x, k, backend='paddle').unsqueeze(-1).unsqueeze(-1)
-
-    mask1 = paddle.arange(m).reshape((m, 1)).tile((1, m))
-    mask2 = paddle.arange(m).reshape((1, m)).tile((m, 1))
-    mask = paddle.to_tensor(paddle.to_tensor((mask1 < mask2), dtype = 'float32'), place = device)
-    X_mask = mask.reshape((m, m, 1, 1))
-
-    for k in range(m):
-        pair_aff = _comp_aff_score(X.reshape((-1, n, n)), K.reshape((-1, n * n, n * n))).reshape((m, m))
-        pair_aff = pair_aff - paddle.to_tensor(paddle.eye(m), place=device) * pair_aff
-        norm = paddle.max(pair_aff)
-
-        X1 = X[:, k].reshape((m, 1, n, n)).tile((1, m, 1, 1)).reshape((-1, n, n))  # X[i, j] = X[i, k]
-        X2 = X[k, :].reshape((1, m, n, n)).tile((m, 1, 1, 1)).reshape((-1, n, n))  # X[i, j] = X[j, k]
-        X_combo = paddle.bmm(X1, X2).reshape((m, m, n, n))
-
-        aff_ori = (_comp_aff_score(X.reshape((-1, n, n)), K.reshape((-1, n * n, n * n))) / norm).reshape((m, m))
-        aff_combo = (_comp_aff_score(X_combo.reshape((-1, n, n)), K.reshape((-1, n * n, n * n))) / norm).reshape((m, m))
-
-        score_ori = aff_ori
-        score_combo = aff_combo
-
-        upt = paddle.to_tensor((score_ori < score_combo), dtype = 'float32')
-        upt = (upt * mask).reshape((m, m, 1, 1))
-        X = X * (1.0 - upt) + X_combo * upt
-        X = X * X_mask + X.transpose((1, 0, 2, 3)).transpose((0, 1, 3, 2)) * (1 - X_mask)
-
-    for k in range(m):
-        pair_aff = _comp_aff_score(X.reshape((-1, n, n)), K.reshape((-1, n * n, n * n))).reshape((m, m))
-        pair_aff = pair_aff - paddle.to_tensor(paddle.eye(m), place=device) * pair_aff
-        norm = paddle.max(pair_aff)
-
-        pair_con = _get_batch_pc_opt(X)
-
-        X1 = X[:, k].reshape((m, 1, n, n)).tile((1, m, 1, 1)).reshape((-1, n, n))  # X[i, j] = X[i, k]
-        X2 = X[k, :].reshape((1, m, n, n)).tile((m, 1, 1, 1)).reshape((-1, n, n))  # X[i, j] = X[j, k]
-        X_combo = paddle.bmm(X1, X2).reshape((m, m, n, n))
-
-        aff_ori = (_comp_aff_score(X.reshape((-1, n, n)), K.reshape((-1, n * n, n * n))) / norm).reshape((m, m))
-        aff_combo = (_comp_aff_score(X_combo.reshape((-1, n, n)), K.reshape((-1, n * n, n * n))) / norm).reshape((m, m))
-
-        con_ori = paddle.sqrt(pair_con)
-        con1 = pair_con[:, k].reshape((m, 1)).tile((1, m))
-        con2 = pair_con[k, :].reshape((1, m)).tile((m, 1))
-        con_combo = paddle.sqrt(con1 * con2)
-
-        score_ori = aff_ori * (1 - param_lambda) + con_ori * param_lambda
-        score_combo = aff_combo * (1 - param_lambda) + con_combo * param_lambda
-
-        upt = paddle.to_tensor((score_ori < score_combo), dtype = 'float32')
-        upt = (upt * mask).reshape((m, m, 1, 1))
-        X = X * (1.0 - upt) + X_combo * upt
-        X = X * X_mask + X.transpose((1, 0, 2, 3)).transpose((0, 1, 3, 2)) * (1 - X_mask)
-    return X
-
-
-def _get_single_pc_opt(X, i, j, Xij=None):
-    """
-    CAO/Floyd helper function (compute consistency)
-    :param X: (m, m, n, n) all the matching results
-    :param i: index
-    :param j: index
-    :return: the consistency of X_ij
-    """
-    m, _, n, _ = X.shape
-    if Xij is None:
-        Xij = X[i, j]
-    X1 = X[i, :].reshape((-1, n, n))
-    X2 = X[:, j].reshape((-1, n, n))
-    X_combo = paddle.bmm(X1, X2)
-    pair_con = 1 - paddle.sum(paddle.abs(Xij - X_combo)) / (2 * n * m)
-    return pair_con
-
-
-def _get_batch_pc_opt(X):
-    """
-    CAO/Floyd-fast helper function (compute consistency in batch)
-    :param X: (m, m, n, n) all the matching results
-    :return: (m, m) the consistency of X
-    """
-    m, _, n, _ = X.shape
-    X1 = X.reshape((m, 1, m, n, n)).tile((1, m, 1, 1, 1)).reshape((-1, n, n))  # X1[i, j, k] = X[i, k]
-    X2 = X.reshape((1, m, m, n, n)).tile((m, 1, 1, 1, 1)).transpose((0, 2, 1, 3, 4))
-    X2 = paddle.reshape(X2, (-1, n, n))  # X2[i, j, k] = X[k, j]
-    X_combo = paddle.bmm(X1, X2).reshape((m, m, m, n, n))
-    X_ori = X.reshape((m, m, 1, n, n)).tile((1, 1, m, 1, 1))
-    pair_con = 1 - paddle.sum(paddle.abs(X_combo - X_ori), axis=(2, 3, 4)) / (2 * n * m)
-    return pair_con
-
-
-def gamgm(
-        A, W, ns, n_univ, U0,
-        init_tau, min_tau, sk_gamma,
-        sk_iter, max_iter, quad_weight,
-        converge_thresh, outlier_thresh, bb_smooth,
-        verbose,
-        cluster_M=None, projector='sinkhorn', hung_iter=True # these arguments are reserved for clustering
-):
-    """
-    Paddle implementation of Graduated Assignment for Multi-Graph Matching (with compatibility for 2GM and clustering)
-    """
-    num_graphs = A.shape[0]
-    if ns is None:
-        ns = paddle.to_tensor(paddle.full((num_graphs,), A.shape[1]), dtype=paddle.int32, place=A.place)
-    n_indices = paddle.cumsum(ns, axis=0)
-
-    # build a super adjacency matrix A
-    supA = paddle.to_tensor(paddle.zeros((n_indices[-1], n_indices[-1])), place=A.place)
-    for i in range(num_graphs):
-        start_n = n_indices[i] - ns[i]
-        end_n = n_indices[i]
-        supA[start_n:end_n, start_n:end_n] = A[i, :ns[i], :ns[i]]
-
-    # handle the type of n_univ
-    if type(n_univ) is paddle.Tensor:
-        n_univ = n_univ.item()
-
-    # randomly init U
-    if U0 is None:
-        U0 = paddle.to_tensor(paddle.full((n_indices[-1], n_univ), 1 / n_univ), place=A.place)
-        U0 += paddle.randn(U0.shape) / 1000
-
-    # init cluster_M if not given
-    if cluster_M is None:
-        cluster_M = paddle.to_tensor(paddle.ones((num_graphs, num_graphs)), place=A.place)
-
-    # reshape W into supW
-    supW = paddle.to_tensor(paddle.zeros((n_indices[-1], n_indices[-1])), place=A.place)
-    for i, j in itertools.product(range(num_graphs), repeat=2):
-        start_x = n_indices[i] - ns[i]
-        end_x = n_indices[i]
-        start_y = n_indices[j] - ns[j]
-        end_y = n_indices[j]
-        supW[start_x:end_x, start_y:end_y] = W[i, j, :ns[i], :ns[j]]
-
-    U = gamgm_real(
-        supA, supW, ns, n_indices, n_univ, num_graphs, U0,
-        init_tau, min_tau, sk_gamma,
-        sk_iter, max_iter, quad_weight,
-        converge_thresh, outlier_thresh,
-        verbose,
-        cluster_M, projector, hung_iter
-    )
-
-    # build MultiMatchingResult
-    result = pygmtools.utils.MultiMatchingResult(True, 'paddle')
-
-    for i in range(num_graphs):
-        start_n = n_indices[i] - ns[i]
-        end_n = n_indices[i]
-        result[i] = U[start_n:end_n]
-
-    return result
-
-
-def gamgm_real(
-        supA, supW, ns, n_indices, n_univ, num_graphs, U0,
-        init_tau, min_tau, sk_gamma,
-        sk_iter, max_iter, quad_weight,
-        converge_thresh, outlier_thresh,
-        verbose,
-        cluster_M, projector, hung_iter # these arguments are reserved for clustering
-        ):
-    """
-    The real forward function of GAMGM
-    """
-    U = U0
-    sinkhorn_tau = init_tau
-    iter_flag = True
-
-    while iter_flag:
-        for i in range(max_iter):
-            # compact matrix form update of V
-            UUt = paddle.mm(U, U.t())
-            lastUUt = UUt
-            cluster_weight = paddle.repeat_interleave(cluster_M, paddle.to_tensor(ns, dtype=paddle.int64), axis=0)
-            cluster_weight = paddle.repeat_interleave(cluster_weight, paddle.to_tensor(ns, dtype=paddle.int64), axis=1)
-            quad = paddle.matmul(paddle.matmul(paddle.matmul(supA, UUt * cluster_weight), supA), U) * quad_weight * 2
-
-            unary = paddle.mm(supW * cluster_weight, U)
-            if verbose:
-                if projector == 'sinkhorn':
-                    print_str = f'tau={sinkhorn_tau:.3e}'
-                else:
-                    print_str = 'hungarian'
-                print(print_str + f' #iter={i}/{max_iter} '
-                      f'quad score: {(quad * U).sum().numpy().squeeze():.3e}, '
-                      f'unary score: {(unary * U).sum().numpy().squeeze():.3e}')
-            V = (quad + unary) / num_graphs
-
-            U_list = []
-            if projector == 'hungarian':
-                n_start = 0
-                for n_end in n_indices:
-                    U_list.append(pygmtools.hungarian(V[n_start:n_end, :n_univ], backend='paddle'))
-                    n_start = n_end
-            elif projector == 'sinkhorn':
-                if paddle.all(ns == ns[0]):
-                    if ns[0] <= n_univ:
-                        U_list.append(
-                            sinkhorn(
-                                V.reshape((num_graphs, -1, n_univ)),
-                                max_iter=sk_iter, tau=sinkhorn_tau, batched_operation=True, dummy_row=True
-                            ).reshape((-1, n_univ)))
-                    else:
-                        U_list.append(
-                            sinkhorn(
-                                V.reshape((num_graphs, -1, n_univ)).transpose((0, 2, 1)),
-                                max_iter=sk_iter, tau=sinkhorn_tau, batched_operation=True, dummy_row=True
-                            ).transpose((0, 2, 1)).reshape((-1, n_univ)))
-                else:
-                    V_list = []
-                    n1 = []
-                    n_start = 0
-                    for n_end in n_indices:
-                        V_list.append(V[n_start:n_end, :n_univ])
-                        n1.append(n_end - n_start)
-                        n_start = n_end
-                    V_batch = build_batch(V_list)
-                    n1 = paddle.to_tensor(n1, place=V_batch.place)
-                    U = sinkhorn(V_batch, n1,
-                                 max_iter=sk_iter, tau=sinkhorn_tau, batched_operation=True, dummy_row=True)
-                    n_start = 0
-                    for idx, n_end in enumerate(n_indices):
-                        U_list.append(U[idx, :n_end - n_start, :])
-                        n_start = n_end
-            else:
-                raise NameError('Unknown projecter name: {}'.format(projector))
-
-            U = paddle.concat(U_list, axis=0)
-            if num_graphs == 2:
-                U[:ns[0], :] = paddle.to_tensor(paddle.eye(ns[0], n_univ), place=U.place)
-
-            # calculate gap to discrete
-            if projector == 'sinkhorn' and verbose:
-                U_list_hung = []
-                n_start = 0
-                for n_end in n_indices:
-                    U_list_hung.append(pygmtools.hungarian(V[n_start:n_end, :n_univ], backend='paddle'))
-                    n_start = n_end
-                U_hung = paddle.concat(U_list_hung, axis=0)
-                diff = paddle.linalg.norm(paddle.mm(U, U.t()) - lastUUt)
-                print(f'tau={sinkhorn_tau:.3e} #iter={i}/{max_iter} '
-                      f'gap to discrete: {paddle.mean(paddle.abs(U - U_hung)).numpy().squeeze():.3e}, '
-                      f'iter diff: {diff.numpy().squeeze():.3e}')
-
-            if projector == 'hungarian' and outlier_thresh > 0:
-                U_hung = U
-                UUt = paddle.mm(U_hung, U_hung.t())
-                cluster_weight = paddle.repeat_interleave(cluster_M, paddle.to_tensor(ns, dtype=paddle.int64), axis=0)
-                cluster_weight = paddle.repeat_interleave(cluster_weight, paddle.to_tensor(ns, dtype=paddle.int64), axis=1)
-                quad = paddle.matmul(paddle.matmul(paddle.matmul(supA, UUt * cluster_weight), supA), U_hung) * quad_weight * 2
-                unary = paddle.mm(supW * cluster_weight, U_hung)
-                max_vals = (unary + quad).max(axis=1)
-                U = U * (unary + quad > outlier_thresh)
-                if verbose:
-                    print(f'hungarian #iter={i}/{max_iter} '
-                          f'unary+quad score thresh={outlier_thresh:.3f}, '
-                          f'#>thresh={paddle.sum(max_vals > outlier_thresh).numpy().squeeze()}/{max_vals.shape[0]} '
-                          f'min:{max_vals.min().numpy().squeeze():.4f}, '
-                          f'mean:{max_vals.mean().numpy().squeeze():.4f}, '
-                          f'median:{max_vals.median().numpy().squeeze():.4f}, '
-                          f'max:{max_vals.max().numpy().squeeze():.4f}')
-
-            if paddle.linalg.norm(paddle.mm(U, U.t()) - lastUUt) < converge_thresh:
-                break
-
-        if verbose: print('-' * 20)
-
-        if i == max_iter - 1: # not converged
-            if hung_iter:
-                pass
-            else:
-                U_list = [pygmtools.hungarian(_, backend='paddle') for _ in U_list]
-                U = paddle.concat(U_list, axis=0)
-                break
-
-        # projection control
-        if projector == 'hungarian':
-            break
-        elif sinkhorn_tau > min_tau:
-            sinkhorn_tau *= sk_gamma
-        else:
-            if hung_iter:
-                projector = 'hungarian'
-            else:
-                U_list = [pygmtools.hungarian(_, backend='paddle') for _ in U_list]
-                U = paddle.concat(U_list, axis=0)
-                break
-
-    return U
-
-
-############################################
-#          Neural Network Solvers          #
-############################################
-
-from pygmtools.paddle_modules import *
-
-class PCA_GM_Net(paddle.nn.Layer):
-    """
-    Paddle implementation of PCA-GM and IPCA-GM network
-    """
-    def __init__(self, in_channel, hidden_channel, out_channel, num_layers, cross_iter_num=-1):
-        super(PCA_GM_Net, self).__init__()
-        self.gnn_layer = num_layers
-        for i in range(self.gnn_layer):
-            if i == 0:
-                gnn_layer = Siamese_Gconv(in_channel, hidden_channel)
-            elif 0 < i < self.gnn_layer - 1:
-                gnn_layer = Siamese_Gconv(hidden_channel, hidden_channel)
-            else:
-                gnn_layer = Siamese_Gconv(hidden_channel, out_channel)
-                self.add_sublayer('affinity_{}'.format(i), WeightedInnerProdAffinity(out_channel))
-            self.add_sublayer('gnn_layer_{}'.format(i), gnn_layer)
-            if i == self.gnn_layer - 2:  # only the second last layer will have cross-graph module
-                self.add_sublayer('cross_graph_{}'.format(i), paddle.nn.Linear(hidden_channel * 2, hidden_channel, weight_attr=weight_init))
-                if cross_iter_num <= 0:
-                 self.add_sublayer('affinity_{}'.format(i), WeightedInnerProdAffinity(hidden_channel))
-
-
-    def forward(self, feat1, feat2, A1, A2, n1, n2, cross_iter_num, sk_max_iter, sk_tau):
-        _sinkhorn_func = functools.partial(sinkhorn,
-                                           dummy_row=False, max_iter=sk_max_iter, tau=sk_tau, batched_operation=False)
-        emb1, emb2 = feat1, feat2
-        if cross_iter_num <= 0:
-            # Vanilla PCA-GM
-            for i in range(self.gnn_layer):
-                gnn_layer = getattr(self, 'gnn_layer_{}'.format(i))
-                emb1, emb2 = gnn_layer([A1, emb1], [A2, emb2])
-
-                if i == self.gnn_layer - 2:
-                    affinity = getattr(self, 'affinity_{}'.format(i))
-                    s = affinity(emb1, emb2)
-                    s = _sinkhorn_func(s, n1, n2)
-                    
-                    cross_graph = getattr(self, 'cross_graph_{}'.format(i))
-                    new_emb1 = cross_graph(paddle.concat((emb1, paddle.bmm(s, emb2)), axis=-1))
-                    new_emb2 = cross_graph(paddle.concat((emb2, paddle.bmm(s.transpose([0, 2, 1]), emb1)), axis=-1))
-                    emb1 = new_emb1
-                    emb2 = new_emb2
-
-            affinity = getattr(self, 'affinity_{}'.format(self.gnn_layer - 1))
-            s = affinity(emb1, emb2)
-            s = _sinkhorn_func(s, n1, n2)
-
-        else:
-            # IPCA-GM
-            for i in range(self.gnn_layer - 1):
-                gnn_layer = getattr(self, 'gnn_layer_{}'.format(i))
-                emb1, emb2 = gnn_layer([A1, emb1], [A2, emb2])
-
-            emb1_0, emb2_0 = emb1, emb2
-            s = paddle.zeros((emb1.shape[0], emb1.shape[1], emb2.shape[1]))
-
-            for x in range(cross_iter_num):
-                # cross-graph convolution in second last layer
-                i = self.gnn_layer - 2
-                cross_graph = getattr(self, 'cross_graph_{}'.format(i))
-                emb1 = cross_graph(paddle.concat((emb1_0, paddle.bmm(s, emb2_0)), axis=-1))
-                emb2 = cross_graph(paddle.concat((emb2_0, paddle.bmm(s.transpose([0,2, 1]), emb1_0)), axis=-1))
-
-                # last layer
-                i = self.gnn_layer - 1
-                gnn_layer = getattr(self, 'gnn_layer_{}'.format(i))
-                emb1, emb2 = gnn_layer([A1, emb1], [A2, emb2])
-                affinity = getattr(self, 'affinity_{}'.format(i))
-                s = affinity(emb1, emb2)
-                s = _sinkhorn_func(s, n1, n2)
-
-        return s
-
-pca_gm_pretrain_path = {
-    'voc': ('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=1PoeWfa4v3n4Bk_9VlSUSd7akZf1rL8ct',
-            '03b1dedeed7195aa98431b3c561d5de3'),
-    'willow': ('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=1hgCpvcvt5eoz1xbMbuDyBuYH6SCue6UD',
-               'ebf2dae8593a3640012832858bec3499'),
-    'voc-all': ('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=116_v4rC31T-hq3kE2d_thMKmJ65swrCD',
-                '677c03d7180fefaaee9fcc85a03c8d53')
-}
-
-def pca_gm(feat1, feat2, A1, A2, n1, n2,
-           in_channel, hidden_channel, out_channel, num_layers, sk_max_iter, sk_tau,
-           network, pretrain):
-    """
-    Paddle implementation of PCA-GM
-    """
-    if feat1 is None:
-        forward_pass = False
-        device = 'cpu'
-    else:
-        forward_pass = True
-        device = paddle.device.get_device()
-    if network is None:
-        network = PCA_GM_Net(in_channel, hidden_channel, out_channel, num_layers)
-        network = network.to(device)
-        if pretrain:
-            if pretrain in pca_gm_pretrain_path:
-                url, md5 = pca_gm_pretrain_path[pretrain]
-                filename = pygmtools.utils.download(f'pca_gm_{pretrain}_paddle.pdparams', url, md5)
-                _load_model(network, filename)
-            else:
-                raise ValueError(f'Unknown pretrain tag. Available tags: {pca_gm_pretrain_path.keys()}')
-    if forward_pass:
-        batch_size = feat1.shape[0]
-        if n1 is None:
-            n1 = paddle.to_tensor([feat1.shape[1]] * batch_size)
-        if n2 is None:
-            n2 = paddle.to_tensor([feat2.shape[1]] * batch_size)  
-        result = network(feat1, feat2, A1, A2, n1, n2, -1, sk_max_iter, sk_tau)
-    else:
-        result = None
-    return result, network
-
-
-ipca_gm_pretrain_path = { 
-    'voc': ('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=1bVGl9lhhzkLeWKsjiWYHgzFV-evCo1sh',
-            '2fb842d4fbdeed60ac2846201a24f771'),
-    'willow': ('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=17RdDzNp2SjYahRd9aep5dEbidWJNR4uu',
-               '763ea7b3c0518e27ca16c5ae987eccaa'),
-}
-
-def ipca_gm(feat1, feat2, A1, A2, n1, n2,
-           in_channel, hidden_channel, out_channel, num_layers, cross_iter, sk_max_iter, sk_tau,
-           network, pretrain):
-    """
-    Paddle implementation of IPCA-GM
-    """
-    if feat1 is None:
-        forward_pass = False
-        device = 'cpu'
-    else:
-        forward_pass = True
-        device = paddle.device.get_device()
-    if network is None:
-        network = PCA_GM_Net(in_channel, hidden_channel, out_channel, num_layers, cross_iter)
-        network = network.to(device)
-        if pretrain:
-            if pretrain in ipca_gm_pretrain_path:
-                url, md5 = ipca_gm_pretrain_path[pretrain]
-                filename = pygmtools.utils.download(f'ipca_gm_{pretrain}_paddle.pdparams', url, md5)
-                _load_model(network, filename)
-            else:
-                raise ValueError(f'Unknown pretrain tag. Available tags: {ipca_gm_pretrain_path.keys()}')
-    if forward_pass:
-        batch_size = feat1.shape[0]
-        if n1 is None:
-            n1 = paddle.to_tensor([feat1.shape[1]] * batch_size)
-        if n2 is None:
-            n2 = paddle.to_tensor([feat2.shape[1]] * batch_size)
-        result = network(feat1, feat2, A1, A2, n1, n2, cross_iter, sk_max_iter, sk_tau)
-    else:
-        result = None
-    return result, network
-
-class CIE_Net(paddle.nn.Layer):
-    """
-    Paddle implementation of CIE graph matching network
-    """
-    def __init__(self, in_node_channel, in_edge_channel, hidden_channel, out_channel, num_layers):
-        super(CIE_Net, self).__init__()
-        self.gnn_layer = num_layers
-        for i in range(self.gnn_layer):
-            if i == 0:
-                gnn_layer = Siamese_ChannelIndependentConv(in_node_channel, hidden_channel, in_edge_channel)
-            elif 0 < i < self.gnn_layer - 1:
-                gnn_layer = Siamese_ChannelIndependentConv(hidden_channel, hidden_channel, hidden_channel)
-            else:
-                gnn_layer = Siamese_ChannelIndependentConv(hidden_channel, out_channel, hidden_channel)
-                self.add_sublayer('affinity_{}'.format(i), WeightedInnerProdAffinity(out_channel))
-            self.add_sublayer('gnn_layer_{}'.format(i), gnn_layer)
-            if i == self.gnn_layer - 2:  # only the second last layer will have cross-graph module
-                self.add_sublayer('cross_graph_{}'.format(i), paddle.nn.Linear(hidden_channel * 2, hidden_channel, weight_attr=weight_init))
-                self.add_sublayer('affinity_{}'.format(i), WeightedInnerProdAffinity(hidden_channel))
-
-    def forward(self, feat_node1, feat_node2, A1, A2, feat_edge1, feat_edge2, n1, n2, sk_max_iter, sk_tau):
-        _sinkhorn_func = functools.partial(sinkhorn,
-                                           dummy_row=False, max_iter=sk_max_iter, tau=sk_tau, batched_operation=False)
-        emb1, emb2 = feat_node1, feat_node2
-        emb_edge1, emb_edge2 = feat_edge1, feat_edge2
-        for i in range(self.gnn_layer):
-            gnn_layer = getattr(self, 'gnn_layer_{}'.format(i))
-            # during forward process, the network structure will not change
-            emb1, emb2, emb_edge1, emb_edge2 = gnn_layer([A1, emb1, emb_edge1], [A2, emb2, emb_edge2])
-
-            if i == self.gnn_layer - 2:
-                affinity = getattr(self, 'affinity_{}'.format(i))
-                s = affinity(emb1, emb2)
-                s = _sinkhorn_func(s, n1, n2)
-
-                cross_graph = getattr(self, 'cross_graph_{}'.format(i))
-                new_emb1 = cross_graph(paddle.concat((emb1, paddle.bmm(s, emb2)), axis=-1))
-                new_emb2 = cross_graph(paddle.concat((emb2, paddle.bmm(s.transpose([0,2, 1]), emb1)), axis=-1))
-                emb1 = new_emb1
-                emb2 = new_emb2
-
-        affinity = getattr(self, 'affinity_{}'.format(self.gnn_layer - 1))
-        s = affinity(emb1, emb2)
-        s = _sinkhorn_func(s, n1, n2)
-        return s
-
-cie_pretrain_path = {
-    'voc': ('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=13dtxDfySvfqQcYvPiTd8Pb2xgcXIesAz',
-            '2c52c70e4a8919d24fde261756d1d6c4'),
-    'willow': ('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=1_-G00V3yhJ3IL_Xp6cMcVV9N2vWgEDwk',
-               '2619383120ca67d68c40eebd9dde9d95'),
-}
-
-def cie(feat_node1, feat_node2, A1, A2, feat_edge1, feat_edge2, n1, n2,
-        in_node_channel, in_edge_channel, hidden_channel, out_channel, num_layers, sk_max_iter, sk_tau,
-        network, pretrain):
-    """
-    Paddle implementation of CIE
-    """
-    if feat_node1 is None:
-        forward_pass = False
-        device = 'cpu'
-    else:
-        forward_pass = True
-        device = paddle.device.get_device()
-    if network is None:
-        network = CIE_Net(in_node_channel, in_edge_channel, hidden_channel, out_channel, num_layers)
-        network = network.to(device)
-        if pretrain:
-            if pretrain in cie_pretrain_path:
-                url, md5 = cie_pretrain_path[pretrain]
-                filename = pygmtools.utils.download(f'cie_{pretrain}_paddle.pdparams', url, md5)
-                _load_model(network, filename)
-            else:
-                raise ValueError(f'Unknown pretrain tag. Available tags: {cie_pretrain_path.keys()}')
-
-    if forward_pass:
-        batch_size = feat_node1.shape[0]
-        if n1 is None:
-            n1 = paddle.to_tensor([feat_node1.shape[1]] * batch_size)
-        if n2 is None:
-            n2 = paddle.to_tensor([feat_node1.shape[1]] * batch_size)
-        result = network(feat_node1, feat_node2, A1, A2, feat_edge1, feat_edge2, n1, n2, sk_max_iter, sk_tau)
-    else:
-        result = None
-    return result, network
-
-
-class NGM_Net(paddle.nn.Layer):
-    """
-    Paddle implementation of NGM network
-    """
-
-    def __init__(self, gnn_channels, sk_emb):
-        super(NGM_Net, self).__init__()
-        self.gnn_layer = len(gnn_channels)
-        for i in range(self.gnn_layer):
-            if i == 0:
-                gnn_layer = NGMConvLayer(1, 1,
-                                         gnn_channels[i] + sk_emb, gnn_channels[i],
-                                         sk_channel=sk_emb)
-            else:
-                gnn_layer = NGMConvLayer(gnn_channels[i - 1] + sk_emb, gnn_channels[i - 1],
-                                         gnn_channels[i] + sk_emb, gnn_channels[i],
-                                         sk_channel=sk_emb)
-            self.add_sublayer('gnn_layer_{}'.format(i), gnn_layer)
-        self.classifier = nn.Linear(gnn_channels[-1] + sk_emb, 1)
-
-    def forward(self, K, n1, n2, n1max, n2max, v0, sk_max_iter, sk_tau):
-        _sinkhorn_func = functools.partial(sinkhorn,
-                                           dummy_row=False, max_iter=sk_max_iter, tau=sk_tau, batched_operation=False)
-        emb = v0
-        A = paddle.cast((K != 0), K.dtype)
-        emb_K = K.unsqueeze(-1)
-
-        # NGM qap solver
-        for i in range(self.gnn_layer):
-            gnn_layer = getattr(self, f'gnn_layer_{i}')
-            emb_K, emb = gnn_layer(A, emb_K, emb, n1, n2, sk_func=_sinkhorn_func)
-
-        v = self.classifier(emb)
-        s = v.reshape([v.shape[0], n2max, -1]).transpose((0, 2, 1))
-
-        return _sinkhorn_func(s, n1, n2, dummy_row=True)
-
-ngm_pretrain_path = { 
-    'voc': ('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=1cd5AvddQtpWmENPLEWjJ76cKG7Df43nh',
-            'bf1808dd16304e03ff33133c7ea9de90'),
-    'willow': ('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=1j1kXWsassE3bAVWjPy2g0jUGG2IeODc8',
-               'a5daf06cdaf6cc370928b5f8fc01c585'),
-}
-
-def ngm(K, n1, n2, n1max, n2max, x0, gnn_channels, sk_emb, sk_max_iter, sk_tau, network, return_network, pretrain):
-    """
-    Paddle implementation of NGM
-    """
-    if K is None:
-        forward_pass = False
-        device = 'cpu'
-    else:
-        forward_pass = True
-        device = paddle.device.get_device()
-    if network is None:
-        network = NGM_Net(gnn_channels, sk_emb)
-        network = network.to(device)
-        if pretrain:
-            if pretrain in ngm_pretrain_path:
-                url, md5 = ngm_pretrain_path[pretrain]
-                try:
-                    filename = pygmtools.utils.download(f'ngm_{pretrain}_paddle.pdparams', url, md5)
-                except:
-                    filename = os.path.dirname(__file__) + f'/temp/ngm_{pretrain}_paddle.pdparams'
-                _load_model(network, filename)
-            else:
-                raise ValueError(f'Unknown pretrain tag. Available tags: {ngm_pretrain_path.keys()}')
-
-    if forward_pass:
-        batch_num, n1, n2, n1max, n2max, n1n2, v0 = _check_and_init_gm(K, n1, n2, n1max, n2max, x0)
-        v0 = v0 / paddle.mean(v0)
-        result = network(K, n1, n2, n1max, n2max, v0, sk_max_iter, sk_tau)
-    else:
-        result = None
-    return result, network
-
-
-#############################################
-#              Utils Functions              #
-#############################################
-
-
-def inner_prod_aff_fn(feat1, feat2):
-    """
-    Paddle implementation of inner product affinity function
-    """
-    return paddle.matmul(feat1, feat2.transpose((0, 2, 1)))
-
-
-def gaussian_aff_fn(feat1, feat2, sigma):
-    """
-    Paddle implementation of Gaussian affinity function
-    """
-    feat1 = feat1.unsqueeze(2)
-    feat2 = feat2.unsqueeze(1)
-    return paddle.exp(-((feat1 - feat2) ** 2).sum(axis=-1) / sigma)
-
-
-def build_batch(input, return_ori_dim=False):
-    """
-    Paddle implementation of building a batched tensor
-    """
-    assert type(input[0]) == paddle.Tensor
-    device = input[0].place
-    it = iter(input)
-    t = next(it)
-    max_shape = list(t.shape)
-    ori_shape = [[_] for _ in max_shape]
-    while True:
-        try:
-            t = next(it)
-            for i in range(len(max_shape)):
-                max_shape[i] = int(max(max_shape[i], t.shape[i]))
-                ori_shape[i].append(t.shape[i])
-        except StopIteration:
-            break
-    max_shape = np.array(max_shape)
-
-    padded_ts = []
-    for t in input:
-        pad_pattern = np.zeros((len(max_shape), 2), dtype=np.int64)
-        pad_pattern[:, 1] = max_shape - np.array(t.shape)
-        padded_ts.append(np.pad(t, pad_pattern, 'constant', constant_values=0))
-
-    if return_ori_dim:
-        return paddle.to_tensor(np.stack(padded_ts, axis=0)), tuple([paddle.to_tensor(_, dtype=paddle.int64, place=device) for _ in ori_shape])
-    else:
-        return paddle.to_tensor(np.stack(padded_ts, axis=0))
-
-
-def dense_to_sparse(dense_adj):
-    """
-    Paddle implementation of converting a dense adjacency matrix to a sparse matrix
-    """
-    batch_size = dense_adj.shape[0]
-    conn, ori_shape = build_batch([paddle.nonzero(a, as_tuple=False) for a in dense_adj], return_ori_dim=True)
-    nedges = ori_shape[0]
-    edge_weight = build_batch([dense_adj[b][(conn[b, :, 0], conn[b, :, 1])] for b in range(batch_size)])
-    return conn, paddle.unsqueeze(edge_weight, axis=-1), nedges
-
-
-def compute_affinity_score(X, K):
-    """
-    Paddle implementation of computing affinity score
-    """
-    b, n, _ = X.shape
-    vx = paddle.reshape(X.transpose((0, 2, 1)),(b, -1, 1)) # (b, n*n, 1)
-    vxt = vx.transpose((0, 2, 1))  # (b, 1, n*n)
-    affinity = paddle.bmm(paddle.bmm(vxt, K), vx)
-    return affinity
-
-
-def to_numpy(input):
-    """
-    Paddle function to_numpy
-    """
-    return input.detach().cpu().numpy()
-
-
-def from_numpy(input, device):
-    """
-    Paddle function from_numpy
-    """
-    if device is None:
-        return paddle.to_tensor(input)
-    else:
-        return paddle.to_tensor(input, place=device)
-
-
-def generate_isomorphic_graphs(node_num, graph_num, node_feat_dim):
-    """
-    Paddle implementation of generate_isomorphic_graphs
-    """
-    X_gt = paddle.zeros((graph_num, node_num, node_num))
-    X_gt[0, paddle.arange(0, node_num, dtype=paddle.int64), paddle.arange(0, node_num, dtype=paddle.int64)] = 1
-    for i in range(graph_num):
-        if i > 0:
-            X_gt[i, paddle.arange(0, node_num, dtype=paddle.int64), paddle.randperm(node_num)] = 1
-    joint_X = paddle.reshape(X_gt, (graph_num * node_num, node_num))
-    X_gt = paddle.mm(joint_X, paddle.t(joint_X))
-    X_gt = paddle.transpose(paddle.reshape(X_gt, (graph_num, node_num, graph_num, node_num)), perm=(0, 2, 1, 3))
-    A0 = paddle.rand((node_num, node_num))
-    paddle.diagonal(A0)[:] = 0
-    As = [A0]
-    for i in range(graph_num):
-        if i > 0:
-            As.append(paddle.mm(paddle.mm(X_gt[i, 0], A0), X_gt[0, i]))
-    if node_feat_dim > 0:
-        F0 = paddle.rand((node_num, node_feat_dim))
-        Fs = [F0]
-        for i in range(graph_num):
-            if i > 0:
-                Fs.append(paddle.mm(X_gt[i, 0], F0))
-        return paddle.stack(As, axis=0), X_gt, paddle.stack(Fs, axis=0)
-    else:
-        return paddle.stack(As, axis=0), X_gt
-
-
-def permutation_loss(pred_dsmat, gt_perm, n1, n2):
-    """
-    Paddle implementation of permutation_loss
-    """
-    batch_num = pred_dsmat.shape[0]
-
-    pred_dsmat = pred_dsmat.astype("float32")
-
-    if not ((pred_dsmat >= 0) * (pred_dsmat <= 1)).all():
-        raise ValueError("pred_dsmat contains invalid numerical entries.")
-    if not ((gt_perm >= 0) * (gt_perm <= 1)).all():
-        raise ValueError("gt_perm contains invalid numerical entries.")
-
-    if n1 is None:
-        n1 = paddle.to_tensor([pred_dsmat.shape[1] for _ in range(batch_num)], place=pred_dsmat.place)
-    if n2 is None:
-        n2 = paddle.to_tensor([pred_dsmat.shape[2] for _ in range(batch_num)], place=pred_dsmat.place)
-
-    loss = paddle.to_tensor(0., place=pred_dsmat.place)
-    n_sum = paddle.zeros_like(loss)
-    for b in range(batch_num):
-        batch_slice = [b, slice(n1[b]), slice(n2[b])]
-        loss += paddle.nn.functional.binary_cross_entropy(
-            pred_dsmat[batch_slice],
-            gt_perm[batch_slice],
-            reduction='sum')
-        n_sum += paddle.to_tensor(n1[b].astype(n_sum.dtype), place=pred_dsmat.place)
-
-    return loss / n_sum
-
-
-def _aff_mat_from_node_edge_aff(node_aff: paddle.Tensor, edge_aff: paddle.Tensor, connectivity1: paddle.Tensor, connectivity2: paddle.Tensor,
-                                n1, n2, ne1, ne2):
-    """
-    Paddle implementation of _aff_mat_from_node_edge_aff
-    """
-    if edge_aff is not None:
-        device = edge_aff.place
-        dtype = edge_aff.dtype
-        batch_size = edge_aff.shape[0]
-        if n1 is None:
-            n1 = paddle.to_tensor(np.amax(connectivity1.numpy(), axis=(1, 2)).copy() + 1)
-        if n2 is None:
-            n2 = paddle.to_tensor(np.amax(connectivity2.numpy(), axis=(1, 2)).copy() + 1)
-        if ne1 is None:
-            ne1 = [edge_aff.shape[1]] * batch_size
-        if ne2 is None:
-            ne2 = [edge_aff.shape[2]] * batch_size
-    else:
-        device = node_aff.place
-        dtype = node_aff.dtype
-        batch_size = node_aff.shape[0]
-        if n1 is None:
-            n1 = [node_aff.shape[1]] * batch_size
-        if n2 is None:
-            n2 = [node_aff.shape[2]] * batch_size
-
-    n1max = max(n1)
-    n2max = max(n2)
-    ks = []
-    for b in range(batch_size):
-        k = paddle.to_tensor(paddle.zeros((n2max, n1max, n2max, n1max), dtype=dtype), place=device)
-        # edge-wise affinity
-        if edge_aff is not None:
-            conn1 = connectivity1[b][:ne1[b]].numpy()
-            conn2 = connectivity2[b][:ne2[b]].numpy()
-            edge_indices = np.concatenate([conn1.repeat(ne2[b], axis=0), np.tile(conn2, (ne1[b], 1))], axis=1) # indices: start_g1, end_g1, start_g2, end_g2
-            edge_indices = (edge_indices[:, 2], edge_indices[:, 0], edge_indices[:, 3], edge_indices[:, 1]) # indices: start_g2, start_g1, end_g2, end_g1
-            k[edge_indices] = edge_aff[b, :ne1[b], :ne2[b]].reshape([-1])
-        k = k.reshape((n2max * n1max, n2max * n1max))
-        # node-wise affinity
-        if node_aff is not None:
-            k[np.arange(n2max * n1max), np.arange(n2max * n1max)] = node_aff[b].transpose((1, 0)).reshape([-1])
-        ks.append(k)
-    return paddle.stack(ks, axis=0)
-
-
-def _check_data_type(input: paddle.Tensor, var_name, raise_err):
-    """
-    Paddle implementation of _check_data_type
-    """
-    if raise_err and type(input) is not paddle.Tensor:
-        raise ValueError(f'Expected Paddle Tensor{f" for variable {var_name}" if var_name is not None else ""}, '
-                         f'but got {type(input)}. Perhaps the wrong backend?')
-    return type(input) is paddle.Tensor
-
-
-def _check_shape(input, dim_num):
-    """
-    Paddle implementation of _check_shape
-    """
-    return len(input.shape) == dim_num
-
-
-def _get_shape(input):
-    """
-    Paddle implementation of _get_shape
-    """
-    return input.shape
-
-
-def _squeeze(input, dim):
-    """
-    Paddle implementation of _squeeze
-    """
-    return paddle.squeeze(input, axis=dim)
-
-
-def _unsqueeze(input, dim):
-    """
-    Paddle implementation of _unsqueeze
-    """
-    return paddle.unsqueeze(input, axis=dim)
-
-
-def _transpose(input, dim1, dim2):
-    """
-    Paddle implementation of _transpose
-    """
-    return paddle.transpose(input, (dim2, dim1))
-
-
-def _mm(input1, input2):
-    """
-    Paddle implementation of _mm
-    """
-    return paddle.mm(input1, input2)
-
-def _load_model(model, path):
-    """
-    Load Paddle model from a given path. Unmatched keys shall be shown in paddle warning.
-    """
-    module = model
-    module.set_dict(paddle.load(path))
+# Copyright (c) 2022 Thinklab@SJTU
+# pygmtools is licensed under Mulan PSL v2.
+# You can use this software according to the terms and conditions of the Mulan PSL v2.
+# You may obtain a copy of Mulan PSL v2 at:
+# http://license.coscl.org.cn/MulanPSL2
+# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND,
+# EITHER EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT,
+# MERCHANTABILITY OR FIT FOR A PARTICULAR PURPOSE.
+# See the Mulan PSL v2 for more details.
+
+import itertools
+import functools
+import scipy.special
+import scipy.optimize
+import numpy as np
+import os
+from multiprocessing import Pool
+import pygmtools.utils
+
+#############################################
+#     Linear Assignment Problem Solvers     #
+#############################################
+
+
+def hungarian(s: np.ndarray, n1: np.ndarray=None, n2: np.ndarray=None,
+              unmatch1: np.ndarray=None, unmatch2: np.ndarray=None,
+              nproc: int=1) -> np.ndarray:
+    """
+    numpy implementation of Hungarian algorithm
+    """
+    batch_num = s.shape[0]
+
+    perm_mat = -s
+    if n1 is None:
+        n1 = [None] * batch_num
+    if n2 is None:
+        n2 = [None] * batch_num
+    if unmatch1 is not None:
+        unmatch1 = -unmatch1
+    else:
+        unmatch1 = [None] * batch_num
+    if unmatch2 is not None:
+        unmatch2 = -unmatch2
+    else:
+        unmatch2 = [None] * batch_num
+
+    if nproc > 1:
+        with Pool(processes=nproc) as pool:
+            perm_mat = [_ for _ in perm_mat]
+            mapresult = pool.starmap_async(_hung_kernel, zip(perm_mat, n1, n2, unmatch1, unmatch2))
+            perm_mat = np.stack(mapresult.get())
+    else:
+        perm_mat = np.stack([_hung_kernel(perm_mat[b], n1[b], n2[b], unmatch1[b], unmatch2[b]) for b in range(batch_num)])
+
+    return perm_mat
+
+
+def _hung_kernel(s: np.ndarray, n1=None, n2=None, unmatch1=None, unmatch2=None):
+    """
+    Hungarian kernel function by calling the linear sum assignment solver from Scipy.
+    """
+    if n1 is None:
+        n1 = s.shape[0]
+    if n2 is None:
+        n2 = s.shape[1]
+    if unmatch1 is not None and unmatch2 is not None:
+        upper_left = s[:n1, :n2]
+        upper_right = np.full((n1, n1), float('inf'))
+        np.fill_diagonal(upper_right, unmatch1[:n1])
+        lower_left = np.full((n2, n2), float('inf'))
+        np.fill_diagonal(lower_left, unmatch2[:n2])
+        lower_right = np.zeros((n2, n1))
+
+        large_cost_mat = np.concatenate((np.concatenate((upper_left, upper_right), axis=1),
+                                         np.concatenate((lower_left, lower_right), axis=1)), axis=0)
+
+        row, col = scipy.optimize.linear_sum_assignment(large_cost_mat)
+        valid_idx = np.logical_and(row < n1, col < n2)
+        row = row[valid_idx]
+        col = col[valid_idx]
+    else:
+        row, col = scipy.optimize.linear_sum_assignment(s[:n1, :n2])
+    perm_mat = np.zeros_like(s)
+    perm_mat[row, col] = 1
+    return perm_mat
+
+
+def sinkhorn(s: np.ndarray, nrows: np.ndarray=None, ncols: np.ndarray=None,
+             unmatchrows: np.ndarray=None, unmatchcols: np.ndarray=None,
+             dummy_row: bool=False, max_iter: int=10, tau: float=1., batched_operation: bool=False) -> np.ndarray:
+    """
+    numpy implementation of Sinkhorn algorithm
+    """
+    batch_size = s.shape[0]
+
+    if s.shape[2] >= s.shape[1]:
+        transposed = False
+    else:
+        s = s.transpose((0, 2, 1))
+        nrows, ncols = ncols, nrows
+        unmatchrows, unmatchcols = unmatchcols, unmatchrows
+        transposed = True
+
+    if nrows is None:
+        nrows = np.array([s.shape[1] for _ in range(batch_size)], dtype=int)
+    if ncols is None:
+        ncols = np.array([s.shape[2] for _ in range(batch_size)], dtype=int)
+
+    # ensure that in each dimension we have nrow < ncol
+    transposed_batch = nrows > ncols
+    if np.any(transposed_batch):
+        s_t = s.transpose((0, 2, 1))
+        s_t = np.concatenate((
+            s_t[:, :s.shape[1], :],
+            np.full((batch_size, s.shape[1], s.shape[2]-s.shape[1]), -float('inf'))), axis=2)
+        s = np.where(transposed_batch.reshape(batch_size, 1, 1), s_t, s)
+
+        new_nrows = np.where(transposed_batch, ncols, nrows)
+        new_ncols = np.where(transposed_batch, nrows, ncols)
+        nrows = new_nrows
+        ncols = new_ncols
+
+        if unmatchrows is not None and unmatchcols is not None:
+            unmatchrows_pad = np.concatenate((
+                unmatchrows, np.full((batch_size, unmatchcols.shape[1] - unmatchrows.shape[1]), -float('inf'))),
+            axis=1)
+            new_unmatchrows = np.where(transposed_batch.reshape(batch_size, 1), unmatchcols, unmatchrows_pad)[:, :unmatchrows.shape[1]]
+            new_unmatchcols = np.where(transposed_batch.reshape(batch_size, 1), unmatchrows_pad, unmatchcols)
+            unmatchrows = new_unmatchrows
+            unmatchcols = new_unmatchcols
+
+    # operations are performed on log_s
+    log_s = s / tau
+    if unmatchrows is not None and unmatchcols is not None:
+        unmatchrows = unmatchrows / tau
+        unmatchcols = unmatchcols / tau
+
+    if dummy_row:
+        assert log_s.shape[2] >= log_s.shape[1]
+        dummy_shape = list(log_s.shape)
+        dummy_shape[1] = log_s.shape[2] - log_s.shape[1]
+        ori_nrows = nrows
+        nrows = ncols.copy()
+        log_s = np.concatenate((log_s, np.full(dummy_shape, -float('inf'))), axis=1)
+        if unmatchrows is not None:
+            unmatchrows = np.concatenate((unmatchrows, np.full((dummy_shape[0], dummy_shape[1]), -float('inf'))), axis=1)
+        for b in range(batch_size):
+            log_s[b, ori_nrows[b]:nrows[b], :ncols[b]] = -100
+
+    # assign the unmatch weights
+    if unmatchrows is not None and unmatchcols is not None:
+        new_log_s = np.full((log_s.shape[0], log_s.shape[1]+1, log_s.shape[2]+1), -float('inf'))
+        new_log_s[:, :-1, :-1] = log_s
+        log_s = new_log_s
+        for b in range(batch_size):
+            log_s[b, :nrows[b], ncols[b]] = unmatchrows[b, :nrows[b]]
+            log_s[b, nrows[b], :ncols[b]] = unmatchcols[b, :ncols[b]]
+    row_mask = np.zeros((batch_size, log_s.shape[1], 1), dtype=bool)
+    col_mask = np.zeros((batch_size, 1, log_s.shape[2]), dtype=bool)
+    for b in range(batch_size):
+        row_mask[b, :nrows[b], 0] = 1
+        col_mask[b, 0, :ncols[b]] = 1
+    if unmatchrows is not None and unmatchcols is not None:
+        ncols += 1
+        nrows += 1
+
+    if batched_operation:
+        for b in range(batch_size):
+            log_s[b, nrows[b]:, :] = -float('inf')
+            log_s[b, :, ncols[b]:] = -float('inf')
+
+        for i in range(max_iter):
+            if i % 2 == 0:
+                log_sum = scipy.special.logsumexp(log_s, 2, keepdims=True)
+                log_s = log_s - np.where(row_mask, log_sum, np.zeros_like(log_sum))
+                log_s[np.isnan(log_s)] = -float('inf')
+            else:
+                log_sum = scipy.special.logsumexp(log_s, 1, keepdims=True)
+                log_s = log_s - np.where(col_mask, log_sum, np.zeros_like(log_sum))
+                log_s[np.isnan(log_s)] = -float('inf')
+
+        ret_log_s = log_s
+    else:
+        ret_log_s = np.full((batch_size, log_s.shape[1], log_s.shape[2]), -float('inf'), dtype=log_s.dtype)
+
+        for b in range(batch_size):
+            row_slice = slice(0, nrows[b])
+            col_slice = slice(0, ncols[b])
+            log_s_b = log_s[b, row_slice, col_slice]
+            row_mask_b = row_mask[b, row_slice, :]
+            col_mask_b = col_mask[b, :, col_slice]
+
+            for i in range(max_iter):
+                if i % 2 == 0:
+                    log_sum = scipy.special.logsumexp(log_s_b, 1, keepdims=True)
+                    log_s_b = log_s_b - np.where(row_mask_b, log_sum, np.zeros_like(log_sum))
+                else:
+                    log_sum = scipy.special.logsumexp(log_s_b, 0, keepdims=True)
+                    log_s_b = log_s_b - np.where(col_mask_b, log_sum, np.zeros_like(log_sum))
+
+            ret_log_s[b, row_slice, col_slice] = log_s_b
+
+    if unmatchrows is not None and unmatchcols is not None:
+        nrows -= 1
+        ncols -= 1
+        for b in range(batch_size):
+            ret_log_s[b, :nrows[b] + 1, ncols[b]] = -float('inf')
+            ret_log_s[b, nrows[b], :ncols[b]] = -float('inf')
+        ret_log_s = ret_log_s[:, :-1, :-1]
+
+    if dummy_row:
+        if dummy_shape[1] > 0:
+            ret_log_s = ret_log_s[:, :-dummy_shape[1]]
+        for b in range(batch_size):
+            ret_log_s[b, ori_nrows[b]:nrows[b], :ncols[b]] = -float('inf')
+
+    if np.any(transposed_batch):
+        s_t = ret_log_s.transpose((0, 2, 1))
+        s_t = np.concatenate((
+            s_t[:, :ret_log_s.shape[1], :],
+            np.full((batch_size, ret_log_s.shape[1], ret_log_s.shape[2]-ret_log_s.shape[1]), -float('inf'))), axis=2)
+        ret_log_s = np.where(transposed_batch.reshape(batch_size, 1, 1), s_t, ret_log_s)
+
+    if transposed:
+        ret_log_s = ret_log_s.transpose((0, 2, 1))
+
+    return np.exp(ret_log_s)
+
+
+#############################################
+#    Quadratic Assignment Problem Solvers   #
+#############################################
+
+
+def rrwm(K: np.ndarray, n1: np.ndarray, n2: np.ndarray, n1max, n2max, x0: np.ndarray,
+         max_iter: int, sk_iter: int, alpha: float, beta: float) -> np.ndarray:
+    """
+    numpy implementation of RRWM algorithm.
+    """
+    batch_num, n1, n2, n1max, n2max, n1n2, v0 = _check_and_init_gm(K, n1, n2, n1max, n2max, x0)
+    # rescale the values in K
+    d = K.sum(axis=2, keepdims=True)
+    dmax = d.max(axis=1, keepdims=True)
+    K = K / (dmax + d.min() * 1e-5) # d.min() * 1e-5 for numerical reasons
+    v = v0
+    for i in range(max_iter):
+        # random walk
+        v = np.matmul(K, v)
+        last_v = v
+        n = np.linalg.norm(v, ord=1, axis=1, keepdims=True)
+        v = v / n
+
+        # reweighted jump
+        s = v.reshape((batch_num, n2max, n1max)).transpose((0, 2, 1))
+        s = beta * s / np.amax(s, axis=(1, 2), keepdims=True)
+        v = alpha * sinkhorn(s, n1, n2, max_iter=sk_iter).transpose((0, 2, 1)).reshape((batch_num, n1n2, 1)) + \
+            (1 - alpha) * v
+        n = np.linalg.norm(v, ord=1, axis=1, keepdims=True)
+        v = np.matmul(v, 1 / n)
+
+        if np.linalg.norm((v - last_v).squeeze(axis=-1), ord='fro') < 1e-5:
+            break
+
+    return v.reshape((batch_num, n2max, n1max)).transpose((0, 2, 1))
+
+
+def sm(K: np.ndarray, n1: np.ndarray, n2: np.ndarray, n1max, n2max, x0: np.ndarray,
+       max_iter: int) -> np.ndarray:
+    """
+    numpy implementation of SM algorithm.
+    """
+    batch_num, n1, n2, n1max, n2max, n1n2, v0 = _check_and_init_gm(K, n1, n2, n1max, n2max, x0)
+    v = vlast = v0
+    for i in range(max_iter):
+        v = np.matmul(K, v)
+        n = np.linalg.norm(v, ord=2, axis=1)
+        v = np.matmul(v, (1 / n).reshape((batch_num, 1, 1)))
+        if np.linalg.norm((v - vlast).squeeze(-1), ord='fro') < 1e-5:
+            break
+        vlast = v
+
+    x = v.reshape((batch_num, n2max, n1max)).transpose((0, 2, 1))
+    return x
+
+
+def ipfp(K: np.ndarray, n1: np.ndarray, n2: np.ndarray, n1max, n2max, x0: np.ndarray,
+         max_iter) -> np.ndarray:
+    """
+    numpy implementation of IPFP algorithm
+    """
+    batch_num, n1, n2, n1max, n2max, n1n2, v0 = _check_and_init_gm(K, n1, n2, n1max, n2max, x0)
+    v = v0
+    last_v = v
+
+    def comp_obj_score(v1, K, v2):
+        return np.matmul(np.matmul(v1.reshape((batch_num, 1, -1)), K), v2)
+
+    for i in range(max_iter):
+        cost = np.matmul(K, v).reshape((batch_num, n2max, n1max)).transpose((0, 2, 1))
+        binary_sol = hungarian(cost, n1, n2)
+        binary_v = binary_sol.transpose((0, 2, 1)).reshape((batch_num, -1, 1))
+        alpha = comp_obj_score(v, K, binary_v - v)
+        beta = comp_obj_score(binary_v - v, K, binary_v - v)
+        t0 = alpha / beta
+        v = np.where(np.logical_or(beta <= 0, t0 >= 1), binary_v, v + t0 * (binary_v - v))
+        last_v_sol = comp_obj_score(last_v, K, last_v)
+        if np.max(np.abs(
+                last_v_sol - np.matmul(cost.reshape((batch_num, 1, -1)), binary_sol.reshape((batch_num, -1, 1)))
+        ) / last_v_sol) < 1e-3:
+            break
+        last_v = v
+
+    pred_x = binary_sol
+    return pred_x
+
+
+def _check_and_init_gm(K, n1, n2, n1max, n2max, x0):
+    # get batch number
+    batch_num = K.shape[0]
+    n1n2 = K.shape[1]
+
+    # get values of n1, n2, n1max, n2max and check
+    if n1 is None:
+        n1 = np.full(batch_num, n1max, dtype=int)
+    if n2 is None:
+        n2 = np.full(batch_num, n2max, dtype=int)
+    if n1max is None:
+        n1max = np.max(n1)
+    if n2max is None:
+        n2max = np.max(n2)
+
+    assert n1max * n2max == n1n2, 'the input size of K does not match with n1max * n2max!'
+
+    # initialize x0 (also v0)
+    if x0 is None:
+        x0 = np.zeros((batch_num, n1max, n2max), dtype=K.dtype)
+        for b in range(batch_num):
+            x0[b, 0:n1[b], 0:n2[b]] = 1. / (n1[b] * n2[b])
+    v0 = x0.transpose((0, 2, 1)).reshape((batch_num, n1n2, 1))
+
+    return batch_num, n1, n2, n1max, n2max, n1n2, v0
+
+
+############################################
+#      Multi-Graph Matching Solvers        #
+############################################
+def cao_solver(K, X, num_graph, num_node, max_iter, lambda_init, lambda_step, lambda_max, iter_boost):
+
+    m, n = num_graph, num_node
+    param_lambda = lambda_init
+
+    def _comp_aff_score(x, k):
+        return np.expand_dims(np.expand_dims(pygmtools.utils.compute_affinity_score(x, k, backend='numpy'),axis=-1),axis=-1)
+
+    for iter in range(max_iter):
+        if iter >= iter_boost:
+            param_lambda = np.min([param_lambda * lambda_step, lambda_max])
+        # pair_con = get_batch_pc_opt(X)
+        pair_aff = _comp_aff_score(X.reshape(-1, n, n), K.reshape(-1, n * n, n * n)).reshape(m, m)
+        pair_aff = pair_aff - np.eye(m) * pair_aff
+        norm = np.max(pair_aff)
+        for i in range(m):
+            for j in range(m):
+                if i >= j:
+                    continue
+                aff_ori = _comp_aff_score(X[i, j], K[i, j]) / norm
+                con_ori = _get_single_pc_opt(X, i, j)
+                # con_ori = torch.sqrt(pair_con[i, j])
+                if iter < iter_boost:
+                    score_ori = aff_ori
+                else:
+                    score_ori = aff_ori * (1 - param_lambda) + con_ori * param_lambda
+                X_upt = X[i, j]
+                for k in range(m):
+                    X_combo = np.matmul(X[i, k], X[k, j])
+                    aff_combo = _comp_aff_score(X_combo, K[i, j]) / norm
+                    con_combo = _get_single_pc_opt(X, i, j, X_combo)
+                    # con_combo = torch.sqrt(pair_con[i, k] * pair_con[k, j])
+                    if iter < iter_boost:
+                        score_combo = aff_combo
+                    else:
+                        score_combo = aff_combo * (1 - param_lambda) + con_combo * param_lambda
+                    if score_combo > score_ori:
+                        X_upt = X_combo
+                X[i, j] = X_upt
+                X[j, i] = X_upt.swapaxes(0,1)
+    return X
+
+
+def cao_fast_solver(K, X, num_graph, num_node, max_iter, lambda_init, lambda_step, lambda_max, iter_boost):
+    r"""
+    Numpy implementation of CAO solver in fast config (mode="pc")
+
+    :param K: affinity matrix, (m, m, n*n, n*n)
+    :param X: initial matching, (m, m, n, n)
+    :param num_graph: number of graphs, int
+    :param num_node: number of nodes, int
+    :return: X, (m, m, n, n)
+    """
+    m, n = num_graph, num_node
+    param_lambda = lambda_init
+
+    def _comp_aff_score(x, k):
+        return np.expand_dims(np.expand_dims(pygmtools.utils.compute_affinity_score(x, k, backend='numpy'),axis=-1),axis=-1)
+
+    mask1 = np.arange(m).reshape(m, 1).repeat(m,axis=1)
+    mask2 = np.arange(m).reshape(1, m).repeat(m,axis=0)
+    mask = (mask1 < mask2).astype(float)
+    X_mask = mask.reshape(m, m, 1, 1)
+
+    for iter in range(max_iter):
+        if iter >= iter_boost:
+            param_lambda = np.min([param_lambda * lambda_step, lambda_max])
+
+        pair_aff = _comp_aff_score(X.reshape(-1, n, n), K.reshape(-1, n * n, n * n)).reshape(m, m)
+        pair_aff = pair_aff - np.eye(m) * pair_aff
+        norm = np.max(pair_aff)
+
+        X1 = X.reshape(m, 1, m, n, n)
+        X1 = np.tile(X1,(1, m, 1, 1, 1)).reshape(-1, n, n)  # X1[i,j,k] = X[i,k]
+        X2 = X.reshape(1, m, m, n, n)
+        X2 = np.tile(X2,(m, 1, 1, 1, 1)).swapaxes(1, 2).reshape(-1, n, n)  # X2[i,j,k] = X[k,j]
+        X_combo = np.matmul(X1, X2).reshape(m, m, m, n, n) # X_combo[i,j,k] = X[i, k] * X[k, j]
+
+        aff_ori = (_comp_aff_score(X.reshape(-1, n, n), K.reshape(-1, n * n, n * n)) / norm).reshape(m, m)
+        pair_con = _get_batch_pc_opt(X)
+        con_ori = np.sqrt(pair_con)
+
+        K_repeat = np.repeat(K.reshape(m, m, 1, n * n, n * n),m,axis=2).reshape(-1, n * n, n * n)
+        aff_combo = (_comp_aff_score(X_combo.reshape(-1, n, n), K_repeat) / norm).reshape(m, m, m)
+        con1 = pair_con.reshape(m, 1, m)
+        con1 = np.tile(con1,(1, m, 1))  # con1[i,j,k] = pair_con[i,k]
+        con2 = pair_con.reshape(1, m, m)
+        con2 = np.tile(con2,(m, 1, 1)).swapaxes(1,2)  # con2[i,j,k] = pair_con[j,k]
+        con_combo = np.sqrt(con1 * con2)
+
+        if iter < iter_boost:
+            score_ori = aff_ori
+            score_combo = aff_combo
+        else:
+            score_ori = aff_ori * (1 - param_lambda) + con_ori * param_lambda
+            score_combo = aff_combo * (1 - param_lambda) + con_combo * param_lambda
+        
+        idx = np.argmax(score_combo,axis=-1)
+        score_combo = np.max(score_combo, axis=-1)
+        
+        assert np.all(score_combo + 1e-4 >= score_ori), np.min(score_combo - score_ori)
+        X_upt = X_combo[mask1, mask2, idx, :, :]
+        X = X_upt * X_mask + X_upt.swapaxes(0,1).swapaxes(2,3) * X_mask.swapaxes(0,1) + X * (1 - X_mask - X_mask.swapaxes(0, 1))
+        assert np.all(X.swapaxes(0,1).swapaxes(2,3) == X)
+    return X
+
+
+def mgm_floyd_solver(K, X, num_graph, num_node, param_lambda):
+    m, n = num_graph, num_node
+
+    def _comp_aff_score(x, k):
+        return np.expand_dims(np.expand_dims(pygmtools.utils.compute_affinity_score(x, k, backend='numpy'),axis=-1),axis=-1)
+
+    for k in range(m):
+        pair_aff = _comp_aff_score(X.reshape(-1, n, n), K.reshape(-1, n * n, n * n)).reshape(m, m)
+        pair_aff = pair_aff - np.eye(m) * pair_aff
+        norm = np.max(pair_aff)
+
+        # print("iter:{} aff:{:.4f} con:{:.4f}".format(
+        #     k, torch.mean(pair_aff).item(), torch.mean(get_batch_pc_opt(X)).item()
+        # ))
+
+        for i in range(m):
+            for j in range(m):
+                if i >= j:
+                    continue
+                score_ori = _comp_aff_score(X[i, j], K[i, j]) / norm
+                X_combo = np.matmul(X[i, k], X[k, j])
+                score_combo = _comp_aff_score(X_combo, K[i, j]) / norm
+
+                if score_combo > score_ori:
+                    X[i, j] = X_combo
+                    X[j, i] = X_combo.swapaxes(0, 1)
+
+    for k in range(m):
+        pair_aff = _comp_aff_score(X.reshape(-1, n, n), K.reshape(-1, n * n, n * n)).reshape(m, m)
+        pair_aff = pair_aff - np.eye(m) * pair_aff
+        norm = np.max(pair_aff)
+
+        pair_con = _get_batch_pc_opt(X)
+        for i in range(m):
+            for j in range(m):
+                if i >= j:
+                    continue
+                aff_ori = _comp_aff_score(X[i, j], K[i, j]) / norm
+                con_ori = _get_single_pc_opt(X, i, j)
+                # con_ori = torch.sqrt(pair_con[i, j])
+                score_ori = aff_ori * (1 - param_lambda) + con_ori * param_lambda
+
+                X_combo = np.matmul(X[i, k], X[k, j])
+                aff_combo = _comp_aff_score(X_combo, K[i, j]) / norm
+                con_combo = _get_single_pc_opt(X, i, j, X_combo)
+                # con_combo = torch.sqrt(pair_con[i, k] * pair_con[k, j])
+                score_combo = aff_combo * (1 - param_lambda) + con_combo * param_lambda
+
+                if score_combo > score_ori:
+                    X[i, j] = X_combo
+                    X[j, i] = X_combo.swapaxes(0,1)
+    return X
+
+
+def mgm_floyd_fast_solver(K, X, num_graph, num_node, param_lambda):
+    m, n = num_graph, num_node
+
+    def _comp_aff_score(x, k):
+        return np.expand_dims(np.expand_dims(pygmtools.utils.compute_affinity_score(x, k, backend='numpy'),axis=-1),axis=-1)
+
+    mask1 = np.arange(m).reshape(m, 1).repeat(m,axis=1)
+    mask2 = np.arange(m).reshape(1, m).repeat(m,axis=0)
+    mask = (mask1 < mask2).astype(float)
+    X_mask = mask.reshape(m, m, 1, 1)
+
+    for k in range(m):
+        pair_aff = _comp_aff_score(X.reshape(-1, n, n), K.reshape(-1, n * n, n * n)).reshape(m, m)
+        pair_aff = pair_aff - np.eye(m) * pair_aff
+        norm = np.max(pair_aff)
+
+        # print("iter:{} aff:{:.4f} con:{:.4f}".format(
+        #     k, torch.mean(pair_aff).item(), torch.mean(get_batch_pc_opt(X)).item()
+        # ))
+
+        X1 = X[:, k].reshape(m, 1, n, n)
+        X1 = np.tile(X1,(1, m, 1, 1)).reshape(-1, n, n)  # X[i, j] = X[i, k]
+        X2 = X[k, :].reshape(1, m, n, n)
+        X2 = np.tile(X2,(m, 1, 1, 1)).reshape(-1, n, n)  # X[i, j] = X[j, k]
+        X_combo = np.matmul(X1, X2).reshape(m, m, n, n)
+
+        aff_ori = (_comp_aff_score(X.reshape(-1, n, n), K.reshape(-1, n * n, n * n)) / norm).reshape(m, m)
+        aff_combo = (_comp_aff_score(X_combo.reshape(-1, n, n), K.reshape(-1, n * n, n * n)) / norm).reshape(m, m)
+
+        score_ori = aff_ori
+        score_combo = aff_combo
+
+        upt = (score_ori < score_combo).astype(float)
+        upt = (upt * mask).reshape(m, m, 1, 1)
+        X = X * (1.0 - upt) + X_combo * upt
+        X = X * X_mask + X.swapaxes(0,1).swapaxes(2, 3) * (1 - X_mask)
+
+    for k in range(m):
+        pair_aff = _comp_aff_score(X.reshape(-1, n, n), K.reshape(-1, n * n, n * n)).reshape(m, m)
+        pair_aff = pair_aff - np.eye(m) * pair_aff
+        norm = np.max(pair_aff)
+
+        pair_con = _get_batch_pc_opt(X)
+
+        X1 = X[:, k].reshape(m, 1, n, n)
+        X1 = np.tile(X1,(1, m, 1, 1)).reshape(-1, n, n)  # X[i, j] = X[i, k]
+        X2 = X[k, :].reshape(1, m, n, n)
+        X2 = np.tile(X2,(m, 1, 1, 1)).reshape(-1, n, n)  # X[i, j] = X[j, k]
+        X_combo = np.matmul(X1, X2).reshape(m, m, n, n)
+
+        aff_ori = (_comp_aff_score(X.reshape(-1, n, n), K.reshape(-1, n * n, n * n)) / norm).reshape(m, m)
+        aff_combo = (_comp_aff_score(X_combo.reshape(-1, n, n), K.reshape(-1, n * n, n * n)) / norm).reshape(m, m)
+
+        con_ori = np.sqrt(pair_con)
+        con1 = pair_con[:, k].reshape(m, 1).repeat(m,axis=1)
+        con2 = pair_con[k, :].reshape(1, m).repeat(m,axis=0)
+        con_combo = np.sqrt(con1 * con2)
+
+        score_ori = aff_ori * (1 - param_lambda) + con_ori * param_lambda
+        score_combo = aff_combo * (1 - param_lambda) + con_combo * param_lambda
+
+        upt = (score_ori < score_combo).astype(float)
+        upt = (upt * mask).reshape(m, m, 1, 1)
+        X = X * (1.0 - upt) + X_combo * upt
+        X = X * X_mask + X.swapaxes(0,1).swapaxes(2, 3) * (1 - X_mask)
+    return X
+
+
+def _get_single_pc_opt(X, i, j, Xij=None):
+    """
+    CAO/Floyd helper function (compute consistency)
+    :param X: (m, m, n, n) all the matching results
+    :param i: index
+    :param j: index
+    :return: the consistency of X_ij
+    """
+    m, _, n, _ = X.shape
+    if Xij is None:
+        Xij = X[i, j]
+    X1 = X[i, :].reshape(-1, n, n)
+    X2 = X[:, j].reshape(-1, n, n)
+    X_combo = np.matmul(X1, X2)
+    pair_con = 1 - np.sum(np.abs(Xij - X_combo)) / (2 * n * m)
+    return pair_con
+
+
+def _get_batch_pc_opt(X):
+    """
+    CAO/Floyd-fast helper function (compute consistency in batch)
+    :param X: (m, m, n, n) all the matching results
+    :return: (m, m) the consistency of X
+    """
+    m = X.shape[0]
+    n = X.shape[2]
+    X1 = X.reshape(m, 1, m, n, n)
+    X1 = np.tile(X1,(1, m, 1, 1, 1)).reshape(-1, n, n)  # X1[i, j, k] = X[i, k]
+    X2 = X.reshape(1, m, m, n, n)
+    X2 = np.tile(X2,(m, 1, 1, 1, 1)).swapaxes(1,2).reshape(-1, n, n)  # X2[i, j, k] = X[k, j]
+    X_combo = np.matmul(X1, X2).reshape(m, m, m, n, n)
+    X_ori = X.reshape(m, m, 1, n, n)
+    X_ori = np.tile(X_ori,(1, 1, m, 1, 1))
+    pair_con = 1 - np.sum(np.abs(X_combo - X_ori), axis=(2, 3, 4)) / (2 * n * m)
+    return pair_con
+
+def gamgm(
+        A, W, ns, n_univ, U0,
+        init_tau, min_tau, sk_gamma,
+        sk_iter, max_iter, quad_weight,
+        converge_thresh, outlier_thresh, bb_smooth,
+        verbose,
+        cluster_M=None, projector='sinkhorn', hung_iter=True # these arguments are reserved for clustering
+):
+    """
+    Numpy implementation of Graduated Assignment for Multi-Graph Matching (with compatibility for 2GM and clustering)
+    """
+    num_graphs = A.shape[0]
+    if ns is None:
+        ns = np.full((num_graphs,), A.shape[1], dtype='i4')
+    n_indices = np.cumsum(ns, axis=0)
+
+    # build a super adjacency matrix A
+    supA = np.zeros((n_indices[-1], n_indices[-1]))
+    for i in range(num_graphs):
+        start_n = n_indices[i] - ns[i]
+        end_n = n_indices[i]
+        supA[start_n:end_n, start_n:end_n] = A[i, :ns[i], :ns[i]]
+
+    # handle the type of n_univ
+    if type(n_univ) is np.ndarray:
+        n_univ = n_univ.item()
+
+    # randomly init U
+    if U0 is None:
+        U0 = np.full((n_indices[-1], n_univ), 1 / n_univ)
+        U0 += np.random.rand(n_indices[-1], n_univ) / 1000
+
+    # init cluster_M if not given
+    if cluster_M is None:
+        cluster_M = np.ones((num_graphs, num_graphs))
+
+    # reshape W into supW
+    supW = np.zeros((n_indices[-1], n_indices[-1]))
+    for i, j in itertools.product(range(num_graphs), repeat=2):
+        start_x = n_indices[i] - ns[i]
+        end_x = n_indices[i]
+        start_y = n_indices[j] - ns[j]
+        end_y = n_indices[j]
+        supW[start_x:end_x, start_y:end_y] = W[i, j, :ns[i], :ns[j]]
+
+    U = gamgm_real(
+        supA, supW, ns, n_indices, n_univ, num_graphs, U0,
+        init_tau, min_tau, sk_gamma,
+        sk_iter, max_iter, quad_weight,
+        converge_thresh, outlier_thresh,
+        verbose,
+        cluster_M, projector, hung_iter
+        )
+
+    result = pygmtools.utils.MultiMatchingResult(True, 'numpy')
+
+    for i in range(num_graphs):
+        start_n = n_indices[i] - ns[i]
+        end_n = n_indices[i]
+        result[i] = U[start_n:end_n]
+
+    return result
+
+
+def gamgm_real(
+        supA, supW, ns, n_indices, n_univ, num_graphs, U0,
+        init_tau, min_tau, sk_gamma,
+        sk_iter, max_iter, quad_weight,
+        converge_thresh, outlier_thresh,
+        verbose,
+        cluster_M, projector, hung_iter # these arguments are reserved for clustering
+        ):
+    """
+    The real forward function of GAMGM
+    """
+    U = U0
+    sinkhorn_tau = init_tau
+    iter_flag = True
+
+    while iter_flag:
+        for i in range(max_iter):
+            # compact matrix form update of V
+            UUt = np.matmul(U, U.T)
+            lastUUt = UUt
+            cluster_weight = np.repeat(cluster_M, ns.astype('i4'), axis=0)
+            cluster_weight = np.repeat(cluster_weight, ns.astype('i4'), axis=1)
+            quad = np.matmul(np.matmul(np.matmul(supA, UUt * cluster_weight), supA), U) * quad_weight * 2
+            unary = np.matmul(supW * cluster_weight, U)
+            if verbose:
+                if projector == 'sinkhorn':
+                    print_str = f'tau={sinkhorn_tau:.3e}'
+                else:
+                    print_str = 'hungarian'
+                print(print_str + f' #iter={i}/{max_iter} '
+                      f'quad score: {(quad * U).sum():.3e}, unary score: {(unary * U).sum():.3e}')
+            V = (quad + unary) / num_graphs
+
+            U_list = []
+            if projector == 'hungarian':
+                n_start = 0
+                for n_end in n_indices:
+                    U_list.append(pygmtools.hungarian(V[n_start:n_end, :n_univ], backend='numpy'))
+                    n_start = n_end
+            elif projector == 'sinkhorn':
+                if np.all(ns == ns[0]):
+                    if ns[0] <= n_univ:
+                        U_list.append(
+                            sinkhorn(
+                                V.reshape(num_graphs, -1, n_univ),
+                                max_iter=sk_iter, tau=sinkhorn_tau, batched_operation=True, dummy_row=True
+                            ).reshape(-1, n_univ))
+                    else:
+                        U_list.append(
+                            sinkhorn(
+                                V.reshape(num_graphs, -1, n_univ).swapaxes(1, 2),
+                                max_iter=sk_iter, tau=sinkhorn_tau, batched_operation=True, dummy_row=True
+                            ).swapaxes(1, 2).reshape(-1, n_univ))
+                else:
+                    V_list = []
+                    n1 = []
+                    n_start = 0
+                    for n_end in n_indices:
+                        V_list.append(V[n_start:n_end, :n_univ])
+                        n1.append(n_end - n_start)
+                        n_start = n_end
+                    V_batch = build_batch(V_list)
+                    U = sinkhorn(V_batch, n1,
+                                 max_iter=sk_iter, tau=sinkhorn_tau, batched_operation=True, dummy_row=True)
+                    n_start = 0
+                    for idx, n_end in enumerate(n_indices):
+                        U_list.append(U[idx, :n_end - n_start, :])
+                        n_start = n_end
+            else:
+                raise NameError('Unknown projecter name: {}'.format(projector))
+
+            U = np.concatenate(U_list, axis=0)
+            if num_graphs == 2:
+                U[:ns[0], :] = np.eye(ns[0], n_univ)
+
+            # calculate gap to discrete
+            if projector == 'sinkhorn' and verbose:
+                U_list_hung = []
+                n_start = 0
+                for n_end in n_indices:
+                    U_list_hung.append(pygmtools.hungarian(V[n_start:n_end, :n_univ], backend='numpy'))
+                    n_start = n_end
+                U_hung = np.concatenate(U_list_hung, axis=0)
+                diff = np.linalg.norm(np.matmul(U, U.transpose()) - lastUUt)
+                print(f'tau={sinkhorn_tau:.3e} #iter={i}/{max_iter} '
+                      f'gap to discrete: {np.mean(np.abs(U - U_hung)):.3e}, iter diff: {diff:.3e}')
+
+            if projector == 'hungarian' and outlier_thresh > 0:
+                U_hung = U
+                UUt = np.matmul(U_hung, U_hung.transpose())
+                cluster_weight = np.repeat(cluster_M, ns.astype('i4'), axis=0)
+                cluster_weight = np.repeat(cluster_weight, ns.astype('i4'), axis=1)
+                quad = np.linalg.multi_dot((supA, UUt * cluster_weight, supA, U_hung)) * quad_weight * 2
+                unary = np.matmul(supW * cluster_weight, U_hung)
+                max_vals = (unary + quad).max(axis=1)
+                U = U * (unary + quad > outlier_thresh)
+                if verbose:
+                    print(f'hungarian #iter={i}/{max_iter} '
+                          f'unary+quad score thresh={outlier_thresh:.3f}, '
+                          f'#>thresh={np.sum(max_vals > outlier_thresh)}/{max_vals.shape[0]} '
+                          f'min:{max_vals.min():.4f}, mean:{max_vals.mean():.4f}, '
+                          f'median:{np.median(max_vals):.4f}, max:{max_vals.max():.4f}')
+
+            if np.linalg.norm(np.matmul(U, U.T) - lastUUt) < converge_thresh:
+                break
+
+        if verbose: print('-' * 20)
+
+        if i == max_iter - 1: # not converged
+            if hung_iter:
+                pass
+            else:
+                U_list = [pygmtools.hungarian(_, backend='numpy') for _ in U_list]
+                U = np.concatenate(U_list, axis=0)
+                break
+
+        # projection control
+        if projector == 'hungarian':
+            break
+        elif sinkhorn_tau > min_tau:
+            sinkhorn_tau *= sk_gamma
+        else:
+            if hung_iter:
+                projector = 'hungarian'
+            else:
+                U_list = [pygmtools.hungarian(_, backend='numpy') for _ in U_list]
+                U = np.concatenate(U_list, axis=0)
+                break
+
+    return U
+
+
+############################################
+#          Neural Network Solvers          #
+############################################
+
+from pygmtools.numpy_modules import *
+
+def add_module(self, name: str, module) -> None:
+        self._modules[name] = module
+
+class PCA_GM_Net():
+    """
+    Numpy implementation of PCA-GM and IPCA-GM network
+    """
+    def __init__(self, in_channel, hidden_channel, out_channel, num_layers, cross_iter_num=-1):
+        self.gnn_layer = num_layers
+        self.dict = {}
+        for i in range(self.gnn_layer):
+            if i == 0:
+                gnn_layer = Siamese_Gconv(in_channel, hidden_channel)
+            elif 0 < i < self.gnn_layer - 1:
+                gnn_layer = Siamese_Gconv(hidden_channel, hidden_channel)
+            else:
+                gnn_layer = Siamese_Gconv(hidden_channel, out_channel)
+                self.dict['affinity_{}'.format(i)] =  WeightedInnerProdAffinity(out_channel)
+            self.dict['gnn_layer_{}'.format(i)] = gnn_layer
+            if i == self.gnn_layer - 2:  # only the second last layer will have cross-graph module
+                self.dict['cross_graph_{}'.format(i)] = Linear(hidden_channel * 2, hidden_channel)
+                if cross_iter_num <= 0:
+                    self.dict['affinity_{}'.format(i)] = WeightedInnerProdAffinity(hidden_channel)
+
+    def forward(self, feat1, feat2, A1, A2, n1, n2, cross_iter_num, sk_max_iter, sk_tau):
+        _sinkhorn_func = functools.partial(sinkhorn,
+                                           dummy_row=False, max_iter=sk_max_iter, tau=sk_tau, batched_operation=False)
+        emb1, emb2 = feat1, feat2
+        if cross_iter_num <= 0:
+            # Vanilla PCA-GM
+            for i in range(self.gnn_layer):
+                gnn_layer = self.dict['gnn_layer_{}'.format(i)]
+                emb1, emb2 = gnn_layer.forward([A1, emb1], [A2, emb2])
+                if i == self.gnn_layer - 2:
+                    affinity = self.dict['affinity_{}'.format(i)]
+                    s = affinity.forward(emb1, emb2)
+                    s = _sinkhorn_func(s, n1, n2)
+
+                    cross_graph = self.dict['cross_graph_{}'.format(i)]
+                    new_emb1 = cross_graph.forward(np.concatenate((emb1, np.matmul(s, emb2)), axis=-1))
+                    new_emb2 = cross_graph.forward(np.concatenate((emb2, np.matmul(s.swapaxes(1, 2), emb1)), axis=-1))
+                    emb1 = new_emb1
+                    emb2 = new_emb2
+
+            affinity = self.dict['affinity_{}'.format(self.gnn_layer - 1)]
+            s = affinity.forward(emb1, emb2)
+            s = _sinkhorn_func(s, n1, n2)
+
+        else:
+            # IPCA-GM
+            for i in range(self.gnn_layer - 1):
+                gnn_layer = self.dict['gnn_layer_{}'.format(i)]
+                emb1, emb2 = gnn_layer.forward([A1, emb1], [A2, emb2])
+
+            emb1_0, emb2_0 = emb1, emb2
+            s = np.zeros((emb1.shape[0], emb1.shape[1], emb2.shape[1]))
+
+            for x in range(cross_iter_num):
+                # cross-graph convolution in second last layer
+                i = self.gnn_layer - 2
+                cross_graph = self.dict['cross_graph_{}'.format(i)]
+                emb1 = cross_graph.forward(np.concatenate((emb1_0, np.matmul(s, emb2_0)), axis=-1))
+                emb2 = cross_graph.forward(np.concatenate((emb2_0, np.matmul(s.swapaxes(1, 2), emb1_0)), axis=-1))
+
+                # last layer
+                i = self.gnn_layer - 1
+                gnn_layer = self.dict['gnn_layer_{}'.format(i)]
+                emb1, emb2 = gnn_layer.forward([A1, emb1], [A2, emb2])
+                affinity = self.dict['affinity_{}'.format(i)]
+                s = affinity.forward(emb1, emb2)
+                s = _sinkhorn_func(s, n1, n2)
+
+        return s
+
+
+pca_gm_pretrain_path = {
+    'voc':('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=1En_9f5Zi5rSsS-JTIce7B1BV6ijGEAPd',
+           'd85f97498157d723793b8fc1501841ce'),
+    'willow':('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=1LAnK6ASYu0CO1fEe6WpvMbt5vskuvwLo',
+              'c32f7c8a7a6978619b8fdbb6ad5b505f'),
+    'voc-all':('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=1c_aw4wxEBuY7JFC4Rt8rlcise777n189',
+               '0e2725b3ac51f87f0303bbcfaae5df80')
+}
+
+def pca_gm(feat1, feat2, A1, A2, n1, n2,
+           in_channel, hidden_channel, out_channel, num_layers, sk_max_iter, sk_tau,
+           network, pretrain):
+    """
+    Numpy implementation of PCA-GM
+    """
+    if feat1 is None:
+        forward_pass = False
+    else:
+        forward_pass = True
+    if network is None:
+        network = PCA_GM_Net(in_channel, hidden_channel, out_channel, num_layers)
+        if pretrain:
+            if pretrain in pca_gm_pretrain_path.keys():
+                url, md5 = pca_gm_pretrain_path[pretrain]
+                filename = pygmtools.utils.download(f'pca_gm_{pretrain}_numpy.npy', url, md5)
+                pca_gm_numpy_dict = np.load(filename,allow_pickle=True)
+                for i in range(network.gnn_layer):
+                    gnn_layer = network.dict['gnn_layer_{}'.format(i)]
+                    gnn_layer.gconv.a_fc.weight = pca_gm_numpy_dict.item()['gnn_layer_{}.gconv.a_fc.weight'.format(i)]
+                    gnn_layer.gconv.a_fc.bias = pca_gm_numpy_dict.item()['gnn_layer_{}.gconv.a_fc.bias'.format(i)]
+                    gnn_layer.gconv.u_fc.weight = pca_gm_numpy_dict.item()['gnn_layer_{}.gconv.u_fc.weight'.format(i)]
+                    gnn_layer.gconv.u_fc.bias = pca_gm_numpy_dict.item()['gnn_layer_{}.gconv.u_fc.bias'.format(i)]
+                    if i == network.gnn_layer - 2:
+                        affinity = network.dict['affinity_{}'.format(i)]
+                        affinity.A = pca_gm_numpy_dict.item()['affinity_{}.A'.format(i)]
+                        cross_graph = network.dict['cross_graph_{}'.format(i)]
+                        cross_graph.weight = pca_gm_numpy_dict.item()['cross_graph_{}.weight'.format(i)]
+                        cross_graph.bias = pca_gm_numpy_dict.item()['cross_graph_{}.bias'.format(i)]
+                affinity = affinity = network.dict['affinity_{}'.format(network.gnn_layer - 1)]
+                affinity.A = pca_gm_numpy_dict.item()['affinity_{}.A'.format(network.gnn_layer - 1)]
+            else:
+                raise ValueError(f'Unknown pretrain tag. Available tags: {cie_pretrain_path.keys()}')
+    if forward_pass:
+        batch_size = feat1.shape[0]
+        if n1 is None:
+            n1 = np.array([feat1.shape[1]] * batch_size)
+        if n2 is None:
+            n2 = np.array([feat2.shape[1]] * batch_size)
+        result = network.forward(feat1, feat2, A1, A2, n1, n2, -1, sk_max_iter, sk_tau)
+    else:
+        result = None
+    return result, network
+
+ipca_gm_pretrain_path = {
+    'voc':('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=13g9iBjXZ804bKo6p8wMQe8yNUZBwVGJj',
+           '4479a25558780a4b4c9891b4386659cd'),
+    'willow':('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=1vq0FqjPhiSR80cu9jk0qMljkC4gSFvQA',
+              'ada1df350d45cc877f08e12919993345')
+}
+
+def ipca_gm(feat1, feat2, A1, A2, n1, n2,
+           in_channel, hidden_channel, out_channel, num_layers, cross_iter, sk_max_iter, sk_tau,
+           network, pretrain):
+    """
+    Numpy implementation of IPCA-GM
+    """
+    if feat1 is None:
+        forward_pass = False
+    else:
+        forward_pass = True
+    if network is None:
+        network = PCA_GM_Net(in_channel, hidden_channel, out_channel, num_layers, cross_iter)
+        if pretrain:
+            if pretrain in ipca_gm_pretrain_path.keys():
+                url, md5 = ipca_gm_pretrain_path[pretrain]
+                filename = pygmtools.utils.download(f'ipca_gm_{pretrain}_numpy.npy', url, md5)
+                ipca_gm_numpy_dict = np.load(filename,allow_pickle=True)
+                for i in range(network.gnn_layer-1):
+                    gnn_layer = network.dict['gnn_layer_{}'.format(i)]
+                    gnn_layer.gconv.a_fc.weight = ipca_gm_numpy_dict.item()['gnn_layer_{}.gconv.a_fc.weight'.format(i)]
+                    gnn_layer.gconv.a_fc.bias = ipca_gm_numpy_dict.item()['gnn_layer_{}.gconv.a_fc.bias'.format(i)]
+                    gnn_layer.gconv.u_fc.weight = ipca_gm_numpy_dict.item()['gnn_layer_{}.gconv.u_fc.weight'.format(i)]
+                    gnn_layer.gconv.u_fc.bias = ipca_gm_numpy_dict.item()['gnn_layer_{}.gconv.u_fc.bias'.format(i)]
+                
+                for x in range(cross_iter):
+                    i = network.gnn_layer - 2
+                    cross_graph = network.dict['cross_graph_{}'.format(i)]
+                    cross_graph.weight = ipca_gm_numpy_dict.item()['cross_graph_{}.weight'.format(i)]
+                    cross_graph.bias = ipca_gm_numpy_dict.item()['cross_graph_{}.bias'.format(i)]
+                    
+                    i = network.gnn_layer - 1
+                    gnn_layer = network.dict['gnn_layer_{}'.format(i)]
+                    gnn_layer.gconv.a_fc.weight = ipca_gm_numpy_dict.item()['gnn_layer_{}.gconv.a_fc.weight'.format(i)]
+                    gnn_layer.gconv.a_fc.bias = ipca_gm_numpy_dict.item()['gnn_layer_{}.gconv.a_fc.bias'.format(i)]
+                    gnn_layer.gconv.u_fc.weight = ipca_gm_numpy_dict.item()['gnn_layer_{}.gconv.u_fc.weight'.format(i)]
+                    gnn_layer.gconv.u_fc.bias = ipca_gm_numpy_dict.item()['gnn_layer_{}.gconv.u_fc.bias'.format(i)]
+
+                    affinity = network.dict['affinity_{}'.format(i)]
+                    affinity.A = ipca_gm_numpy_dict.item()['affinity_{}.A'.format(i)]
+            else:
+                raise ValueError(f'Unknown pretrain tag. Available tags: {ipca_gm_pretrain_path.keys()}') 
+    if forward_pass:
+        batch_size = feat1.shape[0]
+        if n1 is None:
+            n1 = np.array([feat1.shape[1]] * batch_size)
+        if n2 is None:
+            n2 = np.array([feat2.shape[1]] * batch_size)
+        result = network.forward(feat1, feat2, A1, A2, n1, n2, cross_iter, sk_max_iter, sk_tau)
+    else:
+        result = None
+    return result, network
+
+
+class CIE_Net():
+    """
+    Numpy implementation of CIE graph matching network
+    """
+    def __init__(self, in_node_channel, in_edge_channel, hidden_channel, out_channel, num_layers):
+        self.gnn_layer = num_layers
+        self.dict = {}
+        for i in range(self.gnn_layer):
+            if i == 0:
+                gnn_layer = Siamese_ChannelIndependentConv(in_node_channel, hidden_channel, in_edge_channel)
+            elif 0 < i < self.gnn_layer - 1:
+                gnn_layer = Siamese_ChannelIndependentConv(hidden_channel, hidden_channel, hidden_channel)
+            else:
+                gnn_layer = Siamese_ChannelIndependentConv(hidden_channel, out_channel, hidden_channel)
+                self.dict['affinity_{}'.format(i)] = WeightedInnerProdAffinity(out_channel)
+            self.dict['gnn_layer_{}'.format(i)] = gnn_layer
+            if i == self.gnn_layer - 2:  # only the second last layer will have cross-graph module
+                self.dict['cross_graph_{}'.format(i)] = Linear(hidden_channel * 2, hidden_channel)
+                self.dict['affinity_{}'.format(i)] = WeightedInnerProdAffinity(hidden_channel)
+
+    def forward(self, feat_node1, feat_node2, A1, A2, feat_edge1, feat_edge2, n1, n2, sk_max_iter, sk_tau):
+        _sinkhorn_func = functools.partial(sinkhorn,
+                                           dummy_row=False, max_iter=sk_max_iter, tau=sk_tau, batched_operation=False)
+        emb1, emb2 = feat_node1, feat_node2
+        emb_edge1, emb_edge2 = feat_edge1, feat_edge2
+        
+        for i in range(self.gnn_layer):
+            gnn_layer = self.dict['gnn_layer_{}'.format(i)]
+            # during forward process, the network structure will not change
+            emb1, emb2, emb_edge1, emb_edge2 = gnn_layer.forward([A1, emb1, emb_edge1], [A2, emb2, emb_edge2])
+            
+            if i == self.gnn_layer - 2:
+                affinity = self.dict['affinity_{}'.format(i)]
+                s = affinity.forward(emb1, emb2)
+                s = _sinkhorn_func(s, n1, n2)
+
+                cross_graph = self.dict['cross_graph_{}'.format(i)]
+                new_emb1 = cross_graph.forward(np.concatenate((emb1, np.matmul(s, emb2)), axis=-1))
+                new_emb2 = cross_graph.forward(np.concatenate((emb2, np.matmul(s.swapaxes(1, 2), emb1)), axis=-1))
+                emb1 = new_emb1
+                emb2 = new_emb2
+        
+        affinity = self.dict['affinity_{}'.format(self.gnn_layer - 1)]
+        s = affinity.forward(emb1, emb2)
+        s = _sinkhorn_func(s, n1, n2)
+        return s
+
+cie_pretrain_path = {
+    'voc':('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=1rP9sJY1fh493LLMWw-7RaeFAMHlbSs2D',
+           '9cbd55fa77d124b95052378643715bae'),
+    'willow':('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=1cMiXrSQjXZ9lDxeB6194z1-luyslVTR8',
+              'bd36e1bf314503c1f1482794e1648b18')
+}
+
+def cie(feat_node1, feat_node2, A1, A2, feat_edge1, feat_edge2, n1, n2,
+        in_node_channel, in_edge_channel, hidden_channel, out_channel, num_layers, sk_max_iter, sk_tau,
+        network, pretrain):
+    """
+    Numpy implementation of CIE
+    """
+    if feat_node1 is None:
+        forward_pass = False
+    else:
+        forward_pass = True
+    if network is None:
+        network = CIE_Net(in_node_channel, in_edge_channel, hidden_channel, out_channel, num_layers)
+        if pretrain:
+            if pretrain in cie_pretrain_path.keys():
+                url, md5 = cie_pretrain_path[pretrain]
+                filename = pygmtools.utils.download(f'cie_{pretrain}_numpy.npy', url, md5)
+                cie_numpy_dict = np.load(filename,allow_pickle=True)
+                for i in range(network.gnn_layer):
+                    gnn_layer = network.dict['gnn_layer_{}'.format(i)]
+                    gnn_layer.gconv.node_fc.weight = cie_numpy_dict.item()['gnn_layer_{}.gconv.node_fc.weight'.format(i)]
+                    gnn_layer.gconv.node_fc.bias = cie_numpy_dict.item()['gnn_layer_{}.gconv.node_fc.bias'.format(i)]
+                    gnn_layer.gconv.node_sfc.weight = cie_numpy_dict.item()['gnn_layer_{}.gconv.node_sfc.weight'.format(i)]
+                    gnn_layer.gconv.node_sfc.bias = cie_numpy_dict.item()['gnn_layer_{}.gconv.node_sfc.bias'.format(i)]
+                    gnn_layer.gconv.edge_fc.weight = cie_numpy_dict.item()['gnn_layer_{}.gconv.edge_fc.weight'.format(i)]
+                    gnn_layer.gconv.edge_fc.bias = cie_numpy_dict.item()['gnn_layer_{}.gconv.edge_fc.bias'.format(i)]
+                    if i == network.gnn_layer - 2:
+                        affinity = network.dict['affinity_{}'.format(i)]
+                        affinity.A = cie_numpy_dict.item()['affinity_{}.A'.format(i)]
+                        cross_graph = network.dict['cross_graph_{}'.format(i)]
+                        cross_graph.weight = cie_numpy_dict.item()['cross_graph_{}.weight'.format(i)]
+                        cross_graph.bias = cie_numpy_dict.item()['cross_graph_{}.bias'.format(i)]
+                affinity = affinity = network.dict['affinity_{}'.format(network.gnn_layer - 1)]
+                affinity.A = cie_numpy_dict.item()['affinity_{}.A'.format(network.gnn_layer - 1)]
+            else:
+                raise ValueError(f'Unknown pretrain tag. Available tags: {cie_pretrain_path.keys()}')
+    if forward_pass:
+        batch_size = feat_node1.shape[0]
+        if n1 is None:
+            n1 = np.array([feat_node1.shape[1]] * batch_size)
+        if n2 is None:
+            n2 = np.array([feat_node1.shape[1]] * batch_size)
+        result = network.forward(feat_node1, feat_node2, A1, A2, feat_edge1, feat_edge2, n1, n2, sk_max_iter, sk_tau)
+    else:
+        result = None
+
+    return result, network
+
+
+class NGM_Net():
+    """
+    Numpy implementation of NGM network
+    """
+    def __init__(self, gnn_channels, sk_emb):
+        self.gnn_layer = len(gnn_channels)
+        self.dict = {}
+        for i in range(self.gnn_layer):
+            if i == 0:
+                gnn_layer = NGMConvLayer(1, 1,
+                                         gnn_channels[i] + sk_emb, gnn_channels[i],
+                                         sk_channel=sk_emb)
+            else:
+                gnn_layer = NGMConvLayer(gnn_channels[i - 1] + sk_emb, gnn_channels[i - 1],
+                                         gnn_channels[i] + sk_emb, gnn_channels[i],
+                                         sk_channel=sk_emb)
+            self.dict['gnn_layer_{}'.format(i)] = gnn_layer
+        self.classifier = Linear(gnn_channels[-1] + sk_emb, 1)
+
+    def forward(self, K, n1, n2, n1max, n2max, v0, sk_max_iter, sk_tau):
+        _sinkhorn_func = functools.partial(sinkhorn,
+                                           dummy_row=False, max_iter=sk_max_iter, tau=sk_tau, batched_operation=False)
+        emb = v0
+        A = (K != 0)
+        emb_K = np.expand_dims(K,axis=-1)
+
+        # NGM qap solver
+        for i in range(self.gnn_layer):
+            gnn_layer = self.dict['gnn_layer_{}'.format(i)]
+            emb_K, emb = gnn_layer.forward(A, emb_K, emb, n1, n2, sk_func=_sinkhorn_func)
+        v = self.classifier.forward(emb)
+        
+        s = v.reshape(v.shape[0], n2max, -1).swapaxes(1, 2)
+        
+        return _sinkhorn_func(s, n1, n2, dummy_row=True)
+
+ngm_pretrain_path = {
+    'voc':('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=1LY93fLCjH5vDcWsjZxGPmXmrYMF8HZIR',
+           '19cd48afab71b3277d2062624934702c'),
+    'willow':('https://drive.google.com/u/0/uc?export=download&confirm=Z-AR&id=1iD8FHqahRsVV_H6o3ByB6nwBHU8sEgnt',
+              '31968e30c399845f34d80733d0118b8b')
+}
+
+def ngm(K, n1, n2, n1max, n2max, x0, gnn_channels, sk_emb, sk_max_iter, sk_tau, network, return_network, pretrain):
+    """
+    Numpy implementation of NGM
+    """
+    if K is None:
+        forward_pass = False
+    else:
+        forward_pass = True
+    if network is None:
+        network = NGM_Net(gnn_channels, sk_emb)
+        if pretrain:
+            if pretrain in ngm_pretrain_path.keys():
+                url, md5 = ngm_pretrain_path[pretrain]
+                try:
+                    filename = pygmtools.utils.download(f'ngm_{pretrain}_numpy.npy', url, md5)
+                except:
+                    filename = os.path.dirname(__file__) + f'/temp/ngm_{pretrain}_numpy.npy'
+                ngm_numpy_dict = np.load(filename, allow_pickle=True)
+                for i in range(network.gnn_layer):
+                    gnn_layer = network.dict['gnn_layer_{}'.format(i)]
+                    gnn_layer.classifier.weight = ngm_numpy_dict.item()['gnn_layer_{}.classifier.weight'.format(i)]
+                    gnn_layer.classifier.bias = ngm_numpy_dict.item()['gnn_layer_{}.classifier.bias'.format(i)]
+                    gnn_layer.n_func.getitem(0).weight = ngm_numpy_dict.item()['gnn_layer_{}.n_func.0.weight'.format(i)]
+                    gnn_layer.n_func.getitem(0).bias = ngm_numpy_dict.item()['gnn_layer_{}.n_func.0.bias'.format(i)]
+                    gnn_layer.n_func.getitem(2).weight = ngm_numpy_dict.item()['gnn_layer_{}.n_func.2.weight'.format(i)]
+                    gnn_layer.n_func.getitem(2).bias = ngm_numpy_dict.item()['gnn_layer_{}.n_func.2.bias'.format(i)]
+                    gnn_layer.n_self_func.getitem(0).weight = ngm_numpy_dict.item()['gnn_layer_{}.n_self_func.0.weight'.format(i)]
+                    gnn_layer.n_self_func.getitem(0).bias = ngm_numpy_dict.item()['gnn_layer_{}.n_self_func.0.bias'.format(i)]
+                    gnn_layer.n_self_func.getitem(2).weight = ngm_numpy_dict.item()['gnn_layer_{}.n_self_func.2.weight'.format(i)]
+                    gnn_layer.n_self_func.getitem(2).bias = ngm_numpy_dict.item()['gnn_layer_{}.n_self_func.2.bias'.format(i)]
+                network.classifier.weight = ngm_numpy_dict.item()['classifier.weight']
+                network.classifier.bias = ngm_numpy_dict.item()['classifier.bias']
+            else:
+                raise ValueError(f'Unknown pretrain tag. Available tags: {ngm_pretrain_path.keys()}')
+    if forward_pass:
+        batch_num, n1, n2, n1max, n2max, n1n2, v0 = _check_and_init_gm(K, n1, n2, n1max, n2max, x0)
+        v0 = v0 / np.mean(v0)
+        result = network.forward(K, n1, n2, n1max, n2max, v0, sk_max_iter, sk_tau)
+    else:
+        result = None
+    return result, network
+#############################################
+#              Utils Functions              #
+#############################################
+
+
+def inner_prod_aff_fn(feat1, feat2):
+    """
+    numpy implementation of inner product affinity function
+    """
+    return np.matmul(feat1, feat2.swapaxes(1,2))
+
+
+def gaussian_aff_fn(feat1, feat2, sigma):
+    """
+    numpy implementation of Gaussian affinity function
+    """
+    feat1 = np.expand_dims(feat1, axis=2)
+    feat2 = np.expand_dims(feat2, axis=1)
+    return np.exp(-((feat1 - feat2) ** 2).sum(axis=-1) / sigma)
+
+
+def build_batch(input, return_ori_dim=False):
+    """
+    numpy implementation of building a batched np.ndarray
+    """
+    assert type(input[0]) == np.ndarray
+    it = iter(input)
+    t = next(it)
+    max_shape = list(t.shape)
+    ori_shape = tuple([[_] for _ in max_shape])
+    while True:
+        try:
+            t = next(it)
+            for i in range(len(max_shape)):
+                max_shape[i] = int(max(max_shape[i], t.shape[i]))
+                ori_shape[i].append(t.shape[i])
+        except StopIteration:
+            break
+    max_shape = np.array(max_shape)
+
+    padded_ts = []
+    for t in input:
+        pad_pattern = np.zeros((len(max_shape), 2), dtype=np.int64)
+        pad_pattern[:, 1] = max_shape - np.array(t.shape)
+        padded_ts.append(np.pad(t, pad_pattern, 'constant', constant_values=0))
+
+    if return_ori_dim:
+        return np.stack(padded_ts, axis=0), ori_shape
+    else:
+        return np.stack(padded_ts, axis=0)
+
+
+def dense_to_sparse(dense_adj):
+    """
+    numpy implementation of converting a dense adjacency matrix to a sparse matrix
+    """
+    batch_size = dense_adj.shape[0]
+    conn, ori_dim = build_batch([np.stack(np.nonzero(a), axis=1) for a in dense_adj], return_ori_dim=True)
+    nedges = ori_dim[0]
+    edge_weight = build_batch([dense_adj[b][(conn[b, :, 0], conn[b, :, 1])] for b in range(batch_size)])
+    return conn, np.expand_dims(edge_weight, axis=-1), nedges
+
+
+def compute_affinity_score(X, K):
+    """
+    Numpy implementation of computing affinity score
+    """
+    b, n, _ = X.shape
+    vx = X.swapaxes(1,2).reshape(b, -1, 1)  # (b, n*n, 1)
+    vxt = vx.swapaxes(1, 2)  # (b, 1, n*n)
+    affinity = np.squeeze(np.squeeze(np.matmul(np.matmul(vxt, K), vx),axis=-1),axis=-1)
+    return affinity
+
+
+def to_numpy(input):
+    """
+    identity function
+    """
+    return input
+
+
+def from_numpy(input, device):
+    """
+    identity function
+    """
+    return input
+
+
+def generate_isomorphic_graphs(node_num, graph_num, node_feat_dim=0):
+    """
+    Numpy implementation of generate_isomorphic_graphs
+    """
+    X_gt = np.zeros((graph_num, node_num, node_num))
+    X_gt[0, np.arange(0, node_num, dtype='i4'), np.arange(0, node_num, dtype='i4')] = 1
+    for i in range(graph_num):
+        if i > 0:
+            X_gt[i, np.arange(0, node_num, dtype='i4'), np.random.permutation(node_num)] = 1
+    joint_X = X_gt.reshape(graph_num * node_num, node_num)
+    X_gt = np.matmul(joint_X, joint_X.T)
+    X_gt = X_gt.reshape(graph_num, node_num, graph_num, node_num).transpose(0, 2, 1, 3)
+    A0 = np.random.rand(node_num, node_num)
+    A0[np.arange(node_num),np.arange(node_num)] = 0
+    As = [A0]
+    for i in range(graph_num):
+        if i > 0:
+            As.append(np.matmul(np.matmul(X_gt[i, 0], A0), X_gt[0, i]))
+    if node_feat_dim > 0:
+        F0 = np.random.rand(node_num, node_feat_dim)
+        Fs = [F0]
+        for i in range(graph_num):
+            if i > 0:
+                Fs.append(np.matmul(X_gt[i, 0], F0))
+        return np.stack(As,axis=0), X_gt, np.stack(Fs,axis=0)
+    else:
+        return np.stack(As,axis=0), X_gt
+
+def _aff_mat_from_node_edge_aff(node_aff: np.ndarray, edge_aff: np.ndarray, connectivity1: np.ndarray, connectivity2: np.ndarray,
+                                n1, n2, ne1, ne2):
+    """
+    numpy implementation of _aff_mat_from_node_edge_aff
+    """
+    if edge_aff is not None:
+        dtype = edge_aff.dtype
+        batch_size = edge_aff.shape[0]
+        if n1 is None:
+            n1 = np.amax(connectivity1, axis=(1, 2)).copy() + 1
+        if n2 is None:
+            n2 = np.amax(connectivity2, axis=(1, 2)).copy() + 1
+        if ne1 is None:
+            ne1 = [edge_aff.shape[1]] * batch_size
+        if ne2 is None:
+            ne2 = [edge_aff.shape[2]] * batch_size
+    else:
+        dtype = node_aff.dtype
+        batch_size = node_aff.shape[0]
+        if n1 is None:
+            n1 = [node_aff.shape[1]] * batch_size
+        if n2 is None:
+            n2 = [node_aff.shape[2]] * batch_size
+
+    n1max = max(n1)
+    n2max = max(n2)
+    ks = []
+    for b in range(batch_size):
+        k = np.zeros((n2max, n1max, n2max, n1max), dtype=dtype)
+        # edge-wise affinity
+        if edge_aff is not None:
+            conn1 = connectivity1[b][:ne1[b]]
+            conn2 = connectivity2[b][:ne2[b]]
+            edge_indices = np.concatenate([conn1.repeat(ne2[b], axis=0), np.tile(conn2, (ne1[b], 1))], axis=1) # indices: start_g1, end_g1, start_g2, end_g2
+            edge_indices = (edge_indices[:, 2], edge_indices[:, 0], edge_indices[:, 3], edge_indices[:, 1]) # indices: start_g2, start_g1, end_g2, end_g1
+            k[edge_indices] = edge_aff[b, :ne1[b], :ne2[b]].reshape(-1)
+        k = k.reshape((n2max * n1max, n2max * n1max))
+        # node-wise affinity
+        if node_aff is not None:
+            k[np.arange(n2max * n1max), np.arange(n2max * n1max)] = node_aff[b].T.reshape(-1)
+        ks.append(k)
+
+    return np.stack(ks, axis=0)
+
+
+def _check_data_type(input: np.ndarray, var_name, raise_err):
+    """
+    numpy implementation of _check_data_type
+    """
+    if raise_err and type(input) is not np.ndarray:
+        raise ValueError(f'Expected numpy ndarray{f" for variable {var_name}" if var_name is not None else ""}, '
+                         f'but got {type(input)}. Perhaps the wrong backend?')
+    return type(input) is np.ndarray
+
+
+def _check_shape(input: np.ndarray, dim_num):
+    """
+    numpy implementation of _check_shape
+    """
+    return len(input.shape) == dim_num
+
+
+def _get_shape(input: np.ndarray):
+    """
+    numpy implementation of _get_shape
+    """
+    return input.shape
+
+
+def _squeeze(input: np.ndarray, dim):
+    """
+    numpy implementation of _squeeze
+    """
+    return np.squeeze(input, axis=dim)
+
+
+def _unsqueeze(input: np.ndarray, dim):
+    """
+    numpy implementation of _unsqueeze
+    """
+    return np.expand_dims(input, axis=dim)
+
+
+def _transpose(input: np.ndarray, dim1, dim2):
+    """
+    numpy implementation of _transpose
+    """
+    return np.swapaxes(input, dim1, dim2)
+
+
+def _mm(input1: np.ndarray, input2: np.ndarray):
+    """
+    numpy implementation of _mm
+    """
+    return np.matmul(input1, input2)
```

### Comparing `pygmtools-0.3.8/pygmtools/paddle_modules.py` & `pygmtools-0.3.8a0/pygmtools/paddle_modules.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,268 +1,268 @@
-import paddle
-import paddle.nn as nn
-import paddle.nn.functional as F
-from paddle import Tensor
-from typing import Tuple, Optional, List, Union
-import math
-
-############################################
-#            Affinity Modules              #
-############################################
-
-class WeightedInnerProdAffinity(nn.Layer):
-    """
-    Weighted inner product affinity layer to compute the affinity matrix from feature space.
-    M = X * A * Y^T
-    Parameter: scale of weight d
-    Input: feature X, Y
-    Output: affinity matrix M
-    """
-    def __init__(self, d):
-        super(WeightedInnerProdAffinity, self).__init__()
-        self.d = d
-        self.A=paddle.create_parameter([self.d, self.d],dtype="float32")
-        self.reset_parameters()
-
-    def reset_parameters(self):
-        stdv = 1. / math.sqrt(self.d)
-        self.A.set_value(paddle.uniform([self.d, self.d],min=-stdv, max=stdv)+paddle.eye(self.d))
-
-    def forward(self, X, Y):
-        assert X.shape[2] == Y.shape[2] == self.d
-        M = paddle.matmul(X, self.A)
-        M = paddle.matmul(M, Y.transpose([0, 2, 1]))
-        return M
-
-
-############################################
-#         Graph Convolution Modules        #
-############################################
-
-weight_init = nn.initializer.KaimingUniform(fan_in=None, negative_slope=math.sqrt(5), nonlinearity='leaky_relu')
-
-class Gconv(nn.Layer):
-    r"""
-    Graph Convolutional Layer which is inspired and developed based on Graph Convolutional Network (GCN).
-    Inspired by `Kipf and Welling. Semi-Supervised Classification with Graph Convolutional Networks. ICLR 2017.
-    <https://arxiv.org/abs/1609.02907>`_
-
-    :param in_features: the dimension of input node features
-    :param out_features: the dimension of output node features
-    """
-    def __init__(self, in_features: int, out_features: int):
-        super(Gconv, self).__init__()
-        self.num_inputs = in_features
-        self.num_outputs = out_features
-        self.a_fc = nn.Linear(self.num_inputs, self.num_outputs, weight_attr=weight_init)
-        self.u_fc = nn.Linear(self.num_inputs, self.num_outputs, weight_attr=weight_init)
-
-    def forward(self, A: Tensor, x: Tensor, norm: bool=True) -> Tensor:
-        r"""
-        Forward computation of graph convolution network.
-
-        :param A: :math:`(b\times n\times n)` {0,1} adjacency matrix. :math:`b`: batch size, :math:`n`: number of nodes
-        :param x: :math:`(b\times n\times d)` input node embedding. :math:`d`: feature dimension
-        :param norm: normalize connectivity matrix or not
-        :return: :math:`(b\times n\times d^\prime)` new node embedding
-        """
-        if norm is True:
-            A = F.normalize(A, p=1, axis=-2)
-        ax = self.a_fc(x)
-        ux = self.u_fc(x)
-        x = paddle.bmm(A, F.relu(ax)) + F.relu(ux) # has size (bs, N, num_outputs)
-        return x
-
-class Siamese_Gconv(nn.Layer):
-    r"""
-    Siamese Gconv neural network for processing arbitrary number of graphs.
-
-    :param in_features: the dimension of input node features
-    :param num_features: the dimension of output node features
-    """
-    def __init__(self, in_features, num_features):
-        super(Siamese_Gconv, self).__init__()
-        self.gconv = Gconv(in_features, num_features)
-
-    def forward(self, g1: Tuple[Tensor, Tensor, Tensor, int], *args) -> Union[Tensor, List[Tensor]]:
-        r"""
-        Forward computation of Siamese Gconv.
-
-        :param g1: The first graph, which is a tuple of (:math:`(b\times n\times n)` {0,1} adjacency matrix,
-         :math:`(b\times n\times d)` input node embedding, normalize connectivity matrix or not)
-        :param args: Other graphs
-        :return: A list of tensors composed of new node embeddings :math:`(b\times n\times d^\prime)`
-        """
-        emb1 = self.gconv(*g1)
-        if len(args) == 0:
-            return emb1
-        else:
-            returns = [emb1]
-            for g in args:
-                returns.append(self.gconv(*g))
-            return returns
-
-class ChannelIndependentConv(nn.Layer):
-    r"""
-    Channel Independent Embedding Convolution.
-    Proposed by `"Yu et al. Learning deep graph matching with channel-independent embedding and Hungarian attention.
-    ICLR 2020." <https://openreview.net/forum?id=rJgBd2NYPH>`_
-
-    :param in_features: the dimension of input node features
-    :param out_features: the dimension of output node features
-    :param in_edges: the dimension of input edge features
-    :param out_edges: (optional) the dimension of output edge features. It needs to be the same as ``out_features``
-    """
-    def __init__(self, in_features: int, out_features: int, in_edges: int, out_edges: int=None):
-        super(ChannelIndependentConv, self).__init__()
-        if out_edges is None:
-            out_edges = out_features
-        self.in_features = in_features
-        self.out_features = out_features
-        self.out_edges = out_edges
-        # self.node_fc = nn.Linear(in_features, out_features // self.out_edges)
-        self.node_fc = nn.Linear(in_features, out_features, weight_attr=weight_init) 
-        self.node_sfc = nn.Linear(in_features, out_features, weight_attr=weight_init) 
-        self.edge_fc = nn.Linear(in_edges, self.out_edges, weight_attr=weight_init) 
-
-    def forward(self, A: Tensor, emb_node: Tensor, emb_edge: Tensor, mode: int=1) -> Tuple[Tensor, Tensor]:
-        r"""
-        :param A: :math:`(b\times n\times n)` {0,1} adjacency matrix. :math:`b`: batch size, :math:`n`: number of nodes
-        :param emb_node: :math:`(b\times n\times d_n)` input node embedding. :math:`d_n`: node feature dimension
-        :param emb_edge: :math:`(b\times n\times n\times d_e)` input edge embedding. :math:`d_e`: edge feature dimension
-        :param mode: 1 or 2, refer to the paper for details
-        :return: :math:`(b\times n\times d^\prime)` new node embedding,
-         :math:`(b\times n\times n\times d^\prime)` new edge embedding
-        """
-        if mode == 1:
-            node_x = self.node_fc(emb_node)
-            node_sx = self.node_sfc(emb_node)
-            edge_x = self.edge_fc(emb_edge)
-
-            A = A.unsqueeze(-1)
-            A = paddle.multiply(A.expand_as(edge_x), edge_x)
-
-            node_x = paddle.matmul(A.transpose([0,3,1,2]), node_x.unsqueeze(2).transpose([0,3,1,2]))
-            node_x = node_x.squeeze(-1).transpose([0,2,1])
-            node_x = F.relu(node_x) + F.relu(node_sx)
-            edge_x = F.relu(edge_x)
-
-            return node_x, edge_x
-
-        elif mode == 2:
-            node_x = self.node_fc(emb_node)
-            node_sx = self.node_sfc(emb_node)
-            edge_x = self.edge_fc(emb_edge)
-
-            d_x = node_x.unsqueeze(1) - node_x.unsqueeze(2)
-            d_x = paddle.sum(d_x ** 2, axis=3, keepdim=False)
-            d_x = paddle.exp(-d_x)
-
-            A = A.unsqueeze(-1)
-            A = paddle.multiply(A.expand_as(edge_x), edge_x)
-
-            node_x = paddle.matmul(A.transpose([0,3,1, 2]), node_x.unsqueeze(2).transpose([0,3,1,2]))
-            node_x = node_x.squeeze(-1).transpose([0,2, 1])
-            node_x = F.relu(node_x) + F.relu(node_sx)
-            edge_x = F.relu(edge_x)
-            return node_x, edge_x
-
-        else:
-            raise ValueError('Unknown mode {}. Possible options: 1 or 2'.format(mode))
-
-class Siamese_ChannelIndependentConv(nn.Layer):
-    r"""
-    Siamese Channel Independent Conv neural network for processing arbitrary number of graphs.
-
-    :param in_features: the dimension of input node features
-    :param num_features: the dimension of output node features
-    :param in_edges: the dimension of input edge features
-    :param out_edges: (optional) the dimension of output edge features. It needs to be the same as ``num_features``
-    """
-    def __init__(self, in_features, num_features, in_edges, out_edges=None):
-        super(Siamese_ChannelIndependentConv, self).__init__()
-        self.in_feature = in_features
-        self.gconv = ChannelIndependentConv(in_features, num_features, in_edges, out_edges)
-
-    def forward(self, g1: Tuple[Tensor, Tensor, Optional[bool]], *args) -> List[Tensor]:
-        r"""
-        Forward computation of Siamese Channel Independent Conv.
-
-        :param g1: The first graph, which is a tuple of (:math:`(b\times n\times n)` {0,1} adjacency matrix,
-         :math:`(b\times n\times d_n)` input node embedding, :math:`(b\times n\times n\times d_e)` input edge embedding,
-         mode (``1`` or ``2``))
-        :param args: Other graphs
-        :return: A list of tensors composed of new node embeddings :math:`(b\times n\times d^\prime)`, appended with new
-         edge embeddings :math:`(b\times n\times n\times d^\prime)`
-        """
-        emb1, emb_edge1 = self.gconv(*g1)
-        embs = [emb1]
-        emb_edges = [emb_edge1]
-        for g in args:
-            emb2, emb_edge2 = self.gconv(*g)
-            embs.append(emb2), emb_edges.append(emb_edge2)
-        return embs + emb_edges
-
-class NGMConvLayer(nn.Layer):
-    def __init__(self, in_node_features, in_edge_features, out_node_features, out_edge_features,
-                 sk_channel=0):
-        super(NGMConvLayer, self).__init__()
-        self.in_nfeat = in_node_features
-        self.in_efeat = in_edge_features
-        self.out_efeat = out_edge_features
-        self.sk_channel = sk_channel
-        assert out_node_features == out_edge_features + self.sk_channel
-        if self.sk_channel > 0:
-            self.out_nfeat = out_node_features - self.sk_channel
-            self.classifier = nn.Linear(self.out_nfeat, self.sk_channel)
-        else:
-            self.out_nfeat = out_node_features
-            self.classifier = None
-
-        self.n_func = nn.Sequential(
-            nn.Linear(self.in_nfeat, self.out_nfeat),
-            # nn.Linear(self.in_nfeat, self.out_nfeat // self.out_efeat),
-            nn.ReLU(),
-            nn.Linear(self.out_nfeat, self.out_nfeat),
-            # nn.Linear(self.out_nfeat // self.out_efeat, self.out_nfeat // self.out_efeat),
-            nn.ReLU(),
-        )
-
-        self.n_self_func = nn.Sequential(
-            nn.Linear(self.in_nfeat, self.out_nfeat),
-            nn.ReLU(),
-            nn.Linear(self.out_nfeat, self.out_nfeat),
-            nn.ReLU()
-        )
-
-    def forward(self, A, W, x, n1=None, n2=None, norm=True, sk_func=None):
-        """
-        :param A: adjacent matrix in 0/1 (b x n x n)
-        :param W: edge feature tensor (b x n x n x feat_dim)
-        :param x: node feature tensor (b x n x feat_dim)
-        """
-        W_new = W
-
-        if norm is True:
-            A = nn.functional.normalize(A, p=1, axis=2)
-
-        x1 = self.n_func(x)
-        x2 = paddle.matmul((A.unsqueeze(-1) * W_new).transpose(perm=[0, 3, 1, 2]), \
-            x1.unsqueeze(2).transpose(perm=[0, 3, 1, 2])).squeeze(-1).transpose((0,2,1))
-        x2 += self.n_self_func(x)
-
-        if self.classifier is not None:
-            assert n1.max() * n2.max() == x.shape[1]
-            assert sk_func is not None
-            x3 = self.classifier(x2)
-            n1_rep = paddle.repeat_interleave(n1, self.sk_channel, axis=0)
-            n2_rep = paddle.repeat_interleave(n2, self.sk_channel, axis=0)
-            x4 = x3.transpose(perm=[0, 2, 1]).reshape(
-                (x.shape[0] * self.sk_channel, n2.max().item(), n1.max().item())).transpose((0,2,1))
-            x5 = sk_func(x4, n1_rep, n2_rep, dummy_row=True).transpose((0,2,1))
-
-            x6 = x5.reshape((x.shape[0], self.sk_channel, n1.max().item() * n2.max().item())).transpose((0, 2, 1))
-            x_new = paddle.concat((x2, x6), axis=-1)
-        else:
-            x_new = x2
-
+import paddle
+import paddle.nn as nn
+import paddle.nn.functional as F
+from paddle import Tensor
+from typing import Tuple, Optional, List, Union
+import math
+
+############################################
+#            Affinity Modules              #
+############################################
+
+class WeightedInnerProdAffinity(nn.Layer):
+    """
+    Weighted inner product affinity layer to compute the affinity matrix from feature space.
+    M = X * A * Y^T
+    Parameter: scale of weight d
+    Input: feature X, Y
+    Output: affinity matrix M
+    """
+    def __init__(self, d):
+        super(WeightedInnerProdAffinity, self).__init__()
+        self.d = d
+        self.A=paddle.create_parameter([self.d, self.d],dtype="float32")
+        self.reset_parameters()
+
+    def reset_parameters(self):
+        stdv = 1. / math.sqrt(self.d)
+        self.A.set_value(paddle.uniform([self.d, self.d],min=-stdv, max=stdv)+paddle.eye(self.d))
+
+    def forward(self, X, Y):
+        assert X.shape[2] == Y.shape[2] == self.d
+        M = paddle.matmul(X, self.A)
+        M = paddle.matmul(M, Y.transpose([0, 2, 1]))
+        return M
+
+
+############################################
+#         Graph Convolution Modules        #
+############################################
+
+weight_init = nn.initializer.KaimingUniform(fan_in=None, negative_slope=math.sqrt(5), nonlinearity='leaky_relu')
+
+class Gconv(nn.Layer):
+    r"""
+    Graph Convolutional Layer which is inspired and developed based on Graph Convolutional Network (GCN).
+    Inspired by `Kipf and Welling. Semi-Supervised Classification with Graph Convolutional Networks. ICLR 2017.
+    <https://arxiv.org/abs/1609.02907>`_
+
+    :param in_features: the dimension of input node features
+    :param out_features: the dimension of output node features
+    """
+    def __init__(self, in_features: int, out_features: int):
+        super(Gconv, self).__init__()
+        self.num_inputs = in_features
+        self.num_outputs = out_features
+        self.a_fc = nn.Linear(self.num_inputs, self.num_outputs, weight_attr=weight_init)
+        self.u_fc = nn.Linear(self.num_inputs, self.num_outputs, weight_attr=weight_init)
+
+    def forward(self, A: Tensor, x: Tensor, norm: bool=True) -> Tensor:
+        r"""
+        Forward computation of graph convolution network.
+
+        :param A: :math:`(b\times n\times n)` {0,1} adjacency matrix. :math:`b`: batch size, :math:`n`: number of nodes
+        :param x: :math:`(b\times n\times d)` input node embedding. :math:`d`: feature dimension
+        :param norm: normalize connectivity matrix or not
+        :return: :math:`(b\times n\times d^\prime)` new node embedding
+        """
+        if norm is True:
+            A = F.normalize(A, p=1, axis=-2)
+        ax = self.a_fc(x)
+        ux = self.u_fc(x)
+        x = paddle.bmm(A, F.relu(ax)) + F.relu(ux) # has size (bs, N, num_outputs)
+        return x
+
+class Siamese_Gconv(nn.Layer):
+    r"""
+    Siamese Gconv neural network for processing arbitrary number of graphs.
+
+    :param in_features: the dimension of input node features
+    :param num_features: the dimension of output node features
+    """
+    def __init__(self, in_features, num_features):
+        super(Siamese_Gconv, self).__init__()
+        self.gconv = Gconv(in_features, num_features)
+
+    def forward(self, g1: Tuple[Tensor, Tensor, Tensor, int], *args) -> Union[Tensor, List[Tensor]]:
+        r"""
+        Forward computation of Siamese Gconv.
+
+        :param g1: The first graph, which is a tuple of (:math:`(b\times n\times n)` {0,1} adjacency matrix,
+         :math:`(b\times n\times d)` input node embedding, normalize connectivity matrix or not)
+        :param args: Other graphs
+        :return: A list of tensors composed of new node embeddings :math:`(b\times n\times d^\prime)`
+        """
+        emb1 = self.gconv(*g1)
+        if len(args) == 0:
+            return emb1
+        else:
+            returns = [emb1]
+            for g in args:
+                returns.append(self.gconv(*g))
+            return returns
+
+class ChannelIndependentConv(nn.Layer):
+    r"""
+    Channel Independent Embedding Convolution.
+    Proposed by `"Yu et al. Learning deep graph matching with channel-independent embedding and Hungarian attention.
+    ICLR 2020." <https://openreview.net/forum?id=rJgBd2NYPH>`_
+
+    :param in_features: the dimension of input node features
+    :param out_features: the dimension of output node features
+    :param in_edges: the dimension of input edge features
+    :param out_edges: (optional) the dimension of output edge features. It needs to be the same as ``out_features``
+    """
+    def __init__(self, in_features: int, out_features: int, in_edges: int, out_edges: int=None):
+        super(ChannelIndependentConv, self).__init__()
+        if out_edges is None:
+            out_edges = out_features
+        self.in_features = in_features
+        self.out_features = out_features
+        self.out_edges = out_edges
+        # self.node_fc = nn.Linear(in_features, out_features // self.out_edges)
+        self.node_fc = nn.Linear(in_features, out_features, weight_attr=weight_init) 
+        self.node_sfc = nn.Linear(in_features, out_features, weight_attr=weight_init) 
+        self.edge_fc = nn.Linear(in_edges, self.out_edges, weight_attr=weight_init) 
+
+    def forward(self, A: Tensor, emb_node: Tensor, emb_edge: Tensor, mode: int=1) -> Tuple[Tensor, Tensor]:
+        r"""
+        :param A: :math:`(b\times n\times n)` {0,1} adjacency matrix. :math:`b`: batch size, :math:`n`: number of nodes
+        :param emb_node: :math:`(b\times n\times d_n)` input node embedding. :math:`d_n`: node feature dimension
+        :param emb_edge: :math:`(b\times n\times n\times d_e)` input edge embedding. :math:`d_e`: edge feature dimension
+        :param mode: 1 or 2, refer to the paper for details
+        :return: :math:`(b\times n\times d^\prime)` new node embedding,
+         :math:`(b\times n\times n\times d^\prime)` new edge embedding
+        """
+        if mode == 1:
+            node_x = self.node_fc(emb_node)
+            node_sx = self.node_sfc(emb_node)
+            edge_x = self.edge_fc(emb_edge)
+
+            A = A.unsqueeze(-1)
+            A = paddle.multiply(A.expand_as(edge_x), edge_x)
+
+            node_x = paddle.matmul(A.transpose([0,3,1,2]), node_x.unsqueeze(2).transpose([0,3,1,2]))
+            node_x = node_x.squeeze(-1).transpose([0,2,1])
+            node_x = F.relu(node_x) + F.relu(node_sx)
+            edge_x = F.relu(edge_x)
+
+            return node_x, edge_x
+
+        elif mode == 2:
+            node_x = self.node_fc(emb_node)
+            node_sx = self.node_sfc(emb_node)
+            edge_x = self.edge_fc(emb_edge)
+
+            d_x = node_x.unsqueeze(1) - node_x.unsqueeze(2)
+            d_x = paddle.sum(d_x ** 2, axis=3, keepdim=False)
+            d_x = paddle.exp(-d_x)
+
+            A = A.unsqueeze(-1)
+            A = paddle.multiply(A.expand_as(edge_x), edge_x)
+
+            node_x = paddle.matmul(A.transpose([0,3,1, 2]), node_x.unsqueeze(2).transpose([0,3,1,2]))
+            node_x = node_x.squeeze(-1).transpose([0,2, 1])
+            node_x = F.relu(node_x) + F.relu(node_sx)
+            edge_x = F.relu(edge_x)
+            return node_x, edge_x
+
+        else:
+            raise ValueError('Unknown mode {}. Possible options: 1 or 2'.format(mode))
+
+class Siamese_ChannelIndependentConv(nn.Layer):
+    r"""
+    Siamese Channel Independent Conv neural network for processing arbitrary number of graphs.
+
+    :param in_features: the dimension of input node features
+    :param num_features: the dimension of output node features
+    :param in_edges: the dimension of input edge features
+    :param out_edges: (optional) the dimension of output edge features. It needs to be the same as ``num_features``
+    """
+    def __init__(self, in_features, num_features, in_edges, out_edges=None):
+        super(Siamese_ChannelIndependentConv, self).__init__()
+        self.in_feature = in_features
+        self.gconv = ChannelIndependentConv(in_features, num_features, in_edges, out_edges)
+
+    def forward(self, g1: Tuple[Tensor, Tensor, Optional[bool]], *args) -> List[Tensor]:
+        r"""
+        Forward computation of Siamese Channel Independent Conv.
+
+        :param g1: The first graph, which is a tuple of (:math:`(b\times n\times n)` {0,1} adjacency matrix,
+         :math:`(b\times n\times d_n)` input node embedding, :math:`(b\times n\times n\times d_e)` input edge embedding,
+         mode (``1`` or ``2``))
+        :param args: Other graphs
+        :return: A list of tensors composed of new node embeddings :math:`(b\times n\times d^\prime)`, appended with new
+         edge embeddings :math:`(b\times n\times n\times d^\prime)`
+        """
+        emb1, emb_edge1 = self.gconv(*g1)
+        embs = [emb1]
+        emb_edges = [emb_edge1]
+        for g in args:
+            emb2, emb_edge2 = self.gconv(*g)
+            embs.append(emb2), emb_edges.append(emb_edge2)
+        return embs + emb_edges
+
+class NGMConvLayer(nn.Layer):
+    def __init__(self, in_node_features, in_edge_features, out_node_features, out_edge_features,
+                 sk_channel=0):
+        super(NGMConvLayer, self).__init__()
+        self.in_nfeat = in_node_features
+        self.in_efeat = in_edge_features
+        self.out_efeat = out_edge_features
+        self.sk_channel = sk_channel
+        assert out_node_features == out_edge_features + self.sk_channel
+        if self.sk_channel > 0:
+            self.out_nfeat = out_node_features - self.sk_channel
+            self.classifier = nn.Linear(self.out_nfeat, self.sk_channel)
+        else:
+            self.out_nfeat = out_node_features
+            self.classifier = None
+
+        self.n_func = nn.Sequential(
+            nn.Linear(self.in_nfeat, self.out_nfeat),
+            # nn.Linear(self.in_nfeat, self.out_nfeat // self.out_efeat),
+            nn.ReLU(),
+            nn.Linear(self.out_nfeat, self.out_nfeat),
+            # nn.Linear(self.out_nfeat // self.out_efeat, self.out_nfeat // self.out_efeat),
+            nn.ReLU(),
+        )
+
+        self.n_self_func = nn.Sequential(
+            nn.Linear(self.in_nfeat, self.out_nfeat),
+            nn.ReLU(),
+            nn.Linear(self.out_nfeat, self.out_nfeat),
+            nn.ReLU()
+        )
+
+    def forward(self, A, W, x, n1=None, n2=None, norm=True, sk_func=None):
+        """
+        :param A: adjacent matrix in 0/1 (b x n x n)
+        :param W: edge feature tensor (b x n x n x feat_dim)
+        :param x: node feature tensor (b x n x feat_dim)
+        """
+        W_new = W
+
+        if norm is True:
+            A = nn.functional.normalize(A, p=1, axis=2)
+
+        x1 = self.n_func(x)
+        x2 = paddle.matmul((A.unsqueeze(-1) * W_new).transpose(perm=[0, 3, 1, 2]), \
+            x1.unsqueeze(2).transpose(perm=[0, 3, 1, 2])).squeeze(-1).transpose((0,2,1))
+        x2 += self.n_self_func(x)
+
+        if self.classifier is not None:
+            assert n1.max() * n2.max() == x.shape[1]
+            assert sk_func is not None
+            x3 = self.classifier(x2)
+            n1_rep = paddle.repeat_interleave(n1, self.sk_channel, axis=0)
+            n2_rep = paddle.repeat_interleave(n2, self.sk_channel, axis=0)
+            x4 = x3.transpose(perm=[0, 2, 1]).reshape(
+                (x.shape[0] * self.sk_channel, n2.max().item(), n1.max().item())).transpose((0,2,1))
+            x5 = sk_func(x4, n1_rep, n2_rep, dummy_row=True).transpose((0,2,1))
+
+            x6 = x5.reshape((x.shape[0], self.sk_channel, n1.max().item() * n2.max().item())).transpose((0, 2, 1))
+            x_new = paddle.concat((x2, x6), axis=-1)
+        else:
+            x_new = x2
+
         return W_new, x_new
```

### Comparing `pygmtools-0.3.8/pygmtools/pytorch_modules.py` & `pygmtools-0.3.8a0/pygmtools/pytorch_modules.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,287 +1,287 @@
-# Copyright (c) 2022 Thinklab@SJTU
-# pygmtools is licensed under Mulan PSL v2.
-# You can use this software according to the terms and conditions of the Mulan PSL v2.
-# You may obtain a copy of Mulan PSL v2 at:
-# http://license.coscl.org.cn/MulanPSL2
-# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND,
-# EITHER EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT,
-# MERCHANTABILITY OR FIT FOR A PARTICULAR PURPOSE.
-# See the Mulan PSL v2 for more details.
-
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-from torch.nn.parameter import Parameter
-from torch import Tensor
-from typing import Tuple, Optional, List, Union
-import math
-
-
-############################################
-#            Affinity Modules              #
-############################################
-
-
-class WeightedInnerProdAffinity(nn.Module):
-    """
-    Weighted inner product affinity layer to compute the affinity matrix from feature space.
-    M = X * A * Y^T
-    Parameter: scale of weight d
-    Input: feature X, Y
-    Output: affinity matrix M
-    """
-    def __init__(self, d):
-        super(WeightedInnerProdAffinity, self).__init__()
-        self.d = d
-        self.A = Parameter(Tensor(self.d, self.d))
-        self.reset_parameters()
-
-    def reset_parameters(self):
-        stdv = 1. / math.sqrt(self.d)
-        self.A.data.uniform_(-stdv, stdv)
-        self.A.data += torch.eye(self.d)
-
-    def forward(self, X, Y):
-        assert X.shape[2] == Y.shape[2] == self.d
-        M = torch.matmul(X, self.A)
-        M = torch.matmul(M, Y.transpose(1, 2))
-        return M
-
-
-############################################
-#         Graph Convolution Modules        #
-############################################
-
-
-class Gconv(nn.Module):
-    r"""
-    Graph Convolutional Layer which is inspired and developed based on Graph Convolutional Network (GCN).
-    Inspired by `Kipf and Welling. Semi-Supervised Classification with Graph Convolutional Networks. ICLR 2017.
-    <https://arxiv.org/abs/1609.02907>`_
-
-    :param in_features: the dimension of input node features
-    :param out_features: the dimension of output node features
-    """
-    def __init__(self, in_features: int, out_features: int):
-        super(Gconv, self).__init__()
-        self.num_inputs = in_features
-        self.num_outputs = out_features
-        self.a_fc = nn.Linear(self.num_inputs, self.num_outputs)
-        self.u_fc = nn.Linear(self.num_inputs, self.num_outputs)
-
-    def forward(self, A: Tensor, x: Tensor, norm: bool=True) -> Tensor:
-        r"""
-        Forward computation of graph convolution network.
-
-        :param A: :math:`(b\times n\times n)` {0,1} adjacency matrix. :math:`b`: batch size, :math:`n`: number of nodes
-        :param x: :math:`(b\times n\times d)` input node embedding. :math:`d`: feature dimension
-        :param norm: normalize connectivity matrix or not
-        :return: :math:`(b\times n\times d^\prime)` new node embedding
-        """
-        if norm is True:
-            A = F.normalize(A, p=1, dim=-2)
-        ax = self.a_fc(x)
-        ux = self.u_fc(x)
-        x = torch.bmm(A, F.relu(ax)) + F.relu(ux) # has size (bs, N, num_outputs)
-        return x
-
-
-class ChannelIndependentConv(nn.Module):
-    r"""
-    Channel Independent Embedding Convolution.
-    Proposed by `"Yu et al. Learning deep graph matching with channel-independent embedding and Hungarian attention.
-    ICLR 2020." <https://openreview.net/forum?id=rJgBd2NYPH>`_
-
-    :param in_features: the dimension of input node features
-    :param out_features: the dimension of output node features
-    :param in_edges: the dimension of input edge features
-    :param out_edges: (optional) the dimension of output edge features. It needs to be the same as ``out_features``
-    """
-    def __init__(self, in_features: int, out_features: int, in_edges: int, out_edges: int=None):
-        super(ChannelIndependentConv, self).__init__()
-        if out_edges is None:
-            out_edges = out_features
-        self.in_features = in_features
-        self.out_features = out_features
-        self.out_edges = out_edges
-        # self.node_fc = nn.Linear(in_features, out_features // self.out_edges)
-        self.node_fc = nn.Linear(in_features, out_features)
-        self.node_sfc = nn.Linear(in_features, out_features)
-        self.edge_fc = nn.Linear(in_edges, self.out_edges)
-
-    def forward(self, A: Tensor, emb_node: Tensor, emb_edge: Tensor, mode: int=1) -> Tuple[Tensor, Tensor]:
-        r"""
-        :param A: :math:`(b\times n\times n)` {0,1} adjacency matrix. :math:`b`: batch size, :math:`n`: number of nodes
-        :param emb_node: :math:`(b\times n\times d_n)` input node embedding. :math:`d_n`: node feature dimension
-        :param emb_edge: :math:`(b\times n\times n\times d_e)` input edge embedding. :math:`d_e`: edge feature dimension
-        :param mode: 1 or 2, refer to the paper for details
-        :return: :math:`(b\times n\times d^\prime)` new node embedding,
-         :math:`(b\times n\times n\times d^\prime)` new edge embedding
-        """
-        if mode == 1:
-            node_x = self.node_fc(emb_node)
-            node_sx = self.node_sfc(emb_node)
-            edge_x = self.edge_fc(emb_edge)
-
-            A = A.unsqueeze(-1)
-            A = torch.mul(A.expand_as(edge_x), edge_x)
-
-            node_x = torch.matmul(A.transpose(2, 3).transpose(1, 2),
-                                  node_x.unsqueeze(2).transpose(2, 3).transpose(1, 2))
-            node_x = node_x.squeeze(-1).transpose(1, 2)
-            node_x = F.relu(node_x) + F.relu(node_sx)
-            edge_x = F.relu(edge_x)
-
-            return node_x, edge_x
-
-        # The following code lines are not called in pygmtools
-        # elif mode == 2:
-        #     node_x = self.node_fc(emb_node)
-        #     node_sx = self.node_sfc(emb_node)
-        #     edge_x = self.edge_fc(emb_edge)
-        #
-        #     d_x = node_x.unsqueeze(1) - node_x.unsqueeze(2)
-        #     d_x = torch.sum(d_x ** 2, dim=3, keepdim=False)
-        #     d_x = torch.exp(-d_x)
-        #
-        #     A = A.unsqueeze(-1)
-        #     A = torch.mul(A.expand_as(edge_x), edge_x)
-        #
-        #     node_x = torch.matmul(A.transpose(2, 3).transpose(1, 2),
-        #                           node_x.unsqueeze(2).transpose(2, 3).transpose(1, 2))
-        #     node_x = node_x.squeeze(-1).transpose(1, 2)
-        #     node_x = F.relu(node_x) + F.relu(node_sx)
-        #     edge_x = F.relu(edge_x)
-        #     return node_x, edge_x
-
-        else:
-            raise ValueError('Unknown mode {}. Possible options: 1 or 2'.format(mode))
-
-
-class Siamese_Gconv(nn.Module):
-    r"""
-    Siamese Gconv neural network for processing arbitrary number of graphs.
-
-    :param in_features: the dimension of input node features
-    :param num_features: the dimension of output node features
-    """
-    def __init__(self, in_features, num_features):
-        super(Siamese_Gconv, self).__init__()
-        self.gconv = Gconv(in_features, num_features)
-
-    def forward(self, g1: Tuple[Tensor, Tensor, Tensor, int], *args) -> Union[Tensor, List[Tensor]]:
-        r"""
-        Forward computation of Siamese Gconv.
-
-        :param g1: The first graph, which is a tuple of (:math:`(b\times n\times n)` {0,1} adjacency matrix,
-         :math:`(b\times n\times d)` input node embedding, normalize connectivity matrix or not)
-        :param args: Other graphs
-        :return: A list of tensors composed of new node embeddings :math:`(b\times n\times d^\prime)`
-        """
-        # embx are tensors of size (bs, N, num_features)
-        emb1 = self.gconv(*g1)
-        if len(args) == 0:
-            return emb1
-        else:
-            returns = [emb1]
-            for g in args:
-                returns.append(self.gconv(*g))
-            return returns
-
-
-class Siamese_ChannelIndependentConv(nn.Module):
-    r"""
-    Siamese Channel Independent Conv neural network for processing arbitrary number of graphs.
-
-    :param in_features: the dimension of input node features
-    :param num_features: the dimension of output node features
-    :param in_edges: the dimension of input edge features
-    :param out_edges: (optional) the dimension of output edge features. It needs to be the same as ``num_features``
-    """
-    def __init__(self, in_features, num_features, in_edges, out_edges=None):
-        super(Siamese_ChannelIndependentConv, self).__init__()
-        self.in_feature = in_features
-        self.gconv = ChannelIndependentConv(in_features, num_features, in_edges, out_edges)
-
-    def forward(self, g1: Tuple[Tensor, Tensor, Optional[bool]], *args) -> List[Tensor]:
-        r"""
-        Forward computation of Siamese Channel Independent Conv.
-
-        :param g1: The first graph, which is a tuple of (:math:`(b\times n\times n)` {0,1} adjacency matrix,
-         :math:`(b\times n\times d_n)` input node embedding, :math:`(b\times n\times n\times d_e)` input edge embedding,
-         mode (``1`` or ``2``))
-        :param args: Other graphs
-        :return: A list of tensors composed of new node embeddings :math:`(b\times n\times d^\prime)`, appended with new
-         edge embeddings :math:`(b\times n\times n\times d^\prime)`
-        """
-        emb1, emb_edge1 = self.gconv(*g1)
-        embs = [emb1]
-        emb_edges = [emb_edge1]
-        for g in args:
-            emb2, emb_edge2 = self.gconv(*g)
-            embs.append(emb2), emb_edges.append(emb_edge2)
-        return embs + emb_edges
-
-
-class NGMConvLayer(nn.Module):
-    def __init__(self, in_node_features, in_edge_features, out_node_features, out_edge_features,
-                 sk_channel=0):
-        super(NGMConvLayer, self).__init__()
-        self.in_nfeat = in_node_features
-        self.in_efeat = in_edge_features
-        self.out_efeat = out_edge_features
-        self.sk_channel = sk_channel
-        assert out_node_features == out_edge_features + self.sk_channel
-        if self.sk_channel > 0:
-            self.out_nfeat = out_node_features - self.sk_channel
-            self.classifier = nn.Linear(self.out_nfeat, self.sk_channel)
-        else:
-            self.out_nfeat = out_node_features
-            self.classifier = None
-
-        self.n_func = nn.Sequential(
-            nn.Linear(self.in_nfeat, self.out_nfeat),
-            #nn.Linear(self.in_nfeat, self.out_nfeat // self.out_efeat),
-            nn.ReLU(),
-            nn.Linear(self.out_nfeat, self.out_nfeat),
-            #nn.Linear(self.out_nfeat // self.out_efeat, self.out_nfeat // self.out_efeat),
-            nn.ReLU(),
-        )
-
-        self.n_self_func = nn.Sequential(
-            nn.Linear(self.in_nfeat, self.out_nfeat),
-            nn.ReLU(),
-            nn.Linear(self.out_nfeat, self.out_nfeat),
-            nn.ReLU()
-        )
-
-    def forward(self, A, W, x, n1=None, n2=None, norm=True, sk_func=None):
-        """
-        :param A: adjacent matrix in 0/1 (b x n x n)
-        :param W: edge feature tensor (b x n x n x feat_dim)
-        :param x: node feature tensor (b x n x feat_dim)
-        """
-        W_new = W
-
-        if norm is True:
-            A = F.normalize(A, p=1, dim=2)
-
-        x1 = self.n_func(x)
-        x2 = torch.matmul((A.unsqueeze(-1) * W_new).permute(0, 3, 1, 2), x1.unsqueeze(2).permute(0, 3, 1, 2)).squeeze(-1).transpose(1, 2)
-        x2 += self.n_self_func(x)
-
-        if self.classifier is not None:
-            assert n1.max() * n2.max() == x.shape[1]
-            assert sk_func is not None
-            x3 = self.classifier(x2)
-            n1_rep = torch.repeat_interleave(n1, self.sk_channel, dim=0)
-            n2_rep = torch.repeat_interleave(n2, self.sk_channel, dim=0)
-            x4 = x3.permute(0,2,1).reshape(x.shape[0] * self.sk_channel, n2.max(), n1.max()).transpose(1, 2)
-            x5 = sk_func(x4, n1_rep, n2_rep, dummy_row=True).transpose(2, 1).contiguous()
-
-            x6 = x5.reshape(x.shape[0], self.sk_channel, n1.max() * n2.max()).permute(0, 2, 1)
-            x_new = torch.cat((x2, x6), dim=-1)
-        else:
-            x_new = x2
-
-        return W_new, x_new
+# Copyright (c) 2022 Thinklab@SJTU
+# pygmtools is licensed under Mulan PSL v2.
+# You can use this software according to the terms and conditions of the Mulan PSL v2.
+# You may obtain a copy of Mulan PSL v2 at:
+# http://license.coscl.org.cn/MulanPSL2
+# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND,
+# EITHER EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT,
+# MERCHANTABILITY OR FIT FOR A PARTICULAR PURPOSE.
+# See the Mulan PSL v2 for more details.
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from torch.nn.parameter import Parameter
+from torch import Tensor
+from typing import Tuple, Optional, List, Union
+import math
+
+
+############################################
+#            Affinity Modules              #
+############################################
+
+
+class WeightedInnerProdAffinity(nn.Module):
+    """
+    Weighted inner product affinity layer to compute the affinity matrix from feature space.
+    M = X * A * Y^T
+    Parameter: scale of weight d
+    Input: feature X, Y
+    Output: affinity matrix M
+    """
+    def __init__(self, d):
+        super(WeightedInnerProdAffinity, self).__init__()
+        self.d = d
+        self.A = Parameter(Tensor(self.d, self.d))
+        self.reset_parameters()
+
+    def reset_parameters(self):
+        stdv = 1. / math.sqrt(self.d)
+        self.A.data.uniform_(-stdv, stdv)
+        self.A.data += torch.eye(self.d)
+
+    def forward(self, X, Y):
+        assert X.shape[2] == Y.shape[2] == self.d
+        M = torch.matmul(X, self.A)
+        M = torch.matmul(M, Y.transpose(1, 2))
+        return M
+
+
+############################################
+#         Graph Convolution Modules        #
+############################################
+
+
+class Gconv(nn.Module):
+    r"""
+    Graph Convolutional Layer which is inspired and developed based on Graph Convolutional Network (GCN).
+    Inspired by `Kipf and Welling. Semi-Supervised Classification with Graph Convolutional Networks. ICLR 2017.
+    <https://arxiv.org/abs/1609.02907>`_
+
+    :param in_features: the dimension of input node features
+    :param out_features: the dimension of output node features
+    """
+    def __init__(self, in_features: int, out_features: int):
+        super(Gconv, self).__init__()
+        self.num_inputs = in_features
+        self.num_outputs = out_features
+        self.a_fc = nn.Linear(self.num_inputs, self.num_outputs)
+        self.u_fc = nn.Linear(self.num_inputs, self.num_outputs)
+
+    def forward(self, A: Tensor, x: Tensor, norm: bool=True) -> Tensor:
+        r"""
+        Forward computation of graph convolution network.
+
+        :param A: :math:`(b\times n\times n)` {0,1} adjacency matrix. :math:`b`: batch size, :math:`n`: number of nodes
+        :param x: :math:`(b\times n\times d)` input node embedding. :math:`d`: feature dimension
+        :param norm: normalize connectivity matrix or not
+        :return: :math:`(b\times n\times d^\prime)` new node embedding
+        """
+        if norm is True:
+            A = F.normalize(A, p=1, dim=-2)
+        ax = self.a_fc(x)
+        ux = self.u_fc(x)
+        x = torch.bmm(A, F.relu(ax)) + F.relu(ux) # has size (bs, N, num_outputs)
+        return x
+
+
+class ChannelIndependentConv(nn.Module):
+    r"""
+    Channel Independent Embedding Convolution.
+    Proposed by `"Yu et al. Learning deep graph matching with channel-independent embedding and Hungarian attention.
+    ICLR 2020." <https://openreview.net/forum?id=rJgBd2NYPH>`_
+
+    :param in_features: the dimension of input node features
+    :param out_features: the dimension of output node features
+    :param in_edges: the dimension of input edge features
+    :param out_edges: (optional) the dimension of output edge features. It needs to be the same as ``out_features``
+    """
+    def __init__(self, in_features: int, out_features: int, in_edges: int, out_edges: int=None):
+        super(ChannelIndependentConv, self).__init__()
+        if out_edges is None:
+            out_edges = out_features
+        self.in_features = in_features
+        self.out_features = out_features
+        self.out_edges = out_edges
+        # self.node_fc = nn.Linear(in_features, out_features // self.out_edges)
+        self.node_fc = nn.Linear(in_features, out_features)
+        self.node_sfc = nn.Linear(in_features, out_features)
+        self.edge_fc = nn.Linear(in_edges, self.out_edges)
+
+    def forward(self, A: Tensor, emb_node: Tensor, emb_edge: Tensor, mode: int=1) -> Tuple[Tensor, Tensor]:
+        r"""
+        :param A: :math:`(b\times n\times n)` {0,1} adjacency matrix. :math:`b`: batch size, :math:`n`: number of nodes
+        :param emb_node: :math:`(b\times n\times d_n)` input node embedding. :math:`d_n`: node feature dimension
+        :param emb_edge: :math:`(b\times n\times n\times d_e)` input edge embedding. :math:`d_e`: edge feature dimension
+        :param mode: 1 or 2, refer to the paper for details
+        :return: :math:`(b\times n\times d^\prime)` new node embedding,
+         :math:`(b\times n\times n\times d^\prime)` new edge embedding
+        """
+        if mode == 1:
+            node_x = self.node_fc(emb_node)
+            node_sx = self.node_sfc(emb_node)
+            edge_x = self.edge_fc(emb_edge)
+
+            A = A.unsqueeze(-1)
+            A = torch.mul(A.expand_as(edge_x), edge_x)
+
+            node_x = torch.matmul(A.transpose(2, 3).transpose(1, 2),
+                                  node_x.unsqueeze(2).transpose(2, 3).transpose(1, 2))
+            node_x = node_x.squeeze(-1).transpose(1, 2)
+            node_x = F.relu(node_x) + F.relu(node_sx)
+            edge_x = F.relu(edge_x)
+
+            return node_x, edge_x
+
+        # The following code lines are not called in pygmtools
+        # elif mode == 2:
+        #     node_x = self.node_fc(emb_node)
+        #     node_sx = self.node_sfc(emb_node)
+        #     edge_x = self.edge_fc(emb_edge)
+        #
+        #     d_x = node_x.unsqueeze(1) - node_x.unsqueeze(2)
+        #     d_x = torch.sum(d_x ** 2, dim=3, keepdim=False)
+        #     d_x = torch.exp(-d_x)
+        #
+        #     A = A.unsqueeze(-1)
+        #     A = torch.mul(A.expand_as(edge_x), edge_x)
+        #
+        #     node_x = torch.matmul(A.transpose(2, 3).transpose(1, 2),
+        #                           node_x.unsqueeze(2).transpose(2, 3).transpose(1, 2))
+        #     node_x = node_x.squeeze(-1).transpose(1, 2)
+        #     node_x = F.relu(node_x) + F.relu(node_sx)
+        #     edge_x = F.relu(edge_x)
+        #     return node_x, edge_x
+
+        else:
+            raise ValueError('Unknown mode {}. Possible options: 1 or 2'.format(mode))
+
+
+class Siamese_Gconv(nn.Module):
+    r"""
+    Siamese Gconv neural network for processing arbitrary number of graphs.
+
+    :param in_features: the dimension of input node features
+    :param num_features: the dimension of output node features
+    """
+    def __init__(self, in_features, num_features):
+        super(Siamese_Gconv, self).__init__()
+        self.gconv = Gconv(in_features, num_features)
+
+    def forward(self, g1: Tuple[Tensor, Tensor, Tensor, int], *args) -> Union[Tensor, List[Tensor]]:
+        r"""
+        Forward computation of Siamese Gconv.
+
+        :param g1: The first graph, which is a tuple of (:math:`(b\times n\times n)` {0,1} adjacency matrix,
+         :math:`(b\times n\times d)` input node embedding, normalize connectivity matrix or not)
+        :param args: Other graphs
+        :return: A list of tensors composed of new node embeddings :math:`(b\times n\times d^\prime)`
+        """
+        # embx are tensors of size (bs, N, num_features)
+        emb1 = self.gconv(*g1)
+        if len(args) == 0:
+            return emb1
+        else:
+            returns = [emb1]
+            for g in args:
+                returns.append(self.gconv(*g))
+            return returns
+
+
+class Siamese_ChannelIndependentConv(nn.Module):
+    r"""
+    Siamese Channel Independent Conv neural network for processing arbitrary number of graphs.
+
+    :param in_features: the dimension of input node features
+    :param num_features: the dimension of output node features
+    :param in_edges: the dimension of input edge features
+    :param out_edges: (optional) the dimension of output edge features. It needs to be the same as ``num_features``
+    """
+    def __init__(self, in_features, num_features, in_edges, out_edges=None):
+        super(Siamese_ChannelIndependentConv, self).__init__()
+        self.in_feature = in_features
+        self.gconv = ChannelIndependentConv(in_features, num_features, in_edges, out_edges)
+
+    def forward(self, g1: Tuple[Tensor, Tensor, Optional[bool]], *args) -> List[Tensor]:
+        r"""
+        Forward computation of Siamese Channel Independent Conv.
+
+        :param g1: The first graph, which is a tuple of (:math:`(b\times n\times n)` {0,1} adjacency matrix,
+         :math:`(b\times n\times d_n)` input node embedding, :math:`(b\times n\times n\times d_e)` input edge embedding,
+         mode (``1`` or ``2``))
+        :param args: Other graphs
+        :return: A list of tensors composed of new node embeddings :math:`(b\times n\times d^\prime)`, appended with new
+         edge embeddings :math:`(b\times n\times n\times d^\prime)`
+        """
+        emb1, emb_edge1 = self.gconv(*g1)
+        embs = [emb1]
+        emb_edges = [emb_edge1]
+        for g in args:
+            emb2, emb_edge2 = self.gconv(*g)
+            embs.append(emb2), emb_edges.append(emb_edge2)
+        return embs + emb_edges
+
+
+class NGMConvLayer(nn.Module):
+    def __init__(self, in_node_features, in_edge_features, out_node_features, out_edge_features,
+                 sk_channel=0):
+        super(NGMConvLayer, self).__init__()
+        self.in_nfeat = in_node_features
+        self.in_efeat = in_edge_features
+        self.out_efeat = out_edge_features
+        self.sk_channel = sk_channel
+        assert out_node_features == out_edge_features + self.sk_channel
+        if self.sk_channel > 0:
+            self.out_nfeat = out_node_features - self.sk_channel
+            self.classifier = nn.Linear(self.out_nfeat, self.sk_channel)
+        else:
+            self.out_nfeat = out_node_features
+            self.classifier = None
+
+        self.n_func = nn.Sequential(
+            nn.Linear(self.in_nfeat, self.out_nfeat),
+            #nn.Linear(self.in_nfeat, self.out_nfeat // self.out_efeat),
+            nn.ReLU(),
+            nn.Linear(self.out_nfeat, self.out_nfeat),
+            #nn.Linear(self.out_nfeat // self.out_efeat, self.out_nfeat // self.out_efeat),
+            nn.ReLU(),
+        )
+
+        self.n_self_func = nn.Sequential(
+            nn.Linear(self.in_nfeat, self.out_nfeat),
+            nn.ReLU(),
+            nn.Linear(self.out_nfeat, self.out_nfeat),
+            nn.ReLU()
+        )
+
+    def forward(self, A, W, x, n1=None, n2=None, norm=True, sk_func=None):
+        """
+        :param A: adjacent matrix in 0/1 (b x n x n)
+        :param W: edge feature tensor (b x n x n x feat_dim)
+        :param x: node feature tensor (b x n x feat_dim)
+        """
+        W_new = W
+
+        if norm is True:
+            A = F.normalize(A, p=1, dim=2)
+
+        x1 = self.n_func(x)
+        x2 = torch.matmul((A.unsqueeze(-1) * W_new).permute(0, 3, 1, 2), x1.unsqueeze(2).permute(0, 3, 1, 2)).squeeze(-1).transpose(1, 2)
+        x2 += self.n_self_func(x)
+
+        if self.classifier is not None:
+            assert n1.max() * n2.max() == x.shape[1]
+            assert sk_func is not None
+            x3 = self.classifier(x2)
+            n1_rep = torch.repeat_interleave(n1, self.sk_channel, dim=0)
+            n2_rep = torch.repeat_interleave(n2, self.sk_channel, dim=0)
+            x4 = x3.permute(0,2,1).reshape(x.shape[0] * self.sk_channel, n2.max(), n1.max()).transpose(1, 2)
+            x5 = sk_func(x4, n1_rep, n2_rep, dummy_row=True).transpose(2, 1).contiguous()
+
+            x6 = x5.reshape(x.shape[0], self.sk_channel, n1.max() * n2.max()).permute(0, 2, 1)
+            x_new = torch.cat((x2, x6), dim=-1)
+        else:
+            x_new = x2
+
+        return W_new, x_new
```

### Comparing `pygmtools-0.3.8/pygmtools/tensorflow_backend.py` & `pygmtools-0.3.8a0/pygmtools/tensorflow_backend.py`

 * *Files identical despite different names*

### Comparing `pygmtools-0.3.8/pygmtools/utils.py` & `pygmtools-0.3.8a0/pygmtools/utils.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,1257 +1,1257 @@
-"""
-Utility functions: problem formulating, data processing, and beyond.
-"""
-
-# Copyright (c) 2022 Thinklab@SJTU
-# pygmtools is licensed under Mulan PSL v2.
-# You can use this software according to the terms and conditions of the Mulan PSL v2.
-# You may obtain a copy of Mulan PSL v2 at:
-# http://license.coscl.org.cn/MulanPSL2
-# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND,
-# EITHER EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT,
-# MERCHANTABILITY OR FIT FOR A PARTICULAR PURPOSE.
-# See the Mulan PSL v2 for more details.
-
-import time
-import functools
-import importlib
-import copy
-import requests
-import os
-from appdirs import user_cache_dir
-import hashlib
-import shutil
-from tqdm.auto import tqdm
-import inspect
-import wget
-import numpy as np
-import pygmtools
-
-NOT_IMPLEMENTED_MSG = \
-    'The backend function for {} is not implemented. ' \
-    'If you are a user, please check the spelling, and use other backends as workarounds. ' \
-    'If you are a developer, it will be truly appreciated if you could develop and share your' \
-    ' implementation with the community! See our Github: https://github.com/Thinklab-SJTU/pygmtools'
-
-
-def build_aff_mat(node_feat1, edge_feat1, connectivity1, node_feat2, edge_feat2, connectivity2,
-                  n1=None, ne1=None, n2=None, ne2=None,
-                  node_aff_fn=None, edge_aff_fn=None,
-                  backend=None):
-    r"""
-    Build affinity matrix for graph matching from input node/edge features. The affinity matrix encodes both node-wise
-    and edge-wise affinities and formulates the Quadratic Assignment Problem (QAP), which is the mathematical form of
-    graph matching.
-
-    :param node_feat1: :math:`(b\times n_1 \times f_{node})` the node feature of graph1
-    :param edge_feat1: :math:`(b\times ne_1 \times f_{edge})` the edge feature of graph1
-    :param connectivity1: :math:`(b\times ne_1 \times 2)` sparse connectivity information of graph 1.
-                          ``connectivity1[i, j, 0]`` is the starting node index of edge ``j`` at batch ``i``, and
-                          ``connectivity1[i, j, 1]`` is the ending node index of edge ``j`` at batch ``i``
-    :param node_feat2: :math:`(b\times n_2 \times f_{node})` the node feature of graph2
-    :param edge_feat2: :math:`(b\times ne_2 \times f_{edge})` the edge feature of graph2
-    :param connectivity2: :math:`(b\times ne_2 \times 2)` sparse connectivity information of graph 2.
-                          ``connectivity2[i, j, 0]`` is the starting node index of edge ``j`` at batch ``i``, and
-                          ``connectivity2[i, j, 1]`` is the ending node index of edge ``j`` at batch ``i``
-    :param n1: :math:`(b)` number of nodes in graph1. If not given, it will be inferred based on the shape of
-               ``node_feat1`` or the values in ``connectivity1``
-    :param ne1: :math:`(b)` number of edges in graph1. If not given, it will be inferred based on the shape of
-               ``edge_feat1``
-    :param n2: :math:`(b)` number of nodes in graph2. If not given, it will be inferred based on the shape of
-               ``node_feat2`` or the values in ``connectivity2``
-    :param ne2: :math:`(b)` number of edges in graph2. If not given, it will be inferred based on the shape of
-               ``edge_feat2``
-    :param node_aff_fn: (default: inner_prod_aff_fn) the node affinity function with the characteristic
-                        ``node_aff_fn(2D Tensor, 2D Tensor) -> 2D Tensor``, which accepts two node feature tensors and
-                        outputs the node-wise affinity tensor. See :func:`~pygmtools.utils.inner_prod_aff_fn` as an
-                        example.
-    :param edge_aff_fn: (default: inner_prod_aff_fn) the edge affinity function with the characteristic
-                        ``edge_aff_fn(2D Tensor, 2D Tensor) -> 2D Tensor``, which accepts two edge feature tensors and
-                        outputs the edge-wise affinity tensor. See :func:`~pygmtools.utils.inner_prod_aff_fn` as an
-                        example.
-    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
-    :return: :math:`(b\times n_1n_2 \times n_1n_2)` the affinity matrix
-
-    .. note::
-        This function also supports non-batched input, by ignoring all batch dimensions in the input tensors.
-
-    .. dropdown:: Numpy Example
-
-        ::
-
-            >>> import numpy as np
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'numpy'
-
-            # Generate a batch of graphs
-            >>> batch_size = 10
-            >>> A1 = np.random.rand(batch_size, 4, 4)
-            >>> A2 = np.random.rand(batch_size, 4, 4)
-            >>> n1 = n2 = np.repeat([4], batch_size)
-
-            # Build affinity matrix by the default inner-product function
-            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
-            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
-            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, ne1, n2, ne2)
-
-            # Build affinity matrix by gaussian kernel
-            >>> import functools
-            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.)
-            >>> K2 = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, ne1, n2, ne2, edge_aff_fn=gaussian_aff)
-
-            # Build affinity matrix based on node features
-            >>> F1 = np.random.rand(batch_size, 4, 10)
-            >>> F2 = np.random.rand(batch_size, 4, 10)
-            >>> K3 = pygm.utils.build_aff_mat(F1, edge1, conn1, F2, edge2, conn2, n1, ne1, n2, ne2, edge_aff_fn=gaussian_aff)
-
-            # The affinity matrices K, K2, K3 can be further processed by GM solvers
-
-    .. dropdown:: Pytorch Example
-
-        ::
-
-            >>> import torch
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'pytorch'
-
-            # Generate a batch of graphs
-            >>> batch_size = 10
-            >>> A1 = torch.rand(batch_size, 4, 4)
-            >>> A2 = torch.rand(batch_size, 4, 4)
-            >>> n1 = n2 = torch.tensor([4] * batch_size)
-
-            # Build affinity matrix by the default inner-product function
-            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
-            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
-            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, ne1, n2, ne2)
-
-            # Build affinity matrix by gaussian kernel
-            >>> import functools
-            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.)
-            >>> K2 = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, ne1, n2, ne2, edge_aff_fn=gaussian_aff)
-
-            # Build affinity matrix based on node features
-            >>> F1 = torch.rand(batch_size, 4, 10)
-            >>> F2 = torch.rand(batch_size, 4, 10)
-            >>> K3 = pygm.utils.build_aff_mat(F1, edge1, conn1, F2, edge2, conn2, n1, ne1, n2, ne2, edge_aff_fn=gaussian_aff)
-
-            # The affinity matrices K, K2, K3 can be further processed by GM solvers
-    
-    .. dropdown:: Paddle Example
-
-        ::
-            >>> import paddle
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'paddle'
-
-            # Generate a batch of graphs
-            >>> batch_size = 10
-            >>> A1 = paddle.rand((batch_size, 4, 4))
-            >>> A2 = paddle.rand((batch_size, 4, 4))
-            >>> n1 = n2 = paddle.t0_tensor([4] * batch_size)
-
-            # Build affinity matrix by the default inner-product function
-            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
-            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
-            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, ne1, n2, ne2)
-
-            # Build affinity matrix by gaussian kernel
-            >>> import functools
-            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.)
-            >>> K2 = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, ne1, n2, ne2, edge_aff_fn=gaussian_aff)
-
-            # Build affinity matrix based on node features
-            >>> F1 = paddle.rand((batch_size, 4, 10))
-            >>> F2 = paddle.rand((batch_size, 4, 10))
-            >>> K3 = pygm.utils.build_aff_mat(F1, edge1, conn1, F2, edge2, conn2, n1, ne1, n2, ne2, edge_aff_fn=gaussian_aff)
-
-            # The affinity matrices K, K2, K3 can be further processed by GM solvers
-
-    .. dropdown:: mindspore Example
-
-        ::
-
-            >>> import mindspore
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'mindspore'
-
-            # Generate a batch of graphs
-            >>> batch_size = 10
-            >>> A1 = mindspore.numpy.rand((batch_size, 4, 4))
-            >>> A2 = mindspore.numpy.rand((batch_size, 4, 4))
-            >>> n1 = n2 = mindspore.Tensor([4] * batch_size)
-
-            # Build affinity matrix by the default inner-product function
-            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
-            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
-            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, ne1, n2, ne2)
-
-            # Build affinity matrix by gaussian kernel
-            >>> import functools
-            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.)
-            >>> K2 = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, ne1, n2, ne2, edge_aff_fn=gaussian_aff)
-
-            # Build affinity matrix based on node features
-            >>> F1 = mindspore.numpy.rand((batch_size, 4, 10))
-            >>> F2 = mindspore.numpy.rand((batch_size, 4, 10))
-            >>> K3 = pygm.utils.build_aff_mat(F1, edge1, conn1, F2, edge2, conn2, n1, ne1, n2, ne2, edge_aff_fn=gaussian_aff)
-
-            # The affinity matrices K, K2, K3 can be further processed by GM solvers
-
-    """
-    if backend is None:
-        backend = pygmtools.BACKEND
-    __get_shape = functools.partial(_get_shape, backend=backend)
-
-    # check the correctness of input
-    batch_size = None
-    non_batched_input = False
-    if node_feat1 is not None or node_feat2 is not None:
-        assert all([_ is not None for _ in (node_feat1, node_feat2)]), \
-            'The following arguments must all be given if you want to compute node-wise affinity: ' \
-            'node_feat1, node_feat2'
-        _check_data_type(node_feat1, backend)
-        _check_data_type(node_feat2, backend)
-        if all([_check_shape(_, 2, backend) for _ in (node_feat1, node_feat2)]):
-            non_batched_input = True
-            node_feat1, node_feat2 = [_unsqueeze(_, 0, backend) for _ in (node_feat1, node_feat2)]
-            if type(n1) is int: n1 = from_numpy(np.array([n1]), backend=backend)
-            if type(n2) is int: n2 = from_numpy(np.array([n2]), backend=backend)
-        elif all([_check_shape(_, 3, backend) for _ in (node_feat1, node_feat2)]):
-            pass
-        else:
-            raise ValueError(
-                f'The shape of the following tensors are illegal, expected 3-dimensional, '
-                f'got node_feat1={len(__get_shape(node_feat1))}d; node_feat2={len(__get_shape(node_feat2))}d!'
-            )
-        if batch_size is None:
-            batch_size = __get_shape(node_feat1)[0]
-        assert __get_shape(node_feat1)[0] == __get_shape(node_feat2)[0] == batch_size, 'batch size mismatch'
-    if edge_feat1 is not None or edge_feat2 is not None:
-        assert all([_ is not None for _ in (edge_feat1, edge_feat2, connectivity1, connectivity2)]), \
-            'The following arguments must all be given if you want to compute edge-wise affinity: ' \
-            'edge_feat1, edge_feat2, connectivity1, connectivity2'
-        if all([_check_shape(_, 2, backend) for _ in (edge_feat1, edge_feat2, connectivity1, connectivity2)]):
-            non_batched_input = True
-            edge_feat1, edge_feat2, connectivity1, connectivity2 = \
-                [_unsqueeze(_, 0, backend) for _ in (edge_feat1, edge_feat2, connectivity1, connectivity2)]
-            if type(ne1) is int: ne1 = from_numpy(np.array([ne1]), backend=backend)
-            if type(ne2) is int: ne2 = from_numpy(np.array([ne2]), backend=backend)
-        elif all([_check_shape(_, 3, backend) for _ in (edge_feat1, edge_feat2, connectivity1, connectivity2)]):
-            pass
-        else:
-            raise ValueError(
-                f'The shape of the following tensors are illegal, expected 3-dimensional, '
-                f'got edge_feat1:{len(__get_shape(edge_feat1))}d; edge_feat2:{len(__get_shape(edge_feat2))}d; '
-                f'connectivity1:{len(__get_shape(connectivity1))}d; connectivity2:{len(__get_shape(connectivity2))}d!'
-            )
-        assert __get_shape(connectivity1)[2] == __get_shape(connectivity1)[2] == 2, \
-            'the last dimension of connectivity1, connectivity2 must be 2-dimensional'
-        if batch_size is None:
-            batch_size = __get_shape(edge_feat1)[0]
-        assert __get_shape(edge_feat1)[0] == __get_shape(edge_feat2)[0] == __get_shape(connectivity1)[0] == \
-               __get_shape(connectivity2)[0] == batch_size, 'batch size mismatch'
-
-    # assign the default affinity functions if not given
-    if node_aff_fn is None:
-        node_aff_fn = functools.partial(inner_prod_aff_fn, backend=backend)
-    if edge_aff_fn is None:
-        edge_aff_fn = functools.partial(inner_prod_aff_fn, backend=backend)
-
-    node_aff = node_aff_fn(node_feat1, node_feat2) if node_feat1 is not None else None
-    edge_aff = edge_aff_fn(edge_feat1, edge_feat2) if edge_feat1 is not None else None
-
-    result = _aff_mat_from_node_edge_aff(node_aff, edge_aff, connectivity1, connectivity2, n1, n2, ne1, ne2,
-                                         backend=backend)
-    if non_batched_input:
-        return _squeeze(result, 0, backend)
-    else:
-        return result
-
-
-def inner_prod_aff_fn(feat1, feat2, backend=None):
-    r"""
-    Inner product affinity function. The affinity is defined as
-
-    .. math::
-        \mathbf{f}_1^\top \cdot \mathbf{f}_2
-
-    :param feat1: :math:`(b\times n_1 \times f)` the feature vectors :math:`\mathbf{f}_1`
-    :param feat2: :math:`(b\times n_2 \times f)` the feature vectors :math:`\mathbf{f}_2`
-    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
-    :return: :math:`(b\times n_1\times n_2)` element-wise inner product affinity matrix
-    """
-    if backend is None:
-        backend = pygmtools.BACKEND
-
-    _check_data_type(feat1, backend)
-    _check_data_type(feat2, backend)
-    args = (feat1, feat2)
-    try:
-        mod = importlib.import_module(f'pygmtools.{backend}_backend')
-        fn = mod.inner_prod_aff_fn
-    except (ModuleNotFoundError, AttributeError):
-        raise NotImplementedError(
-            NOT_IMPLEMENTED_MSG.format(backend)
-        )
-    return fn(*args)
-
-
-def gaussian_aff_fn(feat1, feat2, sigma=1., backend=None):
-    r"""
-    Gaussian kernel affinity function. The affinity is defined as
-
-    .. math::
-        \exp(-\frac{(\mathbf{f}_1 - \mathbf{f}_2)^2}{\sigma})
-
-    :param feat1: :math:`(b\times n_1 \times f)` the feature vectors :math:`\mathbf{f}_1`
-    :param feat2: :math:`(b\times n_2 \times f)` the feature vectors :math:`\mathbf{f}_2`
-    :param sigma: (default: 1) the parameter :math:`\sigma` in Gaussian kernel
-    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
-    :return:  :math:`(b\times n_1\times n_2)` element-wise Gaussian affinity matrix
-    """
-    if backend is None:
-        backend = pygmtools.BACKEND
-
-    _check_data_type(feat1, backend)
-    _check_data_type(feat2, backend)
-    args = (feat1, feat2, sigma)
-    try:
-        mod = importlib.import_module(f'pygmtools.{backend}_backend')
-        fn = mod.gaussian_aff_fn
-    except (ModuleNotFoundError, AttributeError):
-        raise NotImplementedError(
-            NOT_IMPLEMENTED_MSG.format(backend)
-        )
-    return fn(*args)
-
-
-def build_batch(input, return_ori_dim=False, backend=None):
-    r"""
-    Build a batched tensor from a list of tensors. If the list of tensors are with different sizes of dimensions, it
-    will be padded to the largest dimension.
-
-    The batched tensor and the number of original dimensions will be returned.
-
-    :param input: list of input tensors
-    :param return_ori_dim: (default: False) return the original dimension
-    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
-    :return: batched tensor, (if ``return_ori_dim=True``) a list of the original dimensions
-
-    .. dropdown:: Numpy Example
-
-        ::
-
-            >>> import numpy as np
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'numpy'
-
-            # batched adjacency matrices
-            >>> A1 = np.random.rand(4, 4)
-            >>> A2 = np.random.rand(5, 5)
-            >>> A3 = np.random.rand(3, 3)
-            >>> batched_A, ori_shape = pygm.utils.build_batch([A1, A2, A3], return_ori_dim=True)
-            >>> batched_A.shape
-            (3, 5, 5)
-            >>> ori_shape
-            ([4, 5, 3], [4, 5, 3])
-
-            # batched node features (feature dimension=10)
-            >>> F1 = np.random.rand(4, 10)
-            >>> F2 = np.random.rand(5, 10)
-            >>> F3 = np.random.rand(3, 10)
-            >>> batched_F = pygm.utils.build_batch([F1, F2, F3])
-            >>> batched_F.shape
-            (3, 5, 10)
-
-    .. dropdown:: Pytorch Example
-
-        ::
-
-            >>> import torch
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'pytorch'
-
-            # batched adjacency matrices
-            >>> A1 = torch.rand(4, 4)
-            >>> A2 = torch.rand(5, 5)
-            >>> A3 = torch.rand(3, 3)
-            >>> batched_A, ori_shape = pygm.utils.build_batch([A1, A2, A3], return_ori_dim=True)
-            >>> batched_A.shape
-            torch.Size([3, 5, 5])
-            >>> ori_shape
-            (tensor([4, 5, 3]), tensor([4, 5, 3]))
-
-            # batched node features (feature dimension=10)
-            >>> F1 = torch.rand(4, 10)
-            >>> F2 = torch.rand(5, 10)
-            >>> F3 = torch.rand(3, 10)
-            >>> batched_F = pygm.utils.build_batch([F1, F2, F3])
-            >>> batched_F.shape
-            torch.Size([3, 5, 10])
-
-    .. dropdown:: Paddle Example
-
-        ::
-
-            >>> import paddle
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'paddle'
-
-            # batched adjacency matrices
-            >>> A1 = paddle.rand((4, 4))
-            >>> A2 = paddle.rand((5, 5))
-            >>> A3 = paddle.rand((3, 3))
-            >>> batched_A, ori_shape = pygm.utils.build_batch([A1, A2, A3], return_ori_dim=True)
-            >>> batched_A.shape
-            [3, 5, 5]
-            >>> ori_shape
-            (Tensor(shape=[3], dtype=int64, place=Place(cpu), stop_gradient=True, [4, 5, 3]),
-             Tensor(shape=[3], dtype=int64, place=Place(cpu), stop_gradient=True, [4, 5, 3]))
-
-            # batched node features (feature dimension=10)
-            >>> F1 = paddle.rand((4, 10))
-            >>> F2 = paddle.rand((5, 10))
-            >>> F3 = paddle.rand((3, 10))
-            >>> batched_F = pygm.utils.build_batch([F1, F2, F3])
-            >>> batched_F.shape
-            [3, 5, 10]
-
-    .. dropdown:: mindspore Example
-
-        ::
-
-            >>> import mindspore
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'mindspore'
-
-            # batched adjacency matrices
-            >>> A1 = mindspore.numpy.rand((4, 4))
-            >>> A2 = mindspore.numpy.rand((5, 5))
-            >>> A3 = mindspore.numpy.rand((3, 3))
-            >>> batched_A, ori_shape = pygm.utils.build_batch([A1, A2, A3], return_ori_dim=True)
-            >>> batched_A.shape
-            (3, 5, 5)
-            >>> ori_shape
-            (Tensor(shape=[3], dtype=Int64, value= [4, 5, 3]),
-             Tensor(shape=[3], dtype=Int64, value= [4, 5, 3]))
-
-            # batched node features (feature dimension=10)
-            >>> F1 = mindspore.numpy.rand((4, 10))
-            >>> F2 = mindspore.numpy.rand((5, 10))
-            >>> F3 = mindspore.numpy.rand((3, 10))
-            >>> batched_F = pygm.utils.build_batch([F1, F2, F3])
-            >>> batched_F.shape
-            (3, 5, 10)
-
-    """
-    if backend is None:
-        backend = pygmtools.BACKEND
-    for item in input:
-        _check_data_type(item, backend)
-    args = (input, return_ori_dim)
-    try:
-        mod = importlib.import_module(f'pygmtools.{backend}_backend')
-        fn = mod.build_batch
-    except ImportError and AttributeError:
-        raise NotImplementedError(
-            NOT_IMPLEMENTED_MSG.format(backend)
-        )
-    return fn(*args)
-
-
-def dense_to_sparse(dense_adj, backend=None):
-    r"""
-    Convert a dense connectivity/adjacency matrix to a sparse connectivity/adjacency matrix and an edge weight tensor.
-
-    :param dense_adj: :math:`(b\times n\times n)` the dense adjacency matrix. This function also supports non-batched
-                      input where the batch dimension ``b`` is ignored
-    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
-    :return: if batched input:
-             :math:`(b\times ne\times 2)` sparse connectivity matrix, :math:`(b\times ne\times 1)` edge weight tensor,
-             :math:`(b)` number of edges
-
-             if non-batched input:
-             :math:`(ne\times 2)` sparse connectivity matrix, :math:`(ne\times 1)` edge weight tensor,
-
-    .. dropdown:: Numpy Example
-
-        ::
-
-            >>> import numpy as np
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'numpy'
-            >>> np.random.seed(0)
-
-            >>> batch_size = 10
-            >>> A = np.random.rand(batch_size, 4, 4)
-            >>> A[:, np.arange(4), np.arange(4)] = 0 # remove the diagonal elements
-            >>> A.shape
-            (10, 4, 4)
-
-            >>> conn, edge, ne = pygm.utils.dense_to_sparse(A)
-            >>> conn.shape # connectivity: (batch x num_edge x 2)
-            (10, 12, 2)
-
-            >>> edge.shape # edge feature (batch x num_edge x feature_dim)
-            (10, 12, 1)
-
-            >>> ne
-            [12, 12, 12, 12, 12, 12, 12, 12, 12, 12]
-
-    .. dropdown:: Pytorch Example
-
-        ::
-
-            >>> import torch
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'pytorch'
-            >>> _ = torch.manual_seed(0)
-
-            >>> batch_size = 10
-            >>> A = torch.rand(batch_size, 4, 4)
-            >>> torch.diagonal(A, dim1=1, dim2=2)[:] = 0 # remove the diagonal elements
-            >>> A.shape
-            torch.Size([10, 4, 4])
-
-            >>> conn, edge, ne = pygm.utils.dense_to_sparse(A)
-            >>> conn.shape # connectivity: (batch x num_edge x 2)
-            torch.Size([10, 12, 2])
-
-            >>> edge.shape # edge feature (batch x num_edge x feature_dim)
-            torch.Size([10, 12, 1])
-
-            >>> ne
-            tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])
-
-    .. dropdown:: Paddle Example
-
-        ::
-
-            >>> import paddle
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'paddle'
-            >>> paddle.seed(0)
-
-            >>> batch_size = 10
-            >>> A = paddle.rand((batch_size, 4, 4))
-            >>> paddle.diagonal(A, axis1=1, axis2=2)[:] = 0 # remove the diagonal elements
-            >>> A.shape
-            [10, 4, 4]
-
-            >>> conn, edge, ne = pygm.utils.dense_to_sparse(A)
-            >>> conn.shape # connectivity: (batch x num_edge x 2)
-            torch.Size([10, 16, 2])
-
-            >>> edge.shape # edge feature (batch x num_edge x feature_dim)
-            torch.Size([10, 16, 1])
-
-            >>> ne
-            Tensor(shape=[10], dtype=int64, place=Place(cpu), stop_gradient=True,
-                    [16, 16, 16, 16, 16, 16, 16, 16, 16, 16])
-
-    .. dropdown:: mindspore Example
-
-        ::
-
-            >>> import mindspore
-            >>> import pygmtools as pygm
-            >>> pygm.BACKEND = 'mindspore'
-            >>> _ = mindspore.set_seed(0)
-
-            >>> batch_size = 10
-            >>> A = mindspore.numpy.rand((batch_size, 4, 4))
-            >>> mindspore.numpy.diagonal(A, axis1=1, axis2=2)[:] = 0 # remove the diagonal elements
-            >>> A.shape
-            (10, 4, 4)
-
-            >>> conn, edge, ne = pygm.utils.dense_to_sparse(A)
-            >>> conn.shape # connectivity: (batch x num_edge x 2)
-            (10, 16, 2)
-
-            >>> edge.shape # edge feature (batch x num_edge x feature_dim)
-            (10, 16, 1)
-
-            >>> ne
-            [16 16 16 16 16 16 16 16 16 16]
-
-    """
-    if backend is None:
-        backend = pygmtools.BACKEND
-    _check_data_type(dense_adj, backend)
-    if _check_shape(dense_adj, 2, backend):
-        dense_adj = _unsqueeze(dense_adj, 0, backend)
-        non_batched_input = True
-    elif _check_shape(dense_adj, 3, backend):
-        non_batched_input = False
-    else:
-        raise ValueError(f'the input argument dense_adj is expected to be 2-dimensional or 3-dimensional, got '
-                         f'dense_adj:{len(_get_shape(dense_adj))}!')
-
-    args = (dense_adj,)
-    try:
-        mod = importlib.import_module(f'pygmtools.{backend}_backend')
-        fn = mod.dense_to_sparse
-    except (ModuleNotFoundError, AttributeError):
-        raise NotImplementedError(
-            NOT_IMPLEMENTED_MSG.format(backend)
-        )
-
-    result = fn(*args)
-    if non_batched_input:
-        return _squeeze(result[0], 0, backend), _squeeze(result[1], 0, backend)
-    else:
-        return result
-
-
-def compute_affinity_score(X, K, backend=None):
-    r"""
-    Compute the affinity score of graph matching. It is the objective score of the corresponding Quadratic Assignment
-    Problem.
-
-    .. math::
-
-        \texttt{vec}(\mathbf{X})^\top \mathbf{K} \texttt{vec}(\mathbf{X})
-
-    here :math:`\texttt{vec}` means column-wise vectorization.
-
-    :param X: :math:`(b\times n_1 \times n_2)` the permutation matrix that represents the matching result
-    :param K: :math:`(b\times n_1n_2 \times n_1n_2)` the affinity matrix
-    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
-    :return: :math:`(b)` the objective score
-
-    .. note::
-
-       This function also supports non-batched input if the batch dimension of ``X, K`` is ignored.
-
-    .. dropdown:: Pytorch Example
-
-        ::
-
-            >>> import pygmtools as pygm
-            >>> import torch
-            >>> pygm.BACKEND = 'pytorch'
-
-            # Generate a graph matching problem
-            >>> X_gt = torch.zeros(4, 4)
-            >>> X_gt[torch.arange(0, 4, dtype=torch.int64), torch.randperm(4)] =1
-            >>> A1 = torch.rand(4, 4)
-            >>> A2 = torch.mm(torch.mm(X_gt.transpose(0,1), A1), X_gt)
-            >>> conn1, edge1 = pygm.utils.dense_to_sparse(A1)
-            >>> conn2, edge2 = pygm.utils.dense_to_sparse(A2)
-            >>> import functools
-            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.)
-            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, None, None, None, None, edge_aff_fn=gaussian_aff)
-
-            # Compute the objective score of ground truth matching
-            >>> pygm.utils.compute_affinity_score(X_gt, K)
-            tensor(16.)
-
-    """
-    if backend is None:
-        backend = pygmtools.BACKEND
-    _check_data_type(X, backend)
-    _check_data_type(K, backend)
-    if _check_shape(X, 2, backend) and _check_shape(K, 2, backend):
-        X = _unsqueeze(X, 0, backend)
-        K = _unsqueeze(K, 0, backend)
-        non_batched_input = True
-    elif _check_shape(X, 3, backend) and _check_shape(X, 3, backend):
-        non_batched_input = False
-    else:
-        raise ValueError(f'the input argument K, X are expected to have the same number of dimensions (=2 or 3), got'
-                         f'X:{len(_get_shape(X))} and K:{len(_get_shape(K))}!')
-    args = (X, K)
-    try:
-        mod = importlib.import_module(f'pygmtools.{backend}_backend')
-        fn = mod.compute_affinity_score
-    except (ModuleNotFoundError, AttributeError):
-        raise NotImplementedError(
-            NOT_IMPLEMENTED_MSG.format(backend)
-        )
-
-    result = fn(*args)
-    if non_batched_input:
-        return _squeeze(result, 0, backend)
-    else:
-        return result
-
-
-def to_numpy(input, backend=None):
-    r"""
-    Convert a tensor to a numpy ndarray.
-    This is the helper function to convert tensors across different backends via numpy.
-
-    :param input: input tensor/:mod:`~pygmtools.utils.MultiMatchingResult`
-    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
-    :return: numpy ndarray
-    """
-    if backend is None:
-        backend = pygmtools.BACKEND
-    args = (input,)
-    # pygmtools built-in types
-    if type(input) is MultiMatchingResult:
-        fn = MultiMatchingResult.to_numpy
-    # tf/torch/.. tensor types
-    else:
-        try:
-            mod = importlib.import_module(f'pygmtools.{backend}_backend')
-            fn = mod.to_numpy
-        except (ModuleNotFoundError, AttributeError):
-            raise NotImplementedError(
-                NOT_IMPLEMENTED_MSG.format(backend)
-            )
-    return fn(*args)
-
-
-def from_numpy(input, device=None, backend=None):
-    r"""
-    Convert a numpy ndarray to a tensor.
-    This is the helper function to convert tensors across different backends via numpy.
-
-    :param input: input ndarray/:mod:`~pygmtools.utils.MultiMatchingResult`
-    :param device: (default: None) the target device
-    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
-    :return: tensor for the backend
-    """
-    if backend is None:
-        backend = pygmtools.BACKEND
-    args = (input, device)
-    # pygmtools built-in types
-    if type(input) is MultiMatchingResult:
-        fn = functools.partial(MultiMatchingResult.from_numpy, new_backend=backend)
-    # tf/torch/.. tensor types
-    else:
-        try:
-            mod = importlib.import_module(f'pygmtools.{backend}_backend')
-            fn = mod.from_numpy
-        except (ModuleNotFoundError, AttributeError):
-            raise NotImplementedError(
-                NOT_IMPLEMENTED_MSG.format(backend)
-            )
-    return fn(*args)
-
-
-def generate_isomorphic_graphs(node_num, graph_num=2, node_feat_dim=0, backend=None):
-    r"""
-    Generate a set of isomorphic graphs, for testing purposes and examples.
-
-    :param node_num: number of nodes in each graph
-    :param graph_num: (default: 2) number of graphs
-    :param node_feat_dim: (default: 0) number of node feature dimensions
-    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
-    :return: if ``graph_num==2``, this function returns :math:`(m\times n \times n)` the adjacency matrix, and
-             :math:`(n \times n)` the permutation matrix;
-
-             else, this function returns :math:`(m\times n \times n)` the adjacency matrix, and
-             :math:`(m\times m\times n \times n)` the multi-matching permutation matrix
-    """
-    if backend is None:
-        backend = pygmtools.BACKEND
-    args = (node_num, graph_num, node_feat_dim)
-    assert node_num > 0 and graph_num >= 2, "input data not understood."
-    try:
-        mod = importlib.import_module(f'pygmtools.{backend}_backend')
-        fn = mod.generate_isomorphic_graphs
-    except (ModuleNotFoundError, AttributeError):
-        raise NotImplementedError(
-            NOT_IMPLEMENTED_MSG.format(backend)
-        )
-    if node_feat_dim > 0:
-        As, X_gt, Fs = fn(*args)
-        if graph_num == 2:
-            return As, X_gt[0, 1], Fs
-        else:
-            return As, X_gt, Fs
-    else:
-        As, X_gt = fn(*args)
-        if graph_num == 2:
-            return As, X_gt[0, 1]
-        else:
-            return As, X_gt
-
-
-class MultiMatchingResult:
-    r"""
-    A memory-efficient class for multi-graph matching results. For non-cycle consistent results, the dense storage
-    for :math:`m` graphs with :math:`n` nodes requires a size of :math:`(m\times m \times n \times n)`, and this
-    implementation requires :math:`((m-1)\times m \times n \times n / 2)`. For cycle consistent result, this
-    implementation requires only :math:`(m\times n\times n)`.
-
-    .. dropdown:: Numpy Example
-
-        ::
-
-            >>> import numpy as np
-            >>> import pygmtools as pygm
-            >>> np.random.seed(0)
-
-            >>> X = pygm.utils.MultiMatchingResult(backend='numpy')
-            >>> X[0, 1] = np.zeros((4, 4))
-            >>> X[0, 1][np.arange(0, 4, dtype=np.int64), np.random.permutation(4)] = 1
-            >>> X
-            MultiMatchingResult:
-            {'0,1': array([[0., 0., 1., 0.],
-                [0., 0., 0., 1.],
-                [0., 1., 0., 0.],
-                [1., 0., 0., 0.]])}
-            >>> X[1, 0]
-            array([[0., 0., 0., 1.],
-                [0., 0., 1., 0.],
-                [1., 0., 0., 0.],
-                [0., 1., 0., 0.]])
-    """
-    def __init__(self, cycle_consistent=False, backend=None):
-        self.match_dict = {}
-        self._cycle_consistent = cycle_consistent
-        if backend is None:
-            self.backend = pygmtools.BACKEND
-        else:
-            self.backend = backend
-
-    def __getitem__(self, item):
-        assert len(item) == 2, "key should be the indices of two graphs, e.g. (0, 1)"
-        idx1, idx2 = item
-        if self._cycle_consistent:
-            return _mm(self.match_dict[idx1], _transpose(self.match_dict[idx2], 0, 1, self.backend), self.backend)
-        else:
-            if idx1 < idx2:
-                return self.match_dict[f'{idx1},{idx2}']
-            else:
-                return _transpose(self.match_dict[f'{idx2},{idx1}'], 0, 1, self.backend)
-
-    def __setitem__(self, key, value):
-        if self._cycle_consistent:
-            assert type(key) is int, "key should be the index of one graph, and value should be the matching to universe"
-            self.match_dict[key] = value
-        else:
-            assert len(key) == 2, "key should be the indices of two graphs, e.g. (0, 1)"
-            idx1, idx2 = key
-            if idx1 < idx2:
-                self.match_dict[f'{idx1},{idx2}'] = value
-            else:
-                self.match_dict[f'{idx2},{idx1}'] = _transpose(value, 0, 1, self.backend)
-
-    def __str__(self):
-        return 'MultiMatchingResult:\n' + self.match_dict.__str__()
-
-    def __repr__(self):
-        return 'MultiMatchingResult:\n' + self.match_dict.__repr__()
-
-    @staticmethod
-    def from_numpy(data, device=None, new_backend=None):
-        r"""
-        Convert a numpy-backend MultiMatchingResult data to another backend.
-
-        :param data: the numpy-backend data
-        :param device: (default: None) the target device
-        :param new_backend: (default: ``pygmtools.BACKEND`` variable) the target backend
-        :return: a new MultiMatchingResult instance for ``new_backend`` on ``device``
-        """
-        new_data = copy.deepcopy(data)
-        new_data.from_numpy_(device, new_backend)
-        return new_data
-
-    @staticmethod
-    def to_numpy(data):
-        r"""
-        Convert an any-type MultiMatchingResult to numpy backend.
-
-        :param data: the any-type data
-        :return: a new MultiMatchingResult instance for numpy
-        """
-        new_data = copy.deepcopy(data)
-        new_data.to_numpy_()
-        return new_data
-
-    def from_numpy_(self, device=None, new_backend=None):
-        """
-        In-place operation for :func:`~pygmtools.utils.MultiMatchingResult.from_numpy`.
-        """
-        if self.backend != 'numpy':
-            raise ValueError('Attempting to convert from non-numpy data.')
-        if new_backend is None:
-            new_backend = pygmtools.BACKEND
-        self.backend = new_backend
-        for k, v in self.match_dict.items():
-            self.match_dict[k] = from_numpy(v, device, self.backend)
-
-    def to_numpy_(self):
-        """
-        In-place operation for :func:`~pygmtools.utils.MultiMatchingResult.to_numpy`.
-        """
-        for k, v in self.match_dict.items():
-            self.match_dict[k] = to_numpy(v, self.backend)
-        self.backend = 'numpy'
-
-
-def get_network(nn_solver_func, **params):
-    r"""
-    Get the network object of a neural network solver.
-
-    :param nn_solver_func: the neural network solver function, for example ``pygm.pca_gm``
-    :param params: keyword parameters to define the neural network
-    :return: the network object
-
-    .. dropdown:: Pytorch Example
-
-        ::
-
-            >>> import pygmtools as pygm
-            >>> import torch
-            >>> pygm.BACKEND = 'pytorch'
-            >>> pygm.utils.get_network(pygm.pca_gm, pretrain='willow')
-            PCA_GM_Net(
-              (gnn_layer_0): Siamese_Gconv(
-                (gconv): Gconv(
-                  (a_fc): Linear(in_features=1024, out_features=2048, bias=True)
-                  (u_fc): Linear(in_features=1024, out_features=2048, bias=True)
-                )
-              )
-              (cross_graph_0): Linear(in_features=4096, out_features=2048, bias=True)
-              (affinity_0): WeightedInnerProdAffinity()
-              (affinity_1): WeightedInnerProdAffinity()
-              (gnn_layer_1): Siamese_Gconv(
-                (gconv): Gconv(
-                  (a_fc): Linear(in_features=2048, out_features=2048, bias=True)
-                  (u_fc): Linear(in_features=2048, out_features=2048, bias=True)
-                )
-              )
-            )
-
-            # the neural network can be integrated into a deep learning pipeline
-            >>> net = pygm.utils.get_network(pygm.pca_gm, in_channel=1024, hidden_channel=2048, out_channel=512, num_layers=3, pretrain=False)
-            >>> optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)
-
-    """
-    if 'return_network' in params:
-        params.pop('return_network')
-    # count parameters w/o default value
-    sig = inspect.signature(nn_solver_func)
-    required_params = 0
-    for p in sig.parameters.items():
-        if p[1].default is inspect._empty:
-            required_params += 1
-    _, net = nn_solver_func(*[None] * required_params, # fill the required parameters by None
-                            return_network=True, **params)
-    return net
-
-
-def permutation_loss(pred_dsmat, gt_perm, n1=None, n2=None, backend=None):
-    r"""
-    Binary cross entropy loss between two permutations, also known as "permutation loss".
-    Proposed by `"Wang et al. Learning Combinatorial Embedding Networks for Deep Graph Matching. ICCV 2019."
-    <http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Learning_Combinatorial_Embedding_Networks_for_Deep_Graph_Matching_ICCV_2019_paper.pdf>`_
-
-    .. math::
-        L_{perm} =- \sum_{i \in \mathcal{V}_1, j \in \mathcal{V}_2}
-        \left(\mathbf{X}^{gt}_{i,j} \log \mathbf{S}_{i,j} + (1-\mathbf{X}^{gt}_{i,j}) \log (1-\mathbf{S}_{i,j}) \right)
-
-    where :math:`\mathcal{V}_1, \mathcal{V}_2` are vertex sets for two graphs.
-
-    :param pred_dsmat: :math:`(b\times n_1 \times n_2)` predicted doubly-stochastic matrix :math:`(\mathbf{S})`
-    :param gt_perm: :math:`(b\times n_1 \times n_2)` ground truth permutation matrix :math:`(\mathbf{X}^{gt})`
-    :param n1: (optional) :math:`(b)` number of exact pairs in the first graph.
-    :param n2: (optional) :math:`(b)` number of exact pairs in the second graph.
-    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
-    :return: :math:`(1)` averaged permutation loss
-
-    .. note::
-        We support batched instances with different number of nodes, therefore ``n1`` and ``n2`` are
-        required if you want to specify the exact number of nodes of each instance in the batch.
-
-    .. note::
-        For batched input, this loss function computes the averaged loss among all instances in the batch. This function
-        also supports non-batched input if the batch dimension (:math:`b`) is ignored.
-    """
-    if backend is None:
-        backend = pygmtools.BACKEND
-    _check_data_type(pred_dsmat, backend)
-    _check_data_type(gt_perm, backend)
-    dsmat_shape = _get_shape(pred_dsmat, backend)
-    perm_shape = _get_shape(gt_perm, backend)
-    if len(dsmat_shape) == len(perm_shape) == 2:
-        pred_dsmat = _unsqueeze(pred_dsmat, 0, backend)
-        gt_perm = _unsqueeze(gt_perm, 0, backend)
-    elif len(dsmat_shape) == len(perm_shape) == 3:
-        pass
-    else:
-        raise ValueError(f'the input arguments pred_dsmat and gt_perm are expected to be 2-dimensional or 3-dimensional,'
-                         f' got pred_dsmat:{len(dsmat_shape)}, gt_perm:{len(perm_shape)}!')
-
-    for d1, d2 in zip(dsmat_shape, perm_shape):
-        if d1 != d2:
-            raise ValueError(f'dimension mismatch for pred_dsmat and gt_perm, got pred_dsmat:{dsmat_shape}, gt_perm:{gt_perm}!')
-
-    args = (pred_dsmat, gt_perm, n1, n2)
-    try:
-        mod = importlib.import_module(f'pygmtools.{backend}_backend')
-        fn = mod.permutation_loss
-    except (ModuleNotFoundError, AttributeError):
-        raise NotImplementedError(
-            NOT_IMPLEMENTED_MSG.format(backend)
-        )
-
-    return fn(*args)
-
-
-###################################################
-#   Private Functions that Unseeable from Users   #
-###################################################
-
-
-def _aff_mat_from_node_edge_aff(node_aff, edge_aff, connectivity1, connectivity2,
-                                n1, n2, ne1, ne2,
-                                backend=None):
-    r"""
-    Build affinity matrix K from node and edge affinity matrices.
-
-    :param node_aff: :math:`(b\times n_1 \times n_2)` the node affinity matrix
-    :param edge_aff: :math:`(b\times ne_1 \times ne_2)` the edge affinity matrix
-    :param connectivity1: :math:`(b\times ne_1 \times 2)` sparse connectivity information of graph 1
-    :param connectivity2: :math:`(b\times ne_2 \times 2)` sparse connectivity information of graph 2
-    :param n1: :math:`(b)` number of nodes in graph1. If not given, it will be inferred based on the shape of
-               ``node_feat1`` or the values in ``connectivity1``
-    :param ne1: :math:`(b)` number of edges in graph1. If not given, it will be inferred based on the shape of
-               ``edge_feat1``
-    :param n2: :math:`(b)` number of nodes in graph2. If not given, it will be inferred based on the shape of
-               ``node_feat2`` or the values in ``connectivity2``
-    :param ne2: :math:`(b)` number of edges in graph2. If not given, it will be inferred based on the shape of
-               ``edge_feat2``
-    :return: :math:`(b\times n_1n_2 \times n_1n_2)` the affinity matrix
-    """
-    if backend is None:
-        backend = pygmtools.BACKEND
-    args = (node_aff, edge_aff, connectivity1, connectivity2, n1, n2, ne1, ne2)
-    try:
-        mod = importlib.import_module(f'pygmtools.{backend}_backend')
-        fn = mod._aff_mat_from_node_edge_aff
-    except (ModuleNotFoundError, AttributeError):
-        raise NotImplementedError(
-            NOT_IMPLEMENTED_MSG.format(backend)
-        )
-    return fn(*args)
-
-
-def _check_data_type(input, *args):
-    r"""
-    Check whether the input data meets the backend. If not met, it will raise an ValueError
-    Three overloads of this function:
-    _check_data_type(input, backend)
-    _check_data_type(input, var_name, backend)
-    _check_data_type(input, var_name, raise_err, backend)
-
-    :param input: input data (must be Tensor/ndarray)
-    :param var_name: name of the variable
-    :param raise_err: raise an error if input data not true
-    :return: True or False
-    """
-    if len(args) == 3:
-        var_name, raise_err, backend = args
-    elif len(args) == 2:
-        var_name, backend = args
-        raise_err = True
-    elif len(args) == 1:
-        backend = args[0]
-        var_name = None
-        raise_err = True
-    elif len(args) == 0:
-        backend = None
-        var_name = None
-        raise_err = True
-    else:
-        raise RuntimeError(f'Unknown arguments: {args}')
-
-    if backend is None:
-        backend = pygmtools.BACKEND
-    args = (input, var_name, raise_err)
-    try:
-        mod = importlib.import_module(f'pygmtools.{backend}_backend')
-        fn = mod._check_data_type
-    except (ModuleNotFoundError, AttributeError):
-        raise NotImplementedError(
-            NOT_IMPLEMENTED_MSG.format(backend)
-        )
-    return fn(*args)
-
-
-def _check_shape(input, num_dim, backend=None):
-    r"""
-    Check the shape of the input tensor
-
-    :param input: the input tensor
-    :param num_dim: number of dimensions
-    :return: True or False
-    """
-    if backend is None:
-        backend = pygmtools.BACKEND
-    args = (input, num_dim)
-    try:
-        mod = importlib.import_module(f'pygmtools.{backend}_backend')
-        fn = mod._check_shape
-    except (ModuleNotFoundError, AttributeError):
-        raise NotImplementedError(
-            NOT_IMPLEMENTED_MSG.format(backend)
-        )
-    return fn(*args)
-
-
-def _get_shape(input, backend=None):
-    r"""
-    Get the shape of the input tensor
-
-    :param input: the input tensor
-    :return: a list of ints indicating the shape
-    """
-    if backend is None:
-        backend = pygmtools.BACKEND
-    args = (input,)
-    try:
-        mod = importlib.import_module(f'pygmtools.{backend}_backend')
-        fn = mod._get_shape
-    except (ModuleNotFoundError, AttributeError):
-        raise NotImplementedError(
-            NOT_IMPLEMENTED_MSG.format(backend)
-        )
-    return fn(*args)
-
-
-def _squeeze(input, dim, backend=None):
-    r"""
-    Squeeze the input tensor at the given dimension. This function is expected to behave the same as torch.squeeze
-
-    :param input: input tensor
-    :param dim: squeezed dimension
-    :return: squeezed tensor
-    """
-    if backend is None:
-        backend = pygmtools.BACKEND
-    args = (input, dim)
-    try:
-        mod = importlib.import_module(f'pygmtools.{backend}_backend')
-        fn = mod._squeeze
-    except (ModuleNotFoundError, AttributeError):
-        raise NotImplementedError(
-            NOT_IMPLEMENTED_MSG.format(backend)
-        )
-    return fn(*args)
-
-
-def _unsqueeze(input, dim, backend=None):
-    r"""
-    Unsqueeze the input tensor at the given dimension. This function is expected to behave the same as torch.unsqueeze
-
-    :param input: input tensor
-    :param dim: unsqueezed dimension
-    :return: unsqueezed tensor
-    """
-    if backend is None:
-        backend = pygmtools.BACKEND
-    args = (input, dim)
-    try:
-        mod = importlib.import_module(f'pygmtools.{backend}_backend')
-        fn = mod._unsqueeze
-    except (ModuleNotFoundError, AttributeError):
-        raise NotImplementedError(
-            NOT_IMPLEMENTED_MSG.format(backend)
-        )
-    return fn(*args)
-
-
-def _transpose(input, dim1, dim2, backend=None):
-    r"""
-    Swap the dim1 and dim2 dimensions of the input tensor.
-
-    :param input: input tensor
-    :param dim1: swapped dimension 1
-    :param dim2: swapped dimension 2
-    :return: transposed tensor
-    """
-    if backend is None:
-        backend = pygmtools.BACKEND
-    args = (input, dim1, dim2)
-    try:
-        mod = importlib.import_module(f'pygmtools.{backend}_backend')
-        fn = mod._transpose
-    except (ModuleNotFoundError, AttributeError):
-        raise NotImplementedError(
-            NOT_IMPLEMENTED_MSG.format(backend)
-        )
-    return fn(*args)
-
-
-def _mm(input1, input2, backend=None):
-    r"""
-    Matrix multiplication.
-
-    :param input1: input tensor 1
-    :param input2: input tensor 2
-    :return: multiplication result
-    """
-    if backend is None:
-        backend = pygmtools.BACKEND
-    args = (input1, input2)
-    try:
-        mod = importlib.import_module(f'pygmtools.{backend}_backend')
-        fn = mod._mm
-    except (ModuleNotFoundError, AttributeError):
-        raise NotImplementedError(
-            NOT_IMPLEMENTED_MSG.format(backend)
-        )
-    return fn(*args)
-
-def download(filename, url, md5=None, retries=10, to_cache=True):
-    r"""
-    Check if content exits. If not, download the content to ``<user cache path>/pygmtools/<filename>``. ``<user cache path>``
-    depends on your system. For example, on Debian, it should be ``$HOME/.cache``.
-    :param filename: the destination file name
-    :param url: the url
-    :param md5: (optional) the md5sum to verify the content. It should match the result of ``md5sum file`` on Linux.
-    :param retries: (default: 5) max number of retries
-    :return: the full path to the file: ``<user cache path>/pygmtools/<filename>``
-    """
-    if retries <= 0:
-        raise RuntimeError('Max Retries exceeded!')
-
-    if to_cache:
-        dirs = user_cache_dir("pygmtools")
-        if not os.path.exists(dirs):
-            os.makedirs(dirs)
-        filename = os.path.join(dirs, filename)
-    if not os.path.exists(filename):
-        print(f'\nDownloading to {filename}...')
-        if retries % 2 == 1:
-            try:
-                down_res = requests.get(url, stream=True)
-                file_size = int(down_res.headers.get('Content-Length', 0))
-                with tqdm.wrapattr(down_res.raw, "read", total=file_size) as content:
-                    with open(filename, 'wb') as file:
-                        shutil.copyfileobj(content, file)
-            except requests.exceptions.ConnectionError as err:
-                print('Warning: Network error. Retrying...\n', err)
-                return download(filename, url, md5, retries - 1)
-        else:
-            try:
-                wget.download(url,out=filename)
-            except:
-                return download(filename, url, md5, retries - 1)
-    if md5 is not None:
-        md5_returned = _get_md5(filename)
-        if md5 != md5_returned:
-            print('Warning: MD5 check failed for the downloaded content. Retrying...')
-            os.remove(filename)
-            time.sleep(1)
-            return download(filename, url, md5, retries - 1)
-    return filename
-
-def _get_md5(filename):
-    hash_md5 = hashlib.md5()
-    chunk = 8192
-    with open(filename, 'rb') as file_to_check:
-        while True:
-            buffer = file_to_check.read(chunk)
-            if not buffer:
-                break
-            hash_md5.update(buffer)
-        md5_returned = hash_md5.hexdigest()
-        return md5_returned
+"""
+Utility functions: problem formulating, data processing, and beyond.
+"""
+
+# Copyright (c) 2022 Thinklab@SJTU
+# pygmtools is licensed under Mulan PSL v2.
+# You can use this software according to the terms and conditions of the Mulan PSL v2.
+# You may obtain a copy of Mulan PSL v2 at:
+# http://license.coscl.org.cn/MulanPSL2
+# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND,
+# EITHER EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT,
+# MERCHANTABILITY OR FIT FOR A PARTICULAR PURPOSE.
+# See the Mulan PSL v2 for more details.
+
+import time
+import functools
+import importlib
+import copy
+import requests
+import os
+from appdirs import user_cache_dir
+import hashlib
+import shutil
+from tqdm.auto import tqdm
+import inspect
+import wget
+import numpy as np
+import pygmtools
+
+NOT_IMPLEMENTED_MSG = \
+    'The backend function for {} is not implemented. ' \
+    'If you are a user, please check the spelling, and use other backends as workarounds. ' \
+    'If you are a developer, it will be truly appreciated if you could develop and share your' \
+    ' implementation with the community! See our Github: https://github.com/Thinklab-SJTU/pygmtools'
+
+
+def build_aff_mat(node_feat1, edge_feat1, connectivity1, node_feat2, edge_feat2, connectivity2,
+                  n1=None, ne1=None, n2=None, ne2=None,
+                  node_aff_fn=None, edge_aff_fn=None,
+                  backend=None):
+    r"""
+    Build affinity matrix for graph matching from input node/edge features. The affinity matrix encodes both node-wise
+    and edge-wise affinities and formulates the Quadratic Assignment Problem (QAP), which is the mathematical form of
+    graph matching.
+
+    :param node_feat1: :math:`(b\times n_1 \times f_{node})` the node feature of graph1
+    :param edge_feat1: :math:`(b\times ne_1 \times f_{edge})` the edge feature of graph1
+    :param connectivity1: :math:`(b\times ne_1 \times 2)` sparse connectivity information of graph 1.
+                          ``connectivity1[i, j, 0]`` is the starting node index of edge ``j`` at batch ``i``, and
+                          ``connectivity1[i, j, 1]`` is the ending node index of edge ``j`` at batch ``i``
+    :param node_feat2: :math:`(b\times n_2 \times f_{node})` the node feature of graph2
+    :param edge_feat2: :math:`(b\times ne_2 \times f_{edge})` the edge feature of graph2
+    :param connectivity2: :math:`(b\times ne_2 \times 2)` sparse connectivity information of graph 2.
+                          ``connectivity2[i, j, 0]`` is the starting node index of edge ``j`` at batch ``i``, and
+                          ``connectivity2[i, j, 1]`` is the ending node index of edge ``j`` at batch ``i``
+    :param n1: :math:`(b)` number of nodes in graph1. If not given, it will be inferred based on the shape of
+               ``node_feat1`` or the values in ``connectivity1``
+    :param ne1: :math:`(b)` number of edges in graph1. If not given, it will be inferred based on the shape of
+               ``edge_feat1``
+    :param n2: :math:`(b)` number of nodes in graph2. If not given, it will be inferred based on the shape of
+               ``node_feat2`` or the values in ``connectivity2``
+    :param ne2: :math:`(b)` number of edges in graph2. If not given, it will be inferred based on the shape of
+               ``edge_feat2``
+    :param node_aff_fn: (default: inner_prod_aff_fn) the node affinity function with the characteristic
+                        ``node_aff_fn(2D Tensor, 2D Tensor) -> 2D Tensor``, which accepts two node feature tensors and
+                        outputs the node-wise affinity tensor. See :func:`~pygmtools.utils.inner_prod_aff_fn` as an
+                        example.
+    :param edge_aff_fn: (default: inner_prod_aff_fn) the edge affinity function with the characteristic
+                        ``edge_aff_fn(2D Tensor, 2D Tensor) -> 2D Tensor``, which accepts two edge feature tensors and
+                        outputs the edge-wise affinity tensor. See :func:`~pygmtools.utils.inner_prod_aff_fn` as an
+                        example.
+    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
+    :return: :math:`(b\times n_1n_2 \times n_1n_2)` the affinity matrix
+
+    .. note::
+        This function also supports non-batched input, by ignoring all batch dimensions in the input tensors.
+
+    .. dropdown:: Numpy Example
+
+        ::
+
+            >>> import numpy as np
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'numpy'
+
+            # Generate a batch of graphs
+            >>> batch_size = 10
+            >>> A1 = np.random.rand(batch_size, 4, 4)
+            >>> A2 = np.random.rand(batch_size, 4, 4)
+            >>> n1 = n2 = np.repeat([4], batch_size)
+
+            # Build affinity matrix by the default inner-product function
+            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
+            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
+            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, ne1, n2, ne2)
+
+            # Build affinity matrix by gaussian kernel
+            >>> import functools
+            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.)
+            >>> K2 = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, ne1, n2, ne2, edge_aff_fn=gaussian_aff)
+
+            # Build affinity matrix based on node features
+            >>> F1 = np.random.rand(batch_size, 4, 10)
+            >>> F2 = np.random.rand(batch_size, 4, 10)
+            >>> K3 = pygm.utils.build_aff_mat(F1, edge1, conn1, F2, edge2, conn2, n1, ne1, n2, ne2, edge_aff_fn=gaussian_aff)
+
+            # The affinity matrices K, K2, K3 can be further processed by GM solvers
+
+    .. dropdown:: Pytorch Example
+
+        ::
+
+            >>> import torch
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'pytorch'
+
+            # Generate a batch of graphs
+            >>> batch_size = 10
+            >>> A1 = torch.rand(batch_size, 4, 4)
+            >>> A2 = torch.rand(batch_size, 4, 4)
+            >>> n1 = n2 = torch.tensor([4] * batch_size)
+
+            # Build affinity matrix by the default inner-product function
+            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
+            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
+            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, ne1, n2, ne2)
+
+            # Build affinity matrix by gaussian kernel
+            >>> import functools
+            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.)
+            >>> K2 = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, ne1, n2, ne2, edge_aff_fn=gaussian_aff)
+
+            # Build affinity matrix based on node features
+            >>> F1 = torch.rand(batch_size, 4, 10)
+            >>> F2 = torch.rand(batch_size, 4, 10)
+            >>> K3 = pygm.utils.build_aff_mat(F1, edge1, conn1, F2, edge2, conn2, n1, ne1, n2, ne2, edge_aff_fn=gaussian_aff)
+
+            # The affinity matrices K, K2, K3 can be further processed by GM solvers
+    
+    .. dropdown:: Paddle Example
+
+        ::
+            >>> import paddle
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'paddle'
+
+            # Generate a batch of graphs
+            >>> batch_size = 10
+            >>> A1 = paddle.rand((batch_size, 4, 4))
+            >>> A2 = paddle.rand((batch_size, 4, 4))
+            >>> n1 = n2 = paddle.t0_tensor([4] * batch_size)
+
+            # Build affinity matrix by the default inner-product function
+            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
+            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
+            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, ne1, n2, ne2)
+
+            # Build affinity matrix by gaussian kernel
+            >>> import functools
+            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.)
+            >>> K2 = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, ne1, n2, ne2, edge_aff_fn=gaussian_aff)
+
+            # Build affinity matrix based on node features
+            >>> F1 = paddle.rand((batch_size, 4, 10))
+            >>> F2 = paddle.rand((batch_size, 4, 10))
+            >>> K3 = pygm.utils.build_aff_mat(F1, edge1, conn1, F2, edge2, conn2, n1, ne1, n2, ne2, edge_aff_fn=gaussian_aff)
+
+            # The affinity matrices K, K2, K3 can be further processed by GM solvers
+
+    .. dropdown:: mindspore Example
+
+        ::
+
+            >>> import mindspore
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'mindspore'
+
+            # Generate a batch of graphs
+            >>> batch_size = 10
+            >>> A1 = mindspore.numpy.rand((batch_size, 4, 4))
+            >>> A2 = mindspore.numpy.rand((batch_size, 4, 4))
+            >>> n1 = n2 = mindspore.Tensor([4] * batch_size)
+
+            # Build affinity matrix by the default inner-product function
+            >>> conn1, edge1, ne1 = pygm.utils.dense_to_sparse(A1)
+            >>> conn2, edge2, ne2 = pygm.utils.dense_to_sparse(A2)
+            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, ne1, n2, ne2)
+
+            # Build affinity matrix by gaussian kernel
+            >>> import functools
+            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.)
+            >>> K2 = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, n1, ne1, n2, ne2, edge_aff_fn=gaussian_aff)
+
+            # Build affinity matrix based on node features
+            >>> F1 = mindspore.numpy.rand((batch_size, 4, 10))
+            >>> F2 = mindspore.numpy.rand((batch_size, 4, 10))
+            >>> K3 = pygm.utils.build_aff_mat(F1, edge1, conn1, F2, edge2, conn2, n1, ne1, n2, ne2, edge_aff_fn=gaussian_aff)
+
+            # The affinity matrices K, K2, K3 can be further processed by GM solvers
+
+    """
+    if backend is None:
+        backend = pygmtools.BACKEND
+    __get_shape = functools.partial(_get_shape, backend=backend)
+
+    # check the correctness of input
+    batch_size = None
+    non_batched_input = False
+    if node_feat1 is not None or node_feat2 is not None:
+        assert all([_ is not None for _ in (node_feat1, node_feat2)]), \
+            'The following arguments must all be given if you want to compute node-wise affinity: ' \
+            'node_feat1, node_feat2'
+        _check_data_type(node_feat1, backend)
+        _check_data_type(node_feat2, backend)
+        if all([_check_shape(_, 2, backend) for _ in (node_feat1, node_feat2)]):
+            non_batched_input = True
+            node_feat1, node_feat2 = [_unsqueeze(_, 0, backend) for _ in (node_feat1, node_feat2)]
+            if type(n1) is int: n1 = from_numpy(np.array([n1]), backend=backend)
+            if type(n2) is int: n2 = from_numpy(np.array([n2]), backend=backend)
+        elif all([_check_shape(_, 3, backend) for _ in (node_feat1, node_feat2)]):
+            pass
+        else:
+            raise ValueError(
+                f'The shape of the following tensors are illegal, expected 3-dimensional, '
+                f'got node_feat1={len(__get_shape(node_feat1))}d; node_feat2={len(__get_shape(node_feat2))}d!'
+            )
+        if batch_size is None:
+            batch_size = __get_shape(node_feat1)[0]
+        assert __get_shape(node_feat1)[0] == __get_shape(node_feat2)[0] == batch_size, 'batch size mismatch'
+    if edge_feat1 is not None or edge_feat2 is not None:
+        assert all([_ is not None for _ in (edge_feat1, edge_feat2, connectivity1, connectivity2)]), \
+            'The following arguments must all be given if you want to compute edge-wise affinity: ' \
+            'edge_feat1, edge_feat2, connectivity1, connectivity2'
+        if all([_check_shape(_, 2, backend) for _ in (edge_feat1, edge_feat2, connectivity1, connectivity2)]):
+            non_batched_input = True
+            edge_feat1, edge_feat2, connectivity1, connectivity2 = \
+                [_unsqueeze(_, 0, backend) for _ in (edge_feat1, edge_feat2, connectivity1, connectivity2)]
+            if type(ne1) is int: ne1 = from_numpy(np.array([ne1]), backend=backend)
+            if type(ne2) is int: ne2 = from_numpy(np.array([ne2]), backend=backend)
+        elif all([_check_shape(_, 3, backend) for _ in (edge_feat1, edge_feat2, connectivity1, connectivity2)]):
+            pass
+        else:
+            raise ValueError(
+                f'The shape of the following tensors are illegal, expected 3-dimensional, '
+                f'got edge_feat1:{len(__get_shape(edge_feat1))}d; edge_feat2:{len(__get_shape(edge_feat2))}d; '
+                f'connectivity1:{len(__get_shape(connectivity1))}d; connectivity2:{len(__get_shape(connectivity2))}d!'
+            )
+        assert __get_shape(connectivity1)[2] == __get_shape(connectivity1)[2] == 2, \
+            'the last dimension of connectivity1, connectivity2 must be 2-dimensional'
+        if batch_size is None:
+            batch_size = __get_shape(edge_feat1)[0]
+        assert __get_shape(edge_feat1)[0] == __get_shape(edge_feat2)[0] == __get_shape(connectivity1)[0] == \
+               __get_shape(connectivity2)[0] == batch_size, 'batch size mismatch'
+
+    # assign the default affinity functions if not given
+    if node_aff_fn is None:
+        node_aff_fn = functools.partial(inner_prod_aff_fn, backend=backend)
+    if edge_aff_fn is None:
+        edge_aff_fn = functools.partial(inner_prod_aff_fn, backend=backend)
+
+    node_aff = node_aff_fn(node_feat1, node_feat2) if node_feat1 is not None else None
+    edge_aff = edge_aff_fn(edge_feat1, edge_feat2) if edge_feat1 is not None else None
+
+    result = _aff_mat_from_node_edge_aff(node_aff, edge_aff, connectivity1, connectivity2, n1, n2, ne1, ne2,
+                                         backend=backend)
+    if non_batched_input:
+        return _squeeze(result, 0, backend)
+    else:
+        return result
+
+
+def inner_prod_aff_fn(feat1, feat2, backend=None):
+    r"""
+    Inner product affinity function. The affinity is defined as
+
+    .. math::
+        \mathbf{f}_1^\top \cdot \mathbf{f}_2
+
+    :param feat1: :math:`(b\times n_1 \times f)` the feature vectors :math:`\mathbf{f}_1`
+    :param feat2: :math:`(b\times n_2 \times f)` the feature vectors :math:`\mathbf{f}_2`
+    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
+    :return: :math:`(b\times n_1\times n_2)` element-wise inner product affinity matrix
+    """
+    if backend is None:
+        backend = pygmtools.BACKEND
+
+    _check_data_type(feat1, backend)
+    _check_data_type(feat2, backend)
+    args = (feat1, feat2)
+    try:
+        mod = importlib.import_module(f'pygmtools.{backend}_backend')
+        fn = mod.inner_prod_aff_fn
+    except (ModuleNotFoundError, AttributeError):
+        raise NotImplementedError(
+            NOT_IMPLEMENTED_MSG.format(backend)
+        )
+    return fn(*args)
+
+
+def gaussian_aff_fn(feat1, feat2, sigma=1., backend=None):
+    r"""
+    Gaussian kernel affinity function. The affinity is defined as
+
+    .. math::
+        \exp(-\frac{(\mathbf{f}_1 - \mathbf{f}_2)^2}{\sigma})
+
+    :param feat1: :math:`(b\times n_1 \times f)` the feature vectors :math:`\mathbf{f}_1`
+    :param feat2: :math:`(b\times n_2 \times f)` the feature vectors :math:`\mathbf{f}_2`
+    :param sigma: (default: 1) the parameter :math:`\sigma` in Gaussian kernel
+    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
+    :return:  :math:`(b\times n_1\times n_2)` element-wise Gaussian affinity matrix
+    """
+    if backend is None:
+        backend = pygmtools.BACKEND
+
+    _check_data_type(feat1, backend)
+    _check_data_type(feat2, backend)
+    args = (feat1, feat2, sigma)
+    try:
+        mod = importlib.import_module(f'pygmtools.{backend}_backend')
+        fn = mod.gaussian_aff_fn
+    except (ModuleNotFoundError, AttributeError):
+        raise NotImplementedError(
+            NOT_IMPLEMENTED_MSG.format(backend)
+        )
+    return fn(*args)
+
+
+def build_batch(input, return_ori_dim=False, backend=None):
+    r"""
+    Build a batched tensor from a list of tensors. If the list of tensors are with different sizes of dimensions, it
+    will be padded to the largest dimension.
+
+    The batched tensor and the number of original dimensions will be returned.
+
+    :param input: list of input tensors
+    :param return_ori_dim: (default: False) return the original dimension
+    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
+    :return: batched tensor, (if ``return_ori_dim=True``) a list of the original dimensions
+
+    .. dropdown:: Numpy Example
+
+        ::
+
+            >>> import numpy as np
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'numpy'
+
+            # batched adjacency matrices
+            >>> A1 = np.random.rand(4, 4)
+            >>> A2 = np.random.rand(5, 5)
+            >>> A3 = np.random.rand(3, 3)
+            >>> batched_A, ori_shape = pygm.utils.build_batch([A1, A2, A3], return_ori_dim=True)
+            >>> batched_A.shape
+            (3, 5, 5)
+            >>> ori_shape
+            ([4, 5, 3], [4, 5, 3])
+
+            # batched node features (feature dimension=10)
+            >>> F1 = np.random.rand(4, 10)
+            >>> F2 = np.random.rand(5, 10)
+            >>> F3 = np.random.rand(3, 10)
+            >>> batched_F = pygm.utils.build_batch([F1, F2, F3])
+            >>> batched_F.shape
+            (3, 5, 10)
+
+    .. dropdown:: Pytorch Example
+
+        ::
+
+            >>> import torch
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'pytorch'
+
+            # batched adjacency matrices
+            >>> A1 = torch.rand(4, 4)
+            >>> A2 = torch.rand(5, 5)
+            >>> A3 = torch.rand(3, 3)
+            >>> batched_A, ori_shape = pygm.utils.build_batch([A1, A2, A3], return_ori_dim=True)
+            >>> batched_A.shape
+            torch.Size([3, 5, 5])
+            >>> ori_shape
+            (tensor([4, 5, 3]), tensor([4, 5, 3]))
+
+            # batched node features (feature dimension=10)
+            >>> F1 = torch.rand(4, 10)
+            >>> F2 = torch.rand(5, 10)
+            >>> F3 = torch.rand(3, 10)
+            >>> batched_F = pygm.utils.build_batch([F1, F2, F3])
+            >>> batched_F.shape
+            torch.Size([3, 5, 10])
+
+    .. dropdown:: Paddle Example
+
+        ::
+
+            >>> import paddle
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'paddle'
+
+            # batched adjacency matrices
+            >>> A1 = paddle.rand((4, 4))
+            >>> A2 = paddle.rand((5, 5))
+            >>> A3 = paddle.rand((3, 3))
+            >>> batched_A, ori_shape = pygm.utils.build_batch([A1, A2, A3], return_ori_dim=True)
+            >>> batched_A.shape
+            [3, 5, 5]
+            >>> ori_shape
+            (Tensor(shape=[3], dtype=int64, place=Place(cpu), stop_gradient=True, [4, 5, 3]),
+             Tensor(shape=[3], dtype=int64, place=Place(cpu), stop_gradient=True, [4, 5, 3]))
+
+            # batched node features (feature dimension=10)
+            >>> F1 = paddle.rand((4, 10))
+            >>> F2 = paddle.rand((5, 10))
+            >>> F3 = paddle.rand((3, 10))
+            >>> batched_F = pygm.utils.build_batch([F1, F2, F3])
+            >>> batched_F.shape
+            [3, 5, 10]
+
+    .. dropdown:: mindspore Example
+
+        ::
+
+            >>> import mindspore
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'mindspore'
+
+            # batched adjacency matrices
+            >>> A1 = mindspore.numpy.rand((4, 4))
+            >>> A2 = mindspore.numpy.rand((5, 5))
+            >>> A3 = mindspore.numpy.rand((3, 3))
+            >>> batched_A, ori_shape = pygm.utils.build_batch([A1, A2, A3], return_ori_dim=True)
+            >>> batched_A.shape
+            (3, 5, 5)
+            >>> ori_shape
+            (Tensor(shape=[3], dtype=Int64, value= [4, 5, 3]),
+             Tensor(shape=[3], dtype=Int64, value= [4, 5, 3]))
+
+            # batched node features (feature dimension=10)
+            >>> F1 = mindspore.numpy.rand((4, 10))
+            >>> F2 = mindspore.numpy.rand((5, 10))
+            >>> F3 = mindspore.numpy.rand((3, 10))
+            >>> batched_F = pygm.utils.build_batch([F1, F2, F3])
+            >>> batched_F.shape
+            (3, 5, 10)
+
+    """
+    if backend is None:
+        backend = pygmtools.BACKEND
+    for item in input:
+        _check_data_type(item, backend)
+    args = (input, return_ori_dim)
+    try:
+        mod = importlib.import_module(f'pygmtools.{backend}_backend')
+        fn = mod.build_batch
+    except ImportError and AttributeError:
+        raise NotImplementedError(
+            NOT_IMPLEMENTED_MSG.format(backend)
+        )
+    return fn(*args)
+
+
+def dense_to_sparse(dense_adj, backend=None):
+    r"""
+    Convert a dense connectivity/adjacency matrix to a sparse connectivity/adjacency matrix and an edge weight tensor.
+
+    :param dense_adj: :math:`(b\times n\times n)` the dense adjacency matrix. This function also supports non-batched
+                      input where the batch dimension ``b`` is ignored
+    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
+    :return: if batched input:
+             :math:`(b\times ne\times 2)` sparse connectivity matrix, :math:`(b\times ne\times 1)` edge weight tensor,
+             :math:`(b)` number of edges
+
+             if non-batched input:
+             :math:`(ne\times 2)` sparse connectivity matrix, :math:`(ne\times 1)` edge weight tensor,
+
+    .. dropdown:: Numpy Example
+
+        ::
+
+            >>> import numpy as np
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'numpy'
+            >>> np.random.seed(0)
+
+            >>> batch_size = 10
+            >>> A = np.random.rand(batch_size, 4, 4)
+            >>> A[:, np.arange(4), np.arange(4)] = 0 # remove the diagonal elements
+            >>> A.shape
+            (10, 4, 4)
+
+            >>> conn, edge, ne = pygm.utils.dense_to_sparse(A)
+            >>> conn.shape # connectivity: (batch x num_edge x 2)
+            (10, 12, 2)
+
+            >>> edge.shape # edge feature (batch x num_edge x feature_dim)
+            (10, 12, 1)
+
+            >>> ne
+            [12, 12, 12, 12, 12, 12, 12, 12, 12, 12]
+
+    .. dropdown:: Pytorch Example
+
+        ::
+
+            >>> import torch
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'pytorch'
+            >>> _ = torch.manual_seed(0)
+
+            >>> batch_size = 10
+            >>> A = torch.rand(batch_size, 4, 4)
+            >>> torch.diagonal(A, dim1=1, dim2=2)[:] = 0 # remove the diagonal elements
+            >>> A.shape
+            torch.Size([10, 4, 4])
+
+            >>> conn, edge, ne = pygm.utils.dense_to_sparse(A)
+            >>> conn.shape # connectivity: (batch x num_edge x 2)
+            torch.Size([10, 12, 2])
+
+            >>> edge.shape # edge feature (batch x num_edge x feature_dim)
+            torch.Size([10, 12, 1])
+
+            >>> ne
+            tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12])
+
+    .. dropdown:: Paddle Example
+
+        ::
+
+            >>> import paddle
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'paddle'
+            >>> paddle.seed(0)
+
+            >>> batch_size = 10
+            >>> A = paddle.rand((batch_size, 4, 4))
+            >>> paddle.diagonal(A, axis1=1, axis2=2)[:] = 0 # remove the diagonal elements
+            >>> A.shape
+            [10, 4, 4]
+
+            >>> conn, edge, ne = pygm.utils.dense_to_sparse(A)
+            >>> conn.shape # connectivity: (batch x num_edge x 2)
+            torch.Size([10, 16, 2])
+
+            >>> edge.shape # edge feature (batch x num_edge x feature_dim)
+            torch.Size([10, 16, 1])
+
+            >>> ne
+            Tensor(shape=[10], dtype=int64, place=Place(cpu), stop_gradient=True,
+                    [16, 16, 16, 16, 16, 16, 16, 16, 16, 16])
+
+    .. dropdown:: mindspore Example
+
+        ::
+
+            >>> import mindspore
+            >>> import pygmtools as pygm
+            >>> pygm.BACKEND = 'mindspore'
+            >>> _ = mindspore.set_seed(0)
+
+            >>> batch_size = 10
+            >>> A = mindspore.numpy.rand((batch_size, 4, 4))
+            >>> mindspore.numpy.diagonal(A, axis1=1, axis2=2)[:] = 0 # remove the diagonal elements
+            >>> A.shape
+            (10, 4, 4)
+
+            >>> conn, edge, ne = pygm.utils.dense_to_sparse(A)
+            >>> conn.shape # connectivity: (batch x num_edge x 2)
+            (10, 16, 2)
+
+            >>> edge.shape # edge feature (batch x num_edge x feature_dim)
+            (10, 16, 1)
+
+            >>> ne
+            [16 16 16 16 16 16 16 16 16 16]
+
+    """
+    if backend is None:
+        backend = pygmtools.BACKEND
+    _check_data_type(dense_adj, backend)
+    if _check_shape(dense_adj, 2, backend):
+        dense_adj = _unsqueeze(dense_adj, 0, backend)
+        non_batched_input = True
+    elif _check_shape(dense_adj, 3, backend):
+        non_batched_input = False
+    else:
+        raise ValueError(f'the input argument dense_adj is expected to be 2-dimensional or 3-dimensional, got '
+                         f'dense_adj:{len(_get_shape(dense_adj))}!')
+
+    args = (dense_adj,)
+    try:
+        mod = importlib.import_module(f'pygmtools.{backend}_backend')
+        fn = mod.dense_to_sparse
+    except (ModuleNotFoundError, AttributeError):
+        raise NotImplementedError(
+            NOT_IMPLEMENTED_MSG.format(backend)
+        )
+
+    result = fn(*args)
+    if non_batched_input:
+        return _squeeze(result[0], 0, backend), _squeeze(result[1], 0, backend)
+    else:
+        return result
+
+
+def compute_affinity_score(X, K, backend=None):
+    r"""
+    Compute the affinity score of graph matching. It is the objective score of the corresponding Quadratic Assignment
+    Problem.
+
+    .. math::
+
+        \texttt{vec}(\mathbf{X})^\top \mathbf{K} \texttt{vec}(\mathbf{X})
+
+    here :math:`\texttt{vec}` means column-wise vectorization.
+
+    :param X: :math:`(b\times n_1 \times n_2)` the permutation matrix that represents the matching result
+    :param K: :math:`(b\times n_1n_2 \times n_1n_2)` the affinity matrix
+    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
+    :return: :math:`(b)` the objective score
+
+    .. note::
+
+       This function also supports non-batched input if the batch dimension of ``X, K`` is ignored.
+
+    .. dropdown:: Pytorch Example
+
+        ::
+
+            >>> import pygmtools as pygm
+            >>> import torch
+            >>> pygm.BACKEND = 'pytorch'
+
+            # Generate a graph matching problem
+            >>> X_gt = torch.zeros(4, 4)
+            >>> X_gt[torch.arange(0, 4, dtype=torch.int64), torch.randperm(4)] =1
+            >>> A1 = torch.rand(4, 4)
+            >>> A2 = torch.mm(torch.mm(X_gt.transpose(0,1), A1), X_gt)
+            >>> conn1, edge1 = pygm.utils.dense_to_sparse(A1)
+            >>> conn2, edge2 = pygm.utils.dense_to_sparse(A2)
+            >>> import functools
+            >>> gaussian_aff = functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.)
+            >>> K = pygm.utils.build_aff_mat(None, edge1, conn1, None, edge2, conn2, None, None, None, None, edge_aff_fn=gaussian_aff)
+
+            # Compute the objective score of ground truth matching
+            >>> pygm.utils.compute_affinity_score(X_gt, K)
+            tensor(16.)
+
+    """
+    if backend is None:
+        backend = pygmtools.BACKEND
+    _check_data_type(X, backend)
+    _check_data_type(K, backend)
+    if _check_shape(X, 2, backend) and _check_shape(K, 2, backend):
+        X = _unsqueeze(X, 0, backend)
+        K = _unsqueeze(K, 0, backend)
+        non_batched_input = True
+    elif _check_shape(X, 3, backend) and _check_shape(X, 3, backend):
+        non_batched_input = False
+    else:
+        raise ValueError(f'the input argument K, X are expected to have the same number of dimensions (=2 or 3), got'
+                         f'X:{len(_get_shape(X))} and K:{len(_get_shape(K))}!')
+    args = (X, K)
+    try:
+        mod = importlib.import_module(f'pygmtools.{backend}_backend')
+        fn = mod.compute_affinity_score
+    except (ModuleNotFoundError, AttributeError):
+        raise NotImplementedError(
+            NOT_IMPLEMENTED_MSG.format(backend)
+        )
+
+    result = fn(*args)
+    if non_batched_input:
+        return _squeeze(result, 0, backend)
+    else:
+        return result
+
+
+def to_numpy(input, backend=None):
+    r"""
+    Convert a tensor to a numpy ndarray.
+    This is the helper function to convert tensors across different backends via numpy.
+
+    :param input: input tensor/:mod:`~pygmtools.utils.MultiMatchingResult`
+    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
+    :return: numpy ndarray
+    """
+    if backend is None:
+        backend = pygmtools.BACKEND
+    args = (input,)
+    # pygmtools built-in types
+    if type(input) is MultiMatchingResult:
+        fn = MultiMatchingResult.to_numpy
+    # tf/torch/.. tensor types
+    else:
+        try:
+            mod = importlib.import_module(f'pygmtools.{backend}_backend')
+            fn = mod.to_numpy
+        except (ModuleNotFoundError, AttributeError):
+            raise NotImplementedError(
+                NOT_IMPLEMENTED_MSG.format(backend)
+            )
+    return fn(*args)
+
+
+def from_numpy(input, device=None, backend=None):
+    r"""
+    Convert a numpy ndarray to a tensor.
+    This is the helper function to convert tensors across different backends via numpy.
+
+    :param input: input ndarray/:mod:`~pygmtools.utils.MultiMatchingResult`
+    :param device: (default: None) the target device
+    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
+    :return: tensor for the backend
+    """
+    if backend is None:
+        backend = pygmtools.BACKEND
+    args = (input, device)
+    # pygmtools built-in types
+    if type(input) is MultiMatchingResult:
+        fn = functools.partial(MultiMatchingResult.from_numpy, new_backend=backend)
+    # tf/torch/.. tensor types
+    else:
+        try:
+            mod = importlib.import_module(f'pygmtools.{backend}_backend')
+            fn = mod.from_numpy
+        except (ModuleNotFoundError, AttributeError):
+            raise NotImplementedError(
+                NOT_IMPLEMENTED_MSG.format(backend)
+            )
+    return fn(*args)
+
+
+def generate_isomorphic_graphs(node_num, graph_num=2, node_feat_dim=0, backend=None):
+    r"""
+    Generate a set of isomorphic graphs, for testing purposes and examples.
+
+    :param node_num: number of nodes in each graph
+    :param graph_num: (default: 2) number of graphs
+    :param node_feat_dim: (default: 0) number of node feature dimensions
+    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
+    :return: if ``graph_num==2``, this function returns :math:`(m\times n \times n)` the adjacency matrix, and
+             :math:`(n \times n)` the permutation matrix;
+
+             else, this function returns :math:`(m\times n \times n)` the adjacency matrix, and
+             :math:`(m\times m\times n \times n)` the multi-matching permutation matrix
+    """
+    if backend is None:
+        backend = pygmtools.BACKEND
+    args = (node_num, graph_num, node_feat_dim)
+    assert node_num > 0 and graph_num >= 2, "input data not understood."
+    try:
+        mod = importlib.import_module(f'pygmtools.{backend}_backend')
+        fn = mod.generate_isomorphic_graphs
+    except (ModuleNotFoundError, AttributeError):
+        raise NotImplementedError(
+            NOT_IMPLEMENTED_MSG.format(backend)
+        )
+    if node_feat_dim > 0:
+        As, X_gt, Fs = fn(*args)
+        if graph_num == 2:
+            return As, X_gt[0, 1], Fs
+        else:
+            return As, X_gt, Fs
+    else:
+        As, X_gt = fn(*args)
+        if graph_num == 2:
+            return As, X_gt[0, 1]
+        else:
+            return As, X_gt
+
+
+class MultiMatchingResult:
+    r"""
+    A memory-efficient class for multi-graph matching results. For non-cycle consistent results, the dense storage
+    for :math:`m` graphs with :math:`n` nodes requires a size of :math:`(m\times m \times n \times n)`, and this
+    implementation requires :math:`((m-1)\times m \times n \times n / 2)`. For cycle consistent result, this
+    implementation requires only :math:`(m\times n\times n)`.
+
+    .. dropdown:: Numpy Example
+
+        ::
+
+            >>> import numpy as np
+            >>> import pygmtools as pygm
+            >>> np.random.seed(0)
+
+            >>> X = pygm.utils.MultiMatchingResult(backend='numpy')
+            >>> X[0, 1] = np.zeros((4, 4))
+            >>> X[0, 1][np.arange(0, 4, dtype=np.int64), np.random.permutation(4)] = 1
+            >>> X
+            MultiMatchingResult:
+            {'0,1': array([[0., 0., 1., 0.],
+                [0., 0., 0., 1.],
+                [0., 1., 0., 0.],
+                [1., 0., 0., 0.]])}
+            >>> X[1, 0]
+            array([[0., 0., 0., 1.],
+                [0., 0., 1., 0.],
+                [1., 0., 0., 0.],
+                [0., 1., 0., 0.]])
+    """
+    def __init__(self, cycle_consistent=False, backend=None):
+        self.match_dict = {}
+        self._cycle_consistent = cycle_consistent
+        if backend is None:
+            self.backend = pygmtools.BACKEND
+        else:
+            self.backend = backend
+
+    def __getitem__(self, item):
+        assert len(item) == 2, "key should be the indices of two graphs, e.g. (0, 1)"
+        idx1, idx2 = item
+        if self._cycle_consistent:
+            return _mm(self.match_dict[idx1], _transpose(self.match_dict[idx2], 0, 1, self.backend), self.backend)
+        else:
+            if idx1 < idx2:
+                return self.match_dict[f'{idx1},{idx2}']
+            else:
+                return _transpose(self.match_dict[f'{idx2},{idx1}'], 0, 1, self.backend)
+
+    def __setitem__(self, key, value):
+        if self._cycle_consistent:
+            assert type(key) is int, "key should be the index of one graph, and value should be the matching to universe"
+            self.match_dict[key] = value
+        else:
+            assert len(key) == 2, "key should be the indices of two graphs, e.g. (0, 1)"
+            idx1, idx2 = key
+            if idx1 < idx2:
+                self.match_dict[f'{idx1},{idx2}'] = value
+            else:
+                self.match_dict[f'{idx2},{idx1}'] = _transpose(value, 0, 1, self.backend)
+
+    def __str__(self):
+        return 'MultiMatchingResult:\n' + self.match_dict.__str__()
+
+    def __repr__(self):
+        return 'MultiMatchingResult:\n' + self.match_dict.__repr__()
+
+    @staticmethod
+    def from_numpy(data, device=None, new_backend=None):
+        r"""
+        Convert a numpy-backend MultiMatchingResult data to another backend.
+
+        :param data: the numpy-backend data
+        :param device: (default: None) the target device
+        :param new_backend: (default: ``pygmtools.BACKEND`` variable) the target backend
+        :return: a new MultiMatchingResult instance for ``new_backend`` on ``device``
+        """
+        new_data = copy.deepcopy(data)
+        new_data.from_numpy_(device, new_backend)
+        return new_data
+
+    @staticmethod
+    def to_numpy(data):
+        r"""
+        Convert an any-type MultiMatchingResult to numpy backend.
+
+        :param data: the any-type data
+        :return: a new MultiMatchingResult instance for numpy
+        """
+        new_data = copy.deepcopy(data)
+        new_data.to_numpy_()
+        return new_data
+
+    def from_numpy_(self, device=None, new_backend=None):
+        """
+        In-place operation for :func:`~pygmtools.utils.MultiMatchingResult.from_numpy`.
+        """
+        if self.backend != 'numpy':
+            raise ValueError('Attempting to convert from non-numpy data.')
+        if new_backend is None:
+            new_backend = pygmtools.BACKEND
+        self.backend = new_backend
+        for k, v in self.match_dict.items():
+            self.match_dict[k] = from_numpy(v, device, self.backend)
+
+    def to_numpy_(self):
+        """
+        In-place operation for :func:`~pygmtools.utils.MultiMatchingResult.to_numpy`.
+        """
+        for k, v in self.match_dict.items():
+            self.match_dict[k] = to_numpy(v, self.backend)
+        self.backend = 'numpy'
+
+
+def get_network(nn_solver_func, **params):
+    r"""
+    Get the network object of a neural network solver.
+
+    :param nn_solver_func: the neural network solver function, for example ``pygm.pca_gm``
+    :param params: keyword parameters to define the neural network
+    :return: the network object
+
+    .. dropdown:: Pytorch Example
+
+        ::
+
+            >>> import pygmtools as pygm
+            >>> import torch
+            >>> pygm.BACKEND = 'pytorch'
+            >>> pygm.utils.get_network(pygm.pca_gm, pretrain='willow')
+            PCA_GM_Net(
+              (gnn_layer_0): Siamese_Gconv(
+                (gconv): Gconv(
+                  (a_fc): Linear(in_features=1024, out_features=2048, bias=True)
+                  (u_fc): Linear(in_features=1024, out_features=2048, bias=True)
+                )
+              )
+              (cross_graph_0): Linear(in_features=4096, out_features=2048, bias=True)
+              (affinity_0): WeightedInnerProdAffinity()
+              (affinity_1): WeightedInnerProdAffinity()
+              (gnn_layer_1): Siamese_Gconv(
+                (gconv): Gconv(
+                  (a_fc): Linear(in_features=2048, out_features=2048, bias=True)
+                  (u_fc): Linear(in_features=2048, out_features=2048, bias=True)
+                )
+              )
+            )
+
+            # the neural network can be integrated into a deep learning pipeline
+            >>> net = pygm.utils.get_network(pygm.pca_gm, in_channel=1024, hidden_channel=2048, out_channel=512, num_layers=3, pretrain=False)
+            >>> optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)
+
+    """
+    if 'return_network' in params:
+        params.pop('return_network')
+    # count parameters w/o default value
+    sig = inspect.signature(nn_solver_func)
+    required_params = 0
+    for p in sig.parameters.items():
+        if p[1].default is inspect._empty:
+            required_params += 1
+    _, net = nn_solver_func(*[None] * required_params, # fill the required parameters by None
+                            return_network=True, **params)
+    return net
+
+
+def permutation_loss(pred_dsmat, gt_perm, n1=None, n2=None, backend=None):
+    r"""
+    Binary cross entropy loss between two permutations, also known as "permutation loss".
+    Proposed by `"Wang et al. Learning Combinatorial Embedding Networks for Deep Graph Matching. ICCV 2019."
+    <http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Learning_Combinatorial_Embedding_Networks_for_Deep_Graph_Matching_ICCV_2019_paper.pdf>`_
+
+    .. math::
+        L_{perm} =- \sum_{i \in \mathcal{V}_1, j \in \mathcal{V}_2}
+        \left(\mathbf{X}^{gt}_{i,j} \log \mathbf{S}_{i,j} + (1-\mathbf{X}^{gt}_{i,j}) \log (1-\mathbf{S}_{i,j}) \right)
+
+    where :math:`\mathcal{V}_1, \mathcal{V}_2` are vertex sets for two graphs.
+
+    :param pred_dsmat: :math:`(b\times n_1 \times n_2)` predicted doubly-stochastic matrix :math:`(\mathbf{S})`
+    :param gt_perm: :math:`(b\times n_1 \times n_2)` ground truth permutation matrix :math:`(\mathbf{X}^{gt})`
+    :param n1: (optional) :math:`(b)` number of exact pairs in the first graph.
+    :param n2: (optional) :math:`(b)` number of exact pairs in the second graph.
+    :param backend: (default: ``pygmtools.BACKEND`` variable) the backend for computation.
+    :return: :math:`(1)` averaged permutation loss
+
+    .. note::
+        We support batched instances with different number of nodes, therefore ``n1`` and ``n2`` are
+        required if you want to specify the exact number of nodes of each instance in the batch.
+
+    .. note::
+        For batched input, this loss function computes the averaged loss among all instances in the batch. This function
+        also supports non-batched input if the batch dimension (:math:`b`) is ignored.
+    """
+    if backend is None:
+        backend = pygmtools.BACKEND
+    _check_data_type(pred_dsmat, backend)
+    _check_data_type(gt_perm, backend)
+    dsmat_shape = _get_shape(pred_dsmat, backend)
+    perm_shape = _get_shape(gt_perm, backend)
+    if len(dsmat_shape) == len(perm_shape) == 2:
+        pred_dsmat = _unsqueeze(pred_dsmat, 0, backend)
+        gt_perm = _unsqueeze(gt_perm, 0, backend)
+    elif len(dsmat_shape) == len(perm_shape) == 3:
+        pass
+    else:
+        raise ValueError(f'the input arguments pred_dsmat and gt_perm are expected to be 2-dimensional or 3-dimensional,'
+                         f' got pred_dsmat:{len(dsmat_shape)}, gt_perm:{len(perm_shape)}!')
+
+    for d1, d2 in zip(dsmat_shape, perm_shape):
+        if d1 != d2:
+            raise ValueError(f'dimension mismatch for pred_dsmat and gt_perm, got pred_dsmat:{dsmat_shape}, gt_perm:{gt_perm}!')
+
+    args = (pred_dsmat, gt_perm, n1, n2)
+    try:
+        mod = importlib.import_module(f'pygmtools.{backend}_backend')
+        fn = mod.permutation_loss
+    except (ModuleNotFoundError, AttributeError):
+        raise NotImplementedError(
+            NOT_IMPLEMENTED_MSG.format(backend)
+        )
+
+    return fn(*args)
+
+
+###################################################
+#   Private Functions that Unseeable from Users   #
+###################################################
+
+
+def _aff_mat_from_node_edge_aff(node_aff, edge_aff, connectivity1, connectivity2,
+                                n1, n2, ne1, ne2,
+                                backend=None):
+    r"""
+    Build affinity matrix K from node and edge affinity matrices.
+
+    :param node_aff: :math:`(b\times n_1 \times n_2)` the node affinity matrix
+    :param edge_aff: :math:`(b\times ne_1 \times ne_2)` the edge affinity matrix
+    :param connectivity1: :math:`(b\times ne_1 \times 2)` sparse connectivity information of graph 1
+    :param connectivity2: :math:`(b\times ne_2 \times 2)` sparse connectivity information of graph 2
+    :param n1: :math:`(b)` number of nodes in graph1. If not given, it will be inferred based on the shape of
+               ``node_feat1`` or the values in ``connectivity1``
+    :param ne1: :math:`(b)` number of edges in graph1. If not given, it will be inferred based on the shape of
+               ``edge_feat1``
+    :param n2: :math:`(b)` number of nodes in graph2. If not given, it will be inferred based on the shape of
+               ``node_feat2`` or the values in ``connectivity2``
+    :param ne2: :math:`(b)` number of edges in graph2. If not given, it will be inferred based on the shape of
+               ``edge_feat2``
+    :return: :math:`(b\times n_1n_2 \times n_1n_2)` the affinity matrix
+    """
+    if backend is None:
+        backend = pygmtools.BACKEND
+    args = (node_aff, edge_aff, connectivity1, connectivity2, n1, n2, ne1, ne2)
+    try:
+        mod = importlib.import_module(f'pygmtools.{backend}_backend')
+        fn = mod._aff_mat_from_node_edge_aff
+    except (ModuleNotFoundError, AttributeError):
+        raise NotImplementedError(
+            NOT_IMPLEMENTED_MSG.format(backend)
+        )
+    return fn(*args)
+
+
+def _check_data_type(input, *args):
+    r"""
+    Check whether the input data meets the backend. If not met, it will raise an ValueError
+    Three overloads of this function:
+    _check_data_type(input, backend)
+    _check_data_type(input, var_name, backend)
+    _check_data_type(input, var_name, raise_err, backend)
+
+    :param input: input data (must be Tensor/ndarray)
+    :param var_name: name of the variable
+    :param raise_err: raise an error if input data not true
+    :return: True or False
+    """
+    if len(args) == 3:
+        var_name, raise_err, backend = args
+    elif len(args) == 2:
+        var_name, backend = args
+        raise_err = True
+    elif len(args) == 1:
+        backend = args[0]
+        var_name = None
+        raise_err = True
+    elif len(args) == 0:
+        backend = None
+        var_name = None
+        raise_err = True
+    else:
+        raise RuntimeError(f'Unknown arguments: {args}')
+
+    if backend is None:
+        backend = pygmtools.BACKEND
+    args = (input, var_name, raise_err)
+    try:
+        mod = importlib.import_module(f'pygmtools.{backend}_backend')
+        fn = mod._check_data_type
+    except (ModuleNotFoundError, AttributeError):
+        raise NotImplementedError(
+            NOT_IMPLEMENTED_MSG.format(backend)
+        )
+    return fn(*args)
+
+
+def _check_shape(input, num_dim, backend=None):
+    r"""
+    Check the shape of the input tensor
+
+    :param input: the input tensor
+    :param num_dim: number of dimensions
+    :return: True or False
+    """
+    if backend is None:
+        backend = pygmtools.BACKEND
+    args = (input, num_dim)
+    try:
+        mod = importlib.import_module(f'pygmtools.{backend}_backend')
+        fn = mod._check_shape
+    except (ModuleNotFoundError, AttributeError):
+        raise NotImplementedError(
+            NOT_IMPLEMENTED_MSG.format(backend)
+        )
+    return fn(*args)
+
+
+def _get_shape(input, backend=None):
+    r"""
+    Get the shape of the input tensor
+
+    :param input: the input tensor
+    :return: a list of ints indicating the shape
+    """
+    if backend is None:
+        backend = pygmtools.BACKEND
+    args = (input,)
+    try:
+        mod = importlib.import_module(f'pygmtools.{backend}_backend')
+        fn = mod._get_shape
+    except (ModuleNotFoundError, AttributeError):
+        raise NotImplementedError(
+            NOT_IMPLEMENTED_MSG.format(backend)
+        )
+    return fn(*args)
+
+
+def _squeeze(input, dim, backend=None):
+    r"""
+    Squeeze the input tensor at the given dimension. This function is expected to behave the same as torch.squeeze
+
+    :param input: input tensor
+    :param dim: squeezed dimension
+    :return: squeezed tensor
+    """
+    if backend is None:
+        backend = pygmtools.BACKEND
+    args = (input, dim)
+    try:
+        mod = importlib.import_module(f'pygmtools.{backend}_backend')
+        fn = mod._squeeze
+    except (ModuleNotFoundError, AttributeError):
+        raise NotImplementedError(
+            NOT_IMPLEMENTED_MSG.format(backend)
+        )
+    return fn(*args)
+
+
+def _unsqueeze(input, dim, backend=None):
+    r"""
+    Unsqueeze the input tensor at the given dimension. This function is expected to behave the same as torch.unsqueeze
+
+    :param input: input tensor
+    :param dim: unsqueezed dimension
+    :return: unsqueezed tensor
+    """
+    if backend is None:
+        backend = pygmtools.BACKEND
+    args = (input, dim)
+    try:
+        mod = importlib.import_module(f'pygmtools.{backend}_backend')
+        fn = mod._unsqueeze
+    except (ModuleNotFoundError, AttributeError):
+        raise NotImplementedError(
+            NOT_IMPLEMENTED_MSG.format(backend)
+        )
+    return fn(*args)
+
+
+def _transpose(input, dim1, dim2, backend=None):
+    r"""
+    Swap the dim1 and dim2 dimensions of the input tensor.
+
+    :param input: input tensor
+    :param dim1: swapped dimension 1
+    :param dim2: swapped dimension 2
+    :return: transposed tensor
+    """
+    if backend is None:
+        backend = pygmtools.BACKEND
+    args = (input, dim1, dim2)
+    try:
+        mod = importlib.import_module(f'pygmtools.{backend}_backend')
+        fn = mod._transpose
+    except (ModuleNotFoundError, AttributeError):
+        raise NotImplementedError(
+            NOT_IMPLEMENTED_MSG.format(backend)
+        )
+    return fn(*args)
+
+
+def _mm(input1, input2, backend=None):
+    r"""
+    Matrix multiplication.
+
+    :param input1: input tensor 1
+    :param input2: input tensor 2
+    :return: multiplication result
+    """
+    if backend is None:
+        backend = pygmtools.BACKEND
+    args = (input1, input2)
+    try:
+        mod = importlib.import_module(f'pygmtools.{backend}_backend')
+        fn = mod._mm
+    except (ModuleNotFoundError, AttributeError):
+        raise NotImplementedError(
+            NOT_IMPLEMENTED_MSG.format(backend)
+        )
+    return fn(*args)
+
+def download(filename, url, md5=None, retries=10, to_cache=True):
+    r"""
+    Check if content exits. If not, download the content to ``<user cache path>/pygmtools/<filename>``. ``<user cache path>``
+    depends on your system. For example, on Debian, it should be ``$HOME/.cache``.
+    :param filename: the destination file name
+    :param url: the url
+    :param md5: (optional) the md5sum to verify the content. It should match the result of ``md5sum file`` on Linux.
+    :param retries: (default: 5) max number of retries
+    :return: the full path to the file: ``<user cache path>/pygmtools/<filename>``
+    """
+    if retries <= 0:
+        raise RuntimeError('Max Retries exceeded!')
+
+    if to_cache:
+        dirs = user_cache_dir("pygmtools")
+        if not os.path.exists(dirs):
+            os.makedirs(dirs)
+        filename = os.path.join(dirs, filename)
+    if not os.path.exists(filename):
+        print(f'\nDownloading to {filename}...')
+        if retries % 2 == 1:
+            try:
+                down_res = requests.get(url, stream=True)
+                file_size = int(down_res.headers.get('Content-Length', 0))
+                with tqdm.wrapattr(down_res.raw, "read", total=file_size) as content:
+                    with open(filename, 'wb') as file:
+                        shutil.copyfileobj(content, file)
+            except requests.exceptions.ConnectionError as err:
+                print('Warning: Network error. Retrying...\n', err)
+                return download(filename, url, md5, retries - 1)
+        else:
+            try:
+                wget.download(url,out=filename)
+            except:
+                return download(filename, url, md5, retries - 1)
+    if md5 is not None:
+        md5_returned = _get_md5(filename)
+        if md5 != md5_returned:
+            print('Warning: MD5 check failed for the downloaded content. Retrying...')
+            os.remove(filename)
+            time.sleep(1)
+            return download(filename, url, md5, retries - 1)
+    return filename
+
+def _get_md5(filename):
+    hash_md5 = hashlib.md5()
+    chunk = 8192
+    with open(filename, 'rb') as file_to_check:
+        while True:
+            buffer = file_to_check.read(chunk)
+            if not buffer:
+                break
+            hash_md5.update(buffer)
+        md5_returned = hash_md5.hexdigest()
+        return md5_returned
```

### Comparing `pygmtools-0.3.8/pygmtools.egg-info/PKG-INFO` & `pygmtools-0.3.8a0/pygmtools.egg-info/PKG-INFO`

 * *Files 5% similar despite different names*

```diff
@@ -1,223 +1,223 @@
-Metadata-Version: 2.1
-Name: pygmtools
-Version: 0.3.8
-Summary: pygmtools provides graph matching solvers in Python API and supports numpy and pytorch backends. pygmtools also provides dataset API for standard graph matching benchmarks.
-Home-page: https://pygmtools.readthedocs.io/
-Author: ThinkLab at SJTU
-License: Mulan PSL v2
-Classifier: License :: OSI Approved :: Mulan Permissive Software License v2 (MulanPSL-2.0)
-Classifier: Programming Language :: Python :: 3 :: Only
-Classifier: Operating System :: OS Independent
-Classifier: Environment :: GPU :: NVIDIA CUDA
-Classifier: Environment :: Console
-Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
-Classifier: Topic :: Scientific/Engineering :: Image Recognition
-Classifier: Topic :: Scientific/Engineering :: Mathematics
-Requires-Python: >=3.7
-Description-Content-Type: text/markdown
-License-File: LICENSE
-
-
-<img src="https://pygmtools.readthedocs.io/en/latest/_static/images/pygmtools_logo.svg" alt="pygmtools: Python Graph Matching Tools" width="800"/>
-
-[![PyPi version](https://badgen.net/pypi/v/pygmtools/)](https://pypi.org/pypi/pygmtools/)
-[![PyPI pyversions](https://img.shields.io/badge/dynamic/json?color=blue&label=python&query=info.requires_python&url=https%3A%2F%2Fpypi.org%2Fpypi%2Fpygmtools%2Fjson)](https://pypi.python.org/pypi/pygmtools/)
-[![Downloads](https://pepy.tech/badge/pygmtools)](https://pepy.tech/project/pygmtools)
-[![Documentation Status](https://readthedocs.org/projects/pygmtools/badge/?version=latest)](https://pygmtools.readthedocs.io/en/latest/?badge=latest)
-[![codecov](https://codecov.io/gh/Thinklab-SJTU/pygmtools/branch/main/graph/badge.svg?token=Q68XTY0N0C)](https://codecov.io/gh/Thinklab-SJTU/pygmtools)
-[![discord channel](https://img.shields.io/discord/1028701206526304317.svg?&color=blueviolet&label=discord)](https://discord.gg/8m6n7rRz9T)
-[![QQ group](https://img.shields.io/badge/QQ%20group-696401889-blue)](https://qm.qq.com/cgi-bin/qm/qr?k=QolXYJn_M5ilDEM9e2jEjlPnJ02Ktabd&jump_from=webapi&authKey=6zG6D/Js4YF5h5zj778aO5MDKOXBwPFi8gQ4LsXJN8Hn1V8uCVGV81iT4J/FjPGT)
-[![GitHub stars](https://img.shields.io/github/stars/Thinklab-SJTU/pygmtools.svg?style=social&label=Star&maxAge=8640)](https://GitHub.com/Thinklab-SJTU/pygmtools/stargazers/) 
-
------------------------------------------
-
-``pygmtools`` (Python Graph Matching Tools) provides graph matching solvers in Python and is easily accessible via:
-
-```bash
-$ pip install pygmtools
-```
-
-Official documentation: https://pygmtools.readthedocs.io
-
-Source code: https://github.com/Thinklab-SJTU/pygmtools
-
-Graph matching is a fundamental yet challenging problem in pattern recognition, data mining, and others.
-Graph matching aims to find node-to-node correspondence among multiple graphs, by solving an NP-hard combinatorial
-optimization problem.
-
-Doing graph matching in Python used to be difficult, and this library wants to make researchers' lives easier. 
-To highlight, ``pygmtools`` has the following features:
-
-* *Support various solvers*, including traditional combinatorial solvers (including linear, quadratic, and multi-graph) 
-  and novel deep learning-based solvers;
-* *Support various backends*, including ``numpy`` which is universally accessible, and some state-of-the-art deep 
-  learning architectures with GPU support: 
-  ``pytorch``, ``paddle``, ``jittor``, ``Tensorflow``; 
-* *Deep learning friendly*, the operations are designed to best preserve the gradient during computation and batched 
-  operations support for the best performance.
-  
-## Installation
-
-You can install the stable release on PyPI:
-
-```bash
-$ pip install pygmtools
-```
-
-or get the latest version by running:
-
-```bash
-$ pip install -U https://github.com/Thinklab-SJTU/pygmtools/archive/master.zip # with --user for user install (no root)
-```
-
-Now the pygmtools is available with the ``numpy`` backend.
-
-The following packages are required, and shall be automatically installed by ``pip``:
-
-```
-Python >= 3.7
-requests >= 2.25.1
-scipy >= 1.4.1
-Pillow >= 7.2.0
-numpy >= 1.18.5
-easydict >= 1.7
-appdirs >= 1.4.4
-tqdm >= 4.64.1
-wget >= 3.2
-```
-  
-## Available Graph Matching Solvers
-This library offers user-friendly API for the following solvers:
-
-* [Two-Graph Matching Solvers](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.classic_solvers.html)
-    * Linear assignment solvers including the differentiable soft 
-      [Sinkhorn algorithm](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.linear_solvers.sinkhorn.html) [1], 
-      and the exact solver [Hungarian](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.linear_solvers.hungarian.html) [2].
-    * Soft and differentiable quadratic assignment solvers, including [spectral graph matching](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.classic_solvers.sm.html) [3] 
-      and [random-walk-based graph matching](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.classic_solvers.rrwm.html) [4].
-    * Discrete (non-differentiable) quadratic assignment solver 
-      [integer projected fixed point method](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.classic_solvers.ipfp.html) [5]. 
-* [Multi-Graph Matching Solvers](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.multi_graph_solvers.html)
-    * [Composition based Affinity Optimization (CAO) solver](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.multi_graph_solvers.cao.html) [6] 
-      by optimizing the affinity score, meanwhile gradually infusing the consistency.
-    * Multi-Graph Matching based on 
-      [Floyd shortest path algorithm](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.multi_graph_solvers.mgm_floyd.html) [7].
-    * [Graduated-assignment based multi-graph matching solver](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.multi_graph_solvers.gamgm.html) [8][9]
-      by graduated annealing of Sinkhorn‚Äôs temperature.
-* [Neural Graph Matching Solvers](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.neural_solvers.html)
-    * Intra-graph and cross-graph embedding based neural graph matching solvers 
-      [PCA-GM](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.neural_solvers.pca_gm.html) 
-      and [IPCA-GM](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.neural_solvers.ipca_gm.html) [10]
-      for matching individual graphs.
-    * [Channel independent embedding (CIE)](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.neural_solvers.cie.html) [11]
-      based neural graph matching solver for matching individual graphs.
-    * [Neural graph matching solver (NGM)](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.neural_solvers.ngm.html) [12]
-      for the general quadratic assignment formulation.
-
-## Available Backends
-This library is designed to support multiple backends with the same set of API. 
-Please follow the official instructions to install your backend.
-
-The following backends are available:
-
-* [Numpy](https://numpy.org/) (**default** backend, CPU only)
-
-<img src="https://pygmtools.readthedocs.io/en/latest/_images/numpy_logo.png" alt="numpy logo" width="200"/>
-
-* [PyTorch](https://pytorch.org/) (GPU friendly, deep learning friendly)
-
-<img src="https://pygmtools.readthedocs.io/en/latest/_images/pytorch_logo.png" alt="pytorch logo" width="200"/>
-
-* [Jittor](https://github.com/Jittor/Jittor) (GPU friendly, JIT support, deep learning friendly)
-
-<img src="https://pygmtools.readthedocs.io/en/latest/_images/jittor_logo.png" alt="jittor logo" width="200"/>
-
-* [PaddlePaddle](https://www.paddlepaddle.org.cn/en) (GPU friendly, deep learning friendly)
-
-<img src="https://pygmtools.readthedocs.io/en/latest/_images/paddle_logo.png" alt="paddle logo" width="200"/>
-
-* [Tensorflow](https://tensorflow.google.cn/) (GPU friendly, deep learning friendly)
-
-<img src="https://pygmtools.readthedocs.io/en/latest/_images/tensorflow_logo.png" alt="tensorflow logo" width="200"/>
-
-### Development status (0.3.8)
-
-|                     | Numpy | PyTorch | Jittor | PaddlePaddle | Tensorflow | MindSpore |
-| ------------------- | ----- | ------- | ------ | ------------ | ---------- | --------- |
-| Linear Solvers      | ‚úî     | ‚úî       | ‚úî      | ‚úî            | ‚úî         | ‚úî        |
-| Classic Solvers     | ‚úî     | ‚úî       | ‚úî      | ‚úî            | ‚úî         | ‚úî        |
-| Multi-Graph Solvers | ‚úî    | ‚úî       | ‚úî      | ‚úî            | üìÜ         | üìÜ        |
-| Neural Solvers      | ‚úî    | ‚úî       | ‚úî      | ‚úî           | üìÜ         | üìÜ        |
-| Examples Gallery    | ‚úî    | ‚úî       | ‚úî      | ‚úî           | üìÜ         | üìÜ        |
-
-‚úî: Supported; üìÜ: Planned for future versions (contributions welcomed!).
-
-For more details, please [read the documentation](https://pygmtools.readthedocs.io/en/latest/guide/get_started.html#install-other-backends).
-
-## Pretrained Models
-
-The library includes several neural network solvers. The pretrained models shall be automatically downloaded upon 
-needed from Google Drive. If you are experiencing issues accessing Google Drive, please download the pretrained models
-manually and put them at ``~/.cache/pygmtools`` (for Linux).
-
-Available at:
-[[google drive]](https://drive.google.com/drive/folders/1O7vkIW8QXBJsNsHUIRiSw91HJ_0FAzu_?usp=sharing)
-[[baidu drive]](https://pan.baidu.com/s/1MvzfM52NJeLWx2JXbbc6HA?pwd=x8bv)
-
-## The Deep Graph Matching Benchmark
-
-``pygmtools`` is also featured with a standard data interface of several graph matching benchmarks. Please read 
-[the corresponding documentation](https://pygmtools.readthedocs.io/en/latest/guide/benchmark.html) for details.
-
-We also maintain a repository containing non-trivial implementation of deep graph matching models, please check out
-[ThinkMatch](https://thinkmatch.readthedocs.io/) if you are interested!
-
-## Chat with the Community
-
-If you have any questions, or if you are experiencing any issues, feel free to raise an issue on GitHub. 
-
-We also offer the following chat rooms if you are more comfortable with them:
-
-* Discord (for English speakers): 
-  
-  [![discord](https://discordapp.com/api/guilds/1028701206526304317/widget.png?style=banner2)](https://discord.gg/8m6n7rRz9T)
-
-* QQ Group (for Chinese speakers)/QQÁæ§(‰∏≠ÊñáÁî®Êà∑): 696401889
-  
-  [![ThinkMatch/pygmtools‰∫§ÊµÅÁæ§](http://pub.idqqimg.com/wpa/images/group.png)](https://qm.qq.com/cgi-bin/qm/qr?k=NlPuwwvaFaHzEWD8w7jSOTzoqSLIM80V&jump_from=webapi&authKey=chI2htrWDujQed6VtVid3V1NXEoJvwz3MVwruax6x5lQIvLsC8BmpmzBJOCzhtQd)
-
-## Contributing
-Any contributions/ideas/suggestions from the community is welcomed! Before starting your contribution, please read the
-[Contributing Guide](https://github.com/Thinklab-SJTU/pygmtools/blob/main/CONTRIBUTING.md).
-
-## Developers and Maintainers
-
-``pygmtools`` is currently developed and maintained by members from [ThinkLab](http://thinklab.sjtu.edu.cn) at 
-Shanghai Jiao Tong University. 
-
-## References
-<!--MLA style references-->
-
-[1] Sinkhorn, Richard, and Paul Knopp. "Concerning nonnegative matrices and doubly stochastic matrices." Pacific Journal of Mathematics 21.2 (1967): 343-348.
-
-[2] Munkres, James. "Algorithms for the assignment and transportation problems." Journal of the society for industrial and applied mathematics 5.1 (1957): 32-38.
-
-[3] Leordeanu, Marius, and Martial Hebert. "A spectral technique for correspondence problems using pairwise constraints." International Conference on Computer Vision (2005).
-
-[4] Cho, Minsu, Jungmin Lee, and Kyoung Mu Lee. "Reweighted random walks for graph matching." European conference on Computer vision. Springer, Berlin, Heidelberg, 2010.
-
-[5] Leordeanu, Marius, Martial Hebert, and Rahul Sukthankar. "An integer projected fixed point method for graph matching and map inference." Advances in neural information processing systems 22 (2009).
-
-[6] Yan, Junchi, et al. "Multi-graph matching via affinity optimization with graduated consistency regularization." IEEE transactions on pattern analysis and machine intelligence 38.6 (2015): 1228-1242.
-
-[7] Jiang, Zetian, Tianzhe Wang, and Junchi Yan. "Unifying offline and online multi-graph matching via finding shortest paths on supergraph." IEEE transactions on pattern analysis and machine intelligence 43.10 (2020): 3648-3663.
-
-[8] Sol√©-Ribalta, Albert, and Francesc Serratosa. "Graduated assignment algorithm for multiple graph matching based on a common labeling." International Journal of Pattern Recognition and Artificial Intelligence 27.01 (2013): 1350001.
-
-[9] Wang, Runzhong, Junchi Yan, and Xiaokang Yang. "Graduated assignment for joint multi-graph matching and clustering with application to unsupervised graph matching network learning." Advances in Neural Information Processing Systems 33 (2020): 19908-19919.
-
-[10] Wang, Runzhong, Junchi Yan, and Xiaokang Yang. "Combinatorial learning of robust deep graph matching: an embedding based approach." IEEE Transactions on Pattern Analysis and Machine Intelligence (2020).
-
-[11] Yu, Tianshu, et al. "Learning deep graph matching with channel-independent embedding and hungarian attention." International conference on learning representations. 2019.
-
-[12] Wang, Runzhong, Junchi Yan, and Xiaokang Yang. "Neural graph matching network: Learning lawler‚Äôs quadratic assignment problem with extension to hypergraph and multiple-graph matching." IEEE Transactions on Pattern Analysis and Machine Intelligence (2021).
+Metadata-Version: 2.1
+Name: pygmtools
+Version: 0.3.8a0
+Summary: pygmtools provides graph matching solvers in Python API and supports numpy and pytorch backends. pygmtools also provides dataset API for standard graph matching benchmarks.
+Home-page: https://pygmtools.readthedocs.io/
+Author: ThinkLab at SJTU
+License: Mulan PSL v2
+Classifier: License :: OSI Approved :: Mulan Permissive Software License v2 (MulanPSL-2.0)
+Classifier: Programming Language :: Python :: 3 :: Only
+Classifier: Operating System :: OS Independent
+Classifier: Environment :: GPU :: NVIDIA CUDA
+Classifier: Environment :: Console
+Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
+Classifier: Topic :: Scientific/Engineering :: Image Recognition
+Classifier: Topic :: Scientific/Engineering :: Mathematics
+Requires-Python: >=3.7
+Description-Content-Type: text/markdown
+License-File: LICENSE
+
+
+<img src="https://pygmtools.readthedocs.io/en/latest/_static/images/pygmtools_logo.svg" alt="pygmtools: Python Graph Matching Tools" width="800"/>
+
+[![PyPi version](https://badgen.net/pypi/v/pygmtools/)](https://pypi.org/pypi/pygmtools/)
+[![PyPI pyversions](https://img.shields.io/badge/dynamic/json?color=blue&label=python&query=info.requires_python&url=https%3A%2F%2Fpypi.org%2Fpypi%2Fpygmtools%2Fjson)](https://pypi.python.org/pypi/pygmtools/)
+[![Downloads](https://pepy.tech/badge/pygmtools)](https://pepy.tech/project/pygmtools)
+[![Documentation Status](https://readthedocs.org/projects/pygmtools/badge/?version=latest)](https://pygmtools.readthedocs.io/en/latest/?badge=latest)
+[![codecov](https://codecov.io/gh/Thinklab-SJTU/pygmtools/branch/main/graph/badge.svg?token=Q68XTY0N0C)](https://codecov.io/gh/Thinklab-SJTU/pygmtools)
+[![discord channel](https://img.shields.io/discord/1028701206526304317.svg?&color=blueviolet&label=discord)](https://discord.gg/8m6n7rRz9T)
+[![QQ group](https://img.shields.io/badge/QQ%20group-696401889-blue)](https://qm.qq.com/cgi-bin/qm/qr?k=QolXYJn_M5ilDEM9e2jEjlPnJ02Ktabd&jump_from=webapi&authKey=6zG6D/Js4YF5h5zj778aO5MDKOXBwPFi8gQ4LsXJN8Hn1V8uCVGV81iT4J/FjPGT)
+[![GitHub stars](https://img.shields.io/github/stars/Thinklab-SJTU/pygmtools.svg?style=social&label=Star&maxAge=8640)](https://GitHub.com/Thinklab-SJTU/pygmtools/stargazers/) 
+
+-----------------------------------------
+
+``pygmtools`` (Python Graph Matching Tools) provides graph matching solvers in Python and is easily accessible via:
+
+```bash
+$ pip install pygmtools
+```
+
+Official documentation: https://pygmtools.readthedocs.io
+
+Source code: https://github.com/Thinklab-SJTU/pygmtools
+
+Graph matching is a fundamental yet challenging problem in pattern recognition, data mining, and others.
+Graph matching aims to find node-to-node correspondence among multiple graphs, by solving an NP-hard combinatorial
+optimization problem.
+
+Doing graph matching in Python used to be difficult, and this library wants to make researchers' lives easier. 
+To highlight, ``pygmtools`` has the following features:
+
+* *Support various solvers*, including traditional combinatorial solvers (including linear, quadratic, and multi-graph) 
+  and novel deep learning-based solvers;
+* *Support various backends*, including ``numpy`` which is universally accessible, and some state-of-the-art deep 
+  learning architectures with GPU support: 
+  ``pytorch``, ``paddle``, ``jittor``, ``Tensorflow``; 
+* *Deep learning friendly*, the operations are designed to best preserve the gradient during computation and batched 
+  operations support for the best performance.
+  
+## Installation
+
+You can install the stable release on PyPI:
+
+```bash
+$ pip install pygmtools
+```
+
+or get the latest version by running:
+
+```bash
+$ pip install -U https://github.com/Thinklab-SJTU/pygmtools/archive/master.zip # with --user for user install (no root)
+```
+
+Now the pygmtools is available with the ``numpy`` backend.
+
+The following packages are required, and shall be automatically installed by ``pip``:
+
+```
+Python >= 3.7
+requests >= 2.25.1
+scipy >= 1.4.1
+Pillow >= 7.2.0
+numpy >= 1.18.5
+easydict >= 1.7
+appdirs >= 1.4.4
+tqdm >= 4.64.1
+wget >= 3.2
+```
+  
+## Available Graph Matching Solvers
+This library offers user-friendly API for the following solvers:
+
+* [Two-Graph Matching Solvers](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.classic_solvers.html)
+    * Linear assignment solvers including the differentiable soft 
+      [Sinkhorn algorithm](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.linear_solvers.sinkhorn.html) [1], 
+      and the exact solver [Hungarian](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.linear_solvers.hungarian.html) [2].
+    * Soft and differentiable quadratic assignment solvers, including [spectral graph matching](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.classic_solvers.sm.html) [3] 
+      and [random-walk-based graph matching](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.classic_solvers.rrwm.html) [4].
+    * Discrete (non-differentiable) quadratic assignment solver 
+      [integer projected fixed point method](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.classic_solvers.ipfp.html) [5]. 
+* [Multi-Graph Matching Solvers](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.multi_graph_solvers.html)
+    * [Composition based Affinity Optimization (CAO) solver](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.multi_graph_solvers.cao.html) [6] 
+      by optimizing the affinity score, meanwhile gradually infusing the consistency.
+    * Multi-Graph Matching based on 
+      [Floyd shortest path algorithm](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.multi_graph_solvers.mgm_floyd.html) [7].
+    * [Graduated-assignment based multi-graph matching solver](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.multi_graph_solvers.gamgm.html) [8][9]
+      by graduated annealing of Sinkhorn‚Äôs temperature.
+* [Neural Graph Matching Solvers](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.neural_solvers.html)
+    * Intra-graph and cross-graph embedding based neural graph matching solvers 
+      [PCA-GM](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.neural_solvers.pca_gm.html) 
+      and [IPCA-GM](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.neural_solvers.ipca_gm.html) [10]
+      for matching individual graphs.
+    * [Channel independent embedding (CIE)](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.neural_solvers.cie.html) [11]
+      based neural graph matching solver for matching individual graphs.
+    * [Neural graph matching solver (NGM)](https://pygmtools.readthedocs.io/en/latest/api/_autosummary/pygmtools.neural_solvers.ngm.html) [12]
+      for the general quadratic assignment formulation.
+
+## Available Backends
+This library is designed to support multiple backends with the same set of API. 
+Please follow the official instructions to install your backend.
+
+The following backends are available:
+
+* [Numpy](https://numpy.org/) (**default** backend, CPU only)
+
+<img src="https://pygmtools.readthedocs.io/en/latest/_images/numpy_logo.png" alt="numpy logo" width="200"/>
+
+* [PyTorch](https://pytorch.org/) (GPU friendly, deep learning friendly)
+
+<img src="https://pygmtools.readthedocs.io/en/latest/_images/pytorch_logo.png" alt="pytorch logo" width="200"/>
+
+* [Jittor](https://github.com/Jittor/Jittor) (GPU friendly, JIT support, deep learning friendly)
+
+<img src="https://pygmtools.readthedocs.io/en/latest/_images/jittor_logo.png" alt="jittor logo" width="200"/>
+
+* [PaddlePaddle](https://www.paddlepaddle.org.cn/en) (GPU friendly, deep learning friendly)
+
+<img src="https://pygmtools.readthedocs.io/en/latest/_images/paddle_logo.png" alt="paddle logo" width="200"/>
+
+* [Tensorflow](https://tensorflow.google.cn/) (GPU friendly, deep learning friendly)
+
+<img src="https://pygmtools.readthedocs.io/en/latest/_images/tensorflow_logo.png" alt="tensorflow logo" width="200"/>
+
+### Development status (0.3.8)
+
+|                     | Numpy | PyTorch | Jittor | PaddlePaddle | Tensorflow | MindSpore |
+| ------------------- | ----- | ------- | ------ | ------------ | ---------- | --------- |
+| Linear Solvers      | ‚úî     | ‚úî       | ‚úî      | ‚úî            | ‚úî         | ‚úî        |
+| Classic Solvers     | ‚úî     | ‚úî       | ‚úî      | ‚úî            | ‚úî         | ‚úî        |
+| Multi-Graph Solvers | ‚úî    | ‚úî       | ‚úî      | ‚úî            | üìÜ         | üìÜ        |
+| Neural Solvers      | ‚úî    | ‚úî       | ‚úî      | ‚úî           | üìÜ         | üìÜ        |
+| Examples Gallery    | ‚úî    | ‚úî       | ‚úî      | ‚úî           | üìÜ         | üìÜ        |
+
+‚úî: Supported; üìÜ: Planned for future versions (contributions welcomed!).
+
+For more details, please [read the documentation](https://pygmtools.readthedocs.io/en/latest/guide/get_started.html#install-other-backends).
+
+## Pretrained Models
+
+The library includes several neural network solvers. The pretrained models shall be automatically downloaded upon 
+needed from Google Drive. If you are experiencing issues accessing Google Drive, please download the pretrained models
+manually and put them at ``~/.cache/pygmtools`` (for Linux).
+
+Available at:
+[[google drive]](https://drive.google.com/drive/folders/1O7vkIW8QXBJsNsHUIRiSw91HJ_0FAzu_?usp=sharing)
+[[baidu drive]](https://pan.baidu.com/s/1MvzfM52NJeLWx2JXbbc6HA?pwd=x8bv)
+
+## The Deep Graph Matching Benchmark
+
+``pygmtools`` is also featured with a standard data interface of several graph matching benchmarks. Please read 
+[the corresponding documentation](https://pygmtools.readthedocs.io/en/latest/guide/benchmark.html) for details.
+
+We also maintain a repository containing non-trivial implementation of deep graph matching models, please check out
+[ThinkMatch](https://thinkmatch.readthedocs.io/) if you are interested!
+
+## Chat with the Community
+
+If you have any questions, or if you are experiencing any issues, feel free to raise an issue on GitHub. 
+
+We also offer the following chat rooms if you are more comfortable with them:
+
+* Discord (for English speakers): 
+  
+  [![discord](https://discordapp.com/api/guilds/1028701206526304317/widget.png?style=banner2)](https://discord.gg/8m6n7rRz9T)
+
+* QQ Group (for Chinese speakers)/QQÁæ§(‰∏≠ÊñáÁî®Êà∑): 696401889
+  
+  [![ThinkMatch/pygmtools‰∫§ÊµÅÁæ§](http://pub.idqqimg.com/wpa/images/group.png)](https://qm.qq.com/cgi-bin/qm/qr?k=NlPuwwvaFaHzEWD8w7jSOTzoqSLIM80V&jump_from=webapi&authKey=chI2htrWDujQed6VtVid3V1NXEoJvwz3MVwruax6x5lQIvLsC8BmpmzBJOCzhtQd)
+
+## Contributing
+Any contributions/ideas/suggestions from the community is welcomed! Before starting your contribution, please read the
+[Contributing Guide](https://github.com/Thinklab-SJTU/pygmtools/blob/main/CONTRIBUTING.md).
+
+## Developers and Maintainers
+
+``pygmtools`` is currently developed and maintained by members from [ThinkLab](http://thinklab.sjtu.edu.cn) at 
+Shanghai Jiao Tong University. 
+
+## References
+<!--MLA style references-->
+
+[1] Sinkhorn, Richard, and Paul Knopp. "Concerning nonnegative matrices and doubly stochastic matrices." Pacific Journal of Mathematics 21.2 (1967): 343-348.
+
+[2] Munkres, James. "Algorithms for the assignment and transportation problems." Journal of the society for industrial and applied mathematics 5.1 (1957): 32-38.
+
+[3] Leordeanu, Marius, and Martial Hebert. "A spectral technique for correspondence problems using pairwise constraints." International Conference on Computer Vision (2005).
+
+[4] Cho, Minsu, Jungmin Lee, and Kyoung Mu Lee. "Reweighted random walks for graph matching." European conference on Computer vision. Springer, Berlin, Heidelberg, 2010.
+
+[5] Leordeanu, Marius, Martial Hebert, and Rahul Sukthankar. "An integer projected fixed point method for graph matching and map inference." Advances in neural information processing systems 22 (2009).
+
+[6] Yan, Junchi, et al. "Multi-graph matching via affinity optimization with graduated consistency regularization." IEEE transactions on pattern analysis and machine intelligence 38.6 (2015): 1228-1242.
+
+[7] Jiang, Zetian, Tianzhe Wang, and Junchi Yan. "Unifying offline and online multi-graph matching via finding shortest paths on supergraph." IEEE transactions on pattern analysis and machine intelligence 43.10 (2020): 3648-3663.
+
+[8] Sol√©-Ribalta, Albert, and Francesc Serratosa. "Graduated assignment algorithm for multiple graph matching based on a common labeling." International Journal of Pattern Recognition and Artificial Intelligence 27.01 (2013): 1350001.
+
+[9] Wang, Runzhong, Junchi Yan, and Xiaokang Yang. "Graduated assignment for joint multi-graph matching and clustering with application to unsupervised graph matching network learning." Advances in Neural Information Processing Systems 33 (2020): 19908-19919.
+
+[10] Wang, Runzhong, Junchi Yan, and Xiaokang Yang. "Combinatorial learning of robust deep graph matching: an embedding based approach." IEEE Transactions on Pattern Analysis and Machine Intelligence (2020).
+
+[11] Yu, Tianshu, et al. "Learning deep graph matching with channel-independent embedding and hungarian attention." International conference on learning representations. 2019.
+
+[12] Wang, Runzhong, Junchi Yan, and Xiaokang Yang. "Neural graph matching network: Learning lawler‚Äôs quadratic assignment problem with extension to hypergraph and multiple-graph matching." IEEE Transactions on Pattern Analysis and Machine Intelligence (2021).
```

### Comparing `pygmtools-0.3.8/pygmtools.egg-info/SOURCES.txt` & `pygmtools-0.3.8a0/pygmtools.egg-info/SOURCES.txt`

 * *Files 11% similar despite different names*

```diff
@@ -1,26 +1,28 @@
 LICENSE
 MANIFEST.in
 README.md
 setup.py
 pygmtools/__init__.py
+pygmtools/a_star.cp310-win_amd64.pyd
 pygmtools/benchmark.py
 pygmtools/classic_solvers.py
 pygmtools/dataset.py
 pygmtools/dataset_config.py
 pygmtools/jittor_backend.py
 pygmtools/jittor_modules.py
 pygmtools/linear_solvers.py
 pygmtools/mindspore_backend.py
 pygmtools/multi_graph_solvers.py
 pygmtools/neural_solvers.py
 pygmtools/numpy_backend.py
 pygmtools/numpy_modules.py
 pygmtools/paddle_backend.py
 pygmtools/paddle_modules.py
+pygmtools/pytorch_astar_modules.py
 pygmtools/pytorch_backend.py
 pygmtools/pytorch_modules.py
 pygmtools/tensorflow_backend.py
 pygmtools/tensorflow_modules.py
 pygmtools/utils.py
 pygmtools.egg-info/PKG-INFO
 pygmtools.egg-info/SOURCES.txt
```

### Comparing `pygmtools-0.3.8/tests/test_classic_solvers.py` & `pygmtools-0.3.8a0/tests/test_classic_solvers.py`

 * *Files 1% similar despite different names*

```diff
@@ -342,12 +342,12 @@
         'max_iter': [10],
         'edge_aff_fn': [functools.partial(pygm.utils.gaussian_aff_fn, sigma=1.)],
         'node_aff_fn': [functools.partial(pygm.utils.gaussian_aff_fn, sigma=.1)]
     }, backends)
 
 
 if __name__ == '__main__':
-    test_hungarian('')
-    test_sinkhorn('')
-    test_rrwm('')
-    test_sm('')
-    test_ipfp('')
+    test_hungarian('all')
+    test_sinkhorn('all')
+    test_rrwm('all')
+    test_sm('all')
+    test_ipfp('all')
```

### Comparing `pygmtools-0.3.8/tests/test_dataset.py` & `pygmtools-0.3.8a0/tests/test_dataset.py`

 * *Files identical despite different names*

### Comparing `pygmtools-0.3.8/tests/test_misc.py` & `pygmtools-0.3.8a0/tests/test_misc.py`

 * *Files identical despite different names*

### Comparing `pygmtools-0.3.8/tests/test_multi_graph_solvers.py` & `pygmtools-0.3.8a0/tests/test_multi_graph_solvers.py`

 * *Files identical despite different names*

### Comparing `pygmtools-0.3.8/tests/test_neural_solvers.py` & `pygmtools-0.3.8a0/tests/test_neural_solvers.py`

 * *Files identical despite different names*

### Comparing `pygmtools-0.3.8/tests/test_utils.py` & `pygmtools-0.3.8a0/tests/test_utils.py`

 * *Files identical despite different names*

