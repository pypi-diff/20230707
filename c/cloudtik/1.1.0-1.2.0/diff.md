# Comparing `tmp/cloudtik-1.1.0-cp39-cp39-manylinux2014_x86_64.whl.zip` & `tmp/cloudtik-1.2.0-cp39-cp39-manylinux2014_x86_64.whl.zip`

## zipinfo {}

```diff
@@ -1,734 +1,795 @@
-Zip file size: 6181337 bytes, number of entries: 732
--rw-rw-r--  2.0 unx      140 b- defN 23-Jun-05 07:07 cloudtik/__init__.py
+Zip file size: 6256125 bytes, number of entries: 793
+-rw-rw-r--  2.0 unx      140 b- defN 23-Jul-07 08:48 cloudtik/__init__.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/core/__init__.py
--rw-rw-r--  2.0 unx    31841 b- defN 23-Jun-03 07:38 cloudtik/core/api.py
--rw-rw-r--  2.0 unx     7087 b- defN 23-May-25 11:01 cloudtik/core/command_executor.py
--rw-rw-r--  2.0 unx    57632 b- defN 23-Jun-05 07:07 cloudtik/core/config-schema.json
+-rw-rw-r--  2.0 unx    38133 b- defN 23-Jun-12 06:09 cloudtik/core/api.py
+-rw-rw-r--  2.0 unx     7087 b- defN 23-Jun-07 07:37 cloudtik/core/command_executor.py
+-rw-rw-r--  2.0 unx    59840 b- defN 23-Jul-07 08:48 cloudtik/core/config-schema.json
 -rw-rw-r--  2.0 unx      794 b- defN 23-May-21 03:18 cloudtik/core/job_waiter.py
--rw-rw-r--  2.0 unx    12163 b- defN 23-May-24 02:32 cloudtik/core/node_provider.py
--rw-rw-r--  2.0 unx     6200 b- defN 23-Jun-01 12:01 cloudtik/core/runtime.py
+-rw-rw-r--  2.0 unx    12163 b- defN 23-Jun-07 07:37 cloudtik/core/node_provider.py
+-rw-rw-r--  2.0 unx     6200 b- defN 23-Jun-07 07:37 cloudtik/core/runtime.py
 -rw-rw-r--  2.0 unx     2054 b- defN 23-May-21 03:18 cloudtik/core/scaling_policy.py
--rw-rw-r--  2.0 unx     2175 b- defN 23-May-29 06:54 cloudtik/core/tags.py
--rw-rw-r--  2.0 unx    14561 b- defN 23-Jun-05 07:07 cloudtik/core/workspace-schema.json
--rw-rw-r--  2.0 unx     3984 b- defN 23-May-21 03:18 cloudtik/core/workspace_provider.py
+-rw-rw-r--  2.0 unx     2175 b- defN 23-Jun-07 07:37 cloudtik/core/tags.py
+-rw-rw-r--  2.0 unx    14561 b- defN 23-Jul-07 08:48 cloudtik/core/workspace-schema.json
+-rw-rw-r--  2.0 unx     3984 b- defN 23-Jun-07 07:37 cloudtik/core/workspace_provider.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/core/_private/__init__.py
 -rw-rw-r--  2.0 unx     3244 b- defN 23-May-21 03:18 cloudtik/core/_private/call_context.py
--rw-rw-r--  2.0 unx    26421 b- defN 23-May-27 09:00 cloudtik/core/_private/cli_logger.py
+-rw-rw-r--  2.0 unx    26512 b- defN 23-Jun-10 11:53 cloudtik/core/_private/cli_logger.py
 -rw-rw-r--  2.0 unx      527 b- defN 23-May-21 03:18 cloudtik/core/_private/concurrent_cache.py
--rw-rw-r--  2.0 unx    10524 b- defN 23-Jun-03 07:38 cloudtik/core/_private/constants.py
--rw-rw-r--  2.0 unx    27882 b- defN 23-Jun-05 05:40 cloudtik/core/_private/core_utils.py
+-rw-rw-r--  2.0 unx    10524 b- defN 23-Jun-07 07:37 cloudtik/core/_private/constants.py
+-rw-rw-r--  2.0 unx    27882 b- defN 23-Jun-07 07:37 cloudtik/core/_private/core_utils.py
 -rw-rw-r--  2.0 unx      928 b- defN 23-May-21 03:18 cloudtik/core/_private/crypto.py
 -rw-rw-r--  2.0 unx      996 b- defN 23-May-21 03:18 cloudtik/core/_private/debug.py
--rw-rw-r--  2.0 unx     6686 b- defN 23-May-30 05:26 cloudtik/core/_private/docker.py
+-rw-rw-r--  2.0 unx     7705 b- defN 23-Jul-03 02:01 cloudtik/core/_private/docker.py
 -rw-rw-r--  2.0 unx     5765 b- defN 23-May-21 03:18 cloudtik/core/_private/event_system.py
 -rw-rw-r--  2.0 unx      778 b- defN 23-May-21 03:18 cloudtik/core/_private/log_timer.py
 -rw-rw-r--  2.0 unx     7351 b- defN 23-May-21 03:18 cloudtik/core/_private/logging_utils.py
--rw-rw-r--  2.0 unx     9821 b- defN 23-Jun-03 07:38 cloudtik/core/_private/parameter.py
+-rw-rw-r--  2.0 unx     9821 b- defN 23-Jun-07 07:37 cloudtik/core/_private/parameter.py
 -rw-rw-r--  2.0 unx     9586 b- defN 23-May-21 03:18 cloudtik/core/_private/prometheus_metrics.py
--rw-rw-r--  2.0 unx    12169 b- defN 23-May-31 06:06 cloudtik/core/_private/providers.py
--rw-rw-r--  2.0 unx    10420 b- defN 23-May-27 06:35 cloudtik/core/_private/resource_spec.py
--rw-rw-r--  2.0 unx     4856 b- defN 23-Jun-01 12:01 cloudtik/core/_private/runtime_factory.py
--rw-rw-r--  2.0 unx    52664 b- defN 23-Jun-03 07:38 cloudtik/core/_private/services.py
--rw-rw-r--  2.0 unx    15709 b- defN 23-May-29 01:12 cloudtik/core/_private/subprocess_output_util.py
--rw-rw-r--  2.0 unx   114241 b- defN 23-Jun-03 09:02 cloudtik/core/_private/utils.py
+-rw-rw-r--  2.0 unx    12169 b- defN 23-Jun-07 07:37 cloudtik/core/_private/providers.py
+-rw-rw-r--  2.0 unx    10420 b- defN 23-Jun-07 07:37 cloudtik/core/_private/resource_spec.py
+-rw-rw-r--  2.0 unx     4856 b- defN 23-Jun-07 07:37 cloudtik/core/_private/runtime_factory.py
+-rw-rw-r--  2.0 unx     1675 b- defN 23-Jun-12 06:09 cloudtik/core/_private/script_registry.py
+-rw-rw-r--  2.0 unx    52664 b- defN 23-Jun-07 07:37 cloudtik/core/_private/services.py
+-rw-rw-r--  2.0 unx    15709 b- defN 23-Jun-07 07:37 cloudtik/core/_private/subprocess_output_util.py
+-rw-rw-r--  2.0 unx   115899 b- defN 23-Jun-12 08:13 cloudtik/core/_private/utils.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/core/_private/cluster/__init__.py
--rw-rw-r--  2.0 unx     5681 b- defN 23-Jun-01 12:01 cloudtik/core/_private/cluster/cluster_config.py
--rw-rw-r--  2.0 unx    23389 b- defN 23-Jun-02 10:25 cloudtik/core/_private/cluster/cluster_dump.py
--rw-rw-r--  2.0 unx     4451 b- defN 23-May-29 06:54 cloudtik/core/_private/cluster/cluster_exec.py
--rw-rw-r--  2.0 unx    16339 b- defN 23-May-21 03:18 cloudtik/core/_private/cluster/cluster_metrics.py
--rw-rw-r--  2.0 unx     6061 b- defN 23-May-21 03:18 cloudtik/core/_private/cluster/cluster_metrics_updater.py
--rw-rw-r--  2.0 unx   157481 b- defN 23-Jun-03 09:02 cloudtik/core/_private/cluster/cluster_operator.py
--rw-rw-r--  2.0 unx    72464 b- defN 23-May-27 09:00 cloudtik/core/_private/cluster/cluster_scaler.py
--rw-rw-r--  2.0 unx     4432 b- defN 23-May-30 05:26 cloudtik/core/_private/cluster/cluster_tunnel_request.py
+-rw-rw-r--  2.0 unx     5681 b- defN 23-Jun-07 07:37 cloudtik/core/_private/cluster/cluster_config.py
+-rw-rw-r--  2.0 unx    23994 b- defN 23-Jun-12 06:09 cloudtik/core/_private/cluster/cluster_dump.py
+-rw-rw-r--  2.0 unx     4405 b- defN 23-Jun-11 07:05 cloudtik/core/_private/cluster/cluster_exec.py
+-rw-rw-r--  2.0 unx    16339 b- defN 23-Jun-07 07:37 cloudtik/core/_private/cluster/cluster_metrics.py
+-rw-rw-r--  2.0 unx     6061 b- defN 23-Jun-07 07:37 cloudtik/core/_private/cluster/cluster_metrics_updater.py
+-rw-rw-r--  2.0 unx   161232 b- defN 23-Jun-12 06:09 cloudtik/core/_private/cluster/cluster_operator.py
+-rw-rw-r--  2.0 unx    72464 b- defN 23-Jun-11 02:32 cloudtik/core/_private/cluster/cluster_scaler.py
+-rw-rw-r--  2.0 unx     4432 b- defN 23-Jun-07 07:37 cloudtik/core/_private/cluster/cluster_tunnel_request.py
 -rw-rw-r--  2.0 unx     3399 b- defN 23-May-21 03:18 cloudtik/core/_private/cluster/cluster_utils.py
 -rw-rw-r--  2.0 unx     2899 b- defN 23-May-21 03:18 cloudtik/core/_private/cluster/event_summarizer.py
 -rw-rw-r--  2.0 unx     4904 b- defN 23-May-21 03:18 cloudtik/core/_private/cluster/node_launcher.py
 -rw-rw-r--  2.0 unx     2799 b- defN 23-May-21 03:18 cloudtik/core/_private/cluster/node_tracker.py
--rw-rw-r--  2.0 unx    30707 b- defN 23-Jun-03 07:38 cloudtik/core/_private/cluster/resource_demand_scheduler.py
--rw-rw-r--  2.0 unx     2832 b- defN 23-May-21 03:18 cloudtik/core/_private/cluster/resource_scaling_policy.py
--rw-rw-r--  2.0 unx    23347 b- defN 23-May-21 03:18 cloudtik/core/_private/cluster/scaling_policies.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/core/_private/command_executor/__init__.py
--rw-rw-r--  2.0 unx     3099 b- defN 23-May-21 03:18 cloudtik/core/_private/command_executor/command_executor.py
--rw-rw-r--  2.0 unx    22949 b- defN 23-May-30 05:26 cloudtik/core/_private/command_executor/docker_command_executor.py
--rw-rw-r--  2.0 unx     8206 b- defN 23-May-25 03:58 cloudtik/core/_private/command_executor/host_command_executor.py
--rw-rw-r--  2.0 unx     9259 b- defN 23-May-21 03:18 cloudtik/core/_private/command_executor/kubernetes_command_executor.py
--rw-rw-r--  2.0 unx     5744 b- defN 23-May-25 11:01 cloudtik/core/_private/command_executor/local_command_executor.py
--rw-rw-r--  2.0 unx    10252 b- defN 23-May-29 01:50 cloudtik/core/_private/command_executor/ssh_command_executor.py
+-rw-rw-r--  2.0 unx    30707 b- defN 23-Jun-07 07:37 cloudtik/core/_private/cluster/resource_demand_scheduler.py
+-rw-rw-r--  2.0 unx     2832 b- defN 23-Jun-07 07:37 cloudtik/core/_private/cluster/resource_scaling_policy.py
+-rw-rw-r--  2.0 unx    23347 b- defN 23-Jun-07 07:37 cloudtik/core/_private/cluster/scaling_policies.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-07 07:37 cloudtik/core/_private/command_executor/__init__.py
+-rw-rw-r--  2.0 unx     3099 b- defN 23-Jun-07 07:37 cloudtik/core/_private/command_executor/command_executor.py
+-rw-rw-r--  2.0 unx    22864 b- defN 23-Jul-03 02:01 cloudtik/core/_private/command_executor/docker_command_executor.py
+-rw-rw-r--  2.0 unx     8206 b- defN 23-Jun-07 07:37 cloudtik/core/_private/command_executor/host_command_executor.py
+-rw-rw-r--  2.0 unx     9259 b- defN 23-Jun-07 07:37 cloudtik/core/_private/command_executor/kubernetes_command_executor.py
+-rw-rw-r--  2.0 unx     5744 b- defN 23-Jun-07 07:37 cloudtik/core/_private/command_executor/local_command_executor.py
+-rw-rw-r--  2.0 unx    10252 b- defN 23-Jun-07 07:37 cloudtik/core/_private/command_executor/ssh_command_executor.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/core/_private/job_waiter/__init__.py
 -rw-rw-r--  2.0 unx      681 b- defN 23-May-21 03:18 cloudtik/core/_private/job_waiter/job_waiter_chain.py
 -rw-rw-r--  2.0 unx     4667 b- defN 23-May-21 03:18 cloudtik/core/_private/job_waiter/job_waiter_factory.py
--rw-rw-r--  2.0 unx      279 b- defN 23-Jun-05 07:07 cloudtik/core/_private/job_waiter/screen-session.sh
+-rw-rw-r--  2.0 unx      279 b- defN 23-Jul-07 08:48 cloudtik/core/_private/job_waiter/screen-session.sh
 -rw-rw-r--  2.0 unx     3448 b- defN 23-May-21 03:18 cloudtik/core/_private/job_waiter/session_job_waiter.py
--rw-rw-r--  2.0 unx      275 b- defN 23-Jun-05 07:07 cloudtik/core/_private/job_waiter/tmux-session.sh
+-rw-rw-r--  2.0 unx      275 b- defN 23-Jul-07 08:48 cloudtik/core/_private/job_waiter/tmux-session.sh
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/core/_private/metrics/__init__.py
--rw-rw-r--  2.0 unx     4039 b- defN 23-May-27 06:35 cloudtik/core/_private/metrics/k8s_utils.py
--rw-rw-r--  2.0 unx     6477 b- defN 23-May-21 03:18 cloudtik/core/_private/metrics/metrics_collector.py
+-rw-rw-r--  2.0 unx     4039 b- defN 23-Jun-07 07:37 cloudtik/core/_private/metrics/k8s_utils.py
+-rw-rw-r--  2.0 unx     6477 b- defN 23-Jun-07 07:37 cloudtik/core/_private/metrics/metrics_collector.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/core/_private/node/__init__.py
--rw-rw-r--  2.0 unx    35199 b- defN 23-May-29 01:12 cloudtik/core/_private/node/node_services.py
--rw-rw-r--  2.0 unx    30439 b- defN 23-May-26 13:08 cloudtik/core/_private/node/node_updater.py
+-rw-rw-r--  2.0 unx    35199 b- defN 23-Jun-07 07:37 cloudtik/core/_private/node/node_services.py
+-rw-rw-r--  2.0 unx    30439 b- defN 23-Jun-07 07:37 cloudtik/core/_private/node/node_updater.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/core/_private/service/__init__.py
 -rw-rw-r--  2.0 unx    12506 b- defN 23-May-21 03:18 cloudtik/core/_private/service/cloudtik_cluster_controller.py
--rw-rw-r--  2.0 unx    17125 b- defN 23-May-21 03:18 cloudtik/core/_private/service/cloudtik_log_monitor_service.py
--rw-rw-r--  2.0 unx    10873 b- defN 23-Jun-03 07:38 cloudtik/core/_private/service/cloudtik_node_monitor_service.py
+-rw-rw-r--  2.0 unx    17125 b- defN 23-Jun-07 07:37 cloudtik/core/_private/service/cloudtik_log_monitor_service.py
+-rw-rw-r--  2.0 unx    10873 b- defN 23-Jun-07 07:37 cloudtik/core/_private/service/cloudtik_node_monitor_service.py
 -rw-rw-r--  2.0 unx     2062 b- defN 23-May-21 03:18 cloudtik/core/_private/service/cloudtik_process_reaper.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/core/_private/state/__init__.py
 -rw-rw-r--  2.0 unx     8234 b- defN 23-May-21 03:18 cloudtik/core/_private/state/control_state.py
--rw-rw-r--  2.0 unx     3987 b- defN 23-May-27 06:35 cloudtik/core/_private/state/file_state_store.py
+-rw-rw-r--  2.0 unx     3987 b- defN 23-Jun-07 07:37 cloudtik/core/_private/state/file_state_store.py
 -rw-rw-r--  2.0 unx     3006 b- defN 23-May-21 03:18 cloudtik/core/_private/state/kv_store.py
 -rw-rw-r--  2.0 unx     4265 b- defN 23-May-21 03:18 cloudtik/core/_private/state/redis_shards_client.py
 -rw-rw-r--  2.0 unx     2358 b- defN 23-May-21 03:18 cloudtik/core/_private/state/redis_shards_scanner.py
--rw-rw-r--  2.0 unx     4872 b- defN 23-May-21 03:18 cloudtik/core/_private/state/scaling_state.py
+-rw-rw-r--  2.0 unx     4872 b- defN 23-Jun-07 07:37 cloudtik/core/_private/state/scaling_state.py
 -rw-rw-r--  2.0 unx      707 b- defN 23-May-21 03:18 cloudtik/core/_private/state/state_node_manager.py
 -rw-rw-r--  2.0 unx     1642 b- defN 23-May-21 03:18 cloudtik/core/_private/state/state_table_store.py
 -rw-rw-r--  2.0 unx     1256 b- defN 23-May-21 03:18 cloudtik/core/_private/state/store_client.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/core/_private/workspace/__init__.py
--rw-rw-r--  2.0 unx    20017 b- defN 23-May-29 01:12 cloudtik/core/_private/workspace/workspace_operator.py
--rwxrwxr-x  2.0 unx 15621376 b- defN 23-Jun-05 07:07 cloudtik/core/thirdparty/redis/cloudtik-redis-server
+-rw-rw-r--  2.0 unx    20017 b- defN 23-Jun-07 07:37 cloudtik/core/_private/workspace/workspace_operator.py
+-rwxrwxr-x  2.0 unx 15621376 b- defN 23-Jul-07 08:48 cloudtik/core/thirdparty/redis/cloudtik-redis-server
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/providers/__init__.py
--rw-rw-r--  2.0 unx     4149 b- defN 23-Jun-05 07:07 cloudtik/providers/commands.yaml
--rw-rw-r--  2.0 unx     2100 b- defN 23-Jun-05 07:07 cloudtik/providers/defaults.yaml
--rw-rw-r--  2.0 unx       65 b- defN 23-Jun-05 07:07 cloudtik/providers/workspace-defaults.yaml
+-rw-rw-r--  2.0 unx     4149 b- defN 23-Jul-07 08:48 cloudtik/providers/commands.yaml
+-rw-rw-r--  2.0 unx     2100 b- defN 23-Jul-07 08:48 cloudtik/providers/defaults.yaml
+-rw-rw-r--  2.0 unx       65 b- defN 23-Jul-07 08:48 cloudtik/providers/workspace-defaults.yaml
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/providers/_private/__init__.py
 -rw-rw-r--  2.0 unx      663 b- defN 23-May-21 03:18 cloudtik/providers/_private/utils.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/providers/_private/_azure/__init__.py
--rw-rw-r--  2.0 unx    12272 b- defN 23-Jun-05 07:07 cloudtik/providers/_private/_azure/azure-vm-template.json
+-rw-rw-r--  2.0 unx    12272 b- defN 23-Jul-07 08:48 cloudtik/providers/_private/_azure/azure-vm-template.json
 -rw-rw-r--  2.0 unx     2122 b- defN 23-May-21 03:18 cloudtik/providers/_private/_azure/azure_identity_credential_adapter.py
--rw-rw-r--  2.0 unx   137248 b- defN 23-May-24 02:32 cloudtik/providers/_private/_azure/config.py
--rw-rw-r--  2.0 unx    16134 b- defN 23-May-21 03:18 cloudtik/providers/_private/_azure/node_provider.py
--rw-rw-r--  2.0 unx    11895 b- defN 23-May-21 03:18 cloudtik/providers/_private/_azure/utils.py
--rw-rw-r--  2.0 unx     3820 b- defN 23-May-21 03:18 cloudtik/providers/_private/_azure/workspace_provider.py
+-rw-rw-r--  2.0 unx   137248 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/_azure/config.py
+-rw-rw-r--  2.0 unx    16134 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/_azure/node_provider.py
+-rw-rw-r--  2.0 unx    11895 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/_azure/utils.py
+-rw-rw-r--  2.0 unx     3820 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/_azure/workspace_provider.py
 -rw-rw-r--  2.0 unx     1185 b- defN 23-May-21 03:18 cloudtik/providers/_private/_kubernetes/__init__.py
--rw-rw-r--  2.0 unx    68215 b- defN 23-Jun-02 10:25 cloudtik/providers/_private/_kubernetes/config.py
--rwxrwxr-x  2.0 unx      760 b- defN 23-Jun-05 07:07 cloudtik/providers/_private/_kubernetes/kubectl-rsync.sh
--rw-rw-r--  2.0 unx    12174 b- defN 23-May-21 03:18 cloudtik/providers/_private/_kubernetes/node_provider.py
--rw-rw-r--  2.0 unx     7673 b- defN 23-Jun-02 10:25 cloudtik/providers/_private/_kubernetes/utils.py
--rw-rw-r--  2.0 unx     2568 b- defN 23-May-21 03:18 cloudtik/providers/_private/_kubernetes/workspace_provider.py
+-rw-rw-r--  2.0 unx    68215 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/_kubernetes/config.py
+-rwxrwxr-x  2.0 unx      760 b- defN 23-Jul-07 08:48 cloudtik/providers/_private/_kubernetes/kubectl-rsync.sh
+-rw-rw-r--  2.0 unx    12174 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/_kubernetes/node_provider.py
+-rw-rw-r--  2.0 unx     7673 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/_kubernetes/utils.py
+-rw-rw-r--  2.0 unx     2568 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/_kubernetes/workspace_provider.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/providers/_private/_kubernetes/aws_eks/__init__.py
--rw-rw-r--  2.0 unx    32404 b- defN 23-May-21 03:18 cloudtik/providers/_private/_kubernetes/aws_eks/config.py
+-rw-rw-r--  2.0 unx    32404 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/_kubernetes/aws_eks/config.py
 -rw-rw-r--  2.0 unx     2163 b- defN 23-May-21 03:18 cloudtik/providers/_private/_kubernetes/aws_eks/utils.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/providers/_private/_kubernetes/azure_aks/__init__.py
--rw-rw-r--  2.0 unx    51932 b- defN 23-May-21 03:18 cloudtik/providers/_private/_kubernetes/azure_aks/config.py
+-rw-rw-r--  2.0 unx    51932 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/_kubernetes/azure_aks/config.py
 -rw-rw-r--  2.0 unx     3794 b- defN 23-May-21 03:18 cloudtik/providers/_private/_kubernetes/azure_aks/utils.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/providers/_private/_kubernetes/gcp_gke/__init__.py
--rw-rw-r--  2.0 unx    37680 b- defN 23-May-21 03:18 cloudtik/providers/_private/_kubernetes/gcp_gke/config.py
--rw-rw-r--  2.0 unx     1813 b- defN 23-May-21 03:18 cloudtik/providers/_private/_kubernetes/gcp_gke/utils.py
+-rw-rw-r--  2.0 unx    37680 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/_kubernetes/gcp_gke/config.py
+-rw-rw-r--  2.0 unx     1813 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/_kubernetes/gcp_gke/utils.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/providers/_private/aliyun/__init__.py
--rw-rw-r--  2.0 unx    88596 b- defN 23-May-24 02:32 cloudtik/providers/_private/aliyun/config.py
+-rw-rw-r--  2.0 unx    88596 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/aliyun/config.py
 -rw-rw-r--  2.0 unx    20231 b- defN 23-May-21 03:18 cloudtik/providers/_private/aliyun/node_provider.py
--rw-rw-r--  2.0 unx    54733 b- defN 23-May-21 03:18 cloudtik/providers/_private/aliyun/utils.py
--rw-rw-r--  2.0 unx     4225 b- defN 23-May-21 03:18 cloudtik/providers/_private/aliyun/workspace_provider.py
+-rw-rw-r--  2.0 unx    54733 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/aliyun/utils.py
+-rw-rw-r--  2.0 unx     4225 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/aliyun/workspace_provider.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/providers/_private/aws/__init__.py
--rw-rw-r--  2.0 unx   132443 b- defN 23-May-24 02:32 cloudtik/providers/_private/aws/config.py
--rw-rw-r--  2.0 unx    23041 b- defN 23-May-24 02:32 cloudtik/providers/_private/aws/node_provider.py
--rw-rw-r--  2.0 unx    12288 b- defN 23-May-21 03:18 cloudtik/providers/_private/aws/utils.py
--rw-rw-r--  2.0 unx     3825 b- defN 23-May-21 03:18 cloudtik/providers/_private/aws/workspace_provider.py
+-rw-rw-r--  2.0 unx   132443 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/aws/config.py
+-rw-rw-r--  2.0 unx    23041 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/aws/node_provider.py
+-rw-rw-r--  2.0 unx    12288 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/aws/utils.py
+-rw-rw-r--  2.0 unx     3825 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/aws/workspace_provider.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/providers/_private/gcp/__init__.py
--rw-rw-r--  2.0 unx   110331 b- defN 23-May-24 02:32 cloudtik/providers/_private/gcp/config.py
+-rw-rw-r--  2.0 unx   110331 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/gcp/config.py
 -rw-rw-r--  2.0 unx    21748 b- defN 23-May-21 03:18 cloudtik/providers/_private/gcp/node.py
--rw-rw-r--  2.0 unx    10246 b- defN 23-May-28 11:25 cloudtik/providers/_private/gcp/node_provider.py
--rw-rw-r--  2.0 unx    16323 b- defN 23-May-21 03:18 cloudtik/providers/_private/gcp/utils.py
--rw-rw-r--  2.0 unx     4368 b- defN 23-May-21 03:18 cloudtik/providers/_private/gcp/workspace_provider.py
+-rw-rw-r--  2.0 unx    10246 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/gcp/node_provider.py
+-rw-rw-r--  2.0 unx    16323 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/gcp/utils.py
+-rw-rw-r--  2.0 unx     4368 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/gcp/workspace_provider.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/providers/_private/huaweicloud/__init__.py
--rw-rw-r--  2.0 unx    73630 b- defN 23-May-24 02:32 cloudtik/providers/_private/huaweicloud/config.py
--rw-rw-r--  2.0 unx    14431 b- defN 23-May-21 03:18 cloudtik/providers/_private/huaweicloud/node_provider.py
--rw-rw-r--  2.0 unx    10750 b- defN 23-May-21 03:18 cloudtik/providers/_private/huaweicloud/utils.py
--rw-rw-r--  2.0 unx     4431 b- defN 23-May-21 03:18 cloudtik/providers/_private/huaweicloud/workspace_provider.py
+-rw-rw-r--  2.0 unx    73630 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/huaweicloud/config.py
+-rw-rw-r--  2.0 unx    14431 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/huaweicloud/node_provider.py
+-rw-rw-r--  2.0 unx    10750 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/huaweicloud/utils.py
+-rw-rw-r--  2.0 unx     4431 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/huaweicloud/workspace_provider.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/providers/_private/local/__init__.py
--rw-rw-r--  2.0 unx     9159 b- defN 23-Jun-02 10:25 cloudtik/providers/_private/local/config.py
--rw-rw-r--  2.0 unx     1237 b- defN 23-May-26 09:41 cloudtik/providers/_private/local/local_docker_command_executor.py
--rw-rw-r--  2.0 unx    11574 b- defN 23-May-29 01:12 cloudtik/providers/_private/local/local_scheduler.py
--rw-rw-r--  2.0 unx     4441 b- defN 23-May-26 09:41 cloudtik/providers/_private/local/node_provider.py
--rw-rw-r--  2.0 unx     1770 b- defN 23-May-27 02:22 cloudtik/providers/_private/local/state_store.py
--rw-rw-r--  2.0 unx      434 b- defN 23-Jun-05 05:24 cloudtik/providers/_private/local/utils.py
--rw-rw-r--  2.0 unx     4394 b- defN 23-May-26 09:41 cloudtik/providers/_private/local/workspace_config.py
--rw-rw-r--  2.0 unx     3833 b- defN 23-May-26 01:38 cloudtik/providers/_private/local/workspace_provider.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-26 01:38 cloudtik/providers/_private/onpremise/__init__.py
--rw-rw-r--  2.0 unx     5934 b- defN 23-May-30 05:26 cloudtik/providers/_private/onpremise/cloud_simulator_scheduler.py
--rw-rw-r--  2.0 unx     8255 b- defN 23-Jun-02 10:25 cloudtik/providers/_private/onpremise/config.py
--rw-rw-r--  2.0 unx     6016 b- defN 23-May-29 01:12 cloudtik/providers/_private/onpremise/node_provider.py
--rw-rw-r--  2.0 unx     7026 b- defN 23-May-29 01:12 cloudtik/providers/_private/onpremise/state_store.py
--rw-rw-r--  2.0 unx     5993 b- defN 23-May-28 11:25 cloudtik/providers/_private/onpremise/workspace_config.py
--rw-rw-r--  2.0 unx     3385 b- defN 23-May-28 11:25 cloudtik/providers/_private/onpremise/workspace_provider.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-26 01:38 cloudtik/providers/_private/virtual/__init__.py
--rw-rw-r--  2.0 unx    14869 b- defN 23-Jun-02 10:25 cloudtik/providers/_private/virtual/config.py
--rw-rw-r--  2.0 unx     5860 b- defN 23-Jun-01 12:01 cloudtik/providers/_private/virtual/node_provider.py
--rw-rw-r--  2.0 unx     3287 b- defN 23-Jun-05 07:07 cloudtik/providers/_private/virtual/sshd_config
--rw-rw-r--  2.0 unx     1193 b- defN 23-Jun-05 05:24 cloudtik/providers/_private/virtual/utils.py
--rw-rw-r--  2.0 unx    24469 b- defN 23-Jun-05 05:40 cloudtik/providers/_private/virtual/virtual_container_scheduler.py
--rw-rw-r--  2.0 unx     1777 b- defN 23-May-26 01:38 cloudtik/providers/_private/virtual/virtual_docker_command_executor.py
--rw-rw-r--  2.0 unx    16327 b- defN 23-Jun-02 10:25 cloudtik/providers/_private/virtual/workspace_config.py
--rw-rw-r--  2.0 unx     4567 b- defN 23-Jun-01 12:01 cloudtik/providers/_private/virtual/workspace_provider.py
+-rw-rw-r--  2.0 unx     9159 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/local/config.py
+-rw-rw-r--  2.0 unx     1237 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/local/local_docker_command_executor.py
+-rw-rw-r--  2.0 unx    11574 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/local/local_scheduler.py
+-rw-rw-r--  2.0 unx     4441 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/local/node_provider.py
+-rw-rw-r--  2.0 unx     1770 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/local/state_store.py
+-rw-rw-r--  2.0 unx      434 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/local/utils.py
+-rw-rw-r--  2.0 unx     4394 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/local/workspace_config.py
+-rw-rw-r--  2.0 unx     3833 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/local/workspace_provider.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/onpremise/__init__.py
+-rw-rw-r--  2.0 unx     5934 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/onpremise/cloud_simulator_scheduler.py
+-rw-rw-r--  2.0 unx     8255 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/onpremise/config.py
+-rw-rw-r--  2.0 unx     6016 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/onpremise/node_provider.py
+-rw-rw-r--  2.0 unx     7026 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/onpremise/state_store.py
+-rw-rw-r--  2.0 unx     5993 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/onpremise/workspace_config.py
+-rw-rw-r--  2.0 unx     3385 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/onpremise/workspace_provider.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/virtual/__init__.py
+-rw-rw-r--  2.0 unx    14869 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/virtual/config.py
+-rw-rw-r--  2.0 unx     5860 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/virtual/node_provider.py
+-rw-rw-r--  2.0 unx     3287 b- defN 23-Jul-07 08:48 cloudtik/providers/_private/virtual/sshd_config
+-rw-rw-r--  2.0 unx     1193 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/virtual/utils.py
+-rw-rw-r--  2.0 unx    24515 b- defN 23-Jul-03 02:01 cloudtik/providers/_private/virtual/virtual_container_scheduler.py
+-rw-rw-r--  2.0 unx     1777 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/virtual/virtual_docker_command_executor.py
+-rw-rw-r--  2.0 unx    16327 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/virtual/workspace_config.py
+-rw-rw-r--  2.0 unx     4567 b- defN 23-Jun-07 07:37 cloudtik/providers/_private/virtual/workspace_provider.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/providers/aliyun/__init__.py
--rw-rw-r--  2.0 unx       54 b- defN 23-Jun-05 07:07 cloudtik/providers/aliyun/commands.yaml
--rw-rw-r--  2.0 unx     4472 b- defN 23-Jun-05 07:07 cloudtik/providers/aliyun/defaults.yaml
--rw-rw-r--  2.0 unx      417 b- defN 23-Jun-05 07:07 cloudtik/providers/aliyun/workspace-defaults.yaml
+-rw-rw-r--  2.0 unx       54 b- defN 23-Jul-07 08:48 cloudtik/providers/aliyun/commands.yaml
+-rw-rw-r--  2.0 unx     4472 b- defN 23-Jul-07 08:48 cloudtik/providers/aliyun/defaults.yaml
+-rw-rw-r--  2.0 unx      417 b- defN 23-Jul-07 08:48 cloudtik/providers/aliyun/workspace-defaults.yaml
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/providers/aws/__init__.py
--rw-rw-r--  2.0 unx       54 b- defN 23-Jun-05 07:07 cloudtik/providers/aws/commands.yaml
--rw-rw-r--  2.0 unx     3340 b- defN 23-Jun-05 07:07 cloudtik/providers/aws/defaults.yaml
--rw-rw-r--  2.0 unx      412 b- defN 23-Jun-05 07:07 cloudtik/providers/aws/workspace-defaults.yaml
+-rw-rw-r--  2.0 unx       54 b- defN 23-Jul-07 08:48 cloudtik/providers/aws/commands.yaml
+-rw-rw-r--  2.0 unx     3340 b- defN 23-Jul-07 08:48 cloudtik/providers/aws/defaults.yaml
+-rw-rw-r--  2.0 unx      412 b- defN 23-Jul-07 08:48 cloudtik/providers/aws/workspace-defaults.yaml
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/providers/azure/__init__.py
--rw-rw-r--  2.0 unx       54 b- defN 23-Jun-05 07:07 cloudtik/providers/azure/commands.yaml
--rw-rw-r--  2.0 unx     3028 b- defN 23-Jun-05 07:07 cloudtik/providers/azure/defaults.yaml
--rw-rw-r--  2.0 unx      619 b- defN 23-Jun-05 07:07 cloudtik/providers/azure/workspace-defaults.yaml
+-rw-rw-r--  2.0 unx       54 b- defN 23-Jul-07 08:48 cloudtik/providers/azure/commands.yaml
+-rw-rw-r--  2.0 unx     3028 b- defN 23-Jul-07 08:48 cloudtik/providers/azure/defaults.yaml
+-rw-rw-r--  2.0 unx      619 b- defN 23-Jul-07 08:48 cloudtik/providers/azure/workspace-defaults.yaml
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/providers/gcp/__init__.py
--rw-rw-r--  2.0 unx      464 b- defN 23-Jun-05 07:07 cloudtik/providers/gcp/commands.yaml
--rw-rw-r--  2.0 unx     3982 b- defN 23-Jun-05 07:07 cloudtik/providers/gcp/defaults.yaml
--rw-rw-r--  2.0 unx      499 b- defN 23-Jun-05 07:07 cloudtik/providers/gcp/workspace-defaults.yaml
+-rw-rw-r--  2.0 unx      464 b- defN 23-Jul-07 08:48 cloudtik/providers/gcp/commands.yaml
+-rw-rw-r--  2.0 unx     3982 b- defN 23-Jul-07 08:48 cloudtik/providers/gcp/defaults.yaml
+-rw-rw-r--  2.0 unx      499 b- defN 23-Jul-07 08:48 cloudtik/providers/gcp/workspace-defaults.yaml
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/providers/huaweicloud/__init__.py
--rw-rw-r--  2.0 unx       54 b- defN 23-Jun-05 07:07 cloudtik/providers/huaweicloud/commands.yaml
--rw-rw-r--  2.0 unx     2166 b- defN 23-Jun-05 07:07 cloudtik/providers/huaweicloud/defaults.yaml
--rw-rw-r--  2.0 unx      431 b- defN 23-Jun-05 07:07 cloudtik/providers/huaweicloud/workspace-defaults.yaml
+-rw-rw-r--  2.0 unx       54 b- defN 23-Jul-07 08:48 cloudtik/providers/huaweicloud/commands.yaml
+-rw-rw-r--  2.0 unx     2166 b- defN 23-Jul-07 08:48 cloudtik/providers/huaweicloud/defaults.yaml
+-rw-rw-r--  2.0 unx      431 b- defN 23-Jul-07 08:48 cloudtik/providers/huaweicloud/workspace-defaults.yaml
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/providers/kubernetes/__init__.py
--rw-rw-r--  2.0 unx      265 b- defN 23-Jun-05 07:07 cloudtik/providers/kubernetes/commands.yaml
--rw-rw-r--  2.0 unx     6896 b- defN 23-Jun-05 07:07 cloudtik/providers/kubernetes/defaults.yaml
--rw-rw-r--  2.0 unx     2154 b- defN 23-Jun-05 07:07 cloudtik/providers/kubernetes/workspace-defaults.yaml
+-rw-rw-r--  2.0 unx      265 b- defN 23-Jul-07 08:48 cloudtik/providers/kubernetes/commands.yaml
+-rw-rw-r--  2.0 unx     6896 b- defN 23-Jul-07 08:48 cloudtik/providers/kubernetes/defaults.yaml
+-rw-rw-r--  2.0 unx     2154 b- defN 23-Jul-07 08:48 cloudtik/providers/kubernetes/workspace-defaults.yaml
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/providers/kubernetes/cloudtik_operator/__init__.py
 -rw-rw-r--  2.0 unx    12787 b- defN 23-May-21 03:18 cloudtik/providers/kubernetes/cloudtik_operator/operator.py
 -rw-rw-r--  2.0 unx    14228 b- defN 23-May-21 03:18 cloudtik/providers/kubernetes/cloudtik_operator/operator_utils.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/providers/local/__init__.py
--rw-rw-r--  2.0 unx       54 b- defN 23-Jun-05 07:07 cloudtik/providers/local/commands.yaml
--rw-rw-r--  2.0 unx      761 b- defN 23-Jun-05 07:07 cloudtik/providers/local/defaults.yaml
--rw-rw-r--  2.0 unx      142 b- defN 23-Jun-05 07:07 cloudtik/providers/local/workspace-defaults.yaml
--rw-rw-r--  2.0 unx        0 b- defN 23-May-26 01:38 cloudtik/providers/onpremise/__init__.py
--rw-rw-r--  2.0 unx       54 b- defN 23-Jun-05 07:07 cloudtik/providers/onpremise/commands.yaml
--rw-rw-r--  2.0 unx      973 b- defN 23-Jun-05 07:07 cloudtik/providers/onpremise/defaults.yaml
--rw-rw-r--  2.0 unx       96 b- defN 23-Jun-05 07:07 cloudtik/providers/onpremise/workspace-defaults.yaml
--rw-rw-r--  2.0 unx        0 b- defN 23-May-26 01:38 cloudtik/providers/onpremise/service/__init__.py
--rw-rw-r--  2.0 unx    11268 b- defN 23-May-29 01:12 cloudtik/providers/onpremise/service/cloudtik_cloud_simulator.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-26 01:38 cloudtik/providers/virtual/__init__.py
--rw-rw-r--  2.0 unx      273 b- defN 23-Jun-05 07:07 cloudtik/providers/virtual/commands.yaml
--rw-rw-r--  2.0 unx      751 b- defN 23-Jun-05 07:07 cloudtik/providers/virtual/defaults.yaml
--rw-rw-r--  2.0 unx       94 b- defN 23-Jun-05 07:07 cloudtik/providers/virtual/workspace-defaults.yaml
+-rw-rw-r--  2.0 unx       54 b- defN 23-Jul-07 08:48 cloudtik/providers/local/commands.yaml
+-rw-rw-r--  2.0 unx      761 b- defN 23-Jul-07 08:48 cloudtik/providers/local/defaults.yaml
+-rw-rw-r--  2.0 unx      142 b- defN 23-Jul-07 08:48 cloudtik/providers/local/workspace-defaults.yaml
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-07 07:37 cloudtik/providers/onpremise/__init__.py
+-rw-rw-r--  2.0 unx       54 b- defN 23-Jul-07 08:48 cloudtik/providers/onpremise/commands.yaml
+-rw-rw-r--  2.0 unx      973 b- defN 23-Jul-07 08:48 cloudtik/providers/onpremise/defaults.yaml
+-rw-rw-r--  2.0 unx       96 b- defN 23-Jul-07 08:48 cloudtik/providers/onpremise/workspace-defaults.yaml
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-07 07:37 cloudtik/providers/onpremise/service/__init__.py
+-rw-rw-r--  2.0 unx    11268 b- defN 23-Jun-07 07:37 cloudtik/providers/onpremise/service/cloudtik_cloud_simulator.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-07 07:37 cloudtik/providers/virtual/__init__.py
+-rw-rw-r--  2.0 unx      273 b- defN 23-Jul-07 08:48 cloudtik/providers/virtual/commands.yaml
+-rw-rw-r--  2.0 unx      751 b- defN 23-Jul-07 08:48 cloudtik/providers/virtual/defaults.yaml
+-rw-rw-r--  2.0 unx       94 b- defN 23-Jul-07 08:48 cloudtik/providers/virtual/workspace-defaults.yaml
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/runtime/__init__.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/runtime/ai/__init__.py
--rw-rw-r--  2.0 unx      995 b- defN 23-May-21 03:18 cloudtik/runtime/ai/api.py
--rw-rw-r--  2.0 unx     1576 b- defN 23-Jun-01 05:24 cloudtik/runtime/ai/runtime.py
--rw-rw-r--  2.0 unx     2410 b- defN 23-Jun-01 05:24 cloudtik/runtime/ai/utils.py
--rw-rw-r--  2.0 unx    75965 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/conf/adlfs_spec.py.patch
--rw-rw-r--  2.0 unx    59836 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/conf/gcsfs_core.py.patch
--rw-rw-r--  2.0 unx     3297 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/conf/horovod_ray_utils.py.patch
--rw-rw-r--  2.0 unx     8909 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/conf/horovod_runner_common_service_driver_service.py.patch
--rw-rw-r--  2.0 unx    11962 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/conf/horovod_runner_driver_driver_service.py.patch
--rw-rw-r--  2.0 unx    15172 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/conf/horovod_runner_gloo_run.py.patch
--rw-rw-r--  2.0 unx    46611 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/conf/horovod_runner_launch.py.patch
--rw-rw-r--  2.0 unx    11013 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/conf/horovod_runner_mpi_run.py.patch
--rw-rw-r--  2.0 unx     1862 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/conf/horovod_runner_util_remote.py.patch
--rw-rw-r--  2.0 unx     9501 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/conf/horovod_spark_driver_driver_service.py.patch
--rw-rw-r--  2.0 unx     4976 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/conf/horovod_spark_gloo_run.py.patch
--rw-rw-r--  2.0 unx     3643 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/conf/horovod_spark_mpi_run.py.patch
--rw-rw-r--  2.0 unx    21895 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/conf/horovod_spark_runner.py.patch
--rw-rw-r--  2.0 unx     2666 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/conf/horovod_spark_task_mpirun_exec_fn.py.patch
--rw-rw-r--  2.0 unx    47801 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/conf/ipex_cpu_launch.py.patch
--rw-rw-r--  2.0 unx     5486 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/conf/mlflow_store_artifact_artifact_repository_registry.py.patch
--rw-rw-r--  2.0 unx     8984 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/conf/mlflow_store_artifact_azure_blob_artifact_repo.py.patch
--rw-rw-r--  2.0 unx     7065 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/conf/mlflow_store_artifact_azure_data_lake_artifact_repo.py.patch
--rw-rw-r--  2.0 unx    28055 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/conf/ossfs_core.py.patch
--rw-rw-r--  2.0 unx      965 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/config/commands.yaml
--rw-rw-r--  2.0 unx       25 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/config/defaults.yaml
--rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/__init__.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-31 12:11 cloudtik/runtime/ai/modeling/classical_ml/__init__.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-31 12:11 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/__init__.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-31 12:11 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/__init__.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/requirements.txt
--rw-rw-r--  2.0 unx     1930 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/config/data-preprocessing-config.yaml
--rw-rw-r--  2.0 unx     1412 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/config/training-config.yaml
--rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/__init__.py
--rw-rw-r--  2.0 unx     3812 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/process_data.py
--rw-rw-r--  2.0 unx     5754 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/run.py
--rw-rw-r--  2.0 unx    17346 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/train_ray.py
--rw-rw-r--  2.0 unx     1198 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/train_spark.py
--rw-rw-r--  2.0 unx     2586 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/utils.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-31 12:11 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/data_processing/__init__.py
--rw-rw-r--  2.0 unx     1261 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/data_processing/data_splitting.py
--rw-rw-r--  2.0 unx     8725 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/data_processing/data_transform.py
--rw-rw-r--  2.0 unx     4316 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/data_processing/post_transform.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-31 12:11 cloudtik/runtime/ai/modeling/graph_modeling/__init__.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-31 12:11 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/__init__.py
--rw-rw-r--  2.0 unx     1045 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/config/tabular2graph.yaml
--rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/__init__.py
--rw-rw-r--  2.0 unx     6775 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/build_graph.py
--rw-rw-r--  2.0 unx    31974 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/launch.py
--rw-rw-r--  2.0 unx     5011 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/map_embeddings.py
--rw-rw-r--  2.0 unx     4400 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/map_embeddings_single.py
--rw-rw-r--  2.0 unx     3124 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/partition_graph.py
--rw-rw-r--  2.0 unx     4620 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/process_data.py
--rw-rw-r--  2.0 unx    11651 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/run.py
--rw-rw-r--  2.0 unx    19752 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/train_graph_sage.py
--rw-rw-r--  2.0 unx    13642 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/train_graph_sage_single.py
--rw-rw-r--  2.0 unx      469 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/utils.py
--rw-rw-r--  2.0 unx       48 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/__init__.py
--rw-rw-r--  2.0 unx     2054 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/dataset.py
--rw-rw-r--  2.0 unx     8559 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/dataset_factory.py
--rw-rw-r--  2.0 unx     6851 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/model.py
--rw-rw-r--  2.0 unx    11082 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/model_factory.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-31 12:11 cloudtik/runtime/ai/modeling/transfer_learning/common/__init__.py
--rw-rw-r--  2.0 unx     5954 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/common/utils.py
--rw-rw-r--  2.0 unx     1747 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/modeling/transfer_learning/common/downloader/README.md
--rw-rw-r--  2.0 unx      674 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/common/downloader/__init__.py
--rw-rw-r--  2.0 unx     5363 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/common/downloader/datasets.py
--rw-rw-r--  2.0 unx     4675 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/common/downloader/models.py
--rw-rw-r--  2.0 unx     3008 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/common/downloader/sources.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-31 12:11 cloudtik/runtime/ai/modeling/transfer_learning/common/pytorch/__init__.py
--rw-rw-r--  2.0 unx    10198 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/common/pytorch/dataset.py
--rw-rw-r--  2.0 unx     8408 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/common/pytorch/model.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-31 12:11 cloudtik/runtime/ai/modeling/transfer_learning/common/pytorch/hugging_face/__init__.py
--rw-rw-r--  2.0 unx    13173 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/common/pytorch/hugging_face/dataset.py
--rw-rw-r--  2.0 unx     3168 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/common/pytorch/hugging_face/model.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-31 12:11 cloudtik/runtime/ai/modeling/transfer_learning/common/tensorflow/__init__.py
--rw-rw-r--  2.0 unx     4958 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/common/tensorflow/dataset.py
--rw-rw-r--  2.0 unx    12893 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/common/tensorflow/model.py
--rw-rw-r--  2.0 unx      674 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/__init__.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-31 12:11 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/__init__.py
--rw-rw-r--  2.0 unx    25689 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/image_anomaly_detection_dataset.py
--rw-rw-r--  2.0 unx    29347 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/image_anomaly_detection_model.py
--rw-rw-r--  2.0 unx     5769 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/utils.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-31 12:11 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/cutpaste/__init__.py
--rw-rw-r--  2.0 unx     6977 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/cutpaste/cutpaste.py
--rw-rw-r--  2.0 unx     2390 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/cutpaste/model.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-31 12:11 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/simsiam/__init__.py
--rw-rw-r--  2.0 unx     2535 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/simsiam/builder.py
--rw-rw-r--  2.0 unx     1380 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/simsiam/loader.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-31 12:11 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/torchvision/__init__.py
--rw-rw-r--  2.0 unx     2131 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/torchvision/image_anomaly_detection_model.py
--rw-rw-r--  2.0 unx      865 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/torchvision/image_anomaly_detection_models.json
--rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/__init__.py
--rw-rw-r--  2.0 unx     1223 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/image_classification_dataset.py
--rw-rw-r--  2.0 unx     3320 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/image_classification_model.py
--rw-rw-r--  2.0 unx     3044 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/image_classification_template.yaml
--rw-rw-r--  2.0 unx        0 b- defN 23-May-31 12:11 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/pytorch/__init__.py
--rw-rw-r--  2.0 unx     6393 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/pytorch/image_classification_dataset.py
--rw-rw-r--  2.0 unx    36293 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/pytorch/image_classification_model.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-31 12:11 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/pytorch/pytorch_hub/__init__.py
--rw-rw-r--  2.0 unx     2444 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/pytorch/pytorch_hub/image_classification_model.py
--rw-rw-r--  2.0 unx     4894 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/pytorch/pytorch_hub/image_classification_models.json
--rw-rw-r--  2.0 unx        0 b- defN 23-May-31 12:11 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/pytorch/torchvision/__init__.py
--rw-rw-r--  2.0 unx     5925 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/pytorch/torchvision/image_classification_dataset.py
--rw-rw-r--  2.0 unx    14721 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/pytorch/torchvision/image_classification_model.py
--rw-rw-r--  2.0 unx    10856 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/pytorch/torchvision/image_classification_models.json
--rw-rw-r--  2.0 unx        0 b- defN 23-May-31 12:11 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/__init__.py
--rw-rw-r--  2.0 unx    10274 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/image_classification_dataset.py
--rw-rw-r--  2.0 unx    30894 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/image_classification_model.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-31 12:11 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/keras/__init__.py
--rw-rw-r--  2.0 unx     3351 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/keras/image_classification_model.py
--rw-rw-r--  2.0 unx     4762 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/keras/image_classification_models.json
--rw-rw-r--  2.0 unx        0 b- defN 23-May-31 12:11 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/tf_hub/__init__.py
--rw-rw-r--  2.0 unx    12300 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/tf_hub/image_classification_model.py
--rw-rw-r--  2.0 unx     6362 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/tf_hub/image_classification_models.json
--rw-rw-r--  2.0 unx        0 b- defN 23-May-31 12:11 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/tfds/__init__.py
--rw-rw-r--  2.0 unx     6852 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/tfds/image_classification_dataset.py
--rw-rw-r--  2.0 unx      674 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/__init__.py
--rw-rw-r--  2.0 unx     2146 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/text_classification_dataset.py
--rw-rw-r--  2.0 unx     3196 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/text_classification_model.py
--rw-rw-r--  2.0 unx      216 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/text_classification_template.yaml
--rw-rw-r--  2.0 unx        0 b- defN 23-May-31 12:11 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/pytorch/__init__.py
--rw-rw-r--  2.0 unx     8957 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/pytorch/text_classification_dataset.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-31 12:11 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/pytorch/hugging_face/__init__.py
--rw-rw-r--  2.0 unx     6389 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/pytorch/hugging_face/text_classification_dataset.py
--rw-rw-r--  2.0 unx     1969 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/pytorch/hugging_face/text_classification_datasets.json
--rw-rw-r--  2.0 unx    43796 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/pytorch/hugging_face/text_classification_model.py
--rw-rw-r--  2.0 unx      933 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/pytorch/hugging_face/text_classification_models.json
--rw-rw-r--  2.0 unx        0 b- defN 23-May-31 12:11 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/tensorflow/__init__.py
--rw-rw-r--  2.0 unx     8713 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/tensorflow/text_classification_dataset.py
--rw-rw-r--  2.0 unx    28242 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/tensorflow/text_classification_model.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-31 12:11 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/tensorflow/tf_hub/__init__.py
--rw-rw-r--  2.0 unx    12625 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/tensorflow/tf_hub/text_classification_model.py
--rw-rw-r--  2.0 unx    14019 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/tensorflow/tf_hub/text_classification_models.json
--rw-rw-r--  2.0 unx        0 b- defN 23-May-31 12:11 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/tensorflow/tfds/__init__.py
--rw-rw-r--  2.0 unx     5203 b- defN 23-May-21 03:18 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/tensorflow/tfds/text_classification_dataset.py
--rw-rw-r--  2.0 unx      529 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/tensorflow/tfds/text_classification_datasets.json
--rw-rw-r--  2.0 unx     7892 b- defN 23-May-21 03:18 cloudtik/runtime/ai/runner/__init__.py
--rw-rw-r--  2.0 unx     3369 b- defN 23-May-21 03:18 cloudtik/runtime/ai/runner/distributed_training_launcher.py
--rw-rw-r--  2.0 unx     1665 b- defN 23-May-21 03:18 cloudtik/runtime/ai/runner/horovod_training_launcher.py
--rw-rw-r--  2.0 unx    18296 b- defN 23-May-21 03:18 cloudtik/runtime/ai/runner/launch.py
--rw-rw-r--  2.0 unx     2078 b- defN 23-May-21 03:18 cloudtik/runtime/ai/runner/launcher.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/runtime/ai/runner/cpu/__init__.py
--rw-rw-r--  2.0 unx     5283 b- defN 23-May-21 03:18 cloudtik/runtime/ai/runner/cpu/default_training_launcher.py
--rw-rw-r--  2.0 unx     5924 b- defN 23-May-21 03:18 cloudtik/runtime/ai/runner/cpu/launcher.py
--rw-rw-r--  2.0 unx    11516 b- defN 23-May-21 03:18 cloudtik/runtime/ai/runner/cpu/multi_instance_launcher.py
--rw-rw-r--  2.0 unx     4168 b- defN 23-May-21 03:18 cloudtik/runtime/ai/runner/cpu/optimized_training_launcher.py
--rw-rw-r--  2.0 unx    18934 b- defN 23-May-21 03:18 cloudtik/runtime/ai/runner/cpu/platform_utils.py
--rw-rw-r--  2.0 unx     5195 b- defN 23-May-21 03:18 cloudtik/runtime/ai/runner/cpu/utils.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/runtime/ai/runner/util/__init__.py
--rw-rw-r--  2.0 unx     9817 b- defN 23-May-21 03:18 cloudtik/runtime/ai/runner/util/safe_shell_exec.py
--rw-rw-r--  2.0 unx     5686 b- defN 23-May-21 03:18 cloudtik/runtime/ai/runner/util/threads.py
--rw-rw-r--  2.0 unx     1406 b- defN 23-May-21 03:18 cloudtik/runtime/ai/runner/util/tiny_shell_exec.py
--rw-rw-r--  2.0 unx     4029 b- defN 23-May-21 03:18 cloudtik/runtime/ai/runner/util/utils.py
--rw-rw-r--  2.0 unx      100 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/scripts/cloudtik-rsh.sh
--rw-rw-r--  2.0 unx     7180 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/scripts/configure.sh
--rw-rw-r--  2.0 unx     7538 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/scripts/install.sh
--rw-rw-r--  2.0 unx     3359 b- defN 23-Jun-05 07:07 cloudtik/runtime/ai/scripts/services.sh
+-rw-rw-r--  2.0 unx      561 b- defN 23-Jun-13 06:45 cloudtik/runtime/ai/__init__.py
+-rw-rw-r--  2.0 unx     1198 b- defN 23-Jun-10 11:53 cloudtik/runtime/ai/api.py
+-rw-rw-r--  2.0 unx     1576 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/runtime.py
+-rw-rw-r--  2.0 unx     2410 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/utils.py
+-rw-rw-r--  2.0 unx    75965 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/conf/adlfs_spec.py.patch
+-rw-rw-r--  2.0 unx    59836 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/conf/gcsfs_core.py.patch
+-rw-rw-r--  2.0 unx     3297 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/conf/horovod_ray_utils.py.patch
+-rw-rw-r--  2.0 unx     8909 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/conf/horovod_runner_common_service_driver_service.py.patch
+-rw-rw-r--  2.0 unx    11962 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/conf/horovod_runner_driver_driver_service.py.patch
+-rw-rw-r--  2.0 unx    15172 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/conf/horovod_runner_gloo_run.py.patch
+-rw-rw-r--  2.0 unx    46611 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/conf/horovod_runner_launch.py.patch
+-rw-rw-r--  2.0 unx    11013 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/conf/horovod_runner_mpi_run.py.patch
+-rw-rw-r--  2.0 unx     1862 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/conf/horovod_runner_util_remote.py.patch
+-rw-rw-r--  2.0 unx     9501 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/conf/horovod_spark_driver_driver_service.py.patch
+-rw-rw-r--  2.0 unx     4976 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/conf/horovod_spark_gloo_run.py.patch
+-rw-rw-r--  2.0 unx     3643 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/conf/horovod_spark_mpi_run.py.patch
+-rw-rw-r--  2.0 unx    21895 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/conf/horovod_spark_runner.py.patch
+-rw-rw-r--  2.0 unx     2666 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/conf/horovod_spark_task_mpirun_exec_fn.py.patch
+-rw-rw-r--  2.0 unx    47801 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/conf/ipex_cpu_launch.py.patch
+-rw-rw-r--  2.0 unx     5486 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/conf/mlflow_store_artifact_artifact_repository_registry.py.patch
+-rw-rw-r--  2.0 unx     8984 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/conf/mlflow_store_artifact_azure_blob_artifact_repo.py.patch
+-rw-rw-r--  2.0 unx     7065 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/conf/mlflow_store_artifact_azure_data_lake_artifact_repo.py.patch
+-rw-rw-r--  2.0 unx    28055 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/conf/ossfs_core.py.patch
+-rw-rw-r--  2.0 unx      965 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/config/commands.yaml
+-rw-rw-r--  2.0 unx       25 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/config/defaults.yaml
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/__init__.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/classical_ml/__init__.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/__init__.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/__init__.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/requirements.txt
+-rw-rw-r--  2.0 unx     1930 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/config/data-processing-config.yaml
+-rw-rw-r--  2.0 unx      210 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/config/dataset-config.yaml
+-rw-rw-r--  2.0 unx     1212 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/config/training-config.yaml
+-rw-rw-r--  2.0 unx     1001 b- defN 23-Jun-22 01:26 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/__init__.py
+-rw-rw-r--  2.0 unx    11593 b- defN 23-Jun-22 01:26 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/run.py
+-rw-rw-r--  2.0 unx     2698 b- defN 23-Jul-05 12:22 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/utils.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-13 06:45 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/data/__init__.py
+-rw-rw-r--  2.0 unx     1252 b- defN 23-Jun-13 06:45 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/data/data_splitting.py
+-rw-rw-r--  2.0 unx     8626 b- defN 23-Jun-13 06:45 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/data/data_transform.py
+-rw-rw-r--  2.0 unx     4306 b- defN 23-Jun-13 06:45 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/data/post_transform.py
+-rw-rw-r--  2.0 unx     3195 b- defN 23-Jun-16 22:44 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/data/process.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-13 06:45 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/model/__init__.py
+-rw-rw-r--  2.0 unx     4847 b- defN 23-Jun-21 11:37 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/model/model.py
+-rw-rw-r--  2.0 unx     2777 b- defN 23-Jun-22 01:26 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/model/predictor.py
+-rw-rw-r--  2.0 unx     6683 b- defN 23-Jun-21 11:37 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/model/trainer.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-13 06:45 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/model/ray/__init__.py
+-rw-rw-r--  2.0 unx     5006 b- defN 23-Jun-13 06:45 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/model/ray/model.py
+-rw-rw-r--  2.0 unx     3908 b- defN 23-Jun-13 06:45 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/model/ray/trainer.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-13 06:45 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/model/spark/__init__.py
+-rw-rw-r--  2.0 unx     1198 b- defN 23-Jun-13 06:45 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/model/spark/trainer.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/graph_modeling/__init__.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/__init__.py
+-rw-rw-r--  2.0 unx     1618 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/config/tabular2graph.yaml
+-rw-rw-r--  2.0 unx     1620 b- defN 23-Jul-03 02:37 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/__init__.py
+-rw-rw-r--  2.0 unx    10245 b- defN 23-Jul-05 12:22 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/build_graph.py
+-rw-rw-r--  2.0 unx     7008 b- defN 23-Jul-05 12:22 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/embeddings.py
+-rw-rw-r--  2.0 unx    33297 b- defN 23-Jul-07 08:46 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/launch.py
+-rw-rw-r--  2.0 unx     2813 b- defN 23-Jul-05 12:22 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/partition_graph.py
+-rw-rw-r--  2.0 unx    23233 b- defN 23-Jul-05 12:22 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/run.py
+-rw-rw-r--  2.0 unx     3400 b- defN 23-Jun-28 10:28 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/tokenizer.py
+-rw-rw-r--  2.0 unx     1560 b- defN 23-Jul-05 12:22 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/utils.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-13 06:45 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/data/__init__.py
+-rw-rw-r--  2.0 unx     4133 b- defN 23-Jul-05 12:22 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/data/process.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-09 13:22 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/__init__.py
+-rw-rw-r--  2.0 unx     2346 b- defN 23-Jul-01 08:50 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/utils.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-25 10:14 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/__init__.py
+-rw-rw-r--  2.0 unx     9669 b- defN 23-Jun-30 01:09 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/model.py
+-rw-rw-r--  2.0 unx     2949 b- defN 23-Jul-05 12:22 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/predict.py
+-rw-rw-r--  2.0 unx     1538 b- defN 23-Jun-27 13:01 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/predictor.py
+-rw-rw-r--  2.0 unx     6718 b- defN 23-Jul-05 12:22 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/train.py
+-rw-rw-r--  2.0 unx     9602 b- defN 23-Jul-05 12:22 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/trainer.py
+-rw-rw-r--  2.0 unx     4456 b- defN 23-Jul-01 08:12 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/utils.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-27 13:01 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/distributed/__init__.py
+-rw-rw-r--  2.0 unx     3947 b- defN 23-Jun-30 01:09 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/distributed/model.py
+-rw-rw-r--  2.0 unx     7358 b- defN 23-Jul-01 08:12 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/distributed/train.py
+-rw-rw-r--  2.0 unx    14232 b- defN 23-Jul-05 12:22 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/distributed/trainer.py
+-rw-rw-r--  2.0 unx     3089 b- defN 23-Jul-05 12:22 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/distributed/utils.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-25 10:14 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/inductive/__init__.py
+-rw-rw-r--  2.0 unx     1811 b- defN 23-Jun-30 01:09 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/inductive/model.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-27 13:01 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/inductive/distributed/__init__.py
+-rw-rw-r--  2.0 unx     1442 b- defN 23-Jun-30 01:09 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/inductive/distributed/model.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-29 02:15 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/transductive/__init__.py
+-rw-rw-r--  2.0 unx     1822 b- defN 23-Jun-30 01:09 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/transductive/model.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-29 02:15 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/transductive/distributed/__init__.py
+-rw-rw-r--  2.0 unx     1113 b- defN 23-Jun-29 02:15 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/transductive/distributed/model.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-24 11:56 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/__init__.py
+-rw-rw-r--  2.0 unx     7668 b- defN 23-Jun-30 01:09 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/model.py
+-rw-rw-r--  2.0 unx     2981 b- defN 23-Jul-05 12:22 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/predict.py
+-rw-rw-r--  2.0 unx     1399 b- defN 23-Jun-27 13:01 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/predictor.py
+-rw-rw-r--  2.0 unx     5995 b- defN 23-Jul-05 12:22 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/train.py
+-rw-rw-r--  2.0 unx     9226 b- defN 23-Jul-05 12:22 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/trainer.py
+-rw-rw-r--  2.0 unx     4620 b- defN 23-Jun-27 11:06 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/utils.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-25 01:10 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/distributed/__init__.py
+-rw-rw-r--  2.0 unx     3547 b- defN 23-Jul-01 07:33 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/distributed/model.py
+-rw-rw-r--  2.0 unx     6607 b- defN 23-Jul-01 08:12 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/distributed/train.py
+-rw-rw-r--  2.0 unx    14868 b- defN 23-Jul-05 12:22 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/distributed/trainer.py
+-rw-rw-r--  2.0 unx     1626 b- defN 23-Jul-05 12:22 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/distributed/utils.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-25 02:14 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/inductive/__init__.py
+-rw-rw-r--  2.0 unx     2096 b- defN 23-Jun-30 01:09 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/inductive/model.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-25 03:53 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/inductive/distributed/__init__.py
+-rw-rw-r--  2.0 unx     1456 b- defN 23-Jun-30 01:09 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/inductive/distributed/model.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-24 11:17 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/transductive/__init__.py
+-rw-rw-r--  2.0 unx     1509 b- defN 23-Jun-30 01:09 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/transductive/model.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-24 11:17 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/transductive/distributed/__init__.py
+-rw-rw-r--  2.0 unx     1087 b- defN 23-Jun-26 04:42 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/transductive/distributed/model.py
+-rw-rw-r--  2.0 unx       48 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/__init__.py
+-rw-rw-r--  2.0 unx     2054 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/dataset.py
+-rw-rw-r--  2.0 unx     8559 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/dataset_factory.py
+-rw-rw-r--  2.0 unx     7221 b- defN 23-Jun-08 23:56 cloudtik/runtime/ai/modeling/transfer_learning/model.py
+-rw-rw-r--  2.0 unx    11102 b- defN 23-Jun-07 09:07 cloudtik/runtime/ai/modeling/transfer_learning/model_factory.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/common/__init__.py
+-rw-rw-r--  2.0 unx     6542 b- defN 23-Jun-16 02:24 cloudtik/runtime/ai/modeling/transfer_learning/common/utils.py
+-rw-rw-r--  2.0 unx     1747 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/modeling/transfer_learning/common/downloader/README.md
+-rw-rw-r--  2.0 unx      674 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/common/downloader/__init__.py
+-rw-rw-r--  2.0 unx     5604 b- defN 23-Jun-07 12:29 cloudtik/runtime/ai/modeling/transfer_learning/common/downloader/datasets.py
+-rw-rw-r--  2.0 unx     4675 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/common/downloader/models.py
+-rw-rw-r--  2.0 unx     3008 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/common/downloader/sources.py
+-rw-rw-r--  2.0 unx      155 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/common/pytorch/__init__.py
+-rw-rw-r--  2.0 unx    10198 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/common/pytorch/dataset.py
+-rw-rw-r--  2.0 unx     9489 b- defN 23-Jun-08 23:56 cloudtik/runtime/ai/modeling/transfer_learning/common/pytorch/model.py
+-rw-rw-r--  2.0 unx     3275 b- defN 23-Jun-10 03:06 cloudtik/runtime/ai/modeling/transfer_learning/common/pytorch/train.py
+-rw-rw-r--  2.0 unx     8844 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/common/pytorch/trainer.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/common/pytorch/hugging_face/__init__.py
+-rw-rw-r--  2.0 unx    13173 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/common/pytorch/hugging_face/dataset.py
+-rw-rw-r--  2.0 unx     1859 b- defN 23-Jun-08 23:56 cloudtik/runtime/ai/modeling/transfer_learning/common/pytorch/hugging_face/model.py
+-rw-rw-r--  2.0 unx      155 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/common/tensorflow/__init__.py
+-rw-rw-r--  2.0 unx     4958 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/common/tensorflow/dataset.py
+-rw-rw-r--  2.0 unx    10400 b- defN 23-Jun-08 23:56 cloudtik/runtime/ai/modeling/transfer_learning/common/tensorflow/model.py
+-rw-rw-r--  2.0 unx     3103 b- defN 23-Jun-10 03:06 cloudtik/runtime/ai/modeling/transfer_learning/common/tensorflow/train.py
+-rw-rw-r--  2.0 unx     6731 b- defN 23-Jun-09 03:17 cloudtik/runtime/ai/modeling/transfer_learning/common/tensorflow/trainer.py
+-rw-rw-r--  2.0 unx      674 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/__init__.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/__init__.py
+-rw-rw-r--  2.0 unx    25668 b- defN 23-Jun-08 23:56 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/image_anomaly_detection_dataset.py
+-rw-rw-r--  2.0 unx    29689 b- defN 23-Jun-09 03:17 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/image_anomaly_detection_model.py
+-rw-rw-r--  2.0 unx     5769 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/utils.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/cutpaste/__init__.py
+-rw-rw-r--  2.0 unx     6977 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/cutpaste/cutpaste.py
+-rw-rw-r--  2.0 unx     2390 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/cutpaste/model.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/simsiam/__init__.py
+-rw-rw-r--  2.0 unx     2535 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/simsiam/builder.py
+-rw-rw-r--  2.0 unx     1380 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/simsiam/loader.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/torchvision/__init__.py
+-rw-rw-r--  2.0 unx     2131 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/torchvision/image_anomaly_detection_model.py
+-rw-rw-r--  2.0 unx      865 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/torchvision/image_anomaly_detection_models.json
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/__init__.py
+-rw-rw-r--  2.0 unx     1223 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/image_classification_dataset.py
+-rw-rw-r--  2.0 unx     3337 b- defN 23-Jun-08 23:56 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/image_classification_model.py
+-rw-rw-r--  2.0 unx     3044 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/image_classification_template.yaml
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/pytorch/__init__.py
+-rw-rw-r--  2.0 unx     6393 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/pytorch/image_classification_dataset.py
+-rw-rw-r--  2.0 unx    33503 b- defN 23-Jun-08 23:56 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/pytorch/image_classification_model.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/pytorch/pytorch_hub/__init__.py
+-rw-rw-r--  2.0 unx     2444 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/pytorch/pytorch_hub/image_classification_model.py
+-rw-rw-r--  2.0 unx     4894 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/pytorch/pytorch_hub/image_classification_models.json
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/pytorch/torchvision/__init__.py
+-rw-rw-r--  2.0 unx     5925 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/pytorch/torchvision/image_classification_dataset.py
+-rw-rw-r--  2.0 unx    15058 b- defN 23-Jun-08 23:56 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/pytorch/torchvision/image_classification_model.py
+-rw-rw-r--  2.0 unx    10856 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/pytorch/torchvision/image_classification_models.json
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/__init__.py
+-rw-rw-r--  2.0 unx    10274 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/image_classification_dataset.py
+-rw-rw-r--  2.0 unx    29790 b- defN 23-Jun-08 23:56 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/image_classification_model.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/keras/__init__.py
+-rw-rw-r--  2.0 unx     3351 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/keras/image_classification_model.py
+-rw-rw-r--  2.0 unx     4762 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/keras/image_classification_models.json
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/tf_hub/__init__.py
+-rw-rw-r--  2.0 unx    13137 b- defN 23-Jun-08 23:56 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/tf_hub/image_classification_model.py
+-rw-rw-r--  2.0 unx     6362 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/tf_hub/image_classification_models.json
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/tfds/__init__.py
+-rw-rw-r--  2.0 unx     6852 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/tfds/image_classification_dataset.py
+-rw-rw-r--  2.0 unx      674 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/__init__.py
+-rw-rw-r--  2.0 unx     2146 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/text_classification_dataset.py
+-rw-rw-r--  2.0 unx     2458 b- defN 23-Jun-08 23:56 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/text_classification_model.py
+-rw-rw-r--  2.0 unx      216 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/text_classification_template.yaml
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/pytorch/__init__.py
+-rw-rw-r--  2.0 unx     8957 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/pytorch/text_classification_dataset.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/pytorch/hugging_face/__init__.py
+-rw-rw-r--  2.0 unx     6527 b- defN 23-Jun-07 12:29 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/pytorch/hugging_face/text_classification_dataset.py
+-rw-rw-r--  2.0 unx     1969 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/pytorch/hugging_face/text_classification_datasets.json
+-rw-rw-r--  2.0 unx    37457 b- defN 23-Jun-08 23:56 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/pytorch/hugging_face/text_classification_model.py
+-rw-rw-r--  2.0 unx      933 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/pytorch/hugging_face/text_classification_models.json
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/tensorflow/__init__.py
+-rw-rw-r--  2.0 unx     8713 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/tensorflow/text_classification_dataset.py
+-rw-rw-r--  2.0 unx    26653 b- defN 23-Jun-08 23:56 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/tensorflow/text_classification_model.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/tensorflow/tf_hub/__init__.py
+-rw-rw-r--  2.0 unx    13454 b- defN 23-Jun-27 13:01 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/tensorflow/tf_hub/text_classification_model.py
+-rw-rw-r--  2.0 unx    14019 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/tensorflow/tf_hub/text_classification_models.json
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/tensorflow/tfds/__init__.py
+-rw-rw-r--  2.0 unx     5203 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/tensorflow/tfds/text_classification_dataset.py
+-rw-rw-r--  2.0 unx      529 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/tensorflow/tfds/text_classification_datasets.json
+-rw-rw-r--  2.0 unx     8310 b- defN 23-Jun-16 22:44 cloudtik/runtime/ai/runner/__init__.py
+-rw-rw-r--  2.0 unx     1767 b- defN 23-Jun-16 22:44 cloudtik/runtime/ai/runner/distributed_training_launcher.py
+-rw-rw-r--  2.0 unx     1404 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/runner/horovod_training_launcher.py
+-rw-rw-r--  2.0 unx    21795 b- defN 23-Jun-12 08:13 cloudtik/runtime/ai/runner/launch.py
+-rw-rw-r--  2.0 unx     2610 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/runner/launcher.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/runner/cpu/__init__.py
+-rw-rw-r--  2.0 unx     5544 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/runner/cpu/default_training_launcher.py
+-rw-rw-r--  2.0 unx     5950 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/runner/cpu/launcher.py
+-rw-rw-r--  2.0 unx    11650 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/runner/cpu/multi_instance_launcher.py
+-rw-rw-r--  2.0 unx     4198 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/runner/cpu/optimized_training_launcher.py
+-rw-rw-r--  2.0 unx    18934 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/runner/cpu/platform_utils.py
+-rw-rw-r--  2.0 unx     5195 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/runner/cpu/utils.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/runner/util/__init__.py
+-rw-rw-r--  2.0 unx    11304 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/runner/util/distributor.py
+-rw-rw-r--  2.0 unx     9817 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/runner/util/safe_shell_exec.py
+-rw-rw-r--  2.0 unx     5686 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/runner/util/threads.py
+-rw-rw-r--  2.0 unx     1406 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/runner/util/tiny_shell_exec.py
+-rw-rw-r--  2.0 unx     4142 b- defN 23-Jun-07 07:37 cloudtik/runtime/ai/runner/util/utils.py
+-rwxrwxr-x  2.0 unx      100 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/scripts/cloudtik-rsh.sh
+-rw-rw-r--  2.0 unx     7180 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/scripts/configure.sh
+-rw-rw-r--  2.0 unx     7695 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/scripts/install.sh
+-rw-rw-r--  2.0 unx     3359 b- defN 23-Jul-07 08:48 cloudtik/runtime/ai/scripts/services.sh
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-17 04:28 cloudtik/runtime/ai/util/__init__.py
+-rw-rw-r--  2.0 unx     2353 b- defN 23-Jul-05 12:22 cloudtik/runtime/ai/util/utils.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/runtime/common/__init__.py
--rw-rw-r--  2.0 unx     1512 b- defN 23-Jun-01 05:24 cloudtik/runtime/common/runtime_base.py
+-rw-rw-r--  2.0 unx     1512 b- defN 23-Jun-07 07:37 cloudtik/runtime/common/runtime_base.py
 -rw-rw-r--  2.0 unx     2331 b- defN 23-May-21 03:18 cloudtik/runtime/common/utils.py
--rw-rw-r--  2.0 unx     1192 b- defN 23-Jun-05 07:07 cloudtik/runtime/common/conf/hadoop/core-site.xml
--rw-rw-r--  2.0 unx     1680 b- defN 23-Jun-05 07:07 cloudtik/runtime/common/conf/hadoop/aliyun/core-site.xml
--rw-rw-r--  2.0 unx     1843 b- defN 23-Jun-05 07:07 cloudtik/runtime/common/conf/hadoop/aws/core-site.xml
--rw-rw-r--  2.0 unx     1681 b- defN 23-Jun-05 07:07 cloudtik/runtime/common/conf/hadoop/azure/core-site.xml
--rw-rw-r--  2.0 unx     2593 b- defN 23-Jun-05 07:07 cloudtik/runtime/common/conf/hadoop/gcp/core-site.xml
--rw-rw-r--  2.0 unx     1515 b- defN 23-Jun-05 07:07 cloudtik/runtime/common/conf/hadoop/huaweicloud/core-site.xml
--rw-rw-r--  2.0 unx     1254 b- defN 23-Jun-05 07:07 cloudtik/runtime/common/scripts/api-credential.sh
--rw-rw-r--  2.0 unx    11396 b- defN 23-Jun-05 07:07 cloudtik/runtime/common/scripts/cloud-storage-fuse.sh
--rw-rw-r--  2.0 unx    10842 b- defN 23-Jun-05 07:07 cloudtik/runtime/common/scripts/hadoop-cloud-credential.sh
--rw-rw-r--  2.0 unx     2300 b- defN 23-Jun-05 07:07 cloudtik/runtime/common/scripts/hadoop-install.sh
--rw-rw-r--  2.0 unx     1044 b- defN 23-Jun-05 07:07 cloudtik/runtime/common/scripts/jdk-install.sh
--rw-rw-r--  2.0 unx     1667 b- defN 23-Jun-05 07:07 cloudtik/runtime/common/scripts/util-functions.sh
+-rw-rw-r--  2.0 unx      984 b- defN 23-Jul-07 08:48 cloudtik/runtime/common/conf/hadoop/core-site.xml
+-rw-rw-r--  2.0 unx     1472 b- defN 23-Jul-07 08:48 cloudtik/runtime/common/conf/hadoop/aliyun/core-site.xml
+-rw-rw-r--  2.0 unx     1635 b- defN 23-Jul-07 08:48 cloudtik/runtime/common/conf/hadoop/aws/core-site.xml
+-rw-rw-r--  2.0 unx     1473 b- defN 23-Jul-07 08:48 cloudtik/runtime/common/conf/hadoop/azure/core-site.xml
+-rw-rw-r--  2.0 unx     2385 b- defN 23-Jul-07 08:48 cloudtik/runtime/common/conf/hadoop/gcp/core-site.xml
+-rw-rw-r--  2.0 unx     1307 b- defN 23-Jul-07 08:48 cloudtik/runtime/common/conf/hadoop/huaweicloud/core-site.xml
+-rw-rw-r--  2.0 unx     1254 b- defN 23-Jul-07 08:48 cloudtik/runtime/common/scripts/api-credential.sh
+-rw-rw-r--  2.0 unx    16126 b- defN 23-Jul-07 08:48 cloudtik/runtime/common/scripts/cloud-storage-fuse.sh
+-rw-rw-r--  2.0 unx    10843 b- defN 23-Jul-07 08:48 cloudtik/runtime/common/scripts/hadoop-cloud-credential.sh
+-rw-rw-r--  2.0 unx     2404 b- defN 23-Jul-07 08:48 cloudtik/runtime/common/scripts/hadoop-install.sh
+-rw-rw-r--  2.0 unx     1044 b- defN 23-Jul-07 08:48 cloudtik/runtime/common/scripts/jdk-install.sh
+-rw-rw-r--  2.0 unx     2466 b- defN 23-Jul-07 08:48 cloudtik/runtime/common/scripts/util-functions.sh
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/runtime/flink/__init__.py
--rw-rw-r--  2.0 unx     2303 b- defN 23-May-21 03:18 cloudtik/runtime/flink/api.py
--rw-rw-r--  2.0 unx     3352 b- defN 23-Jun-01 05:24 cloudtik/runtime/flink/runtime.py
--rw-rw-r--  2.0 unx    15513 b- defN 23-May-21 03:18 cloudtik/runtime/flink/scaling_policy.py
--rw-rw-r--  2.0 unx     2829 b- defN 23-May-31 09:27 cloudtik/runtime/flink/scripts.py
--rw-rw-r--  2.0 unx    15664 b- defN 23-Jun-01 05:24 cloudtik/runtime/flink/utils.py
--rw-rw-r--  2.0 unx      620 b- defN 23-Jun-05 07:07 cloudtik/runtime/flink/conf/flink/flink-conf.yaml
--rw-rw-r--  2.0 unx     2772 b- defN 23-Jun-05 07:07 cloudtik/runtime/flink/conf/hadoop/yarn-site.xml
--rw-rw-r--  2.0 unx      995 b- defN 23-Jun-05 07:07 cloudtik/runtime/flink/config/commands.yaml
--rw-rw-r--  2.0 unx       25 b- defN 23-Jun-05 07:07 cloudtik/runtime/flink/config/defaults.yaml
--rw-rw-r--  2.0 unx      500 b- defN 23-Jun-05 07:07 cloudtik/runtime/flink/scripts/configure.py
--rw-rw-r--  2.0 unx    15805 b- defN 23-Jun-05 07:07 cloudtik/runtime/flink/scripts/configure.sh
--rw-rw-r--  2.0 unx     5882 b- defN 23-Jun-05 07:07 cloudtik/runtime/flink/scripts/install.sh
--rw-rw-r--  2.0 unx     1948 b- defN 23-Jun-05 07:07 cloudtik/runtime/flink/scripts/services.sh
+-rw-rw-r--  2.0 unx     2506 b- defN 23-Jun-10 11:53 cloudtik/runtime/flink/api.py
+-rw-rw-r--  2.0 unx     3352 b- defN 23-Jun-07 07:37 cloudtik/runtime/flink/runtime.py
+-rw-rw-r--  2.0 unx    15513 b- defN 23-Jun-07 07:37 cloudtik/runtime/flink/scaling_policy.py
+-rw-rw-r--  2.0 unx     2829 b- defN 23-Jun-07 07:37 cloudtik/runtime/flink/scripts.py
+-rw-rw-r--  2.0 unx    16408 b- defN 23-Jul-05 08:32 cloudtik/runtime/flink/utils.py
+-rw-rw-r--  2.0 unx      620 b- defN 23-Jul-07 08:48 cloudtik/runtime/flink/conf/flink/flink-conf.yaml
+-rw-rw-r--  2.0 unx     2772 b- defN 23-Jul-07 08:48 cloudtik/runtime/flink/conf/hadoop/yarn-site.xml
+-rw-rw-r--  2.0 unx      995 b- defN 23-Jul-07 08:48 cloudtik/runtime/flink/config/commands.yaml
+-rw-rw-r--  2.0 unx       25 b- defN 23-Jul-07 08:48 cloudtik/runtime/flink/config/defaults.yaml
+-rw-rw-r--  2.0 unx      500 b- defN 23-Jul-07 08:48 cloudtik/runtime/flink/scripts/configure.py
+-rw-rw-r--  2.0 unx    15514 b- defN 23-Jul-07 08:48 cloudtik/runtime/flink/scripts/configure.sh
+-rw-rw-r--  2.0 unx     5882 b- defN 23-Jul-07 08:48 cloudtik/runtime/flink/scripts/install.sh
+-rw-rw-r--  2.0 unx     2263 b- defN 23-Jul-07 08:48 cloudtik/runtime/flink/scripts/services.sh
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/runtime/ganglia/__init__.py
--rw-rw-r--  2.0 unx     1212 b- defN 23-Jun-01 05:24 cloudtik/runtime/ganglia/runtime.py
--rw-rw-r--  2.0 unx      942 b- defN 23-Jun-01 05:24 cloudtik/runtime/ganglia/utils.py
--rw-rw-r--  2.0 unx     8052 b- defN 23-Jun-05 07:07 cloudtik/runtime/ganglia/conf/gmetad.conf
--rw-rw-r--  2.0 unx     7792 b- defN 23-Jun-05 07:07 cloudtik/runtime/ganglia/conf/gmond.conf
--rw-rw-r--  2.0 unx     8006 b- defN 23-Jun-05 07:07 cloudtik/runtime/ganglia/conf/gmond.node.conf
--rw-rw-r--  2.0 unx      255 b- defN 23-Jun-05 07:07 cloudtik/runtime/ganglia/conf/conf.d/diskstat.pyconf
--rw-rw-r--  2.0 unx     1015 b- defN 23-Jun-05 07:07 cloudtik/runtime/ganglia/config/commands.yaml
--rw-rw-r--  2.0 unx       25 b- defN 23-Jun-05 07:07 cloudtik/runtime/ganglia/config/defaults.yaml
--rw-rw-r--  2.0 unx     4135 b- defN 23-Jun-05 07:07 cloudtik/runtime/ganglia/scripts/configure.sh
--rw-rw-r--  2.0 unx     1820 b- defN 23-Jun-05 07:07 cloudtik/runtime/ganglia/scripts/install.sh
--rw-rw-r--  2.0 unx      893 b- defN 23-Jun-05 07:07 cloudtik/runtime/ganglia/scripts/services.sh
+-rw-rw-r--  2.0 unx     1212 b- defN 23-Jun-07 07:37 cloudtik/runtime/ganglia/runtime.py
+-rw-rw-r--  2.0 unx      942 b- defN 23-Jun-07 07:37 cloudtik/runtime/ganglia/utils.py
+-rw-rw-r--  2.0 unx     8052 b- defN 23-Jul-07 08:48 cloudtik/runtime/ganglia/conf/gmetad.conf
+-rw-rw-r--  2.0 unx     7792 b- defN 23-Jul-07 08:48 cloudtik/runtime/ganglia/conf/gmond.conf
+-rw-rw-r--  2.0 unx     8006 b- defN 23-Jul-07 08:48 cloudtik/runtime/ganglia/conf/gmond.node.conf
+-rw-rw-r--  2.0 unx      255 b- defN 23-Jul-07 08:48 cloudtik/runtime/ganglia/conf/conf.d/diskstat.pyconf
+-rw-rw-r--  2.0 unx     1015 b- defN 23-Jul-07 08:48 cloudtik/runtime/ganglia/config/commands.yaml
+-rw-rw-r--  2.0 unx       25 b- defN 23-Jul-07 08:48 cloudtik/runtime/ganglia/config/defaults.yaml
+-rw-rw-r--  2.0 unx     4135 b- defN 23-Jul-07 08:48 cloudtik/runtime/ganglia/scripts/configure.sh
+-rw-rw-r--  2.0 unx     1794 b- defN 23-Jul-07 08:48 cloudtik/runtime/ganglia/scripts/install.sh
+-rw-rw-r--  2.0 unx      893 b- defN 23-Jul-07 08:48 cloudtik/runtime/ganglia/scripts/services.sh
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/runtime/hdfs/__init__.py
--rw-rw-r--  2.0 unx     2139 b- defN 23-Jun-01 05:24 cloudtik/runtime/hdfs/runtime.py
--rw-rw-r--  2.0 unx     2557 b- defN 23-Jun-01 05:24 cloudtik/runtime/hdfs/utils.py
--rw-rw-r--  2.0 unx     2073 b- defN 23-Jun-05 07:07 cloudtik/runtime/hdfs/conf/hadoop/hdfs-site.xml
--rw-rw-r--  2.0 unx      985 b- defN 23-Jun-05 07:07 cloudtik/runtime/hdfs/config/commands.yaml
--rw-rw-r--  2.0 unx       25 b- defN 23-Jun-05 07:07 cloudtik/runtime/hdfs/config/defaults.yaml
--rw-rw-r--  2.0 unx     3034 b- defN 23-Jun-05 07:07 cloudtik/runtime/hdfs/scripts/configure.sh
--rw-rw-r--  2.0 unx      573 b- defN 23-Jun-05 07:07 cloudtik/runtime/hdfs/scripts/install.sh
--rw-rw-r--  2.0 unx      911 b- defN 23-Jun-05 07:07 cloudtik/runtime/hdfs/scripts/services.sh
+-rw-rw-r--  2.0 unx     2139 b- defN 23-Jun-07 07:37 cloudtik/runtime/hdfs/runtime.py
+-rw-rw-r--  2.0 unx     2557 b- defN 23-Jun-07 07:37 cloudtik/runtime/hdfs/utils.py
+-rw-rw-r--  2.0 unx     1228 b- defN 23-Jul-07 08:48 cloudtik/runtime/hdfs/conf/hadoop/core-site.xml
+-rw-rw-r--  2.0 unx     2406 b- defN 23-Jul-07 08:48 cloudtik/runtime/hdfs/conf/hadoop/hdfs-site.xml
+-rw-rw-r--  2.0 unx      985 b- defN 23-Jul-07 08:48 cloudtik/runtime/hdfs/config/commands.yaml
+-rw-rw-r--  2.0 unx       25 b- defN 23-Jul-07 08:48 cloudtik/runtime/hdfs/config/defaults.yaml
+-rw-rw-r--  2.0 unx     3521 b- defN 23-Jul-07 08:48 cloudtik/runtime/hdfs/scripts/configure.sh
+-rw-rw-r--  2.0 unx      573 b- defN 23-Jul-07 08:48 cloudtik/runtime/hdfs/scripts/install.sh
+-rw-rw-r--  2.0 unx      987 b- defN 23-Jul-07 08:48 cloudtik/runtime/hdfs/scripts/services.sh
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/runtime/kafka/__init__.py
--rw-rw-r--  2.0 unx     2426 b- defN 23-Jun-01 05:24 cloudtik/runtime/kafka/runtime.py
--rw-rw-r--  2.0 unx     4091 b- defN 23-Jun-01 05:24 cloudtik/runtime/kafka/utils.py
--rw-rw-r--  2.0 unx     6883 b- defN 23-Jun-05 07:07 cloudtik/runtime/kafka/conf/kafka/server.properties
--rw-rw-r--  2.0 unx      995 b- defN 23-Jun-05 07:07 cloudtik/runtime/kafka/config/commands.yaml
--rw-rw-r--  2.0 unx       25 b- defN 23-Jun-05 07:07 cloudtik/runtime/kafka/config/defaults.yaml
--rw-rw-r--  2.0 unx     2960 b- defN 23-Jun-05 07:07 cloudtik/runtime/kafka/scripts/configure-kafka.sh
--rw-rw-r--  2.0 unx     1386 b- defN 23-Jun-05 07:07 cloudtik/runtime/kafka/scripts/configure.py
--rw-rw-r--  2.0 unx     1152 b- defN 23-Jun-05 07:07 cloudtik/runtime/kafka/scripts/install.sh
--rw-rw-r--  2.0 unx     1002 b- defN 23-Jun-05 07:07 cloudtik/runtime/kafka/scripts/services.sh
+-rw-rw-r--  2.0 unx     2426 b- defN 23-Jun-07 07:37 cloudtik/runtime/kafka/runtime.py
+-rw-rw-r--  2.0 unx     4091 b- defN 23-Jun-07 07:37 cloudtik/runtime/kafka/utils.py
+-rw-rw-r--  2.0 unx     6883 b- defN 23-Jul-07 08:48 cloudtik/runtime/kafka/conf/kafka/server.properties
+-rw-rw-r--  2.0 unx      995 b- defN 23-Jul-07 08:48 cloudtik/runtime/kafka/config/commands.yaml
+-rw-rw-r--  2.0 unx       25 b- defN 23-Jul-07 08:48 cloudtik/runtime/kafka/config/defaults.yaml
+-rw-rw-r--  2.0 unx     2960 b- defN 23-Jul-07 08:48 cloudtik/runtime/kafka/scripts/configure-kafka.sh
+-rw-rw-r--  2.0 unx     1386 b- defN 23-Jul-07 08:48 cloudtik/runtime/kafka/scripts/configure.py
+-rw-rw-r--  2.0 unx     1152 b- defN 23-Jul-07 08:48 cloudtik/runtime/kafka/scripts/install.sh
+-rw-rw-r--  2.0 unx     1002 b- defN 23-Jul-07 08:48 cloudtik/runtime/kafka/scripts/services.sh
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/runtime/metastore/__init__.py
--rw-rw-r--  2.0 unx     2165 b- defN 23-Jun-01 05:24 cloudtik/runtime/metastore/runtime.py
--rw-rw-r--  2.0 unx     2211 b- defN 23-Jun-01 05:24 cloudtik/runtime/metastore/utils.py
--rw-rw-r--  2.0 unx     2191 b- defN 23-Jun-05 07:07 cloudtik/runtime/metastore/conf/hive/metastore-site.xml
--rw-rw-r--  2.0 unx      818 b- defN 23-Jun-05 07:07 cloudtik/runtime/metastore/config/commands.yaml
--rw-rw-r--  2.0 unx       25 b- defN 23-Jun-05 07:07 cloudtik/runtime/metastore/config/defaults.yaml
--rw-rw-r--  2.0 unx     4248 b- defN 23-Jun-05 07:07 cloudtik/runtime/metastore/scripts/configure.sh
--rw-rw-r--  2.0 unx     1725 b- defN 23-Jun-05 07:07 cloudtik/runtime/metastore/scripts/install.sh
--rw-rw-r--  2.0 unx     1099 b- defN 23-Jun-05 07:07 cloudtik/runtime/metastore/scripts/services.sh
+-rw-rw-r--  2.0 unx     2165 b- defN 23-Jun-07 07:37 cloudtik/runtime/metastore/runtime.py
+-rw-rw-r--  2.0 unx     2211 b- defN 23-Jun-07 07:37 cloudtik/runtime/metastore/utils.py
+-rw-rw-r--  2.0 unx     2191 b- defN 23-Jul-07 08:48 cloudtik/runtime/metastore/conf/hive/metastore-site.xml
+-rw-rw-r--  2.0 unx      818 b- defN 23-Jul-07 08:48 cloudtik/runtime/metastore/config/commands.yaml
+-rw-rw-r--  2.0 unx       25 b- defN 23-Jul-07 08:48 cloudtik/runtime/metastore/config/defaults.yaml
+-rw-rw-r--  2.0 unx     4248 b- defN 23-Jul-07 08:48 cloudtik/runtime/metastore/scripts/configure.sh
+-rw-rw-r--  2.0 unx     1725 b- defN 23-Jul-07 08:48 cloudtik/runtime/metastore/scripts/install.sh
+-rw-rw-r--  2.0 unx     1099 b- defN 23-Jul-07 08:48 cloudtik/runtime/metastore/scripts/services.sh
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/runtime/presto/__init__.py
--rw-rw-r--  2.0 unx     2806 b- defN 23-Jun-01 05:24 cloudtik/runtime/presto/runtime.py
--rw-rw-r--  2.0 unx     6525 b- defN 23-Jun-01 05:24 cloudtik/runtime/presto/utils.py
--rw-rw-r--  2.0 unx      384 b- defN 23-Jun-05 07:07 cloudtik/runtime/presto/conf/presto/config.properties
--rw-rw-r--  2.0 unx      250 b- defN 23-Jun-05 07:07 cloudtik/runtime/presto/conf/presto/config.worker.properties
--rw-rw-r--  2.0 unx      220 b- defN 23-Jun-05 07:07 cloudtik/runtime/presto/conf/presto/jvm.config
--rw-rw-r--  2.0 unx       90 b- defN 23-Jun-05 07:07 cloudtik/runtime/presto/conf/presto/node.properties
--rw-rw-r--  2.0 unx      374 b- defN 23-Jun-05 07:07 cloudtik/runtime/presto/conf/presto/catalog/gcs.key-file.json
--rw-rw-r--  2.0 unx      956 b- defN 23-Jun-05 07:07 cloudtik/runtime/presto/conf/presto/catalog/hive-azure-core-site.xml
--rw-rw-r--  2.0 unx       87 b- defN 23-Jun-05 07:07 cloudtik/runtime/presto/conf/presto/catalog/hive.config.properties
--rw-rw-r--  2.0 unx      225 b- defN 23-Jun-05 07:07 cloudtik/runtime/presto/conf/presto/catalog/hive.gcs.properties
--rw-rw-r--  2.0 unx       70 b- defN 23-Jun-05 07:07 cloudtik/runtime/presto/conf/presto/catalog/hive.properties
--rw-rw-r--  2.0 unx      294 b- defN 23-Jun-05 07:07 cloudtik/runtime/presto/conf/presto/catalog/hive.s3.properties
--rw-rw-r--  2.0 unx     1005 b- defN 23-Jun-05 07:07 cloudtik/runtime/presto/config/commands.yaml
--rw-rw-r--  2.0 unx      218 b- defN 23-Jun-05 07:07 cloudtik/runtime/presto/config/defaults.yaml
--rw-rw-r--  2.0 unx      732 b- defN 23-Jun-05 07:07 cloudtik/runtime/presto/scripts/configure.py
--rw-rw-r--  2.0 unx     8442 b- defN 23-Jun-05 07:07 cloudtik/runtime/presto/scripts/configure.sh
--rw-rw-r--  2.0 unx     1898 b- defN 23-Jun-05 07:07 cloudtik/runtime/presto/scripts/install.sh
--rw-rw-r--  2.0 unx      678 b- defN 23-Jun-05 07:07 cloudtik/runtime/presto/scripts/services.sh
+-rw-rw-r--  2.0 unx     2806 b- defN 23-Jun-07 07:37 cloudtik/runtime/presto/runtime.py
+-rw-rw-r--  2.0 unx     6614 b- defN 23-Jul-05 04:32 cloudtik/runtime/presto/utils.py
+-rw-rw-r--  2.0 unx      384 b- defN 23-Jul-07 08:48 cloudtik/runtime/presto/conf/presto/config.properties
+-rw-rw-r--  2.0 unx      250 b- defN 23-Jul-07 08:48 cloudtik/runtime/presto/conf/presto/config.worker.properties
+-rw-rw-r--  2.0 unx      220 b- defN 23-Jul-07 08:48 cloudtik/runtime/presto/conf/presto/jvm.config
+-rw-rw-r--  2.0 unx       90 b- defN 23-Jul-07 08:48 cloudtik/runtime/presto/conf/presto/node.properties
+-rw-rw-r--  2.0 unx      374 b- defN 23-Jul-07 08:48 cloudtik/runtime/presto/conf/presto/catalog/gcs.key-file.json
+-rw-rw-r--  2.0 unx      956 b- defN 23-Jul-07 08:48 cloudtik/runtime/presto/conf/presto/catalog/hive-azure-core-site.xml
+-rw-rw-r--  2.0 unx       87 b- defN 23-Jul-07 08:48 cloudtik/runtime/presto/conf/presto/catalog/hive.config.properties
+-rw-rw-r--  2.0 unx      225 b- defN 23-Jul-07 08:48 cloudtik/runtime/presto/conf/presto/catalog/hive.gcs.properties
+-rw-rw-r--  2.0 unx       70 b- defN 23-Jul-07 08:48 cloudtik/runtime/presto/conf/presto/catalog/hive.properties
+-rw-rw-r--  2.0 unx      294 b- defN 23-Jul-07 08:48 cloudtik/runtime/presto/conf/presto/catalog/hive.s3.properties
+-rw-rw-r--  2.0 unx     1005 b- defN 23-Jul-07 08:48 cloudtik/runtime/presto/config/commands.yaml
+-rw-rw-r--  2.0 unx      218 b- defN 23-Jul-07 08:48 cloudtik/runtime/presto/config/defaults.yaml
+-rw-rw-r--  2.0 unx      732 b- defN 23-Jul-07 08:48 cloudtik/runtime/presto/scripts/configure.py
+-rw-rw-r--  2.0 unx     8442 b- defN 23-Jul-07 08:48 cloudtik/runtime/presto/scripts/configure.sh
+-rw-rw-r--  2.0 unx     1898 b- defN 23-Jul-07 08:48 cloudtik/runtime/presto/scripts/install.sh
+-rw-rw-r--  2.0 unx      678 b- defN 23-Jul-07 08:48 cloudtik/runtime/presto/scripts/services.sh
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/runtime/ray/__init__.py
--rw-rw-r--  2.0 unx     2556 b- defN 23-Jun-01 05:24 cloudtik/runtime/ray/runtime.py
--rw-rw-r--  2.0 unx     7924 b- defN 23-May-21 03:18 cloudtik/runtime/ray/scaling_policy.py
--rw-rw-r--  2.0 unx     3020 b- defN 23-Jun-01 05:24 cloudtik/runtime/ray/utils.py
--rw-rw-r--  2.0 unx      975 b- defN 23-Jun-05 07:07 cloudtik/runtime/ray/config/commands.yaml
--rw-rw-r--  2.0 unx       25 b- defN 23-Jun-05 07:07 cloudtik/runtime/ray/config/defaults.yaml
--rw-rw-r--  2.0 unx      834 b- defN 23-Jun-05 07:07 cloudtik/runtime/ray/scripts/configure.sh
--rw-rw-r--  2.0 unx      498 b- defN 23-Jun-05 07:07 cloudtik/runtime/ray/scripts/install.sh
--rw-rw-r--  2.0 unx      881 b- defN 23-Jun-05 07:07 cloudtik/runtime/ray/scripts/services.sh
+-rw-rw-r--  2.0 unx     2556 b- defN 23-Jun-07 07:37 cloudtik/runtime/ray/runtime.py
+-rw-rw-r--  2.0 unx     7924 b- defN 23-Jun-07 07:37 cloudtik/runtime/ray/scaling_policy.py
+-rw-rw-r--  2.0 unx     3020 b- defN 23-Jun-07 07:37 cloudtik/runtime/ray/utils.py
+-rw-rw-r--  2.0 unx      975 b- defN 23-Jul-07 08:48 cloudtik/runtime/ray/config/commands.yaml
+-rw-rw-r--  2.0 unx       25 b- defN 23-Jul-07 08:48 cloudtik/runtime/ray/config/defaults.yaml
+-rw-rw-r--  2.0 unx      834 b- defN 23-Jul-07 08:48 cloudtik/runtime/ray/scripts/configure.sh
+-rw-rw-r--  2.0 unx      498 b- defN 23-Jul-07 08:48 cloudtik/runtime/ray/scripts/install.sh
+-rw-rw-r--  2.0 unx      881 b- defN 23-Jul-07 08:48 cloudtik/runtime/ray/scripts/services.sh
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/runtime/spark/__init__.py
--rw-rw-r--  2.0 unx     2339 b- defN 23-May-21 03:18 cloudtik/runtime/spark/api.py
+-rw-rw-r--  2.0 unx     2542 b- defN 23-Jun-10 11:53 cloudtik/runtime/spark/api.py
 -rw-rw-r--  2.0 unx     1761 b- defN 23-May-21 03:18 cloudtik/runtime/spark/job_waiter.py
--rw-rw-r--  2.0 unx     3615 b- defN 23-Jun-01 05:24 cloudtik/runtime/spark/runtime.py
--rw-rw-r--  2.0 unx    15691 b- defN 23-May-21 03:18 cloudtik/runtime/spark/scaling_policy.py
--rw-rw-r--  2.0 unx     2861 b- defN 23-May-31 09:27 cloudtik/runtime/spark/scripts.py
--rw-rw-r--  2.0 unx    17757 b- defN 23-Jun-01 05:24 cloudtik/runtime/spark/utils.py
--rw-rw-r--  2.0 unx     2960 b- defN 23-Jun-05 07:07 cloudtik/runtime/spark/conf/hadoop/yarn-site.xml
--rw-rw-r--  2.0 unx     1345 b- defN 23-Jun-05 07:07 cloudtik/runtime/spark/conf/spark/spark-defaults.conf
--rw-rw-r--  2.0 unx      995 b- defN 23-Jun-05 07:07 cloudtik/runtime/spark/config/commands.yaml
--rw-rw-r--  2.0 unx       25 b- defN 23-Jun-05 07:07 cloudtik/runtime/spark/config/defaults.yaml
--rw-rw-r--  2.0 unx      500 b- defN 23-May-31 09:27 cloudtik/runtime/spark/scripts/configure.py
--rw-rw-r--  2.0 unx    15597 b- defN 23-Jun-05 07:07 cloudtik/runtime/spark/scripts/configure.sh
--rw-rw-r--  2.0 unx     7226 b- defN 23-Jun-05 07:07 cloudtik/runtime/spark/scripts/install.sh
--rw-rw-r--  2.0 unx     1951 b- defN 23-Jun-05 07:07 cloudtik/runtime/spark/scripts/services.sh
--rw-rw-r--  2.0 unx        0 b- defN 23-Jun-01 12:01 cloudtik/runtime/sshserver/__init__.py
--rw-rw-r--  2.0 unx     1543 b- defN 23-Jun-01 12:01 cloudtik/runtime/sshserver/runtime.py
--rw-rw-r--  2.0 unx     2493 b- defN 23-Jun-02 10:25 cloudtik/runtime/sshserver/utils.py
--rw-rw-r--  2.0 unx     3287 b- defN 23-Jun-05 07:07 cloudtik/runtime/sshserver/conf/sshd_config
--rw-rw-r--  2.0 unx     1035 b- defN 23-Jun-05 07:07 cloudtik/runtime/sshserver/config/commands.yaml
--rw-rw-r--  2.0 unx       25 b- defN 23-Jun-05 07:07 cloudtik/runtime/sshserver/config/defaults.yaml
--rw-rw-r--  2.0 unx     1742 b- defN 23-Jun-05 07:07 cloudtik/runtime/sshserver/scripts/configure.sh
--rw-rw-r--  2.0 unx      489 b- defN 23-Jun-05 07:07 cloudtik/runtime/sshserver/scripts/install.sh
--rw-rw-r--  2.0 unx     1088 b- defN 23-Jun-05 07:07 cloudtik/runtime/sshserver/scripts/services.sh
+-rw-rw-r--  2.0 unx     3615 b- defN 23-Jun-07 07:37 cloudtik/runtime/spark/runtime.py
+-rw-rw-r--  2.0 unx    15691 b- defN 23-Jun-07 07:37 cloudtik/runtime/spark/scaling_policy.py
+-rw-rw-r--  2.0 unx     2861 b- defN 23-Jun-07 07:37 cloudtik/runtime/spark/scripts.py
+-rw-rw-r--  2.0 unx    18816 b- defN 23-Jul-05 08:32 cloudtik/runtime/spark/utils.py
+-rw-rw-r--  2.0 unx     1281 b- defN 23-Jul-07 08:48 cloudtik/runtime/spark/conf/hadoop/hdfs-site.xml
+-rw-rw-r--  2.0 unx     2960 b- defN 23-Jul-07 08:48 cloudtik/runtime/spark/conf/hadoop/yarn-site.xml
+-rw-rw-r--  2.0 unx     1345 b- defN 23-Jul-07 08:48 cloudtik/runtime/spark/conf/spark/spark-defaults.conf
+-rw-rw-r--  2.0 unx      995 b- defN 23-Jul-07 08:48 cloudtik/runtime/spark/config/commands.yaml
+-rw-rw-r--  2.0 unx       25 b- defN 23-Jul-07 08:48 cloudtik/runtime/spark/config/defaults.yaml
+-rw-rw-r--  2.0 unx      500 b- defN 23-Jun-07 07:37 cloudtik/runtime/spark/scripts/configure.py
+-rw-rw-r--  2.0 unx    17679 b- defN 23-Jul-07 08:48 cloudtik/runtime/spark/scripts/configure.sh
+-rw-rw-r--  2.0 unx     7226 b- defN 23-Jul-07 08:48 cloudtik/runtime/spark/scripts/install.sh
+-rw-rw-r--  2.0 unx     2291 b- defN 23-Jul-07 08:48 cloudtik/runtime/spark/scripts/services.sh
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-07 07:37 cloudtik/runtime/sshserver/__init__.py
+-rw-rw-r--  2.0 unx     1543 b- defN 23-Jun-07 07:37 cloudtik/runtime/sshserver/runtime.py
+-rw-rw-r--  2.0 unx     2493 b- defN 23-Jun-07 07:37 cloudtik/runtime/sshserver/utils.py
+-rw-rw-r--  2.0 unx     3287 b- defN 23-Jul-07 08:48 cloudtik/runtime/sshserver/conf/sshd_config
+-rw-rw-r--  2.0 unx     1035 b- defN 23-Jul-07 08:48 cloudtik/runtime/sshserver/config/commands.yaml
+-rw-rw-r--  2.0 unx       25 b- defN 23-Jul-07 08:48 cloudtik/runtime/sshserver/config/defaults.yaml
+-rw-rw-r--  2.0 unx     2005 b- defN 23-Jul-07 08:48 cloudtik/runtime/sshserver/scripts/configure.sh
+-rw-rw-r--  2.0 unx      489 b- defN 23-Jul-07 08:48 cloudtik/runtime/sshserver/scripts/install.sh
+-rw-rw-r--  2.0 unx     1088 b- defN 23-Jul-07 08:48 cloudtik/runtime/sshserver/scripts/services.sh
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/runtime/trino/__init__.py
--rw-rw-r--  2.0 unx     2803 b- defN 23-Jun-01 05:24 cloudtik/runtime/trino/runtime.py
--rw-rw-r--  2.0 unx     6127 b- defN 23-Jun-01 05:24 cloudtik/runtime/trino/utils.py
--rw-rw-r--  2.0 unx      286 b- defN 23-Jun-05 07:07 cloudtik/runtime/trino/conf/trino/config.properties
--rw-rw-r--  2.0 unx      182 b- defN 23-Jun-05 07:07 cloudtik/runtime/trino/conf/trino/config.worker.properties
--rw-rw-r--  2.0 unx      454 b- defN 23-Jun-05 07:07 cloudtik/runtime/trino/conf/trino/jvm.config
--rw-rw-r--  2.0 unx       90 b- defN 23-Jun-05 07:07 cloudtik/runtime/trino/conf/trino/node.properties
--rw-rw-r--  2.0 unx      374 b- defN 23-Jun-05 07:07 cloudtik/runtime/trino/conf/trino/catalog/gcs.key-file.json
--rw-rw-r--  2.0 unx     1394 b- defN 23-Jun-05 07:07 cloudtik/runtime/trino/conf/trino/catalog/hive-azure-core-site.xml
--rw-rw-r--  2.0 unx       87 b- defN 23-Jun-05 07:07 cloudtik/runtime/trino/conf/trino/catalog/hive.config.properties
--rw-rw-r--  2.0 unx      225 b- defN 23-Jun-05 07:07 cloudtik/runtime/trino/conf/trino/catalog/hive.gcs.properties
--rw-rw-r--  2.0 unx       62 b- defN 23-Jun-05 07:07 cloudtik/runtime/trino/conf/trino/catalog/hive.properties
--rw-rw-r--  2.0 unx      142 b- defN 23-Jun-05 07:07 cloudtik/runtime/trino/conf/trino/catalog/hive.s3.properties
--rw-rw-r--  2.0 unx      995 b- defN 23-Jun-05 07:07 cloudtik/runtime/trino/config/commands.yaml
--rw-rw-r--  2.0 unx      218 b- defN 23-Jun-05 07:07 cloudtik/runtime/trino/config/defaults.yaml
--rw-rw-r--  2.0 unx      731 b- defN 23-Jun-05 07:07 cloudtik/runtime/trino/scripts/configure.py
--rw-rw-r--  2.0 unx    10275 b- defN 23-Jun-05 07:07 cloudtik/runtime/trino/scripts/configure.sh
--rw-rw-r--  2.0 unx     1931 b- defN 23-Jun-05 07:07 cloudtik/runtime/trino/scripts/install.sh
--rw-rw-r--  2.0 unx      673 b- defN 23-Jun-05 07:07 cloudtik/runtime/trino/scripts/services.sh
+-rw-rw-r--  2.0 unx     2803 b- defN 23-Jun-07 07:37 cloudtik/runtime/trino/runtime.py
+-rw-rw-r--  2.0 unx     6127 b- defN 23-Jun-07 07:37 cloudtik/runtime/trino/utils.py
+-rw-rw-r--  2.0 unx      286 b- defN 23-Jul-07 08:48 cloudtik/runtime/trino/conf/trino/config.properties
+-rw-rw-r--  2.0 unx      182 b- defN 23-Jul-07 08:48 cloudtik/runtime/trino/conf/trino/config.worker.properties
+-rw-rw-r--  2.0 unx      454 b- defN 23-Jul-07 08:48 cloudtik/runtime/trino/conf/trino/jvm.config
+-rw-rw-r--  2.0 unx       90 b- defN 23-Jul-07 08:48 cloudtik/runtime/trino/conf/trino/node.properties
+-rw-rw-r--  2.0 unx      374 b- defN 23-Jul-07 08:48 cloudtik/runtime/trino/conf/trino/catalog/gcs.key-file.json
+-rw-rw-r--  2.0 unx     1394 b- defN 23-Jul-07 08:48 cloudtik/runtime/trino/conf/trino/catalog/hive-azure-core-site.xml
+-rw-rw-r--  2.0 unx       87 b- defN 23-Jul-07 08:48 cloudtik/runtime/trino/conf/trino/catalog/hive.config.properties
+-rw-rw-r--  2.0 unx      225 b- defN 23-Jul-07 08:48 cloudtik/runtime/trino/conf/trino/catalog/hive.gcs.properties
+-rw-rw-r--  2.0 unx       62 b- defN 23-Jul-07 08:48 cloudtik/runtime/trino/conf/trino/catalog/hive.properties
+-rw-rw-r--  2.0 unx      142 b- defN 23-Jul-07 08:48 cloudtik/runtime/trino/conf/trino/catalog/hive.s3.properties
+-rw-rw-r--  2.0 unx      995 b- defN 23-Jul-07 08:48 cloudtik/runtime/trino/config/commands.yaml
+-rw-rw-r--  2.0 unx      218 b- defN 23-Jul-07 08:48 cloudtik/runtime/trino/config/defaults.yaml
+-rw-rw-r--  2.0 unx      731 b- defN 23-Jul-07 08:48 cloudtik/runtime/trino/scripts/configure.py
+-rw-rw-r--  2.0 unx    10275 b- defN 23-Jul-07 08:48 cloudtik/runtime/trino/scripts/configure.sh
+-rw-rw-r--  2.0 unx     1931 b- defN 23-Jul-07 08:48 cloudtik/runtime/trino/scripts/install.sh
+-rw-rw-r--  2.0 unx      673 b- defN 23-Jul-07 08:48 cloudtik/runtime/trino/scripts/services.sh
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/runtime/zookeeper/__init__.py
--rw-rw-r--  2.0 unx     2555 b- defN 23-Jun-01 05:24 cloudtik/runtime/zookeeper/runtime.py
--rw-rw-r--  2.0 unx     4568 b- defN 23-Jun-01 05:24 cloudtik/runtime/zookeeper/utils.py
--rw-rw-r--  2.0 unx       18 b- defN 23-Jun-05 07:07 cloudtik/runtime/zookeeper/conf/zookeeper/myid
--rw-rw-r--  2.0 unx     1155 b- defN 23-Jun-05 07:07 cloudtik/runtime/zookeeper/conf/zookeeper/zoo.cfg
--rw-rw-r--  2.0 unx     1035 b- defN 23-Jun-05 07:07 cloudtik/runtime/zookeeper/config/commands.yaml
--rw-rw-r--  2.0 unx       25 b- defN 23-Jun-05 07:07 cloudtik/runtime/zookeeper/config/defaults.yaml
--rw-rw-r--  2.0 unx      704 b- defN 23-Jun-05 07:07 cloudtik/runtime/zookeeper/scripts/configure.py
--rw-rw-r--  2.0 unx     2154 b- defN 23-Jun-05 07:07 cloudtik/runtime/zookeeper/scripts/configure.sh
--rw-rw-r--  2.0 unx     1206 b- defN 23-Jun-05 07:07 cloudtik/runtime/zookeeper/scripts/install.sh
--rw-rw-r--  2.0 unx      918 b- defN 23-Jun-05 07:07 cloudtik/runtime/zookeeper/scripts/services.sh
+-rw-rw-r--  2.0 unx     2555 b- defN 23-Jun-07 07:37 cloudtik/runtime/zookeeper/runtime.py
+-rw-rw-r--  2.0 unx     4568 b- defN 23-Jun-07 07:37 cloudtik/runtime/zookeeper/utils.py
+-rw-rw-r--  2.0 unx       18 b- defN 23-Jul-07 08:48 cloudtik/runtime/zookeeper/conf/zookeeper/myid
+-rw-rw-r--  2.0 unx     1155 b- defN 23-Jul-07 08:48 cloudtik/runtime/zookeeper/conf/zookeeper/zoo.cfg
+-rw-rw-r--  2.0 unx     1035 b- defN 23-Jul-07 08:48 cloudtik/runtime/zookeeper/config/commands.yaml
+-rw-rw-r--  2.0 unx       25 b- defN 23-Jul-07 08:48 cloudtik/runtime/zookeeper/config/defaults.yaml
+-rw-rw-r--  2.0 unx      704 b- defN 23-Jul-07 08:48 cloudtik/runtime/zookeeper/scripts/configure.py
+-rw-rw-r--  2.0 unx     2154 b- defN 23-Jul-07 08:48 cloudtik/runtime/zookeeper/scripts/configure.sh
+-rw-rw-r--  2.0 unx     1206 b- defN 23-Jul-07 08:48 cloudtik/runtime/zookeeper/scripts/install.sh
+-rw-rw-r--  2.0 unx      918 b- defN 23-Jul-07 08:48 cloudtik/runtime/zookeeper/scripts/services.sh
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/scripts/__init__.py
--rw-rw-r--  2.0 unx    23568 b- defN 23-Jun-03 07:38 cloudtik/scripts/head_scripts.py
--rw-rw-r--  2.0 unx    17442 b- defN 23-Jun-03 07:38 cloudtik/scripts/node_scripts.py
--rw-rw-r--  2.0 unx     7781 b- defN 23-May-31 13:06 cloudtik/scripts/runtime_scripts.py
--rw-rw-r--  2.0 unx    37561 b- defN 23-Jun-03 07:38 cloudtik/scripts/scripts.py
+-rw-rw-r--  2.0 unx    24872 b- defN 23-Jun-16 02:24 cloudtik/scripts/head_scripts.py
+-rw-rw-r--  2.0 unx    17137 b- defN 23-Jun-12 06:09 cloudtik/scripts/node_scripts.py
+-rw-rw-r--  2.0 unx     7781 b- defN 23-Jun-07 07:37 cloudtik/scripts/runtime_scripts.py
+-rw-rw-r--  2.0 unx    41089 b- defN 23-Jun-12 06:09 cloudtik/scripts/scripts.py
 -rw-rw-r--  2.0 unx      701 b- defN 23-May-21 03:18 cloudtik/scripts/utils.py
--rw-rw-r--  2.0 unx     7205 b- defN 23-May-21 03:18 cloudtik/scripts/workspace.py
--rw-rw-r--  2.0 unx      249 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/large-highmem.yaml
--rw-rw-r--  2.0 unx      889 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/large.yaml
--rw-rw-r--  2.0 unx      250 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/medium-highmem.yaml
--rw-rw-r--  2.0 unx      771 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/medium.yaml
--rw-rw-r--  2.0 unx      247 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/small-highmem.yaml
--rw-rw-r--  2.0 unx      546 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/small.yaml
--rw-rw-r--  2.0 unx      254 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/standard-highmem.yaml
--rw-rw-r--  2.0 unx      679 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/standard.yaml
--rw-rw-r--  2.0 unx      259 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/very-large-highmem.yaml
--rw-rw-r--  2.0 unx     1000 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/very-large.yaml
--rw-rw-r--  2.0 unx      432 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/gpu/base-2.yaml
--rw-rw-r--  2.0 unx      632 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/gpu/base-4.yaml
--rw-rw-r--  2.0 unx      632 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/gpu/base.yaml
--rw-rw-r--  2.0 unx      277 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/gpu/a100-40/large.yaml
--rw-rw-r--  2.0 unx      279 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/gpu/a100-40/standard.yaml
--rw-rw-r--  2.0 unx      282 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/gpu/a100-40/very-large.yaml
--rw-rw-r--  2.0 unx      278 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/gpu/a100-80/large.yaml
--rw-rw-r--  2.0 unx      280 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/gpu/a100-80/standard.yaml
--rw-rw-r--  2.0 unx      283 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/gpu/a100-80/very-large.yaml
--rw-rw-r--  2.0 unx      271 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/gpu/t4/large.yaml
--rw-rw-r--  2.0 unx      272 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/gpu/t4/medium.yaml
--rw-rw-r--  2.0 unx      346 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/gpu/t4/small.yaml
--rw-rw-r--  2.0 unx      353 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/gpu/t4/standard.yaml
--rw-rw-r--  2.0 unx      350 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/gpu/t4/very-small.yaml
--rw-rw-r--  2.0 unx      276 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/gpu/v100-16/large.yaml
--rw-rw-r--  2.0 unx      279 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/gpu/v100-16/standard.yaml
--rw-rw-r--  2.0 unx      278 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/gpu/v100-16/very-large-x.yaml
--rw-rw-r--  2.0 unx      282 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/gpu/v100-16/very-large.yaml
--rw-rw-r--  2.0 unx      278 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/gpu/v100-32/large.yaml
--rw-rw-r--  2.0 unx      280 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/gpu/v100-32/standard.yaml
--rw-rw-r--  2.0 unx      278 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/gpu/v100-32/very-large.yaml
--rw-rw-r--  2.0 unx      256 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/latest/large-highmem.yaml
--rw-rw-r--  2.0 unx      249 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/latest/large.yaml
--rw-rw-r--  2.0 unx      257 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/latest/medium-highmem.yaml
--rw-rw-r--  2.0 unx      250 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/latest/medium.yaml
--rw-rw-r--  2.0 unx      254 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/latest/small-highmem.yaml
--rw-rw-r--  2.0 unx      247 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/latest/small.yaml
--rw-rw-r--  2.0 unx      261 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/latest/standard-highmem.yaml
--rw-rw-r--  2.0 unx      254 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/latest/standard.yaml
--rw-rw-r--  2.0 unx      266 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/latest/very-large-highmem.yaml
--rw-rw-r--  2.0 unx      254 b- defN 23-Jun-05 07:07 cloudtik/templates/aliyun/latest/very-large.yaml
--rw-rw-r--  2.0 unx      239 b- defN 23-Jun-05 07:07 cloudtik/templates/aws/large-highmem.yaml
--rw-rw-r--  2.0 unx     1650 b- defN 23-Jun-05 07:07 cloudtik/templates/aws/large.yaml
--rw-rw-r--  2.0 unx      240 b- defN 23-Jun-05 07:07 cloudtik/templates/aws/medium-highmem.yaml
--rw-rw-r--  2.0 unx     1270 b- defN 23-Jun-05 07:07 cloudtik/templates/aws/medium.yaml
--rw-rw-r--  2.0 unx      237 b- defN 23-Jun-05 07:07 cloudtik/templates/aws/small-highmem.yaml
--rw-rw-r--  2.0 unx      736 b- defN 23-Jun-05 07:07 cloudtik/templates/aws/small.yaml
--rw-rw-r--  2.0 unx      244 b- defN 23-Jun-05 07:07 cloudtik/templates/aws/standard-highmem.yaml
--rw-rw-r--  2.0 unx     1007 b- defN 23-Jun-05 07:07 cloudtik/templates/aws/standard.yaml
--rw-rw-r--  2.0 unx      249 b- defN 23-Jun-05 07:07 cloudtik/templates/aws/very-large-highmem.yaml
--rw-rw-r--  2.0 unx     1959 b- defN 23-Jun-05 07:07 cloudtik/templates/aws/very-large.yaml
--rw-rw-r--  2.0 unx      442 b- defN 23-Jun-05 07:07 cloudtik/templates/aws/gpu/base.yaml
--rw-rw-r--  2.0 unx      484 b- defN 23-Jun-05 07:07 cloudtik/templates/aws/gpu/a100/very-large.yaml
--rw-rw-r--  2.0 unx      473 b- defN 23-Jun-05 07:07 cloudtik/templates/aws/gpu/t4/large.yaml
--rw-rw-r--  2.0 unx      547 b- defN 23-Jun-05 07:07 cloudtik/templates/aws/gpu/t4/small.yaml
--rw-rw-r--  2.0 unx      551 b- defN 23-Jun-05 07:07 cloudtik/templates/aws/gpu/t4/standard.yaml
--rw-rw-r--  2.0 unx      475 b- defN 23-Jun-05 07:07 cloudtik/templates/aws/gpu/t4/very-large.yaml
--rw-rw-r--  2.0 unx      551 b- defN 23-Jun-05 07:07 cloudtik/templates/aws/gpu/t4/very-small.yaml
--rw-rw-r--  2.0 unx     1085 b- defN 23-Jun-05 07:07 cloudtik/templates/aws/gpu/v100/large.yaml
--rw-rw-r--  2.0 unx      784 b- defN 23-Jun-05 07:07 cloudtik/templates/aws/gpu/v100/standard.yaml
--rw-rw-r--  2.0 unx      485 b- defN 23-Jun-05 07:07 cloudtik/templates/aws/gpu/v100/very-large-x.yaml
--rw-rw-r--  2.0 unx     1091 b- defN 23-Jun-05 07:07 cloudtik/templates/aws/gpu/v100/very-large.yaml
--rw-rw-r--  2.0 unx      247 b- defN 23-Jun-05 07:07 cloudtik/templates/aws/latest/large-highmem.yaml
--rw-rw-r--  2.0 unx      240 b- defN 23-Jun-05 07:07 cloudtik/templates/aws/latest/large.yaml
--rw-rw-r--  2.0 unx      248 b- defN 23-Jun-05 07:07 cloudtik/templates/aws/latest/medium-highmem.yaml
--rw-rw-r--  2.0 unx      241 b- defN 23-Jun-05 07:07 cloudtik/templates/aws/latest/medium.yaml
--rw-rw-r--  2.0 unx      245 b- defN 23-Jun-05 07:07 cloudtik/templates/aws/latest/small-highmem.yaml
--rw-rw-r--  2.0 unx      238 b- defN 23-Jun-05 07:07 cloudtik/templates/aws/latest/small.yaml
--rw-rw-r--  2.0 unx      252 b- defN 23-Jun-05 07:07 cloudtik/templates/aws/latest/standard-highmem.yaml
--rw-rw-r--  2.0 unx      245 b- defN 23-Jun-05 07:07 cloudtik/templates/aws/latest/standard.yaml
--rw-rw-r--  2.0 unx      257 b- defN 23-Jun-05 07:07 cloudtik/templates/aws/latest/very-large-highmem.yaml
--rw-rw-r--  2.0 unx      250 b- defN 23-Jun-05 07:07 cloudtik/templates/aws/latest/very-large.yaml
--rw-rw-r--  2.0 unx      280 b- defN 23-Jun-05 07:07 cloudtik/templates/azure/large-highmem.yaml
--rw-rw-r--  2.0 unx      959 b- defN 23-Jun-05 07:07 cloudtik/templates/azure/large.yaml
--rw-rw-r--  2.0 unx      282 b- defN 23-Jun-05 07:07 cloudtik/templates/azure/medium-highmem.yaml
--rw-rw-r--  2.0 unx      797 b- defN 23-Jun-05 07:07 cloudtik/templates/azure/medium.yaml
--rw-rw-r--  2.0 unx      279 b- defN 23-Jun-05 07:07 cloudtik/templates/azure/small-highmem.yaml
--rw-rw-r--  2.0 unx      481 b- defN 23-Jun-05 07:07 cloudtik/templates/azure/small.yaml
--rw-rw-r--  2.0 unx      285 b- defN 23-Jun-05 07:07 cloudtik/templates/azure/standard-highmem.yaml
--rw-rw-r--  2.0 unx      634 b- defN 23-Jun-05 07:07 cloudtik/templates/azure/standard.yaml
--rw-rw-r--  2.0 unx      285 b- defN 23-Jun-05 07:07 cloudtik/templates/azure/very-large-highmem.yaml
--rw-rw-r--  2.0 unx     1122 b- defN 23-Jun-05 07:07 cloudtik/templates/azure/very-large.yaml
--rw-rw-r--  2.0 unx      296 b- defN 23-Jun-05 07:07 cloudtik/templates/azure/gpu/base.yaml
--rw-rw-r--  2.0 unx      341 b- defN 23-Jun-05 07:07 cloudtik/templates/azure/gpu/a100/large.yaml
--rw-rw-r--  2.0 unx      342 b- defN 23-Jun-05 07:07 cloudtik/templates/azure/gpu/a100/medium.yaml
--rw-rw-r--  2.0 unx      344 b- defN 23-Jun-05 07:07 cloudtik/templates/azure/gpu/a100/standard.yaml
--rw-rw-r--  2.0 unx      342 b- defN 23-Jun-05 07:07 cloudtik/templates/azure/gpu/a100/very-large.yaml
--rw-rw-r--  2.0 unx      331 b- defN 23-Jun-05 07:07 cloudtik/templates/azure/gpu/t4/large.yaml
--rw-rw-r--  2.0 unx      443 b- defN 23-Jun-05 07:07 cloudtik/templates/azure/gpu/t4/small.yaml
--rw-rw-r--  2.0 unx      447 b- defN 23-Jun-05 07:07 cloudtik/templates/azure/gpu/t4/standard.yaml
--rw-rw-r--  2.0 unx      448 b- defN 23-Jun-05 07:07 cloudtik/templates/azure/gpu/t4/very-small.yaml
--rw-rw-r--  2.0 unx      334 b- defN 23-Jun-05 07:07 cloudtik/templates/azure/gpu/v100/large.yaml
--rw-rw-r--  2.0 unx      335 b- defN 23-Jun-05 07:07 cloudtik/templates/azure/gpu/v100/medium.yaml
--rw-rw-r--  2.0 unx      336 b- defN 23-Jun-05 07:07 cloudtik/templates/azure/gpu/v100/standard.yaml
--rw-rw-r--  2.0 unx      287 b- defN 23-Jun-05 07:07 cloudtik/templates/azure/latest/large-highmem.yaml
--rw-rw-r--  2.0 unx      280 b- defN 23-Jun-05 07:07 cloudtik/templates/azure/latest/large.yaml
--rw-rw-r--  2.0 unx      289 b- defN 23-Jun-05 07:07 cloudtik/templates/azure/latest/medium-highmem.yaml
--rw-rw-r--  2.0 unx      282 b- defN 23-Jun-05 07:07 cloudtik/templates/azure/latest/medium.yaml
--rw-rw-r--  2.0 unx      286 b- defN 23-Jun-05 07:07 cloudtik/templates/azure/latest/small-highmem.yaml
--rw-rw-r--  2.0 unx      279 b- defN 23-Jun-05 07:07 cloudtik/templates/azure/latest/small.yaml
--rw-rw-r--  2.0 unx      292 b- defN 23-Jun-05 07:07 cloudtik/templates/azure/latest/standard-highmem.yaml
--rw-rw-r--  2.0 unx      285 b- defN 23-Jun-05 07:07 cloudtik/templates/azure/latest/standard.yaml
--rw-rw-r--  2.0 unx      292 b- defN 23-Jun-05 07:07 cloudtik/templates/azure/latest/very-large-highmem.yaml
--rw-rw-r--  2.0 unx      285 b- defN 23-Jun-05 07:07 cloudtik/templates/azure/latest/very-large.yaml
--rw-rw-r--  2.0 unx      240 b- defN 23-Jun-05 07:07 cloudtik/templates/gcp/large-highmem.yaml
--rw-rw-r--  2.0 unx     1426 b- defN 23-Jun-05 07:07 cloudtik/templates/gcp/large.yaml
--rw-rw-r--  2.0 unx      242 b- defN 23-Jun-05 07:07 cloudtik/templates/gcp/medium-highmem.yaml
--rw-rw-r--  2.0 unx     1210 b- defN 23-Jun-05 07:07 cloudtik/templates/gcp/medium.yaml
--rw-rw-r--  2.0 unx      239 b- defN 23-Jun-05 07:07 cloudtik/templates/gcp/small-highmem.yaml
--rw-rw-r--  2.0 unx      768 b- defN 23-Jun-05 07:07 cloudtik/templates/gcp/small.yaml
--rw-rw-r--  2.0 unx      245 b- defN 23-Jun-05 07:07 cloudtik/templates/gcp/standard-highmem.yaml
--rw-rw-r--  2.0 unx      993 b- defN 23-Jun-05 07:07 cloudtik/templates/gcp/standard.yaml
--rw-rw-r--  2.0 unx      251 b- defN 23-Jun-05 07:07 cloudtik/templates/gcp/very-large-highmem.yaml
--rw-rw-r--  2.0 unx     1649 b- defN 23-Jun-05 07:07 cloudtik/templates/gcp/very-large.yaml
--rw-rw-r--  2.0 unx      793 b- defN 23-Jun-05 07:07 cloudtik/templates/gcp/gpu/base-2.yaml
--rw-rw-r--  2.0 unx     1151 b- defN 23-Jun-05 07:07 cloudtik/templates/gcp/gpu/base-4.yaml
--rw-rw-r--  2.0 unx     1867 b- defN 23-Jun-05 07:07 cloudtik/templates/gcp/gpu/base-8.yaml
--rw-rw-r--  2.0 unx     1206 b- defN 23-Jun-05 07:07 cloudtik/templates/gcp/gpu/base.yaml
--rw-rw-r--  2.0 unx      261 b- defN 23-Jun-05 07:07 cloudtik/templates/gcp/gpu/a100-40/large.yaml
--rw-rw-r--  2.0 unx      261 b- defN 23-Jun-05 07:07 cloudtik/templates/gcp/gpu/a100-40/medium.yaml
--rw-rw-r--  2.0 unx      261 b- defN 23-Jun-05 07:07 cloudtik/templates/gcp/gpu/a100-40/standard.yaml
--rw-rw-r--  2.0 unx      263 b- defN 23-Jun-05 07:07 cloudtik/templates/gcp/gpu/a100-40/utra-large.yaml
--rw-rw-r--  2.0 unx      261 b- defN 23-Jun-05 07:07 cloudtik/templates/gcp/gpu/a100-40/very-large.yaml
--rw-rw-r--  2.0 unx      262 b- defN 23-Jun-05 07:07 cloudtik/templates/gcp/gpu/a100-80/large.yaml
--rw-rw-r--  2.0 unx      262 b- defN 23-Jun-05 07:07 cloudtik/templates/gcp/gpu/a100-80/medium.yaml
--rw-rw-r--  2.0 unx      262 b- defN 23-Jun-05 07:07 cloudtik/templates/gcp/gpu/a100-80/standard.yaml
--rw-rw-r--  2.0 unx      262 b- defN 23-Jun-05 07:07 cloudtik/templates/gcp/gpu/a100-80/very-large.yaml
--rw-rw-r--  2.0 unx      375 b- defN 23-Jun-05 07:07 cloudtik/templates/gcp/gpu/t4/large.yaml
--rw-rw-r--  2.0 unx      375 b- defN 23-Jun-05 07:07 cloudtik/templates/gcp/gpu/t4/medium.yaml
--rw-rw-r--  2.0 unx      450 b- defN 23-Jun-05 07:07 cloudtik/templates/gcp/gpu/t4/small.yaml
--rw-rw-r--  2.0 unx      453 b- defN 23-Jun-05 07:07 cloudtik/templates/gcp/gpu/t4/standard.yaml
--rw-rw-r--  2.0 unx      455 b- defN 23-Jun-05 07:07 cloudtik/templates/gcp/gpu/t4/very-small.yaml
--rw-rw-r--  2.0 unx      380 b- defN 23-Jun-05 07:07 cloudtik/templates/gcp/gpu/v100/large.yaml
--rw-rw-r--  2.0 unx      380 b- defN 23-Jun-05 07:07 cloudtik/templates/gcp/gpu/v100/medium.yaml
--rw-rw-r--  2.0 unx      379 b- defN 23-Jun-05 07:07 cloudtik/templates/gcp/gpu/v100/standard.yaml
--rw-rw-r--  2.0 unx      380 b- defN 23-Jun-05 07:07 cloudtik/templates/gcp/gpu/v100/very-large.yaml
--rw-rw-r--  2.0 unx      258 b- defN 23-Jun-05 07:07 cloudtik/templates/huaweicloud/large-highmem.yaml
--rw-rw-r--  2.0 unx      692 b- defN 23-Jun-05 07:07 cloudtik/templates/huaweicloud/large.yaml
--rw-rw-r--  2.0 unx      258 b- defN 23-Jun-05 07:07 cloudtik/templates/huaweicloud/medium-highmem.yaml
--rw-rw-r--  2.0 unx      630 b- defN 23-Jun-05 07:07 cloudtik/templates/huaweicloud/medium.yaml
--rw-rw-r--  2.0 unx      259 b- defN 23-Jun-05 07:07 cloudtik/templates/huaweicloud/small-highmem.yaml
--rw-rw-r--  2.0 unx      477 b- defN 23-Jun-05 07:07 cloudtik/templates/huaweicloud/small.yaml
--rw-rw-r--  2.0 unx      262 b- defN 23-Jun-05 07:07 cloudtik/templates/huaweicloud/standard-highmem.yaml
--rw-rw-r--  2.0 unx      567 b- defN 23-Jun-05 07:07 cloudtik/templates/huaweicloud/standard.yaml
--rw-rw-r--  2.0 unx      264 b- defN 23-Jun-05 07:07 cloudtik/templates/huaweicloud/very-large-highmem.yaml
--rw-rw-r--  2.0 unx      755 b- defN 23-Jun-05 07:07 cloudtik/templates/huaweicloud/very-large.yaml
--rw-rw-r--  2.0 unx      271 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/large-highmem.yaml
--rw-rw-r--  2.0 unx      386 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/large.yaml
--rw-rw-r--  2.0 unx      272 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/medium-highmem.yaml
--rw-rw-r--  2.0 unx      385 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/medium.yaml
--rw-rw-r--  2.0 unx      270 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/small-highmem.yaml
--rw-rw-r--  2.0 unx      383 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/small.yaml
--rw-rw-r--  2.0 unx      273 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/standard-highmem.yaml
--rw-rw-r--  2.0 unx      383 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/standard.yaml
--rw-rw-r--  2.0 unx      276 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/very-large-highmem.yaml
--rw-rw-r--  2.0 unx      386 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/very-large.yaml
--rw-rw-r--  2.0 unx      309 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/aks/large-highmem.yaml
--rw-rw-r--  2.0 unx      913 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/aks/large.yaml
--rw-rw-r--  2.0 unx      309 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/aks/medium-highmem.yaml
--rw-rw-r--  2.0 unx      802 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/aks/medium.yaml
--rw-rw-r--  2.0 unx      323 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/aks/small-highmem.yaml
--rw-rw-r--  2.0 unx      766 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/aks/small.yaml
--rw-rw-r--  2.0 unx      309 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/aks/standard-highmem.yaml
--rw-rw-r--  2.0 unx      690 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/aks/standard.yaml
--rw-rw-r--  2.0 unx      314 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/aks/very-large-highmem.yaml
--rw-rw-r--  2.0 unx      913 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/aks/very-large.yaml
--rw-rw-r--  2.0 unx      275 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/eks/large-highmem.yaml
--rw-rw-r--  2.0 unx      879 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/eks/large.yaml
--rw-rw-r--  2.0 unx      276 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/eks/medium-highmem.yaml
--rw-rw-r--  2.0 unx      776 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/eks/medium.yaml
--rw-rw-r--  2.0 unx      274 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/eks/small-highmem.yaml
--rw-rw-r--  2.0 unx      672 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/eks/small.yaml
--rw-rw-r--  2.0 unx      277 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/eks/standard-highmem.yaml
--rw-rw-r--  2.0 unx      672 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/eks/standard.yaml
--rw-rw-r--  2.0 unx      280 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/eks/very-large-highmem.yaml
--rw-rw-r--  2.0 unx      879 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/eks/very-large.yaml
--rw-rw-r--  2.0 unx      306 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/gke/large-highmem.yaml
--rw-rw-r--  2.0 unx      972 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/gke/large.yaml
--rw-rw-r--  2.0 unx      307 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/gke/medium-highmem.yaml
--rw-rw-r--  2.0 unx      859 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/gke/medium.yaml
--rw-rw-r--  2.0 unx      303 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/gke/small-highmem.yaml
--rw-rw-r--  2.0 unx      747 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/gke/small.yaml
--rw-rw-r--  2.0 unx      306 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/gke/standard-highmem.yaml
--rw-rw-r--  2.0 unx      747 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/gke/standard.yaml
--rw-rw-r--  2.0 unx      311 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/gke/very-large-highmem.yaml
--rw-rw-r--  2.0 unx      972 b- defN 23-Jun-05 07:07 cloudtik/templates/kubernetes/gke/very-large.yaml
+-rw-rw-r--  2.0 unx     7205 b- defN 23-Jun-07 07:37 cloudtik/scripts/workspace.py
+-rw-rw-r--  2.0 unx      249 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/large-highmem.yaml
+-rw-rw-r--  2.0 unx      889 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/large.yaml
+-rw-rw-r--  2.0 unx      250 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/medium-highmem.yaml
+-rw-rw-r--  2.0 unx      771 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/medium.yaml
+-rw-rw-r--  2.0 unx      247 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/small-highmem.yaml
+-rw-rw-r--  2.0 unx      546 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/small.yaml
+-rw-rw-r--  2.0 unx      254 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/standard-highmem.yaml
+-rw-rw-r--  2.0 unx      679 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/standard.yaml
+-rw-rw-r--  2.0 unx      259 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/very-large-highmem.yaml
+-rw-rw-r--  2.0 unx     1000 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/very-large.yaml
+-rw-rw-r--  2.0 unx      432 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/gpu/base-2.yaml
+-rw-rw-r--  2.0 unx      632 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/gpu/base-4.yaml
+-rw-rw-r--  2.0 unx      632 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/gpu/base.yaml
+-rw-rw-r--  2.0 unx      277 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/gpu/a100-40/large.yaml
+-rw-rw-r--  2.0 unx      279 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/gpu/a100-40/standard.yaml
+-rw-rw-r--  2.0 unx      282 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/gpu/a100-40/very-large.yaml
+-rw-rw-r--  2.0 unx      278 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/gpu/a100-80/large.yaml
+-rw-rw-r--  2.0 unx      280 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/gpu/a100-80/standard.yaml
+-rw-rw-r--  2.0 unx      283 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/gpu/a100-80/very-large.yaml
+-rw-rw-r--  2.0 unx      271 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/gpu/t4/large.yaml
+-rw-rw-r--  2.0 unx      272 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/gpu/t4/medium.yaml
+-rw-rw-r--  2.0 unx      346 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/gpu/t4/small.yaml
+-rw-rw-r--  2.0 unx      353 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/gpu/t4/standard.yaml
+-rw-rw-r--  2.0 unx      350 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/gpu/t4/very-small.yaml
+-rw-rw-r--  2.0 unx      276 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/gpu/v100-16/large.yaml
+-rw-rw-r--  2.0 unx      279 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/gpu/v100-16/standard.yaml
+-rw-rw-r--  2.0 unx      278 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/gpu/v100-16/very-large-x.yaml
+-rw-rw-r--  2.0 unx      282 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/gpu/v100-16/very-large.yaml
+-rw-rw-r--  2.0 unx      278 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/gpu/v100-32/large.yaml
+-rw-rw-r--  2.0 unx      280 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/gpu/v100-32/standard.yaml
+-rw-rw-r--  2.0 unx      278 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/gpu/v100-32/very-large.yaml
+-rw-rw-r--  2.0 unx      256 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/latest/large-highmem.yaml
+-rw-rw-r--  2.0 unx      249 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/latest/large.yaml
+-rw-rw-r--  2.0 unx      257 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/latest/medium-highmem.yaml
+-rw-rw-r--  2.0 unx      250 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/latest/medium.yaml
+-rw-rw-r--  2.0 unx      254 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/latest/small-highmem.yaml
+-rw-rw-r--  2.0 unx      247 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/latest/small.yaml
+-rw-rw-r--  2.0 unx      261 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/latest/standard-highmem.yaml
+-rw-rw-r--  2.0 unx      254 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/latest/standard.yaml
+-rw-rw-r--  2.0 unx      266 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/latest/very-large-highmem.yaml
+-rw-rw-r--  2.0 unx      254 b- defN 23-Jul-07 08:48 cloudtik/templates/aliyun/latest/very-large.yaml
+-rw-rw-r--  2.0 unx      239 b- defN 23-Jul-07 08:48 cloudtik/templates/aws/large-highmem.yaml
+-rw-rw-r--  2.0 unx     1650 b- defN 23-Jul-07 08:48 cloudtik/templates/aws/large.yaml
+-rw-rw-r--  2.0 unx      240 b- defN 23-Jul-07 08:48 cloudtik/templates/aws/medium-highmem.yaml
+-rw-rw-r--  2.0 unx     1270 b- defN 23-Jul-07 08:48 cloudtik/templates/aws/medium.yaml
+-rw-rw-r--  2.0 unx      237 b- defN 23-Jul-07 08:48 cloudtik/templates/aws/small-highmem.yaml
+-rw-rw-r--  2.0 unx      736 b- defN 23-Jul-07 08:48 cloudtik/templates/aws/small.yaml
+-rw-rw-r--  2.0 unx      244 b- defN 23-Jul-07 08:48 cloudtik/templates/aws/standard-highmem.yaml
+-rw-rw-r--  2.0 unx     1007 b- defN 23-Jul-07 08:48 cloudtik/templates/aws/standard.yaml
+-rw-rw-r--  2.0 unx      249 b- defN 23-Jul-07 08:48 cloudtik/templates/aws/very-large-highmem.yaml
+-rw-rw-r--  2.0 unx     1959 b- defN 23-Jul-07 08:48 cloudtik/templates/aws/very-large.yaml
+-rw-rw-r--  2.0 unx      442 b- defN 23-Jul-07 08:48 cloudtik/templates/aws/gpu/base.yaml
+-rw-rw-r--  2.0 unx      484 b- defN 23-Jul-07 08:48 cloudtik/templates/aws/gpu/a100/very-large.yaml
+-rw-rw-r--  2.0 unx      473 b- defN 23-Jul-07 08:48 cloudtik/templates/aws/gpu/t4/large.yaml
+-rw-rw-r--  2.0 unx      547 b- defN 23-Jul-07 08:48 cloudtik/templates/aws/gpu/t4/small.yaml
+-rw-rw-r--  2.0 unx      551 b- defN 23-Jul-07 08:48 cloudtik/templates/aws/gpu/t4/standard.yaml
+-rw-rw-r--  2.0 unx      475 b- defN 23-Jul-07 08:48 cloudtik/templates/aws/gpu/t4/very-large.yaml
+-rw-rw-r--  2.0 unx      551 b- defN 23-Jul-07 08:48 cloudtik/templates/aws/gpu/t4/very-small.yaml
+-rw-rw-r--  2.0 unx     1085 b- defN 23-Jul-07 08:48 cloudtik/templates/aws/gpu/v100/large.yaml
+-rw-rw-r--  2.0 unx      784 b- defN 23-Jul-07 08:48 cloudtik/templates/aws/gpu/v100/standard.yaml
+-rw-rw-r--  2.0 unx      485 b- defN 23-Jul-07 08:48 cloudtik/templates/aws/gpu/v100/very-large-x.yaml
+-rw-rw-r--  2.0 unx     1091 b- defN 23-Jul-07 08:48 cloudtik/templates/aws/gpu/v100/very-large.yaml
+-rw-rw-r--  2.0 unx      247 b- defN 23-Jul-07 08:48 cloudtik/templates/aws/latest/large-highmem.yaml
+-rw-rw-r--  2.0 unx      240 b- defN 23-Jul-07 08:48 cloudtik/templates/aws/latest/large.yaml
+-rw-rw-r--  2.0 unx      248 b- defN 23-Jul-07 08:48 cloudtik/templates/aws/latest/medium-highmem.yaml
+-rw-rw-r--  2.0 unx      241 b- defN 23-Jul-07 08:48 cloudtik/templates/aws/latest/medium.yaml
+-rw-rw-r--  2.0 unx      245 b- defN 23-Jul-07 08:48 cloudtik/templates/aws/latest/small-highmem.yaml
+-rw-rw-r--  2.0 unx      238 b- defN 23-Jul-07 08:48 cloudtik/templates/aws/latest/small.yaml
+-rw-rw-r--  2.0 unx      252 b- defN 23-Jul-07 08:48 cloudtik/templates/aws/latest/standard-highmem.yaml
+-rw-rw-r--  2.0 unx      245 b- defN 23-Jul-07 08:48 cloudtik/templates/aws/latest/standard.yaml
+-rw-rw-r--  2.0 unx      257 b- defN 23-Jul-07 08:48 cloudtik/templates/aws/latest/very-large-highmem.yaml
+-rw-rw-r--  2.0 unx      250 b- defN 23-Jul-07 08:48 cloudtik/templates/aws/latest/very-large.yaml
+-rw-rw-r--  2.0 unx      280 b- defN 23-Jul-07 08:48 cloudtik/templates/azure/large-highmem.yaml
+-rw-rw-r--  2.0 unx      959 b- defN 23-Jul-07 08:48 cloudtik/templates/azure/large.yaml
+-rw-rw-r--  2.0 unx      282 b- defN 23-Jul-07 08:48 cloudtik/templates/azure/medium-highmem.yaml
+-rw-rw-r--  2.0 unx      797 b- defN 23-Jul-07 08:48 cloudtik/templates/azure/medium.yaml
+-rw-rw-r--  2.0 unx      279 b- defN 23-Jul-07 08:48 cloudtik/templates/azure/small-highmem.yaml
+-rw-rw-r--  2.0 unx      481 b- defN 23-Jul-07 08:48 cloudtik/templates/azure/small.yaml
+-rw-rw-r--  2.0 unx      285 b- defN 23-Jul-07 08:48 cloudtik/templates/azure/standard-highmem.yaml
+-rw-rw-r--  2.0 unx      634 b- defN 23-Jul-07 08:48 cloudtik/templates/azure/standard.yaml
+-rw-rw-r--  2.0 unx      285 b- defN 23-Jul-07 08:48 cloudtik/templates/azure/very-large-highmem.yaml
+-rw-rw-r--  2.0 unx     1122 b- defN 23-Jul-07 08:48 cloudtik/templates/azure/very-large.yaml
+-rw-rw-r--  2.0 unx      296 b- defN 23-Jul-07 08:48 cloudtik/templates/azure/gpu/base.yaml
+-rw-rw-r--  2.0 unx      341 b- defN 23-Jul-07 08:48 cloudtik/templates/azure/gpu/a100/large.yaml
+-rw-rw-r--  2.0 unx      342 b- defN 23-Jul-07 08:48 cloudtik/templates/azure/gpu/a100/medium.yaml
+-rw-rw-r--  2.0 unx      344 b- defN 23-Jul-07 08:48 cloudtik/templates/azure/gpu/a100/standard.yaml
+-rw-rw-r--  2.0 unx      342 b- defN 23-Jul-07 08:48 cloudtik/templates/azure/gpu/a100/very-large.yaml
+-rw-rw-r--  2.0 unx      331 b- defN 23-Jul-07 08:48 cloudtik/templates/azure/gpu/t4/large.yaml
+-rw-rw-r--  2.0 unx      443 b- defN 23-Jul-07 08:48 cloudtik/templates/azure/gpu/t4/small.yaml
+-rw-rw-r--  2.0 unx      447 b- defN 23-Jul-07 08:48 cloudtik/templates/azure/gpu/t4/standard.yaml
+-rw-rw-r--  2.0 unx      448 b- defN 23-Jul-07 08:48 cloudtik/templates/azure/gpu/t4/very-small.yaml
+-rw-rw-r--  2.0 unx      334 b- defN 23-Jul-07 08:48 cloudtik/templates/azure/gpu/v100/large.yaml
+-rw-rw-r--  2.0 unx      335 b- defN 23-Jul-07 08:48 cloudtik/templates/azure/gpu/v100/medium.yaml
+-rw-rw-r--  2.0 unx      336 b- defN 23-Jul-07 08:48 cloudtik/templates/azure/gpu/v100/standard.yaml
+-rw-rw-r--  2.0 unx      287 b- defN 23-Jul-07 08:48 cloudtik/templates/azure/latest/large-highmem.yaml
+-rw-rw-r--  2.0 unx      280 b- defN 23-Jul-07 08:48 cloudtik/templates/azure/latest/large.yaml
+-rw-rw-r--  2.0 unx      289 b- defN 23-Jul-07 08:48 cloudtik/templates/azure/latest/medium-highmem.yaml
+-rw-rw-r--  2.0 unx      282 b- defN 23-Jul-07 08:48 cloudtik/templates/azure/latest/medium.yaml
+-rw-rw-r--  2.0 unx      286 b- defN 23-Jul-07 08:48 cloudtik/templates/azure/latest/small-highmem.yaml
+-rw-rw-r--  2.0 unx      279 b- defN 23-Jul-07 08:48 cloudtik/templates/azure/latest/small.yaml
+-rw-rw-r--  2.0 unx      292 b- defN 23-Jul-07 08:48 cloudtik/templates/azure/latest/standard-highmem.yaml
+-rw-rw-r--  2.0 unx      285 b- defN 23-Jul-07 08:48 cloudtik/templates/azure/latest/standard.yaml
+-rw-rw-r--  2.0 unx      292 b- defN 23-Jul-07 08:48 cloudtik/templates/azure/latest/very-large-highmem.yaml
+-rw-rw-r--  2.0 unx      285 b- defN 23-Jul-07 08:48 cloudtik/templates/azure/latest/very-large.yaml
+-rw-rw-r--  2.0 unx      240 b- defN 23-Jul-07 08:48 cloudtik/templates/gcp/large-highmem.yaml
+-rw-rw-r--  2.0 unx     1426 b- defN 23-Jul-07 08:48 cloudtik/templates/gcp/large.yaml
+-rw-rw-r--  2.0 unx      242 b- defN 23-Jul-07 08:48 cloudtik/templates/gcp/medium-highmem.yaml
+-rw-rw-r--  2.0 unx     1210 b- defN 23-Jul-07 08:48 cloudtik/templates/gcp/medium.yaml
+-rw-rw-r--  2.0 unx      239 b- defN 23-Jul-07 08:48 cloudtik/templates/gcp/small-highmem.yaml
+-rw-rw-r--  2.0 unx      768 b- defN 23-Jul-07 08:48 cloudtik/templates/gcp/small.yaml
+-rw-rw-r--  2.0 unx      245 b- defN 23-Jul-07 08:48 cloudtik/templates/gcp/standard-highmem.yaml
+-rw-rw-r--  2.0 unx      993 b- defN 23-Jul-07 08:48 cloudtik/templates/gcp/standard.yaml
+-rw-rw-r--  2.0 unx      251 b- defN 23-Jul-07 08:48 cloudtik/templates/gcp/very-large-highmem.yaml
+-rw-rw-r--  2.0 unx     1649 b- defN 23-Jul-07 08:48 cloudtik/templates/gcp/very-large.yaml
+-rw-rw-r--  2.0 unx      793 b- defN 23-Jul-07 08:48 cloudtik/templates/gcp/gpu/base-2.yaml
+-rw-rw-r--  2.0 unx     1151 b- defN 23-Jul-07 08:48 cloudtik/templates/gcp/gpu/base-4.yaml
+-rw-rw-r--  2.0 unx     1867 b- defN 23-Jul-07 08:48 cloudtik/templates/gcp/gpu/base-8.yaml
+-rw-rw-r--  2.0 unx     1206 b- defN 23-Jul-07 08:48 cloudtik/templates/gcp/gpu/base.yaml
+-rw-rw-r--  2.0 unx      261 b- defN 23-Jul-07 08:48 cloudtik/templates/gcp/gpu/a100-40/large.yaml
+-rw-rw-r--  2.0 unx      261 b- defN 23-Jul-07 08:48 cloudtik/templates/gcp/gpu/a100-40/medium.yaml
+-rw-rw-r--  2.0 unx      261 b- defN 23-Jul-07 08:48 cloudtik/templates/gcp/gpu/a100-40/standard.yaml
+-rw-rw-r--  2.0 unx      263 b- defN 23-Jul-07 08:48 cloudtik/templates/gcp/gpu/a100-40/utra-large.yaml
+-rw-rw-r--  2.0 unx      261 b- defN 23-Jul-07 08:48 cloudtik/templates/gcp/gpu/a100-40/very-large.yaml
+-rw-rw-r--  2.0 unx      262 b- defN 23-Jul-07 08:48 cloudtik/templates/gcp/gpu/a100-80/large.yaml
+-rw-rw-r--  2.0 unx      262 b- defN 23-Jul-07 08:48 cloudtik/templates/gcp/gpu/a100-80/medium.yaml
+-rw-rw-r--  2.0 unx      262 b- defN 23-Jul-07 08:48 cloudtik/templates/gcp/gpu/a100-80/standard.yaml
+-rw-rw-r--  2.0 unx      262 b- defN 23-Jul-07 08:48 cloudtik/templates/gcp/gpu/a100-80/very-large.yaml
+-rw-rw-r--  2.0 unx      375 b- defN 23-Jul-07 08:48 cloudtik/templates/gcp/gpu/t4/large.yaml
+-rw-rw-r--  2.0 unx      375 b- defN 23-Jul-07 08:48 cloudtik/templates/gcp/gpu/t4/medium.yaml
+-rw-rw-r--  2.0 unx      450 b- defN 23-Jul-07 08:48 cloudtik/templates/gcp/gpu/t4/small.yaml
+-rw-rw-r--  2.0 unx      453 b- defN 23-Jul-07 08:48 cloudtik/templates/gcp/gpu/t4/standard.yaml
+-rw-rw-r--  2.0 unx      455 b- defN 23-Jul-07 08:48 cloudtik/templates/gcp/gpu/t4/very-small.yaml
+-rw-rw-r--  2.0 unx      380 b- defN 23-Jul-07 08:48 cloudtik/templates/gcp/gpu/v100/large.yaml
+-rw-rw-r--  2.0 unx      380 b- defN 23-Jul-07 08:48 cloudtik/templates/gcp/gpu/v100/medium.yaml
+-rw-rw-r--  2.0 unx      379 b- defN 23-Jul-07 08:48 cloudtik/templates/gcp/gpu/v100/standard.yaml
+-rw-rw-r--  2.0 unx      380 b- defN 23-Jul-07 08:48 cloudtik/templates/gcp/gpu/v100/very-large.yaml
+-rw-rw-r--  2.0 unx      258 b- defN 23-Jul-07 08:48 cloudtik/templates/huaweicloud/large-highmem.yaml
+-rw-rw-r--  2.0 unx      692 b- defN 23-Jul-07 08:48 cloudtik/templates/huaweicloud/large.yaml
+-rw-rw-r--  2.0 unx      258 b- defN 23-Jul-07 08:48 cloudtik/templates/huaweicloud/medium-highmem.yaml
+-rw-rw-r--  2.0 unx      630 b- defN 23-Jul-07 08:48 cloudtik/templates/huaweicloud/medium.yaml
+-rw-rw-r--  2.0 unx      259 b- defN 23-Jul-07 08:48 cloudtik/templates/huaweicloud/small-highmem.yaml
+-rw-rw-r--  2.0 unx      477 b- defN 23-Jul-07 08:48 cloudtik/templates/huaweicloud/small.yaml
+-rw-rw-r--  2.0 unx      262 b- defN 23-Jul-07 08:48 cloudtik/templates/huaweicloud/standard-highmem.yaml
+-rw-rw-r--  2.0 unx      567 b- defN 23-Jul-07 08:48 cloudtik/templates/huaweicloud/standard.yaml
+-rw-rw-r--  2.0 unx      264 b- defN 23-Jul-07 08:48 cloudtik/templates/huaweicloud/very-large-highmem.yaml
+-rw-rw-r--  2.0 unx      755 b- defN 23-Jul-07 08:48 cloudtik/templates/huaweicloud/very-large.yaml
+-rw-rw-r--  2.0 unx      271 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/large-highmem.yaml
+-rw-rw-r--  2.0 unx      386 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/large.yaml
+-rw-rw-r--  2.0 unx      272 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/medium-highmem.yaml
+-rw-rw-r--  2.0 unx      385 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/medium.yaml
+-rw-rw-r--  2.0 unx      270 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/small-highmem.yaml
+-rw-rw-r--  2.0 unx      383 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/small.yaml
+-rw-rw-r--  2.0 unx      273 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/standard-highmem.yaml
+-rw-rw-r--  2.0 unx      383 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/standard.yaml
+-rw-rw-r--  2.0 unx      276 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/very-large-highmem.yaml
+-rw-rw-r--  2.0 unx      386 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/very-large.yaml
+-rw-rw-r--  2.0 unx      309 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/aks/large-highmem.yaml
+-rw-rw-r--  2.0 unx      913 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/aks/large.yaml
+-rw-rw-r--  2.0 unx      309 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/aks/medium-highmem.yaml
+-rw-rw-r--  2.0 unx      802 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/aks/medium.yaml
+-rw-rw-r--  2.0 unx      323 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/aks/small-highmem.yaml
+-rw-rw-r--  2.0 unx      766 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/aks/small.yaml
+-rw-rw-r--  2.0 unx      309 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/aks/standard-highmem.yaml
+-rw-rw-r--  2.0 unx      690 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/aks/standard.yaml
+-rw-rw-r--  2.0 unx      314 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/aks/very-large-highmem.yaml
+-rw-rw-r--  2.0 unx      913 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/aks/very-large.yaml
+-rw-rw-r--  2.0 unx      275 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/eks/large-highmem.yaml
+-rw-rw-r--  2.0 unx      879 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/eks/large.yaml
+-rw-rw-r--  2.0 unx      276 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/eks/medium-highmem.yaml
+-rw-rw-r--  2.0 unx      776 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/eks/medium.yaml
+-rw-rw-r--  2.0 unx      274 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/eks/small-highmem.yaml
+-rw-rw-r--  2.0 unx      672 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/eks/small.yaml
+-rw-rw-r--  2.0 unx      277 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/eks/standard-highmem.yaml
+-rw-rw-r--  2.0 unx      672 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/eks/standard.yaml
+-rw-rw-r--  2.0 unx      280 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/eks/very-large-highmem.yaml
+-rw-rw-r--  2.0 unx      879 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/eks/very-large.yaml
+-rw-rw-r--  2.0 unx      306 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/gke/large-highmem.yaml
+-rw-rw-r--  2.0 unx      972 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/gke/large.yaml
+-rw-rw-r--  2.0 unx      307 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/gke/medium-highmem.yaml
+-rw-rw-r--  2.0 unx      859 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/gke/medium.yaml
+-rw-rw-r--  2.0 unx      303 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/gke/small-highmem.yaml
+-rw-rw-r--  2.0 unx      747 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/gke/small.yaml
+-rw-rw-r--  2.0 unx      306 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/gke/standard-highmem.yaml
+-rw-rw-r--  2.0 unx      747 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/gke/standard.yaml
+-rw-rw-r--  2.0 unx      311 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/gke/very-large-highmem.yaml
+-rw-rw-r--  2.0 unx      972 b- defN 23-Jul-07 08:48 cloudtik/templates/kubernetes/gke/very-large.yaml
 -rw-rw-r--  2.0 unx        1 b- defN 23-May-21 03:18 cloudtik/tests/__init__.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-May-21 03:18 cloudtik/tests/integration/__init__.py
--rw-rw-r--  2.0 unx     3564 b- defN 23-May-21 03:18 cloudtik/tests/integration/basic_test.py
--rw-rw-r--  2.0 unx     3427 b- defN 23-May-21 03:18 cloudtik/tests/integration/conftest.py
--rw-rw-r--  2.0 unx     2103 b- defN 23-May-21 03:18 cloudtik/tests/integration/constants.py
--rw-rw-r--  2.0 unx     1485 b- defN 23-May-21 03:18 cloudtik/tests/integration/test_aliyun_cluster.py
+-rw-rw-r--  2.0 unx     3564 b- defN 23-Jun-07 07:37 cloudtik/tests/integration/basic_test.py
+-rw-rw-r--  2.0 unx     3427 b- defN 23-Jun-07 07:37 cloudtik/tests/integration/conftest.py
+-rw-rw-r--  2.0 unx     2103 b- defN 23-Jun-07 07:37 cloudtik/tests/integration/constants.py
+-rw-rw-r--  2.0 unx     1485 b- defN 23-Jun-07 07:37 cloudtik/tests/integration/test_aliyun_cluster.py
 -rw-rw-r--  2.0 unx     1425 b- defN 23-May-21 03:18 cloudtik/tests/integration/test_aws_cluster.py
 -rw-rw-r--  2.0 unx     1451 b- defN 23-May-21 03:18 cloudtik/tests/integration/test_azure_cluster.py
 -rw-rw-r--  2.0 unx     1425 b- defN 23-May-21 03:18 cloudtik/tests/integration/test_gcp_cluster.py
--rw-rw-r--  2.0 unx     1548 b- defN 23-May-21 03:18 cloudtik/tests/integration/test_huaweicloud_cluster.py
+-rw-rw-r--  2.0 unx     1548 b- defN 23-Jun-07 07:37 cloudtik/tests/integration/test_huaweicloud_cluster.py
 -rw-rw-r--  2.0 unx        1 b- defN 23-May-21 03:18 cloudtik/tests/unit/__init__.py
--rw-rw-r--  2.0 unx    65497 b- defN 23-Jun-02 10:25 cloudtik/tests/unit/test_cloudtik.py
+-rw-rw-r--  2.0 unx    66777 b- defN 23-Jun-11 03:43 cloudtik/tests/unit/test_cloudtik.py
 -rw-rw-r--  2.0 unx        1 b- defN 23-May-21 03:18 cloudtik/tests/unit/aws/__init__.py
 -rw-rw-r--  2.0 unx     1024 b- defN 23-May-21 03:18 cloudtik/tests/unit/aws/conftest.py
 -rw-rw-r--  2.0 unx     2285 b- defN 23-May-21 03:18 cloudtik/tests/unit/aws/test_aws_batch_tag_update.py
 -rw-rw-r--  2.0 unx        1 b- defN 23-May-21 03:18 cloudtik/tests/unit/core/__init__.py
 -rw-rw-r--  2.0 unx     1079 b- defN 23-May-21 03:18 cloudtik/tests/unit/core/test_crypto.py
 -rw-rw-r--  2.0 unx     1481 b- defN 23-May-21 03:18 cloudtik/tests/unit/core/test_job_waiter.py
--rw-rw-r--  2.0 unx     2340 b- defN 23-May-31 06:06 cloudtik/tests/unit/core/test_provider.py
--rw-rw-r--  2.0 unx    17545 b- defN 23-May-21 03:18 cloudtik/tests/unit/core/test_scaling_policy.py
--rw-rw-r--  2.0 unx    13036 b- defN 23-Jun-05 05:24 cloudtik/tests/unit/core/test_utils.py
+-rw-rw-r--  2.0 unx     2340 b- defN 23-Jun-07 07:37 cloudtik/tests/unit/core/test_provider.py
+-rw-rw-r--  2.0 unx    17545 b- defN 23-Jun-07 07:37 cloudtik/tests/unit/core/test_scaling_policy.py
+-rw-rw-r--  2.0 unx    13036 b- defN 23-Jun-07 07:37 cloudtik/tests/unit/core/test_utils.py
 -rw-rw-r--  2.0 unx        1 b- defN 23-May-21 03:18 cloudtik/tests/unit/core/state/__init__.py
 -rw-rw-r--  2.0 unx     3334 b- defN 23-May-21 03:18 cloudtik/tests/unit/core/state/test_state_table.py
 -rw-rw-r--  2.0 unx        1 b- defN 23-May-21 03:18 cloudtik/tests/unit/gcp/__init__.py
 -rw-rw-r--  2.0 unx     3635 b- defN 23-May-21 03:18 cloudtik/tests/unit/gcp/test_gcp_node_provider.py
--rw-rw-r--  2.0 unx    22509 b- defN 23-Jun-05 07:07 cloudtik-1.1.0.dist-info/METADATA
--rw-rw-r--  2.0 unx      103 b- defN 23-Jun-05 07:07 cloudtik-1.1.0.dist-info/WHEEL
--rw-rw-r--  2.0 unx      392 b- defN 23-Jun-05 07:07 cloudtik-1.1.0.dist-info/entry_points.txt
--rw-rw-r--  2.0 unx        9 b- defN 23-Jun-05 07:07 cloudtik-1.1.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    79690 b- defN 23-Jun-05 07:07 cloudtik-1.1.0.dist-info/RECORD
-732 files, 19826200 bytes uncompressed, 6048719 bytes compressed:  69.5%
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-07 07:37 cloudtik/tests/unit/runtime/__init__.py
+-rw-rw-r--  2.0 unx     4892 b- defN 23-Jun-07 07:37 cloudtik/tests/unit/runtime/test_ai_utils.py
+-rw-rw-r--  2.0 unx    22665 b- defN 23-Jul-07 08:48 cloudtik-1.2.0.dist-info/METADATA
+-rw-rw-r--  2.0 unx      103 b- defN 23-Jul-07 08:48 cloudtik-1.2.0.dist-info/WHEEL
+-rw-rw-r--  2.0 unx      392 b- defN 23-Jul-07 08:48 cloudtik-1.2.0.dist-info/entry_points.txt
+-rw-rw-r--  2.0 unx        9 b- defN 23-Jul-07 08:48 cloudtik-1.2.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    88719 b- defN 23-Jul-07 08:48 cloudtik-1.2.0.dist-info/RECORD
+793 files, 20039821 bytes uncompressed, 6107681 bytes compressed:  69.5%
```

## zipnote {}

```diff
@@ -81,14 +81,17 @@
 
 Filename: cloudtik/core/_private/resource_spec.py
 Comment: 
 
 Filename: cloudtik/core/_private/runtime_factory.py
 Comment: 
 
+Filename: cloudtik/core/_private/script_registry.py
+Comment: 
+
 Filename: cloudtik/core/_private/services.py
 Comment: 
 
 Filename: cloudtik/core/_private/subprocess_output_util.py
 Comment: 
 
 Filename: cloudtik/core/_private/utils.py
@@ -687,48 +690,72 @@
 
 Filename: cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/__init__.py
 Comment: 
 
 Filename: cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/requirements.txt
 Comment: 
 
-Filename: cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/config/data-preprocessing-config.yaml
+Filename: cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/config/data-processing-config.yaml
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/config/dataset-config.yaml
 Comment: 
 
 Filename: cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/config/training-config.yaml
 Comment: 
 
 Filename: cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/__init__.py
 Comment: 
 
-Filename: cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/process_data.py
+Filename: cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/run.py
 Comment: 
 
-Filename: cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/run.py
+Filename: cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/utils.py
 Comment: 
 
-Filename: cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/train_ray.py
+Filename: cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/data/__init__.py
 Comment: 
 
-Filename: cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/train_spark.py
+Filename: cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/data/data_splitting.py
 Comment: 
 
-Filename: cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/utils.py
+Filename: cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/data/data_transform.py
 Comment: 
 
-Filename: cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/data_processing/__init__.py
+Filename: cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/data/post_transform.py
 Comment: 
 
-Filename: cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/data_processing/data_splitting.py
+Filename: cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/data/process.py
 Comment: 
 
-Filename: cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/data_processing/data_transform.py
+Filename: cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/model/__init__.py
 Comment: 
 
-Filename: cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/data_processing/post_transform.py
+Filename: cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/model/model.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/model/predictor.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/model/trainer.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/model/ray/__init__.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/model/ray/model.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/model/ray/trainer.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/model/spark/__init__.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/model/spark/trainer.py
 Comment: 
 
 Filename: cloudtik/runtime/ai/modeling/graph_modeling/__init__.py
 Comment: 
 
 Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/__init__.py
 Comment: 
@@ -738,39 +765,162 @@
 
 Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/__init__.py
 Comment: 
 
 Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/build_graph.py
 Comment: 
 
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/embeddings.py
+Comment: 
+
 Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/launch.py
 Comment: 
 
-Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/map_embeddings.py
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/partition_graph.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/run.py
 Comment: 
 
-Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/map_embeddings_single.py
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/tokenizer.py
 Comment: 
 
-Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/partition_graph.py
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/utils.py
 Comment: 
 
-Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/process_data.py
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/data/__init__.py
 Comment: 
 
-Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/run.py
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/data/process.py
 Comment: 
 
-Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/train_graph_sage.py
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/__init__.py
 Comment: 
 
-Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/train_graph_sage_single.py
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/utils.py
 Comment: 
 
-Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/utils.py
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/__init__.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/model.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/predict.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/predictor.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/train.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/trainer.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/utils.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/distributed/__init__.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/distributed/model.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/distributed/train.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/distributed/trainer.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/distributed/utils.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/inductive/__init__.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/inductive/model.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/inductive/distributed/__init__.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/inductive/distributed/model.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/transductive/__init__.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/transductive/model.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/transductive/distributed/__init__.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/transductive/distributed/model.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/__init__.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/model.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/predict.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/predictor.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/train.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/trainer.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/utils.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/distributed/__init__.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/distributed/model.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/distributed/train.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/distributed/trainer.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/distributed/utils.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/inductive/__init__.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/inductive/model.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/inductive/distributed/__init__.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/inductive/distributed/model.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/transductive/__init__.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/transductive/model.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/transductive/distributed/__init__.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/transductive/distributed/model.py
 Comment: 
 
 Filename: cloudtik/runtime/ai/modeling/transfer_learning/__init__.py
 Comment: 
 
 Filename: cloudtik/runtime/ai/modeling/transfer_learning/dataset.py
 Comment: 
@@ -810,14 +960,20 @@
 
 Filename: cloudtik/runtime/ai/modeling/transfer_learning/common/pytorch/dataset.py
 Comment: 
 
 Filename: cloudtik/runtime/ai/modeling/transfer_learning/common/pytorch/model.py
 Comment: 
 
+Filename: cloudtik/runtime/ai/modeling/transfer_learning/common/pytorch/train.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/transfer_learning/common/pytorch/trainer.py
+Comment: 
+
 Filename: cloudtik/runtime/ai/modeling/transfer_learning/common/pytorch/hugging_face/__init__.py
 Comment: 
 
 Filename: cloudtik/runtime/ai/modeling/transfer_learning/common/pytorch/hugging_face/dataset.py
 Comment: 
 
 Filename: cloudtik/runtime/ai/modeling/transfer_learning/common/pytorch/hugging_face/model.py
@@ -828,14 +984,20 @@
 
 Filename: cloudtik/runtime/ai/modeling/transfer_learning/common/tensorflow/dataset.py
 Comment: 
 
 Filename: cloudtik/runtime/ai/modeling/transfer_learning/common/tensorflow/model.py
 Comment: 
 
+Filename: cloudtik/runtime/ai/modeling/transfer_learning/common/tensorflow/train.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/modeling/transfer_learning/common/tensorflow/trainer.py
+Comment: 
+
 Filename: cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/__init__.py
 Comment: 
 
 Filename: cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/__init__.py
 Comment: 
 
 Filename: cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/image_anomaly_detection_dataset.py
@@ -1044,14 +1206,17 @@
 
 Filename: cloudtik/runtime/ai/runner/cpu/utils.py
 Comment: 
 
 Filename: cloudtik/runtime/ai/runner/util/__init__.py
 Comment: 
 
+Filename: cloudtik/runtime/ai/runner/util/distributor.py
+Comment: 
+
 Filename: cloudtik/runtime/ai/runner/util/safe_shell_exec.py
 Comment: 
 
 Filename: cloudtik/runtime/ai/runner/util/threads.py
 Comment: 
 
 Filename: cloudtik/runtime/ai/runner/util/tiny_shell_exec.py
@@ -1068,14 +1233,20 @@
 
 Filename: cloudtik/runtime/ai/scripts/install.sh
 Comment: 
 
 Filename: cloudtik/runtime/ai/scripts/services.sh
 Comment: 
 
+Filename: cloudtik/runtime/ai/util/__init__.py
+Comment: 
+
+Filename: cloudtik/runtime/ai/util/utils.py
+Comment: 
+
 Filename: cloudtik/runtime/common/__init__.py
 Comment: 
 
 Filename: cloudtik/runtime/common/runtime_base.py
 Comment: 
 
 Filename: cloudtik/runtime/common/utils.py
@@ -1200,14 +1371,17 @@
 
 Filename: cloudtik/runtime/hdfs/runtime.py
 Comment: 
 
 Filename: cloudtik/runtime/hdfs/utils.py
 Comment: 
 
+Filename: cloudtik/runtime/hdfs/conf/hadoop/core-site.xml
+Comment: 
+
 Filename: cloudtik/runtime/hdfs/conf/hadoop/hdfs-site.xml
 Comment: 
 
 Filename: cloudtik/runtime/hdfs/config/commands.yaml
 Comment: 
 
 Filename: cloudtik/runtime/hdfs/config/defaults.yaml
@@ -1380,14 +1554,17 @@
 
 Filename: cloudtik/runtime/spark/scripts.py
 Comment: 
 
 Filename: cloudtik/runtime/spark/utils.py
 Comment: 
 
+Filename: cloudtik/runtime/spark/conf/hadoop/hdfs-site.xml
+Comment: 
+
 Filename: cloudtik/runtime/spark/conf/hadoop/yarn-site.xml
 Comment: 
 
 Filename: cloudtik/runtime/spark/conf/spark/spark-defaults.conf
 Comment: 
 
 Filename: cloudtik/runtime/spark/config/commands.yaml
@@ -2175,23 +2352,29 @@
 
 Filename: cloudtik/tests/unit/gcp/__init__.py
 Comment: 
 
 Filename: cloudtik/tests/unit/gcp/test_gcp_node_provider.py
 Comment: 
 
-Filename: cloudtik-1.1.0.dist-info/METADATA
+Filename: cloudtik/tests/unit/runtime/__init__.py
+Comment: 
+
+Filename: cloudtik/tests/unit/runtime/test_ai_utils.py
+Comment: 
+
+Filename: cloudtik-1.2.0.dist-info/METADATA
 Comment: 
 
-Filename: cloudtik-1.1.0.dist-info/WHEEL
+Filename: cloudtik-1.2.0.dist-info/WHEEL
 Comment: 
 
-Filename: cloudtik-1.1.0.dist-info/entry_points.txt
+Filename: cloudtik-1.2.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: cloudtik-1.1.0.dist-info/top_level.txt
+Filename: cloudtik-1.2.0.dist-info/top_level.txt
 Comment: 
 
-Filename: cloudtik-1.1.0.dist-info/RECORD
+Filename: cloudtik-1.2.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## cloudtik/__init__.py

```diff
@@ -1,4 +1,4 @@
 
 # Replaced with the current commit when building the wheels.
-__commit__ = "6ff84d052f0d05bc191a65bfe4357f46a784853c"
-__version__ = "1.1.0"
+__commit__ = "700397c3607e47d13df8bd79434a085d13e09683"
+__version__ = "1.2.0"
```

## cloudtik/core/api.py

```diff
@@ -6,15 +6,15 @@
 from cloudtik.core._private.call_context import CallContext
 from cloudtik.core._private.cluster.cluster_config import _bootstrap_config, _load_cluster_config
 from cloudtik.core._private.utils import verify_runtime_list, load_head_cluster_config
 from cloudtik.core._private.workspace import workspace_operator
 from cloudtik.core._private.cluster import cluster_operator
 from cloudtik.core._private.event_system import (
     global_event_system)
-from cloudtik.core._private.cli_logger import cli_logger
+from cloudtik.core._private.cli_logger import cli_logger, CliLogger
 from cloudtik.core._private import utils
 from cloudtik.core.workspace_provider import Existence
 
 
 class Workspace:
     def __init__(self, workspace_config: Union[dict, str]) -> None:
         """Create a workspace object to operator on workspace.
@@ -53,20 +53,25 @@
 
     def list_clusters(self) -> Optional[Dict[str, Any]]:
         """Get a list of cluster information running in the workspace"""
         return workspace_operator._list_workspace_clusters(self.config)
 
 
 class Cluster:
-    def __init__(self, cluster_config: Union[dict, str], should_bootstrap: bool = True, no_config_cache: bool = True) -> None:
+    def __init__(
+            self, cluster_config: Union[dict, str],
+            should_bootstrap: bool = True,
+            no_config_cache: bool = True,
+            verbosity: Optional[int] = None) -> None:
         """Create a cluster object to operate on with this API.
 
         Args:
             cluster_config (Union[str, dict]): Either the config dict of the
                 cluster, or a path pointing to a file containing the config.
+            verbosity: If verbosity > 0, print more detailed messages.
         """
         self.cluster_config = cluster_config
         if isinstance(cluster_config, dict):
             if should_bootstrap:
                 self.config = _bootstrap_config(
                     cluster_config, no_config_cache=no_config_cache)
             else:
@@ -74,15 +79,19 @@
         else:
             if not os.path.exists(cluster_config):
                 raise ValueError("Cluster config file not found: {}".format(cluster_config))
             self.config = _load_cluster_config(
                 cluster_config, should_bootstrap=should_bootstrap, no_config_cache=no_config_cache)
 
         # TODO: Each call may need its own call context
-        self.call_context = CallContext()
+        _cli_logger = cli_logger
+        if verbosity is not None:
+            _cli_logger = CliLogger()
+            _cli_logger.set_verbosity(verbosity)
+        self.call_context = CallContext(_cli_logger=_cli_logger)
         self.call_context.set_call_from_api(True)
 
     def start(self,
               no_restart: bool = False,
               restart_only: bool = False) -> None:
         """Create or updates an autoscaling cluster.
 
@@ -235,14 +244,70 @@
             port_forward=port_forward,
             with_output=with_output,
             yes=True,
             job_waiter_name=job_waiter,
             job_log=job_log
         )
 
+    def run(
+            self,
+            script: str,
+            script_args: Optional[List[str]] = None,
+            screen: bool = False,
+            tmux: bool = False,
+            stop: bool = False,
+            start: bool = False,
+            force_update: bool = False,
+            wait_for_workers: bool = False,
+            min_workers: Optional[int] = None,
+            wait_timeout: Optional[int] = None,
+            port_forward: Optional[cluster_operator.Port_forward] = None,
+            with_output: bool = False,
+            job_waiter: Optional[str] = None,
+            job_log: bool = False) -> Optional[str]:
+        """Runs a built-in script (bash or python or a registered command)
+
+        Args:
+            script (str): The script alias, module, or a path.
+            script_args (list): An array of arguments for the script.
+            screen (bool): whether to run in a screen session
+            tmux (bool): whether to run in a tmux session
+            stop (bool): whether to stop the cluster after command run
+            start (bool): whether to start the cluster if not started
+            force_update (bool): if already started, whether force update the configuration if start is true
+            wait_for_workers (bool): whether wait for minimum number of ready workers
+            min_workers (int): The number of workers to wait for ready
+            wait_timeout (int): The timeout for wait for ready
+            port_forward ( (int,int) or list[(int,int)]): port(s) to forward.
+            with_output (bool): Whether to capture command output.
+            job_waiter (str): The job waiter to use for waiting an async job to complete.
+            job_log (bool): Send the output of the job to log file in ~/user/logs.
+        Returns:
+            The output of the command as a string.
+        """
+        return cluster_operator._run_script(
+            config=self.config,
+            call_context=self.call_context,
+            script=script,
+            script_args=script_args,
+            screen=screen,
+            tmux=tmux,
+            stop=stop,
+            start=start,
+            force_update=force_update,
+            wait_for_workers=wait_for_workers,
+            min_workers=min_workers,
+            wait_timeout=wait_timeout,
+            port_forward=port_forward,
+            with_output=with_output,
+            yes=True,
+            job_waiter_name=job_waiter,
+            job_log=job_log
+        )
+
     def rsync(self,
               *,
               source: Optional[str],
               target: Optional[str],
               down: bool,
               node_ip: str = None,
               all_nodes: bool = False,
@@ -452,20 +517,24 @@
                 when specified event occurs.
         """
         cluster_uri = utils.get_cluster_uri(self.config)
         global_event_system.add_callback_handler(cluster_uri, event_name, callback)
 
 
 class ThisCluster:
-    def __init__(self) -> None:
+    def __init__(self, verbosity: Optional[int] = None) -> None:
         """Create a cluster object to operate on from head with this API."""
         self.config = load_head_cluster_config()
 
         # TODO: Each call may need its own call context
-        self.call_context = CallContext()
+        _cli_logger = cli_logger
+        if verbosity is not None:
+            _cli_logger = CliLogger()
+            _cli_logger.set_verbosity(verbosity)
+        self.call_context = CallContext(_cli_logger=_cli_logger)
         self.call_context.set_call_from_api(True)
 
     def exec(self,
              cmd: str,
              *,
              node_ip: str = None,
              all_nodes: bool = False,
@@ -512,14 +581,97 @@
             min_workers=min_workers,
             wait_timeout=wait_timeout,
             port_forward=port_forward,
             with_output=with_output,
             parallel=parallel,
             job_waiter_name=job_waiter)
 
+    def exec(self,
+             cmd: str,
+             *,
+             node_ip: str = None,
+             all_nodes: bool = False,
+             run_env: str = "auto",
+             screen: bool = False,
+             tmux: bool = False,
+             wait_for_workers: bool = False,
+             min_workers: Optional[int] = None,
+             wait_timeout: Optional[int] = None,
+             port_forward: Optional[cluster_operator.Port_forward] = None,
+             with_output: bool = False,
+             parallel: bool = True,
+             job_waiter: Optional[str] = None) -> Optional[str]:
+        """Runs a command on the specified cluster.
+
+        Args:
+            cmd (str): the command to run
+            node_ip (str): node ip on which to run the command
+            all_nodes (bool): whether to run the command on all nodes
+            run_env (str): whether to run the command on the host or in a
+                container. Select between "auto", "host" and "docker".
+            screen (bool): whether to run in a screen session
+            tmux (bool): whether to run in a tmux session
+            wait_for_workers (bool): whether wait for minimum number of ready workers
+            min_workers (int): The number of workers to wait for ready
+            wait_timeout (int): The timeout for wait for ready
+            port_forward ( (int,int) or list[(int,int)]): port(s) to forward.
+            with_output (bool): Whether to capture command output.
+            parallel (bool): Whether to run the commands on nodes in parallel.
+            job_waiter (str): The job waiter to use for waiting an async job to complete.
+        Returns:
+            The output of the command as a string.
+        """
+        return cluster_operator._exec_node_on_head(
+            config=self.config,
+            call_context=self.call_context,
+            node_ip=node_ip,
+            all_nodes=all_nodes,
+            cmd=cmd,
+            run_env=run_env,
+            screen=screen,
+            tmux=tmux,
+            wait_for_workers=wait_for_workers,
+            min_workers=min_workers,
+            wait_timeout=wait_timeout,
+            port_forward=port_forward,
+            with_output=with_output,
+            parallel=parallel,
+            job_waiter_name=job_waiter)
+
+    def run(
+            self,
+            script: str,
+            script_args: Optional[List[str]] = None,
+            wait_for_workers: bool = False,
+            min_workers: Optional[int] = None,
+            wait_timeout: Optional[int] = None,
+            with_output: bool = False) -> Optional[str]:
+        """Runs a built-in script (bash or python or a registered command)
+
+        Args:
+            script (str): The script alias, module, or a path.
+            script_args (list): An array of arguments for the script.
+            wait_for_workers (bool): whether wait for minimum number of ready workers
+            min_workers (int): The number of workers to wait for ready
+            wait_timeout (int): The timeout for wait for ready
+            with_output (bool): Whether to capture command output.
+        Returns:
+            The output of the command as a string.
+        """
+        return cluster_operator._run_script_on_head(
+            config=self.config,
+            call_context=self.call_context,
+            script=script,
+            script_args=script_args,
+            wait_for_workers=wait_for_workers,
+            min_workers=min_workers,
+            wait_timeout=wait_timeout,
+            with_output=with_output
+        )
+
     def rsync(self,
               *,
               source: Optional[str],
               target: Optional[str],
               down: bool,
               node_ip: str = None,
               all_workers: bool = False):
@@ -732,13 +884,7 @@
             Output verbosity (0, 1, 2, 3).
 
             Low verbosity will disable `verbose` and `very_verbose` messages.
 
     """
     cli_logger.configure(
         log_style=log_style, color_mode=color_mode, verbosity=verbosity)
-
-
-def get_docker_host_mount_location(cluster_name: str) -> str:
-    """Return host path that Docker mounts attach to."""
-    docker_mount_prefix = "/tmp/cloudtik/host_mounts/{cluster_name}"
-    return docker_mount_prefix.format(cluster_name=cluster_name)
```

## cloudtik/core/config-schema.json

### Pretty-printed

 * *Similarity: 0.9999995488435715%*

 * *Differences: {"'properties'": "{'runtime': {'properties': {'spark': {'properties': {'hadoop_default_cluster': "*

 * *                 "OrderedDict([('type', 'boolean'), ('description', 'Whether to set Hadoop default "*

 * *                 "to cluster storage if available.'), ('default', False)]), 'auto_detect_hdfs': "*

 * *                 "OrderedDict([('type', 'boolean'), ('description', 'Whether to auto detect and "*

 * *                 "use HDFS service in the same workspace.'), ('default', False)]), "*

 * *                 "'hdfs_mount_m []*

```diff
@@ -901,18 +901,33 @@
                     },
                     "type": "object"
                 },
                 "flink": {
                     "additionalProperties": true,
                     "description": "Flink runtime related configurations",
                     "properties": {
+                        "auto_detect_hdfs": {
+                            "default": false,
+                            "description": "Whether to auto detect and use HDFS service in the same workspace.",
+                            "type": "boolean"
+                        },
+                        "auto_detect_metastore": {
+                            "default": false,
+                            "description": "Whether to auto detect and use metastore service in the same workspace.",
+                            "type": "boolean"
+                        },
                         "config": {
                             "description": "Map of flink configurations",
                             "type": "object"
                         },
+                        "hadoop_default_cluster": {
+                            "default": false,
+                            "description": "Whether to set Hadoop default to cluster storage if available.",
+                            "type": "boolean"
+                        },
                         "hdfs_namenode_uri": {
                             "description": "HDFS service endpoint if Flink need to access HDFS.",
                             "type": "string"
                         },
                         "hive_metastore_uri": {
                             "description": "Metastore service endpoint for Flink to use.",
                             "type": "string"
@@ -984,14 +999,19 @@
                     },
                     "type": "object"
                 },
                 "presto": {
                     "additionalProperties": true,
                     "description": "Presto runtime related configurations",
                     "properties": {
+                        "auto_detect_metastore": {
+                            "default": false,
+                            "description": "Whether to auto detect and use metastore service in the same workspace.",
+                            "type": "boolean"
+                        },
                         "catalogs": {
                             "description": "Presto catalogs to be configured.",
                             "type": "object"
                         },
                         "config": {
                             "description": "Map of Presto server properties.",
                             "type": "object"
@@ -1072,18 +1092,37 @@
                     },
                     "type": "object"
                 },
                 "spark": {
                     "additionalProperties": true,
                     "description": "Spark runtime related configurations",
                     "properties": {
+                        "auto_detect_hdfs": {
+                            "default": false,
+                            "description": "Whether to auto detect and use HDFS service in the same workspace.",
+                            "type": "boolean"
+                        },
+                        "auto_detect_metastore": {
+                            "default": false,
+                            "description": "Whether to auto detect and use metastore service in the same workspace.",
+                            "type": "boolean"
+                        },
                         "config": {
                             "description": "Map of spark configurations, e.g. {\"spark.executor.cores\": \"45\"}",
                             "type": "object"
                         },
+                        "hadoop_default_cluster": {
+                            "default": false,
+                            "description": "Whether to set Hadoop default to cluster storage if available.",
+                            "type": "boolean"
+                        },
+                        "hdfs_mount_method": {
+                            "description": "HDFS mount method: nfs or fuse. Default fuse if not specified.",
+                            "type": "string"
+                        },
                         "hdfs_namenode_uri": {
                             "description": "HDFS service endpoint if Spark need to access HDFS.",
                             "type": "string"
                         },
                         "hive_metastore_uri": {
                             "description": "Metastore service endpoint for Spark to use.",
                             "type": "string"
```

## cloudtik/core/_private/cli_logger.py

```diff
@@ -373,14 +373,18 @@
     def verbosity(self):
         if self._verbosity_overriden:
             return self._verbosity
         elif not self.pretty:
             return 999
         return self._verbosity
 
+    @property
+    def verbosity_overriden(self):
+        return self._verbosity_overriden
+
     def set_verbosity(self, x):
         self._verbosity = x
         self._verbosity_overriden = True
 
     def detect_colors(self):
         """Update color output settings.
```

## cloudtik/core/_private/docker.py

```diff
@@ -1,20 +1,42 @@
+import os
+import uuid
 from pathlib import Path
 from typing import Any, Dict
 
 import cloudtik
 
 try:  # py3
     from shlex import quote
 except ImportError:  # py2
     from pipes import quote
 
 from cloudtik.core._private.cli_logger import cli_logger
 
 
+def get_docker_host_mount_location(cluster_name: str) -> str:
+    """Return host path that Docker mounts attach to."""
+    docker_mount_prefix = "/tmp/cloudtik/mounts/{cluster_name}"
+    return docker_mount_prefix.format(cluster_name=cluster_name)
+
+
+def get_docker_host_mount_location_for_object(
+        cluster_name: str, object_path: str, identical=True) -> str:
+    """Return the docker host mount directory location for a specific target"""
+    docker_mount_prefix = get_docker_host_mount_location(cluster_name)
+    normalized_object_path = object_path.rstrip("/")
+    object_identifier = str(uuid.uuid3(uuid.NAMESPACE_OID, normalized_object_path))
+    object_root = os.path.join(docker_mount_prefix, object_identifier)
+    if identical:
+        return os.path.join(object_root, object_path.lstrip("/"))
+    else:
+        # using a unique uuid instead
+        return os.path.join(object_root, str(uuid.uuid4()))
+
+
 def _check_docker_file_mounts(file_mounts: Dict[str, str]) -> None:
     """Checks if files are passed as file_mounts. This is a problem for Docker
     based clusters because when a file is bind-mounted in Docker, updates to
     the file on the host do not always propagate to the container. Using
     directories is recommended.
     """
     for remote, local in file_mounts.items():
@@ -95,33 +117,35 @@
 def check_docker_image(cname, docker_cmd):
     return _check_helper(cname, ".Config.Image", docker_cmd)
 
 
 def docker_start_cmds(user, image, mount_dict, data_disks, container_name, user_options,
                       cluster_name, home_directory, docker_cmd,
                       network=None, cpus=None, memory=None, labels=None,
-                      port_mappings=None, mounts_mapping=False):
+                      port_mappings=None, mounts_mapping=False, ipc_mode=None):
     mounts = mount_dict
     if mounts_mapping:
-        # Imported here due to circular dependency.
-        from cloudtik.core.api import get_docker_host_mount_location
-        docker_mount_prefix = get_docker_host_mount_location(cluster_name)
-        mounts = {dst: f"{docker_mount_prefix}/{dst}" for dst in mount_dict}
+        mounts = {}
+        for dst in mount_dict:
+            mounts[dst] = get_docker_host_mount_location_for_object(
+                cluster_name, dst)
 
     return _docker_start_cmds(
         user, image, mounts, data_disks, container_name,
         user_options, home_directory, docker_cmd,
-        network, cpus, memory, labels, port_mappings,
+        network=network, cpus=cpus, memory=memory,
+        labels=labels, port_mappings=port_mappings,
+        ipc_mode=ipc_mode
     )
 
 
 def _docker_start_cmds(user, image, mounts, data_disks, container_name,
                        user_options, home_directory, docker_cmd,
                        network=None, cpus=None, memory=None, labels=None,
-                       port_mappings=None):
+                       port_mappings=None, ipc_mode=None):
     # mounts mapping: target -> source
     file_mounts = [
         "-v {src}:{dest}".format(
             src=v, dest=k.replace("~/", home_directory + "/"))
         for k, v in mounts.items()
     ]
     data_disk_mounts = [
@@ -139,22 +163,26 @@
     env_flags = " ".join(
         ["-e {name}={val}".format(name=k, val=v) for k, v in env_vars.items()])
 
     user_options_str = " ".join(user_options)
 
     fuse_flags = "--cap-add SYS_ADMIN --device /dev/fuse --security-opt apparmor:unconfined"
     numactl_flag = "--cap-add SYS_NICE"
-    ipc_flag = "--ipc=host"
     network_flag = "--network={}".format(network) if network else "--network=host"
 
     docker_run = [
         docker_cmd, "run", "--rm", "--name {}".format(container_name), "-d",
         "-it", mount_flags, env_flags, fuse_flags, user_options_str,
-        numactl_flag, ipc_flag, network_flag
+        numactl_flag, network_flag
     ]
+
+    # default IPC mode to host
+    ipc_flag = "--ipc={}".format(ipc_mode if ipc_mode else "host")
+    docker_run += [ipc_flag]
+
     if cpus:
         cpus_flag = "--cpus={}".format(cpus)
         docker_run += [cpus_flag]
     if memory:
         memory_flag = "--memory={}".format(memory)
         docker_run += [memory_flag]
     if labels:
```

## cloudtik/core/_private/utils.py

```diff
@@ -1,11 +1,12 @@
 import base64
 import collections
 import collections.abc
 import copy
+import subprocess
 from datetime import datetime
 import logging
 import hashlib
 import json
 import os
 from typing import Any, Dict, Optional, Tuple, List, Union
 import sys
@@ -3140,17 +3141,17 @@
             ),
         )
     return bundles
 
 
 def parse_resource_list(resource_list_str: str, ) -> Dict[str, int]:
     resource_dict = {}
-    resources = resource_list_str.split(",")
+    resources = [x.strip() for x in resource_list_str.split(",")]
     for resource in resources:
-        resource_parts = resource.split(":")
+        resource_parts = [x.strip() for x in resource.split(":")]
         if len(resource_parts) != 2:
             raise ValueError(
                 "Invalid resource specification. Format: resource_type:amount")
         resource_name = resource_parts[0]
         resource_amount = int(resource_parts[1])
         resource_dict[resource_name] = resource_amount
     return resource_dict
@@ -3165,7 +3166,53 @@
     # try two ways, json or list
     try:
         resources = json.loads(resources_str)
         if not isinstance(resources, dict):
             raise ValueError
     except Exception:
         return parse_resource_list(resources_str)
+
+
+def with_verbose_option(cmds, call_context):
+    _cli_logger = call_context.cli_logger
+    if _cli_logger.verbosity_overriden:
+        verbosity = _cli_logger.verbosity
+        if verbosity > 0:
+            # maximum 10 verbose
+            verbosity = verbosity if verbosity < 10 else 10
+            cmds += ["-v" for _ in range(verbosity)]
+
+
+def run_script(script, script_args, with_output=False):
+    import cloudtik as cloudtik_home
+    # import script registry here because it will search for all the packages for
+    # registering scripts
+    from cloudtik.core._private.script_registry import get_registered_script
+
+    root_path = os.path.abspath(os.path.dirname(cloudtik_home.__file__))
+    script_target = get_registered_script(script)
+    if script_target:
+        # run a registered command pointing to a script
+        script = script_target
+
+    if script.endswith(".sh"):
+        target = os.path.join(root_path, script)
+        cmds = ["bash", quote(target)]
+    elif script.endswith(".py"):
+        target = os.path.join(root_path, script)
+        cmds = [sys.executable, "-u", quote(target)]
+    else:
+        target = script
+        # it should be a python module
+        cmds = [sys.executable, "-u", "-m", quote(target)]
+
+    with_script_args(cmds, script_args)
+
+    final_cmd = " ".join(cmds)
+    try:
+        if with_output:
+            return subprocess.check_output(final_cmd, shell=True)
+        else:
+            subprocess.check_call(final_cmd, shell=True)
+    except subprocess.CalledProcessError as err:
+        print(f"Called process error {err}")
+        raise err
```

## cloudtik/core/_private/cluster/cluster_dump.py

```diff
@@ -15,15 +15,15 @@
 import yaml
 
 from cloudtik.core._private.call_context import CallContext
 from cloudtik.core._private.cli_logger import cli_logger
 from cloudtik.core._private.cluster.cluster_exec import exec_cluster, exec_on_head, rsync_cluster, rsync_on_head
 from cloudtik.core._private.providers import _get_node_provider
 from cloudtik.core._private.utils import get_head_working_ip, get_node_cluster_ip, get_runtime_logs, \
-    get_runtime_processes, _get_node_specific_runtime_types
+    get_runtime_processes, _get_node_specific_runtime_types, with_verbose_option
 from cloudtik.core.tags import CLOUDTIK_TAG_NODE_KIND, NODE_KIND_HEAD, \
     NODE_KIND_WORKER
 
 MAX_PARALLEL_SSH_WORKERS = 8
 
 
 class CommandFailed(RuntimeError):
@@ -402,15 +402,15 @@
     node. The resulting file will be saved locally in a temporary file and
     returned.
 
     Returns:
         Path to a temporary file containing the node's collected data.
 
     """
-    collect_cmd = ["cloudtik", "node", "dump", "--verbosity=0"]
+    collect_cmd = ["cloudtik", "node", "dump", "--silent"]
     collect_cmd += ["--logs"] if parameters.logs else ["--no-logs"]
     collect_cmd += ["--debug-state"] if parameters.debug_state else [
         "--no-debug-state"
     ]
     collect_cmd += ["--pip"] if parameters.pip else ["--no-pip"]
     collect_cmd += ["--processes"] if parameters.processes else [
         "--no-processes"
@@ -425,17 +425,19 @@
 
     kind = "worker" if not remote_node.is_head else "head"
     remote_temp_file = tempfile.mktemp(
         prefix=f"cloudtik_{kind}_{remote_node.host}_", suffix=".tar.gz")
     collect_cmd += ["--output"]
     collect_cmd += [remote_temp_file]
 
+    with_verbose_option(collect_cmd, call_context)
     cmd = " ".join(collect_cmd)
 
-    cli_logger.print(f"Collecting data from remote node: {remote_node.host}")
+    call_context.cli_logger.print(
+        f"Collecting data from remote node: {remote_node.host}")
 
     # execute the command on head, and it will dump to the output file
     exec_on_head(
         config, call_context,
         node_id=remote_node.node_id,
         cmd=cmd)
 
@@ -536,31 +538,44 @@
     # This is to ensure that the parallel SSH calls below do not mess with
     # the users terminal.
     output_redir = call_context.is_output_redirected()
     call_context.set_output_redirected(True)
     allow_interactive = call_context.does_allow_interactive()
     call_context.set_allow_interactive(False)
 
+    _cli_logger = call_context.cli_logger
+
+    failures = 0
     with ThreadPoolExecutor(max_workers=MAX_PARALLEL_SSH_WORKERS) as executor:
+        futures = {}
         for remote_node in remote_nodes:
             # get node type specific runtimes
             node_parameters = copy.deepcopy(parameters)
             node_runtimes = _get_node_specific_runtime_types(config, remote_node.node_id)
             node_parameters.set_runtimes(node_runtimes)
-            executor.submit(
+            futures[remote_node.host] = executor.submit(
                 add_archive_for_remote_node,
                 config=config,
                 call_context=call_context.new_call_context(),
                 archive=archive,
                 remote_node=remote_node,
                 parameters=node_parameters)
 
+        for host, future in futures.items():
+            try:
+                result = future.result()
+            except Exception as e:
+                failures += 1
+                _cli_logger.error("Dump task failed on node {}: {}", host, str(e))
+
     call_context.set_output_redirected(output_redir)
     call_context.set_allow_interactive(allow_interactive)
 
+    if failures > 1:
+        _cli_logger.print("Total {} dump tasks failed.", failures)
     return archive
 
 
 def get_archive_from_head_node(
         config: Dict[str, Any],
         call_context: CallContext,
         nodes: Optional[List[Node]],
@@ -573,15 +588,15 @@
     returned.
 
     Returns:
         Path to a temporary file containing the node's collected data.
 
     """
 
-    collect_cmd = ["cloudtik", "head", "cluster-dump", "--verbosity=0"]
+    collect_cmd = ["cloudtik", "head", "cluster-dump", "--silent"]
     collect_cmd += ["--logs"] if parameters.logs else ["--no-logs"]
     collect_cmd += ["--debug-state"] if parameters.debug_state else [
         "--no-debug-state"
     ]
     collect_cmd += ["--pip"] if parameters.pip else ["--no-pip"]
     collect_cmd += ["--processes"] if parameters.processes else [
         "--no-processes"
@@ -597,17 +612,19 @@
 
     kind = "cluster"
     remote_temp_file = tempfile.mktemp(
         prefix=f"cloudtik_{kind}_", suffix=".tar.gz")
     collect_cmd += ["--output"]
     collect_cmd += [remote_temp_file]
 
+    with_verbose_option(collect_cmd, call_context)
     cmd = " ".join(collect_cmd)
 
-    cli_logger.print(f"Collecting cluster data from head node...")
+    call_context.cli_logger.print(
+        "Collecting cluster data from head node...")
 
     # execute the command on head, and it will dump to the output file
     exec_cluster(
         config, call_context,
         cmd=cmd)
 
     # rsync the output file down to local temp
```

## cloudtik/core/_private/cluster/cluster_exec.py

```diff
@@ -20,16 +20,14 @@
         run_env: str = "auto",
         with_output: bool = False,
         is_head_node: bool = False,
         use_internal_ip: bool = True
 ) -> str:
     """Runs a command on a node of a cluster
     """
-    call_context.set_allow_interactive(True)
-
     updater = create_node_updater_for_exec(
         config=config,
         call_context=call_context,
         node_id=node_id,
         provider=provider,
         start_commands=[],
         is_head_node=is_head_node,
```

## cloudtik/core/_private/cluster/cluster_operator.py

```diff
@@ -63,15 +63,15 @@
     RUNTIME_CONFIG_KEY, DOCKER_CONFIG_KEY, get_running_head_node, \
     get_nodes_for_runtime, with_script_args, encrypt_config, convert_nodes_to_resource, \
     HeadNotRunningError, get_cluster_head_ip, get_command_session_name, ParallelTaskSkipped, \
     CLOUDTIK_CLUSTER_SCALING_STATUS, decode_cluster_scaling_time, RUNTIME_TYPES_CONFIG_KEY, get_node_info, \
     NODE_INFO_NODE_IP, get_cpus_of_node_info, _sum_min_workers, get_memory_of_node_info, sum_worker_gpus, \
     sum_nodes_resource, get_gpus_of_node_info, get_resource_of_node_info, get_resource_info_of_node_type, \
     get_worker_node_type, save_server_process, get_resource_requests_for, _get_head_resource_requests, \
-    get_resource_list_str
+    get_resource_list_str, with_verbose_option, run_script
 
 from cloudtik.core._private.providers import _get_node_provider, _NODE_PROVIDERS
 from cloudtik.core.tags import (
     CLOUDTIK_TAG_NODE_KIND, CLOUDTIK_TAG_LAUNCH_CONFIG, CLOUDTIK_TAG_NODE_NAME,
     NODE_KIND_WORKER, NODE_KIND_HEAD, CLOUDTIK_TAG_USER_NODE_TYPE,
     STATUS_UNINITIALIZED, STATUS_UP_TO_DATE, CLOUDTIK_TAG_NODE_STATUS, STATUS_UPDATE_FAILED, CLOUDTIK_TAG_NODE_NUMBER,
     CLOUDTIK_TAG_HEAD_NODE_NUMBER)
@@ -662,14 +662,16 @@
     cmds = [
         "cloudtik",
         "head",
         "kill-node",
         "--yes",
     ]
     cmds += ["--node-ip={}".format(node_ip)]
+
+    with_verbose_option(cmds, call_context)
     final_cmd = " ".join(cmds)
     _exec_cmd_on_cluster(config,
                          call_context=call_context,
                          cmd=final_cmd)
     return node_ip
 
 
@@ -852,15 +854,15 @@
 
             start = time.time()
             head_node = None
             with _cli_logger.group("Fetching the new head node"):
                 while True:
                     if time.time() - start > 50:
                         _cli_logger.abort("Head node fetch timed out. "
-                                         "Failed to create head node.")
+                                          "Failed to create head node.")
                     nodes = provider.non_terminated_nodes(head_node_tags)
                     if len(nodes) == 1:
                         head_node = nodes[0]
                         break
                     time.sleep(POLL_INTERVAL)
             _cli_logger.newline()
 
@@ -1144,14 +1146,16 @@
             node.
         job_waiter: The waiter object to check the job is done
         session_name: The session name of the job
     """
     assert not (screen and tmux), "Can specify only one of `screen` or `tmux`."
     assert run_env in RUN_ENV_TYPES, "--run_env must be in {}".format(
         RUN_ENV_TYPES)
+
+    # TODO (haifeng): we may not need to set_allow_interactive explicitly here
     # We default this to True to maintain backwards-compatibility
     # In the future we would want to support disabling login-shells
     # and interactivity.
     call_context.set_allow_interactive(True)
 
     use_internal_ip = config.get("bootstrapped", False)
     head_node = _get_running_head_node_ex(
@@ -1375,14 +1379,15 @@
 
     if not down:
         if all_workers:
             cmds += ["--all-workers"]
         else:
             cmds += ["--no-all-workers"]
 
+    with_verbose_option(cmds, call_context)
     final_cmd = " ".join(cmds)
     _exec_cmd_on_cluster(config,
                          call_context=call_context,
                          cmd=final_cmd)
 
 
 def rsync_node_on_head(config: Dict[str, Any],
@@ -1649,15 +1654,15 @@
                logs: bool = True,
                debug_state: bool = True,
                pip: bool = True,
                processes: bool = True,
                processes_verbose: bool = False,
                tempfile: Optional[str] = None,
                runtimes: str = None,
-               verbosity: Optional[int] = None) -> Optional[str]:
+               silent: bool = False) -> Optional[str]:
     if stream and output:
         raise ValueError(
             "You can only use either `--output` or `--stream`, but not both.")
     runtime_list = runtimes.split(",") if runtimes and len(runtimes) > 0 else None
     parameters = GetParameters(
         logs=logs,
         debug_state=debug_state,
@@ -1676,15 +1681,15 @@
             os.write(1, fp.read())
         os.remove(tmp)
         return None
 
     target = output or os.path.join(os.getcwd(), os.path.basename(tmp))
     shutil.move(tmp, target)
 
-    if verbosity is None or verbosity > 0:
+    if not silent:
         cli_logger.print(f"Created local data archive at {target}")
 
     return target
 
 
 def dump_cluster_on_head(
         config: Dict[str, Any],
@@ -1694,20 +1699,20 @@
         output: Optional[str] = None,
         logs: bool = True,
         debug_state: bool = True,
         pip: bool = True,
         processes: bool = True,
         processes_verbose: bool = False,
         temp_file: Optional[str] = None,
-        verbosity: Optional[int] = None) -> Optional[str]:
+        silent: bool = False) -> Optional[str]:
     if stream and output:
         raise ValueError(
             "You can only use either `--output` or `--stream`, but not both.")
 
-    if not stream and (verbosity is None or verbosity > 0):
+    if not stream and not silent:
         _print_cluster_dump_warning(
             call_context,
             logs, debug_state, pip, processes)
 
     head, workers, = _get_nodes_to_dump(config, hosts)
 
     head_node = Node(
@@ -1804,20 +1809,22 @@
         head_only: Optional[bool] = None,
         output: Optional[str] = None,
         logs: bool = True,
         debug_state: bool = True,
         pip: bool = True,
         processes: bool = True,
         processes_verbose: bool = False,
-        tempfile: Optional[str] = None):
+        tempfile: Optional[str] = None,
+        silent: bool = False):
     # Inform the user what kind of logs are collected (before actually
     # collecting, so they can abort)
-    _print_cluster_dump_warning(
-        call_context,
-        logs, debug_state, pip, processes)
+    if not silent:
+        _print_cluster_dump_warning(
+            call_context,
+            logs, debug_state, pip, processes)
 
     head, workers = _get_nodes_to_dump(config, hosts)
 
     head_node = Node(
             node_id=head[0],
             host=head[1],
             is_head=True) if head is not None else None
@@ -1853,16 +1860,17 @@
                    f"{datetime.datetime.now():%Y-%m-%d_%H-%M-%S}.tar.gz"
         output = os.path.join(os.getcwd(), filename)
     else:
         output = os.path.expanduser(output)
 
     shutil.move(archive.file, output)
 
-    _cli_logger.print(
-        f"Created cluster dump archive: {output}")
+    if not silent:
+        _cli_logger.print(
+            f"Created cluster dump archive: {output}")
 
 
 def _show_worker_cpus(config: Dict[str, Any]):
     provider = _get_node_provider(config["provider"], config["cluster_name"])
     worker_cpus = get_worker_cpus(config, provider)
     cli_logger.print(worker_cpus)
 
@@ -2559,15 +2567,16 @@
         parallel: bool = True,
         yes: bool = False,
         job_waiter_name: Optional[str] = None) -> str:
     if not node_ip and not all_nodes:
         if (start or stop) and not yes:
             cli_logger.confirm(
                 yes,
-                "You are about to start or stop cluster {} for this operation. Are you sure that you want to continue?",
+                "You are about to start or stop cluster {} for this operation. "
+                "Are you sure that you want to continue?",
                 config["cluster_name"], _abort=True)
             cli_logger.newline()
 
         # create job waiter, None if no job waiter specified
         job_waiter = _create_job_waiter(
             config, call_context, job_waiter_name)
 
@@ -2659,14 +2668,15 @@
     else:
         cmds += ["--no-parallel"]
 
     if job_waiter_name:
         cmds += ["--job-waiter={}".format(job_waiter_name)]
 
     # TODO (haifeng): handle port forward and with_output for two state cases
+    with_verbose_option(cmds, call_context)
     final_cmd = " ".join(cmds)
 
     job_waiter = _create_job_waiter(
         config, call_context, job_waiter_name)
     return _exec_cluster(
         config,
         call_context=call_context,
@@ -2695,14 +2705,15 @@
 
     Arguments:
         config_file: path to the cluster yaml
         node_ip: the node internal IP to attach
         use_screen: whether to use screen as multiplexer
         use_tmux: whether to use tmux as multiplexer
         override_cluster_name: set the name of the cluster
+        no_config_cache: no use config cache
         new: whether to force a new screen
         port_forward ( (int,int) or list[(int,int)] ): port(s) to forward
         force_to_host: Whether attach to host even running with docker
     """
     config = _load_cluster_config(config_file, override_cluster_name,
                                   no_config_cache=no_config_cache)
     call_context = cli_call_context()
@@ -2719,14 +2730,15 @@
         cmds += ["--tmux"]
     if new:
         cmds += ["--new"]
     if force_to_host:
         cmds += ["--host"]
 
     # TODO (haifeng): handle port forward for two state cases
+    with_verbose_option(cmds, call_context)
     final_cmd = " ".join(cmds)
 
     _exec_cluster(
         config,
         call_context=call_context,
         cmd=final_cmd,
         run_env="auto",
@@ -3066,14 +3078,16 @@
         cmds += ["--runtimes={}".format(quote(runtime_arg))]
     if indent_level:
         cmds += ["--indent-level={}".format(indent_level)]
     if parallel:
         cmds += ["--parallel"]
     else:
         cmds += ["--no-parallel"]
+
+    with_verbose_option(cmds, call_context)
     final_cmd = " ".join(cmds)
 
     _exec_cmd_on_cluster(config,
                          call_context=call_context,
                          cmd=final_cmd)
 
 
@@ -3127,14 +3141,16 @@
         cmds += ["--runtimes={}".format(quote(runtime_arg))]
     if indent_level:
         cmds += ["--indent-level={}".format(indent_level)]
     if parallel:
         cmds += ["--parallel"]
     else:
         cmds += ["--no-parallel"]
+
+    with_verbose_option(cmds, call_context)
     final_cmd = " ".join(cmds)
 
     _exec_cmd_on_cluster(config,
                          call_context=call_context,
                          cmd=final_cmd)
 
 
@@ -3434,14 +3450,15 @@
         # json dump
         bundles_json = json.dumps(bundles)
         cmds += ["--bundles={}".format(
             quote(bundles_json))]
     if up_only:
         cmds += ["--up-only"]
 
+    with_verbose_option(cmds, call_context)
     final_cmd = " ".join(cmds)
     _exec_cmd_on_cluster(config,
                          call_context=call_context,
                          cmd=final_cmd)
 
 
 def scale_cluster_on_head(yes: bool, cpus: int, gpus: int,
@@ -3656,44 +3673,185 @@
 
     if wait_for_workers:
         _wait_for_ready(
             config=config, call_context=call_context,
             min_workers=min_workers, timeout=wait_timeout)
 
 
-def submit_and_exec(config: Dict[str, Any],
-                    call_context: CallContext,
-                    script: str,
-                    script_args,
-                    screen: bool = False,
-                    tmux: bool = False,
-                    stop: bool = False,
-                    start: bool = False,
-                    force_update: bool = False,
-                    wait_for_workers: bool = False,
-                    min_workers: Optional[int] = None,
-                    wait_timeout: Optional[int] = None,
-                    port_forward: Optional[Port_forward] = None,
-                    with_output: bool = False,
-                    yes: bool = False,
-                    job_waiter_name: Optional[str] = None,
-                    job_log: bool = False,
-                    runtime: Optional[str] = None,
-                    runtime_options: Optional[List[str]] = None,
-                    ):
-    cli_logger.doassert(not (screen and tmux),
-                        "`{}` and `{}` are incompatible.", cf.bold("--screen"),
-                        cf.bold("--tmux"))
+def submit_and_exec(
+        config: Dict[str, Any],
+        call_context: CallContext,
+        script: str,
+        script_args,
+        screen: bool = False,
+        tmux: bool = False,
+        stop: bool = False,
+        start: bool = False,
+        force_update: bool = False,
+        wait_for_workers: bool = False,
+        min_workers: Optional[int] = None,
+        wait_timeout: Optional[int] = None,
+        port_forward: Optional[Port_forward] = None,
+        with_output: bool = False,
+        yes: bool = False,
+        job_waiter_name: Optional[str] = None,
+        job_log: bool = False,
+        runtime: Optional[str] = None,
+        runtime_options: Optional[List[str]] = None,
+):
+    def prepare_submit_command():
+        target_name = os.path.basename(script)
+        target = os.path.join("~", "user", "jobs", target_name)
+
+        # Create the "user/jobs" and "user/logs" folder before do upload
+        cmd_mkdir = "mkdir -p ~/user/jobs; mkdir -p ~/user/logs"
+        _exec_cmd_on_cluster(
+            config,
+            call_context=call_context,
+            cmd=cmd_mkdir
+        )
+        cmds = []
+        if urllib.parse.urlparse(script).scheme in ("http", "https"):
+            cmds = ["wget", quote(script), "-O", f"~/user/jobs/{target_name};"]
+        else:
+            # upload the script to cluster
+            _rsync(
+                config,
+                call_context=call_context,
+                source=script,
+                target=target,
+                down=False)
+
+        # Use new target with $HOME instead of ~ for exec
+        target = os.path.join("$HOME", "user", "jobs", target_name)
+        if runtime is not None:
+            runtime_commands = get_runnable_command(
+                config.get(RUNTIME_CONFIG_KEY), target, runtime, runtime_options)
+            if runtime_commands is None:
+                cli_logger.abort("Runtime {} doesn't how to execute your file: {}", runtime, script)
+            cmds += runtime_commands
+        elif target_name.endswith(".py"):
+            cmds += ["python", double_quote(target)]
+        elif target_name.endswith(".sh"):
+            cmds += ["bash", double_quote(target)]
+        else:
+            runtime_commands = get_runnable_command(config.get(RUNTIME_CONFIG_KEY), target)
+            if runtime_commands is None:
+                cli_logger.abort("We don't how to execute your file: {}", script)
+            cmds += runtime_commands
+
+        with_script_args(cmds, script_args)
+
+        # If user uses screen or tmux and job waiter is used
+        # which means to not hold the tmux or screen session, we redirect log with the session name
+        user_cmd = " ".join(cmds)
+        session_name = get_command_session_name(user_cmd, time.time_ns())
+        if job_log or ((screen or tmux) and job_waiter_name is not None):
+            redirect_output = f">$HOME/user/logs/{session_name}.log"
+            cmds += [redirect_output, "2>&1"]
 
+        return cmds, session_name
+
+    _exec_with_prepare(
+        config,
+        call_context=call_context,
+        prepare=prepare_submit_command,
+        prepare_args=(),
+        screen=screen,
+        tmux=tmux,
+        stop=stop,
+        start=start,
+        force_update=force_update,
+        wait_for_workers=wait_for_workers,
+        min_workers=min_workers,
+        wait_timeout=wait_timeout,
+        port_forward=port_forward,
+        with_output=with_output,
+        yes=yes,
+        job_waiter_name=job_waiter_name,
+    )
+
+
+def _run_script(
+        config: Dict[str, Any],
+        call_context: CallContext,
+        script: str,
+        script_args,
+        screen: bool = False,
+        tmux: bool = False,
+        stop: bool = False,
+        start: bool = False,
+        force_update: bool = False,
+        wait_for_workers: bool = False,
+        min_workers: Optional[int] = None,
+        wait_timeout: Optional[int] = None,
+        port_forward: Optional[Port_forward] = None,
+        with_output: bool = False,
+        yes: bool = False,
+        job_waiter_name: Optional[str] = None,
+        job_log: bool = False
+):
+    def prepare_run_script():
+        cmds = ["cloudtik", "node", "run", script]
+        with_script_args(cmds, script_args)
+
+        # If user uses screen or tmux and job waiter is used
+        # which means to not hold the tmux or screen session, we redirect log with the session name
+        user_cmd = " ".join(cmds)
+        session_name = get_command_session_name(user_cmd, time.time_ns())
+        if job_log or ((screen or tmux) and job_waiter_name is not None):
+            redirect_output = f">$HOME/user/logs/{session_name}.log"
+            cmds += [redirect_output, "2>&1"]
+        return cmds, session_name
+
+    _exec_with_prepare(
+        config,
+        call_context=call_context,
+        prepare=prepare_run_script,
+        prepare_args=(),
+        screen=screen,
+        tmux=tmux,
+        stop=stop,
+        start=start,
+        force_update=force_update,
+        wait_for_workers=wait_for_workers,
+        min_workers=min_workers,
+        wait_timeout=wait_timeout,
+        port_forward=port_forward,
+        with_output=with_output,
+        yes=yes,
+        job_waiter_name=job_waiter_name,
+    )
+
+
+def _exec_with_prepare(
+        config: Dict[str, Any],
+        call_context: CallContext,
+        prepare,
+        prepare_args=(),
+        screen: bool = False,
+        tmux: bool = False,
+        stop: bool = False,
+        start: bool = False,
+        force_update: bool = False,
+        wait_for_workers: bool = False,
+        min_workers: Optional[int] = None,
+        wait_timeout: Optional[int] = None,
+        port_forward: Optional[Port_forward] = None,
+        with_output: bool = False,
+        yes: bool = False,
+        job_waiter_name: Optional[str] = None
+):
     assert not (screen and tmux), "Can specify only one of `screen` or `tmux`."
 
     if (start or stop) and not yes:
         cli_logger.confirm(
             yes,
-            "You are about to start or stop cluster {} for this operation. Are you sure that you want to continue?",
+            "You are about to start or stop cluster {} for this operation. "
+            "Are you sure that you want to continue?",
             config["cluster_name"], _abort=True)
         cli_logger.newline()
 
     # create job waiter, None if no job waiter specified
     job_waiter = _create_job_waiter(
         config, call_context, job_waiter_name)
 
@@ -3702,80 +3860,53 @@
         call_context=call_context,
         start=start,
         force_update=force_update,
         wait_for_workers=wait_for_workers,
         min_workers=min_workers,
         wait_timeout=wait_timeout,
     )
-
-    target_name = os.path.basename(script)
-    target = os.path.join("~", "user", "jobs", target_name)
-
-    # Create the "user/jobs" and "user/logs" folder before do upload
-    cmd_mkdir = "mkdir -p ~/user/jobs; mkdir -p ~/user/logs"
-    _exec_cmd_on_cluster(
-        config,
-        call_context=call_context,
-        cmd=cmd_mkdir
+    cmds, session_name = prepare(
+        *prepare_args,
     )
-    command_parts = []
-    if urllib.parse.urlparse(script).scheme in ("http", "https"):
-        command_parts = ["wget", quote(script), "-O", f"~/user/jobs/{target_name};"]
-    else:
-        # upload the script to cluster
-        _rsync(
-            config,
-            call_context=call_context,
-            source=script,
-            target=target,
-            down=False)
-
-    # Use new target with $HOME instead of ~ for exec
-    target = os.path.join("$HOME", "user", "jobs", target_name)
-    if runtime is not None:
-        runtime_commands = get_runnable_command(
-            config.get(RUNTIME_CONFIG_KEY), target, runtime, runtime_options)
-        if runtime_commands is None:
-            cli_logger.abort("Runtime {} doesn't how to execute your file: {}", runtime, script)
-        command_parts += runtime_commands
-    elif target_name.endswith(".py"):
-        command_parts += ["python", double_quote(target)]
-    elif target_name.endswith(".sh"):
-        command_parts += ["bash", double_quote(target)]
-    else:
-        runtime_commands = get_runnable_command(config.get(RUNTIME_CONFIG_KEY), target)
-        if runtime_commands is None:
-            cli_logger.abort("We don't how to execute your file: {}", script)
-        command_parts += runtime_commands
-
-    with_script_args(command_parts, script_args)
-
-    # If user uses screen or tmux and job waiter is used
-    # which means to not hold the tmux or screen session, we redirect log with the session name
-    user_cmd = " ".join(command_parts)
-    session_name = get_command_session_name(user_cmd, time.time_ns())
-    if job_log or ((screen or tmux) and job_waiter is not None):
-        redirect_output = f">$HOME/user/logs/{session_name}.log"
-        command_parts += [redirect_output, "2>&1"]
 
-    cmd = " ".join(command_parts)
+    cmd = " ".join(cmds)
     return _exec_cluster(
         config,
         call_context=call_context,
         cmd=cmd,
         screen=screen,
         tmux=tmux,
         stop=stop,
         start=False,
         port_forward=port_forward,
         with_output=with_output,
         job_waiter=job_waiter,
         session_name=session_name)
 
 
+def _run_script_on_head(
+        config: Dict[str, Any],
+        call_context: CallContext,
+        script: str,
+        script_args: Optional[List[str]] = None,
+        wait_for_workers: bool = False,
+        min_workers: Optional[int] = None,
+        wait_timeout: Optional[int] = None,
+        with_output: bool = False):
+    # wait for workers if needed
+    if wait_for_workers:
+        _wait_for_ready(
+            config=config, call_context=call_context,
+            min_workers=min_workers, timeout=wait_timeout)
+
+    return run_script(
+        script, script_args,
+        with_output=with_output)
+
+
 def _get_workers_ready(config: Dict[str, Any], provider):
     workers = _get_worker_nodes(config)
     workers_info = get_nodes_info(provider, workers)
 
     # get working nodes which are ready
     workers_ready = _get_node_number_in_status(workers_info, STATUS_UP_TO_DATE)
     return workers_ready
```

## cloudtik/core/_private/cluster/cluster_scaler.py

```diff
@@ -187,15 +187,15 @@
         self.config_hash = None
         # TODO: Each node updater may need its own CallContext
         # The call context for node updater
         self.call_context = CallContext()
 
         # Setup verbosity based on logging level
         if logger.isEnabledFor(logging.DEBUG):
-            self.call_context.cli_logger.set_verbosity(1)
+            self.call_context.cli_logger.set_verbosity(2)
         else:
             self.call_context.cli_logger.set_verbosity(0)
 
         # Keep this before self.reset (self.provider needs to be created
         # exactly once).
         self.provider = None
         # Keep this before self.reset (if an exception occurs in reset
```

## cloudtik/core/_private/command_executor/docker_command_executor.py

```diff
@@ -11,15 +11,16 @@
 from cloudtik.core._private.constants import \
     CLOUDTIK_DEFAULT_SHARED_MEMORY_MAX_BYTES, \
     CLOUDTIK_DATA_DISK_MOUNT_POINT
 from cloudtik.core._private.docker import check_bind_mounts_cmd, \
     check_docker_running_cmd, \
     check_docker_image, \
     docker_start_cmds, \
-    with_docker_exec, get_configured_docker_image, get_docker_cmd, with_docker_cmd
+    with_docker_exec, get_configured_docker_image, get_docker_cmd, with_docker_cmd, \
+    get_docker_host_mount_location_for_object
 
 logger = logging.getLogger(__name__)
 
 # How long to wait for a node to start, in seconds
 CHECK_DOCKER_RUNTIME_NUMBER_OF_RETRIES = 5
 
 
@@ -83,28 +84,30 @@
             with_output=with_output,
             ssh_options_override_ssh_key=ssh_options_override_ssh_key,
             cmd_to_print=cmd_to_print,
             silent=silent)
 
     def run_rsync_up(self, source, target, options=None):
         options = options or {}
-        host_destination = os.path.join(
-            self._get_docker_host_mount_location(
-                self.host_command_executor.cluster_name), target.lstrip("/"))
+        do_with_rsync = self._check_container_status() and not options.get(
+                "docker_mount_if_possible", False)
+        identical = False if do_with_rsync else True
+        host_destination = get_docker_host_mount_location_for_object(
+            self.host_command_executor.cluster_name, target,
+            identical=identical)
 
         host_mount_location = os.path.dirname(host_destination.rstrip("/"))
         self.host_command_executor.run(
             f"mkdir -p {host_mount_location} && chown -R "
             f"{self.host_command_executor.ssh_user} {host_mount_location}",
             silent=self.call_context.is_rsync_silent())
 
         self.host_command_executor.run_rsync_up(
             source, host_destination, options=options)
-        if self._check_container_status() and not options.get(
-                "docker_mount_if_possible", False):
+        if do_with_rsync:
             if os.path.isdir(source):
                 # Adding a "." means that docker copies the *contents*
                 # Without it, docker copies the source *into* the target
                 host_destination += "/."
 
             # This path may not exist inside the container. This ensures
             # that the path is created!
@@ -121,27 +124,29 @@
                 "{} && rsync -e '{} exec -i' -avz {} {}:{}".format(
                     prefix, self.get_docker_cmd(), host_destination,
                     self.container_name, self._docker_expand_user(target)),
                 silent=self.call_context.is_rsync_silent())
 
     def run_rsync_down(self, source, target, options=None):
         options = options or {}
-        host_source = os.path.join(
-            self._get_docker_host_mount_location(
-                self.host_command_executor.cluster_name), source.lstrip("/"))
+        do_with_rsync = not options.get("docker_mount_if_possible", False)
+        identical = False if do_with_rsync else True
+        host_source = get_docker_host_mount_location_for_object(
+            self.host_command_executor.cluster_name, source,
+            identical=identical)
         host_mount_location = os.path.dirname(host_source.rstrip("/"))
         self.host_command_executor.run(
             f"mkdir -p {host_mount_location} && chown -R "
             f"{self.host_command_executor.ssh_user} {host_mount_location}",
             silent=self.call_context.is_rsync_silent())
         if source[-1] == "/":
             source += "."
             # Adding a "." means that docker copies the *contents*
             # Without it, docker copies the source *into* the target
-        if not options.get("docker_mount_if_possible", False):
+        if do_with_rsync:
             # NOTE: `--delete` is okay here because the container is the source
             # of truth.
             self.host_command_executor.run(
                 "rsync -e '{} exec -i' -avz --delete {}:{} {}".format(
                     self.get_docker_cmd(), self.container_name,
                     self._docker_expand_user(source), host_source),
                 silent=self.call_context.is_rsync_silent())
@@ -337,14 +342,15 @@
                 self.get_docker_cmd(),
                 network=self.docker_config.get("network"),
                 cpus=self.docker_config.get("cpus"),
                 memory=self.docker_config.get("memory"),
                 labels=self.docker_config.get("labels"),
                 port_mappings=self.docker_config.get("port_mappings"),
                 mounts_mapping=self.docker_config.get("mounts_mapping", True),
+                ipc_mode=self.docker_config.get("ipc_mode"),
             )
             self.run_with_retry(
                 start_command, run_env="host")
             docker_run_executed = True
 
         # Explicitly copy in bootstrap files.
         for mount in bootstrap_mounts:
@@ -354,17 +360,16 @@
                     #  a stopped instance,  /tmp may be deleted and `run_init`
                     # is called before the first `file_sync` happens
                     self.run_rsync_up(file_mounts[mount], mount)
                 self.host_command_executor.run_with_retry(
                     "rsync -e '{cmd} exec -i' -avz {src} {container}:{dst}".
                     format(
                         cmd=self.get_docker_cmd(),
-                        src=os.path.join(
-                            self._get_docker_host_mount_location(
-                                self.host_command_executor.cluster_name), mount),
+                        src=get_docker_host_mount_location_for_object(
+                                self.host_command_executor.cluster_name, mount),
                         container=self.container_name,
                         dst=self._docker_expand_user(mount)))
                 try:
                     # Check if the current user has read permission.
                     # If they do not, try to change ownership!
                     self.run_with_retry(
                         f"cat {mount} >/dev/null 2>&1 || "
@@ -460,20 +465,14 @@
 
             return run_options + [f"--shm-size='{shm_size}b'"]
         except Exception as e:
             logger.warning(
                 f"Received error while trying to auto-compute SHM size {e}")
             return run_options
 
-    def _get_docker_host_mount_location(self, cluster_name: str) -> str:
-        """Return the docker host mount directory location."""
-        # Imported here due to circular dependency in imports.
-        from cloudtik.core.api import get_docker_host_mount_location
-        return get_docker_host_mount_location(cluster_name)
-
     def _get_host_data_disks(self):
         mount_point = CLOUDTIK_DATA_DISK_MOUNT_POINT
         data_disks_string = self.run_with_retry(
             "([ -d {} ] && ls --color=no {}) || true".format(mount_point, mount_point),
             with_output=True,
             run_env="host").decode("utf-8").strip()
         if data_disks_string is None or data_disks_string == "":
```

## cloudtik/providers/_private/virtual/virtual_container_scheduler.py

```diff
@@ -430,14 +430,15 @@
 
         # prepare docker config
         docker_config = _get_merged_docker_config_from_node_config(
             self.docker_config, node_config)
         # make a copy before change it
         docker_config = copy.deepcopy(docker_config)
         docker_config["mounts_mapping"] = False
+        docker_config["ipc_mode"] = "private"
 
         port_mappings = node_config.get("port_mappings")
         if port_mappings:
             docker_config["port_mappings"] = copy.deepcopy(port_mappings)
 
         if is_head_node and not _is_use_internal_ip(self.provider_config):
             # set bridge ip as external ip store in tags
```

## cloudtik/runtime/ai/__init__.py

```diff
@@ -0,0 +1,36 @@
+00000000: 696d 706f 7274 206f 730a 0a23 2064 6566  import os..# def
+00000010: 696e 6520 7468 6520 7275 6e74 696d 6520  ine the runtime 
+00000020: 7363 7269 7074 2061 6c69 6173 2077 6869  script alias whi
+00000030: 6368 2063 616e 2062 6520 7275 6e20 7369  ch can be run si
+00000040: 6d70 6c79 0a23 2061 766f 6964 2074 6f20  mply.# avoid to 
+00000050: 7573 6520 696d 706f 7274 2073 6f20 7468  use import so th
+00000060: 6174 2074 6865 2064 6570 656e 6465 6e63  at the dependenc
+00000070: 6965 7320 7769 6c6c 206e 6f74 2063 6175  ies will not cau
+00000080: 7365 2065 7272 6f72 0a5f 7363 7269 7074  se error._script
+00000090: 5f61 6c69 6173 6573 5f20 3d20 7b0a 2020  _aliases_ = {.  
+000000a0: 2020 2261 692e 6c61 756e 6368 223a 206f    "ai.launch": o
+000000b0: 732e 7061 7468 2e6a 6f69 6e28 0a20 2020  s.path.join(.   
+000000c0: 2020 2020 206f 732e 7061 7468 2e64 6972       os.path.dir
+000000d0: 6e61 6d65 285f 5f66 696c 655f 5f29 2c0a  name(__file__),.
+000000e0: 2020 2020 2020 2020 2272 756e 6e65 722f          "runner/
+000000f0: 6c61 756e 6368 2e70 7922 292c 0a20 2020  launch.py"),.   
+00000100: 2022 6169 2e6d 6f64 656c 696e 672e 6772   "ai.modeling.gr
+00000110: 6170 685f 7361 6765 223a 206f 732e 7061  aph_sage": os.pa
+00000120: 7468 2e6a 6f69 6e28 0a20 2020 2020 2020  th.join(.       
+00000130: 206f 732e 7061 7468 2e64 6972 6e61 6d65   os.path.dirname
+00000140: 285f 5f66 696c 655f 5f29 2c0a 2020 2020  (__file__),.    
+00000150: 2020 2020 226d 6f64 656c 696e 672f 6772      "modeling/gr
+00000160: 6170 685f 6d6f 6465 6c69 6e67 2f67 7261  aph_modeling/gra
+00000170: 7068 5f73 6167 652f 6d6f 6465 6c69 6e67  ph_sage/modeling
+00000180: 2f72 756e 2e70 7922 292c 0a20 2020 2022  /run.py"),.    "
+00000190: 6169 2e6d 6f64 656c 696e 672e 7867 626f  ai.modeling.xgbo
+000001a0: 6f73 7422 3a20 6f73 2e70 6174 682e 6a6f  ost": os.path.jo
+000001b0: 696e 280a 2020 2020 2020 2020 6f73 2e70  in(.        os.p
+000001c0: 6174 682e 6469 726e 616d 6528 5f5f 6669  ath.dirname(__fi
+000001d0: 6c65 5f5f 292c 0a20 2020 2020 2020 2022  le__),.        "
+000001e0: 6d6f 6465 6c69 6e67 2f63 6c61 7373 6963  modeling/classic
+000001f0: 616c 5f6d 6c2f 636c 6173 7369 6669 6361  al_ml/classifica
+00000200: 7469 6f6e 5f61 6e64 5f72 6567 7265 7373  tion_and_regress
+00000210: 696f 6e2f 7867 626f 6f73 742f 6d6f 6465  ion/xgboost/mode
+00000220: 6c69 6e67 2f72 756e 2e70 7922 292c 0a7d  ling/run.py"),.}
+00000230: 0a                                       .
```

## cloudtik/runtime/ai/api.py

```diff
@@ -1,29 +1,35 @@
 """IMPORTANT: this is an experimental interface and not currently stable."""
 
-from typing import Union
+from typing import Union, Optional
 
 from cloudtik.core.api import Cluster, ThisCluster
 from cloudtik.runtime.ai.utils import get_runtime_services
 
 
 class AICluster(Cluster):
-    def __init__(self, cluster_config: Union[dict, str], should_bootstrap: bool = True) -> None:
+    def __init__(
+            self, cluster_config: Union[dict, str],
+            should_bootstrap: bool = True,
+            no_config_cache: bool = True,
+            verbosity: Optional[int] = None) -> None:
         """Create a Spark cluster object to operate on with this API.
 
         Args:
             cluster_config (Union[str, dict]): Either the config dict of the
                 cluster, or a path pointing to a file containing the config.
         """
-        Cluster.__init__(self, cluster_config, should_bootstrap)
+        super().__init__(
+            cluster_config, should_bootstrap,
+            no_config_cache, verbosity)
 
     def get_services(self):
         return get_runtime_services(self.config)
 
 
 class ThisAICluster(ThisCluster):
-    def __init__(self) -> None:
+    def __init__(self, verbosity: Optional[int] = None) -> None:
         """Create a Spark cluster object to operate on with this API on head."""
-        ThisCluster.__init__(self)
+        super().__init__(verbosity)
 
     def get_services(self):
         return get_runtime_services(self.config)
```

## cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/config/training-config.yaml

```diff
@@ -1,15 +1,7 @@
-data_spec:
-  target_col: is_fraud?
-  ignore_cols: ['merchant_name','user', 'card', 'split']
-  data_split:
-    train: df[df["year"]<2018]
-    valid: df[df["year"]==2018]
-    test: df[df["year"]>2018]
-
 model_spec:
   model_type: xgboost
   model_params:
     learning_rate: 0.1
     eval_metric: 'aucpr'
     objective: 'binary:logistic'
   training_params:
```

## cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/__init__.py

```diff
@@ -0,0 +1,63 @@
+00000000: 6672 6f6d 2063 6c6f 7564 7469 6b2e 7275  from cloudtik.ru
+00000010: 6e74 696d 652e 6169 2e6d 6f64 656c 696e  ntime.ai.modelin
+00000020: 672e 636c 6173 7369 6361 6c5f 6d6c 2e63  g.classical_ml.c
+00000030: 6c61 7373 6966 6963 6174 696f 6e5f 616e  lassification_an
+00000040: 645f 7265 6772 6573 7369 6f6e 2e78 6762  d_regression.xgb
+00000050: 6f6f 7374 2e6d 6f64 656c 696e 672e 7275  oost.modeling.ru
+00000060: 6e20 5c0a 2020 2020 696d 706f 7274 2072  n \.    import r
+00000070: 756e 2061 7320 5f72 756e 0a0a 0a63 6c61  un as _run...cla
+00000080: 7373 204d 6f64 656c 696e 6741 7267 7328  ss ModelingArgs(
+00000090: 6f62 6a65 6374 293a 0a20 2020 2064 6566  object):.    def
+000000a0: 205f 5f69 6e69 745f 5f28 7365 6c66 293a   __init__(self):
+000000b0: 0a20 2020 2020 2020 2073 656c 662e 7369  .        self.si
+000000c0: 6e67 6c65 5f6e 6f64 6520 3d20 4661 6c73  ngle_node = Fals
+000000d0: 650a 2020 2020 2020 2020 7365 6c66 2e6e  e.        self.n
+000000e0: 6f5f 7072 6f63 6573 735f 6461 7461 203d  o_process_data =
+000000f0: 2046 616c 7365 0a20 2020 2020 2020 2073   False.        s
+00000100: 656c 662e 6e6f 5f74 7261 696e 203d 2046  elf.no_train = F
+00000110: 616c 7365 0a20 2020 2020 2020 2073 656c  alse.        sel
+00000120: 662e 6e6f 5f70 7265 6469 6374 203d 2046  f.no_predict = F
+00000130: 616c 7365 0a0a 2020 2020 2020 2020 7365  alse..        se
+00000140: 6c66 2e69 6e5f 6d65 6d6f 7279 203d 2046  lf.in_memory = F
+00000150: 616c 7365 0a20 2020 2020 2020 2073 656c  alse.        sel
+00000160: 662e 7261 775f 6461 7461 5f70 6174 6820  f.raw_data_path 
+00000170: 3d20 4e6f 6e65 0a20 2020 2020 2020 2073  = None.        s
+00000180: 656c 662e 6461 7461 5f70 726f 6365 7373  elf.data_process
+00000190: 696e 675f 636f 6e66 6967 203d 204e 6f6e  ing_config = Non
+000001a0: 650a 2020 2020 2020 2020 7365 6c66 2e6e  e.        self.n
+000001b0: 6f5f 7361 7665 203d 2046 616c 7365 0a20  o_save = False. 
+000001c0: 2020 2020 2020 2073 656c 662e 7072 6f63         self.proc
+000001d0: 6573 7365 645f 6461 7461 5f70 6174 6820  essed_data_path 
+000001e0: 3d20 4e6f 6e65 0a0a 2020 2020 2020 2020  = None..        
+000001f0: 7365 6c66 2e64 6174 6173 6574 5f63 6f6e  self.dataset_con
+00000200: 6669 6720 3d20 4e6f 6e65 0a20 2020 2020  fig = None.     
+00000210: 2020 2073 656c 662e 7472 6169 6e69 6e67     self.training
+00000220: 5f63 6f6e 6669 6720 3d20 4e6f 6e65 0a20  _config = None. 
+00000230: 2020 2020 2020 2073 656c 662e 7465 6d70         self.temp
+00000240: 5f64 6972 203d 204e 6f6e 650a 2020 2020  _dir = None.    
+00000250: 2020 2020 7365 6c66 2e6f 7574 7075 745f      self.output_
+00000260: 6469 7220 3d20 4e6f 6e65 0a20 2020 2020  dir = None.     
+00000270: 2020 2073 656c 662e 6d6f 6465 6c5f 6669     self.model_fi
+00000280: 6c65 203d 204e 6f6e 650a 2020 2020 2020  le = None.      
+00000290: 2020 7365 6c66 2e70 7265 6469 6374 5f6f    self.predict_o
+000002a0: 7574 7075 7420 3d20 4e6f 6e65 0a0a 2020  utput = None..  
+000002b0: 2020 2020 2020 2320 7261 7920 7061 7261        # ray para
+000002c0: 6d73 0a20 2020 2020 2020 2073 656c 662e  ms.        self.
+000002d0: 6e75 6d5f 6163 746f 7273 203d 2035 0a20  num_actors = 5. 
+000002e0: 2020 2020 2020 2073 656c 662e 6370 7573         self.cpus
+000002f0: 5f70 6572 5f61 6374 6f72 203d 2031 350a  _per_actor = 15.
+00000300: 2020 2020 2020 2020 7365 6c66 2e67 7075          self.gpu
+00000310: 735f 7065 725f 6163 746f 7220 3d20 2d31  s_per_actor = -1
+00000320: 0a20 2020 2020 2020 2073 656c 662e 656c  .        self.el
+00000330: 6173 7469 635f 7472 6169 6e69 6e67 203d  astic_training =
+00000340: 2046 616c 7365 0a20 2020 2020 2020 2073   False.        s
+00000350: 656c 662e 6d61 785f 6661 696c 6564 5f61  elf.max_failed_a
+00000360: 6374 6f72 7320 3d20 340a 2020 2020 2020  ctors = 4.      
+00000370: 2020 7365 6c66 2e6d 6178 5f61 6374 6f72    self.max_actor
+00000380: 5f72 6573 7461 7274 7320 3d20 380a 2020  _restarts = 8.  
+00000390: 2020 2020 2020 7365 6c66 2e63 6865 636b        self.check
+000003a0: 706f 696e 745f 6672 6571 7565 6e63 7920  point_frequency 
+000003b0: 3d20 350a 0a0a 6465 6620 7275 6e28 6172  = 5...def run(ar
+000003c0: 6773 3a20 4d6f 6465 6c69 6e67 4172 6773  gs: ModelingArgs
+000003d0: 293a 0a20 2020 2072 6574 7572 6e20 5f72  ):.    return _r
+000003e0: 756e 2861 7267 7329 0a                   un(args).
```

## cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/run.py

```diff
@@ -1,35 +1,88 @@
 import argparse
+import os
+import tempfile
 
-from .process_data import process_data
-from .utils import existing_file, existing_directory, load_config, read_csv_files, DATA_ENGINE_PANDAS, DATA_ENGINE_MODIN
+from cloudtik.runtime.ai.modeling.classical_ml.classification_and_regression.xgboost.modeling.data.process \
+    import process_data
+from cloudtik.runtime.ai.modeling.classical_ml.classification_and_regression.xgboost.modeling.utils import \
+    existing_file, existing_path, load_config, read_csv_files, DATA_ENGINE_PANDAS, DATA_ENGINE_MODIN
+
+
+def _get_config_dir():
+    return os.path.join(
+        os.path.dirname(os.path.dirname(__file__)), "config")
+
+
+def _process_data(args, data_engine):
+    if not args.raw_data_path:
+        raise ValueError(
+            "Must specify the raw data path which contains data to be processed.")
+
+    if not args.data_processing_config:
+        # default to the built-in data_processing_config.yaml if not specified
+        args.data_processing_config = os.path.join(
+            _get_config_dir(), "data-processing-config.yaml")
+        print("data-processing-config is not specified. Default to: {}".format(
+            args.data_processing_config))
 
+    if args.in_memory or args.no_save:
+        processed_data_path = None
+    else:
+        processed_data_path = args.processed_data_path
+        if not processed_data_path:
+            raise RuntimeError(
+                "Please specify the file path of processed data.")
+    train_data, test_data = process_data(
+        raw_data_path=args.raw_data_path,
+        data_engine=data_engine,
+        data_processing_config=args.data_processing_config,
+        output_file=processed_data_path
+    )
+    return train_data, test_data
+
+
+def _train_on_data(
+        args, df,
+        dataset_config, training_config,
+        temp_dir, model_file, in_memory):
+    train_data_spec = load_config(dataset_config)
+    training_config = load_config(training_config)
 
-def train_on_ray(
-        df, training_config, temp_dir, model_file,
-        in_memory, on_ray=False, ray_params=None):
-    print('start training models on ray...')
-    config = load_config(training_config)
-    train_data_spec = config['data_spec']
-    hpo_spec = config.get('hpo_spec')
-    train_model_spec = config.get('model_spec')
+    hpo_spec = training_config.get('hpo_spec')
+    train_model_spec = training_config.get('model_spec')
     if hpo_spec is None and train_model_spec is None:
         raise RuntimeError("Must specify either hpo_spec or model_spec.")
 
     if hpo_spec is not None:
         print("Do training with hyper parameter optimization (HPO).")
     else:
         print("Do training without hyper parameter optimization (HPO).")
 
-    from .train_ray import train_ray
-    train_ray(
-        train_data_spec, df,
-        train_model_spec, in_memory,
-        tmp_path=temp_dir, model_file=model_file,
-        on_ray=on_ray, ray_params=ray_params, hpo_spec=hpo_spec)
+    on_ray = True if not args.single_node else False
+
+    print('Start training model...')
+    if on_ray:
+        ray_params = get_ray_params(args)
+        from cloudtik.runtime.ai.modeling.classical_ml.classification_and_regression.\
+            xgboost.modeling.model.ray.trainer import train
+        train(
+            train_data_spec, df,
+            train_model_spec, in_memory,
+            tmp_path=temp_dir, model_file=model_file,
+            hpo_spec=hpo_spec,
+            ray_params=ray_params)
+    else:
+        from cloudtik.runtime.ai.modeling.classical_ml.classification_and_regression.\
+            xgboost.modeling.model.trainer import train
+        train(
+            train_data_spec, df,
+            train_model_spec, in_memory,
+            tmp_path=temp_dir, model_file=model_file,
+            hpo_spec=hpo_spec)
 
 
 def get_ray_params(args):
     ray_params = {}
     if args.num_actors:
         ray_params["num_actors"] = args.num_actors
     if args.cpus_per_actor:
@@ -43,119 +96,229 @@
     if args.max_actor_restarts:
         ray_params["max_actor_restarts"] = args.max_actor_restarts
     if args.checkpoint_frequency:
         ray_params["checkpoint_frequency"] = args.checkpoint_frequency
     return ray_params
 
 
+def _check_temp_dir(args):
+    if args.single_node:
+        if not args.temp_dir:
+            # for single node, get get a default temp dir from /tmp
+            args.temp_dir = tempfile.mkdtemp()
+            print("temp-dir is not specified. Default to: {}".format(
+                args.temp_dir))
+    else:
+        if not args.temp_dir:
+            raise ValueError(
+                "Must specify the temp-dir for storing the shared intermediate data")
+
+
+def _check_predict_output(args):
+    if not args.predict_output and args.output_dir:
+        args.predict_output = os.path.join(
+            args.output_dir, "predict_output.csv")
+        print("predict-output is not specified. Default to: {}".format(
+            args.predict_output))
+
+
+def _train(args, data_engine, train_data, test_data):
+    if not args.in_memory and not args.processed_data_path:
+        raise ValueError(
+            "Must specify the processed-data-path which contains processed data to be trained.")
+    _check_temp_dir(args)
+
+    if not args.dataset_config:
+        # default to the built-in training_config.yaml if not specified
+        args.dataset_config = os.path.join(
+            _get_config_dir(), "dataset-config.yaml")
+        print("dataset-config is not specified. Default to: {}".format(
+            args.dataset_config))
+
+    if not args.training_config:
+        # default to the built-in training_config.yaml if not specified
+        args.training_config = os.path.join(
+            _get_config_dir(), "training-config.yaml")
+        print("training-config is not specified. Default to: {}".format(
+            args.training_config))
+
+    if args.in_memory:
+        if data_engine == DATA_ENGINE_PANDAS:
+            import pandas as pd
+        else:
+            import modin.pandas as pd
+        data = pd.concat([train_data, test_data])
+    else:
+        print(f"loading data from: {args.processed_data_path}")
+        data = read_csv_files(
+            args.processed_data_path, engine=DATA_ENGINE_PANDAS)
+
+    _train_on_data(
+        args, data,
+        dataset_config=args.dataset_config,
+        training_config=args.training_config,
+        temp_dir=args.temp_dir,
+        model_file=args.model_file,
+        in_memory=args.in_memory
+    )
+
+
+def _predict(args, test_data):
+    # for prediction the dataset config can be empty explicitly
+    # for indicating dataset_config is not needed
+    if args.dataset_config is None:
+        # default to the built-in training_config.yaml if None
+        args.dataset_config = os.path.join(
+            _get_config_dir(), "dataset-config.yaml")
+        print("dataset-config is not specified. Default to: {}. "
+              "You can set a empty value explicitly if it is not needed.".format(args.dataset_config))
+
+    if not args.in_memory and not args.processed_data_path:
+        raise ValueError(
+            "Must specify the processed-data-path which contains processed data to predict.")
+    if args.in_memory:
+        # predict data always on pandas?
+        import pandas as pd
+        data = test_data
+    else:
+        print(f"loading data from: {args.processed_data_path}")
+        data = read_csv_files(
+            args.processed_data_path, engine=DATA_ENGINE_PANDAS)
+
+    if not args.model_file:
+        raise ValueError(
+            "Must specify the model-file to predict.")
+
+    _check_predict_output(args)
+
+    data_spec = load_config(
+        args.dataset_config) if args.dataset_config else None
+    from cloudtik.runtime.ai.modeling.classical_ml.classification_and_regression. \
+        xgboost.modeling.model.predictor import predict
+
+    print('Start predicting...')
+    predict(
+        data, args.model_file,
+        data_spec=data_spec,
+        predict_output=args.predict_output
+    )
+    print('End predicting.')
+
+
 def run(args):
     data_engine = DATA_ENGINE_PANDAS
-    if not args.is_single_node:
+    if not args.single_node:
         data_engine = DATA_ENGINE_MODIN
-
+    train_data, test_data = (None, None)
     if not args.no_process_data or args.in_memory:
-        if args.in_memory or args.no_save:
-            processed_data_file = None
-        else:
-            processed_data_file = args.processed_data_file
-            if not processed_data_file:
-                raise RuntimeError("Please specify the file path of processed data.")
-        train_data, test_data = process_data(
-            raw_data_path=args.raw_data_path,
-            data_engine=data_engine,
-            data_processing_config=args.data_processing_config,
-            output_file=processed_data_file
-        )
-
-    if not args.no_training:
-        if args.in_memory:
-            if data_engine == DATA_ENGINE_PANDAS:
-                import pandas as pd
-            else:
-                import modin.pandas as pd
-            data = pd.concat([train_data, test_data])
-        else:
-            data = read_csv_files(
-                args.processed_data_file, engine=DATA_ENGINE_PANDAS)
+        train_data, test_data = _process_data(
+            args, data_engine)
 
-        on_ray = False
-        if not args.is_single_node:
-            on_ray = True
-
-        ray_params = get_ray_params(args)
-        train_on_ray(
-            data,
-            training_config=args.training_config,
-            temp_dir=args.temp_dir,
-            model_file=args.model_file,
-            in_memory=args.in_memory,
-            on_ray=on_ray,
-            ray_params=ray_params,
+    if not args.no_train:
+        _train(
+            args, data_engine, train_data, test_data)
+
+    if not args.no_predict:
+        _predict(
+            args, test_data,
         )
 
 
 if __name__ == "__main__":
     parser = argparse.ArgumentParser(description="Process data")
     parser.add_argument(
-        "--single_node", default=False, action="store_true",
+        "--single-node", "--single_node",
+        default=False, action="store_true",
         help="To do single node training")
     parser.add_argument(
-        "--no_process_data", default=False, action="store_true",
+        "--no-process-data", "--no_process_data",
+        default=False, action="store_true",
         help="whether to do data process")
     parser.add_argument(
-        "--no_training", default=False, action="store_true",
+        "--no-train", "--no_train",
+        default=False, action="store_true",
         help="whether to do training")
+    parser.add_argument(
+        "--no-predict", "--no_predict",
+        default=False, action="store_true",
+        help="whether to do predict")
 
     parser.add_argument(
-        "--in_memory", default=False, action="store_true",
+        "--in-memory", "--in_memory",
+        default=False, action="store_true",
         help="whether do in memory data processing and training without save processed data to file.")
 
     parser.add_argument(
-        "--raw_data_path", type=existing_directory, help="The path contains the raw data files")
+        "--raw-data-path", "--raw_data_path",
+        type=existing_path,
+        help="The path contains the raw data files or the file.")
+    parser.add_argument(
+        "--data-processing-config", "--data_processing_config",
+        type=existing_file, help="The path to the data processing config file")
+    parser.add_argument(
+        "--training-config", "--training_config",
+        type=existing_file,
+        help="The path to the training config file")
     parser.add_argument(
-        "--data_processing_config", type=existing_file, help="The path to the data processing config file")
+        "--no-save", "--no_save",
+        default=False, action="store_true",
+        help="whether to save the processed data file")
     parser.add_argument(
-        "--training_config", type=existing_file, help="The path to the training config file")
+        "--processed-data-path", "--processed_data_path",
+        type=str,
+        help="The path to the output processed data")
     parser.add_argument(
-        "--no_save", default=False, action="store_true",
-        help="whether to save the processed data file")
+        "--temp-dir", "--temp_dir",
+        type=str,
+        help="The path to the shared intermediate data")
     parser.add_argument(
-        "--processed_data_file",
+        "--output-dir", "--output_dir",
         type=str,
-        help="The path to the output processed data file")
+        help="The path to the output if not specified explicitly")
     parser.add_argument(
-        "--model_file",
+        "--model-file", "--model_file",
         type=str,
         help="The path to the output model file")
+    parser.add_argument(
+        "--predict-output", "--predict_output",
+        type=str,
+        help="The path to the predict output")
 
     # Ray params
     #     num_actors: 5
     #     cpus_per_actor: 15
     #     elastic_training: True
     #     max_failed_actors: 4
     #     max_actor_restarts: 8
     parser.add_argument(
-        "--num_actors", type=int, default=5,
+        "--num-actors", "--num_actors",
+        type=int, default=5,
         help="The number of actors")
     parser.add_argument(
-        "--cpus_per_actor", type=int, default=15,
+        "--cpus-per-actor", "--cpus_per_actor",
+        type=int, default=15,
         help="The number of cpus per actor")
     parser.add_argument(
-        "--gpus_per_actor", type=int,
+        "--gpus-per-actor", "--gpus_per_actor",
+        type=int, default=-1,
         help="The number of gpus per actor")
     parser.add_argument(
-        "--elastic_training", action="store_true",
+        "--elastic-training", "--elastic_training",
+        action="store_true", default=False,
         help="whether to use elastic training")
     parser.add_argument(
-        "--max_failed_actors", type=int, default=4,
+        "--max-failed-actors", "--max_failed_actors",
+        type=int, default=4,
         help="The max number of failed actors")
     parser.add_argument(
-        "--max_actor_restarts", type=int, default=8,
+        "--max-actor-restarts", "--max_actor_restarts",
+        type=int, default=8,
         help="The max number of actor restarts")
     parser.add_argument(
-        "--checkpoint_frequency", type=int,
+        "--checkpoint-frequency", "--checkpoint_frequency",
+        type=int, default=5,
         help="The checkpoint frequency")
 
     args = parser.parse_args()
     print(args)
 
     run(args)
```

## cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/utils.py

```diff
@@ -1,15 +1,15 @@
 import argparse
 import os
-from pathlib import Path
-import shutil
 import numpy as np
 import glob
 import yaml
 
+from cloudtik.runtime.ai.util.utils import clean_dir
+
 DATA_ENGINE_PANDAS = 'pandas'
 DATA_ENGINE_MODIN = 'modin'
 
 
 def existing_directory(raw_path):
     if not os.path.isdir(raw_path):
         raise argparse.ArgumentTypeError(
@@ -22,14 +22,22 @@
     if not os.path.isfile(raw_path):
         raise argparse.ArgumentTypeError(
             '"{}" is not an existing file'.format(raw_path)
         )
     return os.path.abspath(raw_path)
 
 
+def existing_path(raw_path):
+    if not os.path.exists(raw_path):
+        raise argparse.ArgumentTypeError(
+            '"{}" is not an existing directory or file'.format(raw_path)
+        )
+    return os.path.abspath(raw_path)
+
+
 def read_csv_file(file, pd, ignore_cols=None):
     csv = pd.read_csv(file)
     if ignore_cols is not None:
         print("dropping columns...")
         csv.drop(columns=ignore_cols, inplace=True)
     else:
         print("reading without dropping columns...")
@@ -56,23 +64,16 @@
         csv = read_csv_file(file, pd, ignore_cols)
         df.append(csv)
     data = pd.concat(df)
     print(f"data has the shape {data.shape}")
     return data
 
 
-def make_dir(path):
-    path = Path(path)
-    if path.exists() and path.is_dir():
-        shutil.rmtree(path)
-    os.makedirs(path)
-
-
 def partition_data(df, save_format, save_data_path, num_partitions):
-    make_dir(save_data_path)
+    clean_dir(save_data_path)
     if save_format == 'csv':
         df_splits = np.array_split(df, num_partitions)
         for i, data in enumerate(df_splits):
             data.to_csv(f"{save_data_path}/partition_{i}.csv", index=False)
     else:
         print("other data format not supported")
```

## cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/config/tabular2graph.yaml

```diff
@@ -1,19 +1,32 @@
 # Copyright (C) 2023 Intel Corporation
 # SPDX-License-Identifier: MIT
 
+node_types:
+  - card
+  - merchant
 node_columns:
-  - card_id
-  - merchant_id
+  card_id: card
+  merchant_id: merchant
+#list of column names you want to include in graph as node features
+#if column names have special characters or spaces wrap in double quotes
+node_features:
+  card_id: [card_id]
+  merchant_id: [merchant_id]
 #provide the column names in your CSV file that contain the entity (node) IDs
 #Edge type name in string triplet: [source node type, relation type, destination node type].
 edge_types:
-  - [card_id, transaction, merchant_id]
-  - [merchant_id, sym_transaction, card_id]
+  - [card_id, pay, merchant_id]
+  - [merchant_id, charge, card_id]
+reverse_edges:
+  pay: charge
+  charge: pay
 #list of column names you want to include in graph as edge features
 #if column names have special characters or spaces wrap in double quotes
-edge_features: [year,month,day,time,amount,merchant_name,merchant_city,merchant_state,zip,mcc,"use_chip_Chip Transaction","use_chip_Online Transaction","use_chip_Swipe Transaction","Bad CVV","Bad Card Number","Bad Expiration","Bad PIN","Bad Zipcode","Insufficient Balance","Technical Glitch"]
+edge_features:
+  pay: [year,month,day,time,amount,merchant_name,merchant_city,merchant_state,zip,mcc,"use_chip_Chip Transaction","use_chip_Online Transaction","use_chip_Swipe Transaction","Bad CVV","Bad Card Number","Bad Expiration","Bad PIN","Bad Zipcode","Insufficient Balance","Technical Glitch"]
+  charge: [year,month,day,time,amount,merchant_name,merchant_city,merchant_state,zip,mcc,"use_chip_Chip Transaction","use_chip_Online Transaction","use_chip_Swipe Transaction","Bad CVV","Bad Card Number","Bad Expiration","Bad PIN","Bad Zipcode","Insufficient Balance","Technical Glitch"]
 #column name in CSV that containers the label
 edge_label: "is_fraud?"
 #column name in CSV that contains train/test/val splits.
 #It is required (0,1,2) are used respectively and all three splits need to be present
 edge_split: split
```

## cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/__init__.py

```diff
@@ -0,0 +1,102 @@
+00000000: 6672 6f6d 2063 6c6f 7564 7469 6b2e 7275  from cloudtik.ru
+00000010: 6e74 696d 652e 6169 2e6d 6f64 656c 696e  ntime.ai.modelin
+00000020: 672e 6772 6170 685f 6d6f 6465 6c69 6e67  g.graph_modeling
+00000030: 2e67 7261 7068 5f73 6167 652e 6d6f 6465  .graph_sage.mode
+00000040: 6c69 6e67 2e72 756e 205c 0a20 2020 2069  ling.run \.    i
+00000050: 6d70 6f72 7420 2872 756e 2061 7320 5f72  mport (run as _r
+00000060: 756e 2c20 6765 745f 6461 7461 5f77 6974  un, get_data_wit
+00000070: 685f 656d 6265 6464 696e 6773 5f70 6174  h_embeddings_pat
+00000080: 6829 0a0a 0a63 6c61 7373 204d 6f64 656c  h)...class Model
+00000090: 696e 6741 7267 7328 6f62 6a65 6374 293a  ingArgs(object):
+000000a0: 0a20 2020 2064 6566 205f 5f69 6e69 745f  .    def __init_
+000000b0: 5f28 7365 6c66 293a 0a20 2020 2020 2020  _(self):.       
+000000c0: 2073 656c 662e 7369 6e67 6c65 5f6e 6f64   self.single_nod
+000000d0: 6520 3d20 5472 7565 0a20 2020 2020 2020  e = True.       
+000000e0: 2073 656c 662e 6e6f 5f70 726f 6365 7373   self.no_process
+000000f0: 5f64 6174 6120 3d20 4661 6c73 650a 2020  _data = False.  
+00000100: 2020 2020 2020 7365 6c66 2e6e 6f5f 6275        self.no_bu
+00000110: 696c 645f 6772 6170 6820 3d20 4661 6c73  ild_graph = Fals
+00000120: 650a 2020 2020 2020 2020 7365 6c66 2e6e  e.        self.n
+00000130: 6f5f 7061 7274 6974 696f 6e5f 6772 6170  o_partition_grap
+00000140: 6820 3d20 4661 6c73 650a 2020 2020 2020  h = False.      
+00000150: 2020 7365 6c66 2e6e 6f5f 7472 6169 6e20    self.no_train 
+00000160: 3d20 4661 6c73 650a 2020 2020 2020 2020  = False.        
+00000170: 7365 6c66 2e6e 6f5f 7072 6564 6963 7420  self.no_predict 
+00000180: 3d20 4661 6c73 650a 0a20 2020 2020 2020  = False..       
+00000190: 2073 656c 662e 7261 775f 6461 7461 5f70   self.raw_data_p
+000001a0: 6174 6820 3d20 4e6f 6e65 0a20 2020 2020  ath = None.     
+000001b0: 2020 2073 656c 662e 7072 6f63 6573 7365     self.processe
+000001c0: 645f 6461 7461 5f70 6174 6820 3d20 4e6f  d_data_path = No
+000001d0: 6e65 0a0a 2020 2020 2020 2020 7365 6c66  ne..        self
+000001e0: 2e74 656d 705f 6469 7220 3d20 4e6f 6e65  .temp_dir = None
+000001f0: 0a20 2020 2020 2020 2073 656c 662e 6f75  .        self.ou
+00000200: 7470 7574 5f64 6972 203d 204e 6f6e 650a  tput_dir = None.
+00000210: 2020 2020 2020 2020 7365 6c66 2e6d 6f64          self.mod
+00000220: 656c 5f66 696c 6520 3d20 4e6f 6e65 0a20  el_file = None. 
+00000230: 2020 2020 2020 2073 656c 662e 6461 7461         self.data
+00000240: 7365 745f 6e61 6d65 203d 2022 7461 6266  set_name = "tabf
+00000250: 6f72 6d65 7222 0a20 2020 2020 2020 2073  ormer".        s
+00000260: 656c 662e 7461 6275 6c61 7232 6772 6170  elf.tabular2grap
+00000270: 6820 3d20 4e6f 6e65 0a20 2020 2020 2020  h = None.       
+00000280: 2073 656c 662e 7472 6169 6e5f 6f75 7470   self.train_outp
+00000290: 7574 203d 204e 6f6e 650a 2020 2020 2020  ut = None.      
+000002a0: 2020 7365 6c66 2e70 7265 6469 6374 5f6f    self.predict_o
+000002b0: 7574 7075 7420 3d20 4e6f 6e65 0a0a 2020  utput = None..  
+000002c0: 2020 2020 2020 7365 6c66 2e64 6174 615f        self.data_
+000002d0: 7769 7468 5f65 6d62 6564 6469 6e67 735f  with_embeddings_
+000002e0: 6e61 6d65 203d 204e 6f6e 650a 0a20 2020  name = None..   
+000002f0: 2020 2020 2073 656c 662e 686f 7374 7320       self.hosts 
+00000300: 3d20 4e6f 6e65 0a20 2020 2020 2020 2073  = None.        s
+00000310: 656c 662e 6772 6170 685f 6e61 6d65 203d  elf.graph_name =
+00000320: 2022 7461 6266 6f72 6d65 725f 6772 6170   "tabformer_grap
+00000330: 6822 0a20 2020 2020 2020 2073 656c 662e  h".        self.
+00000340: 6e75 6d5f 7061 7274 7320 3d20 300a 2020  num_parts = 0.  
+00000350: 2020 2020 2020 7365 6c66 2e6e 756d 5f68        self.num_h
+00000360: 6f70 7320 3d20 310a 2020 2020 2020 2020  ops = 1.        
+00000370: 7365 6c66 2e6e 756d 5f74 7261 696e 6572  self.num_trainer
+00000380: 7320 3d20 310a 2020 2020 2020 2020 7365  s = 1.        se
+00000390: 6c66 2e6e 756d 5f73 616d 706c 6572 7320  lf.num_samplers 
+000003a0: 3d20 300a 2020 2020 2020 2020 7365 6c66  = 0.        self
+000003b0: 2e6e 756d 5f73 6572 7665 7273 203d 2031  .num_servers = 1
+000003c0: 0a0a 2020 2020 2020 2020 7365 6c66 2e6e  ..        self.n
+000003d0: 756d 5f73 6572 7665 725f 7468 7265 6164  um_server_thread
+000003e0: 7320 3d20 310a 2020 2020 2020 2020 7365  s = 1.        se
+000003f0: 6c66 2e6e 756d 5f6f 6d70 5f74 6872 6561  lf.num_omp_threa
+00000400: 6473 203d 204e 6f6e 650a 0a20 2020 2020  ds = None..     
+00000410: 2020 2023 2054 6865 7365 2064 6566 6175     # These defau
+00000420: 6c74 7320 6172 6520 7365 7420 666f 7220  lts are set for 
+00000430: 6469 7374 7269 6275 7465 6420 7472 6169  distributed trai
+00000440: 6e69 6e67 0a20 2020 2020 2020 2073 656c  ning.        sel
+00000450: 662e 6e75 6d5f 6570 6f63 6873 203d 2031  f.num_epochs = 1
+00000460: 300a 2020 2020 2020 2020 7365 6c66 2e6e  0.        self.n
+00000470: 756d 5f68 6964 6465 6e20 3d20 3634 0a20  um_hidden = 64. 
+00000480: 2020 2020 2020 2073 656c 662e 6e75 6d5f         self.num_
+00000490: 6c61 7965 7273 203d 2032 0a20 2020 2020  layers = 2.     
+000004a0: 2020 2073 656c 662e 6661 6e5f 6f75 7420     self.fan_out 
+000004b0: 3d20 2235 352c 3635 220a 2020 2020 2020  = "55,65".      
+000004c0: 2020 7365 6c66 2e62 6174 6368 5f73 697a    self.batch_siz
+000004d0: 6520 3d20 3230 3438 0a20 2020 2020 2020  e = 2048.       
+000004e0: 2073 656c 662e 6261 7463 685f 7369 7a65   self.batch_size
+000004f0: 5f65 7661 6c20 3d20 3130 3030 3030 300a  _eval = 1000000.
+00000500: 2020 2020 2020 2020 7365 6c66 2e65 7661          self.eva
+00000510: 6c5f 6576 6572 7920 3d20 310a 2020 2020  l_every = 1.    
+00000520: 2020 2020 7365 6c66 2e6c 7220 3d20 302e      self.lr = 0.
+00000530: 3030 3035 0a0a 2020 2020 2020 2020 7365  0005..        se
+00000540: 6c66 2e6c 6f67 5f65 7665 7279 203d 2032  lf.log_every = 2
+00000550: 300a 2020 2020 2020 2020 7365 6c66 2e6e  0.        self.n
+00000560: 756d 5f64 6c5f 776f 726b 6572 7320 3d20  um_dl_workers = 
+00000570: 340a 0a20 2020 2020 2020 2073 656c 662e  4..        self.
+00000580: 6865 7465 726f 6765 6e65 6f75 7320 3d20  heterogeneous = 
+00000590: 4661 6c73 650a 2020 2020 2020 2020 7365  False.        se
+000005a0: 6c66 2e72 656c 6174 696f 6e73 203d 204e  lf.relations = N
+000005b0: 6f6e 650a 2020 2020 2020 2020 7365 6c66  one.        self
+000005c0: 2e69 6e64 7563 7469 7665 203d 2046 616c  .inductive = Fal
+000005d0: 7365 0a20 2020 2020 2020 2073 656c 662e  se.        self.
+000005e0: 6e6f 6465 5f66 6561 7475 7265 203d 204e  node_feature = N
+000005f0: 6f6e 650a 2020 2020 2020 2020 7365 6c66  one.        self
+00000600: 2e65 7863 6c75 6465 5f72 6576 6572 7365  .exclude_reverse
+00000610: 5f65 6467 6573 203d 2054 7275 650a 0a0a  _edges = True...
+00000620: 6465 6620 7275 6e28 6172 6773 3a20 4d6f  def run(args: Mo
+00000630: 6465 6c69 6e67 4172 6773 293a 0a20 2020  delingArgs):.   
+00000640: 2072 6574 7572 6e20 5f72 756e 2861 7267   return _run(arg
+00000650: 7329 0a0a                                s)..
```

## cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/build_graph.py

```diff
@@ -1,60 +1,70 @@
-# Copyright (C) 2023 Intel Corporation
-# SPDX-License-Identifier: MIT
+"""
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+Author: Chen Haifeng
+"""
+
 
 import pandas as pd
 import numpy as np
 
 import time
 import yaml
 import os
-import argparse
-from collections import OrderedDict
 
-from .utils import existing_file
+from cloudtik.runtime.ai.modeling.graph_modeling.graph_sage.modeling.tokenizer import tokenize_node_ids, \
+    get_node_type_columns, values_of_node, get_mapped_column_of, get_node_type_of_column
+from cloudtik.runtime.ai.util.utils import clean_dir
 
 
 def build_graph(
         input_file, output_dir, dataset_name,
         tabular2graph):
     with open(tabular2graph, "r") as file:
         config = yaml.safe_load(file)
 
+    node_types = config["node_types"]
+    node_columns = config["node_columns"]
+    node_features = config.get("node_features")
+    edge_types = config["edge_types"]
+    edge_features = config.get("edge_features")
+
+    print("Build graph for:")
+    print("    node types:", node_types)
+    print("    edge types:", edge_types)
+
     output_dataset_dir = os.path.join(output_dir, dataset_name)
     print(output_dataset_dir)
-    os.makedirs(output_dataset_dir, exist_ok=True)
+    clean_dir(output_dataset_dir)
 
-    # 1. Load CSV file output of Classical ML edge featurization workflow
+    # 1. Load CSV file output for preprocessing
     print("Loading processed data")
     start = time.time()
     df = pd.read_csv(input_file)  # , nrows=10000)
     t_load_data = time.time()
-    print("Time lo load processed data", t_load_data - start)
+    print("Time to load processed data", t_load_data - start)
 
     # 2. Renumbering - generating node/edge ids starting from zero
     print("Node renumbering")
+    # heterogeneous mapping where all types start from zero
+    mapping, col_map = tokenize_node_ids(
+        df, config, heterogeneous=True)
 
-    def column_index(series, offset=0):
-        return {k: v + offset for v, k in enumerate(series.value_counts().index.values)}
-
-    # create dictionary of dictionary to stare node mapping for all node types
-    offset = 0
-    dict = OrderedDict()
-    # create mapping dictionary between original IDs and incremental IDs starting at zero
-    col_map = {}
-    for i, node in enumerate(config["node_columns"]):
-        key = str(node + "_2idx")
-        dict[key] = column_index(df[config["node_columns"][i]], offset=offset)
-        new_col_name = node + "_Idx"
-        col_map[node] = new_col_name
-        # add new Idx to dataframe
-        df[new_col_name] = df[config["node_columns"][i]].map(dict[key])
-        # offset = len(dict[key]) #remove if doing hetero mapping where all types start from zero
     t_renum = time.time()
-    print("Re-enumerated column map: ", col_map)
     print("Time to renumerate", t_renum - t_load_data)
 
     # 3. create masks for train, val and test splits (add new columns with masks)
     if config["edge_split"]:
         df = pd.concat(
             [
                 df,
@@ -68,110 +78,172 @@
     # 4. Prepare CSVDataset files for DGL to ingest and create graph
     print("Writing data into set of CSV files (nodes/edges)")
     # The specs for yaml file content and node/edge CSV file please refer to:
     # https://docs.dgl.ai/en/1.0.x/guide/data-loadcsv.html#guide-data-pipeline-loadcsv
 
     # programmatically create meta.yaml expected by DGL from yaml config
     list_of_n_dict = []
-    for i, node in enumerate(col_map.keys()):
+    for i, node_type in enumerate(node_types):
         list_of_n_dict.append(
-            {"file_name": "nodes_" + str(i) + ".csv", "ntype": col_map[node]}
+            {"file_name": "nodes_" + str(i) + ".csv", "ntype": node_type}
         )
 
     list_of_e_dict = []
-    for i, edge_type in enumerate(config["edge_types"]):
-        # replace node ids with the re-enumerated Idx
-        edge_type[0] = col_map[edge_type[0]]
-        edge_type[2] = col_map[edge_type[2]]
-        print(edge_type)
-        # keep edge type from tabular2graph.yaml and update node type to the new Idx ones
+    for i, edge_type in enumerate(edge_types):
+        src_node_type = get_node_type_of_column(edge_type[0], node_columns)
+        dst_node_type = get_node_type_of_column(edge_type[2], node_columns)
+        etype = [src_node_type, edge_type[1], dst_node_type]
         list_of_e_dict.append(
-            {"file_name": "edges_" + str(i) + ".csv", "etype": edge_type}
+            {"file_name": "edges_" + str(i) + ".csv", "etype": etype}
         )
 
     with open(os.path.join(output_dataset_dir, "meta.yaml"), "w") as f:
-        python_data = {
+        meta_yaml = {
             "dataset_name": dataset_name,
             "node_data": list_of_n_dict,
             "edge_data": list_of_e_dict,
         }
-        data = yaml.dump(python_data, f, sort_keys=False, default_flow_style=False)
+        data = yaml.dump(meta_yaml, f, sort_keys=False, default_flow_style=False)
 
-    with open(os.path.join(output_dataset_dir, "meta.yaml"), "r") as file:
-        meta_yaml = yaml.safe_load(file)
+    # avoid to write and read immediately (if the file system points to a distributed file system)
+    # with open(os.path.join(output_dataset_dir, "meta.yaml"), "r") as file:
+    #     meta_yaml = yaml.safe_load(file)
     print("\nmeta_yaml: \n", meta_yaml)
 
     # DGL node/edge csv headers
     # edge_header = ["src_id", "dst_id", "label", "train_mask", "val_mask","test_mask","feat"]
     # node_header = ["node_id", "label", "train_mask", "val_mask","test_mask","feat"]
 
+    if node_features is None:
+        node_features = {}
+
+    # write nodes_.csv files
+    node_type_columns = get_node_type_columns(node_columns)
+    for i, node_meta in enumerate(meta_yaml["node_data"]):
+        node_type = node_meta["ntype"]
+        file_name = node_meta["file_name"]
+        print("\nWriting {} node: {}".format(node_type, file_name))
+
+        col_map_of_node = col_map[node_type]
+        columns = node_type_columns[node_type]
+
+        # check the node features and its validation
+        num_features = _get_num_features_of_node(columns, node_features)
+        if num_features <= 0:
+            # if there is no future columns, simple path
+            mapped_columns = [col_map_of_node[column] for column in columns]
+            node_values = values_of_node(df, mapped_columns)
+            np.savetxt(
+                os.path.join(output_dataset_dir, file_name),
+                node_values.unique(),
+                delimiter=",",
+                header="node_id",
+                comments="",
+            )
+        else:
+            print("Features for node:", {column: node_features.get(column) for column in columns})
+
+            node_header = ["node_id", "feat"]  # minimum required
+            node_df_cols = ["node_id", "node_feat_as_str"]
+
+            # get list of tuples, each tuple is (mapped_column, feature_columns)
+            # all the columns of the same node type must have the same identical features if has
+            mapped_columns = [(col_map_of_node[column], node_features.get(column)) for column in columns]
+            # all the columns will renamed in the form of (node_id, feat_1, feat_2, ...) and concat vertically
+            df_node_columns = _concat_columns_of_node(df, mapped_columns)
+
+            # use the node_id to drop the duplicates
+            df_unique = df_node_columns.drop_duplicates(subset=['node_id'])
+
+            feat_keys = [f"feat_{i}" for i in range(num_features)]
+            # Note: feat_as_str needs to be a string of comma separated values
+            # enclosed in double quotes for dgl default parser to work
+            df_unique["node_feat_as_str"] = df_unique[feat_keys].astype(float).astype(str).apply(",".join, axis=1)
+            if len(feat_keys) == 1:
+                # append a "," if there is only one value for forcing to double quotes
+                df_unique["node_feat_as_str"] = df_unique["node_feat_as_str"] + ","
+
+            assert len(node_df_cols) == len(node_header)
+            df_unique[node_df_cols].to_csv(
+                os.path.join(output_dataset_dir, file_name),
+                index=False,
+                header=node_header,
+            )
+
     # write edges_.csv files
-    for i, edge_type in enumerate(meta_yaml["edge_data"]):
-        print("\nWriting: ", edge_type["file_name"])
+    for i, edge_meta in enumerate(meta_yaml["edge_data"]):
+        etype = edge_meta["etype"]
+        edge_type = etype[1]
+        file_name = edge_meta["file_name"]
+        print("\nWriting {} edge: {}".format(edge_type, file_name))
+
         edge_header = ["src_id", "dst_id"]  # minimum required
-        edge_df_cols = [edge_type["etype"][0], edge_type["etype"][2]]
+        src_id = get_mapped_column_of(edge_types[i][0], col_map, node_columns)
+        dst_id = get_mapped_column_of(edge_types[i][2], col_map, node_columns)
+        edge_df_cols = [src_id, dst_id]
         if config["edge_label"]:
             edge_header.append("label")
             edge_df_cols.append(config["edge_label"])
         if config["edge_split"]:
             # it is required that split has 3 values (0,1,2) for train test val respectively
             edge_header.extend(["train_mask", "val_mask", "test_mask"])
             edge_df_cols.extend(["masks_0", "masks_1", "masks_2"])
             # edge_header.extend(["train_mask"])
             # edge_df_cols.extend(["masks_0"])
-        if config["edge_features"]:
-            feat_keys = config["edge_features"]
-            print("features for CSVDataset edges: ", feat_keys)
+        if edge_features and edge_type in edge_features:
+            features = edge_features[edge_type]
+            print("Features for edge:", features)
+            data_columns = set(df.columns)
+            feat_keys = [feature for feature in features if feature in data_columns]
+            if len(feat_keys) != len(features):
+                print("Valid features for edge:", feat_keys)
             # Note: feat_as_str needs to be a string of comma separated values
             # enclosed in double quotes for dgl default parser to work
-            df["edge_feat_as_str"] = df[feat_keys].astype(str).apply(",".join, axis=1)
+            df["edge_feat_as_str"] = df[feat_keys].astype(float).astype(str).apply(",".join, axis=1)
+            if len(feat_keys) == 1:
+                # append a "," if there is only one value for forcing to double quotes
+                df_unique["node_feat_as_str"] = df_unique["node_feat_as_str"] + ","
             edge_header.append("feat")
             edge_df_cols.append("edge_feat_as_str")
         assert len(edge_df_cols) == len(edge_header)
         df[edge_df_cols].to_csv(
-            os.path.join(output_dataset_dir, edge_type["file_name"]),
+            os.path.join(output_dataset_dir, file_name),
             index=False,
             header=edge_header,
         )
-    # write nodes_.csv files
-    for i, node in enumerate(meta_yaml["node_data"]):
-        print("\nWriting: ", meta_yaml["node_data"][i]["file_name"])
-        print(df[meta_yaml["node_data"][i]["ntype"]].unique())
-        np.savetxt(
-            os.path.join(output_dataset_dir, meta_yaml["node_data"][i]["file_name"]),
-            df[meta_yaml["node_data"][i]["ntype"]].unique(),
-            delimiter=",",
-            header="node_id",
-            comments="",
-        )
 
     t_csv_dataset = time.time()
     print("Time to write CSVDatasets", t_csv_dataset - t_renum)
 
 
-def main(args):
-    build_graph(
-        input_file=args.input_file,
-        output_dir=args.output_dir,
-        dataset_name=args.dataset_name,
-        tabular2graph=args.tabular2graph
-    )
-
-
-if __name__ == "__main__":
-    parser = argparse.ArgumentParser(description="Build Graph Arguments")
-    parser.add_argument(
-        "--input_file",
-        type=existing_file,
-        default="",
-        help="Input file with path of the processed data in csv) ")
-    parser.add_argument(
-        "--dataset_name", default="tabformer_hetero", type=str, help="dataset name")
-    parser.add_argument(
-        "--output_dir", default="", help="The path to the output")
-    parser.add_argument(
-        "--tabular2graph", required=True, help="The path to the tabular2graph.yaml")
-
-    args = parser.parse_args()
-    print(args)
-
-    main(args)
+def _get_num_features_of_node(columns, node_features):
+    num_features = -1
+    for column in columns:
+        num = len(node_features.get(column, []))
+        if num_features >= 0:
+            if num != num_features:
+                raise ValueError("Columns for the node type have different number of features.")
+        else:
+            num_features = num
+    return num_features
+
+
+def _concat_columns_of_node(df, mapped_columns):
+    node_dfs = []
+    for mapped_column in mapped_columns:
+        id_column = mapped_column[0]
+        feature_columns = mapped_column[1]
+        # select the needed columns
+        sel_columns = [id_column]
+        sel_columns += feature_columns
+        node_df = df[sel_columns]
+        # rename the columns
+        rename_map = {id_column: "node_id"}
+        for i, feature_column in enumerate(feature_columns):
+            rename_map[feature_column] = f"feat_{i}"
+        node_df.rename(columns=rename_map, inplace=True)
+        node_dfs.append(node_df)
+    if len(node_dfs) == 1:
+        df_node_columns = node_dfs[0]
+    else:
+        df_node_columns = pd.concat(node_dfs, ignore_index=True)
+    return df_node_columns
```

## cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/launch.py

```diff
@@ -53,17 +53,47 @@
 
 def get_remote_command(local_command, host,
                        use_ssh=False, port=None, identity_file=None, timeout_s=None):
     return get_ssh_command(local_command, host, port, identity_file, timeout_s) if use_ssh \
         else get_cloudtik_ssh_command(local_command, host)
 
 
+def get_local_num_omp_threads(num_omp_threads, num_workers):
+    if num_omp_threads is None:
+        # TODO: cores information of container cases
+        # Here we assume all machines have the same number of CPU cores as the machine
+        # where the launch script runs.
+        num_omp_threads = max(
+            multiprocessing.cpu_count() - num_workers, 1
+        )
+        print(
+            "The number of OMP threads is set to",
+            num_omp_threads,
+        )
+    return num_omp_threads
+
+
+def get_worker_num_omp_threads(num_omp_threads, num_trainers):
+    if num_omp_threads is None:
+        # TODO: handle the case the working node is not the same as workers
+        # Here we assume all machines have the same number of CPU cores as the machine
+        # where the launch script runs.
+        num_omp_threads = max(
+            multiprocessing.cpu_count() // 2 // num_trainers, 1
+        )
+        print(
+            "The number of OMP threads per trainer is set to",
+            num_omp_threads,
+        )
+    return num_omp_threads
+
+
 def cleanup_proc(get_all_remote_pids, conn):
     """This process tries to clean up the remote training tasks."""
-    print("cleanupu process runs")
+    print("cleanup process runs")
     # This process should not handle SIGINT.
     signal.signal(signal.SIGINT, signal.SIG_IGN)
 
     data = conn.recv()
     # If the launch process exits normally, this process doesn't need to do anything.
     if data == "exit":
         sys.exit(0)
@@ -300,29 +330,38 @@
         "python3.7",
         "python3.8",
         "python3.9",
         "python3",
         # for backwards compatibility, accept python2 but technically DGL is a py3 library, so this is not recommended
         "python2.7",
         "python2",
+        "python",
     )
-    # If none of the candidate python bins match, then we go with the default `python`
-    python_bin = "python"
+
+    # find the python bin which appears first
+    python_bin_index = len(udf_command)
+    python_bin = None
     for candidate_python_bin in python_bin_allowlist:
-        if candidate_python_bin in udf_command:
+        python_bin_to_match = "{} ".format(candidate_python_bin)
+        index = udf_command.find(python_bin_to_match)
+        if 0 <= index < python_bin_index:
+            python_bin_index = index
             python_bin = candidate_python_bin
-            break
+
+    if not python_bin:
+        raise RuntimeError("Must have python bin in the command.")
 
     # transforms the udf_command from:
     #     python path/to/dist_trainer.py arg0 arg1
     # to:
     #     python -m torch.distributed.launch [DIST TORCH ARGS] path/to/dist_trainer.py arg0 arg1
     # Note: if there are multiple python commands in `udf_command`, this may do the Wrong Thing, eg launch each
     #       python command within the torch distributed launcher.
-    new_udf_command = udf_command.replace(python_bin, f"{python_bin} {torch_dist_cmd}")
+    insert_pos = index + len(python_bin)
+    new_udf_command = udf_command[:insert_pos] + " " + torch_dist_cmd + udf_command[insert_pos:]
 
     return new_udf_command
 
 
 def construct_dgl_server_env_vars(
     num_samplers: int,
     num_server_threads: int,
@@ -608,14 +647,16 @@
     if dry_run:
         print("Currently it's in dry run mode which means no jobs will be launched.")
     servers_cmd = []
     clients_cmd = []
     hosts = []
     thread_list = []
     server_count_per_machine = 0
+    num_omp_threads = get_worker_num_omp_threads(
+        num_omp_threads, num_trainers)
 
     # Get the IP addresses of the cluster.
     ip_config_file = os.path.join(workspace, ip_config)
     with open(ip_config_file) as f:
         for line in f:
             result = line.strip().split()
             if len(result) == 2:
@@ -656,15 +697,15 @@
         )
         for i in range(len(hosts) * server_count_per_machine):
             ip, _ = hosts[int(i / server_count_per_machine)]
             server_env_vars_cur = f"{server_env_vars} DGL_SERVER_ID={i}"
             cmd = wrap_cmd_with_local_envvars(udf_command, server_env_vars_cur)
             cmd = (
                 wrap_cmd_with_extra_envvars(cmd, extra_envs)
-                if len(extra_envs) > 0
+                if extra_envs
                 else cmd
             )
             cmd = "cd " + str(workspace) + "; " + cmd
             servers_cmd.append(cmd)
             if not dry_run:
                 thread_list.append(
                     execute_remote(
@@ -703,15 +744,15 @@
             node_rank=node_id,
             master_addr=master_addr,
             master_port=master_port,
         )
         cmd = wrap_cmd_with_local_envvars(torch_dist_udf_command, client_env_vars)
         cmd = (
             wrap_cmd_with_extra_envvars(cmd, extra_envs)
-            if len(extra_envs) > 0
+            if extra_envs
             else cmd
         )
         cmd = "cd " + str(workspace) + "; " + cmd
         clients_cmd.append(cmd)
         if not dry_run:
             thread_list.append(
                 execute_remote(
@@ -753,117 +794,118 @@
     if err != 0:
         print("Task failed")
         sys.exit(-1)
 
 
 def launch_local(
         udf_command, workspace,
+        num_workers,
         num_omp_threads,
         extra_envs=None):
+    num_omp_threads = get_local_num_omp_threads(
+        num_omp_threads, num_workers)
     env_vars_template = (
         "OMP_NUM_THREADS={OMP_NUM_THREADS} "
     )
     env_vars = env_vars_template.format(
         OMP_NUM_THREADS=os.environ.get("OMP_NUM_THREADS", str(num_omp_threads)),
     )
 
     cmd = wrap_cmd_with_local_envvars(udf_command, env_vars)
     cmd = (
         wrap_cmd_with_extra_envvars(cmd, extra_envs)
-        if len(extra_envs) > 0
+        if extra_envs
         else cmd
     )
     cmd = "cd " + str(workspace) + "; " + cmd
     try:
         subprocess.check_call(cmd, shell=True)
     except subprocess.CalledProcessError as err:
         print(f"Called process error {err}")
         raise err
 
 
 def main():
     parser = argparse.ArgumentParser(description="Launch a distributed job")
-    parser.add_argument("--ssh_port", type=int, default=22, help="SSH Port.")
     parser.add_argument(
-        "--ssh_username",
+        "--ssh-port", "--ssh_port",
+        type=int, default=22, help="SSH Port.")
+    parser.add_argument(
+        "--ssh-username", "--ssh_username",
         default="",
         help="Optional. When issuing commands (via ssh) to cluster, use the provided username in the ssh cmd. "
         "Example: If you provide --ssh_username=bob, then the ssh command will be like: 'ssh bob@1.2.3.4 CMD' "
         "instead of 'ssh 1.2.3.4 CMD'",
     )
     parser.add_argument(
         "--workspace",
         type=str,
         help="Path of user directory of distributed tasks. \
                         This is used to specify a destination location where \
                         the contents of current directory will be rsyncd",
     )
     parser.add_argument(
-        "--num_trainers",
+        "--num-trainers", "--num_trainers",
         type=int,
         help="The number of trainer processes per machine",
     )
     parser.add_argument(
-        "--num_omp_threads",
+        "--num-omp-threads", "--num_omp_threads",
         type=int,
         help="The number of OMP threads per trainer",
     )
     parser.add_argument(
-        "--num_samplers",
+        "--num-samplers", "--num_samplers",
         type=int,
         default=0,
         help="The number of sampler processes per trainer process",
     )
     parser.add_argument(
-        "--num_servers",
+        "--num-servers", "--num_servers",
         type=int,
         help="The number of server processes per machine",
     )
     parser.add_argument(
-        "--num_server_threads",
-        type=int,
-        default=1,
+        "--num-server-threads", "--num_server_threads",
+        type=int, default=1,
         help="The number of OMP threads in the server process. \
                             It should be small if server processes and trainer processes run on \
                             the same machine. By default, it is 1.",
     )
     parser.add_argument(
-        "--part_config",
+        "--part-config", "--part_config",
         type=str,
         help="The file (in workspace) of the partition config",
     )
     parser.add_argument(
-        "--ip_config",
+        "--ip-config", "--ip_config",
         type=str,
         help="The file (in workspace) of IP configuration for server processes",
     )
     parser.add_argument(
-        "--graph_format",
-        type=str,
-        default="csc",
+        "--graph-format", "--graph_format",
+        type=str, default="csc",
         help='The format of the graph structure of each partition. \
                         The allowed formats are csr, csc and coo. A user can specify multiple \
                         formats, separated by ",". For example, the graph format is "csr,csc".',
     )
     parser.add_argument(
-        "--extra_envs",
-        nargs="+",
-        type=str,
-        default=[],
+        "--extra-envs", "--extra_envs",
+        nargs="+", type=str, default=[],
         help="Extra environment parameters need to be set. For example, \
                         you can set the LD_LIBRARY_PATH and NCCL_DEBUG by adding: \
                         --extra_envs LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH NCCL_DEBUG=INFO ",
     )
     parser.add_argument(
-        "--keep_alive",
+        "--keep-alive", "--keep_alive",
         action="store_true",
         help="Servers keep alive when clients exit",
     )
     parser.add_argument(
-        "--server_name",
+        "--server-name", "--server_name",
         type=str,
         help="Used to check whether there exist alive servers",
     )
     args, udf_command = parser.parse_known_args()
     if args.keep_alive:
         assert (
             args.server_name is not None
@@ -887,24 +929,14 @@
     ), "A user has to specify a workspace with --workspace."
     assert (
         args.part_config is not None
     ), "A user has to specify a partition configuration file with --part_config."
     assert (
         args.ip_config is not None
     ), "A user has to specify an IP configuration file with --ip_config."
-    if args.num_omp_threads is None:
-        # Here we assume all machines have the same number of CPU cores as the machine
-        # where the launch script runs.
-        args.num_omp_threads = max(
-            multiprocessing.cpu_count() // 2 // args.num_trainers, 1
-        )
-        print(
-            "The number of OMP threads per trainer is set to",
-            args.num_omp_threads,
-        )
 
     udf_command = str(udf_command[0])
     if "python" not in udf_command:
         raise RuntimeError(
             "DGL launching script can only support Python executable file."
         )
     submit_jobs(args, udf_command)
```

## cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/partition_graph.py

```diff
@@ -1,44 +1,62 @@
-# Copyright (C) 2023 Intel Corporation
-# SPDX-License-Identifier: MIT
+"""
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+Author: Chen Haifeng
+"""
+
 
-import argparse
 import time
 import os
-import torch
 import dgl
 
+from cloudtik.runtime.ai.modeling.graph_modeling.graph_sage.modeling.model.utils import get_common_node_features, \
+    get_common_edge_features
+from cloudtik.runtime.ai.modeling.graph_modeling.graph_sage.modeling.utils import torch_save
+from cloudtik.runtime.ai.util.utils import clean_dir
+
 
 def partition_graph(
         dataset_dir, output_dir, graph_name,
-        num_parts, num_hops):
+        num_parts, num_hops,
+        heterogeneous=False):
     print("Random seed used in partitioning")
     dgl.random.seed(1)
 
     # create directories to save the partitions
-    os.makedirs(output_dir, exist_ok=True)
+    clean_dir(output_dir)
 
     # load and preprocess dataset
     print("Loading data")
     start = time.time()
     # set force_reload=False if no changes on input graph (much faster otherwise ingestion ~30min)
     dataset = dgl.data.CSVDataset(dataset_dir, force_reload=False)
-    print("time to load dataset from CSVs: ", time.time() - start)
+    print("time to load dataset from CSVs:", time.time() - start)
 
-    hg = dataset[0]  # only one graph
-    print(hg)
-    print("etype to read train/test/val from: ", hg.canonical_etypes[0][1])
-
-    E = hg.num_edges(hg.canonical_etypes[0][1])
-    reverse_eids = torch.cat([torch.arange(E, 2 * E), torch.arange(0, E)])
-    print("First reverse id is:  ", reverse_eids[0])
-
-    # convert graph to homogeneous
-    g = dgl.to_homogeneous(hg)
-    print(g)
+    graph = dataset[0]  # only one graph
+    print(graph)
+
+    # convert graph to homogeneous if needed
+    if not heterogeneous:
+        ndata = get_common_node_features(graph)
+        edata = get_common_edge_features(graph)
+        g = dgl.to_homogeneous(graph, ndata=ndata, edata=edata)
+        print(g)
+    else:
+        g = graph
 
     # part_method='random' works
     # part_method='metis' is giving a "free(): corrupted unsorted chunks error"
     # with multigraph (multiple links between same pair of nodes)
     # part_method='metis' works if you first do "dgl.to_simple(hg)" to keep single edge between pair of nodes
     # but that is not appropriate since we want to keep all multigraph edges
 
@@ -49,57 +67,22 @@
         num_hops=num_hops,
         part_method="random",
         out_path=output_dir,
         balance_edges=True,
         return_mapping=True,
     )
 
-    torch.save(nmap, os.path.join(output_dir, "nmap.pt"))
-    torch.save(emap, os.path.join(output_dir, "emap.pt"))
+    torch_save(nmap, os.path.join(output_dir, "nmap.pt"))
+    torch_save(emap, os.path.join(output_dir, "emap.pt"))
 
     # Load first partition to verify
     (
         g,
         node_feats,
         edge_feats,
         gpb,
         graph_name,
         ntypes_list,
         etypes_list,
     ) = dgl.distributed.load_partition(
         os.path.join(output_dir, graph_name + ".json"), 0
     )
-
-
-def main(args):
-    partition_graph(
-        dataset_dir=args.dataset_dir,
-        output_dir=args.output_dir,
-        graph_name=args.graph_name,
-        num_parts=args.num_parts,
-        num_hops=args.num_hops
-    )
-
-
-if __name__ == "__main__":
-    parser = argparse.ArgumentParser(description="Graph Partition")
-    parser.add_argument(
-        "--dataset_dir",
-        help="Path to CSVDataset graph folder ")
-    parser.add_argument(
-        "--output_dir",
-        help="Output folder to store partitions. It will create a sub folder for different partition sets")
-    parser.add_argument(
-        "--graph_name",
-        type=str,
-        default="tabformer_full_homo",
-        help="The graph name")
-    parser.add_argument(
-        "--num_parts", type=int, default=2, help="number of partitions")
-    parser.add_argument(
-        "--num_hops", type=int, default=1,
-        help="number of hops of nodes we include in a partition as HALO nodes")
-
-    args = parser.parse_args()
-    print(args)
-
-    main(args)
```

## cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/run.py

```diff
@@ -1,351 +1,688 @@
+"""
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+Author: Chen Haifeng
+"""
+
 import argparse
 import os
 import sys
+import tempfile
 
-from .utils import existing_file
-from .build_graph import build_graph
-from .partition_graph import partition_graph
-from .map_embeddings import map_embeddings
-from .map_embeddings_single import map_embeddings_single
-from .launch import launch_jobs, launch_local
+from cloudtik.runtime.ai.modeling.graph_modeling.graph_sage.modeling.data.process \
+    import process_data
+from cloudtik.runtime.ai.modeling.graph_modeling.graph_sage.modeling.model.\
+    homogeneous.predict import predict as predict_homogeneous
+from cloudtik.runtime.ai.modeling.graph_modeling.graph_sage.modeling.model.\
+    heterogeneous.predict import predict as predict_heterogeneous
+from cloudtik.runtime.ai.modeling.graph_modeling.graph_sage.modeling.utils import \
+    existing_path
+from cloudtik.runtime.ai.modeling.graph_modeling.graph_sage.modeling.build_graph import \
+    build_graph
+from cloudtik.runtime.ai.modeling.graph_modeling.graph_sage.modeling.partition_graph import \
+    partition_graph
+from cloudtik.runtime.ai.modeling.graph_modeling.graph_sage.modeling.embeddings import \
+    apply_embeddings, _map_node_embeddings, _map_model_embeddings
+from cloudtik.runtime.ai.modeling.graph_modeling.graph_sage.modeling.launch import \
+    launch_jobs, launch_local
+from cloudtik.runtime.ai.util.utils import load_config_from, clean_file
 
 GNN_HOME_PATH = os.path.abspath(os.path.dirname(__file__))
 
 
 def _get_dataset_output_dir(temp_dir):
     return os.path.join(temp_dir, "dataset")
 
 
 def _get_dataset_dir(temp_dir, dataset_name):
     dataset_output_dir = _get_dataset_output_dir(temp_dir)
     return os.path.join(dataset_output_dir, dataset_name)
 
 
-def _get_partition_dir(temp_dir):
-    return os.path.join(temp_dir, "partition")
+def _get_partition_dir(temp_dir, heterogeneous):
+    dir_name = "{type}_partition".format(
+        type=_get_graph_type(heterogeneous))
+    return os.path.join(temp_dir, dir_name)
 
 
-def _get_partition_config(temp_dir, graph_name):
-    partition_dir = _get_partition_dir(temp_dir)
+def _get_partition_config(temp_dir, graph_name, heterogeneous):
+    partition_dir = _get_partition_dir(temp_dir, heterogeneous)
     return os.path.join(partition_dir, graph_name + ".json")
 
 
 def _get_ip_config(temp_dir):
     return os.path.join(temp_dir,  "ip_config.txt")
 
 
-def _get_model_file(output_dir):
-    return os.path.join(output_dir, "model_graphsage_2L_64.pt")
+def _get_model_file(output_dir, args):
+    model_file_name = "graphsage_{type}_{num_layers}L_{num_hidden}.pt".format(
+        type=_get_model_type(args.inductive),
+        num_layers=args.num_layers,
+        num_hidden=args.num_hidden)
+    return os.path.join(output_dir, model_file_name)
+
 
+def _get_graph_type(heterogeneous):
+    return "heterogeneous" if heterogeneous else "homogeneous"
 
-def _get_node_embeddings_dir(output_dir):
-    return output_dir
 
+def _get_model_type(inductive):
+    return "inductive" if inductive else "transductive"
 
-def _get_node_embeddings_file(output_dir, node_embeddings_name):
-    node_embeddings_dir = _get_node_embeddings_dir(output_dir)
-    return os.path.join(
-        node_embeddings_dir, node_embeddings_name + ".pt")
 
+def _get_data_with_embeddings_file(args):
+    data_with_embeddings_name = args.data_with_embeddings_name
+    if not data_with_embeddings_name:
+        data_with_embeddings_name = "data_with_{}_embeddings.csv".format(
+            _get_model_type(args.inductive))
 
-def _get_mapped_output_file(output_dir):
-    return os.path.join(output_dir, "tabular_with_gnn_embeddings.csv")
+    return os.path.join(args.output_dir, data_with_embeddings_name)
 
 
 def _get_hosts(hosts):
     if not hosts:
         # TODO: use cloudtik API to retrieve the hosts to run
         raise RuntimeError("Distributed training must specify the hosts argument.")
 
     host_list = hosts.split(',')
     return host_list
 
 
 def _save_ip_config(ip_config_file, hosts):
     host_list = _get_hosts(hosts)
+    clean_file(ip_config_file)
     with open(ip_config_file, "w+") as f:
         for host in host_list:
             f.write("{}\n".format(host))
 
 
 def _get_num_parts(hosts):
     host_list = _get_hosts(hosts)
     return len(host_list)
 
 
-def _train_single(args):
+def _check_temp_dir(args):
+    if args.single_node:
+        if not args.temp_dir:
+            # for single node, get get a default temp dir from /tmp
+            args.temp_dir = tempfile.mkdtemp()
+            print("temp-dir is not specified. Default to: {}".format(
+                args.temp_dir))
+    else:
+        if not args.temp_dir:
+            raise ValueError(
+                "Must specify the temp-dir for storing the shared intermediate data")
+
+
+def _check_tabular2graph(args):
+    if not args.tabular2graph:
+        # default to the built-in tabular2graph.yaml if not specified
+        args.tabular2graph = os.path.join(
+            os.path.dirname(os.path.dirname(__file__)),
+            "config/tabular2graph.yaml")
+        print("tabular2graph is not specified. Default to: {}".format(
+            args.tabular2graph))
+
+
+def _check_model_file(args):
+    if not args.model_file:
+        args.model_file = _get_model_file(args.output_dir, args)
+        print("model-file is not specified. Default to: {}".format(
+            args.model_file))
+
+
+def _check_train_output(args):
+    if not args.train_output:
+        graph_type = _get_graph_type(args.heterogeneous)
+        args.train_output = os.path.join(
+            args.output_dir, "train_{}_node_embeddings.pt".format(graph_type))
+        print("train-output is not specified. Default to: {}".format(
+            args.train_output))
+
+
+def _check_predict_output(args):
+    if not args.predict_output:
+        graph_type = _get_graph_type(args.heterogeneous)
+        args.predict_output = os.path.join(
+            args.output_dir, "predict_{}_node_embeddings.pt".format(graph_type))
+        print("predict-output is not specified. Default to: {}".format(
+            args.predict_output))
+
+
+def get_data_with_embeddings_path(args):
+    return _get_data_with_embeddings_file(args)
+
+
+def _process_data(args):
+    if not args.raw_data_path:
+        raise ValueError(
+            "Must specify the raw data file which contains raw data to be processed.")
+    if not args.processed_data_path:
+        raise RuntimeError("Please specify the processed-data-path for storing of processed data.")
+    process_data(
+        raw_data_path=args.raw_data_path,
+        output_file=args.processed_data_path,
+    )
+
+
+def _build_graph(args):
+    if not args.processed_data_path:
+        raise ValueError(
+            "Must specify the input file which contains the processed data.")
+    _check_temp_dir(args)
+    _check_tabular2graph(args)
+
+    dataset_output_dir = _get_dataset_output_dir(args.temp_dir)
+    build_graph(
+        input_file=args.processed_data_path,
+        output_dir=dataset_output_dir,
+        dataset_name=args.dataset_name,
+        tabular2graph=args.tabular2graph
+    )
+
+
+def _partition_graph(args):
+    if not args.temp_dir:
+        raise ValueError(
+            "Must specify the temp dir which stored the intermediate data.")
+    partition_dir = _get_partition_dir(args.temp_dir, args.heterogeneous)
+    dataset_dir = _get_dataset_dir(
+        args.temp_dir, args.dataset_name)
+    if not args.num_parts:
+        args.num_parts = _get_num_parts(args.hosts)
+    partition_graph(
+        dataset_dir=dataset_dir,
+        output_dir=partition_dir,
+        graph_name=args.graph_name,
+        num_parts=args.num_parts,
+        num_hops=args.num_hops,
+        heterogeneous=args.heterogeneous
+    )
+
+
+def _get_optional_train_args(args):
+    optional_args = ""
+    if args.heterogeneous:
+        if args.relations:
+            optional_args += ' --relations "{relations}"'.format(
+                relations=args.relations)
+
+    # passing in the reverse-edges if exists in config
+    tabular2graph = load_config_from(args.tabular2graph)
+    reverse_edges = tabular2graph.get("reverse_edges")
+    if reverse_edges:
+        reverse_edges_str = ",".join(
+            ["{}:{}".format(k, v) for k, v in reverse_edges.items()])
+        optional_args += ' --reverse_edges "{reverse_edges}"'.format(
+            reverse_edges=reverse_edges_str)
+
+        if args.exclude_reverse_edges:
+            optional_args += ' --exclude_reverse_edges'
+
+    if args.inductive:
+        optional_args += " --inductive"
+        if args.node_feature:
+            optional_args += ' --node_feature "{node_feature}"'.format(
+                node_feature=args.node_feature)
+    return optional_args
+
+
+def _train_local(args):
+    if not args.temp_dir:
+        raise ValueError(
+            "Must specify the temp dir which stored the intermediate data.")
+    # make sure the output dir exists
+    if not args.output_dir:
+        raise ValueError(
+            "Must specify the output dir for storing results.")
+    os.makedirs(args.output_dir, exist_ok=True)
+
+    _check_model_file(args)
+    _check_train_output(args)
+
     # Call launch which run a single local training processes
-    model_file = _get_model_file(args.output_dir)
-    node_embeddings_file = _get_node_embeddings_file(
-        args.output_dir, args.node_embeddings_name)
     dataset_dir = _get_dataset_dir(
         args.temp_dir, args.dataset_name)
 
-    exec_script = os.path.join(GNN_HOME_PATH, "train_graph_sage_single.py")
+    workspace = GNN_HOME_PATH
+    graph_type = _get_graph_type(args.heterogeneous)
+    exec_script = os.path.join(
+        GNN_HOME_PATH, "model", graph_type, "train.py")
     job_command = (
         'numactl -N 0 {python_exe} -u '
         '{exec_script} '
         '--model_file {model_file} '
         '--node_embeddings_file {node_embeddings_file} '
         '--dataset_dir {dataset_dir} '
-        '--num_epoch {num_epoch} '
+        '--num_epoch {num_epochs} '
         '--num_hidden {num_hidden} '
         '--num_layers {num_layers} '
         '--lr {lr} '
         '--fan_out {fan_out} '
         '--batch_size {batch_size} '
         '--batch_size_eval {batch_size_eval} '
         '--eval_every {eval_every} '
         '--num_dl_workers {num_dl_workers} '
         .format(
             python_exe=sys.executable,
             exec_script=exec_script,
-            model_file=model_file,
-            node_embeddings_file=node_embeddings_file,
+            model_file=args.model_file,
+            node_embeddings_file=args.train_output,
             dataset_dir=dataset_dir,
-            num_epoch=args.num_epoch,
+            num_epochs=args.num_epochs,
             num_hidden=args.num_hidden,
             num_layers=args.num_layers,
             lr=args.lr,
             fan_out=args.fan_out,
             batch_size=args.batch_size,
             batch_size_eval=args.batch_size_eval,
             eval_every=args.eval_every,
             num_dl_workers=args.num_dl_workers,
         )
     )
+
+    optional_args = _get_optional_train_args(args)
+    if optional_args:
+        job_command += optional_args
+
     launch_local(
-        job_command,
+        job_command, workspace,
+        num_workers=args.num_dl_workers,
         num_omp_threads=args.num_omp_threads
     )
 
 
 def _train_distributed(args):
+    # For distributed training, the temp_dir and the output_dir must be shared
+    # directory being able to accessed by the nodes.
+    if not args.temp_dir:
+        raise ValueError(
+            "Must specify the temp dir which stored the intermediate data.")
+    # make sure the output dir exists
+    if not args.output_dir:
+        raise ValueError(
+            "Must specify the output dir for storing results.")
+    os.makedirs(args.output_dir, exist_ok=True)
+
+    _check_model_file(args)
+    _check_train_output(args)
+
     # Call launch which run the distributed training processes
-    model_file = _get_model_file(args.output_dir)
-    node_embeddings_file = _get_node_embeddings_file(
-        args.output_dir, args.node_embeddings_name)
     dataset_dir = _get_dataset_dir(
         args.temp_dir, args.dataset_name)
-    part_config = _get_partition_config(args.temp_dir, args.graph_name)
+    part_config = _get_partition_config(
+        args.temp_dir, args.graph_name, args.heterogeneous)
     ip_config = _get_ip_config(args.temp_dir)
 
     # Save IP config to shared ip config file
     _save_ip_config(ip_config, args.hosts)
 
     workspace = GNN_HOME_PATH
-    exec_script = os.path.join(GNN_HOME_PATH, "train_graph_sage.py")
+    graph_type = _get_graph_type(args.heterogeneous)
+    node_embeddings_file = os.path.join(
+        args.output_dir, "{}_node_embeddings.pt".format(graph_type))
+    # save to a dist prefixed model file
+    dist_model_file_name = "dist_{}".format(
+        os.path.basename(args.model_file))
+    dist_model_file = os.path.join(
+        os.path.dirname(args.model_file), dist_model_file_name)
+
+    exec_script = os.path.join(GNN_HOME_PATH, "model",
+                               graph_type, "distributed", "train.py")
+
     job_command = (
         'numactl -N 0 {python_exe} '
         '{exec_script} '
         '--model_file {model_file} '
         '--node_embeddings_file {node_embeddings_file} '
         '--dataset_dir {dataset_dir} '
         '--graph_name {graph_name} '
         '--ip_config {ip_config} '
         '--part_config {part_config} '
-        '--num_epoch {num_epoch} '
+        '--num_epochs {num_epochs} '
         '--num_hidden {num_hidden} '
         '--num_layers {num_layers} '
         '--lr {lr} '
         '--fan_out {fan_out} '
         '--batch_size {batch_size} '
         '--batch_size_eval {batch_size_eval} '
         '--eval_every {eval_every} '
         '--log_every {log_every} '
-        '--remove_edge '
         .format(
             python_exe=sys.executable,
             exec_script=exec_script,
-            model_file=model_file,
+            model_file=dist_model_file,
             node_embeddings_file=node_embeddings_file,
             dataset_dir=dataset_dir,
             graph_name=args.graph_name,
             ip_config=ip_config,
             part_config=part_config,
-            num_epoch=args.num_epoch,
+            num_epochs=args.num_epochs,
             num_hidden=args.num_hidden,
             num_layers=args.num_layers,
             lr=args.lr,
             fan_out=args.fan_out,
             batch_size=args.batch_size,
             batch_size_eval=args.batch_size_eval,
             eval_every=args.eval_every,
             log_every=args.log_every,
         )
     )
+
+    optional_args = _get_optional_train_args(args)
+    if optional_args:
+        job_command += optional_args
+
     launch_jobs(
         job_command, workspace,
         ip_config=ip_config,
         part_config=part_config,
         num_servers=args.num_servers,
         num_trainers=args.num_trainers,
         num_samplers=args.num_samplers,
         num_server_threads=args.num_server_threads,
         num_omp_threads=args.num_omp_threads
     )
 
+    # the train process finished, map the partitioned node embeddings to global
+    partition_dir = _get_partition_dir(args.temp_dir, args.heterogeneous)
+    _map_node_embeddings(
+        node_embeddings_file,
+        partition_dir=partition_dir,
+        output_file=args.train_output)
+    _map_model_embeddings(
+        args.inductive,
+        dist_model_file,
+        partition_dir=partition_dir,
+        model_file=args.model_file)
+
+
+def _train(args):
+    _check_tabular2graph(args)
 
-def _map_and_save_embeddings(args):
-    mapped_output_file = _get_mapped_output_file(args.output_dir)
-    node_embeddings_dir = _get_node_embeddings_dir(args.output_dir)
     if args.single_node:
-        map_embeddings_single(
-            processed_data_file=args.input_file,
-            node_embeddings_dir=node_embeddings_dir,
-            node_embeddings_name=args.node_embeddings_name,
-            output_file=mapped_output_file,
-            tabular2graph=args.tabular2graph
-        )
+        _train_local(args)
     else:
-        partition_dir = _get_partition_dir(args.temp_dir)
-        map_embeddings(
-            processed_data_file=args.input_file,
-            partition_dir=partition_dir,
-            node_embeddings_dir=node_embeddings_dir,
-            node_embeddings_name=args.node_embeddings_name,
-            output_file=mapped_output_file,
-            tabular2graph=args.tabular2graph
-        )
+        _train_distributed(args)
+
+
+def _predict_node_embeddings(args):
+    # make sure the output dir exists
+    if not args.output_dir:
+        raise ValueError(
+            "Must specify the output dir for storing results.")
+    os.makedirs(args.output_dir, exist_ok=True)
+
+    _check_model_file(args)
+    if not os.path.exists(args.model_file):
+        raise ValueError(
+            "The model file doesn't exist: {}.".format(args.model_file))
+    _check_predict_output(args)
+
+    dataset_dir = _get_dataset_dir(
+        args.temp_dir, args.dataset_name)
+
+    if args.heterogeneous:
+        predict_heterogeneous(
+            dataset_dir,
+            model_file=args.model_file,
+            num_hidden=args.num_hidden,
+            num_layers=args.num_layers,
+            relations=args.relations,
+            inductive=args.inductive,
+            node_feature=args.node_feature,
+            predict_output=args.predict_output,
+            batch_size=args.batch_size_eval)
+    else:
+        predict_homogeneous(
+            dataset_dir,
+            model_file=args.model_file,
+            num_hidden=args.num_hidden,
+            num_layers=args.num_layers,
+            inductive=args.inductive,
+            node_feature=args.node_feature,
+            predict_output=args.predict_output,
+            batch_size=args.batch_size_eval)
+
+
+def _predict(args):
+    # The training node embeddings are not enough
+    # predict to get the node embeddings (cases such as new node added)
+    # TODO: optimize if there is no new node, we can use the trained node embeddings
+    _predict_node_embeddings(args)
+    _apply_embeddings_to_data(args)
+
+
+def _apply_embeddings_to_data(args):
+    if not args.processed_data_path:
+        raise ValueError(
+            "Must specify the processed data file which contains the processed data.")
+    if not args.output_dir:
+        raise ValueError(
+            "Must specify the output dir to store data with node embeddings.")
+    _check_tabular2graph(args)
+    _check_predict_output(args)
+
+    node_embeddings_file = args.predict_output
+    if not os.path.exists(node_embeddings_file):
+        raise ValueError(
+            "The node embeddings file doesn't exist: {}."
+            "This file is generated by predicting on a graph.".format(node_embeddings_file))
+
+    output_file = _get_data_with_embeddings_file(args)
+    apply_embeddings(
+        processed_data_path=args.processed_data_path,
+        node_embeddings_file=node_embeddings_file,
+        output_file=output_file,
+        tabular2graph=args.tabular2graph,
+        heterogeneous=args.heterogeneous
+    )
 
 
 def run(args):
+    if not args.no_process_data:
+        _process_data(args)
+
     # run build the graph
     if not args.no_build_graph:
-        dataset_output_dir = _get_dataset_output_dir(args.temp_dir)
-        build_graph(
-            input_file=args.input_file,
-            output_dir=dataset_output_dir,
-            dataset_name=args.dataset_name,
-            tabular2graph=args.tabular2graph
-        )
+        _build_graph(args)
 
     if not args.single_node and not args.no_partition_graph:
-        partition_dir = _get_partition_dir(args.temp_dir)
-        dataset_dir = _get_dataset_dir(
-            args.temp_dir, args.dataset_name)
-        if not args.num_parts:
-            args.num_parts = _get_num_parts(args.hosts)
-        partition_graph(
-            dataset_dir=dataset_dir,
-            output_dir=partition_dir,
-            graph_name=args.graph_name,
-            num_parts=args.num_parts,
-            num_hops=args.num_hops
-        )
+        _partition_graph(args)
 
-    if not args.no_train_graph:
-        if args.single_node:
-            _train_single(args)
-        else:
-            _train_distributed(args)
+    if not args.no_train:
+        _train(args)
 
-    if not args.no_map_embeddings:
-        _map_and_save_embeddings(args)
+    if not args.no_predict:
+        _predict(args)
 
 
 if __name__ == "__main__":
     parser = argparse.ArgumentParser(description="Run GNN Training")
     parser.add_argument(
-        "--single_node", default=False, action="store_true",
+        "--single-node", "--single_node",
+        default=False, action="store_true",
         help="To do single node training")
     parser.add_argument(
-        "--no_build_graph", default=False, action="store_true",
+        "--no-process-data", "--no_process_data",
+        default=False, action="store_true",
+        help="whether to do data process")
+    parser.add_argument(
+        "--no-build-graph", "--no_build_graph",
+        default=False, action="store_true",
         help="whether to build graph")
     parser.add_argument(
-        "--no_partition_graph", default=False, action="store_true",
+        "--no-partition-graph", "--no_partition_graph",
+        default=False, action="store_true",
         help="whether to partition graph")
     parser.add_argument(
-        "--no_train_graph", default=False, action="store_true",
-        help="whether to train graph")
+        "--no-train", "--no_train",
+        default=False, action="store_true",
+        help="whether to do training")
     parser.add_argument(
-        "--no_map_embeddings", default=False, action="store_true",
-        help="whether to map embeddings")
+        "--no-predict", "--no_predict",
+        default=False, action="store_true",
+        help="whether to do predict")
 
     parser.add_argument(
-        "--input_file",
-        type=existing_file,
-        default="",
-        help="Input file with path of the processed data in csv) ")
+        "--raw-data-path", "--raw_data_path",
+        type=existing_path,
+        help="The path to the raw transaction data")
     parser.add_argument(
-        "--temp_dir", default="", help="The path to the intermediate data")
+        "--processed-data-path", "--processed_data_path",
+        type=str,
+        help="The path to the output processed data")
     parser.add_argument(
-        "--output_dir", default="", help="The path to the output")
+        "--model-file", "--model_file",
+        type=str,
+        help="The path to the output model file")
     parser.add_argument(
-        "--dataset_name", default="tabformer_hetero", type=str, help="The dataset name")
+        "--temp-dir", "--temp_dir",
+        type=str,
+        help="The path to the intermediate data")
+    parser.add_argument(
+        "--output-dir", "--output_dir",
+        type=str,
+        help="The path to the output")
     parser.add_argument(
-        "--tabular2graph", required=True, help="The path to the tabular2graph.yaml")
+        "--dataset-name", "--dataset_name",
+        type=str, default="tabformer",
+        help="The dataset name")
+    parser.add_argument(
+        "--tabular2graph",
+        type=str,
+        help="The path to the tabular2graph.yaml")
 
     # Train
     parser.add_argument(
-        "--node_embeddings_name",
+        "--train-output", "--train_output",
+        type=str,
+        help="The path to the train output node embeddings file")
+
+    # Predict
+    parser.add_argument(
+        "--predict-output", "--predict_output",
+        type=str,
+        help="The path to the predict output node embeddings file")
+    parser.add_argument(
+        "--data-with-embeddings-name", "--data_with_embeddings_name",
         type=str,
-        default="node_emb",
-        help="The path to the node embedding file")
+        help="The path to save the data with embeddings file")
 
     # Distributed training
-    parser.add_argument("--hosts", default="", type=str,
-                        help="List of hosts separated with comma for launching tasks. ")
-    # Partition graph parameters
     parser.add_argument(
-        "--graph_name",
+        "--hosts",
         type=str,
-        default="tabformer_full_homo",
+        help="List of hosts separated with comma for launching tasks. ")
+    # Partition graph parameters
+    parser.add_argument(
+        "--graph-name", "--graph_name",
+        type=str, default="tabformer_graph",
         help="The graph name")
     parser.add_argument(
-        "--num_parts", type=int, default=0, help="number of partitions")
+        "--num-parts", "--num_parts",
+        type=int, default=0,
+        help="number of partitions")
     parser.add_argument(
-        "--num_hops", type=int, default=1,
+        "--num-hops", "--num_hops",
+        type=int, default=1,
         help="number of hops of nodes we include in a partition as HALO nodes")
 
     parser.add_argument(
-        "--num_trainers",
-        type=int,
-        default=1,
+        "--num-trainers", "--num_trainers",
+        type=int, default=1,
         help="The number of trainer processes per machine",
     )
     parser.add_argument(
-        "--num_samplers",
-        type=int,
-        default=2,
+        "--num-samplers", "--num_samplers",
+        type=int, default=0,
         help="The number of sampler processes per trainer process",
     )
     parser.add_argument(
-        "--num_servers",
-        type=int,
-        default=1,
+        "--num-servers", "--num_servers",
+        type=int, default=1,
         help="The number of server processes per machine",
     )
     parser.add_argument(
-        "--num_server_threads",
-        type=int,
-        default=1,
-        help="The number of OMP threads in the server process. \
-                                It should be small if server processes and trainer processes run on \
-                                the same machine. By default, it is 1.",
+        "--num-server-threads", "--num_server_threads",
+        type=int, default=1,
+        help="The number of OMP threads in the server process. "
+             "It should be small if server processes and trainer processes run "
+             "on the same machine. By default, it is 1.",
     )
     parser.add_argument(
-        "--num_omp_threads",
+        "--num-omp-threads", "--num_omp_threads",
         type=int,
         help="The number of OMP threads per trainer",
     )
 
-    parser.add_argument("--num_epochs", type=int, default=10)
-    parser.add_argument("--num_hidden", type=int, default=64)
-    parser.add_argument("--num_layers", type=int, default=2)
-    parser.add_argument("--fan_out", type=str, default="55,65")
-    parser.add_argument("--batch_size", type=int, default=2048)
-    parser.add_argument("--batch_size_eval", type=int, default=1000000)
-    parser.add_argument("--eval_every", type=int, default=1)
-    parser.add_argument("--lr", type=float, default=0.0005)
+    # These defaults are set for distributed training
+    parser.add_argument("--num-epochs", "--num_epochs",
+                        type=int, default=10)
+    parser.add_argument("--num-hidden", "--num_hidden",
+                        type=int, default=64)
+    parser.add_argument("--num-layers", "--num_layers",
+                        type=int, default=2)
+    parser.add_argument("--fan-out", "--fan_out",
+                        type=str, default="55,65")
+    parser.add_argument("--batch-size", "--batch_size",
+                        type=int, default=2048)
+    parser.add_argument("--batch-size-eval", "--batch_size_eval",
+                        type=int, default=1000000)
+    parser.add_argument("--eval-every", "--eval_every",
+                        type=int, default=1)
+    parser.add_argument("--lr",
+                        type=float, default=0.0005)
 
     # distributed only
-    parser.add_argument("--log_every", type=int, default=20)
+    parser.add_argument("--log-every", "--log_every",
+                        type=int, default=20)
 
     # single only
-    parser.add_argument("--num_dl_workers", type=int, default=4)
+    parser.add_argument("--num-dl-workers", "--num_dl_workers",
+                        type=int, default=4)
+
+    # Heterogeneous
+    parser.add_argument(
+        "--heterogeneous",
+        action="store_true", default=False,
+        help="Train a model with heterogeneous graph"
+    )
+    parser.add_argument(
+        "--relations",
+        type=str,
+        help="The comma separated list of edge relations for the heterogeneous model.")
+
+    # inductive or transductive training
+    parser.add_argument(
+        "--inductive",
+        action="store_true", default=False,
+        help="Train an inductive model"
+    )
+
+    # Inductive
+    parser.add_argument(
+        "--node-feature", "--node_feature",
+        type=str,
+        help="The feature name to use for node. If not set, will use node id.")
+
+    parser.add_argument(
+        "--exclude-reverse-edges", "--exclude_reverse_edges",
+        default=False, action="store_true",
+        help="whether to exclude reverse edges during sampling",
+    )
 
     args = parser.parse_args()
     print(args)
 
     run(args)
```

## cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/utils.py

```diff
@@ -1,10 +1,30 @@
+"""
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+Author: Chen Haifeng
+"""
+
 import argparse
 import os
 
+import torch
+
+from cloudtik.runtime.ai.util.utils import clean_file
+
 
 def existing_directory(raw_path):
     if not os.path.isdir(raw_path):
         raise argparse.ArgumentTypeError(
             '"{}" is not an existing directory'.format(raw_path)
         )
     return os.path.abspath(raw_path)
@@ -12,7 +32,27 @@
 
 def existing_file(raw_path):
     if not os.path.isfile(raw_path):
         raise argparse.ArgumentTypeError(
             '"{}" is not an existing file'.format(raw_path)
         )
     return os.path.abspath(raw_path)
+
+
+def existing_path(raw_path):
+    if not os.path.exists(raw_path):
+        raise argparse.ArgumentTypeError(
+            '"{}" is not an existing directory or file'.format(raw_path)
+        )
+    return os.path.abspath(raw_path)
+
+
+def torch_save(
+        obj: object, target_file):
+    clean_file(target_file)
+    torch.save(obj, target_file)
+
+
+def df_to_csv(
+        df, target_file, index=True):
+    clean_file(target_file)
+    df.to_csv(target_file, index=index)
```

## cloudtik/runtime/ai/modeling/transfer_learning/model.py

```diff
@@ -65,15 +65,18 @@
     def load_from_directory(self, model_dir: str):
         """
         Load a model from a directory
         """
         pass
 
     @abc.abstractmethod
-    def train(self, dataset: Dataset, output_dir, epochs=1, initial_checkpoints=None, do_eval=True):
+    def train(
+            self, dataset: Dataset, output_dir, *,
+            epochs=1, initial_checkpoints=None, do_eval=True,
+            **kwargs):
         """
         Train the model using the specified dataset
         """
         pass
 
     @abc.abstractmethod
     def evaluate(self, dataset: Dataset):
@@ -97,18 +100,18 @@
     @abc.abstractmethod
     def export(self, output_dir: str):
         """
         Export the serialized model to an output directory
         """
         pass
 
-    @abc.abstractmethod
-    def write_inc_config_file(self, config_file_path, dataset, batch_size, overwrite=False, **kwargs):
+    def export_neural_compressor_config(
+            self, config_file_path, dataset, batch_size, overwrite=False, **kwargs):
         """
-        Writes an Intel Neural Compressor compatible config file to the specified path usings args from the
+        Writes a Neural Compressor compatible config file to the specified path usings args from the
         specified dataset and parameters. This is currently only supported for TF custom image classification
         datasets.
 
         Args:
             config_file_path (str): Destination path on where to write the .yaml config file.
             dataset (Dataset): A dataset object
             batch_size (int): Batch size to use for quantization and evaluation
@@ -117,80 +120,79 @@
 
         Returns:
             None
 
         Raises:
             FileExistsError if the config file already exists and overwrite is set to False
             ValueError if the parameters are not within the expected values
-            NotImplementedError if the model or dataset does not support INC yet
+            NotImplementedError if the model or dataset does not support Neural Compressor yet
         """
-        pass
+        raise NotImplementedError("Writing Neural Compressor config files has not be implemented for this model.")
 
-    @abc.abstractmethod
     def quantize(self, saved_model_dir, output_dir, inc_config_path):
         """
-        Performs post training quantization using the Intel Neural Compressor on the model from the saved_model_dir
+        Performs post training quantization using the Neural Compressor on the model from the saved_model_dir
         using the specified config file. The quantized model is written to the output directory.
 
         Args:
             saved_model_dir (str): Source directory for the model to quantize.
             output_dir (str): Writable output directory to save the quantized model
-            inc_config_path (str): Path to an INC config file (.yaml)
+            inc_config_path (str): Path to a Neural Compressor config file (.yaml)
 
         Returns:
             None
 
         Raises:
-            NotImplementedError if the model does not support INC yet
+            NotImplementedError if the model does not support Neural Compressor yet
             NotADirectoryError if the saved_model_dir is not a directory
             FileNotFoundError if a saved_model.pb is not found in the saved_model_dir or if the inc_config_path file
             is not found.
             FileExistsError if the output_dir already has a saved_model.pb file
         """
-        pass
+        raise NotImplementedError("Post training quantization has not been implemented for this model.")
 
-    @abc.abstractmethod
-    def optimize_graph(self, saved_model_dir, output_dir):
+    def optimize_network(
+            self, saved_model_dir, output_dir):
         """
-        Performs FP32 graph optimization using the Intel Neural Compressor on the model in the saved_model_dir
+        Performs FP32 network optimization on the model in the saved_model_dir
         and writes the inference-optimized model to the output_dir. Graph optimization includes converting
         variables to constants, removing training-only operations like checkpoint saving, stripping out parts
         of the graph that are never reached, removing debug operations like CheckNumerics, folding batch
         normalization ops into the pre-calculated weights, and fusing common operations into unified versions.
 
         Args:
             saved_model_dir (str): Source directory for the model to optimize
             output_dir (str): Writable output directory to save the optimized model
 
         Returns:
             None
 
         Raises:
-            NotImplementedError if the model does not support INC yet
+            NotImplementedError if the model does not support network optimization yet
             NotADirectoryError if the saved_model_dir is not a directory
             FileNotFoundError if a saved_model.pb is not found in the saved_model_dir
             FileExistsError if the output_dir already has a saved_model.pb file
         """
-        pass
+        raise NotImplementedError("Network optimization has not been implemented for this model.")
 
-    @abc.abstractmethod
-    def benchmark(self, saved_model_dir, inc_config_path, mode='performance'):
+    def benchmark_neural_compressor(
+            self, saved_model_dir, inc_config_path, mode='performance'):
         """
-        Use INC to benchmark the specified model for performance or accuracy.
+        Use neural compressor to benchmark the specified model for performance or accuracy.
 
         Args:
             saved_model_dir (str): Path to the directory where the saved model is located
-            inc_config_path (str): Path to an INC config file (.yaml)
+            inc_config_path (str): Path to neural compressor config file (.yaml)
             mode (str): performance or accuracy (defaults to performance)
 
         Returns:
             None
 
         Raises:
-            NotImplementedError if the model does not support INC yet
+            NotImplementedError if the model does not support neural compressor yet
             NotADirectoryError if the saved_model_dir is not a directory
             FileNotFoundError if a saved_model.pb is not found in the saved_model_dir or if the inc_config_path file
             is not found.
             ValueError if an unexpected mode is provided
         """
-        raise NotImplementedError("INC benchmarking is not supported for this model")
+        raise NotImplementedError("Neural compressor benchmarking is not supported for this model.")
```

## cloudtik/runtime/ai/modeling/transfer_learning/model_factory.py

```diff
@@ -130,15 +130,15 @@
 
     """
     model_module_class = _get_model_module_class(category, framework)
     model_class = locate(model_module_class)
     return model_class(model_name, model, **kwargs)
 
 
-def get_model(model_name: str, category: str, framework: str, source: str = None, **kwargs):
+def get_model(model_name: str, category: str = None, framework: str = None, source: str = None, **kwargs):
     """A factory method for creating models.
 
         Args:
             model_name (str): name of model
             category (str): the category of the model
             framework (str): framework: pytorch or tensorflow
             source (str): source of the model
@@ -203,15 +203,15 @@
 def search_model(model_name, category: str = None, framework: str = None, source: str = None):
     models = search_models(category, framework, source)
     candidate_models = {}
 
     for model_category in models.keys():
         if model_name in models[model_category]:
             # Found a matching model
-            candidate_models[category] = models[model_category][model_name]
+            candidate_models[model_category] = models[model_category][model_name]
 
     return candidate_models
 
 
 def search_models(category: str = None, framework: str = None, source: str = None):
     """
     Search 3 levels of folder: category, framework and source for models json file.
```

## cloudtik/runtime/ai/modeling/transfer_learning/common/utils.py

```diff
@@ -20,14 +20,35 @@
 import os
 import urllib.request
 import re
 import shutil
 import tarfile
 from zipfile import ZipFile
 
+from enum import Enum, auto
+
+
+class FrameworkType(Enum):
+    TENSORFLOW = auto()
+    PYTORCH = auto()
+
+    def __str__(self):
+        return self.name.lower()
+
+    @staticmethod
+    def from_str(framework_str):
+        if framework_str.lower() == "tensorflow":
+            return FrameworkType.TENSORFLOW
+        elif framework_str.lower() == "pytorch":
+            return FrameworkType.PYTORCH
+        else:
+            options = [e.name for e in FrameworkType]
+            raise ValueError("Unsupported framework: {} (Select from: {})".format(
+                framework_str, options))
+
 
 def read_json_file(json_file_path):
     """
     Reads a json file an returns a dictionary representing the file contents
 
     :param json_file_path: Path to the json file
     :return: Dictionary
```

## cloudtik/runtime/ai/modeling/transfer_learning/common/downloader/datasets.py

```diff
@@ -94,18 +94,24 @@
             try:
                 return dataset_class(self._dataset_dir, download=True, split=split)
             except TypeError:
                 return dataset_class(self._dataset_dir, download=True, train=split == 'train')
 
         elif self._source == DatasetSource.HUGGING_FACE:
             from datasets import load_dataset
+            in_memory = self._args['in_memory'] if "in_memory" in self._args else None
             if 'subset' in self._args:
-                return load_dataset(self._dataset_name, self._args['subset'], split=split, cache_dir=self._dataset_dir)
+                return load_dataset(
+                    self._dataset_name, self._args['subset'],
+                    split=split, cache_dir=self._dataset_dir,
+                    keep_in_memory=in_memory)
             else:
-                return load_dataset(self._dataset_name, split=split, cache_dir=self._dataset_dir)
+                return load_dataset(
+                    self._dataset_name, split=split, cache_dir=self._dataset_dir,
+                    keep_in_memory=in_memory)
 
         elif self._source == DatasetSource.GENERIC:
             file_path = utils.download_file(self._url, self._dataset_dir)
             if os.path.isfile(file_path):
                 if tarfile.is_tarfile(file_path):
                     contents = utils.extract_tar_file(file_path, self._dataset_dir)
                 elif zipfile.is_zipfile(file_path):
```

## cloudtik/runtime/ai/modeling/transfer_learning/common/pytorch/__init__.py

```diff
@@ -0,0 +1,10 @@
+00000000: 696d 706f 7274 206f 730a 0a0a 6465 6620  import os...def 
+00000010: 6765 745f 7472 6169 6e5f 7363 7269 7074  get_train_script
+00000020: 2829 3a0a 2020 2020 7468 6973 5f64 6972  ():.    this_dir
+00000030: 203d 206f 732e 7061 7468 2e64 6972 6e61   = os.path.dirna
+00000040: 6d65 285f 5f66 696c 655f 5f29 0a20 2020  me(__file__).   
+00000050: 2074 7261 696e 5f73 6372 6970 7420 3d20   train_script = 
+00000060: 6f73 2e70 6174 682e 6a6f 696e 2874 6869  os.path.join(thi
+00000070: 735f 6469 722c 2022 7472 6169 6e2e 7079  s_dir, "train.py
+00000080: 2229 0a20 2020 2072 6574 7572 6e20 7472  ").    return tr
+00000090: 6169 6e5f 7363 7269 7074 0a              ain_script.
```

## cloudtik/runtime/ai/modeling/transfer_learning/common/pytorch/model.py

```diff
@@ -13,33 +13,41 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 # SPDX-License-Identifier: Apache-2.0
 #
-
+import datetime
 import inspect
 import os
+import shutil
+
 import dill
 import numpy
 import random
 import torch
 
+from cloudtik.runtime.ai.modeling.transfer_learning.common.pytorch import get_train_script
 from cloudtik.runtime.ai.modeling.transfer_learning.model import PretrainedModel
 from cloudtik.runtime.ai.modeling.transfer_learning.common.utils import verify_directory
+from cloudtik.runtime.ai.runner import run_command
 
 
 class PyTorchModel(PretrainedModel):
     """
     Base class to represent a PyTorch model
     """
 
     def __init__(self, model_name: str):
         super().__init__(model_name)
+        self._model = None  # This gets initialized later
+        self._optimizer = None  # This gets initialized later
+        self._optimizer_class = None  # This gets initialized in subclass
+        self._loss = None  # This gets initialized in subclass
         self._lr_scheduler = None
         self._history = {}
 
         # Setup warnings module to set warnings to go to stdout
         import warnings
         import sys
 
@@ -54,24 +62,17 @@
     def _set_seed(self, seed):
         if seed is not None:
             os.environ['PYTHONHASHSEED'] = str(seed)
             random.seed(seed)
             numpy.random.seed(seed)
             torch.manual_seed(seed)
 
-    def _check_train_inputs(self, output_dir, dataset, dataset_type, epochs, initial_checkpoints,
-                            distributed, hostfile):
+    def _check_train_inputs(self, output_dir, dataset, dataset_type, epochs, initial_checkpoints):
         verify_directory(output_dir)
 
-        if distributed:
-            if hostfile:
-                if not (os.path.isfile(hostfile) or isinstance(hostfile, str)):
-                    raise ValueError("hostfile could not be resolved as a file or a string. "
-                                     "Please create a new file or provide a comma separated list of IP addresses")
-
         if not isinstance(dataset, dataset_type):
             raise TypeError("The dataset must be a {} but found a {}".format(dataset_type, type(dataset)))
 
         if not dataset.info['preprocessing_info']:
             raise ValueError("Dataset hasn't been preprocessed yet.")
 
         if not isinstance(epochs, int):
@@ -103,32 +104,14 @@
         """
         # Verify that the model directory exists
         verify_directory(model_dir, require_directory_exists=True)
         model_copy = torch.load(os.path.join(model_dir, 'model.pt'))
         self._model = dill.loads(model_copy)
         self._optimizer = self._optimizer_class(self._model.parameters(), lr=self._learning_rate)
 
-    def optimize_graph(self, saved_model_dir, output_dir):
-        """
-        Performs FP32 graph optimization using the Intel Neural Compressor on the model in the saved_model_dir
-        and writes the inference-optimized model to the output_dir. Graph optimization includes converting
-        variables to constants, removing training-only operations like checkpoint saving, stripping out parts
-        of the graph that are never reached, removing debug operations like CheckNumerics, folding batch
-        normalization ops into the pre-calculated weights, and fusing common operations into unified versions.
-        Args:
-            saved_model_dir (str): Source directory for the model to optimize.
-            output_dir (str): Writable output directory to save the optimized model
-        Returns:
-            None
-        Raises:
-            NotImplementedError because this hasn't been implemented yet for PyTorch
-        """
-        raise NotImplementedError("Only TensorFlow graph optimization is currently supported by the \
-                                                                      Intel Neural Compressor (INC)")
-
     def list_layers(self, verbose=False):
         """
         Lists all of the named modules (e.g. features, avgpool, classifier) and layers
         (ReLU, MaxPool2d, Dropout, Linear, etc) in a given PyTorch model
 
         Args:
             verbose (bool): True/False option set by default to be False, displays only high-level modules
@@ -185,7 +168,70 @@
             raise RuntimeError('The model must be trained at least one epoch before its layers can be unfrozen.')
 
         # Unfreeze everything in the layer
         for (name, module) in self._model.named_children():
             if name == layer_name:
                 for param in module.parameters():
                     param.requires_grad = True
+
+    def save_objects(self, dataset, output_dir, temp_dir):
+        """
+        Helper function to export dataset and model objects to disk for distributed job
+
+        Args:
+            dataset: Dataset object to save. It must be an object of
+                Dataset with info, train, test, and validation
+                subsets properties which can be accessed.
+            output_dir (str): Path to a directory where the dataset and model objects are saved.
+                The path/file of the save objects is returned.
+            temp_dir (str): The temp data dir at local.
+        """
+        now = datetime.datetime.now()
+        filename = f"torch_objects_{now:%Y-%m-%d_%H-%M-%S}.obj"
+        objects_path = os.path.join(output_dir, filename)
+
+        # save to temporary path and then move
+        if temp_dir:
+            save_objects_path = os.path.join(temp_dir, filename)
+        else:
+            save_objects_path = objects_path
+
+        objects_to_save = {
+            "dataset": dataset.dataset,
+            "info": dataset.info,
+            "train_subset": dataset.train_subset,
+            "test_subset": dataset.test_subset,
+            "validation_subset": dataset.validation_subset,
+            "model": self._model,
+            "optimizer": self._optimizer,
+            "loss": self._loss
+        }
+
+        torch.save(objects_to_save, save_objects_path)
+
+        if temp_dir:
+            shutil.move(save_objects_path, objects_path)
+
+        return objects_path
+
+    @staticmethod
+    def fit_distributed(
+            nnodes, nproc_per_node, hosts, hostfile,
+            epochs, batch_size, ipex_optimize,
+            objects_path, category):
+        train_script = get_train_script()
+        command = [
+            train_script, "--objects-path", objects_path,
+            "--category", category,
+            "--epochs", str(epochs), "--batch-size", str(batch_size)
+        ]
+        if ipex_optimize:
+            command += ['--ipex']
+            command += ['--backend', 'ccl']
+
+        run_command(
+            command,
+            nnodes=nnodes,
+            nproc_per_node=nproc_per_node,
+            hosts=hosts,
+            hostfile=hostfile
+        )
```

## cloudtik/runtime/ai/modeling/transfer_learning/common/pytorch/hugging_face/model.py

```diff
@@ -14,59 +14,34 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 # SPDX-License-Identifier: Apache-2.0
 #
 
-import os
-import inspect
-import torch
-
-from cloudtik.runtime.ai.modeling.transfer_learning.model import PretrainedModel
+from cloudtik.runtime.ai.modeling.transfer_learning.common.pytorch.model import PyTorchModel
 from cloudtik.runtime.ai.modeling.transfer_learning.common.utils import verify_directory
 
 
-class HuggingFaceModel(PretrainedModel):
+class HuggingFaceModel(PyTorchModel):
     """
     Base class to represent a Hugging Face model
     """
 
     def __init__(self, model_name: str):
         super().__init__(model_name)
-        self._history = {}
-
-    def _update_history(self, key, value):
-        if key not in self._history:
-            self._history[key] = []
-        self._history[key].extend([value])
-
-    def _check_optimizer_loss(self, optimizer, loss):
-        if optimizer is not None and (not inspect.isclass(optimizer) or
-                                      torch.optim.Optimizer not in inspect.getmro(optimizer)):
-            raise TypeError("The optimizer input must be a class (not an instance) of type torch.optim.Optimizer or "
-                            "None but found a {}. Example: torch.optim.AdamW".format(type(optimizer)))
-        if loss is not None and (not inspect.isclass(loss) or
-                                 torch.nn.modules.loss._Loss not in inspect.getmro(loss)):
-            raise TypeError("The optimizer input must be a class (not an instance) of type "
-                            "torch.nn.modules.loss._Loss or None but found a {}. "
-                            "Example: torch.nn.CrossEntropyLoss".format(type(loss)))
 
-    def _check_train_inputs(self, output_dir, dataset, dataset_type, extra_layers, epochs, distributed, hostfile):
+    def _check_train_inputs(self, output_dir, dataset, dataset_type, epochs, extra_layers):
         verify_directory(output_dir)
 
-        if distributed:
-            if hostfile is not None and not os.path.exists(os.path.join(os.getcwd(), hostfile)):
-                raise FileNotFoundError("Could not find hostfile. Consider creating one")
-
         if not isinstance(dataset, dataset_type):
             raise TypeError("The dataset must be a {} but found a {}".format(dataset_type, type(dataset)))
 
         if not dataset.info['preprocessing_info']:
             raise ValueError("Dataset hasn't been preprocessed yet.")
 
+        if not isinstance(epochs, int):
+            raise TypeError("Invalid type for the epochs arg. Expected an int but found a {}".format(type(epochs)))
+
         if extra_layers:
             if not isinstance(extra_layers, list) or not all(isinstance(n, int) for n in extra_layers):
                 raise ValueError("extra_layers argument must be a list of integers but found a {}".format(extra_layers))
-
-        if not isinstance(epochs, int):
-            raise TypeError("Invalid type for the epochs arg. Expected an int but found a {}".format(type(epochs)))
```

## cloudtik/runtime/ai/modeling/transfer_learning/common/tensorflow/__init__.py

```diff
@@ -0,0 +1,10 @@
+00000000: 696d 706f 7274 206f 730a 0a0a 6465 6620  import os...def 
+00000010: 6765 745f 7472 6169 6e5f 7363 7269 7074  get_train_script
+00000020: 2829 3a0a 2020 2020 7468 6973 5f64 6972  ():.    this_dir
+00000030: 203d 206f 732e 7061 7468 2e64 6972 6e61   = os.path.dirna
+00000040: 6d65 285f 5f66 696c 655f 5f29 0a20 2020  me(__file__).   
+00000050: 2074 7261 696e 5f73 6372 6970 7420 3d20   train_script = 
+00000060: 6f73 2e70 6174 682e 6a6f 696e 2874 6869  os.path.join(thi
+00000070: 735f 6469 722c 2022 7472 6169 6e2e 7079  s_dir, "train.py
+00000080: 2229 0a20 2020 2072 6574 7572 6e20 7472  ").    return tr
+00000090: 6169 6e5f 7363 7269 7074 0a              ain_script.
```

## cloudtik/runtime/ai/modeling/transfer_learning/common/tensorflow/model.py

```diff
@@ -13,39 +13,45 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 # SPDX-License-Identifier: Apache-2.0
 #
-
+import datetime
 import inspect
 import os
 import dill
-import re
 import shutil
 import random
 import numpy as np
 
 import tensorflow as tf
 
+from cloudtik.runtime.ai.modeling.transfer_learning.common.tensorflow import get_train_script
 from cloudtik.runtime.ai.modeling.transfer_learning.model import PretrainedModel
 from cloudtik.runtime.ai.modeling.transfer_learning.common.utils import \
     verify_directory, validate_model_name
+from cloudtik.runtime.ai.runner import run_command
 from cloudtik.runtime.ai.runner.cpu.platform_utils import PlatformUtil
 
 
 class TensorflowModel(PretrainedModel):
     """
     Base class to represent a TF pretrained model
     """
 
     def __init__(self, model_name: str):
-        self._model = None
         super().__init__(model_name)
+        self._model = None  # This gets initialized later
+        self._optimizer = None  # This gets initialized later
+        self._optimizer_class = None  # This gets initialized in subclass
+        self._loss_class = None  # This gets initialized in subclass
+        self._loss_args = None  # This gets initialized in subclass
+
         os.environ["TF_ENABLE_ONEDNN_OPTS"] = "1"
         self._history = {}
 
     @property
     def framework(self):
         return "tensorflow"
 
@@ -171,120 +177,79 @@
             self._model.save(saved_model_dir)
             print("Saved model directory:", saved_model_dir)
 
             return saved_model_dir
         else:
             raise ValueError("Unable to export the model, because it hasn't been loaded or trained yet")
 
-    def export_for_distributed(self, train_data, val_data):
-        # TODO: handle distributed training
-        distributed_data_dir = ""
+    def save_objects(self, train_data, val_data, output_dir, temp_dir=None):
+        now = datetime.datetime.now()
+        path_name = f"tensorflow_objects_{now:%Y-%m-%d_%H-%M-%S}"
+        objects_path = os.path.join(output_dir, path_name)
+
+        # save to temporary path and then move
+        if temp_dir:
+            save_objects_path = os.path.join(temp_dir, path_name)
+        else:
+            save_objects_path = objects_path
+        os.makedirs(save_objects_path, exist_ok=True)
 
         # Save the model
         tf.keras.models.save_model(
             model=self._model,
-            filepath=distributed_data_dir,
+            filepath=save_objects_path,
             overwrite=True,
             include_optimizer=False
         )
 
         # Save the optimizer object
         tf.train.Checkpoint(optimizer=self._optimizer).save(
-            os.path.join(distributed_data_dir, 'saved_optimizer'))
+            os.path.join(save_objects_path, 'saved_optimizer'))
 
         # Save the loss class name and its args
-        with open(os.path.join(distributed_data_dir, 'saved_loss'), 'wb') as f:
+        with open(os.path.join(save_objects_path, 'saved_loss'), 'wb') as f:
             dill.dump((self._loss_class, self._loss_args), f)
 
         # Save the dataset(s)
-        train_data.save(os.path.join(distributed_data_dir, 'train_data'))
+        train_data.save(os.path.join(save_objects_path, 'train_data'))
         print(type(train_data))
         if val_data:
-            val_data.save(os.path.join(distributed_data_dir, 'val_data'))
-
-    def cleanup_saved_objects_for_distributed(self):
-        # TODO: handle distributed training
-        distributed_data_dir = ""
-
-        dirs = ['train_data', 'val_data', 'variables', 'assets', 'model_checkpoints']
-        files = ['checkpoint', 'keras_metadata.pb']
-
-        for f in os.listdir(distributed_data_dir):
-            full_path = os.path.join(distributed_data_dir, f)
-            if os.path.isdir(full_path) and f in dirs:
-                try:
-                    shutil.rmtree(full_path)
-                except FileNotFoundError:
-                    print("'{}' already cleaned up.".format(f))
-            elif os.path.isfile(full_path) and (f in files or f.startswith('saved')):
-                try:
-                    os.remove(full_path)
-                except FileNotFoundError:
-                    print("'{}' already cleaned up.".format(f))
-
-    def _parse_hostfile(self, hostfile):
-        """
-            Parses the hostfile and returns the required command. Contents of hostfile must contain IP addresses
-            (or) hostnames in any of the following forms. Note that all lines must be of the same form:
-                "127.0.0.1"
-                "127.0.0.1 slots=2"
-                "127.0.0.1:2"
-                "hostname-example.com"
-                "hostname-example.com slots=2"
-                "hostname-example.com:2"
+            val_data.save(os.path.join(save_objects_path, 'val_data'))
 
-            Args:
-                hostfile (str): File path of the hostfile
-
-            Returns:
-                hostfile_info dict containing ip_addresses and slots
+        if temp_dir:
+            # move data from save_objects_path to objects_path
+            shutil.move(save_objects_path, objects_path)
 
-        """
-        import socket
-        valid_regexes = {
-            'valid_ip': r"^((25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])\.){3}(25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])$",  # noqa: E501
-            'valid_ip_with_slot': r"^((25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])\.){3}(25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9]) slots=[0-9]{1,2}$",  # noqa: E501
-            'valid_ip_with_colon': r"^((25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])\.){3}(25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9]):\d{1,2}$",  # noqa: E501
-            'valid_hostname': r"^(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\-]*[a-zA-Z0-9])\.)*([A-Za-z0-9]|[A-Za-z0-9][A-Za-z0-9\-]*[A-Za-z0-9])$",  # noqa: E501
-            'valid_hostname_with_slot': r"^(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\-]*[a-zA-Z0-9])\.)*([A-Za-z0-9]|[A-Za-z0-9][A-Za-z0-9\-]*[A-Za-z0-9]) slots=[0-9]{1,2}$",  # noqa: E501
-            'valid_hostname_with_colon': r"^(([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\-]*[a-zA-Z0-9])\.)*([A-Za-z0-9]|[A-Za-z0-9][A-Za-z0-9\-]*[A-Za-z0-9]):\d{1,2}$"  # noqa: E501
-        }
-        lines = []
-        with open(hostfile, 'r') as f:
-            lines = [line.rstrip() for line in f]
-
-        matches = []
-        for line in lines:
-            found = False
-            for k, v in valid_regexes.items():
-                if re.match(v, line):
-                    found = True
-                    matches.append(k)
-                    break
-            if not found:
-                raise ValueError("Invalid line in the hostfile: {}".format(line))
-
-        hostfile_info = {
-            'ip_addresses': [],
-            'slots': []
-        }
-        for line, match in zip(lines, matches):
-            if match == 'valid_ip':
-                hostfile_info['ip_addresses'].append(line)
-                hostfile_info['slots'].append(0)
-            elif match == 'valid_ip_with_slot':
-                hostfile_info['ip_addresses'].append(line.split(' slots=')[0])
-                hostfile_info['slots'].append(line.split(' slots=')[1])
-            elif match == 'valid_ip_with_colon':
-                hostfile_info['ip_addresses'].append(line.split(':')[0])
-                hostfile_info['slots'].append(line.split(':')[1])
-            elif match == 'valid_hostname':
-                hostfile_info['ip_addresses'].append(socket.gethostbyname(line))
-                hostfile_info['slots'].append(0)
-            elif match == 'valid_hostname_with_slot':
-                hostfile_info['ip_addresses'].append(socket.gethostbyname(line.split(' slots=')[0]))
-                hostfile_info['slots'].append(line.split(' slots=')[1])
-            elif match == 'valid_hostname_with_colon':
-                hostfile_info['ip_addresses'].append(socket.gethostbyname(line.split(':')[0]))
-                hostfile_info['slots'].append(line.split(':')[1])
+        return objects_path
 
-        return hostfile_info
+    def cleanup_objects(self, objects_path):
+        if os.path.isdir(objects_path):
+            try:
+                shutil.rmtree(objects_path)
+            except FileNotFoundError:
+                pass
+
+    def fit_distributed(
+            self, epochs, shuffle,
+            nnodes, nproc_per_node, hosts, hostfile,
+            objects_path, use_horovod, category):
+        train_script = get_train_script()
+        command = [
+            train_script, "--objects-path", objects_path,
+            "--category", category,
+            "--epochs", str(epochs)
+        ]
+        if shuffle:
+            command += ['--shuffle']
+
+        launcher = None
+        if use_horovod:
+            launcher = "horovod"
+
+        run_command(
+            command,
+            nnodes=nnodes,
+            nproc_per_node=nproc_per_node,
+            hosts=hosts,
+            hostfile=hostfile,
+            launcher=launcher
+        )
```

## cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/image_anomaly_detection_dataset.py

```diff
@@ -28,15 +28,15 @@
 from torch.utils.data import DataLoader as loader
 from torchvision.datasets import DatasetFolder
 import torchvision.transforms as T
 from torchvision.datasets.folder import default_loader, IMG_EXTENSIONS
 
 from cloudtik.runtime.ai.modeling.transfer_learning.image_anomaly_detection.pytorch.simsiam import loader as ssloader
 from cloudtik.runtime.ai.modeling.transfer_learning.image_anomaly_detection.pytorch.cutpaste.cutpaste import \
-    CutPasteNormal, CutPasteScar,CutPaste3Way, CutPasteUnion, get_cutpaste_transforms
+    CutPasteNormal, CutPasteScar, CutPaste3Way, CutPasteUnion, get_cutpaste_transforms
 from cloudtik.runtime.ai.modeling.transfer_learning.common.pytorch.dataset import PyTorchDataset
 
 
 class AnomalyImageFolder(DatasetFolder):
     """Inherits from DatasetFolder.
 
     This class overrides the find_classes() and make_dataset() methods of DatasetFolder to support filtering for
@@ -239,15 +239,15 @@
             defects = [c for c in classes if c != 'good']
         self._defects = defects
 
         # The dataset name is only used for informational purposes. If one isn't given, use the directory name
         if not dataset_name:
             dataset_name = os.path.basename(dataset_dir)
 
-        PyTorchDataset.__init__(self, dataset_dir, dataset_name, dataset_catalog=None)
+        PyTorchDataset.__init__(self, dataset_dir, dataset_name)
 
         self._info = {
             "name": dataset_name,
             "dataset_dir": dataset_dir
         }
         self._num_workers = num_workers
         self.train_sampler = None
```

## cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/image_anomaly_detection_model.py

```diff
@@ -31,23 +31,24 @@
 import torch.optim.lr_scheduler as lr_scheduler
 from torchvision.models import resnet18, resnet50
 
 from cloudtik.runtime.ai.modeling.transfer_learning.image_anomaly_detection.pytorch.image_anomaly_detection_dataset \
     import PyTorchImageAnomalyDetectionDataset
 from cloudtik.runtime.ai.modeling.transfer_learning.image_anomaly_detection.pytorch.simsiam \
     import builder
-from cloudtik.runtime.ai.modeling.transfer_learning.common import utils
 
 from cloudtik.runtime.ai.modeling.transfer_learning.image_anomaly_detection.pytorch.cutpaste.model \
     import ProjectionNet
 from cloudtik.runtime.ai.modeling.transfer_learning.image_anomaly_detection.pytorch.cutpaste.cutpaste \
-    import CutPasteNormal,CutPasteScar, CutPaste3Way, CutPasteUnion
+    import CutPasteNormal, CutPasteScar, CutPaste3Way, CutPasteUnion
 
 from cloudtik.runtime.ai.modeling.transfer_learning.common.utils \
     import verify_directory, validate_model_name
+from cloudtik.runtime.ai.modeling.transfer_learning.image_anomaly_detection.pytorch.utils import adjust_learning_rate, \
+    _fit_simsiam, save_checkpoint, _fit_cutpaste, find_threshold
 from cloudtik.runtime.ai.modeling.transfer_learning.image_classification.pytorch.image_classification_model \
     import PyTorchImageClassificationModel
 
 
 def extract_features(model, data, layer_name, pooling):
     """
     Extracts features of the layers specified using a layer name
@@ -121,22 +122,24 @@
     Class to represent a PyTorch model for image classification
     """
 
     def __init__(self, model_name: str, model=None, optimizer=None, loss=None, **kwargs):
         """
         Class constructor
         """
-        PyTorchImageClassificationModel.__init__(self, model_name, model, optimizer, loss, **kwargs)
+        PyTorchImageClassificationModel.__init__(
+            self, model_name, model, optimizer, loss, **kwargs)
         self.simsiam = False
         self.cutpaste = False
         self._layer_name = 'layer3'
         self._pooling = 'avg'
         self._kernel_size = 2
 
-    def _check_train_inputs(self, output_dir, dataset, dataset_type, pooling, kernel_size, pca_threshold):
+    def _check_train_inputs(self, output_dir, dataset, dataset_type,
+                            pooling, kernel_size, pca_threshold):
         verify_directory(output_dir)
 
         if not isinstance(dataset, dataset_type):
             raise TypeError("The dataset must be a {} but found a {}".format(dataset_type, type(dataset)))
 
         if pooling not in ['avg', 'max']:
             raise TypeError("The specified pooling is not supported. It must be one of: {}".format(['avg', 'max']))
@@ -243,49 +246,50 @@
                         {'params': self._model.predictor.parameters(), 'fix_lr': True}]
         optimizer = torch.optim.SGD(optim_params, init_lr, momentum=0.9, weight_decay=1e-4)
         num_images = len(dataset.train_subset)
 
         best_least_Loss = float('inf')
         is_best_ans = False
         file_name_least_loss = ""
-        print("Fine-tuning Simsiam Model on ", epochs, "epochs using ", num_images, " training images")
+        print("Fine-tuning Simsiam Model on", epochs, "epochs using", num_images, "training images")
         self._model.train()
+        model = self._model
 
         if ipex_optimize:
             import intel_extension_for_pytorch as ipex
             model, optimizer = ipex.optimize(
-                self._model, optimizer=optimizer,
+                model, optimizer=optimizer,
                 dtype=torch.bfloat16 if precision == 'bfloat16' else torch.float32)
 
         valid_model_name = validate_model_name(self.model_name)
         checkpoint_dir = os.path.join(output_dir, "{}_checkpoints".format(valid_model_name))
         verify_directory(checkpoint_dir)
 
         for epoch in range(0, self.epochs):
-            utils.adjust_learning_rate(optimizer, init_lr, epoch, self.epochs)
+            adjust_learning_rate(optimizer, init_lr, epoch, self.epochs)
 
-            curr_loss = utils._fit_simsiam(dataloader, model, criterion, optimizer, epoch, precision)
+            curr_loss = _fit_simsiam(dataloader, model, criterion, optimizer, epoch, precision)
             if generate_checkpoints:
                 if (curr_loss < best_least_Loss):
                     best_least_Loss = curr_loss
                     is_best_ans = True
                     file_name_least_loss = 'simsiam_checkpoint_{:04d}.pth.tar'.format(epoch)
 
-                utils.save_checkpoint({
+                save_checkpoint({
                     'epoch': epoch + 1,
                     'arch': self.model_name,
                     'state_dict': model.state_dict(),
                     'optimizer': optimizer.state_dict(),
                 }, is_best_ans, file_name_least_loss,
                     best_least_Loss, checkpoint_dir)
                 is_best_ans = False
             else:
                 if epoch == self.epochs - 1:
                     file_name_least_loss = 'simsiam_checkpoint_{:04d}.pth.tar'.format(epoch)
-                    utils.save_checkpoint({
+                    save_checkpoint({
                         'epoch': epoch + 1,
                         'arch': self.model_name,
                         'state_dict': model.state_dict(),
                     }, is_best=True, filename=file_name_least_loss,
                         loss=curr_loss, checkpoint_dir=checkpoint_dir)
 
         print('No. Of Epochs=', self.epochs)
@@ -352,52 +356,54 @@
                 optimizer = torch.optim.SGD(self._model.parameters(), lr=0.03, momentum=momentum,
                                             weight_decay=weight_decay)
                 scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, epochs)
             elif optim == "adam":
                 optimizer = torch.optim.Adam(self._model.parameters(), lr=0.03, weight_decay=weight_decay)
                 scheduler = None
             else:
-                print(f"ERROR unkown optimizer: {optim}")
+                raise ValueError(f"Unsupported optimizer: {optim}")
 
             num_images = len(dataset.train_subset)
             best_least_Loss = float('inf')
             is_best_ans = False
             file_name_least_loss = ""
-            print("Fine-tuning CUT-PASTE Model on ", epochs, "epochs using ", num_images, " training images")
+            print("Fine-tuning CUT-PASTE Model on", epochs, "epochs using", num_images, "training images")
             self._model.train()
+            model = self._model
 
             if ipex_optimize:
                 import intel_extension_for_pytorch as ipex
                 model, optimizer = ipex.optimize(
-                    self._model, optimizer=optimizer,
+                    model, optimizer=optimizer,
                     dtype=torch.bfloat16 if precision == 'bfloat16' else torch.float32)
 
             for step in range(epochs):
                 epoch = int(step / 1)
-                curr_loss = utils._fit_cutpaste(dataloader, model, criterion,
-                                                optimizer, epoch, freeze_resnet, scheduler, precision)
+                curr_loss = _fit_cutpaste(
+                    dataloader, model, criterion,
+                    optimizer, epoch, freeze_resnet, scheduler, precision)
                 if generate_checkpoints:
                     if (curr_loss < best_least_Loss):
                         best_least_Loss = curr_loss
                         is_best_ans = True
                         file_name_least_loss = 'cutpaste_checkpoint_{:04d}.pth.tar'.format(step)
 
                     # Saves the Best Intermediate Checkpoints got till this step.
-                    utils.save_checkpoint({
+                    save_checkpoint({
                         'epoch': step + 1,
                         'arch': self.model_name,
                         'state_dict': model.state_dict(),
                         'optimizer': optimizer.state_dict(),
                     }, is_best=is_best_ans, filename=file_name_least_loss,
                         loss=best_least_Loss, checkpoint_dir=checkpoint_dir)
                     is_best_ans = False
                 else:
                     if epoch == epochs - 1:
                         file_name_least_loss = 'cutpaste_checkpoint_{:04d}.pth.tar'.format(step)
-                        utils.save_checkpoint({
+                        save_checkpoint({
                             'epoch': step + 1,
                             'arch': self.model_name,
                             'state_dict': model.state_dict(),
                         }, is_best=True, filename=file_name_least_loss,
                             loss=curr_loss, checkpoint_dir=checkpoint_dir)
 
             self._model = self.load_checkpoint_weights(self.model_name, checkpoint_dir,
@@ -410,16 +416,17 @@
                 if os.path.isfile(f) and self.cutpaste:
                     models.append(f)
             model_path = os.path.basename(max(models, key=os.path.getctime))
             self._model = self.load_checkpoint_weights(self.model_name, checkpoint_dir,
                                                        model_path, feature_extractor='cutpaste')
         return self._model
 
-    def train(self, dataset: PyTorchImageAnomalyDetectionDataset, output_dir, epochs=2,
-              initial_checkpoints=None, seed=None, generate_checkpoints=False, ipex_optimize=False,
+    def train(self, dataset: PyTorchImageAnomalyDetectionDataset, output_dir, *,
+              epochs=2, initial_checkpoints=None, do_eval=True,
+              seed=None, ipex_optimize=False, generate_checkpoints=False,
               batch_size=64, feature_dim=2048, pred_dim=512, pooling='avg',
               kernel_size=2, pca_threshold=0.99, simsiam=False, cutpaste=False, cutpaste_type='normal',
               freeze_resnet=20, head_layer=2, optim='sgd', layer_name='layer3', precision='float32'):
         """
             Trains the model using the specified image anomaly detection dataset.
 
             Args:
@@ -427,18 +434,20 @@
                 output_dir (str): Path to a writeable directory for output files
                 batch_size (int): batch size for every forward opeartion, default is 64
                 layer_name (str): The layer name whose output is desired for the extracted features
                 feature_dim (int): Feature dimension, default is 2048
                 pred_dim (int): Hidden dimension of the predictor, default is 512
                 epochs (int): Number of epochs to train the model
                 initial_checkpoints (str): Path to checkpoint weights to load
+                do_eval (bool): If do_eval is True and the dataset has a validation subset, the model will be evaluated
+                    at the end of each epoch.
                 seed (int): Optional, set a seed for reproducibility
+                ipex_optimize (bool): Use Intel Extension for PyTorch (IPEX). Defaults to False.
                 generate_checkpoints (bool): Whether to save/preserve the best weights during
                                              SimSiam or CutPaste training, default is False.
-                ipex_optimize (bool): Use Intel Extension for PyTorch (IPEX). Defaults to False.
                 pooling (str): Pooling to be applied on the extracted layer ('avg' or 'max'), default is 'avg'
                 kernel_size (int): Kernel size in the pooling layer, default is 2
                 pca_threshold (float): Threshold to apply to PCA model, default is 0.99
                 simsiam (bool): Boolean option to enable/disable simsiam training, default is False
                 cutpaste (bool): Boolean option to enable/disable cutpaste training, default is False
                 cutpaste-type (str): cutpaste variant to use, default is normal
                 freeze_resnet (int): Epochs upto which we freeze ResNet layers and only train
@@ -446,16 +455,17 @@
                 head_layer (int): number of layers in the projection head, default is 1
                 optim (str): Choice of optimizer to use for training, default is sgd
                 precision (str): precision in which model to be trained, default is float32.
 
             Returns:
                 Fitted principal components
         """
-        self._check_train_inputs(output_dir, dataset, PyTorchImageAnomalyDetectionDataset, pooling,
-                                 kernel_size, pca_threshold)
+        self._check_train_inputs(
+            output_dir, dataset, PyTorchImageAnomalyDetectionDataset,
+            pooling, kernel_size, pca_threshold)
 
         self._pooling = pooling
         self._kernel_size = kernel_size
         self._pca_threshold = pca_threshold
         self._layer_name = layer_name
         self.batch_size = batch_size
         self.simsiam = simsiam
@@ -555,15 +565,15 @@
                 scores[count: count + num_im] = -fre_score
                 gt[count:count + num_im] = labels
                 count += num_im
 
             gt = gt.numpy()
 
         fpr_binary, tpr_binary, thres = metrics.roc_curve(gt, scores)
-        threshold = utils.find_threshold(fpr_binary, tpr_binary, thres)
+        threshold = find_threshold(fpr_binary, tpr_binary, thres)
         auc_roc_binary = metrics.auc(fpr_binary, tpr_binary)
         accuracy_score = metrics.accuracy_score(gt, [1 if i >= threshold else 0 for i in scores])
         print(f'AUROC: {auc_roc_binary * 100}')
         print(f'Accuracy: {accuracy_score * 100}')
 
         return threshold, auc_roc_binary
```

## cloudtik/runtime/ai/modeling/transfer_learning/image_classification/image_classification_model.py

```diff
@@ -67,21 +67,21 @@
     @property
     def dropout_layer_rate(self):
         """
         The probability of any one node being dropped when a dropout layer is used
         """
         return self._dropout_layer_rate
 
-    def get_inc_config_template_dict(self):
+    def get_neural_compressor_config_template(self):
         """
-        Returns a dictionary for a config template compatible with the Intel Neural Compressor.
+        Returns a dictionary for a config template compatible with the Neural Compressor.
 
         It loads the yaml file image_classification_template.yaml and then fills in parameters
         that the model knows about (like framework and model name). There are still more parameters that need to be
-        filled in before using the config with INC (like the dataset information, image size, etc).
+        filled in before using the config with Neural Compressor (like the dataset information, image size, etc).
         """
         this_dir = os.path.dirname(__file__)
         template_file_path = os.path.join(this_dir, "image_classification_template.yaml")
 
         if not os.path.exists(template_file_path):
             raise FileNotFoundError("Unable to find the image recognition config template at:", template_file_path)
```

## cloudtik/runtime/ai/modeling/transfer_learning/image_classification/pytorch/image_classification_model.py

```diff
@@ -20,15 +20,14 @@
 
 import copy
 import inspect
 import os
 import time
 import dill
 import yaml
-import subprocess
 
 from tqdm import tqdm
 import torch
 
 from cloudtik.runtime.ai.modeling.transfer_learning.common.utils import \
     verify_directory, validate_model_name
 from cloudtik.runtime.ai.modeling.transfer_learning.common.pytorch.model import PyTorchModel
@@ -54,28 +53,24 @@
 
         # extra properties that will become configurable in the future
         self._do_fine_tuning = False
         self._dropout_layer_rate = None
         self._device = 'cpu'
         self._lr_scheduler = None
         self._generate_checkpoints = True
-
-        # placeholder for model definition
-        self._model = None
         self._num_classes = None
 
         PyTorchModel.__init__(self, model_name)
         ImageClassificationModel.__init__(self, self._image_size, self._do_fine_tuning, self._dropout_layer_rate,
                                           self._model_name)
 
         # set up the configurable optimizer and loss functions
         self._check_optimizer_loss(optimizer, loss)
         self._optimizer_class = optimizer if optimizer else torch.optim.Adam
         self._opt_args = {k: v for k, v in kwargs.items() if k in inspect.getfullargspec(self._optimizer_class).args}
-        self._optimizer = None  # This gets initialized later
         self._loss_class = loss if loss else torch.nn.CrossEntropyLoss
         self._loss_args = {k: v for k, v in kwargs.items() if k in inspect.getfullargspec(self._loss_class).args}
         self._loss = self._loss_class(**self._loss_args)
 
         if model is None:
             self._model = None
         elif isinstance(model, str):
@@ -236,70 +231,28 @@
                 torch.save({
                     'epoch': epochs,
                     'model_state_dict': self._model.state_dict(),
                     'optimizer_state_dict': self._optimizer.state_dict(),
                     'loss': train_epoch_loss,
                 }, os.path.join(checkpoint_dir, 'checkpoint.pt'))
 
-    def _fit_distributed(self, hostfile, nnodes, nproc_per_node, epochs, batch_size, ipex_optimize):
-        # TODO: for distributed
-        # distributed_vision_script = os.path.join(TLT_DISTRIBUTED_DIR, "run_train_pyt.py")
-        distributed_vision_script = "run_train_pyt.py"
-
-        default_port = '29500'
-        default_master_addr = '127.0.0.1'
-
-        addresses = []
-
-        if hostfile is not None:
-            if os.path.isfile(hostfile):
-                # if addresses are given as line separated IP addresses
-                with open(hostfile) as hf:
-                    addresses = hf.readlines()
-                addresses = [a.strip('\n') for a in addresses]
-            else:
-                # if addresses are given as a comma separated IP addresses
-                addresses = hostfile.split(',')
-
-            default_master_addr = addresses[0]
-
-            # If port is given in the format of "0.0.0.0:9999"
-            if ':' in default_master_addr:
-                colon_index = default_master_addr.index(':')
-                default_port = default_master_addr[colon_index + 1:]
-                default_master_addr = default_master_addr[:colon_index]
-
-                # We create/rewrite the hostfile to contain only IP addresses
-                with open('hostfile', 'w') as hf:
-                    for addr in addresses:
-                        if ':' in addr:
-                            addr = addr[:addr.index(':')]
-                        hf.write(addr + '\n')
-                hostfile = 'hostfile'
-
-        bash_command = 'python -m intel_extension_for_pytorch.cpu.launch --distributed'
-        bash_command += ' --hostfile {}'.format(hostfile)
-        bash_command += ' --nnodes {}'.format(nnodes)
-        bash_command += ' --nproc_per_node {}'.format(nproc_per_node)
-        bash_command += ' {}'.format(distributed_vision_script)
-        bash_command += ' --master_addr {}'.format(default_master_addr)
-        bash_command += ' --master_port {}'.format(default_port)
-        bash_command += ' --backend {}'.format('ccl')
-        bash_command += ' --use_case {}'.format('image_classification')
-        bash_command += ' --epochs {}'.format(epochs)
-        bash_command += ' --batch_size {}'.format(batch_size)
-        if not ipex_optimize:
-            bash_command += ' --disable_ipex'
-
-        print(bash_command)
-        subprocess.run(bash_command.split(' '))
-
-    def train(self, dataset: ImageClassificationDataset, output_dir, epochs=1, initial_checkpoints=None,
-              do_eval=True, early_stopping=False, lr_decay=True, seed=None, ipex_optimize=False, distributed=False,
-              hostfile=None, nnodes=1, nproc_per_node=1):
+    def _fit_distributed(
+            self, nnodes, nproc_per_node, hosts, hostfile,
+            epochs, batch_size, ipex_optimize, objects_path):
+        self.fit_distributed(
+            nnodes, nproc_per_node, hosts, hostfile,
+            epochs, batch_size, ipex_optimize,
+            objects_path, category="image_classification"
+        )
+
+    def train(self, dataset: ImageClassificationDataset, output_dir, *,
+              epochs=1, initial_checkpoints=None, do_eval=True,
+              early_stopping=False, lr_decay=True, seed=None, ipex_optimize=False,
+              distributed=False, nnodes=1, nproc_per_node=1, hosts=None, hostfile=None,
+              shared_dir=None, temp_dir=None):
         """
             Trains the model using the specified image classification dataset. The first time training is called, it
             will get the model from torchvision and add on a fully-connected dense layer with linear activation
             based on the number of classes in the specified dataset. The model and optimizer are defined and trained
             for the specified number of epochs.
 
             Args:
@@ -312,24 +265,28 @@
                 early_stopping (bool): Enable early stopping if convergence is reached while training
                     at the end of each epoch.
                 lr_decay (bool): If lr_decay is True and do_eval is True, learning rate decay on the validation loss
                     is applied at the end of each epoch.
                 seed (int): Optionally set a seed for reproducibility.
                 ipex_optimize (bool): Use Intel Extension for PyTorch (IPEX). Defaults to False.
                 distributed (bool): Boolean flag to use distributed training. Defaults to False.
-                hostfile (str): Name of the hostfile for distributed training. Defaults to None.
                 nnodes (int): Number of nodes to use for distributed training. Defaults to 1.
                 nproc_per_node (int): Number of processes to spawn per node to use for distributed training. Defaults
                 to 1.
+                hosts (str): hosts list for distributed training. Defaults to None.
+                hostfile (str): Name of the hostfile for distributed training. Defaults to None.
+                shared_dir (str): The shared data dir for distributed training.
+                temp_dir (str): The temp data dir at local.
 
             Returns:
                 Trained PyTorch model object
         """
-        self._check_train_inputs(output_dir, dataset, ImageClassificationDataset, epochs, initial_checkpoints,
-                                 distributed, hostfile)
+        self._check_train_inputs(
+            output_dir, dataset, ImageClassificationDataset,
+            epochs, initial_checkpoints)
 
         dataset_num_classes = len(dataset.class_names)
 
         # Check that the number of classes matches the model outputs
         if dataset_num_classes != self.num_classes:
             raise RuntimeError("The number of model outputs ({}) differs from the number of dataset classes ({})".
                                format(self.num_classes, dataset_num_classes))
@@ -340,19 +297,21 @@
 
         if initial_checkpoints:
             checkpoint = torch.load(initial_checkpoints)
             self._model.load_state_dict(checkpoint['model_state_dict'])
             self._optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
 
         if distributed:
-            # TODO: for distributed
-            # self.export_for_distributed(TLT_DISTRIBUTED_DIR, dataset)
+            objects_path = self.save_objects(
+                dataset, shared_dir, temp_dir)
             batch_size = dataset._preprocessed['batch_size']
-            self._fit_distributed(hostfile, nnodes, nproc_per_node, epochs, batch_size, ipex_optimize)
-
+            self._fit_distributed(
+                nnodes, nproc_per_node, hosts, hostfile,
+                epochs, batch_size, ipex_optimize,
+                objects_path)
         else:
             # Call ipex.optimize
             if ipex_optimize:
                 import intel_extension_for_pytorch as ipex
                 self._model, self._optimizer = ipex.optimize(self._model, optimizer=self._optimizer)
             self._fit(output_dir, dataset, epochs, do_eval, early_stopping, lr_decay)
 
@@ -463,21 +422,22 @@
             torch.save(model_copy, os.path.join(saved_model_dir, 'model.pt'))
             print("Saved model directory:", saved_model_dir)
 
             return saved_model_dir
         else:
             raise ValueError("Unable to export the model, because it hasn't been trained yet")
 
-    def write_inc_config_file(self, config_file_path, dataset, batch_size, overwrite=False,
-                              resize_interpolation='bicubic', accuracy_criterion_relative=0.01, exit_policy_timeout=0,
-                              exit_policy_max_trials=50, tuning_random_seed=9527,
-                              tuning_workspace=''):
+    def export_neural_compressor_config(
+            self, config_file_path, dataset, batch_size, overwrite=False,
+            resize_interpolation='bicubic', accuracy_criterion_relative=0.01, exit_policy_timeout=0,
+            exit_policy_max_trials=50, tuning_random_seed=9527,
+            tuning_workspace=''):
         """
-        Writes an INC compatible config file to the specified path usings args from the specified dataset and
-        parameters.
+        Writes a neural compressor compatible config file to the specified path
+        using args from the specified dataset and parameters.
 
         Args:
             config_file_path (str): Destination path on where to write the .yaml config file.
             dataset (Dataset): A dataset object
             batch_size (int): Batch size to use for quantization and evaluation
             overwrite (bool): Specify whether or not to overwrite the config_file_path, if it already exists
                               (default: False)
@@ -486,29 +446,31 @@
             accuracy_criterion_relative (float): Relative accuracy loss (default: 0.01, which is 1%)
             exit_policy_timeout (int): Tuning timeout in seconds (default: 0). Tuning processing finishes when the
                                        timeout or max_trials is reached. A tuning timeout of 0 means that the tuning
                                        phase stops when the accuracy criterion is met.
             exit_policy_max_trials (int): Maximum number of tuning trials (default: 50). Tuning processing finishes when
                                           the timeout or or max_trials is reached.
             tuning_random_seed (int): Random seed for deterministic tuning (default: 9527).
-            tuning_workspace (dir): Path the INC nc_workspace folder. If the string is empty and the OUTPUT_DIR env var
-                                    is set, that output directory will be used. If the string is empty and the
-                                    OUTPUT_DIR env var is not set, the default INC nc_workspace location will be used.
+            tuning_workspace (dir): Path the neural compressor nc_workspace folder.
+                                    If the string is empty and the OUTPUT_DIR env var is set,
+                                    that output directory will be used.
+                                    If the string is empty and the OUTPUT_DIR env var is not set,
+                                    the default neural compressor nc_workspace location will be used.
         Returns:
             None
         Raises:
             FileExistsError if the config file already exists and overwrite is set to False.
             ValueError if the parameters are not within the expected values
             NotImplementedError if the dataset type is not TFCustomImageClassificationDataset.
         """
         if os.path.isfile(config_file_path) and not overwrite:
             raise FileExistsError('A file already exists at: {}. Provide a new file path or set overwrite=True',
                                   config_file_path)
 
-        # We can setup the a custom dataset to use the ImageFolder dataset option in INC.
+        # We can setup the a custom dataset to use the ImageFolder dataset option in neural compressor.
         # They don't have a PyTorch Dataset option, so for now, we only support custom datasets for quantization
         if dataset is not PyTorchImageClassificationDataset \
                 and type(dataset) != PyTorchImageClassificationDataset:
             raise NotImplementedError('quantization has only been implemented for PyTorch image classification models '
                                       'with custom datasets')
 
         if batch_size and not isinstance(batch_size, int) or batch_size < 1:
@@ -534,16 +496,16 @@
         if tuning_random_seed and not isinstance(tuning_random_seed, int) or tuning_random_seed < 0:
             raise ValueError('Invalid value for tuning random seed ({}). Expected a positive integer.'.
                              format(tuning_random_seed))
 
         if not isinstance(tuning_workspace, str):
             raise ValueError('Invalid value for the nc_workspace directory. Expected a string.')
 
-        # Get the image recognition Intel Neural Compressor template
-        config_template = ImageClassificationModel.get_inc_config_template_dict(self)
+        # Get the image recognition Neural Compressor template
+        config_template = ImageClassificationModel.get_neural_compressor_config_template(self)
 
         # Collect the different data loaders into a list, so that we can update them all the with the data transforms
         dataloader_configs = []
 
         # If tuning_workspace is undefined, use the OUTPUT_DIR, if the env var exists
         if not tuning_workspace:
             output_dir_env_var = os.getenv('OUTPUT_DIR', '')
@@ -631,21 +593,21 @@
 
         # Write the config file
         with open(config_file_path, "w") as config_file:
             yaml.dump(config_template, config_file, sort_keys=False)
 
     def quantize(self, saved_model_dir, output_dir, inc_config_path):
         """
-        Performs post training quantization using the Intel Neural Compressor on the model from the saved_model_dir
+        Performs post training quantization using the Neural Compressor on the model from the saved_model_dir
         using the specified config file. The quantized model is written to the output directory.
 
         Args:
             saved_model_dir (str): Source directory for the model to quantize.
             output_dir (str): Writable output directory to save the quantized model
-            inc_config_path (str): Path to an INC config file (.yaml)
+            inc_config_path (str): Path to a neural compressor config file (.yaml)
 
         Returns:
             None
 
         Raises:
             NotADirectoryError if the model is not a directory
             FileNotFoundError if a model.pt is not found in the model or if the inc_config_path file
@@ -680,22 +642,22 @@
             quantized_model.save(output_dir)
             import subprocess
             # Change the model filename from best_model.pt to model.pt to match our convention
             p = subprocess.Popen(["mv", output_dir + "/best_model.pt", output_dir + "/model.pt"],
                                  stdout=subprocess.PIPE)
             stdout, stderr = p.communicate()
 
-    def benchmark(self, saved_model_dir, inc_config_path, mode='performance', model_type='fp32'):
+    def benchmark_neural_compressor(self, saved_model_dir, inc_config_path, mode='performance', model_type='fp32'):
         """
-        Use INC to benchmark the specified model for performance or accuracy. You must specify whether the
+        Use neural compressor to benchmark the specified model for performance or accuracy. You must specify whether the
         input model is fp32 or int8. IPEX int8 models are not supported yet.
 
         Args:
             saved_model_dir (str): Path to the directory where the saved model is located
-            inc_config_path (str): Path to an INC config file (.yaml)
+            inc_config_path (str): Path to a neural compressor config file (.yaml)
             mode (str): Performance or accuracy (defaults to performance)
             model_type (str): Floating point (fp32) or quantized integer (int8) model type
         Returns:
             None
         Raises:
             NotADirectoryError if the saved_model_dir is not a directory
             FileNotFoundError if a model.pt is not found in the saved_model_dir or if the inc_config_path file
@@ -725,32 +687,8 @@
         elif model_type == "int8":
             try:
                 from neural_compressor.utils.pytorch import load
                 evaluator = Benchmark(inc_config_path)
                 evaluator.model = common.Model(load(os.path.join(saved_model_dir, 'model.pt'), self._model))
                 return evaluator(mode)
             except AssertionError:
-                raise NotImplementedError("This model type is not yet supported by INC benchmarking")
-
-    def export_for_distributed(self, output_dir, dataset):
-        """
-        Helper function to export dataset and model objects to disk for distributed job
-
-        Args:
-            output_dir (str): Path to a directory where the dataset and model objects are saved.
-                Default file name for saving the objects is "torch_saved_objects.obj"
-            dataset (ImageClassificationDataset): Dataset object to save. It must be an object of
-                ImageClassificationDataset so that the dataset info, train, test, and validation
-                subsets can be accessed.
-        """
-
-        objects_to_save = {
-            "dataset": dataset.dataset,
-            "info": dataset.info,
-            "train_subset": dataset.train_subset,
-            "test_subset": dataset.test_subset,
-            "validation_subset": dataset.validation_subset,
-            "model": self._model,
-            "optimizer": self._optimizer,
-            "loss": self._loss
-        }
-        torch.save(objects_to_save, os.path.join(output_dir, "torch_saved_objects.obj"))
+                raise NotImplementedError("This model type is not yet supported by neural compressor benchmarking")
```

## cloudtik/runtime/ai/modeling/transfer_learning/image_classification/pytorch/torchvision/image_classification_model.py

```diff
@@ -107,17 +107,20 @@
 
             if ipex_optimize and not self._distributed:
                 import intel_extension_for_pytorch as ipex
                 self._model, self._optimizer = ipex.optimize(self._model, optimizer=self._optimizer)
         self._num_classes = num_classes
         return self._model, self._optimizer
 
-    def train(self, dataset: ImageClassificationDataset, output_dir, epochs=1, initial_checkpoints=None,
-              do_eval=True, early_stopping=False, lr_decay=True, seed=None, extra_layers=None, ipex_optimize=False,
-              distributed=False, hostfile=None, nnodes=1, nproc_per_node=1):
+    def train(self, dataset: ImageClassificationDataset, output_dir, *,
+              epochs=1, initial_checkpoints=None, do_eval=True,
+              early_stopping=False, lr_decay=True, seed=None,
+              ipex_optimize=False, extra_layers=None,
+              distributed=False, nnodes=1, nproc_per_node=1, hosts=None, hostfile=None,
+              shared_dir=None, temp_dir=None):
         """
             Trains the model using the specified image classification dataset. The first time training is called, it
             will get the model from torchvision and add on a fully-connected dense layer with linear activation
             based on the number of classes in the specified dataset. The model and optimizer are defined and trained
             for the specified number of epochs.
 
             Args:
@@ -128,31 +131,35 @@
                     latest checkpoint will be used.
                 do_eval (bool): If do_eval is True and the dataset has a validation subset, the model will be evaluated
                     at the end of each epoch.
                 early_stopping (bool): Enable early stopping if convergence is reached while training
                 lr_decay (bool): If lr_decay is True and do_eval is True, learning rate decay on the validation loss
                     is applied at the end of each epoch.
                 seed (int): Optionally set a seed for reproducibility.
+                ipex_optimize (bool): Use Intel Extension for PyTorch (IPEX). Defaults to False.
                 extra_layers (list[int]): Optionally insert additional dense layers between the base model and output
                     layer. This can help increase accuracy when fine-tuning a PyTorch model.
                     The input should be a list of integers representing the number and size of the layers,
                     for example [1024, 512] will insert two dense layers, the first with 1024 neurons and the
                     second with 512 neurons.
-                ipex_optimize (bool): Use Intel Extension for PyTorch (IPEX). Defaults to False.
                 distributed (bool): Boolean flag to use distributed training. Defaults to False.
-                hostfile (str): Name of the hostfile for distributed training. Defaults to None.
                 nnodes (int): Number of nodes to use for distributed training. Defaults to 1.
                 nproc_per_node (int): Number of processes to spawn per node to use for distributed training. Defaults
                 to 1.
+                hosts (str): hosts list for distributed training. Defaults to None.
+                hostfile (str): Name of the hostfile for distributed training. Defaults to None.
+                shared_dir (str): The shared data dir for distributed training.
+                temp_dir (str): The temp data dir at local.
 
             Returns:
                 Trained PyTorch model object
         """
-        self._check_train_inputs(output_dir, dataset, ImageClassificationDataset, epochs, initial_checkpoints,
-                                 distributed, hostfile)
+        self._check_train_inputs(
+            output_dir, dataset, ImageClassificationDataset,
+            epochs, initial_checkpoints)
 
         self._distributed = distributed
 
         if extra_layers:
             if not isinstance(extra_layers, list):
                 raise TypeError("The extra_layers parameter must be a list of ints but found {}".format(
                     type(extra_layers)))
@@ -185,18 +192,21 @@
 
             # Call ipex.optimize now, since we didn't call it from _get_hub_model()
             if ipex_optimize and not distributed:
                 import intel_extension_for_pytorch as ipex
                 self._model, self._optimizer = ipex.optimize(self._model, optimizer=self._optimizer)
 
         if distributed:
-            # TODO: for distributed
-            # self.export_for_distributed(TLT_DISTRIBUTED_DIR, dataset)
+            objects_path = self.save_objects(
+                dataset, shared_dir, temp_dir)
             batch_size = dataset._preprocessed['batch_size']
-            self._fit_distributed(hostfile, nnodes, nproc_per_node, epochs, batch_size, ipex_optimize)
+            self._fit_distributed(
+                nnodes, nproc_per_node, hosts, hostfile,
+                epochs, batch_size, ipex_optimize,
+                objects_path)
         else:
             self._model.train()
             self._fit(output_dir, dataset, epochs, do_eval, early_stopping, lr_decay)
 
         return self._history
 
     def evaluate(self, dataset: ImageClassificationDataset, use_test_set=False):
```

## cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/image_classification_model.py

```diff
@@ -64,15 +64,14 @@
 
         # set up the configurable optimizer and loss functions
         self._check_optimizer_loss(optimizer, loss)
         config = {'from_logits': True}
         config.update(kwargs)
         self._optimizer_class = optimizer if optimizer else tf.keras.optimizers.Adam
         self._opt_args = {k: v for k, v in config.items() if k in inspect.getfullargspec(self._optimizer_class).args}
-        self._optimizer = None  # This gets initialized later
         self._loss_class = loss if loss else tf.keras.losses.SparseCategoricalCrossentropy
         self._loss_args = {k: v for k, v in config.items() if k in inspect.getfullargspec(self._loss_class).args}
         self._loss = self._loss_class(**self._loss_args)
 
         if model is None:
             self._model = None
         elif isinstance(model, str):
@@ -153,69 +152,30 @@
         else:
             train_dataset = dataset.dataset
 
         validation_data = dataset.validation_subset if do_eval else None
 
         return callbacks, train_dataset, validation_data
 
-    def _fit_distributed(self, epochs, shuffle, hostfile, nnodes, nproc_per_node, use_horovod):
-        import subprocess
-        # TODO: handle distributed training
-        # distributed_vision_script = os.path.join(TLT_DISTRIBUTED_DIR, 'tensorflow', 'run_train_tf.py')
-        distributed_vision_script = os.path.join('tensorflow', 'run_train_tf.py')
-
-        if use_horovod:
-            run_cmd = 'horovodrun'
-        else:
-            run_cmd = 'mpirun'
-
-            # mpirun requires these flags to be set
-            run_cmd += ' --allow-run-as-root -bind-to none -map-by slot -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x NCCL_SOCKET_IFNAME=^lo,docker0 -mca pml ob1 -mca btl ^openib -mca btl_tcp_if_exclude lo,docker0'  # noqa: E501
-
-        hostfile_cmd = ''
-        np_cmd = ''
-        if os.path.isfile(hostfile):
-
-            hostfile_info = self._parse_hostfile(hostfile)
-            node_count = 0
-            if sum(hostfile_info['slots']) == 0:
-                for ip_addr in hostfile_info['ip_addresses']:
-                    hostfile_cmd += '{}:{},'.format(ip_addr, nproc_per_node)
-                    node_count += 1
-                    if node_count == nnodes:
-                        break
-
-            elif sum(hostfile_info['slots']) == nnodes * nproc_per_node:
-                for ip_addr, slots in zip(hostfile_info['ip_addresses'], hostfile_info['slots']):
-                    hostfile_cmd += '{}:{},'.format(ip_addr, slots)
-            else:
-                print("WARNING: nproc_per_node and slots in hostfile do not add up. Making equal slots for all nodes")
-                for ip_addr in hostfile_info['ip_addresses']:
-                    hostfile_cmd += '{}:{},'.format(ip_addr, nproc_per_node)
-
-            hostfile_cmd = hostfile_cmd[:-1]  # Remove trailing comma
-
-            # Final check to correct the `-np` flag's value
-            nprocs = nnodes * nproc_per_node
-            np_cmd = str(nprocs)
-        else:
-            raise ValueError("Error: Invalid file \'{}\'".format(hostfile))
-        script_cmd = 'python ' + distributed_vision_script
-        script_cmd += ' --use_case {}'.format('image_classification')
-        script_cmd += ' --epochs {}'.format(epochs)
-        if shuffle:
-            script_cmd += ' --shuffle'
-
-        bash_command = run_cmd.split(' ') + ['-np', np_cmd, '-H', hostfile_cmd] + script_cmd.split(' ')
-        print(' '.join(str(e) for e in bash_command))
-        subprocess.run(bash_command)
-
-    def train(self, dataset: ImageClassificationDataset, output_dir, epochs=1, initial_checkpoints=None,
-              do_eval=True, early_stopping=False, lr_decay=True, seed=None, enable_auto_mixed_precision=None,
-              shuffle_files=True, distributed=False, hostfile=None, nnodes=1, nproc_per_node=1,
+    def _fit_distributed(
+            self, epochs, shuffle,
+            nnodes, nproc_per_node, hosts, hostfile,
+            objects_path, use_horovod):
+        self.fit_distributed(
+            epochs, shuffle,
+            nnodes, nproc_per_node, hosts, hostfile,
+            objects_path, use_horovod, category="image_classification"
+        )
+
+    def train(self, dataset: ImageClassificationDataset, output_dir, *,
+              epochs=1, initial_checkpoints=None, do_eval=True,
+              early_stopping=False, lr_decay=True, seed=None,
+              enable_auto_mixed_precision=None, shuffle_files=True,
+              distributed=False, nnodes=1, nproc_per_node=1, hosts=None, hostfile=None,
+              shared_dir=None, temp_dir=None,
               **kwargs):
         """
         Trains the model using the specified image classification dataset. The model is compiled and trained for
         the specified number of epochs. If a path to initial checkpoints is provided, those weights are loaded before
         training.
 
         Args:
@@ -234,56 +194,72 @@
                 It is recommended to enable auto mixed precision training when running on platforms that support
                 bfloat16 (Intel third or fourth generation Xeon processors). If it is enabled on a platform that
                 does not support bfloat16, it can be detrimental to the training performance. If
                 enable_auto_mixed_precision is set to None, auto mixed precision will be automatically enabled when
                 running with Intel fourth generation Xeon processors, and disabled for other platforms.
             shuffle_files (bool): Boolean specifying whether to shuffle the training data before each epoch.
             seed (int): Optionally set a seed for reproducibility.
+            distributed (bool): Boolean flag to use distributed training. Defaults to False.
+            nnodes (int): Number of nodes to use for distributed training. Defaults to 1.
+            nproc_per_node (int): Number of processes to spawn per node to use for distributed training. Defaults
+            to 1.
+            hosts (str): hosts list for distributed training. Defaults to None.
+            hostfile (str): Name of the hostfile for distributed training. Defaults to None.
+            shared_dir (str): The shared data dir for distributed training.
+            temp_dir (str): The temp data dir at locals.
 
         Returns:
             History object from the model.fit() call
 
         Raises:
            FileExistsError if the output directory is a file
            TypeError if the dataset specified is not an ImageClassificationDataset
            TypeError if the output_dir parameter is not a string
            TypeError if the epochs parameter is not a integer
            TypeError if the initial_checkpoints parameter is not a string
            RuntimeError if the number of model classes is different from the number of dataset classes
         """
 
-        self._check_train_inputs(output_dir, dataset, ImageClassificationDataset, epochs, initial_checkpoints)
+        self._check_train_inputs(
+            output_dir, dataset, ImageClassificationDataset,
+            epochs, initial_checkpoints)
 
         dataset_num_classes = len(dataset.class_names)
 
         # Check that the number of classes matches the model outputs
         if dataset_num_classes != self.num_classes:
             raise RuntimeError("The number of model outputs ({}) differs from the number of dataset classes ({})".
                                format(self.num_classes, dataset_num_classes))
 
         self._set_seed(seed)
 
         # Set auto mixed precision
         self.set_auto_mixed_precision(enable_auto_mixed_precision)
 
-        callbacks, train_data, val_data = self._get_train_callbacks(dataset, output_dir, initial_checkpoints, do_eval,
-                                                                    early_stopping, lr_decay)
+        callbacks, train_data, val_data = self._get_train_callbacks(
+            dataset, output_dir, initial_checkpoints,
+            do_eval, early_stopping, lr_decay)
 
         if distributed:
+            objects_path = self.save_objects(
+                train_data, val_data,
+                shared_dir, temp_dir)
             try:
-                self.export_for_distributed(train_data, val_data)
-                self._fit_distributed(epochs, shuffle_files, hostfile, nnodes, nproc_per_node,
-                                      kwargs.get('use_horovod'))
+                self._fit_distributed(
+                    epochs, shuffle_files,
+                    nnodes, nproc_per_node, hosts, hostfile, objects_path,
+                    kwargs.get('use_horovod'))
             except Exception as err:
                 print("Error: \'{}\' occured while distributed training".format(err))
             finally:
-                self.cleanup_saved_objects_for_distributed()
+                self.cleanup_objects(objects_path)
         else:
-            history = self._model.fit(train_data, epochs=epochs, shuffle=shuffle_files, callbacks=callbacks,
-                                      validation_data=val_data)
+            history = self._model.fit(
+                train_data, epochs=epochs, shuffle=shuffle_files,
+                callbacks=callbacks, validation_data=val_data)
             self._history = history.history
             return self._history
 
     def evaluate(self, dataset: ImageClassificationDataset, use_test_set=False):
         """
         Evaluate the accuracy of the model on a dataset.
 
@@ -327,20 +303,22 @@
         if return_type == 'class':
             return np.argmax(predictions, axis=-1)
         elif return_type == 'probabilities':
             return tf.nn.softmax(predictions)
         else:
             return predictions
 
-    def write_inc_config_file(self, config_file_path, dataset, batch_size, overwrite=False,
-                              resize_interpolation='bicubic', accuracy_criterion_relative=0.01, exit_policy_timeout=0,
-                              exit_policy_max_trials=50, tuning_random_seed=9527,
-                              tuning_workspace=''):
+    def export_neural_compressor_config(
+            self, config_file_path, dataset, batch_size, overwrite=False,
+            resize_interpolation='bicubic', accuracy_criterion_relative=0.01, exit_policy_timeout=0,
+            exit_policy_max_trials=50, tuning_random_seed=9527,
+            tuning_workspace=''):
         """
-        Writes an INC compatible config file to the specified path usings args from the specified dataset and
+        Writes a neural compressor compatible config file to the specified path
+        using args from the specified dataset and
         parameters. This is currently only supported for TF custom image classification datasets.
 
         Args:
             config_file_path (str): Destination path on where to write the .yaml config file.
             dataset (Dataset): A dataset object
             batch_size (int): Batch size to use for quantization and evaluation
             overwrite (bool): Specify whether or not to overwrite the config_file_path, if it already exists
@@ -350,31 +328,36 @@
             accuracy_criterion_relative (float): Relative accuracy loss (default: 0.01, which is 1%)
             exit_policy_timeout (int): Tuning timeout in seconds (default: 0). Tuning processing finishes when the
                                        timeout or max_trials is reached. A tuning timeout of 0 means that the tuning
                                        phase stops when the accuracy criterion is met.
             exit_policy_max_trials (int): Maximum number of tuning trials (default: 50). Tuning processing finishes when
                                           the timeout or or max_trials is reached.
             tuning_random_seed (int): Random seed for deterministic tuning (default: 9527).
-            tuning_workspace (dir): Path the INC nc_workspace folder. If the string is empty and the OUTPUT_DIR env var
-                                    is set, that output directory will be used. If the string is empty and the
+            tuning_workspace (dir): Path the neural compressor nc_workspace folder.
+                                    If the string is empty and the OUTPUT_DIR env var is set,
+                                    that output directory will be used.
+                                    If the string is empty and the OUTPUT_DIR env var is not set,
+                                    the default neural compressor nc_workspace location will be used.
         Returns:
             None
 
         Raises:
             FileExistsError if the config file already exists and overwrite is set to False.
             ValueError if the parameters are not within the expected values
             NotImplementedError if the dataset type is not TFCustomImageClassificationDataset.
         """
         if os.path.isfile(config_file_path) and not overwrite:
             raise FileExistsError('A file already exists at: {}. Provide a new file path or set overwrite=True',
                                   config_file_path)
 
-        # We can setup the a custom dataset to use the ImageFolder dataset option in INC. They don't have a TFDS option,
+        # We can set up the custom dataset to use the ImageFolder dataset option in neural compressor.
+        # They don't have a TFDS option,
         # so for now, we only support custom datasets for quantization
-        if dataset is not TensorflowImageClassificationDataset and type(dataset) != TensorflowImageClassificationDataset:
+        if dataset is not TensorflowImageClassificationDataset and type(
+                dataset) != TensorflowImageClassificationDataset:
             raise NotImplementedError('Quantization has only been implemented for TF image classification models '
                                       'with custom datasets')
 
         if batch_size and not isinstance(batch_size, int) or batch_size < 1:
             raise ValueError('Invalid value for batch size ({}). Expected a positive integer.'.format(batch_size))
 
         if resize_interpolation not in ['bilinear', 'nearest', 'bicubic']:
@@ -397,16 +380,16 @@
         if tuning_random_seed and not isinstance(tuning_random_seed, int) or tuning_random_seed < 0:
             raise ValueError('Invalid value for tuning random seed ({}). Expected a positive integer.'.
                              format(tuning_random_seed))
 
         if not isinstance(tuning_workspace, str):
             raise ValueError('Invalid value for the nc_workspace directory. Expected a string.')
 
-        # Get the image recognition Intel Neural Compressor template
-        config_template = ImageClassificationModel.get_inc_config_template_dict(self)
+        # Get the image recognition Neural Compressor template
+        config_template = ImageClassificationModel.get_neural_compressor_config_template(self)
 
         # Collect the different data loaders into a list, so that we can update them all the with the data transforms
         dataloader_configs = []
 
         # If tuning_workspace is undefined, use the OUTPUT_DIR, if the env var exists
         if not tuning_workspace:
             output_dir_env_var = os.getenv('OUTPUT_DIR', '')
@@ -484,17 +467,17 @@
         if not os.path.exists(os.path.dirname(config_file_path)):
             os.makedirs(os.path.dirname(config_file_path))
 
         # Write the config file
         with open(config_file_path, "w") as config_file:
             yaml.dump(config_template, config_file)
 
-    def optimize_graph(self, saved_model_dir, output_dir):
+    def optimize_network(self, saved_model_dir, output_dir):
         """
-        Performs FP32 graph optimization using the Intel Neural Compressor on the model in the saved_model_dir
+        Performs FP32 graph optimization using the Neural Compressor on the model in the saved_model_dir
         and writes the inference-optimized model to the output_dir. Graph optimization includes converting
         variables to constants, removing training-only operations like checkpoint saving, stripping out parts
         of the graph that are never reached, removing debug operations like CheckNumerics, folding batch
         normalization ops into the pre-calculated weights, and fusing common operations into unified versions.
 
         Args:
             saved_model_dir (str): Source directory for the model to optimize
@@ -530,21 +513,21 @@
 
         # If optimization was successful, save the model
         if optimized_graph:
             optimized_graph.save(output_dir)
 
     def quantize(self, saved_model_dir, output_dir, inc_config_path):
         """
-        Performs post training quantization using the Intel Neural Compressor on the model from the saved_model_dir
+        Performs post training quantization using the Neural Compressor on the model from the saved_model_dir
         using the specified config file. The quantized model is written to the output directory
 
         Args:
             saved_model_dir (str): Source directory for the model to quantize.
             output_dir (str): Writable output directory to save the quantized model
-            inc_config_path (str): Path to an INC config file (.yaml)
+            inc_config_path (str): Path to a neural compressor config file (.yaml)
 
         Returns:
             None
 
         Raises:
             NotADirectoryError if the saved_model_dir is not a directory
             FileNotFoundError if a saved_model.pb is not found in the saved_model_dir or if the inc_config_path file
@@ -575,21 +558,21 @@
         quantizer.model = saved_model_dir
         quantized_model = quantizer.fit()
 
         # If quantization was successful, save the model
         if quantized_model:
             quantized_model.save(output_dir)
 
-    def benchmark(self, saved_model_dir, inc_config_path, mode='performance'):
+    def benchmark_neural_compressor(self, saved_model_dir, inc_config_path, mode='performance'):
         """
-        Use INC to benchmark the specified model for performance or accuracy.
+        Use neural compressor to benchmark the specified model for performance or accuracy.
 
         Args:
             saved_model_dir (str): Path to the directory where the saved model is located
-            inc_config_path (str): Path to an INC config file (.yaml)
+            inc_config_path (str): Path to a neural compressor config file (.yaml)
             mode (str): performance or accuracy (defaults to performance)
 
         Returns:
             None
 
         Raises:
             NotADirectoryError if the saved_model_dir is not a directory
```

## cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/tf_hub/image_classification_model.py

```diff
@@ -97,18 +97,21 @@
             self._model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))
 
             self._model.summary(print_fn=print)
 
         self._num_classes = num_classes
         return self._model
 
-    def train(self, dataset: ImageClassificationDataset, output_dir, epochs=1, initial_checkpoints=None,
-              do_eval=True, early_stopping=False, lr_decay=True, seed=None, enable_auto_mixed_precision=None,
-              shuffle_files=True, extra_layers=None, distributed=False, hostfile=None,
-              nnodes=1, nproc_per_node=1, **kwargs):
+    def train(self, dataset: ImageClassificationDataset, output_dir, *,
+              epochs=1, initial_checkpoints=None, do_eval=True,
+              early_stopping=False, lr_decay=True, seed=None,
+              enable_auto_mixed_precision=None, shuffle_files=True, extra_layers=None,
+              distributed=False, nnodes=1, nproc_per_node=1, hosts=None, hostfile=None,
+              shared_dir=None, temp_dir=None,
+              **kwargs):
         """
             Trains the model using the specified image classification dataset. The first time training is called, it
             will get the feature extractor layer from TF Hub and add on a dense layer based on the number of classes
             in the specified dataset. The model is compiled and trained for the specified number of epochs. If a
             path to initial checkpoints is provided, those weights are loaded before training.
 
             Args:
@@ -131,28 +134,37 @@
                     running with Intel fourth generation Xeon processors, and disabled for other platforms.
                 shuffle_files (bool): Boolean specifying whether to shuffle the training data before each epoch.
                 seed (int): Optionally set a seed for reproducibility.
                 extra_layers (list[int]): Optionally insert additional dense layers between the base model and output
                     layer. This can help increase accuracy when fine-tuning a TFHub model. The input should be a list of
                     integers representing the number and size of the layers, for example [1024, 512] will insert two
                     dense layers, the first with 1024 neurons and the second with 512 neurons.
-
+                distributed (bool): Boolean flag to use distributed training. Defaults to False.
+                nnodes (int): Number of nodes to use for distributed training. Defaults to 1.
+                nproc_per_node (int): Number of processes to spawn per node to use for distributed training. Defaults
+                to 1.
+                hosts (str): hosts list for distributed training. Defaults to None.
+                hostfile (str): Name of the hostfile for distributed training. Defaults to None.
+                shared_dir (str): The shared data dir for distributed training.
+                temp_dir (str): The temp data dir at local.
             Returns:
                 History object from the model.fit() call
 
             Raises:
                FileExistsError if the output directory is a file
                TypeError if the dataset specified is not an ImageClassificationDataset
                TypeError if the output_dir parameter is not a string
                TypeError if the epochs parameter is not a integer
                TypeError if the initial_checkpoints parameter is not a string
                TypeError if the extra_layers parameter is not a list of integers
         """
 
-        self._check_train_inputs(output_dir, dataset, ImageClassificationDataset, epochs, initial_checkpoints)
+        self._check_train_inputs(
+            output_dir, dataset, ImageClassificationDataset,
+            epochs, initial_checkpoints)
 
         if extra_layers:
             if not isinstance(extra_layers, list):
                 raise TypeError("The extra_layers parameter must be a list of ints but found {}".format(
                     type(extra_layers)))
             else:
                 for layer in extra_layers:
@@ -168,24 +180,31 @@
         self._set_seed(seed)
 
         # Set auto mixed precision
         self.set_auto_mixed_precision(enable_auto_mixed_precision)
 
         self._model = self._get_hub_model(dataset_num_classes, extra_layers)
 
-        callbacks, train_data, val_data = self._get_train_callbacks(dataset, output_dir, initial_checkpoints, do_eval,
-                                                                    early_stopping, lr_decay)
+        callbacks, train_data, val_data = self._get_train_callbacks(
+            dataset, output_dir, initial_checkpoints,
+            do_eval, early_stopping, lr_decay)
 
         if distributed:
-            self.export_for_distributed(train_data, val_data)
-            self._fit_distributed(epochs, shuffle_files, hostfile, nnodes, nproc_per_node, kwargs.get('use_horovod'))
-            self.cleanup_saved_objects_for_distributed()
+            objects_path = self.save_objects(
+                train_data, val_data,
+                shared_dir, temp_dir)
+            self._fit_distributed(
+                epochs, shuffle_files,
+                nnodes, nproc_per_node, hosts, hostfile,
+                objects_path, kwargs.get('use_horovod'))
+            self.cleanup_objects(objects_path)
         else:
-            history = self._model.fit(train_data, epochs=epochs, shuffle=shuffle_files, callbacks=callbacks,
-                                      validation_data=val_data)
+            history = self._model.fit(
+                train_data, epochs=epochs, shuffle=shuffle_files,
+                callbacks=callbacks, validation_data=val_data)
             self._history = history.history
             return self._history
 
     def evaluate(self, dataset: ImageClassificationDataset, use_test_set=False):
         """
         Evaluate the accuracy of the model on a dataset.
```

## cloudtik/runtime/ai/modeling/transfer_learning/text_classification/text_classification_model.py

```diff
@@ -33,50 +33,38 @@
     def __init__(self, model_name: str, dropout_layer_rate: float):
         self._dropout_layer_rate = dropout_layer_rate
         PretrainedModel.__init__(self, model_name)
 
         # Default learning rate for text models
         self._learning_rate = 3e-5
 
-    def get_inc_config_template_dict(self):
+    def get_neural_compressor_config_template(self):
         """
-        Returns a dictionary for a config template compatible with the Intel Neural Compressor.
+        Returns a dictionary for a config template compatible with the Neural Compressor.
 
         It loads the yaml file text_classification_template.yaml and then fills in parameters
         that the model knows about (like framework and model name). There are still more parameters that need to be
-        filled in before using the config with INC (like the dataset information, size, etc).
+        filled in before using the config with Neural Compressor (like the dataset information, size, etc).
         """
         this_dir = os.path.dirname(__file__)
         template_file_path = os.path.join(this_dir, "text_classification_template.yaml")
 
         if not os.path.exists(template_file_path):
             raise FileNotFoundError("Unable to find the config template at:", template_file_path)
 
         with open(template_file_path, 'r') as template_yaml:
             config_template = yaml.safe_load(template_yaml)
 
         # Update parameters that we know in the template
         config_template["model"]["framework"] = str(self.framework)
-        config_template["model"]["name"] = self.model_name65
+        config_template["model"]["name"] = self.model_name
 
         return config_template
 
     @property
     @abc.abstractmethod
     def num_classes(self):
         pass
 
     @property
     def dropout_layer_rate(self):
         return self._dropout_layer_rate
-
-    def write_inc_config_file(self, config_file_path, dataset, batch_size, overwrite=False, **kwargs):
-        raise NotImplementedError("Writing INC config files has not be implemented yet for text classification")
-
-    def quantize(self, saved_model_dir, output_dir, inc_config_path):
-        raise NotImplementedError("Post training quantization has not been implemented yet for text classification")
-
-    def optimize_graph(self, saved_model_dir, output_dir):
-        raise NotImplementedError("Optimize graph has not been implemented yet for text classification")
-
-    def benchmark(self, saved_model_dir, inc_config_path, mode='performance'):
-        raise NotImplementedError("Benchmarking has not been implemented yet for text classification")
```

## cloudtik/runtime/ai/modeling/transfer_learning/text_classification/pytorch/hugging_face/text_classification_dataset.py

```diff
@@ -35,15 +35,15 @@
 class HuggingFaceTextClassificationDataset(TextClassificationDataset, HuggingFaceDataset):
     """
     A text classification dataset from the Hugging Face datasets catalog
     """
 
     def __init__(self, dataset_dir, dataset_name, split=['train'],
                  num_workers=0, shuffle_files=True,
-                 distributed=False):
+                 distributed=False, in_memory=None):
         if not isinstance(split, list):
             raise ValueError("Value of split argument must be a list.")
 
         TextClassificationDataset.__init__(self, dataset_dir, dataset_name)
         self._preprocessed = {}
         self._split = split
         self._data_loader = None
@@ -52,14 +52,15 @@
         self._validation_loader = None
         self._train_subset = None
         self._test_subset = None
         self._validation_subset = None
         self._num_workers = num_workers
         self._shuffle = shuffle_files
         self._distributed = distributed
+        self._in_memory = in_memory
         self._info = {
             'name': dataset_name
         }
 
         if len(split) == 1:
             self._validation_type = None  # Train & evaluate on the whole dataset
 
@@ -124,18 +125,20 @@
         # We separate the dataset_name by checking whether it has the format of "dataset/subset"
         if '/' in dataset_name:
             main_dataset = dataset_name.split('/')[0]
             subset = dataset_name.split('/')[1]
 
         if subset is not None:
             downloader = DataDownloader(
-                main_dataset, self._dataset_dir, source='hugging_face', subset=subset)
+                main_dataset, self._dataset_dir, source='hugging_face',
+                subset=subset, in_memory=self._in_memory)
         else:
             downloader = DataDownloader(
-                main_dataset, self._dataset_dir, source='hugging_face')
+                main_dataset, self._dataset_dir, source='hugging_face',
+                in_memory=self._in_memory)
         return downloader.download(split=split)
 
     @property
     def dataset(self) -> Dataset:
         """
         Returns datasets.arrow_dataset.Dataset object
         """
```

## cloudtik/runtime/ai/modeling/transfer_learning/text_classification/pytorch/hugging_face/text_classification_model.py

```diff
@@ -13,19 +13,17 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 # SPDX-License-Identifier: Apache-2.0
 #
-
 import inspect
 import os
 import time
-import subprocess
 import yaml
 import dill
 import numpy as np
 from requests.adapters import ProxyError
 
 from tqdm import tqdm
 import torch
@@ -85,50 +83,24 @@
         TextClassificationModel.__init__(self, model_name, self._dropout_layer_rate)
         HuggingFaceModel.__init__(self, model_name)
 
         # set up the configurable optimizer and loss functions
         self._check_optimizer_loss(optimizer, loss)
         self._optimizer_class = optimizer if optimizer else torch.optim.AdamW
         self._opt_args = {k: v for k, v in kwargs.items() if k in inspect.getfullargspec(self._optimizer_class).args}
-        self._optimizer = None  # This gets initialized later
         self._loss_class = loss if loss else torch.nn.CrossEntropyLoss
         self._loss_args = {k: v for k, v in kwargs.items() if k in inspect.getfullargspec(self._loss_class).args}
         self._loss = self._loss_class(**self._loss_args)
 
         # model definition
         self.hub_name = model_map[self._model_name]["hub_name"]
-        self._model = None
         self._num_classes = None
         self._trainer = None
         self._history = None
 
-    def export_for_distributed(self, output_dir, dataset):
-        """
-        Helper function to export dataset and model objects to disk for distributed job
-
-        Args:
-            output_dir (str): Path to a directory where the dataset and model objects are saved.
-                Default file name for saving the objects is "hf_saved_objects.obj"
-            dataset (HuggingFaceTextClassificationDataset): Dataset object to save. It must be an object of
-                HuggingFaceTextClassificationDataset so that the dataset info, train, test, and validation
-                subsets can be accessed.
-        """
-
-        objects_to_save = {
-            "dataset": dataset.dataset,
-            "info": dataset.info,
-            "train_subset": dataset.train_subset,
-            "test_subset": dataset.test_subset,
-            "validation_subset": dataset.validation_subset,
-            "model": self._model,
-            "optimizer": self._optimizer,
-            "loss": self._loss
-        }
-        torch.save(objects_to_save, os.path.join(output_dir, "hf_saved_objects.obj"))
-
     @property
     def num_classes(self):
         """
         The number of output neurons in the model; equal to the number of classes in the dataset
         """
         return self._num_classes
 
@@ -269,88 +241,44 @@
                 torch.save({
                     'epoch': epochs,
                     'model_state_dict': self._model.state_dict(),
                     'optimizer_state_dict': self._optimizer.state_dict(),
                     'loss': train_epoch_loss,
                 }, os.path.join(checkpoint_dir, 'checkpoint.pt'))
 
-    def _fit_distributed(self, hostfile, nnodes, nproc_per_node, epochs, batch_size, ipex_optimize):
-        # TODO: handle distributed training
-        # distributed_text_script = os.path.join(TLT_DISTRIBUTED_DIR, "run_train_pyt.py")
-        distributed_text_script = "run_train_pyt.py"
-
-        default_port = '29500'
-        default_master_addr = '127.0.0.1'
-
-        addresses = []
-
-        if hostfile is not None:
-            if os.path.isfile(hostfile):
-                # if addresses are given as line separated IP addresses
-                with open(hostfile) as hf:
-                    addresses = hf.readlines()
-                addresses = [a.strip('\n') for a in addresses]
-            else:
-                # if addresses are given as a comma separated IP addresses
-                addresses = hostfile.split(',')
-
-            default_master_addr = addresses[0]
-
-            # If port is given in the format of "0.0.0.0:9999"
-            if ':' in default_master_addr:
-                colon_index = default_master_addr.index(':')
-                default_port = default_master_addr[colon_index + 1:]
-                default_master_addr = default_master_addr[:colon_index]
-
-                # We create/rewrite the hostfile to contain only IP addresses
-                with open('hostfile', 'w') as hf:
-                    for addr in addresses:
-                        if ':' in addr:
-                            addr = addr[:addr.index(':')]
-                        hf.write(addr + '\n')
-                hostfile = 'hostfile'
-
-        bash_command = 'python -m intel_extension_for_pytorch.cpu.launch --distributed'
-        bash_command += ' --hostfile {}'.format(hostfile)
-        bash_command += ' --nnodes {}'.format(nnodes)
-        bash_command += ' --nproc_per_node {}'.format(nproc_per_node)
-        bash_command += ' {}'.format(distributed_text_script)
-        bash_command += ' --master_addr {}'.format(default_master_addr)
-        bash_command += ' --master_port {}'.format(default_port)
-        bash_command += ' --backend {}'.format('ccl')
-        bash_command += ' --use_case {}'.format('text_classification')
-        bash_command += ' --epochs {}'.format(epochs)
-        bash_command += ' --batch_size {}'.format(batch_size)
-        if not ipex_optimize:
-            bash_command += ' --disable_ipex'
-
-        print(bash_command)
-        subprocess.run(bash_command.split(' '))
+    def _fit_distributed(
+            self, nnodes, nproc_per_node, hosts, hostfile,
+            epochs, batch_size, ipex_optimize, objects_path):
+        self.fit_distributed(
+            nnodes, nproc_per_node, hosts, hostfile,
+            epochs, batch_size, ipex_optimize,
+            objects_path, category="text_classification"
+        )
 
     def train(
-        self,
-        dataset,
-        output_dir: str,
-        epochs: int = 1,
-        initial_checkpoints=None,
-        do_eval: bool = True,
-        early_stopping: bool = False,
-        lr_decay: bool = True,
-        seed: int = None,
-        learning_rate: float = 1e-5,
-        extra_layers: list = None,
-        device: str = "cpu",
-        ipex_optimize: bool = False,
-        use_trainer: bool = False,
-        force_download: bool = False,
-        distributed: bool = False,
-        hostfile: str = None,
-        nnodes: int = 1,
-        nproc_per_node: int = 1,
-        **kwargs
+            self,
+            dataset, output_dir: str, *,
+            epochs: int = 1, initial_checkpoints=None, do_eval: bool = True,
+            early_stopping: bool = False,
+            lr_decay: bool = True,
+            seed: int = None,
+            ipex_optimize: bool = False,
+            extra_layers: list = None,
+            learning_rate: float = 1e-5,
+            device: str = "cpu",
+            use_trainer: bool = False,
+            force_download: bool = False,
+            distributed=False,
+            nnodes=1,
+            nproc_per_node=1,
+            hosts=None,
+            hostfile=None,
+            shared_dir=None,
+            temp_dir=None,
+            **kwargs
     ):
         """
         Trains the model using the specified text classification dataset.
 
         Args:
             dataset (TextClassificationDataset/datasets.arrow_dataset.Dataset): The dataset to use for training.
                 If a train subset has been defined, that subset will be used to fit the model. Otherwise, the
@@ -359,44 +287,48 @@
             epochs (int): The number of training epochs [default: 1]
             initial_checkpoints (str): Path to checkpoint weights to load. If the path provided is a directory, the
                 latest checkpoint will be used.
             do_eval (bool): If do_eval is True and the dataset has a validation subset, the model will be evaluated
                 at the end of each epoch.
             early_stopping (bool): Enable early stopping if convergence is reached while training
                 at the end of each epoch.
-            learning_rate (float): Learning rate for the model to train. Defaults to 1e-5
             lr_decay (bool): If lr_decay is True and do_eval is True, learning rate decay on the validation loss
                 is applied at the end of each epoch.
             seed (int): Optionally set a seed for reproducibility.
+            ipex_optimize (bool): Optimize the model using Intel Extension for PyTorch. Defaults to False
             extra_layers (list[int]): Optionally insert additional dense layers between the base model and output
                 layer. This can help increase accuracy when fine-tuning a PyTorch model.
                 The input should be a list of integers representing the number and size of the layers,
                 for example [1024, 512] will insert two dense layers, the first with 1024 neurons and the
                 second with 512 neurons.
+            learning_rate (float): Learning rate for the model to train. Defaults to 1e-5
             device (str): Device to train the model. Defaults to "cpu"
-            ipex_optimize (bool): Optimize the model using Intel Extension for PyTorch. Defaults to False
             use_trainer (bool): If use_trainer is True, then the model training is done using the Hugging Face Trainer
                 and if use_trainer is False, the model training is done using native PyTorch training loop
             force_download (bool): Downloads the model with default parameters. Defaults to False.
             distributed (bool): Boolean flag to use distributed training. Defaults to False.
-            hostfile (str): Name of the hostfile for distributed training. Defaults to None.
             nnodes (int): Number of nodes to use for distributed training. Defaults to 1.
             nproc_per_node (int): Number of processes to spawn per node to use for distributed training. Defaults
-                to 1.
+            to 1.
+            hosts (str): hosts list for distributed training. Defaults to None.
+            hostfile (str): Name of the hostfile for distributed training. Defaults to None.
+            shared_dir (str): The shared data dir for distributed training.
+            temp_dir (str): The temp data dir at local.
 
         Returns:
             Dictionary containing the model training history
 
         Raises:
             TypeError if the dataset specified is not a TextClassificationDataset/datasets.arrow_dataset.Dataset
             ValueError if the given dataset has not been preprocessed yet
 
         """
-        self._check_train_inputs(output_dir, dataset, TextClassificationDataset,
-                                 extra_layers, epochs, distributed, hostfile)
+        self._check_train_inputs(
+            output_dir, dataset, TextClassificationDataset,
+            epochs, extra_layers)
 
         if not self._model:
             self._num_classes = len(dataset.class_names)
             downloader = ModelDownloader(self.hub_name, model_dir=None, source='hugging_face',
                                          num_labels=self._num_classes, force_download=force_download)
             try:
                 self._model = downloader.download()
@@ -467,18 +399,20 @@
             )
 
             self._trainer.train()
             if do_eval:
                 self._history = self._trainer.evaluate()
                 print("Val Acc: {:.5f}".format(self._history.get("eval_accuracy")))
         elif distributed:
-            # TODO: handle distributed training
-            # self.export_for_distributed(output_dir=TLT_DISTRIBUTED_DIR, dataset=dataset)
-            self._fit_distributed(hostfile, nnodes, nproc_per_node, epochs, dataset._preprocessed["batch_size"],
-                                  ipex_optimize)
+            objects_path = self.save_objects(
+                dataset, shared_dir, temp_dir)
+            self._fit_distributed(
+                nnodes, nproc_per_node, hosts, hostfile,
+                epochs, dataset._preprocessed["batch_size"], ipex_optimize,
+                objects_path)
         else:
             self._trainer = None
             self._model.train()
             if ipex_optimize:
                 import intel_extension_for_pytorch as ipex
                 self._model, self._optimizer = ipex.optimize(self._model, optimizer=self._optimizer)
             # Call the _fit method to train the model with native PyTorch API
@@ -634,35 +568,22 @@
             torch.save(model_copy, os.path.join(saved_model_dir, 'model.pt'))
             print("Saved model directory:", saved_model_dir)
 
             return saved_model_dir
         else:
             raise ValueError("Unable to export the model, because it hasn't been trained yet")
 
-    def load_from_directory(self, model_dir: str, num_classes: int):
+    def export_neural_compressor_config(
+            self, config_file_path, dataset, batch_size, overwrite=False,
+            resize_interpolation='bicubic', accuracy_criterion_relative=0.01, exit_policy_timeout=0,
+            exit_policy_max_trials=50, tuning_random_seed=9527,
+            tuning_workspace=''):
         """
-        Loads a saved pytorch model from the given model_dir directory
-
-        Args:
-            model_dir(str): Path to the saved model directory
-            num_classes(int): Number of class labels
-        """
-
-        verify_directory(model_dir, require_directory_exists=True)
-        model_copy = torch.load(os.path.join(model_dir, 'model.pt'))
-        self._model = dill.loads(model_copy)
-        self._optimizer = self._optimizer_class(self._model.parameters(), lr=self._learning_rate)
-
-    def write_inc_config_file(self, config_file_path, dataset, batch_size, overwrite=False,
-                              resize_interpolation='bicubic', accuracy_criterion_relative=0.01, exit_policy_timeout=0,
-                              exit_policy_max_trials=50, tuning_random_seed=9527,
-                              tuning_workspace=''):
-        """
-        Writes an INC compatible config file to the specified path usings args from the specified dataset and
-        parameters.
+        Writes a Neural Compressor compatible config file to the specified path
+        using args from the specified dataset and parameters.
 
         Args:
             config_file_path (str): Destination path on where to write the .yaml config file.
             dataset (Dataset): A dataset object
             batch_size (int): Batch size to use for quantization and evaluation
             overwrite (bool): Specify whether or not to overwrite the config_file_path, if it already exists
                               (default: False)
@@ -671,17 +592,19 @@
             accuracy_criterion_relative (float): Relative accuracy loss (default: 0.01, which is 1%)
             exit_policy_timeout (int): Tuning timeout in seconds (default: 0). Tuning processing finishes when the
                                        timeout or max_trials is reached. A tuning timeout of 0 means that the tuning
                                        phase stops when the accuracy criterion is met.
             exit_policy_max_trials (int): Maximum number of tuning trials (default: 50). Tuning processing finishes when
                                           the timeout or or max_trials is reached.
             tuning_random_seed (int): Random seed for deterministic tuning (default: 9527).
-            tuning_workspace (dir): Path the INC nc_workspace folder. If the string is empty and the OUTPUT_DIR env var
-                                    is set, that output directory will be used. If the string is empty and the
-                                    OUTPUT_DIR env var is not set, the default INC nc_workspace location will be used.
+            tuning_workspace (dir): Path the Neural Compressor nc_workspace folder.
+                                    If the string is empty and the OUTPUT_DIR env var is set,
+                                    that output directory will be used.
+                                    If the string is empty and the OUTPUT_DIR env var is not set,
+                                    the default Neural Compressor nc_workspace location will be used.
         Returns:
             None
         Raises:
             FileExistsError if the config file already exists and overwrite is set to False.
             ValueError if the parameters are not within the expected values
             NotImplementedError if the dataset type is not HFCustomImageClassificationDataset.
         """
@@ -719,16 +642,16 @@
         if tuning_random_seed and not isinstance(tuning_random_seed, int) or tuning_random_seed < 0:
             raise ValueError('Invalid value for tuning random seed ({}). Expected a positive integer.'.
                              format(tuning_random_seed))
 
         if not isinstance(tuning_workspace, str):
             raise ValueError('Invalid value for the nc_workspace directory. Expected a string.')
 
-        # Get the Intel Neural Compressor template
-        config_template = TextClassificationModel.get_inc_config_template_dict(self)
+        # Get the Neural Compressor template
+        config_template = TextClassificationModel.get_neural_compressor_config_template(self)
 
         # Collect the different data loaders into a list, so that we can update them all the with the data transforms
         dataloader_configs = []
 
         # If tuning_workspace is undefined, use the OUTPUT_DIR, if the env var exists
         if not tuning_workspace:
             output_dir_env_var = os.getenv('OUTPUT_DIR', '')
@@ -801,21 +724,21 @@
 
         # Write the config file
         with open(config_file_path, "w") as config_file:
             yaml.dump(config_template, config_file, sort_keys=False)
 
     def quantize(self, saved_model_dir, output_dir, inc_config_path):
         """
-        Performs post training quantization using the Intel Neural Compressor on the model from the saved_model_dir
+        Performs post training quantization using the Neural Compressor on the model from the saved_model_dir
         using the specified config file. The quantized model is written to the output directory.
 
         Args:
             saved_model_dir (str): Source directory for the model to quantize.
             output_dir (str): Writable output directory to save the quantized model
-            inc_config_path (str): Path to an INC config file (.yaml)
+            inc_config_path (str): Path to a Neural Compressor config file (.yaml)
 
         Returns:
             None
 
         Raises:
             NotADirectoryError if the model is not a directory
             FileNotFoundError if a model.pt is not found in the model or if the inc_config_path file
@@ -850,76 +773,7 @@
         if quantized_model:
             quantized_model.save(output_dir)
             import subprocess
             # Change the model filename from best_model.pt to model.pt to match our convention
             p = subprocess.Popen(["mv", output_dir + "/best_model.pt", output_dir + "/model.pt"],
                                  stdout=subprocess.PIPE)
             stdout, stderr = p.communicate()
-
-    def list_layers(self, verbose=False):
-        """
-        Lists all of the named modules (e.g. features, avgpool, classifier) and layers
-        (ReLU, MaxPool2d, Dropout, Linear, etc) in a given PyTorch model
-
-        Args:
-            verbose (bool): True/False option set by default to be False, displays only high-level modules
-        """
-
-        if self._model is None:
-            raise RuntimeError('The model must be trained at least one epoch before its layers can be summarized.')
-
-        # Display a high-level list of the modules e.g. features, avgpool, classifier
-        print("\nModel Layers\n============")
-        for (name, module) in self._model.named_children():
-            if not verbose or not list(module.named_children()):
-                print('{}: {}/{} parameters are trainable'.format(
-                    name, sum(p.numel() for p in module.parameters() if p.requires_grad),
-                    sum(p.numel() for p in module.parameters())))
-            else:
-                print('{}:'.format(name))
-                for (layer_name, layer) in module.named_children():
-                    print('  {}: {}/{} parameters are trainable'.format(
-                        layer_name, sum(p.numel() for p in layer.parameters() if p.requires_grad),
-                        sum(p.numel() for p in layer.parameters())))
-
-        trainable_parameters = sum(p.numel() for p in self._model.parameters() if p.requires_grad)
-        print('\nTotal Trainable Parameters: {}/{}'.format(
-            trainable_parameters,
-            sum(p.numel() for p in self._model.parameters())))
-
-        return trainable_parameters
-
-    def freeze_layer(self, layer_name):
-        """
-        Freezes the model's layer using a layer name
-        Args:
-            layer_name (string): The layer name that will be frozen in the model
-        """
-
-        if self._model is None:
-            raise RuntimeError('The model must be trained at least one epoch before its layers can be frozen.')
-
-        # Freeze everything in the layer
-        for (name, module) in self._model.named_children():
-            if name == layer_name:
-                for param in module.parameters():
-                    param.requires_grad = False
-
-        return
-
-    def unfreeze_layer(self, layer_name):
-        """
-        Unfreezes the model's layer using a layer name
-        Args:
-            layer_name (string): The layer name that will be frozen in the model
-        """
-
-        if self._model is None:
-            raise RuntimeError('The model must be trained at least one epoch before its layers can be unfrozen.')
-
-        # Unfreeze everything in the layer
-        for (name, module) in self._model.named_children():
-            if name == layer_name:
-                for param in module.parameters():
-                    param.requires_grad = True
-
-        return
```

## cloudtik/runtime/ai/modeling/transfer_learning/text_classification/tensorflow/text_classification_model.py

```diff
@@ -47,28 +47,24 @@
     """
 
     def __init__(self, model_name: str, model=None, optimizer=None, loss=None, **kwargs):
         # extra properties that should become configurable in the future
         self._dropout_layer_rate = 0.1
         self._epsilon = 1e-08
         self._generate_checkpoints = True
-
-        # placeholder for model definition
-        self._model = None
         self._num_classes = None
 
         TensorflowModel.__init__(self, model_name)
         TextClassificationModel.__init__(
             self, model_name, dropout_layer_rate=self._dropout_layer_rate)
 
         # set up the configurable optimizer and loss functions
         self._check_optimizer_loss(optimizer, loss)
         self._optimizer_class = optimizer if optimizer else tf.keras.optimizers.Adam
         self._opt_args = {k: v for k, v in kwargs.items() if k in inspect.getfullargspec(self._optimizer_class).args}
-        self._optimizer = None  # This gets initialized later
         self._loss_class = loss  # This can be None, default function is defined later
         if self._loss_class:
             self._loss_args = {k: v for k, v in kwargs.items() if k in inspect.getfullargspec(self._loss_class).args}
         else:
             self._loss_args = {}
         self._loss = None  # This gets initialized later
 
@@ -158,71 +154,30 @@
                 min_lr=0.0000000001))
 
         train_data = dataset.train_subset if dataset.train_subset else dataset.dataset
         validation_data = dataset.validation_subset if do_eval else None
 
         return callbacks, train_data, validation_data
 
-    def _fit_distributed(self, epochs, shuffle, hostfile, nnodes, nproc_per_node, use_horovod):
-        import subprocess
-        # TODO: handle distributed training
-        # distributed_vision_script = os.path.join(TLT_DISTRIBUTED_DIR, 'tensorflow', 'run_train_tf.py')
-        distributed_vision_script = os.path.join('tensorflow', 'run_train_tf.py')
-
-        if use_horovod:
-            run_cmd = 'horovodrun'
-        else:
-            run_cmd = 'mpirun'
-
-            # mpirun requires these flags to be set
-            run_cmd += ' --allow-run-as-root -bind-to none -map-by slot -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x NCCL_SOCKET_IFNAME=^lo,docker0 -mca pml ob1 -mca btl ^openib -mca btl_tcp_if_exclude lo,docker0'  # noqa: E501
-
-        hostfile_cmd = ''
-        np_cmd = ''
-        if os.path.isfile(hostfile):
-
-            hostfile_info = self._parse_hostfile(hostfile)
-            node_count = 0
-            if sum(hostfile_info['slots']) == 0:
-                for ip_addr in hostfile_info['ip_addresses']:
-                    hostfile_cmd += '{}:{},'.format(ip_addr, nproc_per_node)
-                    node_count += 1
-                    if node_count == nnodes:
-                        break
-
-            elif sum(hostfile_info['slots']) == nnodes * nproc_per_node:
-                for ip_addr, slots in zip(hostfile_info['ip_addresses'], hostfile_info['slots']):
-                    hostfile_cmd += '{}:{},'.format(ip_addr, slots)
-            else:
-                print("WARNING: nproc_per_node and slots in hostfile do not add up. Making equal slots for all nodes")
-                for ip_addr in hostfile_info['ip_addresses']:
-                    hostfile_cmd += '{}:{},'.format(ip_addr, nproc_per_node)
-
-            hostfile_cmd = hostfile_cmd[:-1]  # Remove trailing comma
-
-            # Final check to correct the `-np` flag's value
-            # nprocs = len(hostfile_info['ip_addresses']) * nproc_per_node
-            nprocs = nnodes * nproc_per_node
-            np_cmd = str(nprocs)
-        else:
-            raise ValueError("Error: Invalid file \'{}\'".format(hostfile))
-
-        script_cmd = 'python ' + distributed_vision_script
-        script_cmd += ' --use_case {}'.format('text_classification')
-        script_cmd += ' --epochs {}'.format(epochs)
-        if shuffle:
-            script_cmd += ' --shuffle'
-
-        bash_command = run_cmd.split(' ') + ['-np', np_cmd, '-H', hostfile_cmd] + script_cmd.split(' ')
-        print(' '.join(str(e) for e in bash_command))
-        subprocess.run(bash_command)
-
-    def train(self, dataset: TextClassificationDataset, output_dir, epochs=1, initial_checkpoints=None,
-              do_eval=True, early_stopping=False, lr_decay=True, seed=None, enable_auto_mixed_precision=None,
-              shuffle_files=True, distributed=False, hostfile=None, nnodes=1, nproc_per_node=1,
+    def _fit_distributed(
+            self, epochs, shuffle,
+            nnodes, nproc_per_node, hosts, hostfile,
+            objects_path, use_horovod):
+        self.fit_distributed(
+            epochs, shuffle,
+            nnodes, nproc_per_node, hosts, hostfile,
+            objects_path, use_horovod, category="text_classification"
+        )
+
+    def train(self, dataset: TextClassificationDataset, output_dir, *,
+              epochs=1, initial_checkpoints=None, do_eval=True,
+              early_stopping=False, lr_decay=True, seed=None,
+              enable_auto_mixed_precision=None, shuffle_files=True,
+              distributed=False, nnodes=1, nproc_per_node=1, hosts=None, hostfile=None,
+              shared_dir=None, temp_dir=None,
               **kwargs):
         """
            Trains the model using the specified binary text classification dataset. If a path to initial checkpoints is
            provided, those weights are loaded before training.
 
            Args:
                dataset (TextClassificationDataset): The dataset to use for training. If a train subset has been
@@ -243,54 +198,69 @@
                     It is recommended to enable auto mixed precision training when running on platforms that support
                     bfloat16 (Intel third or fourth generation Xeon processors). If it is enabled on a platform that
                     does not support bfloat16, it can be detrimental to the training performance. If
                     enable_auto_mixed_precision is set to None, auto mixed precision will be automatically enabled when
                     running with Intel fourth generation Xeon processors, and disabled for other platforms.
                shuffle_files (bool): Boolean specifying whether to shuffle the training data before each epoch.
                seed (int): Optionally set a seed for reproducibility.
-
+               distributed (bool): Boolean flag to use distributed training. Defaults to False.
+               nnodes (int): Number of nodes to use for distributed training. Defaults to 1.
+               nproc_per_node (int): Number of processes to spawn per node to use for distributed training. Defaults
+               to 1.
+               hosts (str): hosts list for distributed training. Defaults to None.
+               hostfile (str): Name of the hostfile for distributed training. Defaults to None.
+               shared_dir (str): The shared data dir for distributed training.
+               temp_dir (str): The temp data dir at local.
            Returns:
                History object from the model.fit() call
 
            Raises:
                FileExistsError if the output directory is a file
                TypeError if the dataset specified is not a TextClassificationDataset
                TypeError if the output_dir parameter is not a string
                TypeError if the epochs parameter is not a integer
                TypeError if the initial_checkpoints parameter is not a string
                NotImplementedError if the specified dataset has more than 2 classes
         """
-        self._check_train_inputs(output_dir, dataset, TextClassificationDataset, epochs, initial_checkpoints)
+        self._check_train_inputs(
+            output_dir, dataset, TextClassificationDataset,
+            epochs, initial_checkpoints)
 
         dataset_num_classes = len(dataset.class_names)
 
         if dataset_num_classes != 2:
             raise NotImplementedError("Training is only supported for binary text classification. The specified dataset"
                                       " has {} classes, but expected 2 classes.".format(dataset_num_classes))
 
         self._set_seed(seed)
 
         # Set auto mixed precision
         self.set_auto_mixed_precision(enable_auto_mixed_precision)
 
-        callbacks, train_data, val_data = self._get_train_callbacks(dataset, output_dir, initial_checkpoints, do_eval,
-                                                                    early_stopping, lr_decay, dataset_num_classes)
+        callbacks, train_data, val_data = self._get_train_callbacks(
+            dataset, output_dir, initial_checkpoints, do_eval,
+            early_stopping, lr_decay, dataset_num_classes)
 
         if distributed:
+            objects_path = self.save_objects(
+                train_data, val_data,
+                shared_dir, temp_dir)
             try:
-                self.export_for_distributed(train_data, val_data)
-                self._fit_distributed(epochs, shuffle_files, hostfile, nnodes, nproc_per_node,
-                                      kwargs.get('use_horovod'))
+                self._fit_distributed(
+                    epochs, shuffle_files,
+                    nnodes, nproc_per_node, hosts, hostfile,
+                    objects_path, kwargs.get('use_horovod'))
             except Exception as err:
                 print("Error: \'{}\' occured while distributed training".format(err))
             finally:
-                self.cleanup_saved_objects_for_distributed()
+                self.cleanup_objects(objects_path)
         else:
-            history = self._model.fit(train_data, validation_data=val_data, epochs=epochs, shuffle=shuffle_files,
-                                      callbacks=callbacks)
+            history = self._model.fit(
+                train_data, validation_data=val_data, epochs=epochs,
+                shuffle=shuffle_files, callbacks=callbacks)
 
             self._history = history.history
 
             return self._history
 
     def evaluate(self, dataset: TextClassificationDataset, use_test_set=False):
         """
@@ -348,21 +318,22 @@
 
         # If a single string is passed in, make it a list so that it's compatible with the keras model predict
         if isinstance(input_samples, str):
             input_samples = [input_samples]
 
         return tf.sigmoid(self._model.predict(input_samples)).numpy()
 
-    def write_inc_config_file(self, config_file_path, dataset, batch_size, overwrite=False,
-                              resize_interpolation='bicubic', accuracy_criterion_relative=0.01, exit_policy_timeout=0,
-                              exit_policy_max_trials=50, tuning_random_seed=9527,
-                              tuning_workspace=''):
+    def export_neural_compressor_config(
+            self, config_file_path, dataset, batch_size, overwrite=False,
+            resize_interpolation='bicubic', accuracy_criterion_relative=0.01, exit_policy_timeout=0,
+            exit_policy_max_trials=50, tuning_random_seed=9527,
+            tuning_workspace=''):
         """
-        Writes an Intel Neural Compressor compatible config file to the specified path usings args from the specified
-        dataset and parameters.
+        Writes a Neural Compressor compatible config file to the specified path
+        using args from the specified dataset and parameters.
 
         Args:
             config_file_path (str): Destination path on where to write the .yaml config file.
             dataset (Dataset): A dataset object
             batch_size (int): Batch size to use for quantization and evaluation
             overwrite (bool): Specify whether or not to overwrite the config_file_path, if it already exists
                               (default: False)
@@ -371,17 +342,17 @@
             accuracy_criterion_relative (float): Relative accuracy loss (default: 0.01, which is 1%)
             exit_policy_timeout (int): Tuning timeout in seconds (default: 0). Tuning processing finishes when the
                                        timeout or max_trials is reached. A tuning timeout of 0 means that the tuning
                                        phase stops when the accuracy criterion is met.
             exit_policy_max_trials (int): Maximum number of tuning trials (default: 50). Tuning processing finishes when
                                           the timeout or or max_trials is reached.
             tuning_random_seed (int): Random seed for deterministic tuning (default: 9527).
-            tuning_workspace (dir): Path the Intel Neural Compressor nc_workspace folder. If the string is empty and the
+            tuning_workspace (dir): Path the Neural Compressor nc_workspace folder. If the string is empty and the
                                     OUTPUT_DIR env var is set, that output directory will be used. If the string is
-                                    empty and the OUTPUT_DIR env var is not set, the default Intel Neural Compressor
+                                    empty and the OUTPUT_DIR env var is not set, the default Neural Compressor
                                     nc_workspace location will be used.
         Returns:
             None
         Raises:
             FileExistsError if the config file already exists and overwrite is set to False.
             ValueError if the parameters are not within the expected values
             NotImplementedError if the dataset type is not TFCustomImageClassificationDataset.
@@ -420,16 +391,16 @@
         if tuning_random_seed and not isinstance(tuning_random_seed, int) or tuning_random_seed < 0:
             raise ValueError('Invalid value for tuning random seed ({}). Expected a positive integer.'.
                              format(tuning_random_seed))
 
         if not isinstance(tuning_workspace, str):
             raise ValueError('Invalid value for the nc_workspace directory. Expected a string.')
 
-        # Get the Intel Neural Compressor template
-        config_template = TextClassificationModel.get_inc_config_template_dict(self)
+        # Get the Neural Compressor template
+        config_template = TextClassificationModel.get_neural_compressor_config_template(self)
 
         # Collect the different data loaders into a list, so that we can update them all the with the data transforms
         dataloader_configs = []
 
         # If tuning_workspace is undefined, use the OUTPUT_DIR, if the env var exists
         if not tuning_workspace:
             output_dir_env_var = os.getenv('OUTPUT_DIR', '')
@@ -502,21 +473,21 @@
 
         # Write the config file
         with open(config_file_path, "w") as config_file:
             yaml.dump(config_template, config_file, sort_keys=False)
 
     def quantize(self, saved_model_dir, output_dir, inc_config_path):
         """
-        Performs post training quantization using the Intel Neural Compressor on the model from the saved_model_dir
+        Performs post training quantization using the Neural Compressor on the model from the saved_model_dir
         using the specified config file. The quantized model is written to the output directory.
 
         Args:
             saved_model_dir (str): Source directory for the model to quantize.
             output_dir (str): Writable output directory to save the quantized model
-            inc_config_path (str): Path to an Intel Neural Compressor config file (.yaml)
+            inc_config_path (str): Path to a Neural Compressor config file (.yaml)
 
         Returns:
             None
 
         Raises:
             NotADirectoryError if the model is not a directory
             FileNotFoundError if a saved_model.pb is not found in the model or if the inc_config_path file
```

## cloudtik/runtime/ai/modeling/transfer_learning/text_classification/tensorflow/tf_hub/text_classification_model.py

```diff
@@ -109,18 +109,21 @@
             self._model.add(tf.keras.layers.Dense(dense_layer_dims, activation=None, name='classifier'))
 
             self._model.summary(print_fn=print)
 
         self._num_classes = num_classes
         return self._model
 
-    def train(self, dataset: TextClassificationDataset, output_dir, epochs=1, initial_checkpoints=None,
-              do_eval=True, early_stopping=False, lr_decay=True, seed=None, enable_auto_mixed_precision=None,
-              shuffle_files=True, extra_layers=None, distributed=False, hostfile=None, nnodes=1,
-              nproc_per_node=1, **kwargs):
+    def train(self, dataset: TextClassificationDataset, output_dir, *,
+              epochs=1, initial_checkpoints=None, do_eval=True,
+              early_stopping=False, lr_decay=True, seed=None,
+              enable_auto_mixed_precision=None, shuffle_files=True, extra_layers=None,
+              distributed=False, nnodes=1, nproc_per_node=1, hosts=None, hostfile=None,
+              shared_dir=None, temp_dir=None,
+              **kwargs):
         """
            Trains the model using the specified binary text classification dataset. If a path to initial checkpoints is
            provided, those weights are loaded before training.
 
            Args:
                dataset (TextClassificationDataset): The dataset to use for training. If a train subset has been
                                                     defined, that subset will be used to fit the model. Otherwise, the
@@ -144,27 +147,37 @@
                     running with Intel fourth generation Xeon processors, and disabled for other platforms.
                shuffle_files (bool): Boolean specifying whether to shuffle the training data before each epoch.
                extra_layers (list[int]): Optionally insert additional dense layers between the base model and output
                     layer. This can help increase accuracy when fine-tuning a TFHub model. The input should be a list of
                     integers representing the number and size of the layers, for example [1024, 512] will insert two
                     dense layers, the first with 1024 neurons and the second with 512 neurons.
                seed (int): Optionally set a seed for reproducibility.
+               distributed (bool): Boolean flag to use distributed training. Defaults to False.
+               nnodes (int): Number of nodes to use for distributed training. Defaults to 1.
+               nproc_per_node (int): Number of processes to spawn per node to use for distributed training. Defaults
+               to 1.
+               hosts (str): hosts list for distributed training. Defaults to None.
+               hostfile (str): Name of the hostfile for distributed training. Defaults to None.
+               shared_dir (str): The shared data dir for distributed training.
+               temp_dir (str): The temp data dir at local.
 
            Returns:
                History object from the model.fit() call
 
            Raises:
                FileExistsError if the output directory is a file
                TypeError if the dataset specified is not a TextClassificationDataset
                TypeError if the output_dir parameter is not a string
                TypeError if the epochs parameter is not a integer
                TypeError if the initial_checkpoints parameter is not a string
                TypeError if the extra_layers parameter is not a list of integers
         """
-        self._check_train_inputs(output_dir, dataset, TextClassificationDataset, epochs, initial_checkpoints)
+        self._check_train_inputs(
+            output_dir, dataset, TextClassificationDataset,
+            epochs, initial_checkpoints)
 
         if extra_layers:
             if not isinstance(extra_layers, list):
                 raise TypeError("The extra_layers parameter must be a list of ints but found {}".format(
                     type(extra_layers)))
             else:
                 for layer in extra_layers:
@@ -180,26 +193,33 @@
         self.set_auto_mixed_precision(enable_auto_mixed_precision)
 
         # If the number of classes doesn't match what was used before, clear out the previous model
         if dataset_num_classes != self.num_classes:
             self._model = None
 
         self._model = self._get_hub_model(dataset_num_classes, extra_layers)
-        print("Num dataset classes: ", dataset_num_classes)
+        print("Num dataset classes:", dataset_num_classes)
 
-        callbacks, train_data, val_data = self._get_train_callbacks(dataset, output_dir, initial_checkpoints, do_eval,
-                                                                    early_stopping, lr_decay, dataset_num_classes)
+        callbacks, train_data, val_data = self._get_train_callbacks(
+            dataset, output_dir, initial_checkpoints, do_eval,
+            early_stopping, lr_decay, dataset_num_classes)
 
         if distributed:
-            self.export_for_distributed(train_data, val_data)
-            self._fit_distributed(epochs, shuffle_files, hostfile, nnodes, nproc_per_node, kwargs.get('use_horovod'))
-            self.cleanup_saved_objects_for_distributed()
+            objects_path = self.save_objects(
+                train_data, val_data,
+                shared_dir, temp_dir)
+            self._fit_distributed(
+                epochs, shuffle_files,
+                nnodes, nproc_per_node, hosts, hostfile,
+                objects_path, kwargs.get('use_horovod'))
+            self.cleanup_objects(objects_path)
         else:
-            history = self._model.fit(train_data, validation_data=val_data, epochs=epochs, shuffle=shuffle_files,
-                                      callbacks=callbacks)
+            history = self._model.fit(
+                train_data, validation_data=val_data, epochs=epochs,
+                shuffle=shuffle_files, callbacks=callbacks)
 
             self._history = history.history
 
             return self._history
 
     def evaluate(self, dataset: TextClassificationDataset, use_test_set=False):
         """
```

## cloudtik/runtime/ai/runner/__init__.py

```diff
@@ -1,16 +1,23 @@
 class _LaunchArgs(object):
     def __init__(self):
-        self.nnodes = None
-        self.nproc_per_node = None
-        self.program = None
-        self.program_args = None
+        self.command = None
         self.run_func = None
         self.executable = None
 
+        # nodes and processes
+        # If nnodes and nproc_per_node is specified
+        # hosts/hostfile can be host address only without slots
+        # Or you can specify hosts with slots and specify the num_proc
+        # all these are handled by Distributor
+
+        self.num_proc = None
+        self.nnodes = None
+        self.nproc_per_node = None
+
         # host arguments
         self.hosts = None
         self.hostfile = None
 
         self.master_addr = "127.0.0.1"
         self.master_port = 29500
 
@@ -25,15 +32,15 @@
         self.module = False
         self.no_python = False
         self.log_path = None
         self.log_file_prefix = None
 
         # library arguments
         # MPI
-        self.more_mpi_args = None
+        self.mpi_args = None
         self.tcp_flag = None
         self.binding_args = None
         self.num_nccl_streams = None
         self.thread_affinity = None
 
         # CPU, CCL, IOMP, Allocator options
         self.use_logical_core = False
@@ -48,14 +55,15 @@
         self.use_mpi = None
 
 
 def run(
         func,
         args=(),
         kwargs=None,
+        num_proc=None,
         nnodes=None,
         nproc_per_node=None,
         hosts=None,
         hostfile=None,
         output_filename=None,
         verbose=None,
         use_gloo=None,
@@ -68,14 +76,15 @@
     Launch a job to run the specified process function and get the return value.
 
     :param func: The function to be run in job processes. The function return value will
                  be collected as the corresponding process return value.
                  This function must be compatible with pickle.
     :param args: Arguments to pass to `func`.
     :param kwargs: Keyword arguments to pass to `func`.
+    :param num_proc: The number of processes for running.
     :param nnodes: The number of nodes. if not specified, use the number of nodes in the hosts
     :param nproc_per_node: The number of process per node.
     :param hosts: List of host names and the number of available slots
                   for running processes on each, of the form: <hostname>:<slots>
                   (e.g.: host1:2,host2:4,host3:1 indicating 2 processes can run on host1,
                   4 on host2, and 1 on host3). If not specified, defaults to using localhost:<num_proc>
     :param hostfile: Path to a host file containing the list of host names and the number of
@@ -93,15 +102,15 @@
                                Horovod will find the common NICs among all the workers.
                                Example: ["eth0", "eth1"].
     :param executable: Optional executable to run when launching the workers. Defaults to `sys.executable`.
     :return: Return a list which contains values return by all processes.
              The index of the list corresponds to the rank of each process.
              Returns only the first min_num_proc results, if set.
     """
-    from .launch import _run
+    from cloudtik.runtime.ai.runner.launch import _run
 
     if kwargs is None:
         kwargs = {}
 
     def wrapped_func():
         return func(*args, **kwargs)
 
@@ -109,51 +118,52 @@
         raise ValueError('Argument hosts and hostfile only allow one provided.')
 
     if use_gloo and use_mpi:
         raise ValueError('Argument use_gloo and use_mpi only allow one set True.')
 
     largs = _LaunchArgs()
 
+    largs.num_proc = num_proc
     largs.nnodes = nnodes
     largs.nproc_per_node = nproc_per_node
     largs.hosts = hosts
     largs.hostfile = hostfile
     largs.launcher = "horovod"
-    largs.more_mpi_args = mpi_args
+    largs.mpi_args = mpi_args
     largs.output_filename = output_filename
     largs.verbose = verbose
     largs.use_gloo = use_gloo
     largs.use_mpi = use_mpi
     largs.nics = set(network_interfaces) if network_interfaces else None
     largs.run_func = wrapped_func
     largs.executable = executable
 
     return _run(largs)
 
 
 def run_command(
-        program,
-        program_args=None,
+        command,
+        num_proc=None,
         nnodes=None,
         nproc_per_node=None,
         hosts=None,
         hostfile=None,
         launcher=None,
         output_filename=None,
         verbose=None,
         use_gloo=None,
         use_mpi=None,
         mpi_args=None,
         network_interfaces=None
        ):
     """
-    Launch a job to run the specified process function and get the return value.
+    Launch command to run the specified process function and get the return value.
 
-    :param program: The program to be run in job processes.
-    :param program_args: The list of program arguments
+    :param command: The command with arguments to be run in job processes.
+    :param num_proc: The number of processes for running.
     :param nnodes: The number of nodes. if not specified, use the number of nodes in the hosts
     :param nproc_per_node: The number of process per node.
     :param hosts: List of host names and the number of available slots
                   for running processes on each, of the form: <hostname>:<slots>
                   (e.g.: host1:2,host2:4,host3:1 indicating 2 processes can run on host1,
                   4 on host2, and 1 on host3). If not specified, defaults to using localhost:<num_proc>
     :param hostfile: Path to a host file containing the list of host names and the number of
@@ -169,32 +179,32 @@
     :param use_mpi: Run using the MPI
     :param mpi_args: Extra arguments for the MPI controller. This is only used when use_mpi is True.
     :param network_interfaces: List of network interfaces to use for communication. If not specified,
                                Horovod will find the common NICs among all the workers.
                                Example: ["eth0", "eth1"].
     :return: None
     """
-    from .launch import _run
+    from cloudtik.runtime.ai.runner.launch import _run
 
     if hosts is not None and hostfile is not None:
         raise ValueError('Argument hosts and hostfile only allow one provided.')
 
     if use_gloo and use_mpi:
         raise ValueError('Argument use_gloo and use_mpi only allow one set True.')
 
     largs = _LaunchArgs()
 
+    largs.num_proc = num_proc
     largs.nnodes = nnodes
     largs.nproc_per_node = nproc_per_node
     largs.hosts = hosts
     largs.hostfile = hostfile
     largs.launcher = launcher
-    largs.more_mpi_args = mpi_args
+    largs.mpi_args = mpi_args
     largs.output_filename = output_filename
     largs.verbose = verbose
     largs.use_gloo = use_gloo
     largs.use_mpi = use_mpi
     largs.nics = set(network_interfaces) if network_interfaces else None
-    largs.program = program
-    largs.program_args = program_args
+    largs.command = command
 
     return _run(largs)
```

## cloudtik/runtime/ai/runner/distributed_training_launcher.py

```diff
@@ -8,91 +8,54 @@
 
 
 class DistributedTrainingLauncher(Launcher):
     r"""
      Launcher for distributed training
      """
 
-    def __init__(self, args):
-        super().__init__(args)
-        self.hosts = None
+    def __init__(self, args, distributor):
+        super().__init__(args, distributor)
 
     def launch(self):
         """
         Set ENVs and launch process for distributed training by calling run with command
         """
-        self.verify_hosts()
         self.set_master()
         self.set_environment()
         self.run()
 
-    def verify_hosts(self):
+    def set_master(self):
         args = self.args
-        # There are 3 cases
-        # 1. local single node training (args.nnodes <= 1, no args.hosts and no args.hostfile)
-        # 2. remote single node training (args.nnodes <= 1, args.hosts or args.hostfile)
-        # 3. remote multi-node training (args.nnodes == 0 or args.nnodes > 1, args.hosts or args.hostfile)
-        if not args.hosts and not os.path.exists(args.hostfile):
-            if args.nnodes is not None and args.nnodes > 1:
-                raise ValueError("hosts or hostfile is necessary when you use multi-node distributed training,")
-            # local single node training
+
+        if self.distributor.distributed:
+            if not args.master_addr or args.master_addr == "127.0.0.1":
+                args.master_addr = self.distributor.hosts[0]["ip"]
+        else:
             if not args.master_addr:
                 args.master_addr = "127.0.0.1"
-            args.nnodes = 1
-        else:
-            # either hosts or hostfile specified, remote training
-            if args.hostfile:
-                host_list = []
-                with open(args.hostfile) as f:
-                    for line in f:
-                        line = line.strip().strip("\n")
-                        host_list.append(line)
-                if not host_list:
-                    raise ValueError("No IP listed in hostfile.")
-            else:
-                # hosts specified
-                host_list = args.hosts.split(',')
-
-            self.hosts = host_list
-            host_number = len(host_list)
-            if not args.nnodes:
-                args.nnodes = host_number
-            elif args.nnodes > host_number:
-                raise ValueError("nnodes {} cannot be greater than the number of hosts {}.".format(
-                    args.nnodes, host_number
-                ))
-            args.master_addr = host_list[0]
 
-    def set_master(self):
-        args = self.args
         # set distributed related environmental variables
+        # This is only necessary for pytorch based distributed training
         self.set_env("MASTER_ADDR", args.master_addr)
         self.set_env("MASTER_PORT", str(args.master_port))
 
     def set_environment(self):
-        args = self.args
-        # Default we run single instance per node
-        if not args.nproc_per_node:
-            args.nproc_per_node = 1
+        # we default to run single proc per node if not specified
+        self.distributor.resolve()
 
     def get_command_to_run(self):
         args = self.args
         cmd = []
-        with_python = not args.no_python
-        if with_python:
-            cmd.append(sys.executable)
-            cmd.append("-u")
-        if args.module:
-            cmd.append("-m")
-        cmd.append(args.program)
-        cmd.extend(args.program_args)
+        self.with_python_command(cmd)
+        cmd.extend(args.command)
         cmd_s = " ".join(cmd)
         if args.log_path:
             log_name = args.log_file_prefix + ".log"
             log_file = os.path.join(args.log_path, log_name)
             cmd_s = "{} 2>&1 | tee {}".format(cmd_s, log_file)
         logger.info(cmd_s)
         return cmd_s
 
     def run(self):
         command = self.get_command_to_run()
         logger.info(command)
+
```

## cloudtik/runtime/ai/runner/horovod_training_launcher.py

```diff
@@ -1,57 +1,48 @@
 import logging
 import sys
 
 from cloudtik.runtime.ai.runner.distributed_training_launcher import DistributedTrainingLauncher
+from cloudtik.runtime.ai.runner.util.utils import is_python_program
 
 logger = logging.getLogger(__name__)
 
 
 class HorovodTrainingLauncher(DistributedTrainingLauncher):
     r"""
      Launcher for distributed training with Horovod
      """
 
-    def __init__(self, args):
-        super().__init__(args)
+    def __init__(self, args, distributor):
+        super().__init__(args, distributor)
 
     def get_command_to_run(self):
         args = self.args
         cmd = []
-        with_python = not args.no_python
-        if with_python:
-            cmd.append(sys.executable)
-            cmd.append("-u")
-        if args.module:
-            cmd.append("-m")
-        cmd.append(args.program)
-        cmd.extend(args.program_args)
+        self.with_python_command(cmd)
+        cmd.extend(args.command)
         return cmd
 
     def run(self):
         args = self.args
 
         # Run with Horovod
         from horovod.runner import _HorovodArgs
         from horovod.runner.launch import _run
 
-        num_proc = args.nnodes * args.nproc_per_node
-
         hargs = _HorovodArgs()
 
-        hargs.num_proc = num_proc
-        if args.hosts:
-            host_slots = ["{}:{}".format(host, args.nproc_per_node) for host in self.hosts]
-            host_slots_list = ",".join(host_slots)
-            hargs.hosts = host_slots_list
-        else:
-            hargs.hostfile = args.hostfile
-        hargs.mpi_args = args.more_mpi_params
+        hargs.num_proc = self.distributor.num_proc
+        hargs.hosts = self.distributor.hosts_slots_str
+
+        hargs.mpi_args = args.mpi_args
         hargs.use_mpi = args.use_mpi
         hargs.use_gloo = args.use_gloo
+        hargs.nics = args.nics
+        hargs.output_filename = args.output_filename
         hargs.verbose = args.verbose
 
         if args.run_func:
             hargs.run_func = args.run_func
             hargs.executable = args.executable
         else:
             command = self.get_command_to_run()
```

## cloudtik/runtime/ai/runner/launch.py

```diff
@@ -1,15 +1,18 @@
+import argparse
 import glob
 import logging
 import os
 import platform
 from argparse import ArgumentParser, REMAINDER
 from argparse import RawTextHelpFormatter
 from datetime import datetime
 
+from cloudtik.runtime.ai.runner.util.distributor import Distributor
+
 logger = logging.getLogger(__name__)
 
 r"""
 This is a script for launching PyTorch training and inference on Intel Xeon CPU with optimal configurations.
 Now, single instance inference/training, multi-instance inference/training and distributed training
 with oneCCL backend is enabled.
 
@@ -109,122 +112,202 @@
 "--enable_tcmalloc" and "--enable_jemalloc" can be used to enable different memory allocator.
 
 """
 
 
 def add_cpu_option_params(parser):
     group = parser.add_argument_group("Parameters for CPU options")
-    group.add_argument("--use_logical_core", action='store_true', default=False,
+    group.add_argument("--use-logical-core", "--use_logical_core",
+                       action='store_true', default=False,
                        help="Whether only use physical cores")
 
 
 def add_distributed_training_params(parser):
-    group = parser.add_argument_group("Distributed Training Parameters With oneCCL backend")
-    group.add_argument("--nnodes", metavar='\b', type=int, default=0,
-                       help="The number of nodes to use for distributed "
-                       "training")
-    group.add_argument("--nproc_per_node", metavar='\b', type=int, default=0,
+    group = parser.add_argument_group("Distributed Training Parameters")
+    group.add_argument('--num-proc', '--num_proc',
+                       action='store', type=int, default=0,
+                       help="The number of process to run for distributed training")
+    group.add_argument("--nnodes",
+                       type=int, default=0,
+                       help="The number of nodes to use for distributed training")
+    group.add_argument("--nproc-per-node", "--nproc_per_node",
+                       action='store', type=int, default=0,
                        help="The number of processes to launch on each node")
+    group.add_argument("--hosts",
+                       default="", type=str,
+                       help="List of hosts separated with comma for launching tasks. "
+                            "When hosts is specified, it implies distributed training. "
+                            "node address which should be either the IP address"
+                            "or the hostname with or without slots.")
+    group.add_argument("--hostfile",
+                       default="", type=str,
+                       help="Hostfile is necessary for multi-node multi-proc "
+                            "training. hostfile includes the node address list "
+                            "node address which should be either the IP address"
+                            "or the hostname with or without slots.")
+
     # ccl control
-    group.add_argument("--ccl_worker_count", metavar='\b', default=4, type=int,
+    group.add_argument("--ccl-worker-count", "--ccl_worker_count",
+                       action='store', dest='ccl_worker_count', default=4, type=int,
                        help="Core numbers per rank used for ccl communication")
     # mpi control
-    group.add_argument("--master_addr", metavar='\b', default="127.0.0.1", type=str,
+    group.add_argument("--master-addr", "--master_addr",
+                       action='store', default="127.0.0.1", type=str,
                        help="Master node (rank 0)'s address, should be either "
                             "the IP address or the hostname of node 0, for "
                             "single node multi-proc training, the "
                             "--master_addr can simply be 127.0.0.1")
-    group.add_argument("--master_port", metavar='\b', default=29500, type=int,
+    group.add_argument("--master-port", "--master_port",
+                       action='store', default=29500, type=int,
                        help="Master node (rank 0)'s free port that needs to "
                             "be used for communication during distributed "
                             "training")
-    group.add_argument("--hostfile", metavar='\b', default="", type=str,
-                       help="Hostfile is necessary for multi-node multi-proc "
-                            "training. hostfile includes the node address list "
-                            "node address which should be either the IP address"
-                            "or the hostname.")
-    group.add_argument("--hosts", metavar='\b', default="", type=str,
-                       help="List of hosts separated with comma for launching tasks. "
-                            "When hosts is specified, it implies distributed training. "
-                            "node address which should be either the IP address"
-                            "or the hostname.")
-    group.add_argument("--more_mpi_params", metavar='\b', default="", type=str,
+    group.add_argument("--mpi-args", "--mpi_args", "--more_mpi_params",
+                       action='store', dest='mpi_args', default="", type=str,
                        help="User can pass more parameters for mpiexec.hydra "
                             "except for -np -ppn -hostfile and -genv I_MPI_PIN_DOMAIN")
 
 
 def add_memory_allocator_params(parser):
     group = parser.add_argument_group("Memory Allocator Parameters")
     # allocator control
-    group.add_argument("--enable_tcmalloc", action='store_true', default=False,
+    group.add_argument("--enable-tcmalloc", "--enable_tcmalloc",
+                       action='store_true', default=False,
                        help="Enable tcmalloc allocator")
-    group.add_argument("--enable_jemalloc", action='store_true', default=False,
+    group.add_argument("--enable-jemalloc", "--enable_jemalloc",
+                       action='store_true', default=False,
                        help="Enable jemalloc allocator")
-    group.add_argument("--use_default_allocator", action='store_true', default=False,
+    group.add_argument("--use-default-allocator", "--use_default_allocator",
+                       action='store_true', default=False,
                        help="Use default memory allocator")
 
 
 def add_multi_instance_params(parser):
     group = parser.add_argument_group("Multi-instance Parameters")
     # multi-instance control
-    group.add_argument("--ncore_per_instance", metavar='\b', default=-1, type=int,
+    group.add_argument("--ninstances",
+                       default=-1, type=int,
+                       help="For multi-instance, you should give the cores number you used for per instance.")
+    group.add_argument("--ncore-per-instance", "--ncore_per_instance",
+                       default=-1, type=int,
                        help="Cores per instance")
-    group.add_argument("--skip_cross_node_cores", action='store_true', default=False,
+    group.add_argument("--skip-cross-node-cores", "--skip_cross_node_cores",
+                       action='store_true', default=False,
                        help="If specified --ncore_per_instance, skips cross-node cores.")
-    group.add_argument("--ninstances", metavar='\b', default=-1, type=int,
-                       help="For multi-instance, you should give the cores number you used for per instance.")
-    group.add_argument("--instance_idx", metavar='\b', default="-1", type=int,
+    group.add_argument("--instance-idx", "--instance_idx",
+                       default="-1", type=int,
                        help="Specify instance index to assign ncores_per_instance for instance_idx; "
                             "otherwise ncore_per_instance will be assigned sequentially to ninstances.")
-    group.add_argument("--latency_mode", action='store_true', default=False,
-                       help="By detault 4 core per instance and use all physical cores")
-    group.add_argument("--throughput_mode", action='store_true', default=False,
+    group.add_argument("--latency-mode", "--latency_mode",
+                       action='store_true', default=False,
+                       help="By default 4 core per instance and use all physical cores")
+    group.add_argument("--throughput-mode", "--throughput_mode",
+                       action='store_true', default=False,
                        help="By default one instance per node and use all physical cores")
-    group.add_argument("--node_id", metavar='\b', default=-1, type=int,
+    group.add_argument("--node-id", "--node_id",
+                       default=-1, type=int,
                        help="node id for multi-instance, by default all nodes will be used")
-    group.add_argument("--disable_numactl", action='store_true', default=False,
+    group.add_argument("--disable-numactl", "--disable_numactl",
+                       action='store_true', default=False,
                        help="Disable numactl")
-    group.add_argument("--disable_taskset", action='store_true', default=False,
+    group.add_argument("--disable-taskset", "--disable_taskset",
+                       action='store_true', default=False,
                        help="Disable taskset")
-    group.add_argument("--core_list", metavar='\b', default=None, type=str,
+    group.add_argument("--core-list", "--core_list",
+                       default=None, type=str,
                        help="Specify the core list as 'core_id, core_id, ....', otherwise, all the cores will be used.")
-    group.add_argument("--benchmark", action='store_true', default=False,
+    group.add_argument("--benchmark",
+                       action='store_true', default=False,
                        help="Enable benchmark config. JeMalloc's MALLOC_CONF has been tuned for low latency. "
                             "Recommend to use this for benchmarking purpose; for other use cases, "
                             "this MALLOC_CONF may cause Out-of-Memory crash.")
 
 
 def add_kmp_iomp_params(parser):
     group = parser.add_argument_group("IOMP Parameters")
-    group.add_argument("--disable_iomp", action='store_true', default=False,
+    group.add_argument("--disable-iomp", "--disable_iomp",
+                       action='store_true', default=False,
                        help="By default, we use Intel OpenMP and libiomp5.so will be add to LD_PRELOAD")
 
 
 def add_auto_ipex_params(parser, auto_ipex_default_enabled=False):
     group = parser.add_argument_group("Code_Free Parameters")
-    group.add_argument("--auto_ipex", action='store_true', default=auto_ipex_default_enabled,
+    group.add_argument("--auto-ipex", "--auto_ipex",
+                       action='store_true', default=auto_ipex_default_enabled,
                        help="Auto enabled the ipex optimization feature")
-    group.add_argument("--dtype", metavar='\b', default="float32", type=str,
+    group.add_argument("--dtype",
+                       default="float32", type=str,
                        choices=['float32', 'bfloat16'],
                        help="The data type to run inference. float32 or bfloat16 is allowed.")
-    group.add_argument("--auto_ipex_verbose", action='store_true', default=False,
+    group.add_argument("--auto-ipex-verbose", "--auto_ipex_verbose",
+                       action='store_true', default=False,
                        help="This flag is only used for debug and UT of auto ipex.")
-    group.add_argument("--disable_ipex_graph_mode", action='store_true', default=False,
+    group.add_argument("--disable-ipex-graph-mode", "--disable_ipex_graph_mode",
+                       action='store_true', default=False,
                        help="Enable the Graph Mode for ipex.optimize")
 
 
+def make_nic_action():
+    # This is an append Action that splits the values on ','
+    class NicAction(argparse.Action):
+        def __init__(self,
+                     option_strings,
+                     dest,
+                     default=None,
+                     type=None,
+                     choices=None,
+                     required=False,
+                     help=None):
+            super(NicAction, self).__init__(
+                option_strings=option_strings,
+                dest=dest,
+                nargs=1,
+                default=default,
+                type=type,
+                choices=choices,
+                required=required,
+                help=help)
+
+        def __call__(self, parser, args, values, option_string=None):
+            if ',' in values[0]:
+                values = values[0].split(',')
+
+            # union the existing dest nics with the new ones
+            items = getattr(args, self.dest, None)
+            items = set() if items is None else items
+            items = items.union(values)
+
+            setattr(args, self.dest, items)
+
+    return NicAction
+
+
 def add_horovod_params(parser):
     group = parser.add_argument_group("Horovod Parameters")
-    group.add_argument('--gloo', action='store_true', dest='use_gloo',
+    group.add_argument('--gloo',
+                       action='store_true', dest='use_gloo',
                        help='Run Horovod using the Gloo controller. This will '
                             'be the default if Horovod was not built with MPI support.')
-    group.add_argument('--mpi', action='store_true', dest='use_mpi',
+    group.add_argument('--mpi',
+                       action='store_true', dest='use_mpi',
                        help='Run Horovod using the MPI controller. This will '
                             'be the default if Horovod was built with MPI support.')
 
+    group.add_argument('--network-interfaces', '--network_interfaces',
+                       action=make_nic_action(), dest='nics',
+                       help='Network interfaces that can be used for communication separated by '
+                            'comma. If not specified, will find the common NICs among all '
+                            'the workers. Example: --network-interfaces "eth0,eth1".')
+
+    group.add_argument('--output-filename', '--output_filename',
+                       action='store',
+                       help='For Gloo, writes stdout / stderr of all processes to a filename of the form '
+                            '<output_filename>/rank.<rank>/<stdout | stderr>. The <rank> will be padded with 0 '
+                            'characters to ensure lexicographical order. For MPI, delegates its behavior to mpirun.')
+
 
 def parse_args():
     """
     Helper function parsing the command line options
     @retval ArgumentParser
     """
     parser = ArgumentParser(
@@ -245,56 +328,60 @@
                     "\n4. Multi-Node multi-process distributed training: (e.g. two nodes)\n"
                     "\n   rank 0: *(IP: 192.168.10.10, and has a free port: 295000)*\n"
                     "\n   >>> cloudtik-ai-run --distributed --nproc_per_node=2\n"
                     "\n       --nnodes=2 --hostfile hostfile python_script args\n"
                     "\n############################################################################# \n",
                     formatter_class=RawTextHelpFormatter)
 
-    parser.add_argument("--multi_instance", action='store_true', default=False,
+    parser.add_argument("--multi-instance", "--multi_instance",
+                        action='store_true', default=False,
                         help="Enable multi-instance, by default one instance per node")
 
-    parser.add_argument('--distributed', action='store_true', default=False,
+    parser.add_argument('--distributed',
+                        action='store_true', default=False,
                         help='Enable distributed training.')
-    parser.add_argument("--launcher", metavar='\b', default="", type=str,
+    parser.add_argument("--launcher",
+                        default="", type=str,
                         help="The launcher to use: default, optimized, horovod")
-    parser.add_argument("-m", "--module", default=False, action="store_true",
+    parser.add_argument("-m", "--module",
+                        default=False, action="store_true",
                         help="Changes each process to interpret the launch script "
                              "as a python module, executing with the same behavior as"
                              "'python -m'.")
 
-    parser.add_argument("--no_python", default=False, action="store_true",
+    parser.add_argument("--no-python", "--no_python",
+                        default=False, action="store_true",
                         help="Do not prepend the --program script with \"python\" - just exec "
                              "it directly. Useful when the script is not a Python script.")
 
-    parser.add_argument("--verbose", default=False, action='store_true',
-                        dest='verbose',
-                        help='If this flag is set, extra messages will be printed.')
-    parser.add_argument("--log_path", metavar='\b', default="", type=str,
+    parser.add_argument("--log-path", "--log_path",
+                        default="", type=str,
                         help="The log file directory. Default path is '', which means disable logging to files.")
-    parser.add_argument("--log_file_prefix", metavar='\b', default="run", type=str,
+    parser.add_argument("--log-file-prefix", "--log_file_prefix",
+                        default="run", type=str,
                         help="log file prefix")
 
+    parser.add_argument("--verbose",
+                        default=False, action='store_true',
+                        help='If this flag is set, extra messages will be printed.')
+
     add_cpu_option_params(parser)
     add_memory_allocator_params(parser)
     add_kmp_iomp_params(parser)
 
     add_distributed_training_params(parser)
     add_multi_instance_params(parser)
 
     add_auto_ipex_params(parser)
 
     add_horovod_params(parser)
 
-    # positional
-    parser.add_argument("program", type=str,
-                        help="The full path to the program/script to be launched. "
-                             "followed by all the arguments for the script")
+    parser.add_argument('command', nargs=argparse.REMAINDER,
+                        help='Command to be executed.')
 
-    # rest from the training program
-    parser.add_argument('program_args', nargs=REMAINDER)
     args = parser.parse_args()
     args.run_func = None
     args.executable = None
     return args
 
 
 def _verify_ld_preload():
@@ -333,49 +420,58 @@
         fileHandler.setFormatter(logFormatter)
 
         # add the handle to root logger
         root_logger.addHandler(fileHandler)
 
 
 def _run(args):
-    if args.distributed and args.multi_instance:
-        raise RuntimeError("Either args.distributed or args.multi_instance should be set")
+    # check either command or func be specified
+    if not args.command and not args.run_func:
+        raise ValueError("Must specify either command or function to launch.")
+
+    distributor = Distributor(
+        args.num_proc,
+        args.nnodes,
+        args.nproc_per_node,
+        args.hosts,
+        args.hostfile,
+    )
 
-    if args.nnodes > 1 or args.hosts or args.hostfile:
+    if distributor.distributed:
         args.distributed = True
 
+    if args.distributed and args.multi_instance:
+        raise RuntimeError("Either args.distributed or args.multi_instance should be set")
+
     if not args.distributed:
         if args.latency_mode and args.throughput_mode:
             raise RuntimeError("Either args.latency_mode or args.throughput_mode should be set")
 
-    if not args.no_python and not args.program.endswith(".py"):
-        raise RuntimeError("For non Python script, you should use '--no_python' parameter.")
-
     env_before = set(os.environ.keys())
 
     # Verify LD_PRELOAD
     _verify_ld_preload()
 
     if args.distributed:
         if args.launcher == "default":
             from cloudtik.runtime.ai.runner.cpu.default_training_launcher \
                 import DefaultTrainingLauncher
-            launcher = DefaultTrainingLauncher(args)
+            launcher = DefaultTrainingLauncher(args, distributor)
         elif args.launcher == "horovod":
             from cloudtik.runtime.ai.runner.horovod_training_launcher \
                 import HorovodTrainingLauncher
-            launcher = HorovodTrainingLauncher(args)
+            launcher = HorovodTrainingLauncher(args, distributor)
         else:
             from cloudtik.runtime.ai.runner.cpu.optimized_training_launcher \
                 import OptimizedTrainingLauncher
-            launcher = OptimizedTrainingLauncher(args)
+            launcher = OptimizedTrainingLauncher(args, distributor)
     else:
         from cloudtik.runtime.ai.runner.cpu.multi_instance_launcher \
             import MultiInstanceLauncher
-        launcher = MultiInstanceLauncher(args)
+        launcher = MultiInstanceLauncher(args, distributor)
 
     launcher.launch()
 
     for x in sorted(set(os.environ.keys()) - env_before):
         logger.debug('{0}={1}'.format(x, os.environ[x]))
```

## cloudtik/runtime/ai/runner/launcher.py

```diff
@@ -1,21 +1,25 @@
 import glob
 import logging
 import os
+import sys
 from os.path import expanduser
 
+from cloudtik.runtime.ai.runner.util.utils import is_python_program
+
 logger = logging.getLogger(__name__)
 
 
 class Launcher:
     r"""
      Base class for launcher
     """
-    def __init__(self, args):
+    def __init__(self, args, distributor):
         self.args = args
+        self.distributor = distributor
 
     def launch(self):
         pass
 
     def add_lib_preload(self, lib_type=None):
         library_paths = []
         if "CONDA_PREFIX" in os.environ:
@@ -53,7 +57,18 @@
         if not env_value:
             logger.warning("{} is None".format(env_name))
         if env_name not in os.environ:
             os.environ[env_name] = env_value
         elif os.environ[env_name] != env_value:
             logger.warning("{} in environment variable is {} while the value you set is {}".format(env_name, os.environ[env_name], env_value))
         self.log_env(env_name)
+
+    def with_python_command(self, cmd):
+        args = self.args
+        with_python = not args.no_python
+        if with_python and (
+                args.module or is_python_program(args.command)):
+            # check whether the program in the command is end with py
+            cmd.append(sys.executable)
+            cmd.append("-u")
+            if args.module:
+                cmd.append("-m")
```

## cloudtik/runtime/ai/runner/cpu/default_training_launcher.py

```diff
@@ -14,59 +14,58 @@
 
 
 class DefaultTrainingLauncher(CPULauncher, DistributedTrainingLauncher):
     r"""
      Launcher for distributed training with MPI launcher
      """
 
-    def __init__(self, args):
-        super().__init__(args)
+    def __init__(self, args, distributor):
+        super().__init__(args, distributor)
 
     def run(self):
         command = self.get_command_to_run()
         if utils.is_impi_or_mpich():
             self._run_command_impi(command)
         else:
             self._run_command_openmpi(command)
 
     def _run_command_openmpi(self, command):
         args = self.args
         # default to use OpenMPI to launch
         _OMPI_FLAGS = ['-mca pml ob1', '-mca btl ^openib']
         _NO_BINDING_ARGS = ['-bind-to none', '-map-by slot']
 
-        num_proc = args.nnodes * args.nproc_per_node
+        num_proc = self.distributor.num_proc
 
         mpi_impl_flags = _OMPI_FLAGS
-        if self.hosts and len(self.hosts) >= _LARGE_CLUSTER_THRESHOLD:
+        if self.distributor.distributed and len(self.distributor.hosts) >= _LARGE_CLUSTER_THRESHOLD:
             mpi_impl_flags.append('-mca plm_rsh_no_tree_spawn true')
             mpi_impl_flags.append(
-                '-mca plm_rsh_num_concurrent {}'.format(len(self.hosts)))
+                '-mca plm_rsh_num_concurrent {}'.format(len(self.distributor.hosts)))
 
         # if user does not specify any hosts, mpirun by default uses local host.
         # There is no need to specify localhost.
-        if self.hosts:
-            host_slots = ["{}:{}".format(host, args.nproc_per_node) for host in self.hosts]
-            host_slots_str = ",".join(host_slots)
+        if self.distributor.distributed:
+            host_slots_str = self.distributor.hosts_slots_str
             hosts_arg = '-{opt} {hosts}'.format(opt='H',
                                                 hosts=host_slots_str)
         else:
             hosts_arg = ''
         binding_args = ' '.join(_NO_BINDING_ARGS)
         basic_args = '--allow-run-as-root --tag-output'
         env = os.environ.copy()
         env_list = ' '.join(
             '-x %s' % key for key in sorted(env.keys()) if utils.is_exportable(key))
 
         def get_cloudtik_rsh():
             runtime_home = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
             return os.path.join(runtime_home, "scripts", "cloudtik-rsh.sh")
 
-        extra_mpi_args = args.more_mpi_params
-        if self.hosts and (not extra_mpi_args or "-mca plm_rsh_agent" not in extra_mpi_args):
+        extra_mpi_args = args.mpi_args
+        if self.distributor.distributed and (not extra_mpi_args or "-mca plm_rsh_agent" not in extra_mpi_args):
             extra_mpi_args = (
                 '{extra_mpi_args} -mca plm_rsh_agent "{rsh_agent}"'
                 .format(extra_mpi_args=extra_mpi_args if extra_mpi_args else '',
                         rsh_agent=get_cloudtik_rsh()))
 
         # Pass all the env variables to the mpirun command.
         mpirun_command = (
@@ -99,30 +98,36 @@
 
         # Execute the mpirun command.
         os.execve('/bin/sh', ['/bin/sh', '-c', mpirun_command], env)
 
     def _run_command_impi(self, command):
         args = self.args
 
+        # make sure that for IMPI cases, all the nodes have the same slots
+        self.distributor.validate_same_slots()
+
+        num_proc = self.distributor.num_proc
+        nproc_per_node = self.distributor.nproc_per_node
+
         cmd = ['mpirun']
         mpi_config = "-l -np {} -ppn {} ".format(
-            args.nnodes * args.nproc_per_node, args.nproc_per_node)
-        mpi_config += args.more_mpi_params
+            num_proc, nproc_per_node)
+        mpi_config += args.mpi_args
 
-        if args.hosts:
-            mpi_config += " -hosts {}".format(args.hosts)
-        elif args.hostfile:
-            mpi_config += " -hostfile {}".format(args.hostfile)
+        if self.distributor.distributed:
+            mpi_config += " -hosts {}".format(self.distributor.hosts_str)
+            # Unified to pass by hosts instead of hostfile
+            # mpi_config += " -hostfile {}".format(hostfile)
 
         def get_cloudtik_rsh():
             runtime_home = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
             return os.path.join(runtime_home, "scripts", "cloudtik-rsh.sh")
 
         # only add this for remote training
-        if args.hosts or args.hostfile:
+        if self.distributor.distributed:
             if "-launcher-exec" not in mpi_config:
                 mpi_config += (
                     ' -launcher rsh -launcher-exec "{launcher_exec}"'.format(
                         launcher_exec=get_cloudtik_rsh()))
 
         cmd.extend(mpi_config.split())
         mpi_command = " ".join(cmd)
```

## cloudtik/runtime/ai/runner/cpu/launcher.py

```diff
@@ -9,16 +9,16 @@
 logger = logging.getLogger(__name__)
 
 
 class CPULauncher(Launcher):
     r"""
      Base class for launcher
     """
-    def __init__(self, args):
-        super().__init__(args)
+    def __init__(self, args, distributor):
+        super().__init__(args, distributor)
         self.cpuinfo = CPUinfo()
 
     def is_numactl_available(self):
         numactl_available = False
         try:
             cmd = ["numactl", "-C", "0", "-m", "0", "ls"]
             r = subprocess.run(cmd, env=os.environ, stdout=subprocess.DEVNULL)
```

## cloudtik/runtime/ai/runner/cpu/multi_instance_launcher.py

```diff
@@ -1,24 +1,26 @@
 import logging
 import os
 import subprocess
 import sys
 
 from cloudtik.runtime.ai.runner.cpu.launcher import CPULauncher
+from cloudtik.runtime.ai.runner.util.utils import is_python_program
 
 logger = logging.getLogger(__name__)
 
 
 class MultiInstanceLauncher(CPULauncher):
     r"""
      Launcher for single instance and multi-instance
      """
 
-    def __init__(self, args):
-        super().__init__(args)
+    def __init__(self, args, distributor):
+        super().__init__(args, distributor)
+        self.program = None
 
     def launch(self):
         args = self.args
         processes = []
         cores = []
         set_kmp_affinity = True
         enable_taskset = False
@@ -142,18 +144,19 @@
                                             set_kmp_affinity,
                                             args.enable_tcmalloc,
                                             args.enable_jemalloc,
                                             args.use_default_allocator,
                                             args.benchmark)
         os.environ["LAUNCH_CMD"] = "#"
 
-        if args.auto_ipex:
+        if args.auto_ipex and is_python_program(args.command):
             import intel_extension_for_pytorch.cpu.auto_ipex as auto_ipex
-            args.program = auto_ipex.apply_monkey_patch(
-                args.program, args.dtype, args.auto_ipex_verbose, args.disable_ipex_graph_mode)
+            program = args.command[0]
+            self.program = auto_ipex.apply_monkey_patch(
+                program, args.dtype, args.auto_ipex_verbose, args.disable_ipex_graph_mode)
 
         for i in range(args.ninstances):
             cmd = []
             cur_process_cores = ""
             if not args.disable_numactl or enable_taskset:
                 if not args.disable_numactl:
                     cmd = ["numactl"]
@@ -188,23 +191,20 @@
                     numa_params += "-m {}".format(",".join(
                         [str(numa_id) for numa_id in self.cpuinfo.numa_aware_check(core_list)]))
                     cmd.extend(numa_params.split())
                 elif enable_taskset:
                     taskset_params = "-c {}".format(cur_process_cores)
                     cmd.extend(taskset_params.split())
 
-            with_python = not args.no_python
-            if with_python:
-                cmd.append(sys.executable)
-                cmd.append("-u")
-            if args.module:
-                cmd.append("-m")
-            cmd.append(args.program)
-
-            cmd.extend(args.program_args)
+            self.with_python_command(cmd)
+            if self.program:
+                cmd.append(self.program)
+                cmd.extend(args.command[1:])
+            else:
+                cmd.extend(args.command)
             os.environ["LAUNCH_CMD"] += " ".join(cmd) + ",#"
             cmd_s = " ".join(cmd)
             if args.log_path:
                 log_name = args.log_file_prefix + "_instance_{}_cores_".format(
                     i) + cur_process_cores.replace(',', '_') + ".log"
                 log_file = os.path.join(args.log_path, log_name)
                 cmd_s = "{} 2>&1 | tee {}".format(cmd_s, log_file)
@@ -220,9 +220,9 @@
             for process in processes:
                 process.wait()
                 if process.returncode != 0:
                     raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd_s)
         finally:
             if args.auto_ipex:
                 # Clean the temp file
-                if os.path.exists(args.program) and args.program.endswith("_auto_ipex"):
-                    os.remove(args.program)
+                if self.program and os.path.exists(self.program) and self.program.endswith("_auto_ipex"):
+                    os.remove(self.program)
```

## cloudtik/runtime/ai/runner/cpu/optimized_training_launcher.py

```diff
@@ -6,16 +6,16 @@
 logger = logging.getLogger(__name__)
 
 
 class OptimizedTrainingLauncher(DefaultTrainingLauncher):
     r"""
      Launcher for distributed training with MPI launcher
      """
-    def __init__(self, args):
-        super().__init__(args)
+    def __init__(self, args, distributor):
+        super().__init__(args, distributor)
 
     def get_mpi_pin_domain(self, nproc_per_node, ccl_worker_count, total_cores, flatten_node_cores):
         '''
         I_MPI_PIN_DOMAIN specify the cores used for every MPI process.
         The first ccl_worker_count cores of every rank for ccl communication
         and the other cores will be used to do computation.
         For example: on CascadeLake 8280 CPU, 2 ranks on one node. ccl_worker_count=4
@@ -56,45 +56,47 @@
                 # affinity += str(proc * cores_per_rank + ccl_worker) + ","
                 # CloudTik: patch end
         affinity = affinity[:-1]
         return affinity
 
     def set_environment(self):
         args = self.args
-        if not args.hosts and not args.hostfile:
+        if not self.distributor.distributed:
             # for local single node
             cpuinfo = self.cpuinfo
         else:
             # use master address for getting cpu info
             cpuinfo = CPUinfo(host_ip=args.master_addr)
 
         node_cores = cpuinfo.node_physical_cores
         total_cores_per_node = cpuinfo.physical_cores()
         if args.use_logical_core:
             node_cores = cpuinfo.node_logical_cores
             total_cores_per_node = cpuinfo.logical_cores()
-        if not args.nproc_per_node:
-            args.nproc_per_node = cpuinfo.sockets()
+
+        self.distributor.resolve(cpuinfo.sockets())
+
+        nproc_per_node = self.distributor.nproc_per_node
 
         flatten_node_cores = []
         for node_numa_cores in node_cores:
             flatten_node_cores.extend(node_numa_cores)
 
         mpi_pin_domain = self.get_mpi_pin_domain(
-            args.nproc_per_node, args.ccl_worker_count, total_cores_per_node, flatten_node_cores)
+            nproc_per_node, args.ccl_worker_count, total_cores_per_node, flatten_node_cores)
         self.set_env("I_MPI_PIN_DOMAIN", mpi_pin_domain)
 
-        ppn = args.nproc_per_node
+        ppn = nproc_per_node
         cores_per_rank = total_cores_per_node // ppn
 
         omp_num_threads = cores_per_rank - args.ccl_worker_count
         self.set_multi_thread_and_allocator(omp_num_threads,
                                             args.disable_iomp,
                                             True,
                                             args.enable_tcmalloc,
                                             args.enable_jemalloc,
                                             args.use_default_allocator)
 
         self.set_env("CCL_WORKER_COUNT", str(args.ccl_worker_count))
         ccl_affinity = self.get_ccl_worker_affinity(
-            args.nproc_per_node, args.ccl_worker_count, total_cores_per_node, flatten_node_cores)
+            nproc_per_node, args.ccl_worker_count, total_cores_per_node, flatten_node_cores)
         self.set_env("CCL_WORKER_AFFINITY", ccl_affinity)
```

## cloudtik/runtime/ai/runner/util/utils.py

```diff
@@ -121,7 +121,13 @@
         return (list(_SMPI_FLAGS_TCP) if tcp_flag else list(_SMPI_FLAGS)), list(_SOCKET_BINDING_ARGS), _SMPI_IMPL
     elif is_mpich(env):
         return list(_MPICH_FLAGS), list(_NO_BINDING_ARGS), _MPICH_IMPL
     elif is_intel_mpi(env):
         return list(_IMPI_FLAGS), [], _IMPI_IMPL
     else:
         return None, None, None
+
+
+def is_python_program(command):
+    if not command:
+        return False
+    return command[0].endswith(".py")
```

## cloudtik/runtime/ai/scripts/install.sh

```diff
@@ -53,19 +53,24 @@
         SQLAlchemy==1.4.46 \
         alembic==1.10.1 \
         pymysql==1.0.3 \
         pyarrow==8.0.0 \
         hyperopt==0.2.7 \
         scikit-learn==1.0.2 \
         xgboost==1.7.5 \
-        transformers==4.29.1 \
+        transformers==4.30.2 \
         pandas==2.0.1 \
         category-encoders==2.6.0 \
         h5py==3.8.0 \
-        lightgbm==3.3.5
+        lightgbm==3.3.5 \
+        tensorflow-text==2.12.1 \
+        datasets~=2.9.0 \
+        tensorflow-datasets~=4.8.2 \
+        tensorflow-hub~=0.12.0 \
+        protobuf==3.20.3
 
     mkdir -p $RUNTIME_PATH/mlflow
 
     echo "Installing deep learning frameworks: tensorflow, pytorch..."
     pip --no-cache-dir -qq install tensorflow==2.12.0
 
     if [ "$AI_WITH_GPU" == "true" ]; then
```

## cloudtik/runtime/common/conf/hadoop/core-site.xml

### cloudtik/runtime/common/conf/hadoop/core-site.xml

```diff
@@ -16,19 +16,11 @@
 <!-- Put site-specific property overrides in this file. -->
 <configuration>
   <property>
     <name>fs.defaultFS</name>
     <value>{%fs.default.name%}</value>
   </property>
   <property>
-    <name>hadoop.proxyuser.root.groups</name>
-    <value>*</value>
-  </property>
-  <property>
-    <name>hadoop.proxyuser.root.hosts</name>
-    <value>*</value>
-  </property>
-  <property>
     <name>io.file.buffer.size</name>
     <value>131072</value>
   </property>
 </configuration>
```

## cloudtik/runtime/common/conf/hadoop/aliyun/core-site.xml

### cloudtik/runtime/common/conf/hadoop/aliyun/core-site.xml

```diff
@@ -30,19 +30,11 @@
   <property>
     <name>fs.oss.endpoint</name>
     <value>{%fs.oss.endpoint%}</value>
     <description>Aliyun OSS endpoint to connect to.</description>
   </property>
   {%hadoop.credential.property%}
   <property>
-    <name>hadoop.proxyuser.root.groups</name>
-    <value>*</value>
-  </property>
-  <property>
-    <name>hadoop.proxyuser.root.hosts</name>
-    <value>*</value>
-  </property>
-  <property>
     <name>io.file.buffer.size</name>
     <value>131072</value>
   </property>
 </configuration>
```

## cloudtik/runtime/common/conf/hadoop/aws/core-site.xml

### cloudtik/runtime/common/conf/hadoop/aws/core-site.xml

```diff
@@ -33,19 +33,11 @@
   </property>
   <property>
     <name>fs.s3a.access.key</name>
     <value>{%fs.s3a.access.key%}</value>
   </property>
   {%hadoop.credential.property%}
   <property>
-    <name>hadoop.proxyuser.root.groups</name>
-    <value>*</value>
-  </property>
-  <property>
-    <name>hadoop.proxyuser.root.hosts</name>
-    <value>*</value>
-  </property>
-  <property>
     <name>io.file.buffer.size</name>
     <value>131072</value>
   </property>
 </configuration>
```

## cloudtik/runtime/common/conf/hadoop/azure/core-site.xml

### cloudtik/runtime/common/conf/hadoop/azure/core-site.xml

```diff
@@ -29,19 +29,11 @@
   </property>
   <property>
     <name>fs.azure.account.auth.type</name>
     <value>OAuth</value>
   </property>
   {%hadoop.credential.property%}
   <property>
-    <name>hadoop.proxyuser.root.groups</name>
-    <value>*</value>
-  </property>
-  <property>
-    <name>hadoop.proxyuser.root.hosts</name>
-    <value>*</value>
-  </property>
-  <property>
     <name>io.file.buffer.size</name>
     <value>131072</value>
   </property>
 </configuration>
```

## cloudtik/runtime/common/conf/hadoop/gcp/core-site.xml

### cloudtik/runtime/common/conf/hadoop/gcp/core-site.xml

```diff
@@ -51,19 +51,11 @@
     <value>777</value>
   </property>
   <property>
     <name>fs.gs.working.dir</name>
     <value>/</value>
   </property>
   <property>
-    <name>hadoop.proxyuser.root.groups</name>
-    <value>*</value>
-  </property>
-  <property>
-    <name>hadoop.proxyuser.root.hosts</name>
-    <value>*</value>
-  </property>
-  <property>
     <name>io.file.buffer.size</name>
     <value>131072</value>
   </property>
 </configuration>
```

## cloudtik/runtime/common/conf/hadoop/huaweicloud/core-site.xml

### cloudtik/runtime/common/conf/hadoop/huaweicloud/core-site.xml

```diff
@@ -26,19 +26,11 @@
   <property>
     <name>fs.obs.endpoint</name>
     <value>{%fs.obs.endpoint.property%}</value>
   </property>
   {%fs.obs.security.provider.property%}
     {%hadoop.credential.property%}
   <property>
-    <name>hadoop.proxyuser.root.groups</name>
-    <value>*</value>
-  </property>
-  <property>
-    <name>hadoop.proxyuser.root.hosts</name>
-    <value>*</value>
-  </property>
-  <property>
     <name>io.file.buffer.size</name>
     <value>131072</value>
   </property>
 </configuration>
```

## cloudtik/runtime/common/scripts/cloud-storage-fuse.sh

```diff
@@ -3,14 +3,24 @@
 # Assumptions for using the functions of this script:
 # 1. HDFS_ENABLED, HDFS_NAMENODE_URI, $XXX_CLOUD_STORAGE variable is set correspondingly.
 # 2. Credential values are exported through the environment variables through provider.with_environment_variables.
 # 3. USER_HOME is set to the current user home.
 # 4. For service functions, CLOUD_FS_MOUNT_PATH is set to the target path for mounting.
 #    For service functions for local hdfs, HEAD_ADDRESS is set.
 
+# Cloud storage fuse mounts:
+# 1. If cloud storage of provider configured:
+#   The cloud storage of provider mounts to CLOUD_FS_MOUNT_PATH
+#   Any cluster local storage mounts to LOCAL_FS_MOUNT_PATH
+# 2. If We are operating without cloud storage of provider:
+#   a. If there is remote cluster storage, it will mount to CLOUD_FS_MOUNT_PATH
+#      Any cluster local storage mounts to LOCAL_FS_MOUNT_PATH
+#   b. If there is no remote cluster storage
+#      Any cluster local storage mounts to CLOUD_FS_MOUNT_PATH
+
 # Configuring functions
 function configure_fuse_options() {
     FUSE_CONF_FILE="/etc/fuse.conf"
     FIND_STR="^user_allow_other"
     if [ `grep -c "$FIND_STR" $FUSE_CONF_FILE` -eq '0' ];then
         sudo sed -i '$auser_allow_other' $FUSE_CONF_FILE
     fi
@@ -23,15 +33,15 @@
           [ -d "$data_disk" ] || continue
           fuse_cache_dir=$data_disk
           break
       done
   fi
 
   if [ -z $fuse_cache_dir ]; then
-      fuse_cache_dir="/mnt/cache/"
+      fuse_cache_dir="/tmp/.cache"
   fi
   echo $fuse_cache_dir
 }
 
 function configure_local_hdfs_fs() {
     configure_fuse_options
 }
@@ -140,42 +150,52 @@
         chmod 600 ${USER_HOME}/.passwd-ossfs
     fi
 }
 
 function configure_cloud_fs() {
     sudo mkdir -p /cloudtik
     sudo chown $(whoami) /cloudtik
-    if [ "$HDFS_ENABLED" == "true" ]; then
-        configure_local_hdfs_fs
-    elif [ ! -z "${HDFS_NAMENODE_URI}" ]; then
-        configure_hdfs_fs
-    elif [ "$AWS_CLOUD_STORAGE" == "true" ]; then
+    # cloud storage from provider
+    if [ "$AWS_CLOUD_STORAGE" == "true" ]; then
         configure_s3_fs
     elif [ "$AZURE_CLOUD_STORAGE" == "true" ]; then
         configure_azure_blob_fs
     elif [ "$GCP_CLOUD_STORAGE" == "true" ]; then
         configure_gcs_fs
     elif [ "$ALIYUN_CLOUD_STORAGE" == "true" ]; then
         configure_aliyun_oss_fs
     fi
+
+    # cluster local storage
+    if [ "$HDFS_ENABLED" == "true" ]; then
+        configure_local_hdfs_fs
+    elif [ ! -z "${HDFS_NAMENODE_URI}" ]; then
+        configure_hdfs_fs
+    fi
 }
 
 # Installing functions
 function install_hdfs_fuse() {
-    if ! type fuse_dfs >/dev/null 2>&1;then
+    if ! type fuse_dfs >/dev/null 2>&1; then
         arch=$(uname -m)
-        sudo wget -q --show-progress https://d30257nes7d4fq.cloudfront.net/downloads/hadoop/fuse_dfs-${HADOOP_VERSION}-${arch} -O /usr/bin/fuse_dfs
-        sudo wget -q --show-progress https://d30257nes7d4fq.cloudfront.net/downloads/hadoop/fuse_dfs_wrapper-${HADOOP_VERSION}.sh -O /usr/bin/fuse_dfs_wrapper.sh
+        sudo wget -q --show-progress ${CLOUDTIK_DOWNLOADS}/hadoop/fuse_dfs-${HADOOP_VERSION}-${arch} -O /usr/bin/fuse_dfs
+        sudo wget -q --show-progress ${CLOUDTIK_DOWNLOADS}/hadoop/fuse_dfs_wrapper-${HADOOP_VERSION}.sh -O /usr/bin/fuse_dfs_wrapper.sh
         sudo chmod +x /usr/bin/fuse_dfs
         sudo chmod +x /usr/bin/fuse_dfs_wrapper.sh
     fi
+
+    # nfs mount may needed
+    which mount.nfs > /dev/null || sudo  apt-get -qq update -y > /dev/null; sudo DEBIAN_FRONTEND=noninteractive apt-get -qq install nfs-common -y > /dev/null
+
+    # install HDFS NFS fix if not installed
+    wget -q --show-progress ${CLOUDTIK_DOWNLOADS}/hadoop/hadoop-hdfs-nfs-${HADOOP_VERSION}.jar -O ${HADOOP_HOME}/share/hadoop/hdfs/hadoop-hdfs-nfs-${HADOOP_VERSION}.jar
 }
 
 function install_s3_fuse() {
-    if ! type s3fs >/dev/null 2>&1;then
+    if ! type s3fs >/dev/null 2>&1; then
         echo "Installing S3 Fuse..."
         sudo apt-get -qq update -y > /dev/null
         sudo apt-get install -qq s3fs -y > /dev/null
     fi
 }
 
 function install_azure_blob_fuse() {
@@ -208,45 +228,102 @@
         sudo apt-get install -qq gdebi-core -y > /dev/null
         sudo gdebi --q --n ${OSS_PACKAGE} > /dev/null
         rm ${OSS_PACKAGE}
     fi
 }
 
 function install_cloud_fuse() {
-    if [ "$HDFS_ENABLED" == "true" ]; then
-        install_hdfs_fuse
-    elif [ ! -z "${HDFS_NAMENODE_URI}" ]; then
-        install_hdfs_fuse
-    elif [ "$AWS_CLOUD_STORAGE" == "true" ]; then
+    # cloud storage from provider
+    if [ "$AWS_CLOUD_STORAGE" == "true" ]; then
         install_s3_fuse
     elif [ "$AZURE_CLOUD_STORAGE" == "true" ]; then
         install_azure_blob_fuse
     elif [ "$GCP_CLOUD_STORAGE" == "true" ]; then
         install_gcs_fuse
     elif [ "$ALIYUN_CLOUD_STORAGE" == "true" ]; then
         install_aliyun_oss_fuse
     fi
+
+    # cluster local storage
+    if [ "$HDFS_ENABLED" == "true" ]; then
+        install_hdfs_fuse
+    elif [ ! -z "${HDFS_NAMENODE_URI}" ]; then
+        install_hdfs_fuse
+    fi
 }
 
 # Service functions
 
 function mount_local_hdfs_fs() {
     fs_default_dir="dfs://${HEAD_ADDRESS}:9000"
-    # Mount local hdfs fuse here
-    mkdir -p ${CLOUD_FS_MOUNT_PATH}
-    echo "Mounting HDFS ${fs_default_dir} to ${CLOUD_FS_MOUNT_PATH}..."
-    fuse_dfs_wrapper.sh -oinitchecks ${fs_default_dir} ${CLOUD_FS_MOUNT_PATH} > /dev/null
+    if [ -z "${MOUNTED_CLOUD_FS}" ]; then
+        FS_MOUNT_PATH=${CLOUD_FS_MOUNT_PATH}
+        MOUNTED_CLOUD_FS=${FS_MOUNT_PATH}
+    else
+        FS_MOUNT_PATH=${LOCAL_FS_MOUNT_PATH}
+    fi
+    # Mount local hdfs here
+    mkdir -p ${FS_MOUNT_PATH}
+
+    # only one NFS Gateway per node is supported
+    if [ "${HDFS_NFS_MOUNTED}" != "true" ] && [ "${HDFS_MOUNT_METHOD}" == "nfs" ]; then
+        HDFS_NFS_MOUNTED=true
+
+        # Use the local HDFS dedicated core-site.xml and hdfs-site.xml
+        LOCAL_HDFS_CONF_DIR=${HADOOP_HOME}/etc/local
+        if [ -d "${LOCAL_HDFS_CONF_DIR}" ]; then
+            export HADOOP_CONF_DIR=${LOCAL_HDFS_CONF_DIR}
+        fi
+
+        echo "Staring HDFS NFS Gateway..."
+        # Please note that portmap needs to run with root privilege
+        sudo -E ${HADOOP_HOME}/bin/hdfs --daemon start portmap
+        ${HADOOP_HOME}/bin/hdfs --daemon start nfs3
+        sleep 3
+
+        echo "Mounting HDFS ${fs_default_dir} with NFS Gateway ${CLOUDTIK_NODE_IP} to ${FS_MOUNT_PATH}..."
+        sudo mount -t nfs -o vers=3,proto=tcp,nolock,noacl,sync ${CLOUDTIK_NODE_IP}:/ ${FS_MOUNT_PATH}
+    else
+        echo "Mounting HDFS ${fs_default_dir} with fuse to ${FS_MOUNT_PATH}..."
+        fuse_dfs_wrapper.sh -oinitchecks ${fs_default_dir} ${FS_MOUNT_PATH} > /dev/null
+    fi
 }
 
 function mount_hdfs_fs() {
     fs_default_dir="${HDFS_NAMENODE_URI:1}"
-    # Mount remote hdfs fuse here
-    mkdir -p ${CLOUD_FS_MOUNT_PATH}
-    echo "Mounting HDFS ${fs_default_dir} to ${CLOUD_FS_MOUNT_PATH}..."
-    fuse_dfs_wrapper.sh -oinitchecks ${fs_default_dir} ${CLOUD_FS_MOUNT_PATH} > /dev/null
+    if [ -z "${MOUNTED_CLOUD_FS}" ]; then
+        FS_MOUNT_PATH=${CLOUD_FS_MOUNT_PATH}
+        MOUNTED_CLOUD_FS=${FS_MOUNT_PATH}
+    else
+        FS_MOUNT_PATH=${LOCAL_FS_MOUNT_PATH}
+    fi
+    # Mount remote hdfs here
+    mkdir -p ${FS_MOUNT_PATH}
+
+    # only one NFS Gateway per node is supported
+    if [ "${HDFS_NFS_MOUNTED}" != "true" ] &&[ "${HDFS_MOUNT_METHOD}" == "nfs" ]; then
+        HDFS_NFS_MOUNTED=true
+
+        # Use the remote HDFS dedicated core-site.xml and hdfs-site.xml
+        REMOTE_HDFS_CONF_DIR=${HADOOP_HOME}/etc/remote
+        if [ -d "${REMOTE_HDFS_CONF_DIR}" ]; then
+            export HADOOP_CONF_DIR=${REMOTE_HDFS_CONF_DIR}
+        fi
+
+        echo "Staring HDFS NFS Gateway..."
+        $HADOOP_HOME/bin/hdfs --daemon start portmap
+        $HADOOP_HOME/bin/hdfs --daemon start nfs3
+        sleep 3
+
+        echo "Mounting HDFS ${fs_default_dir} with NFS Gateway ${CLOUDTIK_NODE_IP} to ${FS_MOUNT_PATH}..."
+        sudo mount -t nfs -o vers=3,proto=tcp,nolock,noacl,sync ${CLOUDTIK_NODE_IP}:/ ${FS_MOUNT_PATH}
+    else
+        echo "Mounting HDFS ${fs_default_dir} with fuse to ${FS_MOUNT_PATH}..."
+        fuse_dfs_wrapper.sh -oinitchecks ${fs_default_dir} ${FS_MOUNT_PATH} > /dev/null
+    fi
 }
 
 function mount_s3_fs() {
     if [ -z "${AWS_S3_BUCKET}" ]; then
         echo "AWS_S3_BUCKET environment variable is not set."
         return
     fi
@@ -255,14 +332,15 @@
     if [ -z "${AWS_S3_ACCESS_KEY_ID}" ] || [ -z "${AWS_S3_SECRET_ACCESS_KEY}" ]; then
         IAM_FLAG="-o iam_role=auto"
     fi
 
     mkdir -p ${CLOUD_FS_MOUNT_PATH}
     echo "Mounting S3 bucket ${AWS_S3_BUCKET} to ${CLOUD_FS_MOUNT_PATH}..."
     s3fs ${AWS_S3_BUCKET} -o use_cache=/tmp -o mp_umask=002 -o multireq_max=5 ${IAM_FLAG} ${CLOUD_FS_MOUNT_PATH} > /dev/null
+    MOUNTED_CLOUD_FS=${CLOUD_FS_MOUNT_PATH}
 }
 
 function mount_azure_blob_fs() {
     if [ -z "${AZURE_CONTAINER}" ]; then
         echo "AZURE_CONTAINER environment variable is not set."
         return
     fi
@@ -276,25 +354,27 @@
         echo "AZURE_STORAGE_ACCOUNT environment variable is not set."
         return
     fi
 
     mkdir -p ${CLOUD_FS_MOUNT_PATH}
     echo "Mounting Azure blob container ${AZURE_CONTAINER}@${AZURE_STORAGE_ACCOUNT} to ${CLOUD_FS_MOUNT_PATH}..."
     blobfuse2 mount ${CLOUD_FS_MOUNT_PATH} --config-file=${USER_HOME}/blobfuse2_config.yaml > /dev/null
+    MOUNTED_CLOUD_FS=${CLOUD_FS_MOUNT_PATH}
 }
 
 function mount_gcs_fs() {
     if [ ! -n "${GCP_GCS_BUCKET}" ]; then
         echo "GCP_GCS_BUCKET environment variable is not set."
         return
     fi
 
     mkdir -p ${CLOUD_FS_MOUNT_PATH}
     echo "Mounting GCS bucket ${GCP_GCS_BUCKET} to ${CLOUD_FS_MOUNT_PATH}..."
     gcsfuse ${GCP_GCS_BUCKET} ${CLOUD_FS_MOUNT_PATH} > /dev/null
+    MOUNTED_CLOUD_FS=${CLOUD_FS_MOUNT_PATH}
 }
 
 function mount_aliyun_oss_fs() {
     if [ -z "${ALIYUN_OSS_BUCKET}" ]; then
         echo "ALIYUN_OSS_BUCKET environment variable is not set."
         return
     fi
@@ -312,29 +392,75 @@
         RAM_ROLE_FLAG="-o ram_role=http://100.100.100.200/latest/meta-data/ram/security-credentials/${ALIYUN_ECS_RAM_ROLE_NAME}"
     fi
 
     mkdir -p ${CLOUD_FS_MOUNT_PATH}
     echo "Mounting Aliyun OSS bucket ${ALIYUN_OSS_BUCKET} to ${CLOUD_FS_MOUNT_PATH}..."
     # TODO: Endpoint setup for ECS for network going internally (for example, oss-cn-hangzhou-internal.aliyuncs.com)
     ossfs ${ALIYUN_OSS_BUCKET} ${CLOUD_FS_MOUNT_PATH} -o use_cache=/tmp -o mp_umask=002 -o url=${ALIYUN_OSS_INTERNAL_ENDPOINT} ${PASSWD_FILE_FLAG} ${RAM_ROLE_FLAG} > /dev/null
+    MOUNTED_CLOUD_FS=${CLOUD_FS_MOUNT_PATH}
 }
 
 function mount_cloud_fs() {
-    if [ "$HDFS_ENABLED" == "true" ]; then
-        mount_local_hdfs_fs
-    elif [ ! -z "${HDFS_NAMENODE_URI}" ]; then
-        mount_hdfs_fs
-    elif [ "$AWS_CLOUD_STORAGE" == "true" ]; then
+    MOUNTED_CLOUD_FS=""
+    # cloud storage from provider
+    if [ "$AWS_CLOUD_STORAGE" == "true" ]; then
         mount_s3_fs
     elif [ "$AZURE_CLOUD_STORAGE" == "true" ]; then
         mount_azure_blob_fs
     elif [ "$GCP_CLOUD_STORAGE" == "true" ]; then
         mount_gcs_fs
     elif [ "$ALIYUN_CLOUD_STORAGE" == "true" ]; then
         mount_aliyun_oss_fs
     fi
+
+    # cluster local storage
+    HDFS_NFS_MOUNTED=false
+    if [ -z "${MOUNTED_CLOUD_FS}" ]; then
+        # cluster local storage from remote cluster
+        if [ ! -z "${HDFS_NAMENODE_URI}" ]; then
+            mount_hdfs_fs
+        fi
+        # cluster local storage from local cluster
+        if [ "$HDFS_ENABLED" == "true" ]; then
+            mount_local_hdfs_fs
+        fi
+    else
+        if [ ! -z "${HDFS_NAMENODE_URI}" ]; then
+            mount_hdfs_fs
+        elif [ "$HDFS_ENABLED" == "true" ]; then
+            mount_local_hdfs_fs
+        fi
+    fi
+}
+
+
+function unmount_fs() {
+    local fs_mount_path="$1"
+    if findmnt -o fstype -l -n ${fs_mount_path} >/dev/null 2>&1; then
+        echo "Unmounting cloud fs at ${fs_mount_path}..."
+        local fstype=$(findmnt -o fstype -l -n ${fs_mount_path})
+        if [ "${fstype}" == "nfs" ]; then
+            sudo umount -f ${fs_mount_path} > /dev/null
+
+            # stopping the NFS gateway services
+            ${HADOOP_HOME}/bin/hdfs --daemon stop nfs3
+            # Please note that portmap needs to run with root privilege
+            sudo -E ${HADOOP_HOME}/bin/hdfs --daemon stop portmap
+        else
+            fusermount -u ${fs_mount_path} > /dev/null
+        fi
+    fi
 }
 
 function unmount_cloud_fs() {
-    echo "Unmounting cloud fs at ${CLOUD_FS_MOUNT_PATH}..."
-    fusermount -u ${CLOUD_FS_MOUNT_PATH} > /dev/null
+    # use findmnt to check the existence and type of the mount
+    # if findmnt doesn't exist, install it
+    which findmnt > /dev/null || sudo  apt-get -qq update -y > /dev/null; sudo DEBIAN_FRONTEND=noninteractive apt-get -qq install util-linux -y > /dev/null
+
+    if [ "${CLOUD_FS_MOUNT_PATH}" != "" ]; then
+        unmount_fs "${CLOUD_FS_MOUNT_PATH}"
+    fi
+
+    if [ "${LOCAL_FS_MOUNT_PATH}" != "" ]; then
+        unmount_fs "${LOCAL_FS_MOUNT_PATH}"
+    fi
 }
```

## cloudtik/runtime/common/scripts/hadoop-cloud-credential.sh

```diff
@@ -201,11 +201,12 @@
     elif [ "${cloud_storage_provider}" == "gcp" ]; then
         update_credential_config_for_gcp
     elif [ "${cloud_storage_provider}" == "aliyun" ]; then
         update_credential_config_for_aliyun
     elif [ "${cloud_storage_provider}" == "huaweicloud" ]; then
         update_credential_config_for_huaweicloud
     fi
+
     if [  -f "$HADOOP_CREDENTIAL_TMP_FILE"  ]; then
         cp  ${HADOOP_CREDENTIAL_TMP_FILE} ${HADOOP_CREDENTIAL_FILE}
     fi
 }
```

## cloudtik/runtime/common/scripts/hadoop-install.sh

```diff
@@ -23,17 +23,18 @@
         hadoop_download_url="http://archive.apache.org/dist/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}${arch_hadoop}.tar.gz"
 
         (cd $RUNTIME_PATH && wget -q --show-progress ${hadoop_download_url} -O hadoop.tar.gz && \
             mkdir -p "$HADOOP_HOME" && \
             tar --extract --file hadoop.tar.gz --directory "$HADOOP_HOME" --strip-components 1 --no-same-owner && \
             rm hadoop.tar.gz && \
             wget -q --show-progress -nc -P "${HADOOP_HOME}/share/hadoop/tools/lib" https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-latest.jar && \
-            wget -q --show-progress https://d30257nes7d4fq.cloudfront.net/downloads/hadoop/hadoop-azure-${HADOOP_VERSION}.jar -O $HADOOP_HOME/share/hadoop/tools/lib/hadoop-azure-${HADOOP_VERSION}.jar && \
-            wget -q --show-progress https://d30257nes7d4fq.cloudfront.net/downloads/hadoop/hadoop-aliyun-${HADOOP_VERSION}.jar -O $HADOOP_HOME/share/hadoop/tools/lib/hadoop-aliyun-${HADOOP_VERSION}.jar && \
-            wget -q --show-progress https://d30257nes7d4fq.cloudfront.net/downloads/hadoop/hadoop-huaweicloud-${HADOOP_VERSION}.jar -O $HADOOP_HOME/share/hadoop/tools/lib/hadoop-huaweicloud-${HADOOP_VERSION}.jar \
+            wget -q --show-progress ${CLOUDTIK_DOWNLOADS}/hadoop/hadoop-azure-${HADOOP_VERSION}.jar -O $HADOOP_HOME/share/hadoop/tools/lib/hadoop-azure-${HADOOP_VERSION}.jar && \
+            wget -q --show-progress ${CLOUDTIK_DOWNLOADS}/hadoop/hadoop-aliyun-${HADOOP_VERSION}.jar -O $HADOOP_HOME/share/hadoop/tools/lib/hadoop-aliyun-${HADOOP_VERSION}.jar && \
+            wget -q --show-progress ${CLOUDTIK_DOWNLOADS}/hadoop/hadoop-huaweicloud-${HADOOP_VERSION}.jar -O $HADOOP_HOME/share/hadoop/tools/lib/hadoop-huaweicloud-${HADOOP_VERSION}.jar && \
+            wget -q --show-progress ${CLOUDTIK_DOWNLOADS}/hadoop/hadoop-hdfs-nfs-${HADOOP_VERSION}.jar -O ${HADOOP_HOME}/share/hadoop/hdfs/hadoop-hdfs-nfs-${HADOOP_VERSION}.jar \
             )
         echo "export HADOOP_HOME=$HADOOP_HOME">> ${USER_HOME}/.bashrc
         echo "export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop">> ${USER_HOME}/.bashrc
         echo "export PATH=\$HADOOP_HOME/bin:\$PATH" >> ${USER_HOME}/.bashrc
         echo "export JAVA_HOME=$JAVA_HOME" >> ${HADOOP_HOME}/etc/hadoop/hadoop-env.sh
         #Add share/hadoop/tools/lib/* into classpath
         echo "export HADOOP_CLASSPATH=\$HADOOP_CLASSPATH:\$HADOOP_HOME/share/hadoop/tools/lib/*" >> ${HADOOP_HOME}/etc/hadoop/hadoop-env.sh
```

## cloudtik/runtime/common/scripts/util-functions.sh

```diff
@@ -1,9 +1,12 @@
 #!/bin/bash
 
+# global variables
+CLOUDTIK_DOWNLOADS="https://d30257nes7d4fq.cloudfront.net/downloads"
+
 function set_head_address() {
     if [ -z "${HEAD_ADDRESS}" ]; then
         if [ $IS_HEAD_NODE == "true" ]; then
             if [ ! -n "${CLOUDTIK_NODE_IP}" ]; then
                 HEAD_ADDRESS=$(hostname -I | awk '{print $1}')
             else
                 HEAD_ADDRESS=${CLOUDTIK_NODE_IP}
@@ -64,7 +67,34 @@
 }
 
 function clean_install_cache() {
     (sudo rm -rf /var/lib/apt/lists/* \
         && sudo apt-get clean \
         && which conda > /dev/null && conda clean -itqy)
 }
+
+function get_data_disk_dirs() {
+    local data_disk_dirs=""
+    if [ -d "/mnt/cloudtik" ]; then
+        for data_disk in /mnt/cloudtik/*; do
+            [ -d "$data_disk" ] || continue
+            if [ -z "$data_disk_dirs" ]; then
+                data_disk_dirs=$data_disk
+            else
+                data_disk_dirs="$data_disk_dirs,$data_disk"
+            fi
+        done
+    fi
+    echo "${data_disk_dirs}"
+}
+
+function get_any_data_disk_dir() {
+    local data_disk_dir=""
+    if [ -d "/mnt/cloudtik" ]; then
+        for data_disk in /mnt/cloudtik/*; do
+            [ -d "$data_disk" ] || continue
+            data_disk_dir=$data_disk
+            break
+        done
+    fi
+    echo "${data_disk_dir}"
+}
```

## cloudtik/runtime/flink/api.py

```diff
@@ -1,25 +1,31 @@
 """IMPORTANT: this is an experimental interface and not currently stable."""
 
-from typing import Union
+from typing import Union, Optional
 
 from cloudtik.core.api import Cluster, ThisCluster
 from cloudtik.runtime.flink.utils import request_rest_jobs, request_rest_yarn, get_runtime_default_storage, \
     get_runtime_services
 
 
 class FlinkCluster(Cluster):
-    def __init__(self, cluster_config: Union[dict, str], should_bootstrap: bool = True) -> None:
+    def __init__(
+            self, cluster_config: Union[dict, str],
+            should_bootstrap: bool = True,
+            no_config_cache: bool = True,
+            verbosity: Optional[int] = None) -> None:
         """Create a Flink cluster object to operate on with this API.
 
         Args:
             cluster_config (Union[str, dict]): Either the config dict of the
                 cluster, or a path pointing to a file containing the config.
         """
-        Cluster.__init__(self, cluster_config, should_bootstrap)
+        super().__init__(
+            cluster_config, should_bootstrap,
+            no_config_cache, verbosity)
 
     def jobs(self, endpoint: str):
         """Make a rest request to Flink History Server
 
         Args:
             endpoint (str): The Spark history server rest endpoint to request
         """
@@ -37,17 +43,17 @@
         return get_runtime_default_storage(self.config)
 
     def get_services(self):
         return get_runtime_services(self.config, "")
 
 
 class ThisFlinkCluster(ThisCluster):
-    def __init__(self) -> None:
+    def __init__(self, verbosity: Optional[int] = None) -> None:
         """Create a Flink cluster object to operate on with this API on head."""
-        ThisCluster.__init__(self)
+        super().__init__(verbosity)
 
     def jobs(self, endpoint: str):
         """Make a rest request to Flink History Server
 
         Args:
             endpoint (str): The Spark history server rest endpoint to request
         """
```

## cloudtik/runtime/flink/utils.py

```diff
@@ -21,14 +21,16 @@
     # The forth element, if node, the process should on all nodes,if head, the process should on head node.
     ["proc_resourcemanager", False, "ResourceManager", "head"],
     ["org.apache.flink.runtime.webmonitor.history.HistoryServer", False, "FlinkHistoryServer", "head"],
     ["proc_nodemanager", False, "NodeManager", "worker"],
 ]
 
 FLINK_RUNTIME_CONFIG_KEY = "flink"
+FLINK_HDFS_NAMENODE_URI_KEY = "hdfs_namenode_uri"
+FLINK_HIVE_METASTORE_URI_KEY = "hive_metastore_uri"
 
 YARN_RESOURCE_MEMORY_RATIO = 0.8
 
 FLINK_TASKMANAGER_MEMORY_RATIO = 1
 FLINK_JOBMANAGER_MEMORY_RATIO = 0.02
 FLINK_JOBMANAGER_MEMORY_MINIMUM = 1024
 FLINK_JOBMANAGER_MEMORY_MAXIMUM = 8192
@@ -112,32 +114,32 @@
     if FLINK_RUNTIME_CONFIG_KEY not in runtime_config:
         runtime_config[FLINK_RUNTIME_CONFIG_KEY] = {}
     flink_config = runtime_config[FLINK_RUNTIME_CONFIG_KEY]
 
     workspace_provider = _get_workspace_provider(cluster_config["provider"], workspace_name)
     global_variables = workspace_provider.subscribe_global_variables(cluster_config)
 
+    # We now support co-existence of local HDFS and remote HDFS
     # 1) Try to use local hdfs first;
     # 2) Try to use defined hdfs_namenode_uri;
     # 3) If subscribed_hdfs_namenode_uri=true,try to subscribe global variables to find remote hdfs_namenode_uri
 
-    if not is_runtime_enabled(runtime_config, "hdfs"):
-        if flink_config.get("hdfs_namenode_uri") is None:
-            if flink_config.get("auto_detect_hdfs", False):
-                hdfs_namenode_uri = global_variables.get("hdfs-namenode-uri")
-                if hdfs_namenode_uri is not None:
-                    flink_config["hdfs_namenode_uri"] = hdfs_namenode_uri
+    if flink_config.get(FLINK_HDFS_NAMENODE_URI_KEY) is None:
+        if flink_config.get("auto_detect_hdfs", False):
+            hdfs_namenode_uri = global_variables.get("hdfs-namenode-uri")
+            if hdfs_namenode_uri is not None:
+                flink_config[FLINK_HDFS_NAMENODE_URI_KEY] = hdfs_namenode_uri
 
     # Check metastore
-    if not is_runtime_enabled(runtime_config, "metastore"):
-        if flink_config.get("hive_metastore_uri") is None:
+    if not is_runtime_enabled(runtime_config, BUILT_IN_RUNTIME_METASTORE):
+        if flink_config.get(FLINK_HIVE_METASTORE_URI_KEY) is None:
             if flink_config.get("auto_detect_metastore", True):
                 hive_metastore_uri = global_variables.get("hive-metastore-uri")
                 if hive_metastore_uri is not None:
-                    flink_config["hive_metastore_uri"] = hive_metastore_uri
+                    flink_config[FLINK_HIVE_METASTORE_URI_KEY] = hive_metastore_uri
 
     return cluster_config
 
 
 def _config_runtime_resources(cluster_config: Dict[str, Any]) -> Dict[str, Any]:
     cluster_resource = _get_cluster_resources(cluster_config)
     container_resource = {"yarn_container_maximum_vcores": cluster_resource["worker_cpu"]}
@@ -229,14 +231,20 @@
     # Merge with the user configurations
     flink_conf.update(flink_config)
 
     # Write back the configuration file
     save_properties_file(flink_conf_file, flink_conf, separator=': ', comments=comments)
 
 
+def _with_hadoop_default(spark_config, runtime_envs):
+    hadoop_default_cluster = spark_config.get("hadoop_default_cluster", False)
+    if hadoop_default_cluster:
+        runtime_envs["HADOOP_DEFAULT_CLUSTER"] = hadoop_default_cluster
+
+
 def _with_runtime_environment_variables(runtime_config, config, provider, node_id: str):
     runtime_envs = {}
     flink_config = runtime_config.get(FLINK_RUNTIME_CONFIG_KEY, {})
     cluster_runtime_config = config.get(RUNTIME_CONFIG_KEY)
 
     # export yarn memory ratio to use if configured by user
     yarn_resource_memory_ratio = flink_config.get("yarn_resource_memory_ratio")
@@ -244,34 +252,36 @@
         runtime_envs["YARN_RESOURCE_MEMORY_RATIO"] = yarn_resource_memory_ratio
 
     # export yarn scheduler
     yarn_scheduler = flink_config.get("yarn_scheduler")
     if yarn_scheduler:
         runtime_envs["YARN_SCHEDULER"] = yarn_scheduler
 
+    _with_hadoop_default(flink_config, runtime_envs)
+
+    # We now support co-existence of local HDFS and remote HDFS, and cloud storage
     # 1) Try to use local hdfs first;
     # 2) Try to use defined hdfs_namenode_uri;
     # 3) Try to use provider storage;
     if is_runtime_enabled(cluster_runtime_config, BUILT_IN_RUNTIME_HDFS):
         runtime_envs["HDFS_ENABLED"] = True
-    else:
-        if flink_config.get("hdfs_namenode_uri") is not None:
-            runtime_envs["HDFS_NAMENODE_URI"] = flink_config.get("hdfs_namenode_uri")
-
-        # We always export the cloud storage even for remote HDFS case
-        node_type_config = get_node_type_config(config, provider, node_id)
-        provider_envs = provider.with_environment_variables(node_type_config, node_id)
-        runtime_envs.update(provider_envs)
+    if flink_config.get(FLINK_HDFS_NAMENODE_URI_KEY) is not None:
+        runtime_envs["HDFS_NAMENODE_URI"] = flink_config.get(FLINK_HDFS_NAMENODE_URI_KEY)
+
+    # We always export the cloud storage even for HDFS case
+    node_type_config = get_node_type_config(config, provider, node_id)
+    provider_envs = provider.with_environment_variables(node_type_config, node_id)
+    runtime_envs.update(provider_envs)
 
     # 1) Try to use local metastore if there is one started;
     # 2) Try to use defined metastore_uri;
     if is_runtime_enabled(cluster_runtime_config, BUILT_IN_RUNTIME_METASTORE):
         runtime_envs["METASTORE_ENABLED"] = True
-    elif flink_config.get("hive_metastore_uri") is not None:
-        runtime_envs["HIVE_METASTORE_URI"] = flink_config.get("hive_metastore_uri")
+    elif flink_config.get(FLINK_HIVE_METASTORE_URI_KEY) is not None:
+        runtime_envs["HIVE_METASTORE_URI"] = flink_config.get(FLINK_HIVE_METASTORE_URI_KEY)
     return runtime_envs
 
 
 def get_runtime_logs():
     hadoop_logs_dir = os.path.join(os.getenv("HADOOP_HOME"), "logs")
     flink_logs_dir = os.path.join(os.getenv("FLINK_HOME"), "logs")
     jupyter_logs_dir = os.path.join(os.getenv("HOME"), "runtime", "jupyter", "logs")
@@ -279,21 +289,29 @@
                 "flink": flink_logs_dir,
                 "jupyter": jupyter_logs_dir
                 }
     return all_logs
 
 
 def _validate_config(config: Dict[str, Any]):
+    runtime_config = config.get(RUNTIME_CONFIG_KEY)
+
     # if HDFS enabled, we ignore the cloud storage configurations
-    if not is_runtime_enabled(config.get(RUNTIME_CONFIG_KEY), "hdfs"):
-        # Check any cloud storage is configured
-        provider_config = config["provider"]
-        if ("storage" not in provider_config) and \
-                not is_use_managed_cloud_storage(config):
-            raise ValueError("No storage configuration found for Flink.")
+    if is_runtime_enabled(runtime_config, BUILT_IN_RUNTIME_HDFS):
+        return
+    # check if there is remote HDFS configured
+    flink_config = runtime_config.get(FLINK_RUNTIME_CONFIG_KEY, {})
+    if flink_config.get(FLINK_HDFS_NAMENODE_URI_KEY) is not None:
+        return
+
+    # Check any cloud storage is configured
+    provider_config = config["provider"]
+    if ("storage" not in provider_config) and \
+            not is_use_managed_cloud_storage(config):
+        raise ValueError("No storage configuration found for Flink.")
 
 
 def _get_runtime_services(cluster_head_ip):
     services = {
         "yarn-web": {
             "name": "Yarn Web UI",
             "url": "http://{}:{}".format(cluster_head_ip, FLINK_YARN_WEB_API_PORT)
```

## cloudtik/runtime/flink/scripts/configure.sh

```diff
@@ -81,14 +81,19 @@
     sed -i "s!{%flink.state.checkpoints.dir%}!${checkpoints_dir}!g" `grep "{%flink.state.checkpoints.dir%}" -rl ./`
     sed -i "s!{%flink.state.savepoints.dir%}!${savepoints_dir}!g" `grep "{%flink.state.savepoints.dir%}" -rl ./`
     sed -i "s!{%flink.historyserver.archive.fs.dir%}!${historyserver_archive_dir}!g" `grep "{%flink.historyserver.archive.fs.dir%}" -rl ./`
 }
 
 function update_config_for_local_hdfs() {
     fs_default_dir="hdfs://${HEAD_ADDRESS}:9000"
+    sed -i "s!{%fs.default.name%}!${fs_default_dir}!g" `grep "{%fs.default.name%}" -rl ./`
+
+    # Still update credential config for cloud provider storage in the case of explict usage
+    update_cloud_storage_credential_config
+
     checkpoints_dir="${fs_default_dir}/${PATH_CHECKPOINTS}"
     savepoints_dir="${fs_default_dir}/${PATH_SAVEPOINTS}"
     historyserver_archive_dir="${fs_default_dir}/${PATH_HISTORY_SERVER}"
 
     update_config_for_flink_dirs
 }
 
@@ -214,48 +219,55 @@
         savepoints_dir="${fs_default_dir}/${PATH_SAVEPOINTS}"
         historyserver_archive_dir="${fs_default_dir}/${PATH_HISTORY_SERVER}"
     fi
 
     update_config_for_flink_dirs
 }
 
-function update_config_for_remote_storage() {
-    if [ "$HDFS_STORAGE" == "true" ]; then
-        update_config_for_hdfs
-    elif [ "${cloud_storage_provider}" == "aws" ]; then
+function update_config_for_hadoop_storage() {
+    if [ "${HADOOP_DEFAULT_CLUSTER}" == "true" ]; then
+        if [ "$HDFS_STORAGE" == "true" ]; then
+            update_config_for_hdfs
+            return 0
+        elif [ "$HDFS_ENABLED" == "true" ]; then
+            update_config_for_local_hdfs
+            return 0
+        fi
+    fi
+    if [ "${cloud_storage_provider}" == "aws" ]; then
         update_config_for_aws
     elif [ "${cloud_storage_provider}" == "azure" ]; then
         update_config_for_azure
     elif [ "${cloud_storage_provider}" == "gcp" ]; then
         update_config_for_gcp
     elif [ "${cloud_storage_provider}" == "aliyun" ]; then
         update_config_for_aliyun
     elif [ "${cloud_storage_provider}" == "huaweicloud" ]; then
         update_config_for_huaweicloud
+    elif [ "$HDFS_STORAGE" == "true" ]; then
+        update_config_for_hdfs
+    elif [ "$HDFS_ENABLED" == "true" ]; then
+        update_config_for_local_hdfs
     fi
 }
 
 function update_config_for_storage() {
     PATH_CHECKPOINTS="shared/flink-checkpoints"
     PATH_SAVEPOINTS="shared/flink-savepoints"
     PATH_HISTORY_SERVER="shared/history-server"
 
-    if [ "$HDFS_ENABLED" == "true" ];then
-        update_config_for_local_hdfs
-    else
-        check_hdfs_storage
-        set_cloud_storage_provider
-        update_config_for_remote_storage
+    check_hdfs_storage
+    set_cloud_storage_provider
+    update_config_for_hadoop_storage
 
-        if [ "${cloud_storage_provider}" != "none" ];then
-            cp -r ${output_dir}/hadoop/${cloud_storage_provider}/core-site.xml  ${HADOOP_HOME}/etc/hadoop/
-        else
-            # Possible remote hdfs without cloud storage
-            cp -r ${output_dir}/hadoop/core-site.xml  ${HADOOP_HOME}/etc/hadoop/
-        fi
+    if [ "${cloud_storage_provider}" != "none" ];then
+        cp -r ${output_dir}/hadoop/${cloud_storage_provider}/core-site.xml  ${HADOOP_HOME}/etc/hadoop/
+    else
+        # Possible hdfs without cloud storage
+        cp -r ${output_dir}/hadoop/core-site.xml  ${HADOOP_HOME}/etc/hadoop/
     fi
 }
 
 function update_yarn_config() {
     yarn_scheduler_class="org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"
     if [ ${YARN_SCHEDULER} == "fair" ];then
         yarn_scheduler_class="org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"
@@ -353,27 +365,14 @@
 
     cp -r ${output_dir}/hadoop/yarn-site.xml  ${HADOOP_HOME}/etc/hadoop/
 
     if [ $IS_HEAD_NODE == "true" ];then
         # update_metastore_config
 
         cp -r ${output_dir}/flink/*  ${FLINK_HOME}/conf
-
-        if [ "$HDFS_ENABLED" == "true" ]; then
-            # Create dirs on hdfs
-            ${HADOOP_HOME}/bin/hdfs --loglevel WARN --daemon start namenode
-            ${HADOOP_HOME}/bin/hadoop --loglevel WARN fs -mkdir -p /${PATH_CHECKPOINTS}
-            ${HADOOP_HOME}/bin/hadoop --loglevel WARN fs -mkdir -p /${PATH_SAVEPOINTS}
-            ${HADOOP_HOME}/bin/hdfs --loglevel WARN --daemon stop namenode
-        else
-            # Create dirs on cloud storage if needed
-            # This needs to be done after hadoop file system has been configured correctly
-            ${HADOOP_HOME}/bin/hadoop --loglevel WARN fs -mkdir -p /${PATH_CHECKPOINTS}
-            ${HADOOP_HOME}/bin/hadoop --loglevel WARN fs -mkdir -p /${PATH_SAVEPOINTS}
-        fi
     fi
 }
 
 function configure_jupyter() {
   if [ $IS_HEAD_NODE == "true" ]; then
       echo Y | jupyter lab --generate-config;
       # Set default password(cloudtik) for JupyterLab
```

## cloudtik/runtime/flink/scripts/install.sh

 * *Ordering differences only*

```diff
@@ -15,23 +15,23 @@
 # Set Hadoop version based on Flink version
 export HADOOP_VERSION=3.3.1
 
 export USER_HOME=/home/$(whoami)
 export RUNTIME_PATH=$USER_HOME/runtime
 mkdir -p $RUNTIME_PATH
 
+# Util functions
+. "$ROOT_DIR"/common/scripts/util-functions.sh
+
 # JDK install function
 . "$ROOT_DIR"/common/scripts/jdk-install.sh
 
 # Hadoop install function
 . "$ROOT_DIR"/common/scripts/hadoop-install.sh
 
-# Util functions
-. "$ROOT_DIR"/common/scripts/util-functions.sh
-
 function install_flink() {
     # install Flink
     export FLINK_HOME=$RUNTIME_PATH/flink
 
     if [ ! -d "${FLINK_HOME}" ]; then
      (cd $RUNTIME_PATH && wget -q --show-progress https://dlcdn.apache.org/flink/flink-${FLINK_VERSION}/flink-${FLINK_VERSION}-bin-scala_2.12.tgz -O flink.tgz && \
         mkdir -p "$FLINK_HOME" && \
```

## cloudtik/runtime/flink/scripts/services.sh

```diff
@@ -25,14 +25,20 @@
 
 set_head_option "$@"
 set_service_command "$@"
 
 case "$SERVICE_COMMAND" in
 start)
     if [ $IS_HEAD_NODE == "true" ]; then
+
+        # Create dirs on cloud storage if needed
+        # This needs to be done after hadoop file system has been configured correctly
+        ${HADOOP_HOME}/bin/hadoop --loglevel WARN fs -mkdir -p /shared/flink-checkpoints
+        ${HADOOP_HOME}/bin/hadoop --loglevel WARN fs -mkdir -p /shared/flink-savepoints
+
         echo "Starting Resource Manager..."
         $HADOOP_HOME/bin/yarn --daemon start resourcemanager
         echo "Starting Flink History Server..."
         # Make sure HADOOP_CLASSPATH is set
         export HADOOP_CLASSPATH=`$HADOOP_HOME/bin/hadoop classpath`
         $FLINK_HOME/bin/historyserver.sh start > /dev/null
         echo "Starting Jupyter..."
```

## cloudtik/runtime/ganglia/scripts/install.sh

```diff
@@ -11,15 +11,15 @@
 . "$ROOT_DIR"/common/scripts/util-functions.sh
 
 function install_ganglia_monitor_python() {
     GANGLIA_LIB=/usr/lib/ganglia
     GANGLIA_MODULE_PYTHON=${GANGLIA_LIB}/modpython.so
     if [ ! -f "${GANGLIA_MODULE_PYTHON}" ]; then
         arch=$(uname -m)
-        wget -q https://d30257nes7d4fq.cloudfront.net/downloads/ganglia/modpython-${arch}.so -O /tmp/modpython.so
+        wget -q ${CLOUDTIK_DOWNLOADS}/ganglia/modpython-${arch}.so -O /tmp/modpython.so
         sudo cp /tmp/modpython.so ${GANGLIA_LIB} && sudo chmod 644 ${GANGLIA_MODULE_PYTHON}
     fi
     sudo DEBIAN_FRONTEND=noninteractive apt-get -qq install -y ganglia-monitor-python python2.7-dev > /dev/null
     which hwinfo > /dev/null || sudo DEBIAN_FRONTEND=noninteractive apt-get -qq install -y hwinfo > /dev/null
 }
 
 function install_ganglia_server() {
```

## cloudtik/runtime/hdfs/conf/hadoop/hdfs-site.xml

### cloudtik/runtime/hdfs/conf/hadoop/hdfs-site.xml

```diff
@@ -59,8 +59,15 @@
     <name>dfs.datanode.socket.write.timeout</name>
     <value>1200000</value>
   </property>
   <property>
     <name>dfs.block.size</name>
     <value>268435456</value>
   </property>
+  <property>
+    <name>dfs.namenode.accesstime.precision</name>
+    <value>3600000</value>
+    <description>The access time for HDFS file is precise up to this value.
+          The default value is 1 hour. Setting a value of 0 disables
+          access times for HDFS.</description>
+  </property>
 </configuration>
```

## cloudtik/runtime/hdfs/scripts/configure.sh

```diff
@@ -17,16 +17,14 @@
 
 function prepare_base_conf() {
     source_dir=$(cd $(dirname ${BASH_SOURCE[0]})/..;pwd)/conf
     output_dir=/tmp/hdfs/conf
     rm -rf  $output_dir
     mkdir -p $output_dir
     cp -r $source_dir/* $output_dir
-    # Include hadoop config file for cloud providers
-    cp -r "$ROOT_DIR"/common/conf/hadoop $output_dir
 }
 
 function check_hadoop_installed() {
     if [ ! -n "${HADOOP_HOME}" ]; then
         echo "HADOOP_HOME environment variable is not set."
         exit 1
     fi
@@ -53,43 +51,54 @@
         sudo rm -rf "${HADOOP_HOME}/data/dfs/dn"
         hdfs_dn_dirs="${HADOOP_HOME}/data/dfs/dn"
     fi
     sed -i "s!{%dfs.namenode.name.dir%}!${hdfs_nn_dirs}!g" `grep "{%dfs.namenode.name.dir%}" -rl ./`
     sed -i "s!{%dfs.datanode.data.dir%}!${hdfs_dn_dirs}!g" `grep "{%dfs.datanode.data.dir%}" -rl ./`
 }
 
-function update_cloud_storage_credential_config() {
-    set_cloud_storage_provider
+function update_proxy_user_for_current_user() {
+    CURRENT_SYSTEM_USER=$(whoami)
 
-    # update hadoop credential config
-    update_credential_config_for_provider
-
-    if [ "${cloud_storage_provider}" != "none" ];then
-        cp -r ${output_dir}/hadoop/${cloud_storage_provider}/core-site.xml ${HADOOP_HOME}/etc/hadoop/
+    if [ "${CURRENT_SYSTEM_USER}" != "root" ]; then
+        HADOOP_PROXY_USER_PROPERTIES="<property>\n\
+        <name>hadoop.proxyuser.${CURRENT_SYSTEM_USER}.groups</name>\n\
+        <value>*</value>\n\
+    </property>\n\
+    <property>\n\
+        <name>hadoop.proxyuser.${CURRENT_SYSTEM_USER}.hosts</name>\n\
+        <value>*</value>\n\
+    </property>"
+        sed -i "s#{%hadoop.proxyuser.properties%}#${HADOOP_PROXY_USER_PROPERTIES}#g" `grep "{%hadoop.proxyuser.properties%}" -rl ./`
     else
-        cp -r ${output_dir}/hadoop/core-site.xml ${HADOOP_HOME}/etc/hadoop/
+        sed -i "s#{%hadoop.proxyuser.properties%}#""#g" `grep "{%hadoop.proxyuser.properties%}" -rl ./`
     fi
 }
 
 function configure_hdfs() {
     prepare_base_conf
     mkdir -p ${HADOOP_HOME}/logs
     cd $output_dir
 
     fs_default_dir="hdfs://${HEAD_ADDRESS}:9000"
     sed -i "s!{%fs.default.name%}!${fs_default_dir}!g" `grep "{%fs.default.name%}" -rl ./`
 
-    # update hadoop credential config
-    update_cloud_storage_credential_config
-
+    update_proxy_user_for_current_user
     update_hdfs_data_disks_config
 
-    cp -r ${output_dir}/hadoop/hdfs-site.xml  ${HADOOP_HOME}/etc/hadoop/
+    HDFS_CONF_DIR=${HADOOP_HOME}/etc/hdfs
+    # copy the existing hadoop conf
+    mkdir -p ${HDFS_CONF_DIR}
+    cp -r  ${HADOOP_HOME}/etc/hadoop/* ${HDFS_CONF_DIR}/
+    # override hdfs conf
+    cp -r ${output_dir}/hadoop/core-site.xml ${HDFS_CONF_DIR}/
+    cp -r ${output_dir}/hadoop/hdfs-site.xml  ${HDFS_CONF_DIR}/
 
     if [ $IS_HEAD_NODE == "true" ];then
+        # TODO: format only once if there is no force format flag
+        export HADOOP_CONF_DIR=${HDFS_CONF_DIR}
         # Stop namenode in case it was running left from last try
         ${HADOOP_HOME}/bin/hdfs --daemon stop namenode > /dev/null 2>&1
         # Format hdfs once
         ${HADOOP_HOME}/bin/hdfs --loglevel WARN namenode -format -force
     fi
 }
```

## cloudtik/runtime/hdfs/scripts/install.sh

 * *Ordering differences only*

```diff
@@ -9,20 +9,20 @@
 
 export HADOOP_VERSION=3.3.1
 
 export USER_HOME=/home/$(whoami)
 export RUNTIME_PATH=$USER_HOME/runtime
 mkdir -p $RUNTIME_PATH
 
+# Util functions
+. "$ROOT_DIR"/common/scripts/util-functions.sh
+
 # JDK install function
 . "$ROOT_DIR"/common/scripts/jdk-install.sh
 
 # Hadoop install function
 . "$ROOT_DIR"/common/scripts/hadoop-install.sh
 
-# Util functions
-. "$ROOT_DIR"/common/scripts/util-functions.sh
-
 set_head_option "$@"
 install_jdk
 install_hadoop
 clean_install_cache
```

## cloudtik/runtime/hdfs/scripts/services.sh

```diff
@@ -14,14 +14,17 @@
     echo "HADOOP_HOME environment variable is not set."
     exit 1
 fi
 
 set_head_option "$@"
 set_service_command "$@"
 
+# HDFS use its own conf dir
+export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hdfs
+
 case "$SERVICE_COMMAND" in
 start)
     if [ $IS_HEAD_NODE == "true" ]; then
         $HADOOP_HOME/bin/hdfs --daemon start namenode
     else
         $HADOOP_HOME/bin/hdfs --daemon start datanode
     fi
```

## cloudtik/runtime/metastore/scripts/install.sh

 * *Ordering differences only*

```diff
@@ -10,23 +10,23 @@
 export HADOOP_VERSION=3.3.1
 export HIVE_VERSION=3.1.2
 
 export USER_HOME=/home/$(whoami)
 export RUNTIME_PATH=$USER_HOME/runtime
 mkdir -p $RUNTIME_PATH
 
+# Util functions
+. "$ROOT_DIR"/common/scripts/util-functions.sh
+
 # JDK install function
 . "$ROOT_DIR"/common/scripts/jdk-install.sh
 
 # Hadoop install function
 . "$ROOT_DIR"/common/scripts/hadoop-install.sh
 
-# Util functions
-. "$ROOT_DIR"/common/scripts/util-functions.sh
-
 function install_mariadb() {
     sudo apt-get -qq update -y > /dev/null
     sudo DEBIAN_FRONTEND=noninteractive apt-get -qq install -y mariadb-server > /dev/null
 }
 
 function install_hive_metastore() {
     # install hive metastore
```

## cloudtik/runtime/presto/utils.py

```diff
@@ -12,14 +12,15 @@
     # The second element, if True, is to filter ps results by command name.
     # The third element is the process name.
     # The forth element, if node, the process should on all nodes,if head, the process should on head node.
     ["com.facebook.presto.server.PrestoServer", False, "PrestoServer", "node"],
 ]
 
 PRESTO_RUNTIME_CONFIG_KEY = "presto"
+PRESTO_HIVE_METASTORE_URI_KEY = "hive_metastore_uri"
 
 JVM_MAX_MEMORY_RATIO = 0.8
 QUERY_MAX_MEMORY_PER_NODE_RATIO = 0.5
 QUERY_MAX_TOTAL_MEMORY_PER_NODE_RATIO = 0.7
 MEMORY_HEAP_HEADROOM_PER_NODE_RATIO = 0.25
 
 
@@ -50,19 +51,19 @@
     presto_config = runtime_config[PRESTO_RUNTIME_CONFIG_KEY]
 
     workspace_provider = _get_workspace_provider(cluster_config["provider"], workspace_name)
     global_variables = workspace_provider.subscribe_global_variables(cluster_config)
 
     # Check metastore
     if not is_runtime_enabled(runtime_config, "metastore"):
-        if presto_config.get("hive_metastore_uri") is None:
+        if presto_config.get(PRESTO_HIVE_METASTORE_URI_KEY) is None:
             if presto_config.get("auto_detect_metastore", True):
                 hive_metastore_uri = global_variables.get("hive-metastore-uri")
                 if hive_metastore_uri is not None:
-                    presto_config["hive_metastore_uri"] = hive_metastore_uri
+                    presto_config[PRESTO_HIVE_METASTORE_URI_KEY] = hive_metastore_uri
 
     return cluster_config
 
 
 def _get_runtime_processes():
     return RUNTIME_PROCESSES
 
@@ -84,16 +85,16 @@
     presto_config = runtime_config.get(PRESTO_RUNTIME_CONFIG_KEY, {})
     cluster_runtime_config = config.get(RUNTIME_CONFIG_KEY)
 
     # 1) Try to use local metastore if there is one started;
     # 2) Try to use defined metastore_uri;
     if is_runtime_enabled(cluster_runtime_config, BUILT_IN_RUNTIME_METASTORE):
         runtime_envs["METASTORE_ENABLED"] = True
-    elif presto_config.get("hive_metastore_uri") is not None:
-        runtime_envs["HIVE_METASTORE_URI"] = presto_config.get("hive_metastore_uri")
+    elif presto_config.get(PRESTO_HIVE_METASTORE_URI_KEY) is not None:
+        runtime_envs["HIVE_METASTORE_URI"] = presto_config.get(PRESTO_HIVE_METASTORE_URI_KEY)
 
     _with_memory_configurations(
         runtime_envs, presto_config=presto_config,
         config=config, provider=provider, node_id=node_id)
 
     # We need export the cloud storage
     node_type_config = get_node_type_config(config, provider, node_id)
```

## cloudtik/runtime/spark/api.py

```diff
@@ -1,25 +1,31 @@
 """IMPORTANT: this is an experimental interface and not currently stable."""
 
-from typing import Union
+from typing import Union, Optional
 
 from cloudtik.core.api import Cluster, ThisCluster
 from cloudtik.runtime.spark.utils import request_rest_applications, request_rest_yarn, get_runtime_default_storage, \
     get_runtime_services
 
 
 class SparkCluster(Cluster):
-    def __init__(self, cluster_config: Union[dict, str], should_bootstrap: bool = True) -> None:
+    def __init__(
+            self, cluster_config: Union[dict, str],
+            should_bootstrap: bool = True,
+            no_config_cache: bool = True,
+            verbosity: Optional[int] = None) -> None:
         """Create a Spark cluster object to operate on with this API.
 
         Args:
             cluster_config (Union[str, dict]): Either the config dict of the
                 cluster, or a path pointing to a file containing the config.
         """
-        Cluster.__init__(self, cluster_config, should_bootstrap)
+        super().__init__(
+            cluster_config, should_bootstrap,
+            no_config_cache, verbosity)
 
     def applications(self, endpoint: str):
         """Make a rest request to Spark History Server
 
         Args:
             endpoint (str): The Spark history server rest endpoint to request
         """
@@ -37,17 +43,17 @@
         return get_runtime_default_storage(self.config)
 
     def get_services(self):
         return get_runtime_services(self.config)
 
 
 class ThisSparkCluster(ThisCluster):
-    def __init__(self) -> None:
+    def __init__(self, verbosity: Optional[int] = None) -> None:
         """Create a Spark cluster object to operate on with this API on head."""
-        ThisCluster.__init__(self)
+        super().__init__(verbosity)
 
     def applications(self, endpoint: str):
         """Make a rest request to Spark History Server
 
         Args:
             endpoint (str): The Spark history server rest endpoint to request
         """
```

## cloudtik/runtime/spark/utils.py

```diff
@@ -24,14 +24,16 @@
     # The forth element, if node, the process should on all nodes,if head, the process should on head node.
     ["proc_resourcemanager", False, "ResourceManager", "head"],
     ["org.apache.spark.deploy.history.HistoryServer", False, "SparkHistoryServer", "head"],
     ["proc_nodemanager", False, "NodeManager", "worker"],
 ]
 
 SPARK_RUNTIME_CONFIG_KEY = "spark"
+SPARK_HDFS_NAMENODE_URI_KEY = "hdfs_namenode_uri"
+SPARK_HIVE_METASTORE_URI_KEY = "hive_metastore_uri"
 
 YARN_RESOURCE_MEMORY_RATIO = 0.8
 SPARK_EXECUTOR_MEMORY_RATIO = 1
 SPARK_DRIVER_MEMORY_RATIO = 0.1
 SPARK_APP_MASTER_MEMORY_RATIO = 0.02
 SPARK_DRIVER_MEMORY_MINIMUM = 1024
 SPARK_DRIVER_MEMORY_MAXIMUM = 8192
@@ -126,32 +128,32 @@
     if SPARK_RUNTIME_CONFIG_KEY not in runtime_config:
         runtime_config[SPARK_RUNTIME_CONFIG_KEY] = {}
     spark_config = runtime_config[SPARK_RUNTIME_CONFIG_KEY]
 
     workspace_provider = _get_workspace_provider(cluster_config["provider"], workspace_name)
     global_variables = workspace_provider.subscribe_global_variables(cluster_config)
 
+    # We now support co-existence of local HDFS and remote HDFS
     # 1) Try to use local hdfs first;
     # 2) Try to use defined hdfs_namenode_uri;
     # 3) If subscribed_hdfs_namenode_uri=true,try to subscribe global variables to find remote hdfs_namenode_uri
 
-    if not is_runtime_enabled(runtime_config, "hdfs"):
-        if spark_config.get("hdfs_namenode_uri") is None:
-            if spark_config.get("auto_detect_hdfs", False):
-                hdfs_namenode_uri = global_variables.get("hdfs-namenode-uri")
-                if hdfs_namenode_uri is not None:
-                    spark_config["hdfs_namenode_uri"] = hdfs_namenode_uri
+    if spark_config.get(SPARK_HDFS_NAMENODE_URI_KEY) is None:
+        if spark_config.get("auto_detect_hdfs", False):
+            hdfs_namenode_uri = global_variables.get("hdfs-namenode-uri")
+            if hdfs_namenode_uri is not None:
+                spark_config[SPARK_HDFS_NAMENODE_URI_KEY] = hdfs_namenode_uri
 
     # Check metastore
-    if not is_runtime_enabled(runtime_config, "metastore"):
-        if spark_config.get("hive_metastore_uri") is None:
+    if not is_runtime_enabled(runtime_config, BUILT_IN_RUNTIME_METASTORE):
+        if spark_config.get(SPARK_HIVE_METASTORE_URI_KEY) is None:
             if spark_config.get("auto_detect_metastore", True):
                 hive_metastore_uri = global_variables.get("hive-metastore-uri")
                 if hive_metastore_uri is not None:
-                    spark_config["hive_metastore_uri"] = hive_metastore_uri
+                    spark_config[SPARK_HIVE_METASTORE_URI_KEY] = hive_metastore_uri
 
     return cluster_config
 
 
 def _config_runtime_resources(cluster_config: Dict[str, Any]) -> Dict[str, Any]:
     cluster_resource = _get_cluster_resources(cluster_config)
     worker_cpu = cluster_resource["worker_cpu"]
@@ -264,14 +266,26 @@
     # Merge with the user configurations
     spark_conf.update(spark_config)
 
     # Write back the configuration file
     save_properties_file(spark_conf_file, spark_conf, separator=' ', comments=comments)
 
 
+def _with_hdfs_mount_method(spark_config, runtime_envs):
+    mount_method = spark_config.get("hdfs_mount_method")
+    if mount_method:
+        runtime_envs["HDFS_MOUNT_METHOD"] = mount_method
+
+
+def _with_hadoop_default(spark_config, runtime_envs):
+    hadoop_default_cluster = spark_config.get("hadoop_default_cluster", False)
+    if hadoop_default_cluster:
+        runtime_envs["HADOOP_DEFAULT_CLUSTER"] = hadoop_default_cluster
+
+
 def _with_runtime_environment_variables(runtime_config, config, provider, node_id: str):
     runtime_envs = {}
     spark_config = runtime_config.get(SPARK_RUNTIME_CONFIG_KEY, {})
     cluster_runtime_config = config.get(RUNTIME_CONFIG_KEY)
 
     # export yarn memory ratio to use if configured by user
     yarn_resource_memory_ratio = spark_config.get("yarn_resource_memory_ratio")
@@ -279,34 +293,39 @@
         runtime_envs["YARN_RESOURCE_MEMORY_RATIO"] = yarn_resource_memory_ratio
 
     # export yarn scheduler
     yarn_scheduler = spark_config.get("yarn_scheduler")
     if yarn_scheduler:
         runtime_envs["YARN_SCHEDULER"] = yarn_scheduler
 
+    _with_hadoop_default(spark_config, runtime_envs)
+
+    # We now support co-existence of local HDFS and remote HDFS, and cloud storage
     # 1) Try to use local hdfs first;
     # 2) Try to use defined hdfs_namenode_uri;
     # 3) Try to use provider storage;
     if is_runtime_enabled(cluster_runtime_config, BUILT_IN_RUNTIME_HDFS):
         runtime_envs["HDFS_ENABLED"] = True
-    else:
-        if spark_config.get("hdfs_namenode_uri") is not None:
-            runtime_envs["HDFS_NAMENODE_URI"] = spark_config.get("hdfs_namenode_uri")
-
-        # We always export the cloud storage even for remote HDFS case
-        node_type_config = get_node_type_config(config, provider, node_id)
-        provider_envs = provider.with_environment_variables(node_type_config, node_id)
-        runtime_envs.update(provider_envs)
+        _with_hdfs_mount_method(spark_config, runtime_envs)
+
+    if spark_config.get(SPARK_HDFS_NAMENODE_URI_KEY) is not None:
+        runtime_envs["HDFS_NAMENODE_URI"] = spark_config.get(SPARK_HDFS_NAMENODE_URI_KEY)
+        _with_hdfs_mount_method(spark_config, runtime_envs)
+
+    # We always export the cloud storage even for HDFS case
+    node_type_config = get_node_type_config(config, provider, node_id)
+    provider_envs = provider.with_environment_variables(node_type_config, node_id)
+    runtime_envs.update(provider_envs)
 
     # 1) Try to use local metastore if there is one started;
     # 2) Try to use defined metastore_uri;
     if is_runtime_enabled(cluster_runtime_config, BUILT_IN_RUNTIME_METASTORE):
         runtime_envs["METASTORE_ENABLED"] = True
-    elif spark_config.get("hive_metastore_uri") is not None:
-        runtime_envs["HIVE_METASTORE_URI"] = spark_config.get("hive_metastore_uri")
+    elif spark_config.get(SPARK_HIVE_METASTORE_URI_KEY) is not None:
+        runtime_envs["HIVE_METASTORE_URI"] = spark_config.get(SPARK_HIVE_METASTORE_URI_KEY)
     return runtime_envs
 
 
 def get_runtime_logs():
     hadoop_logs_dir = os.path.join(os.getenv("HADOOP_HOME"), "logs")
     spark_logs_dir = os.path.join(os.getenv("SPARK_HOME"), "logs")
     jupyter_logs_dir = os.path.join(os.getenv("HOME"), "runtime", "jupyter", "logs")
@@ -314,21 +333,29 @@
                 "spark": spark_logs_dir,
                 "jupyter": jupyter_logs_dir
                 }
     return all_logs
 
 
 def _validate_config(config: Dict[str, Any]):
+    runtime_config = config.get(RUNTIME_CONFIG_KEY)
+
     # if HDFS enabled, we ignore the cloud storage configurations
-    if not is_runtime_enabled(config.get(RUNTIME_CONFIG_KEY), "hdfs"):
-        # Check any cloud storage is configured
-        provider_config = config["provider"]
-        if ("storage" not in provider_config) and \
-                not is_use_managed_cloud_storage(config):
-            raise ValueError("No storage configuration found for Spark.")
+    if is_runtime_enabled(runtime_config, BUILT_IN_RUNTIME_HDFS):
+        return
+    # check if there is remote HDFS configured
+    spark_config = runtime_config.get(SPARK_RUNTIME_CONFIG_KEY, {})
+    if spark_config.get(SPARK_HDFS_NAMENODE_URI_KEY) is not None:
+        return
+
+    # Check any cloud storage is configured
+    provider_config = config["provider"]
+    if ("storage" not in provider_config) and \
+            not is_use_managed_cloud_storage(config):
+        raise ValueError("No storage configuration found for Spark.")
 
 
 def _get_runtime_services(cluster_head_ip):
     services = {
         "yarn-web": {
             "name": "Yarn Web UI",
             "url": "http://{}:{}".format(cluster_head_ip, SPARK_YARN_WEB_API_PORT)
```

## cloudtik/runtime/spark/scripts/configure.sh

```diff
@@ -6,31 +6,38 @@
 
 args=$(getopt -a -o h:: -l head:: -- "$@")
 eval set -- "${args}"
 
 USER_HOME=/home/$(whoami)
 RUNTIME_PATH=$USER_HOME/runtime
 
+# Util functions
+. "$ROOT_DIR"/common/scripts/util-functions.sh
+
 # Hadoop cloud credential configuration functions
 . "$ROOT_DIR"/common/scripts/hadoop-cloud-credential.sh
 
 # Cloud storage fuse functions
 . "$ROOT_DIR"/common/scripts/cloud-storage-fuse.sh
 
-# Util functions
-. "$ROOT_DIR"/common/scripts/util-functions.sh
-
 function prepare_base_conf() {
     source_dir=$(cd $(dirname ${BASH_SOURCE[0]})/..;pwd)/conf
     output_dir=/tmp/spark/conf
     rm -rf  $output_dir
     mkdir -p $output_dir
     cp -r $source_dir/* $output_dir
     # Include hadoop config file for cloud providers
     cp -r "$ROOT_DIR"/common/conf/hadoop $output_dir
+    # Make copy for local and remote HDFS
+    cp $output_dir/hadoop/core-site.xml $output_dir/hadoop/core-site-local.xml
+    sed -i "s!{%fs.default.name%}!{%local.fs.default.name%}!g" $output_dir/hadoop/core-site-local.xml
+    cp $output_dir/hadoop/core-site.xml $output_dir/hadoop/core-site-remote.xml
+    sed -i "s!{%fs.default.name%}!{%remote.fs.default.name%}!g" $output_dir/hadoop/core-site-remote.xml
+
+    cd $output_dir
 }
 
 function check_spark_installed() {
     if [ ! -n "${HADOOP_HOME}" ]; then
         echo "HADOOP_HOME environment variable is not set."
         exit 1
     fi
@@ -97,14 +104,19 @@
 function update_config_for_spark_dirs() {
     sed -i "s!{%spark.eventLog.dir%}!${event_log_dir}!g" `grep "{%spark.eventLog.dir%}" -rl ./`
     sed -i "s!{%spark.sql.warehouse.dir%}!${sql_warehouse_dir}!g" `grep "{%spark.sql.warehouse.dir%}" -rl ./`
 }
 
 function update_config_for_local_hdfs() {
     fs_default_dir="hdfs://${HEAD_ADDRESS}:9000"
+    sed -i "s!{%fs.default.name%}!${fs_default_dir}!g" `grep "{%fs.default.name%}" -rl ./`
+
+    # Still update credential config for cloud provider storage in the case of explict usage
+    update_cloud_storage_credential_config
+
     # event log dir
     event_log_dir="${fs_default_dir}/shared/spark-events"
     sql_warehouse_dir="${fs_default_dir}/shared/spark-warehouse"
 
     update_config_for_spark_dirs
 }
 
@@ -221,44 +233,102 @@
         event_log_dir="${fs_default_dir}/shared/spark-events"
         sql_warehouse_dir="${fs_default_dir}/shared/spark-warehouse"
     fi
 
     update_config_for_spark_dirs
 }
 
-function update_config_for_remote_storage() {
-    if [ "$HDFS_STORAGE" == "true" ]; then
-        update_config_for_hdfs
-    elif [ "${cloud_storage_provider}" == "aws" ]; then
+function update_config_for_hadoop_storage() {
+    if [ "${HADOOP_DEFAULT_CLUSTER}" == "true" ]; then
+        if [ "$HDFS_STORAGE" == "true" ]; then
+            update_config_for_hdfs
+            return 0
+        elif [ "$HDFS_ENABLED" == "true" ]; then
+            update_config_for_local_hdfs
+            return 0
+        fi
+    fi
+    if [ "${cloud_storage_provider}" == "aws" ]; then
         update_config_for_aws
     elif [ "${cloud_storage_provider}" == "azure" ]; then
         update_config_for_azure
     elif [ "${cloud_storage_provider}" == "gcp" ]; then
         update_config_for_gcp
     elif [ "${cloud_storage_provider}" == "aliyun" ]; then
         update_config_for_aliyun
     elif [ "${cloud_storage_provider}" == "huaweicloud" ]; then
         update_config_for_huaweicloud
+    elif [ "$HDFS_STORAGE" == "true" ]; then
+        update_config_for_hdfs
+    elif [ "$HDFS_ENABLED" == "true" ]; then
+        update_config_for_local_hdfs
     fi
 }
 
-function update_config_for_storage() {
-    if [ "$HDFS_ENABLED" == "true" ];then
-        update_config_for_local_hdfs
+function update_nfs_dump_dir() {
+    # set nfs gateway dump dir
+    data_disk_dir=$(get_any_data_disk_dir)
+    if [ -z "$data_disk_dir" ]; then
+        nfs_dump_dir="/tmp/.hdfs-nfs"
     else
-        check_hdfs_storage
-        set_cloud_storage_provider
-        update_config_for_remote_storage
+        nfs_dump_dir="$data_disk_dir/tmp/.hdfs-nfs"
+    fi
+    sed -i "s!{%dfs.nfs3.dump.dir%}!${nfs_dump_dir}!g" `grep "{%dfs.nfs3.dump.dir%}" -rl ./`
+}
 
-        if [ "${cloud_storage_provider}" != "none" ];then
-            cp -r ${output_dir}/hadoop/${cloud_storage_provider}/core-site.xml ${HADOOP_HOME}/etc/hadoop/
-        else
-            # Possible remote hdfs without cloud storage
-            cp -r ${output_dir}/hadoop/core-site.xml ${HADOOP_HOME}/etc/hadoop/
-        fi
+function update_local_storage_config_remote_hdfs() {
+    REMOTE_HDFS_CONF_DIR=${HADOOP_HOME}/etc/remote
+    # copy the existing hadoop conf
+    mkdir -p ${REMOTE_HDFS_CONF_DIR}
+    cp -r  ${HADOOP_HOME}/etc/hadoop/* ${REMOTE_HDFS_CONF_DIR}/
+
+    fs_default_dir="${HDFS_NAMENODE_URI}"
+    sed -i "s!{%remote.fs.default.name%}!${fs_default_dir}!g" ${output_dir}/hadoop/core-site-remote.xml
+
+    # override with remote hdfs conf
+    cp ${output_dir}/hadoop/core-site-remote.xml ${REMOTE_HDFS_CONF_DIR}/core-site.xml
+    cp -r ${output_dir}/hadoop/hdfs-site.xml  ${REMOTE_HDFS_CONF_DIR}/
+}
+
+function update_local_storage_config_local_hdfs() {
+    LOCAL_HDFS_CONF_DIR=${HADOOP_HOME}/etc/local
+    # copy the existing hadoop conf
+    mkdir -p ${LOCAL_HDFS_CONF_DIR}
+    cp -r  ${HADOOP_HOME}/etc/hadoop/* ${LOCAL_HDFS_CONF_DIR}/
+
+    fs_default_dir="hdfs://${HEAD_ADDRESS}:9000"
+    sed -i "s!{%local.fs.default.name%}!${fs_default_dir}!g" ${output_dir}/hadoop/core-site-local.xml
+
+    # override with local hdfs conf
+    cp ${output_dir}/hadoop/core-site-local.xml ${LOCAL_HDFS_CONF_DIR}/core-site.xml
+    cp -r ${output_dir}/hadoop/hdfs-site.xml  ${LOCAL_HDFS_CONF_DIR}/
+}
+
+function update_local_storage_config() {
+    update_nfs_dump_dir
+
+    if [ "${HDFS_STORAGE}" == "true" ]; then
+        update_local_storage_config_remote_hdfs
+    fi
+    if [ "${HDFS_ENABLED}" == "true" ]; then
+        update_local_storage_config_local_hdfs
+    fi
+}
+
+function update_config_for_storage() {
+    check_hdfs_storage
+    set_cloud_storage_provider
+    update_config_for_hadoop_storage
+    update_local_storage_config
+
+    if [ "${cloud_storage_provider}" != "none" ];then
+        cp -r ${output_dir}/hadoop/${cloud_storage_provider}/core-site.xml ${HADOOP_HOME}/etc/hadoop/
+    else
+        # hdfs without cloud storage
+        cp -r ${output_dir}/hadoop/core-site.xml ${HADOOP_HOME}/etc/hadoop/
     fi
 }
 
 function update_yarn_config() {
     yarn_scheduler_class="org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"
     if [ "${YARN_SCHEDULER}" == "fair" ];then
         yarn_scheduler_class="org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"
@@ -291,15 +361,15 @@
         for data_disk in /mnt/cloudtik/*; do
             [ -d "$data_disk" ] || continue
             if [ -z "$local_dirs" ]; then
                 local_dirs=$data_disk
             else
                 local_dirs="$local_dirs,$data_disk"
             fi
-      done
+        done
     fi
 
     # set nodemanager.local-dirs
     nodemanager_local_dirs=$local_dirs
     if [ -z "$nodemanager_local_dirs" ]; then
         nodemanager_local_dirs="${HADOOP_HOME}/data/nodemanager/local-dir"
     fi
@@ -342,39 +412,27 @@
     fi
 }
 
 function configure_hadoop_and_spark() {
     prepare_base_conf
     SPARK_DEFAULTS=${output_dir}/spark/spark-defaults.conf
 
-    cd $output_dir
     sed -i "s/HEAD_ADDRESS/${HEAD_ADDRESS}/g" `grep "HEAD_ADDRESS" -rl ./`
 
     update_yarn_config
     update_spark_runtime_config
     update_data_disks_config
     update_config_for_storage
 
     cp -r ${output_dir}/hadoop/yarn-site.xml ${HADOOP_HOME}/etc/hadoop/
 
     if [ $IS_HEAD_NODE == "true" ];then
         update_metastore_config
 
         cp -r ${output_dir}/spark/* ${SPARK_HOME}/conf
-
-        if [ "$HDFS_ENABLED" == "true" ]; then
-            # Create event log dir on hdfs
-            ${HADOOP_HOME}/bin/hdfs --loglevel WARN --daemon start namenode
-            ${HADOOP_HOME}/bin/hadoop --loglevel WARN fs -mkdir -p /shared/spark-events
-            ${HADOOP_HOME}/bin/hdfs --loglevel WARN --daemon stop namenode
-        else
-            # Create event log dir on cloud storage if needed
-            # This needs to be done after hadoop file system has been configured correctly
-            ${HADOOP_HOME}/bin/hadoop --loglevel WARN fs -mkdir -p /shared/spark-events
-        fi
     fi
 }
 
 function configure_jupyter_for_spark() {
   if [ $IS_HEAD_NODE == "true" ]; then
       echo Y | jupyter lab --generate-config;
       # Set default password(cloudtik) for JupyterLab
```

## cloudtik/runtime/spark/scripts/install.sh

 * *Ordering differences only*

```diff
@@ -15,26 +15,26 @@
 # Set Hadoop version based on Spark version
 export HADOOP_VERSION=3.3.1
 
 export USER_HOME=/home/$(whoami)
 export RUNTIME_PATH=$USER_HOME/runtime
 mkdir -p $RUNTIME_PATH
 
+# Util functions
+. "$ROOT_DIR"/common/scripts/util-functions.sh
+
 # JDK install function
 . "$ROOT_DIR"/common/scripts/jdk-install.sh
 
 # Hadoop install function
 . "$ROOT_DIR"/common/scripts/hadoop-install.sh
 
 # Cloud storage fuse functions
 . "$ROOT_DIR"/common/scripts/cloud-storage-fuse.sh
 
-# Util functions
-. "$ROOT_DIR"/common/scripts/util-functions.sh
-
 function install_spark() {
     # install Spark
     export SPARK_HOME=$RUNTIME_PATH/spark
 
     if [ ! -d "${SPARK_HOME}" ]; then
      (cd $RUNTIME_PATH && wget -q --show-progress https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.2.tgz -O spark.tgz && \
         mkdir -p "$SPARK_HOME" && \
```

## cloudtik/runtime/spark/scripts/services.sh

```diff
@@ -19,28 +19,36 @@
     echo "SPARK_HOME environment variable is not set."
     exit 1
 fi
 
 USER_HOME=/home/$(whoami)
 RUNTIME_PATH=$USER_HOME/runtime
 export CLOUD_FS_MOUNT_PATH=/cloudtik/fs
+export LOCAL_FS_MOUNT_PATH=/cloudtik/localfs
+
+# Util functions
+. "$ROOT_DIR"/common/scripts/util-functions.sh
 
 # Cloud storage fuse functions
 . "$ROOT_DIR"/common/scripts/cloud-storage-fuse.sh
 
 set_head_option "$@"
 set_service_command "$@"
 set_head_address
 
 case "$SERVICE_COMMAND" in
 start)
     # Mount cloud filesystem or hdfs
     mount_cloud_fs
 
     if [ $IS_HEAD_NODE == "true" ]; then
+        # Create event log dir on cloud storage if needed
+        # This needs to be done after hadoop file system has been configured correctly
+        ${HADOOP_HOME}/bin/hadoop --loglevel WARN fs -mkdir -p /shared/spark-events
+
         echo "Starting Resource Manager..."
         $HADOOP_HOME/bin/yarn --daemon start resourcemanager
         echo "Starting Spark History Server..."
         export SPARK_LOCAL_IP=${CLOUDTIK_NODE_IP}; $SPARK_HOME/sbin/start-history-server.sh > /dev/null
         echo "Starting Jupyter..."
         nohup jupyter lab --no-browser > $RUNTIME_PATH/jupyter/logs/jupyterlab.log 2>&1 &
     else
```

## cloudtik/runtime/sshserver/scripts/configure.sh

```diff
@@ -25,30 +25,36 @@
     mkdir -p $output_dir
     cp -r $source_dir/* $output_dir
 }
 
 function configure_ssh_server() {
     prepare_base_conf
 
+    sudo chown $(whoami):users ~/.ssh
+    sudo chmod 700 ~/.ssh
+
     if [ ! -f "${SSH_HOST_KEY_FILE}" ]; then
         # generate the host key pair
         ssh-keygen -b 2048 -t rsa -q -N "" -f ${SSH_HOST_KEY_FILE} && chmod 600 ${SSH_HOST_KEY_FILE}
     fi
 
     if [ -f "${SSH_AUTHORIZED_KEYS_FILE}" ]; then
-        chmod 600 ${SSH_AUTHORIZED_KEYS_FILE}
+        sudo chown $(whoami):users ${SSH_AUTHORIZED_KEYS_FILE}
+        sudo chmod 600 ${SSH_AUTHORIZED_KEYS_FILE}
     fi
 
     cp ${output_dir}/sshd_config ${SSH_SSHD_CONFIG_FILE}
 
     if [ "$IS_HEAD_NODE" == "true" ]; then
         # for head, configure the private key as the default key to login ~/.ssh/id_rsa
         # We don't overwrite the existing id_rsa. Any situation for problems?
         if [ -f "${BOOTSTRAP_PRIVATE_KEY_FILE}" ] && [ ! -f "${SSH_DEFAULT_PRIVATE_KEY_FILE}" ]; then
             cp ${BOOTSTRAP_PRIVATE_KEY_FILE} ${SSH_DEFAULT_PRIVATE_KEY_FILE}
+            sudo chown $(whoami):users ${SSH_DEFAULT_PRIVATE_KEY_FILE}
+            sudo chmod 600 ${SSH_DEFAULT_PRIVATE_KEY_FILE}
         fi
     fi
 }
 
 set_head_option "$@"
 set_head_address
 configure_ssh_server
```

## cloudtik/runtime/trino/scripts/install.sh

 * *Ordering differences only*

```diff
@@ -9,23 +9,23 @@
 
 export TRINO_VERSION=389
 
 export USER_HOME=/home/$(whoami)
 export RUNTIME_PATH=$USER_HOME/runtime
 mkdir -p $RUNTIME_PATH
 
+# Util functions
+. "$ROOT_DIR"/common/scripts/util-functions.sh
+
 # JDK install function
 . "$ROOT_DIR"/common/scripts/jdk-install.sh
 
 # Hadoop install function
 . "$ROOT_DIR"/common/scripts/hadoop-install.sh
 
-# Util functions
-. "$ROOT_DIR"/common/scripts/util-functions.sh
-
 function install_tools() {
     which uuid > /dev/null || sudo apt-get -qq update -y > /dev/null; sudo DEBIAN_FRONTEND=noninteractive apt-get -qq install uuid -y > /dev/null
 }
 
 function install_trino() {
     # install Trino
     export TRINO_HOME=$RUNTIME_PATH/trino
```

## cloudtik/scripts/head_scripts.py

```diff
@@ -8,15 +8,15 @@
                                                cli_logger)
 from cloudtik.core._private.cluster.cluster_operator import (
     debug_status_string, dump_cluster_on_head,
     RUN_ENV_TYPES, teardown_cluster_on_head, cluster_process_status_on_head, rsync_node_on_head, attach_node_on_head,
     start_node_on_head, stop_node_on_head, kill_node_on_head, scale_cluster_on_head,
     _wait_for_ready, _get_worker_node_ips, _get_head_node_ip,
     _show_cluster_status, _monitor_cluster, cli_call_context, _exec_node_on_head,
-    do_health_check, cluster_resource_metrics_on_head, show_info)
+    do_health_check, cluster_resource_metrics_on_head, show_info, _run_script_on_head)
 from cloudtik.core._private.constants import CLOUDTIK_REDIS_DEFAULT_PASSWORD
 from cloudtik.core._private.state import kv_store
 from cloudtik.core._private.state.kv_store import kv_initialize_with_address
 from cloudtik.core._private.utils import CLOUDTIK_CLUSTER_SCALING_ERROR, \
     CLOUDTIK_CLUSTER_SCALING_STATUS, get_head_bootstrap_config, \
     load_head_cluster_config, parse_bundles_json, parse_resources
 from cloudtik.scripts.utils import NaturalOrderGroup, add_command_alias
@@ -148,14 +148,58 @@
         wait_timeout=wait_timeout,
         port_forward=port_forward,
         with_output=with_output,
         parallel=parallel,
         job_waiter_name=job_waiter)
 
 
+@head.command(context_settings={"ignore_unknown_options": True})
+@click.option(
+    "--wait-for-workers",
+    is_flag=True,
+    default=False,
+    help="Whether wait for minimum number of workers to be ready.")
+@click.option(
+    "--min-workers",
+    required=False,
+    type=int,
+    help="The minimum number of workers to wait for ready.")
+@click.option(
+    "--wait-timeout",
+    required=False,
+    type=int,
+    help="The timeout seconds to wait for ready.")
+@click.option(
+    "--with-output",
+    is_flag=True,
+    default=False,
+    help="Whether to capture command output.")
+@click.argument("script", required=True, type=str)
+@click.argument("script_args", nargs=-1)
+@add_click_logging_options
+def run(wait_for_workers, min_workers, wait_timeout,
+        with_output, script, script_args):
+    """Runs a built-in script (bash or python or a registered command).
+
+    If you want to execute any commands or user scripts, use exec or submit.
+    """
+    config = load_head_cluster_config()
+    call_context = cli_call_context()
+
+    _run_script_on_head(
+        config=config,
+        call_context=call_context,
+        script=script,
+        script_args=script_args,
+        wait_for_workers=wait_for_workers,
+        min_workers=min_workers,
+        wait_timeout=wait_timeout,
+        with_output=with_output)
+
+
 @head.command()
 @click.option(
     "--yes",
     "-y",
     is_flag=True,
     default=False,
     help="Don't ask for confirmation.")
@@ -655,56 +699,54 @@
     "--tempfile",
     "-T",
     required=False,
     type=str,
     default=None,
     help="Temporary file to use")
 @click.option(
-    "--verbosity",
+    "--silent",
     required=False,
-    default=None,
-    type=int,
-    hidden=True,
-    help="The integer verbosity to set.")
+    type=bool,
+    is_flag=True,
+    default=False,
+    help="Whether print a warning message for cluster dump.")
 @add_click_logging_options
 def cluster_dump(hosts: Optional[str] = None,
                  stream: bool = False,
                  output: Optional[str] = None,
                  logs: bool = True,
                  debug_state: bool = True,
                  pip: bool = True,
                  processes: bool = True,
                  processes_verbose: bool = False,
                  tempfile: Optional[str] = None,
-                 verbosity: int = None):
+                 silent: bool = False,):
     """Collect cluster data and package into an archive on head.
 
         Usage:
 
             cloudtik head cluster-dump[--stream/--output file]
 
         This script is called on head node to fetch the cluster data.
         """
-    if verbosity is not None:
-        cli_logger.set_verbosity(verbosity)
 
     config = load_head_cluster_config()
     call_context = cli_call_context()
     dump_cluster_on_head(
         config, call_context,
         hosts=hosts,
         stream=stream,
         output=output,
         logs=logs,
         debug_state=debug_state,
         pip=pip,
         processes=processes,
         processes_verbose=processes_verbose,
         temp_file=tempfile,
-        verbosity=verbosity)
+        silent=silent)
 
 
 @click.group(cls=NaturalOrderGroup)
 def runtime():
     """
     Commands running on head for runtime control
     """
@@ -814,14 +856,15 @@
 runtime.add_command(start)
 runtime_add_command_alias(start, name="restart", hidden=True)
 runtime.add_command(stop)
 
 # commands running on head node
 head.add_command(attach)
 head.add_command(exec)
+head.add_command(run)
 head.add_command(scale)
 
 head.add_command(rsync_up)
 head.add_command(rsync_down)
 
 head.add_command(status)
 head.add_command(info)
```

## cloudtik/scripts/node_scripts.py

```diff
@@ -1,12 +1,11 @@
 import logging
 import os
 import subprocess
 import sys
-from shlex import quote
 from socket import socket
 from typing import Optional
 
 import click
 import psutil
 
 from cloudtik.core._private import services
@@ -17,15 +16,15 @@
 from cloudtik.core._private.constants import CLOUDTIK_PROCESSES, \
     CLOUDTIK_REDIS_DEFAULT_PASSWORD, \
     CLOUDTIK_DEFAULT_PORT
 from cloudtik.core._private.core_utils import get_cloudtik_temp_dir
 from cloudtik.core._private.node.node_services import NodeServicesStarter
 from cloudtik.core._private.parameter import StartParams
 from cloudtik.core._private.resource_spec import ResourceSpec
-from cloudtik.core._private.utils import with_script_args, parse_resources_json
+from cloudtik.core._private.utils import parse_resources_json, run_script
 from cloudtik.scripts.utils import NaturalOrderGroup
 
 logger = logging.getLogger(__name__)
 
 
 @click.group(cls=NaturalOrderGroup)
 def node():
@@ -364,14 +363,25 @@
     except OSError:
         # This just means the file doesn't exist.
         pass
     # Wait for the processes to actually stop.
     psutil.wait_procs(stopped, timeout=2)
 
 
+@node.command(context_settings={"ignore_unknown_options": True})
+@click.argument("script", required=True, type=str)
+@click.argument("script_args", nargs=-1)
+def run(script, script_args):
+    """Runs a built-in script (bash or python or a registered command).
+
+    If you want to execute any commands or user scripts, use exec or submit.
+    """
+    run_script(script, script_args)
+
+
 @node.command()
 @click.option(
     "--cpu",
     required=False,
     type=bool,
     is_flag=True,
     default=False,
@@ -451,78 +461,60 @@
     "--tempfile",
     "-T",
     required=False,
     type=str,
     default=None,
     help="Temporary file to use")
 @click.option(
-    "--verbosity",
-    required=False,
-    default=None,
-    type=int,
-    help="The integer verbosity to set")
-@click.option(
     "--runtimes",
     required=False,
     type=str,
     default=None,
     help="The list of runtimes to collect logs from")
+@click.option(
+    "--silent",
+    required=False,
+    type=bool,
+    is_flag=True,
+    default=False,
+    help="Whether print a informational message.")
 @add_click_logging_options
 def dump(
         stream: bool = False,
         output: Optional[str] = None,
         logs: bool = True,
         debug_state: bool = True,
         pip: bool = True,
         processes: bool = True,
         processes_verbose: bool = False,
         tempfile: Optional[str] = None,
-        verbosity: int = None,
-        runtimes: str = None):
+        runtimes: str = None,
+        silent: bool = False):
     """Collect local data and package into an archive.
 
     Usage:
 
         cloudtik node dump [--stream/--output file]
 
     This script is called on remote nodes to fetch their data.
     """
-    # This may stream data to stdout, so no printing here
-    if verbosity is not None:
-        cli_logger.set_verbosity(verbosity)
-
     dump_local(
         stream=stream,
         output=output,
         logs=logs,
         debug_state=debug_state,
         pip=pip,
         processes=processes,
         processes_verbose=processes_verbose,
         tempfile=tempfile,
         runtimes=runtimes,
-        verbosity=verbosity)
-
-
-@node.command(context_settings={"ignore_unknown_options": True})
-@click.argument("script", required=True, type=str)
-@click.argument("script_args", nargs=-1)
-def run_script(script, script_args):
-    """Runs a bash script within Cloudtik python package."""
-    root_path = os.path.abspath(os.path.dirname(os.path.dirname(__file__)))
-    target = os.path.join(root_path, script)
-    command_parts = ["bash", quote(target)]
-
-    with_script_args(command_parts, script_args)
-
-    final_cmd = " ".join(command_parts)
-    os.system(final_cmd)
+        silent=silent)
 
 
 # core commands running on head and worker node
 node.add_command(start)
 node.add_command(stop)
+node.add_command(run)
 node.add_command(resources)
 
-node.add_command(run_script)
 # utility commands running on head or worker node for dump local data
 node.add_command(dump)
```

## cloudtik/scripts/scripts.py

```diff
@@ -17,15 +17,15 @@
 from cloudtik.core._private.cluster.cluster_operator import (
     attach_cluster, create_or_update_cluster, monitor_cluster,
     teardown_cluster, get_head_node_ip, kill_node_from_head, get_worker_node_ips,
     dump_cluster, RUN_ENV_TYPES,
     show_cluster_status, start_ssh_proxy, stop_ssh_proxy, cluster_debug_status,
     cluster_health_check, cluster_process_status, attach_worker, scale_cluster,
     exec_on_nodes, submit_and_exec, _wait_for_ready, _rsync, cli_call_context, cluster_resource_metrics,
-    show_info)
+    show_info, _run_script)
 from cloudtik.core._private.utils import parse_bundles_json, parse_resources
 from cloudtik.scripts.head_scripts import head
 from cloudtik.scripts.node_scripts import node
 from cloudtik.scripts.runtime_scripts import runtime
 from cloudtik.scripts.utils import NaturalOrderGroup, add_command_alias
 from cloudtik.scripts.workspace import workspace
 
@@ -468,21 +468,22 @@
     default="",
     help="The runtime options of the job.")
 @click.argument("script", required=True, type=str)
 @click.argument("script_args", nargs=-1)
 @add_click_logging_options
 def submit(cluster_config_file, cluster_name, screen, tmux, stop, start,
            force_update, wait_for_workers, min_workers, wait_timeout,
-           no_config_cache, port_forward, yes, job_waiter, job_log, runtime, runtime_options,
+           no_config_cache, port_forward, yes, job_waiter, job_log,
+           runtime, runtime_options,
            script, script_args):
     """Uploads and runs a script on the specified cluster.
 
     The script is automatically synced to the following location:
 
-        os.path.join("~/jobs", os.path.basename(script))
+        os.path.join("~/user/jobs", os.path.basename(script))
     """
     # Don't use config cache so that we will run a full bootstrap needed for start
     if start:
         no_config_cache = True
     config = _load_cluster_config(
         cluster_config_file, cluster_name, no_config_cache=no_config_cache)
     port_forward = [(port, port) for port in list(port_forward)]
@@ -505,14 +506,125 @@
         job_waiter_name=job_waiter,
         job_log=job_log,
         runtime=runtime,
         runtime_options=runtime_options,
         )
 
 
+@cli.command(context_settings={"ignore_unknown_options": True})
+@click.argument("cluster_config_file", required=True, type=str)
+@click.option(
+    "--cluster-name",
+    "-n",
+    required=False,
+    type=str,
+    help="Override the configured cluster name.")
+@click.option(
+    "--screen",
+    is_flag=True,
+    default=False,
+    help="Run the command in a screen.")
+@click.option(
+    "--tmux", is_flag=True, default=False, help="Run the command in tmux.")
+@click.option(
+    "--stop",
+    is_flag=True,
+    default=False,
+    help="Stop the cluster after the command finishes running.")
+@click.option(
+    "--start",
+    is_flag=True,
+    default=False,
+    help="Start the cluster if needed.")
+@click.option(
+    "--force-update",
+    is_flag=True,
+    default=False,
+    help="Force update the cluster even if the cluster is running.")
+@click.option(
+    "--wait-for-workers",
+    is_flag=True,
+    default=False,
+    help="Whether wait for minimum number of workers to be ready.")
+@click.option(
+    "--min-workers",
+    required=False,
+    type=int,
+    help="The minimum number of workers to wait for ready.")
+@click.option(
+    "--wait-timeout",
+    required=False,
+    type=int,
+    help="The timeout seconds to wait for ready.")
+@click.option(
+    "--no-config-cache",
+    is_flag=True,
+    default=False,
+    help="Disable the local cluster config cache.")
+@click.option(
+    "--port-forward",
+    "-p",
+    required=False,
+    multiple=True,
+    type=int,
+    help="Port to forward. Use this multiple times to forward multiple ports.")
+@click.option(
+    "--yes",
+    "-y",
+    is_flag=True,
+    default=False,
+    help="Don't ask for confirmation.")
+@click.option(
+    "--job-waiter",
+    required=False,
+    type=str,
+    help="The job waiter to be used to check the completion of the job.")
+@click.option(
+    "--job-log",
+    is_flag=True,
+    default=False,
+    help="Whether redirect the output of the job to log file in ~/user/logs.")
+@click.argument("script", required=True, type=str)
+@click.argument("script_args", nargs=-1)
+@add_click_logging_options
+def run(
+        cluster_config_file, cluster_name, screen, tmux, stop, start,
+        force_update, wait_for_workers, min_workers, wait_timeout,
+        no_config_cache, port_forward, yes, job_waiter, job_log,
+        script, script_args):
+    """Runs a built-in script (bash or python or a registered command).
+
+    If you want to execute any commands or user scripts, use exec or submit.
+    """
+    # Don't use config cache so that we will run a full bootstrap needed for start
+    if start:
+        no_config_cache = True
+    config = _load_cluster_config(
+        cluster_config_file, cluster_name, no_config_cache=no_config_cache)
+    port_forward = [(port, port) for port in list(port_forward)]
+    _run_script(
+        config,
+        call_context=cli_call_context(),
+        script=script,
+        script_args=script_args,
+        screen=screen,
+        tmux=tmux,
+        stop=stop,
+        start=start,
+        force_update=force_update,
+        wait_for_workers=wait_for_workers,
+        min_workers=min_workers,
+        wait_timeout=wait_timeout,
+        port_forward=port_forward,
+        yes=yes,
+        job_waiter_name=job_waiter,
+        job_log=job_log
+        )
+
+
 @cli.command()
 @click.argument("cluster_config_file", required=True, type=str)
 @click.option(
     "--yes",
     "-y",
     is_flag=True,
     default=False,
@@ -1129,27 +1241,35 @@
     default=None,
     help="Temporary file to use")
 @click.option(
     "--no-config-cache",
     is_flag=True,
     default=False,
     help="Disable the local cluster config cache.")
+@click.option(
+    "--silent",
+    required=False,
+    type=bool,
+    is_flag=True,
+    default=False,
+    help="Whether print a warning message for cluster dump.")
 @add_click_logging_options
 def cluster_dump(cluster_config_file: Optional[str] = None,
                  cluster_name: str = None,
                  hosts: Optional[str] = None,
                  head_only: Optional[bool] = None,
                  output: Optional[str] = None,
                  logs: bool = True,
                  debug_state: bool = True,
                  pip: bool = True,
                  processes: bool = True,
                  processes_verbose: bool = False,
                  tempfile: Optional[str] = None,
-                 no_config_cache=False):
+                 no_config_cache=False,
+                 silent=False):
     """Get log data from one or more nodes.
 
     Best used with cluster configs:
 
         cloudtik cluster-dump [cluster.yaml]
 
     Include the --head-only flag to collect data from head node only.
@@ -1168,15 +1288,16 @@
         head_only=head_only,
         output=output,
         logs=logs,
         debug_state=debug_state,
         pip=pip,
         processes=processes,
         processes_verbose=processes_verbose,
-        tempfile=tempfile)
+        tempfile=tempfile,
+        silent=silent)
 
 
 def _add_command_alias(command, name, hidden):
     add_command_alias(cli, command, name, hidden)
 
 
 # commands running on working node for handling a cluster
@@ -1184,14 +1305,15 @@
 _add_command_alias(start, name="up", hidden=True)
 cli.add_command(stop)
 _add_command_alias(stop, name="down", hidden=True)
 
 cli.add_command(attach)
 cli.add_command(exec)
 cli.add_command(submit)
+cli.add_command(run)
 cli.add_command(scale)
 
 cli.add_command(rsync_up)
 _add_command_alias(rsync_up, name="rsync_up", hidden=True)
 cli.add_command(rsync_down)
 _add_command_alias(rsync_down, name="rsync_down", hidden=True)
```

## cloudtik/tests/unit/test_cloudtik.py

```diff
@@ -25,29 +25,29 @@
 from cloudtik.core._private.cli_logger import cli_logger, cf
 from cloudtik.core._private.cluster.cluster_metrics_updater import ClusterMetricsUpdater
 from cloudtik.core._private.cluster.cluster_operator import _should_create_new_head, _set_up_config_for_head_node, \
     POLL_INTERVAL
 from cloudtik.core._private.cluster.cluster_scaler import ClusterScaler, NonTerminatedNodes
 from cloudtik.core._private.cluster.event_summarizer import EventSummarizer
 from cloudtik.core._private.cluster.resource_scaling_policy import ResourceScalingPolicy
-from cloudtik.core._private.docker import validate_docker_config
+from cloudtik.core._private.docker import validate_docker_config, get_docker_host_mount_location, \
+    get_docker_host_mount_location_for_object
 from cloudtik.core._private.event_system import global_event_system, CreateClusterEvent
 from cloudtik.core._private.node.node_updater import NodeUpdater
 from cloudtik.core._private.prometheus_metrics import ClusterPrometheusMetrics
 from cloudtik.core._private.state.control_state import ControlState
 from cloudtik.core._private.state.scaling_state import ScalingStateClient
 from cloudtik.core._private.utils import prepare_config, validate_config, fillout_defaults, \
     set_node_type_min_max_workers, DOCKER_CONFIG_KEY, RUNTIME_CONFIG_KEY, get_cluster_uri, hash_launch_conf, \
     hash_runtime_conf, is_docker_enabled, get_commands_to_run, cluster_booting_completed, merge_cluster_config, \
     with_head_node_ip_environment_variables
 from cloudtik.core._private.cluster import cluster_operator
 from cloudtik.core._private.cluster.cluster_metrics import ClusterMetrics
 from cloudtik.core._private.providers import _NODE_PROVIDERS, _get_node_provider, _PROVIDER_HOMES, \
     _load_aws_provider_home
-from cloudtik.core.api import get_docker_host_mount_location
 
 from cloudtik.core.node_provider import NodeProvider
 from cloudtik.core.tags import CLOUDTIK_TAG_NODE_KIND, CLOUDTIK_TAG_NODE_STATUS, CLOUDTIK_TAG_USER_NODE_TYPE, \
     CLOUDTIK_TAG_CLUSTER_NAME, STATUS_UNINITIALIZED, STATUS_UPDATE_FAILED, NODE_KIND_HEAD, CLOUDTIK_TAG_LAUNCH_CONFIG, \
     CLOUDTIK_TAG_NODE_NAME, CLOUDTIK_TAG_NODE_NUMBER, CLOUDTIK_TAG_HEAD_NODE_NUMBER, NODE_KIND_WORKER, STATUS_UP_TO_DATE
 
 
@@ -993,26 +993,38 @@
         runner.assert_has_call("1.2.3.4", "init_cmd")
         runner.assert_has_call("1.2.3.4", "head_setup_cmd")
         runner.assert_has_call("1.2.3.4", "head_start_cmd")
         runner.assert_has_call("1.2.3.4", pattern="docker run")
         runner.assert_has_call("1.2.3.4", pattern=head_run_option)
         runner.assert_has_call("1.2.3.4", pattern=standard_run_option)
 
-        docker_mount_prefix = get_docker_host_mount_location(
-            SMALL_CLUSTER["cluster_name"]
+        docker_mount_prefix_for_object = get_docker_host_mount_location_for_object(
+            SMALL_CLUSTER["cluster_name"],
+            "~/cloudtik_bootstrap_key.pem"
         )
+        pattern = f"-v {docker_mount_prefix_for_object}"
         runner.assert_not_has_call(
-            "1.2.3.4", pattern=f"-v {docker_mount_prefix}/~/cloudtik_bootstrap_config"
+            "1.2.3.4", pattern=pattern
         )
-        common_container_copy = f"rsync -e.*docker exec -i.*{docker_mount_prefix}/~/"
+        pattern = f"rsync -e.*docker exec -i.*{docker_mount_prefix_for_object}"
         runner.assert_has_call(
-            "1.2.3.4", pattern=common_container_copy + "cloudtik_bootstrap_key.pem"
+            "1.2.3.4", pattern=pattern
+        )
+
+        docker_mount_prefix_for_object = get_docker_host_mount_location_for_object(
+            SMALL_CLUSTER["cluster_name"],
+            "~/cloudtik_bootstrap_config.yaml"
+        )
+        pattern = f"-v {docker_mount_prefix_for_object}"
+        runner.assert_not_has_call(
+            "1.2.3.4", pattern=pattern
         )
+        pattern = f"rsync -e.*docker exec -i.*{docker_mount_prefix_for_object}"
         runner.assert_has_call(
-            "1.2.3.4", pattern=common_container_copy + "cloudtik_bootstrap_config.yaml"
+            "1.2.3.4", pattern=pattern
         )
         return config
 
     def testGetOrCreateHeadNodeFromStopped(self):
         config = self.testGetOrCreateHeadNode()
         config["from"] = None
         self.provider.cache_stopped = True
@@ -1030,28 +1042,43 @@
         self.waitForNodes(1)
         # Init & Setup commands must be run for Docker!
         runner.assert_has_call("1.2.3.4", "init_cmd")
         runner.assert_has_call("1.2.3.4", "head_setup_cmd")
         runner.assert_has_call("1.2.3.4", "head_start_cmd")
         runner.assert_has_call("1.2.3.4", pattern="docker run")
 
-        docker_mount_prefix = get_docker_host_mount_location(
-            SMALL_CLUSTER["cluster_name"]
+        docker_mount_prefix_for_object = get_docker_host_mount_location_for_object(
+            SMALL_CLUSTER["cluster_name"],
+            "~/cloudtik_bootstrap_key.pem"
         )
+        pattern = f"-v {docker_mount_prefix_for_object}"
         runner.assert_not_has_call(
-            "1.2.3.4", pattern=f"-v {docker_mount_prefix}/~/cloudtik_bootstrap_config"
+            "1.2.3.4", pattern=pattern
         )
-        common_container_copy = f"rsync -e.*docker exec -i.*{docker_mount_prefix}/~/"
+        pattern = f"rsync -e.*docker exec -i.*{docker_mount_prefix_for_object}"
         runner.assert_has_call(
-            "1.2.3.4", pattern=common_container_copy + "cloudtik_bootstrap_key.pem"
+            "1.2.3.4", pattern=pattern
         )
+
+        docker_mount_prefix_for_object = get_docker_host_mount_location_for_object(
+            SMALL_CLUSTER["cluster_name"],
+            "~/cloudtik_bootstrap_config.yaml"
+        )
+        pattern = f"-v {docker_mount_prefix_for_object}"
+        runner.assert_not_has_call(
+            "1.2.3.4", pattern=pattern
+        )
+        pattern = f"rsync -e.*docker exec -i.*{docker_mount_prefix_for_object}"
         runner.assert_has_call(
-            "1.2.3.4", pattern=common_container_copy + "cloudtik_bootstrap_config.yaml"
+            "1.2.3.4", pattern=pattern
         )
 
+        docker_mount_prefix = get_docker_host_mount_location(
+            SMALL_CLUSTER["cluster_name"]
+        )
         # This section of code ensures that the following order of commands are executed:
         # 1. mkdir -p {docker_mount_prefix}
         # 2. rsync bootstrap files (over ssh)
         # 3. rsync bootstrap files into container
         commands_with_mount = [
             (i, cmd)
             for i, cmd in enumerate(runner.command_history())
@@ -1196,26 +1223,38 @@
         self.waitForNodes(1)
         runner.assert_has_call("1.2.3.4", "init_cmd")
         runner.assert_has_call("1.2.3.4", "head_setup_cmd")
         runner.assert_has_call("1.2.3.4", "head_start_cmd")
         runner.assert_has_call("1.2.3.4", pattern="docker stop")
         runner.assert_has_call("1.2.3.4", pattern="docker run")
 
-        docker_mount_prefix = get_docker_host_mount_location(
-            SMALL_CLUSTER["cluster_name"]
+        docker_mount_prefix_for_object = get_docker_host_mount_location_for_object(
+            SMALL_CLUSTER["cluster_name"],
+            "~/cloudtik_bootstrap_key.pem"
         )
+        pattern = f"-v {docker_mount_prefix_for_object}"
         runner.assert_not_has_call(
-            "1.2.3.4", pattern=f"-v {docker_mount_prefix}/~/cloudtik_bootstrap_config"
+            "1.2.3.4", pattern=pattern
         )
-        common_container_copy = f"rsync -e.*docker exec -i.*{docker_mount_prefix}/~/"
+        pattern = f"rsync -e.*docker exec -i.*{docker_mount_prefix_for_object}"
         runner.assert_has_call(
-            "1.2.3.4", pattern=common_container_copy + "cloudtik_bootstrap_key.pem"
+            "1.2.3.4", pattern=pattern
+        )
+
+        docker_mount_prefix_for_object = get_docker_host_mount_location_for_object(
+            SMALL_CLUSTER["cluster_name"],
+            "~/cloudtik_bootstrap_config.yaml"
+        )
+        pattern = f"-v {docker_mount_prefix_for_object}"
+        runner.assert_not_has_call(
+            "1.2.3.4", pattern=pattern
         )
+        pattern = f"rsync -e.*docker exec -i.*{docker_mount_prefix_for_object}"
         runner.assert_has_call(
-            "1.2.3.4", pattern=common_container_copy + "cloudtik_bootstrap_config.yaml"
+            "1.2.3.4", pattern=pattern
         )
 
     def testScaleDownMaxWorkers(self):
         """Tests terminating nodes due to max_nodes per type."""
         config = copy.deepcopy(MULTI_WORKER_CLUSTER)
         config["available_node_types"]["m4.large"]["min_workers"] = 3
         config["available_node_types"]["m4.large"]["max_workers"] = 3
```

## Comparing `cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/config/data-preprocessing-config.yaml` & `cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/config/data-processing-config.yaml`

 * *Files identical despite different names*

## Comparing `cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/train_spark.py` & `cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/model/spark/trainer.py`

 * *Files identical despite different names*

## Comparing `cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/data_processing/data_splitting.py` & `cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/data/data_splitting.py`

 * *Files 10% similar despite different names*

```diff
@@ -17,19 +17,19 @@
 
 class DataSplitter:
     def __init__(self, df, param):
         self.df = df 
         self.param = param 
 
     def split(self):
-        match list(self.param.keys())[0]:
-            case 'custom_rules': 
-                self.custom_rules(list(self.param.values())[0])
-            case 'random_split':
-                self.random_split(list(self.param.values())[0])
+        op = list(self.param.keys())[0]
+        if op == 'custom_rules':
+            self.custom_rules(list(self.param.values())[0])
+        elif op == 'random_split':
+            self.random_split(list(self.param.values())[0])
         return self.train_data, self.test_data  
 
     def custom_rules(self, rules):
         train_rules = rules['train']
         test_rules = rules['test']
         df = self.df
         self.train_data = df[eval(train_rules)]
```

## Comparing `cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/data_processing/data_transform.py` & `cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/data/data_transform.py`

 * *Files 14% similar despite different names*

```diff
@@ -22,58 +22,58 @@
         self.df = df 
         self.steps = steps 
         self.tmp = {}
         self.dp_engine = dp_engine 
 
     def transform(self):
         for step in self.steps:
-            match list(step.keys())[0]: 
-                case 'normalize_feature_names': 
-                    self.normalize_feature_names(list(step.values())[0])
-                case 'rename_feature_names':
-                    raise NotImplementedError 
-                case 'drop_features':
-                    raise NotImplementedError
-                case 'outlier_treatment':
-                    raise NotImplementedError
-                case 'categorify': 
-                    self.categorify(list(step.values())[0])
-                case 'strip_chars':
-                    self.strip_chars(list(step.values())[0])
-                case 'combine_cols':
-                    self.combine_cols(list(step.values())[0])
-                case 'change_datatype':
-                    self.change_datatype(list(step.values())[0])
-                case 'time_to_seconds':
-                    self.time_to_seconds(list(step.values())[0])
-                case 'min_max_normalization':
-                    self.min_max_normalization(list(step.values())[0])
-                case 'one_hot_encoding':
-                    self.one_hot_encoding(list(step.values())[0])
-                case 'string_to_list':
-                    self.string_to_list(list(step.values())[0])
-                case 'multi_hot_encoding':
-                    self.multi_hot_encoding(list(step.values())[0])
-                case 'add_constant_feature':
-                    self.add_constant_feature(list(step.values())[0])
-                case 'define_variable':
-                    self.define_variable(list(step.values())[0])
-                case 'modify_on_conditions':
-                    self.modify_on_conditions(list(step.values())[0])
+            op = list(step.keys())[0]
+            if op == 'normalize_feature_names':
+                self.normalize_feature_names(list(step.values())[0])
+            elif op == 'rename_feature_names':
+                raise NotImplementedError
+            elif op == 'drop_features':
+                raise NotImplementedError
+            elif op == 'outlier_treatment':
+                raise NotImplementedError
+            elif op == 'categorify':
+                self.categorify(list(step.values())[0])
+            elif op == 'strip_chars':
+                self.strip_chars(list(step.values())[0])
+            elif op == 'combine_cols':
+                self.combine_cols(list(step.values())[0])
+            elif op == 'change_datatype':
+                self.change_datatype(list(step.values())[0])
+            elif op == 'time_to_seconds':
+                self.time_to_seconds(list(step.values())[0])
+            elif op == 'min_max_normalization':
+                self.min_max_normalization(list(step.values())[0])
+            elif op == 'one_hot_encoding':
+                self.one_hot_encoding(list(step.values())[0])
+            elif op == 'string_to_list':
+                self.string_to_list(list(step.values())[0])
+            elif op == 'multi_hot_encoding':
+                self.multi_hot_encoding(list(step.values())[0])
+            elif op == 'add_constant_feature':
+                self.add_constant_feature(list(step.values())[0])
+            elif op == 'define_variable':
+                self.define_variable(list(step.values())[0])
+            elif op == 'modify_on_conditions':
+                self.modify_on_conditions(list(step.values())[0])
                 
         return self.df 
 
     def normalize_feature_names(self, steps):
         for step in steps:
-            match list(step.keys())[0]:
-                case 'replace_chars':
-                    self.replace_chars(list(step.values())[0])
-                case 'lowercase':
-                    if list(step.values())[0]:
-                        self.to_lowercase()
+            op = list(step.keys())[0]
+            if op == 'replace_chars':
+                self.replace_chars(list(step.values())[0])
+            elif op == 'lowercase':
+                if list(step.values())[0]:
+                    self.to_lowercase()
 
     def replace_chars(self, replacements):
         for key, value in replacements.items():
             self.df.columns = self.df.columns.str.replace(key, value)
 
     def to_lowercase(self):
         self.df.columns = self.df.columns.str.lower()
@@ -89,20 +89,19 @@
     
     def combine_cols(self, features):
         for new_feature, content in features.items():
             for operation, target_feature_list in content.items(): 
                 if len(target_feature_list) < 2:
                     raise ValueError('there is less than 2 items in the list, cannot concatenate')
                 else:
-                    match operation:
-                        case 'concatenate_strings':
-                            tmp_feature = self.df[target_feature_list[0]].astype('str')
-                            for feature in target_feature_list[1:]:
-                                tmp_feature = tmp_feature + self.df[feature].astype('str')
-                            self.df[new_feature] = tmp_feature 
+                    if operation == 'concatenate_strings':
+                        tmp_feature = self.df[target_feature_list[0]].astype('str')
+                        for feature in target_feature_list[1:]:
+                            tmp_feature = tmp_feature + self.df[feature].astype('str')
+                        self.df[new_feature] = tmp_feature
     
     def change_datatype(self, col_dtypes):
         for col, dtype in col_dtypes.items():
             if isinstance(dtype, list):
                 for type in dtype:
                     self.df[col] = self.df[col].astype(type)
             else:
```

## Comparing `cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/data_processing/post_transform.py` & `cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/data/post_transform.py`

 * *Files 3% similar despite different names*

```diff
@@ -74,19 +74,19 @@
         self.train_data = train_data
         self.test_data = test_data  
         self.steps = steps 
         self.dp_engine = dp_engine
 
     def transform(self):
         for step in self.steps:
-            match list(step.keys())[0]: 
-                case 'target_encoding': 
-                    self.target_encoding(list(step.values())[0])
-                case 'label_encoding':
-                    self.label_encoding(list(step.values())[0])
+            op = list(step.keys())[0]
+            if op == 'target_encoding':
+                self.target_encoding(list(step.values())[0])
+            elif op == 'label_encoding':
+                self.label_encoding(list(step.values())[0])
         return self.train_data, self.test_data  
 
     def target_encoding(self, params):
         target_col = params['target_col']
         feature_cols = params['feature_cols']
         smoothing = params['smoothing']
```

## Comparing `cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/process_data.py` & `cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/data/process.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,26 +1,25 @@
 # Copyright (C) 2023 Intel Corporation
 # SPDX-License-Identifier: MIT
 
 import pandas as pd
 import numpy as np
 import time
 import math
-import argparse
 
 from sklearn import preprocessing
 from category_encoders import TargetEncoder
 
-from .utils import existing_file
+from cloudtik.runtime.ai.modeling.graph_modeling.graph_sage.modeling.utils import df_to_csv
 
 
-def process_data(raw_data_file, output_file):
+def process_data(raw_data_path, output_file):
     # Step 1: read and clean the dataframe
     tic = time.time()
-    df = pd.read_csv(raw_data_file)
+    df = pd.read_csv(raw_data_path)
     df.columns = df.columns.str.replace(" ", "_").str.lower()
     print(
         "Time to read the dataframe = {} seconds".format(math.ceil(time.time() - tic))
     )
 
     # Step 2: Categorical, one-hot, multi-hot encoding (independent of data split)
     tic = time.time()
@@ -98,35 +97,14 @@
     # merge train, validation and test dataframes into one
     df_merge = pd.concat([train_df, valtest_df])
 
     print("Time for featurization = {} seconds".format(math.ceil(time.time() - tic)))
 
     # step 5 : write edge features to a file
     tic = time.time()
-    df_merge.to_csv(output_file, index=False)
+    df_to_csv(df_merge, output_file, index=False)
     print(
         "Writing edge features to csv file takes {} seconds".format(
             math.ceil(time.time() - tic)
         )
     )
     print(df_merge.shape)
-
-
-def main(args):
-    process_data(
-        raw_data_file=args.raw_data_file,
-        output_file=args.output_file,
-    )
-
-
-if __name__ == "__main__":
-    parser = argparse.ArgumentParser(description="Process data")
-    parser.add_argument(
-        "--raw_data_file", type=existing_file, help="The path to the raw transaction data file")
-    parser.add_argument(
-        "--output_file",
-        type=str,
-        help="The path to the output processed edge data file",)
-    args = parser.parse_args()
-    print(args)
-
-    main(args)
```

## Comparing `cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/train_graph_sage_single.py` & `cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/distributed/trainer.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,382 +1,347 @@
-# Copyright (C) 2023 Intel Corporation
-# SPDX-License-Identifier: MIT
+"""
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
 
+Author: Chen Haifeng
+"""
+
+import time
+import os
+import numpy as np
 import torch
-import torch.nn as nn
 import torch.nn.functional as F
-
+import torch.optim as optim
 import dgl
-import dgl.nn as dglnn
-from dgl.nn import SAGEConv
-from dgl.nn import EdgePredictor
-from dgl.dataloading import (
-    DataLoader,
-    NeighborSampler,
-    MultiLayerFullNeighborSampler,
-    as_edge_prediction_sampler,
-    negative_sampler,
-)
 
 import tqdm
-import argparse
-import time
-from sklearn.metrics import roc_auc_score
-import numpy as np
-import random
+from sklearn.metrics import roc_auc_score, accuracy_score, average_precision_score
+
+from cloudtik.runtime.ai.modeling.graph_modeling.graph_sage.modeling.model. \
+    heterogeneous.distributed.utils import get_eids_mask, get_eids_from_mask, \
+    get_edge_split_indices, save_node_embeddings
+from cloudtik.runtime.ai.modeling.graph_modeling.graph_sage.modeling.model. \
+    heterogeneous.utils import tensor_dict_flatten, tensor_dict_to
+from cloudtik.runtime.ai.modeling.graph_modeling.graph_sage.modeling.model.utils import \
+    parse_reverse_edges
+from cloudtik.runtime.ai.modeling.graph_modeling.graph_sage.modeling.utils import torch_save
+
+
+class Trainer:
+    def __init__(self, model, model_eval, args):
+        self.model = model
+        self.model_eval = model_eval
+        self.args = args
+
+    def _get_graph_model(self, model):
+        """Return the real graph model"""
+        if not self.args.standalone:
+            return model.module
+        else:
+            return model
+
+    def train(self, g):
+        args = self.args
+        relations = args.relations
+
+        print("Read train/test/val from:", relations)
+
+        # The reverse types must guarantee a reverse edge has the same id
+        reverse_etypes = None
+        if args.exclude_reverse_edges and args.reverse_edges:
+            reverse_etypes = parse_reverse_edges(args.reverse_edges)
+            print("Reverse edges be excluded during the training:", reverse_etypes)
+
+        train_eids_mask = get_eids_mask(
+            g, relations, "train_mask", reverse_etypes)
+        val_eids = get_eids_from_mask(
+            g, relations, "val_mask", reverse_etypes)
+        test_eids = get_eids_from_mask(
+            g, relations, "test_mask", reverse_etypes)
+
+        # The mask ids here are the shuffled ids
+        train_eids = get_edge_split_indices(
+            g, relations, train_eids_mask, reverse_etypes
+        )
 
+        if args.num_gpus == -1:
+            device = torch.device("cpu")
+        else:
+            dev_id = g.rank() % args.num_gpus
+            device = torch.device("cuda:" + str(dev_id))
 
-class GraphSAGE(nn.Module):
-    def __init__(
-        self, in_feats, hidden_size, out_feats, n_layers, activation, aggregator_type
-    ):
-        super(GraphSAGE, self).__init__()
-        self.layers = nn.ModuleList()
-        self.activation = activation
-
-        # input layer
-        self.layers.append(SAGEConv(in_feats, hidden_size, aggregator_type))
-        # hidden layers
-        for i in range(1, n_layers - 1):
-            self.layers.append(SAGEConv(hidden_size, hidden_size, aggregator_type))
-        # output layer
-        self.layers.append(
-            SAGEConv(hidden_size, out_feats, aggregator_type)
-        )  # activation None
-
-    def forward(self, graphs, inputs):
-        h = inputs
-        for l, (layer, block) in enumerate(zip(self.layers, graphs)):
-            h = layer(block, h)
-            if l != len(self.layers) - 1:
-                h = self.activation(h)
-        return h
-
-
-class Model(nn.Module):
-    def __init__(self, vocab_size, hid_size, n_layers):
-        super().__init__()
-
-        self.hid_size = hid_size
-
-        # node embedding
-        self.emb = torch.nn.Embedding(vocab_size, hid_size)
-        # encoder is a 1-layer GraphSAGE model
-        self.encoder = GraphSAGE(hid_size, hid_size, hid_size, n_layers, F.relu, "mean")
-        # decoder is a 3-layer MLP
-        self.decoder = nn.Sequential(
-            nn.Linear(hid_size, hid_size),
-            nn.ReLU(),
-            nn.Linear(hid_size, hid_size),
-            nn.ReLU(),
-            nn.Linear(hid_size, 1),
+        train_dataloader, val_dataloader, test_dataloader = self._create_data_loaders(
+            g, train_eids, val_eids, test_eids, reverse_etypes
         )
-        # cosine similarity with linear
-        # self.predictor=dglnn.EdgePredictor('cos',hid_size,1)
 
-    def forward(self, pair_graph, neg_pair_graph, blocks, x):
-        h = self.emb(x)
-        h = self.encoder(blocks, h)
-
-        pos_src, pos_dst = pair_graph.edges()
-        neg_src, neg_dst = neg_pair_graph.edges()
-        # h_pos = self.predictor(h[pos_src], h[pos_dst])
-        # h_neg = self.predictor(h[neg_src], h[neg_dst])
-        h_pos = self.decoder(h[pos_src] * h[pos_dst])
-        h_neg = self.decoder(h[neg_src] * h[neg_dst])
-        return h_pos, h_neg
-
-    def inference(self, g, device, batch_size):
-        """Layer-wise inference algorithm to compute GNN node embeddings."""
-        # feat = g.ndata['feat']
-        # use pretrained embedding as node features
-        feat = self.emb.weight.data
-        sampler = MultiLayerFullNeighborSampler(1)
-        dataloader = DataLoader(
+        self._train(
+            args, device, g,
+            train_dataloader, val_dataloader, test_dataloader)
+        print("Train ends")
+
+    def _create_data_loaders(
+            self, g, train_eids, val_eids, test_eids, reverse_etypes):
+        args = self.args
+
+        # Create sampler
+        neg_sampler = dgl.dataloading.negative_sampler.Uniform(args.num_negs)
+        sampler = dgl.dataloading.NeighborSampler(
+            [int(fanout) for fanout in args.fan_out.split(",")]
+        )
+        test_sampler = dgl.dataloading.MultiLayerFullNeighborSampler(1)
+        # Create dataloader
+        # "reverse_id" exclude not only edges in minibatch but their reverse edges according to reverse_eids mapping
+        # reverse_eids - The i-th element indicates the ID of the i-th edges reverse edge.
+        exclude = "reverse_types" if reverse_etypes is not None else None
+        train_dataloader = dgl.dataloading.DistEdgeDataLoader(
             g,
-            torch.arange(g.num_nodes()).to(g.device),
+            train_eids,
             sampler,
-            device=device,
-            batch_size=batch_size,
-            shuffle=False,
+            negative_sampler=neg_sampler,
+            exclude=exclude,
+            reverse_etypes=reverse_etypes,
+            batch_size=args.batch_size,
+            shuffle=True,
             drop_last=False,
-            num_workers=4,
         )
-        buffer_device = torch.device("cpu")
-        pin_memory = buffer_device != device
-        # compure representations layer by layer
-        for l, layer in enumerate(self.encoder.layers):
-            y = torch.empty(
-                g.num_nodes(),
-                self.hid_size,
-                device=buffer_device,
-                pin_memory=pin_memory,
-            )
-            feat = feat.to(device)
-            # within a layer iterate over nodes in batches
-            with dataloader.enable_cpu_affinity():
-                for input_nodes, output_nodes, blocks in tqdm.tqdm(
-                    dataloader, desc="Inference"
-                ):
-                    x = feat[input_nodes]
-                    h = layer(blocks[0], x)
-                    if l != len(self.encoder.layers) - 1:
-                        h = F.relu(h)
-                    y[output_nodes] = h.to(buffer_device)
-                feat = y
-        return y
 
+        val_dataloader = dgl.dataloading.DistEdgeDataLoader(
+            g,
+            val_eids,
+            sampler,
+            negative_sampler=neg_sampler,
+            exclude=exclude,
+            reverse_etypes=reverse_etypes,
+            batch_size=args.batch_size_eval,
+            shuffle=True,
+            drop_last=False,
+        )
 
-def evaluate(model, test_dataloader):
-    model.eval()
-    score_all = torch.tensor(())
-    labels_all = torch.tensor(())
-    with test_dataloader.enable_cpu_affinity():
-        with torch.no_grad():
-            for it, (input_nodes, pair_graph, neg_pair_graph, blocks) in enumerate(
-                tqdm.tqdm(test_dataloader)
-            ):
-                x = blocks[0].srcdata[dgl.NID]
-                pos_score, neg_score = model(pair_graph, neg_pair_graph, blocks, x)
-                score = torch.cat([pos_score, neg_score])
-                pos_label = torch.ones_like(pos_score)
-                neg_label = torch.zeros_like(neg_score)
-                labels = torch.cat([pos_label, neg_label])
-                score_all = torch.cat([score_all, score])
-                labels_all = torch.cat([labels_all, labels])
-            rocauc = roc_auc_score(labels_all, score_all)
-    return rocauc
+        test_dataloader = dgl.dataloading.DistEdgeDataLoader(
+            g,
+            test_eids,
+            test_sampler,
+            negative_sampler=neg_sampler,
+            exclude=exclude,
+            reverse_etypes=reverse_etypes,
+            batch_size=args.batch_size_eval,
+            shuffle=False,
+            drop_last=False,
+        )
 
+        return train_dataloader, val_dataloader, test_dataloader
 
-def train(args, device, g, train_dataloader, val_dataloader, test_dataloader, model):
-    best_rocauc = 0
-    best_model_path = args.model_file
-    print("learning rate: ", args.lr)
-    opt = torch.optim.Adam(model.parameters(), lr=args.lr)
-    with train_dataloader.enable_cpu_affinity():
+    def _train(
+            self, args, device, g,
+            train_dataloader, val_dataloader, test_dataloader):
+        model = self.model.to(device)
+
+        if not args.standalone:
+            if args.num_gpus == -1:
+                model = torch.nn.parallel.DistributedDataParallel(
+                    model,
+                )
+            else:
+                dev_id = g.rank() % args.num_gpus
+                model = torch.nn.parallel.DistributedDataParallel(
+                    model,
+                    device_ids=[dev_id],
+                    output_device=dev_id,
+                )
+        optimizer = optim.Adam(model.parameters(), lr=args.lr)
+        graph_model = self._get_graph_model(model)
+
+        # Training loop
+        epoch = 0
+        best_model_path = args.model_file
+        best_rocauc = 0
         for epoch in range(args.num_epochs):
-            start = time.time()
+            num_seeds = 0
+            num_inputs = 0
             step_time = []
-            model.train()
+            sample_t = []
+            feat_copy_t = []
+            forward_t = []
+            backward_t = []
+            update_t = []
+            iter_tput = []
             total_loss = 0
-            total_val_loss = 0.0
-            for it, (input_nodes, pair_graph, neg_pair_graph, blocks) in enumerate(
-                train_dataloader
-            ):
-                # x = blocks[0].srcdata[dgl.NID] #this is equal to input_nodes
-                pos_score, neg_score = model(
-                    pair_graph, neg_pair_graph, blocks, input_nodes
-                )
-                score = torch.cat([pos_score, neg_score])
-                pos_label = torch.ones_like(pos_score)
-                neg_label = torch.zeros_like(neg_score)
-                labels = torch.cat([pos_label, neg_label])
-                loss = F.binary_cross_entropy_with_logits(score, labels)
-                opt.zero_grad()
-                loss.backward()
-                opt.step()
-                total_loss += loss.item()
-                step_t = time.time() - start
-                step_time.append(step_t)
-                start = time.time()
-            # print("Epoch {:05d} | Train Loss {:.4f}".format(epoch, total_loss / (it + 1)))
-            # torch.save(model.state_dict(), best_model_path)
-
-            model.eval()
-
-            with val_dataloader.enable_cpu_affinity():
-                for it2, (input_nodes, pair_graph, neg_pair_graph, blocks) in enumerate(
-                    val_dataloader
+            start = time.time()
+            with model.join():
+                # Loop over the dataloader to sample the computation dependency
+                # graph as a list of blocks.
+                for step, (input_nodes, pos_graph, neg_graph, blocks) in enumerate(
+                    train_dataloader
                 ):
-                    # x = blocks[0].srcdata[dgl.NID] this is same as input_nodes
-                    pos_score, neg_score = model(
-                        pair_graph, neg_pair_graph, blocks, input_nodes
-                    )
+                    tic_step = time.time()
+                    sample_t.append(tic_step - start)
+
+                    copy_t = time.time()
+                    pos_graph = pos_graph.to(device)
+                    neg_graph = neg_graph.to(device)
+                    blocks = [block.to(device) for block in blocks]
+                    x = graph_model.get_inputs(g, input_nodes, blocks)
+                    x = tensor_dict_to(x, device)
+                    feat_copy_t.append(copy_t - tic_step)
+                    copy_time = time.time()
+
+                    # Compute loss and prediction
+                    pos_score, neg_score = model(pos_graph, neg_graph, blocks, x)
+                    pos_score = tensor_dict_flatten(pos_score)
+                    neg_score = tensor_dict_flatten(neg_score)
                     score = torch.cat([pos_score, neg_score])
                     pos_label = torch.ones_like(pos_score)
                     neg_label = torch.zeros_like(neg_score)
                     labels = torch.cat([pos_label, neg_label])
-                    val_loss = F.binary_cross_entropy_with_logits(score, labels)
-                    total_val_loss += val_loss.item()
-                print(
-                    "Epoch {:05d} | Epoch time {:.4f} | Train Loss {:.4f} | Valid Loss {:.4f}".format(
-                        epoch,
-                        np.sum(step_time),
-                        total_loss / (it + 1),
-                        total_val_loss / (it2 + 1),
-                    )
+                    loss = F.binary_cross_entropy_with_logits(score, labels)
+                    forward_end = time.time()
+                    optimizer.zero_grad()
+                    loss.backward()
+                    compute_end = time.time()
+                    forward_t.append(forward_end - copy_time)
+                    backward_t.append(compute_end - forward_end)
+
+                    # Aggregate gradients in multiple nodes.
+                    optimizer.step()
+                    update_t.append(time.time() - compute_end)
+                    total_loss += loss.item()
+                    pos_edges = pos_graph.num_edges()
+
+                    step_t = time.time() - start
+                    step_time.append(step_t)
+                    iter_tput.append(pos_edges / step_t)
+                    num_seeds += pos_edges
+                    if step % args.log_every == 0:
+                        print(
+                            "[{}] Epoch {:05d} | Step {:05d} | Loss {:.4f} | Speed "
+                            "(samples/sec) {:.4f} | time {:.3f}s | sample {:.3f} | "
+                            "copy {:.3f} | forward {:.3f} | backward {:.3f} | "
+                            "update {:.3f}".format(
+                                g.rank(),
+                                epoch,
+                                step,
+                                loss.item(),
+                                np.mean(iter_tput[3:]),
+                                np.sum(step_time[-args.log_every :]),
+                                np.sum(sample_t[-args.log_every :]),
+                                np.sum(feat_copy_t[-args.log_every :]),
+                                np.sum(forward_t[-args.log_every :]),
+                                np.sum(backward_t[-args.log_every :]),
+                                np.sum(update_t[-args.log_every :]),
+                            )
+                        )
+                    start = time.time()
+
+            print(
+                "[{}]Epoch Time(s): {:.4f}, sample: {:.4f}, data copy: {:.4f}, "
+                "forward: {:.4f}, backward: {:.4f}, update: {:.4f}, #seeds: {}, "
+                "#inputs: {}".format(
+                    g.rank(),
+                    np.sum(step_time),
+                    np.sum(sample_t),
+                    np.sum(feat_copy_t),
+                    np.sum(forward_t),
+                    np.sum(backward_t),
+                    np.sum(update_t),
+                    num_seeds,
+                    num_inputs,
                 )
-
-            if (epoch % args.eval_every == 0 and epoch != 0) or (
-                epoch == args.num_epochs - 1
+            )
+            # NOTE: since evaluation is currently done on single rank, after the first epoch,
+            # the epoch time reported by ranks other than zero include the evaluation time
+            # while they are waiting for epoch to complete.
+            if (g.rank() == 0 and epoch % args.eval_every == 0 and epoch != 0) or (
+                g.rank() == 0 and epoch == args.num_epochs - 1
             ):
-                rocauc = evaluate(model, test_dataloader)
+                # load weights into single rank model
+                model_eval = self.model_eval.to(
+                    device
+                )
+                model_eval.load_state_dict(graph_model.state_dict())
+                # calculate test score on full test set
+                with torch.no_grad():
+                    rocauc, ap_score = self.evaluate(
+                        model_eval, device, g, test_dataloader)
+                print("Epoch {:05d} | roc_auc {:.4f}".format(epoch, rocauc))
                 # update best model if needed
                 if best_rocauc < rocauc:
-                    print("Updating best model")
+                    print("updating best model")
                     best_rocauc = rocauc
-                    torch.save(model.state_dict(), best_model_path)
-                print("Epoch {:05d} | Test roc_auc_score {:.4f}".format(epoch, rocauc))
-
+                    torch_save(graph_model.state_dict(), best_model_path)
 
-def main(args):
-    # random seeds for testing
-    seed = 7
-
-    print("Random seed set to: ", seed)
-    random.seed(seed)
-    np.random.seed(seed)
-    dgl.seed(seed)
-    dgl.random.seed(seed)
-    torch.random.manual_seed(seed)
-    torch.manual_seed(seed)
-
-    if not torch.cuda.is_available():
-        args.mode = "cpu"
-    print(f"Training in {args.mode} mode.")
-
-    # load and preprocess dataset
-    print("Loading data")
-    start = time.time()
-    # set force_reload=False if no changes on input graph (much faster otherwise ingestion ~30min)
-    dataset = dgl.data.CSVDataset(args.dataset_dir, force_reload=False)
-    print("Time to load data from CSVs: ", time.time() - start)
-
-    hg = dataset[0]  # only one graph
-    print(hg)
-    print("etype to read train/test/val from: ", hg.canonical_etypes[0][1])
-    train_mask = hg.edges[hg.canonical_etypes[0][1]].data["train_mask"]
-    val_mask = hg.edges[hg.canonical_etypes[0][1]].data["val_mask"]
-    test_mask = hg.edges[hg.canonical_etypes[0][1]].data["test_mask"]
-    train_eidx = torch.nonzero(train_mask, as_tuple=False).squeeze()
-    val_eidx = torch.nonzero(val_mask, as_tuple=False).squeeze()
-    test_eidx = torch.nonzero(test_mask, as_tuple=False).squeeze()
-
-    E = hg.num_edges(hg.canonical_etypes[0][1])
-    reverse_eids = torch.cat([torch.arange(E, 2 * E), torch.arange(0, E)])
-    print("First reverse id is:  ", reverse_eids[0])
-
-    g = dgl.to_homogeneous(hg)
-    g = g.to("cuda" if args.mode == "gpu" else "cpu")
-    device = torch.device("cpu" if args.mode == "cpu" else "cuda")
-
-    train_eidx.to(device)
-    val_eidx.to(device)
-    test_eidx.to(device)
-
-    vocab_size = g.num_nodes()
-
-    # create sampler & dataloaders
-    sampler = NeighborSampler([int(fanout) for fanout in args.fan_out.split(",")])
-    sampler = as_edge_prediction_sampler(
-        sampler,
-        exclude="reverse_id",
-        reverse_eids=reverse_eids,
-        negative_sampler=negative_sampler.Uniform(1),
-    )
-    # no neighbor sampling during test
-    test_sampler = MultiLayerFullNeighborSampler(1)
-    test_sampler = as_edge_prediction_sampler(
-        test_sampler,
-        exclude="reverse_id",
-        reverse_eids=reverse_eids,
-        negative_sampler=negative_sampler.Uniform(1),
-    )
-
-    use_uva = args.mode == "mixed"
-
-    train_dataloader = DataLoader(
-        g,
-        train_eidx,
-        sampler,
-        device=device,
-        batch_size=args.batch_size,
-        shuffle=True,
-        drop_last=False,
-        num_workers=args.num_dl_workers,
-        use_uva=use_uva,
-    )
-
-    val_dataloader = DataLoader(
-        g,
-        val_eidx,
-        sampler,
-        device=device,
-        batch_size=args.batch_size_eval,
-        shuffle=True,
-        drop_last=False,
-        num_workers=args.num_dl_workers,
-        use_uva=use_uva,
-    )
-
-    test_dataloader = DataLoader(
-        g,
-        test_eidx,
-        test_sampler,
-        device=device,
-        batch_size=args.batch_size_eval,
-        shuffle=False,
-        drop_last=False,
-        num_workers=args.num_dl_workers,
-        use_uva=use_uva,
-    )
-
-    model = Model(vocab_size, args.num_hidden, args.num_layers).to(device)
-
-    # model training
-    print("Training...")
-    train(args, device, g, train_dataloader, val_dataloader, test_dataloader, model)
-
-    # to load pretrained model if you just want to do inference
-    #    model.load_state_dict(torch.load(args.model_file))
-    #    node_emb = model.emb.weight.data
-    #    torch.save(node_emb, args.node_embeddings_file)
-
-    print("Inference to generate node representations...")
-    model.eval()
-    model.load_state_dict(torch.load(args.model_file))
-    node_emb = model.inference(g, device, args.batch_size_eval)
-    print("Node embeddings shape: ", node_emb.shape)
-    torch.save(node_emb, args.node_embeddings_file)
-
-    # roc_auc on the different splits after training completion
-    # roc_auc_test = evaluate(model, test_dataloader)
-    # print("final test roc_auc", roc_auc_test)
-
-
-if __name__ == "__main__":
-    parser = argparse.ArgumentParser(description="Graph SAGE")
-    parser.add_argument(
-        "--mode",
-        default="cpu",
-        choices=["cpu", "gpu", "mixed"],
-        help="Training mode. 'cpu' for CPU training, 'gpu' for pure-GPU training, "
-             "'mixed' for CPU-GPU mixed training.")
-    parser.add_argument(
-        "--dataset_dir",
-        default="",
-        help="Path to CSVDataset")
-    parser.add_argument(
-        "--model_file",
-        default="model_graphsage_2L_64.pt",
-        type=str,
-        help="output for model /your_path/model_graphsage_2L_64.pt")
-    parser.add_argument(
-        "--node_embeddings_file",
-        default="node_emb.pt",
-        type=str,
-        help="node emb output: /your_path/node_emb.pt")
-    parser.add_argument("--num_epochs", type=int, default=3)
-    parser.add_argument("--num_hidden", type=int, default=16)
-    parser.add_argument("--num_layers", type=int, default=2)
-    parser.add_argument("--fan_out", type=str, default="10,25")
-    parser.add_argument("--batch_size", type=int, default=1000)
-    parser.add_argument("--batch_size_eval", type=int, default=100000)
-    parser.add_argument("--eval_every", type=int, default=5)
-    parser.add_argument("--lr", type=float, default=0.003)
-
-    parser.add_argument("--num_dl_workers", type=int, default=4)
+            # print average epoch loss  per rank
+            print(
+                "[{}] Epoch {:05d} | Loss {:.4f}".format(
+                    g.rank(), epoch, total_loss / (step + 1)
+                )
+            )
+            epoch += 1
 
-    args = parser.parse_args()
-    print(args)
+        # sync the status for all ranks
+        if not args.standalone:
+            torch.distributed.barrier()
+
+        # train is complete, save node embeddings of the best model
+        # sync for eval and test
+        best_model_path = args.model_file
+        graph_model = self._get_graph_model(model)
+        graph_model.load_state_dict(torch.load(best_model_path))
+        if not args.standalone:
+            # save node embeddings into file
+            torch.nn.Module.eval(model)  # model.eval()
+            with torch.no_grad():
+                x = graph_model.get_inference_inputs(g)
+                node_emb = graph_model.inference(g, x, args.batch_size_eval, device)
+            if g.rank() == 0:
+                save_node_embeddings(node_emb, args.node_embeddings_file)
+            g._client.barrier()
+        else:
+            torch.nn.Module.eval(model)  # model.eval()
+            # save node embeddings into file
+            with torch.no_grad():
+                x = graph_model.get_inference_inputs(g)
+                node_emb = graph_model.inference(g, x, args.batch_size_eval, device)
+                save_node_embeddings(node_emb, args.node_embeddings_file)
+
+        if not args.standalone:
+            torch.distributed.barrier()
+            # torch.distributed.monitored_barrier(timeout=timedelta(minutes=60))
+
+    def evaluate(self, model, device, g, test_dataloader):
+        # evaluate the embeddings on test set
+        torch.nn.Module.eval(model)  # model.eval()
+        score_all = torch.tensor(())
+        labels_all = torch.tensor(())
+        with torch.no_grad():
+            for it, (input_nodes, pair_graph, neg_pair_graph, blocks) in enumerate(
+                tqdm.tqdm(test_dataloader)
+            ):
+                pair_graph = pair_graph.to(device)
+                neg_pair_graph = neg_pair_graph.to(device)
+                blocks = [block.to(device) for block in blocks]
+                x = model.get_inputs(g, input_nodes, blocks)
+                x = tensor_dict_to(x, device)
 
-    main(args)
+                pos_score, neg_score = model(
+                    pair_graph, neg_pair_graph, blocks, x
+                )
+                # Heterogeneous: handle the result as dict of score of each edge types
+                pos_score = tensor_dict_flatten(pos_score)
+                neg_score = tensor_dict_flatten(neg_score)
+                score = torch.cat([pos_score, neg_score])
+                pos_label = torch.ones_like(pos_score)
+                neg_label = torch.zeros_like(neg_score)
+                labels = torch.cat([pos_label, neg_label])
+                score_all = torch.cat([score_all, score])
+                labels_all = torch.cat([labels_all, labels])
+            rocauc = roc_auc_score(labels_all, score_all)
+            ap_score = average_precision_score(labels_all, score_all)
+            return rocauc, ap_score
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## Comparing `cloudtik-1.1.0.dist-info/METADATA` & `cloudtik-1.2.0.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: cloudtik
-Version: 1.1.0
+Version: 1.2.0
 Summary: CloudTik: a cloud scale platform for distributed analytics and AI on public clouds
 Home-page: https://github.com/oap-project/cloudtik.git
 Author: Intel Corporation
 License: Apache 2.0
 Keywords: Distributed Cloud Analytic AI Spark
 Classifier: Programming Language :: Python :: 3.8
 Requires-Python: >=3.8
@@ -30,99 +30,102 @@
 Requires-Dist: colorful
 Requires-Dist: gpustat
 Requires-Dist: gputil
 Requires-Dist: dataclasses ; python_version < "3.7"
 Requires-Dist: numpy (>=1.16) ; python_version < "3.9"
 Requires-Dist: numpy (>=1.19.3) ; python_version >= "3.9"
 Provides-Extra: aks
-Requires-Dist: azure-storage-file-datalake (==12.6.0) ; extra == 'aks'
+Requires-Dist: kopf ; extra == 'aks'
 Requires-Dist: azure-storage-blob (==12.14.1) ; extra == 'aks'
-Requires-Dist: azure-cli (==2.40.0) ; extra == 'aks'
+Requires-Dist: azure-identity (==1.11.0) ; extra == 'aks'
+Requires-Dist: azure-mgmt-privatedns ; extra == 'aks'
+Requires-Dist: adlfs (==2023.1.0) ; extra == 'aks'
 Requires-Dist: urllib3 ; extra == 'aks'
-Requires-Dist: azure-mgmt-rdbms ; extra == 'aks'
 Requires-Dist: kubernetes ; extra == 'aks'
-Requires-Dist: kopf ; extra == 'aks'
+Requires-Dist: azure-cli (==2.40.0) ; extra == 'aks'
 Requires-Dist: azure-mgmt-containerservice ; extra == 'aks'
-Requires-Dist: adlfs (==2023.1.0) ; extra == 'aks'
-Requires-Dist: azure-mgmt-privatedns ; extra == 'aks'
-Requires-Dist: azure-identity (==1.11.0) ; extra == 'aks'
+Requires-Dist: azure-storage-file-datalake (==12.6.0) ; extra == 'aks'
+Requires-Dist: azure-mgmt-rdbms ; extra == 'aks'
 Provides-Extra: aliyun
 Requires-Dist: alibabacloud-tea-openapi (==0.3.7) ; extra == 'aliyun'
 Requires-Dist: alibabacloud-vpc20160428 (==2.0.20) ; extra == 'aliyun'
 Requires-Dist: alibabacloud-vpcpeer20220101 (==1.0.6) ; extra == 'aliyun'
 Requires-Dist: alibabacloud-ecs20140526 (==3.0.4) ; extra == 'aliyun'
 Requires-Dist: alibabacloud-ram20150501 (==1.0.3) ; extra == 'aliyun'
 Requires-Dist: alibabacloud-oss20190517 (==1.0.5) ; extra == 'aliyun'
 Requires-Dist: ossfs (==2023.1.0) ; extra == 'aliyun'
 Provides-Extra: all
-Requires-Dist: alibabacloud-ram20150501 (==1.0.3) ; extra == 'all'
+Requires-Dist: alibabacloud-tea-openapi (==0.3.7) ; extra == 'all'
 Requires-Dist: azure-storage-blob (==12.14.1) ; extra == 'all'
+Requires-Dist: huaweicloudsdknat (==3.1.35) ; extra == 'all'
+Requires-Dist: s3fs (==2022.11.0) ; extra == 'all'
+Requires-Dist: alibabacloud-ecs20140526 (==3.0.4) ; extra == 'all'
+Requires-Dist: google-api-python-client (==2.48.0) ; extra == 'all'
+Requires-Dist: azure-mgmt-containerservice ; extra == 'all'
 Requires-Dist: azure-mgmt-rdbms ; extra == 'all'
-Requires-Dist: botocore ; extra == 'all'
 Requires-Dist: kopf ; extra == 'all'
-Requires-Dist: s3fs (==2022.11.0) ; extra == 'all'
-Requires-Dist: adlfs (==2023.1.0) ; extra == 'all'
-Requires-Dist: huaweicloudsdkeip (==3.1.35) ; extra == 'all'
-Requires-Dist: google-cloud-storage (==2.3.0) ; extra == 'all'
-Requires-Dist: azure-identity (==1.11.0) ; extra == 'all'
-Requires-Dist: alibabacloud-vpc20160428 (==2.0.20) ; extra == 'all'
-Requires-Dist: huaweicloudsdkecs (==3.1.35) ; extra == 'all'
-Requires-Dist: azure-cli (==2.40.0) ; extra == 'all'
+Requires-Dist: alibabacloud-oss20190517 (==1.0.5) ; extra == 'all'
 Requires-Dist: urllib3 ; extra == 'all'
-Requires-Dist: alibabacloud-tea-openapi (==0.3.7) ; extra == 'all'
-Requires-Dist: huaweicloudsdkvpc (==3.1.35) ; extra == 'all'
 Requires-Dist: kubernetes ; extra == 'all'
-Requires-Dist: huaweicloudsdkiam (==3.1.35) ; extra == 'all'
+Requires-Dist: huaweicloudsdkvpc (==3.1.35) ; extra == 'all'
+Requires-Dist: huaweicloudsdkecs (==3.1.35) ; extra == 'all'
 Requires-Dist: ossfs (==2023.1.0) ; extra == 'all'
-Requires-Dist: huaweicloudsdknat (==3.1.35) ; extra == 'all'
-Requires-Dist: alibabacloud-oss20190517 (==1.0.5) ; extra == 'all'
-Requires-Dist: alibabacloud-ecs20140526 (==3.0.4) ; extra == 'all'
-Requires-Dist: google-api-python-client (==2.48.0) ; extra == 'all'
-Requires-Dist: gcsfs (==2022.11.0) ; extra == 'all'
-Requires-Dist: alibabacloud-vpcpeer20220101 (==1.0.6) ; extra == 'all'
+Requires-Dist: huaweicloudsdkeip (==3.1.35) ; extra == 'all'
+Requires-Dist: esdk-obs-python (==3.22.2) ; extra == 'all'
+Requires-Dist: huaweicloudsdkiam (==3.1.35) ; extra == 'all'
+Requires-Dist: huaweicloudsdkims (==3.1.35) ; extra == 'all'
+Requires-Dist: adlfs (==2023.1.0) ; extra == 'all'
 Requires-Dist: boto3 (==1.24.59) ; extra == 'all'
+Requires-Dist: alibabacloud-vpcpeer20220101 (==1.0.6) ; extra == 'all'
+Requires-Dist: alibabacloud-ram20150501 (==1.0.3) ; extra == 'all'
 Requires-Dist: azure-storage-file-datalake (==12.6.0) ; extra == 'all'
-Requires-Dist: huaweicloudsdkims (==3.1.35) ; extra == 'all'
-Requires-Dist: esdk-obs-python (==3.22.2) ; extra == 'all'
-Requires-Dist: azure-mgmt-containerservice ; extra == 'all'
-Requires-Dist: google-cloud-container (==2.21.0) ; extra == 'all'
+Requires-Dist: google-cloud-storage (==2.3.0) ; extra == 'all'
+Requires-Dist: alibabacloud-vpc20160428 (==2.0.20) ; extra == 'all'
+Requires-Dist: azure-identity (==1.11.0) ; extra == 'all'
 Requires-Dist: azure-mgmt-privatedns ; extra == 'all'
+Requires-Dist: gcsfs (==2022.11.0) ; extra == 'all'
+Requires-Dist: google-cloud-container (==2.21.0) ; extra == 'all'
+Requires-Dist: azure-cli (==2.40.0) ; extra == 'all'
+Requires-Dist: protobuf (==3.20.3) ; extra == 'all'
+Requires-Dist: botocore ; extra == 'all'
 Provides-Extra: aws
 Requires-Dist: boto3 (==1.24.59) ; extra == 'aws'
 Requires-Dist: s3fs (==2022.11.0) ; extra == 'aws'
 Requires-Dist: botocore ; extra == 'aws'
 Provides-Extra: azure
 Requires-Dist: azure-cli (==2.40.0) ; extra == 'azure'
 Requires-Dist: azure-identity (==1.11.0) ; extra == 'azure'
 Requires-Dist: azure-storage-blob (==12.14.1) ; extra == 'azure'
 Requires-Dist: azure-storage-file-datalake (==12.6.0) ; extra == 'azure'
 Requires-Dist: azure-mgmt-containerservice ; extra == 'azure'
 Requires-Dist: azure-mgmt-privatedns ; extra == 'azure'
 Requires-Dist: azure-mgmt-rdbms ; extra == 'azure'
 Requires-Dist: adlfs (==2023.1.0) ; extra == 'azure'
 Provides-Extra: eks
+Requires-Dist: kopf ; extra == 'eks'
+Requires-Dist: boto3 (==1.24.59) ; extra == 'eks'
 Requires-Dist: urllib3 ; extra == 'eks'
+Requires-Dist: s3fs (==2022.11.0) ; extra == 'eks'
 Requires-Dist: kubernetes ; extra == 'eks'
 Requires-Dist: botocore ; extra == 'eks'
-Requires-Dist: kopf ; extra == 'eks'
-Requires-Dist: s3fs (==2022.11.0) ; extra == 'eks'
-Requires-Dist: boto3 (==1.24.59) ; extra == 'eks'
 Provides-Extra: gcp
 Requires-Dist: google-api-python-client (==2.48.0) ; extra == 'gcp'
 Requires-Dist: google-cloud-storage (==2.3.0) ; extra == 'gcp'
 Requires-Dist: google-cloud-container (==2.21.0) ; extra == 'gcp'
 Requires-Dist: gcsfs (==2022.11.0) ; extra == 'gcp'
+Requires-Dist: protobuf (==3.20.3) ; extra == 'gcp'
 Provides-Extra: gke
-Requires-Dist: urllib3 ; extra == 'gke'
+Requires-Dist: kopf ; extra == 'gke'
 Requires-Dist: gcsfs (==2022.11.0) ; extra == 'gke'
+Requires-Dist: urllib3 ; extra == 'gke'
 Requires-Dist: kubernetes ; extra == 'gke'
-Requires-Dist: kopf ; extra == 'gke'
+Requires-Dist: google-cloud-container (==2.21.0) ; extra == 'gke'
 Requires-Dist: google-api-python-client (==2.48.0) ; extra == 'gke'
+Requires-Dist: protobuf (==3.20.3) ; extra == 'gke'
 Requires-Dist: google-cloud-storage (==2.3.0) ; extra == 'gke'
-Requires-Dist: google-cloud-container (==2.21.0) ; extra == 'gke'
 Provides-Extra: huaweicloud
 Requires-Dist: huaweicloudsdkecs (==3.1.35) ; extra == 'huaweicloud'
 Requires-Dist: huaweicloudsdkvpc (==3.1.35) ; extra == 'huaweicloud'
 Requires-Dist: huaweicloudsdknat (==3.1.35) ; extra == 'huaweicloud'
 Requires-Dist: huaweicloudsdkeip (==3.1.35) ; extra == 'huaweicloud'
 Requires-Dist: huaweicloudsdkiam (==3.1.35) ; extra == 'huaweicloud'
 Requires-Dist: huaweicloudsdkims (==3.1.35) ; extra == 'huaweicloud'
```

## Comparing `cloudtik-1.1.0.dist-info/RECORD` & `cloudtik-1.2.0.dist-info/RECORD`

 * *Files 5% similar despite different names*

```diff
@@ -1,58 +1,59 @@
-cloudtik/__init__.py,sha256=CdO343ihmKiHvhNPm12H275mOJTqvmYlVC3CyJoKjLE,140
+cloudtik/__init__.py,sha256=IVPYdKEXMVrnB6MRaby7piOo4MU1HTU2iLx7HSFUG5M,140
 cloudtik/core/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-cloudtik/core/api.py,sha256=e0rmyqX8roZCotlgfmcsq0ewuyXRTmpxBIQM2jeJcp4,31841
+cloudtik/core/api.py,sha256=k6KXqLMjecFSm03EAXD4V8XmIk7fPPAFG9d-T7SCco4,38133
 cloudtik/core/command_executor.py,sha256=LUCWHfTxty33IaweFmXsNR0QHs9CSFcg9pl2Cq81Q_U,7087
-cloudtik/core/config-schema.json,sha256=ppPu29pP0b6ydNlHHrWqgTYgFAIaVf5Y2c4eSw9hY_Q,57632
+cloudtik/core/config-schema.json,sha256=Vj8IHt-Q1le55r-ePpIfpx-C7DWl9DOXdNZ48liuR_g,59840
 cloudtik/core/job_waiter.py,sha256=toyNT9aUEm8X1rmKrRT-lqT7uc18jqw6CGdwqiTs2dg,794
 cloudtik/core/node_provider.py,sha256=az5sLI1rUKqlp4TP9YyqMGaN4fNreWYO_giTUsHtXUQ,12163
 cloudtik/core/runtime.py,sha256=FsGwX4VM_yw81Xhsks7OgYyA7i8S9GjZ7zDDCB0xblo,6200
 cloudtik/core/scaling_policy.py,sha256=vKl1nu31lQz1Yl2IaadbnvAIhnWjOiHOXO77CRJR-SY,2054
 cloudtik/core/tags.py,sha256=HuV9_x_31evMLwTa_ODn6ZdGHg0MfrhvEqvuITahgJk,2175
 cloudtik/core/workspace-schema.json,sha256=9PTNo_v0dh4aLEpiES05rvdmD_Cuyt4JN2WDPE3QOVo,14561
 cloudtik/core/workspace_provider.py,sha256=RwEy2fihU1zgEXVC3f3gFsFV2-UdW4wStHQI4ky-PaQ,3984
 cloudtik/core/_private/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/core/_private/call_context.py,sha256=ZQ6Z3UtKLvocGQJdGA3Gf-krchrQyjxlarCIg_E6t5U,3244
-cloudtik/core/_private/cli_logger.py,sha256=nMgdVNrlqATt_cmCxmVy3jQO87YkatNleqD_7RwWL9g,26421
+cloudtik/core/_private/cli_logger.py,sha256=N-h45XRK2qj3_n3qXv3BtUSbDYc3e_cSanon57YWRoc,26512
 cloudtik/core/_private/concurrent_cache.py,sha256=SgnbtMpjl2ja7U1mA--EDkfdyCCuEsPN1eLAi9NnVIA,527
 cloudtik/core/_private/constants.py,sha256=MYEmVayiPwPH4LnFvOR0TDFkb4ZUUBuU9K5kvn83gPM,10524
 cloudtik/core/_private/core_utils.py,sha256=DvVP0MNmrXCKLY5EKzS9Mj8oK5ecPtJTcrwVmFTGjUk,27882
 cloudtik/core/_private/crypto.py,sha256=zYxNI8ly8f361ek3v_wsOwm2ML1BdPQ7iHCe4kQwKMY,928
 cloudtik/core/_private/debug.py,sha256=uOfGu-251q2w4HiD-jLGFLV4JhmJsEdjhp3zVah2BOA,996
-cloudtik/core/_private/docker.py,sha256=rbxbvOUbhWxNGrbKX5pOd0YTZhcMLzDlDosjLCUAB08,6686
+cloudtik/core/_private/docker.py,sha256=86keg8Md2OVfnW2QWW65R3cMQMPGJm5mSydiaie53Is,7705
 cloudtik/core/_private/event_system.py,sha256=fs1wWJlt89AAA-iCrahJuigbbMAdhhZaqm1kTKwq3zw,5765
 cloudtik/core/_private/log_timer.py,sha256=OsqYf9BXhP6Ddjf_sZbh1nxac7gIN8Wqqj69O8yK3Zg,778
 cloudtik/core/_private/logging_utils.py,sha256=0VNzMfDD-PvrezbacjcAlk2cZgtWHUitxyFl508oPNs,7351
 cloudtik/core/_private/parameter.py,sha256=IyhJFmgvfZZ65YVdaoMdmVYhmvDY_JMEt_ITNeiEZjM,9821
 cloudtik/core/_private/prometheus_metrics.py,sha256=vDHWZ8p4CzXO7n5OBRs6Ga7dN3Dt0_PxWdTyDXolQe0,9586
 cloudtik/core/_private/providers.py,sha256=0dErqapY0rPAuATEHOg2J1rGhq9-xfTqUukXtFFgbvw,12169
 cloudtik/core/_private/resource_spec.py,sha256=4x3dXh2nO9fjUJY7Z9w3woO7GLJ5JhO6pQv2_KZbmM8,10420
 cloudtik/core/_private/runtime_factory.py,sha256=yl-FMlSzTTlhsGp8K-WBNTnoDa-mnWO7jt92OFuH_h4,4856
+cloudtik/core/_private/script_registry.py,sha256=x9x0ByV18mu_bqaOCCNuX_-oYhu0ctBvvqFJOA753Rw,1675
 cloudtik/core/_private/services.py,sha256=IpX7GQovz5xJ_DXqibMt9UjVqp2io1-GITQ1LZu2DdA,52664
 cloudtik/core/_private/subprocess_output_util.py,sha256=hzp5xX42iljfy8DEhRNPl-YBCFdzRu_M2355XuOIgsI,15709
-cloudtik/core/_private/utils.py,sha256=ESzcZuXS4V77g-8hNszZDuH46n9hZO_a01n8DoYYC9U,114241
+cloudtik/core/_private/utils.py,sha256=3xc8XrpGY1LxYXVciIvShhl-z8V30jpipKXJgoeWfK8,115899
 cloudtik/core/_private/cluster/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/core/_private/cluster/cluster_config.py,sha256=oZJYMEBiLiSUe_zy7j3CuM12EU99nVfPZrRYrMDCVnM,5681
-cloudtik/core/_private/cluster/cluster_dump.py,sha256=jl7aLXmi-2f0W1rCE2J2hNUm_oJaCeg798gu34ZzW7E,23389
-cloudtik/core/_private/cluster/cluster_exec.py,sha256=oAte5wSv1Ql0nOcvZh6K28lhIkMlPgzSDfPUvOGFviU,4451
+cloudtik/core/_private/cluster/cluster_dump.py,sha256=nSBkQTYeq9ZSLHyXSaoPUgElM7m5tEFe-bEigQJhrOk,23994
+cloudtik/core/_private/cluster/cluster_exec.py,sha256=LnF8dZSpxHEIF7eJCDxshNOqVhTF78PsT1MCCrjuARk,4405
 cloudtik/core/_private/cluster/cluster_metrics.py,sha256=xZ7cmUVIufFucnENSTDXgVaGUfIq1-SuNGkHvQ00MFU,16339
 cloudtik/core/_private/cluster/cluster_metrics_updater.py,sha256=IOrqzNhzRkmjGd_KUpyISvufxz3yvj83i9itqZk0rak,6061
-cloudtik/core/_private/cluster/cluster_operator.py,sha256=I3wkO0_L8FhRillKOvEXP-__4EAOCsmxDYW2TxAhqFs,157481
-cloudtik/core/_private/cluster/cluster_scaler.py,sha256=zuHfiFE9eB2UZKo8O8zkf-1nv9MuNFWjLfqNjgQtR2w,72464
+cloudtik/core/_private/cluster/cluster_operator.py,sha256=lDdVDtj7tFrehnM-ytHzOhkh2DDrXiiLz8zvns-xTHs,161232
+cloudtik/core/_private/cluster/cluster_scaler.py,sha256=a15ureRkmscYCtKI2xUlXdkE3dclRL0RXLws9UvO_8w,72464
 cloudtik/core/_private/cluster/cluster_tunnel_request.py,sha256=GMordtsHv2sZ3R1PGu5AO5ONnaykNuO12RiIok28KZA,4432
 cloudtik/core/_private/cluster/cluster_utils.py,sha256=rjAHQzzx52S1AKDGBPgmSWZPmtjCoijCOcNiLSOPMtU,3399
 cloudtik/core/_private/cluster/event_summarizer.py,sha256=dBZzmcE8WswroSMoFSnMANGRQWciQmxNfjYtPdt4YCs,2899
 cloudtik/core/_private/cluster/node_launcher.py,sha256=UPs9wzw9b0RKJ6jrMQhM2tAG9mWkUEzn00I2llGHQP0,4904
 cloudtik/core/_private/cluster/node_tracker.py,sha256=LoTvGeZ6wBrC1YRJH6MSefZEMn6yOpK0K3swfxUCyCk,2799
 cloudtik/core/_private/cluster/resource_demand_scheduler.py,sha256=0qpc5cVkGIy3LB9V5Q3xuoGm4RBiH-KuHyRF56wJ3gg,30707
 cloudtik/core/_private/cluster/resource_scaling_policy.py,sha256=riimvU5B8l2GXY5jk-iyPhTYUYuFnFiCsANLMI41kBI,2832
 cloudtik/core/_private/cluster/scaling_policies.py,sha256=ih4T-4Le9rEqZRc4Rht4AKZa9NKZFCMEYNcpfMps2HA,23347
 cloudtik/core/_private/command_executor/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/core/_private/command_executor/command_executor.py,sha256=nCYyqxF8oDSkXd7mA1kakV0ypFgonw_gC1O33TqOArE,3099
-cloudtik/core/_private/command_executor/docker_command_executor.py,sha256=f3Q85AsgMs52LtRoTj2Nw5Mnz-M1rxzxmnucfGaIjLU,22949
+cloudtik/core/_private/command_executor/docker_command_executor.py,sha256=2P_xTMmx6--SG5C0HNVRkLa45QY7YQkv9VmBUL5K2XU,22864
 cloudtik/core/_private/command_executor/host_command_executor.py,sha256=rO3B4IhnEcMnitCVgZlXKCKSQXFi9pY0DW_j6Hn5RA8,8206
 cloudtik/core/_private/command_executor/kubernetes_command_executor.py,sha256=NMMw9YudCdAeuFWyc3L7oDRXUWO9euG9_bpalGlc0O8,9259
 cloudtik/core/_private/command_executor/local_command_executor.py,sha256=kyRKu38Gesf2iS6YnlMp0AMv-u-zFJM0G1xuMsY5c7U,5744
 cloudtik/core/_private/command_executor/ssh_command_executor.py,sha256=-OxMKIPsEcww6c7SahEfivsqE8lahM6-kaBLpwW0t7o,10252
 cloudtik/core/_private/job_waiter/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/core/_private/job_waiter/job_waiter_chain.py,sha256=Nvgvk2ZIsyprZesEIdVjWLjOU5zXNqmzDxgPpEQuAg8,681
 cloudtik/core/_private/job_waiter/job_waiter_factory.py,sha256=3AVeH4TU634WGWX5Z_7lZlWvRp4uNBcaT2S0tB6Mm64,4667
@@ -149,15 +150,15 @@
 cloudtik/providers/_private/onpremise/workspace_config.py,sha256=z2q4ZNItbTnSfQpV67JUW2j8kHOGtoXtgzlk2sgOY-c,5993
 cloudtik/providers/_private/onpremise/workspace_provider.py,sha256=NjucrrUBFlZDXDXavykXNpJCNulIqKFkQlckSh177lY,3385
 cloudtik/providers/_private/virtual/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/providers/_private/virtual/config.py,sha256=7xbI3osiv1Vt5yVcUn3Eamuy4omOlcHE_d1NIL0-dBo,14869
 cloudtik/providers/_private/virtual/node_provider.py,sha256=RY6CeqjBl2ykh1Woas0Jmga7XGMui-WBNyY2CsC7Kf4,5860
 cloudtik/providers/_private/virtual/sshd_config,sha256=THdRJLF0iA4IklLwcUrFavbdyqwU3OSYeGMYyqe2jfQ,3287
 cloudtik/providers/_private/virtual/utils.py,sha256=u2orTfESxxPhdVbZGJlFesgOWWuhgE8alCoDdqQIWBs,1193
-cloudtik/providers/_private/virtual/virtual_container_scheduler.py,sha256=DvZcE-pAedFrksjH6VeEgSjX9uOyLbr0C_Rv2NA1KFE,24469
+cloudtik/providers/_private/virtual/virtual_container_scheduler.py,sha256=of5eIPPHpteceZzSEuf1772fVx4Dpa-APGe6qBijI_8,24515
 cloudtik/providers/_private/virtual/virtual_docker_command_executor.py,sha256=cpSgLKs2z4Ti7UFUrNK0tkTnd_OrC5zBDQf-y2E0eL8,1777
 cloudtik/providers/_private/virtual/workspace_config.py,sha256=kBPwyYekevjZdbIMsm6bD344bxJJKTONpXTBwX1auzE,16327
 cloudtik/providers/_private/virtual/workspace_provider.py,sha256=7ghKRxpfIAoUfO5hb2hUg478xVeH6551rS1giKiIfQw,4567
 cloudtik/providers/aliyun/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/providers/aliyun/commands.yaml,sha256=qzNcuo-uWYVxTH_sUqx0n3Fb_5-1S9ySSoVPWuotR5U,54
 cloudtik/providers/aliyun/defaults.yaml,sha256=Jq3PR_gOAMwQ3fUMZoXnF4GpXllNgHYpkmdIioubzgc,4472
 cloudtik/providers/aliyun/workspace-defaults.yaml,sha256=bTEfNHv2B8VAlrF8y8KuqbPCp_om4I7n8NpcmPsw-gc,417
@@ -195,16 +196,16 @@
 cloudtik/providers/onpremise/service/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/providers/onpremise/service/cloudtik_cloud_simulator.py,sha256=ERrdXLGMBZd2fLvdxOOl83C4PmCmGLaOBNi29uBdcU0,11268
 cloudtik/providers/virtual/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/providers/virtual/commands.yaml,sha256=t-MQ95mY4qc8FNaoQ9_wdBOwYYH-g26wYP_NCTc30kY,273
 cloudtik/providers/virtual/defaults.yaml,sha256=JGEZv6Vds9ILsrzn7v6uQRnBDUGjZ-GVUXTn2CZrv7I,751
 cloudtik/providers/virtual/workspace-defaults.yaml,sha256=gQVHxIk00fRB_oL-NvsZvz_gsYtyhVe6sIPbWvoIcew,94
 cloudtik/runtime/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-cloudtik/runtime/ai/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-cloudtik/runtime/ai/api.py,sha256=yEDXwrNSibvpdn7QXIh4qcqOq4pJbytcYxG4t6wEcL4,995
+cloudtik/runtime/ai/__init__.py,sha256=bSoH-z1UVZ9fU6fdNr_HsLpfVEuxIyJ0I63UMaFaoSU,561
+cloudtik/runtime/ai/api.py,sha256=fqAkXxfq5tpGqBDua_mBzFSfN4Jlg33P0Lf4hmVHajE,1198
 cloudtik/runtime/ai/runtime.py,sha256=zZukKl_uGyIjO2-8tKpFt0YtcZxm61hTg5Yllivvthw,1576
 cloudtik/runtime/ai/utils.py,sha256=8RAvibsQW3--aXxCvSkBhN6c5Tpfu10IoyFyP6F42iU,2410
 cloudtik/runtime/ai/conf/adlfs_spec.py.patch,sha256=fWaSSyGz4Vv8JHnMU6RxhDmsv4ZRbgSSyB15GIO7VrQ,75965
 cloudtik/runtime/ai/conf/gcsfs_core.py.patch,sha256=3EigUHXdatWyiOqATRN_9hq4KdVFYwUxiGEs26kL-0c,59836
 cloudtik/runtime/ai/conf/horovod_ray_utils.py.patch,sha256=20mAqz5V2PKb89h3fUx3Apntpjg1LaRkambLI46OuQ4,3297
 cloudtik/runtime/ai/conf/horovod_runner_common_service_driver_service.py.patch,sha256=QfWQPakeV_l0bb941ASLIRp0uNfCQ9wA8o_dPGlxLvI,8909
 cloudtik/runtime/ai/conf/horovod_runner_driver_driver_service.py.patch,sha256=IfYOxXZ-8OZqmYSO52NnlowKiK-u1YM-7XFbQInSQuU,11962
@@ -225,191 +226,248 @@
 cloudtik/runtime/ai/config/commands.yaml,sha256=s_jwgRlpwHIlPv6Y6LhanufDduU7LQW5Fkp0nqzF3dk,965
 cloudtik/runtime/ai/config/defaults.yaml,sha256=5vY5u9K47w2bOdkthw4OAokTI9sr8rcL9kXHrgpsUz8,25
 cloudtik/runtime/ai/modeling/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/runtime/ai/modeling/classical_ml/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/requirements.txt,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/config/data-preprocessing-config.yaml,sha256=hqLI8VTSuGOdHl9gqe69Jt1Gu1rtW9RZyz0RVPwgmN0,1930
-cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/config/training-config.yaml,sha256=qzGQi5saZMJQBL2h9i4XecZGI3K1exeb-YqTUDpAS9E,1412
-cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/process_data.py,sha256=xZyeKZf-UmlJ3ovTP3Yje5gCtDcjg0OmBGl0nDYQH4c,3812
-cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/run.py,sha256=j9LuF6w4DFMHU9j3D-H2wbV-N6u7x85viBvkMibd0bQ,5754
-cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/train_ray.py,sha256=oNYmuZ4NpuLCOQDfAm8P3ldQS_eiNGNEj6Ukn5u9MA0,17346
-cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/train_spark.py,sha256=6B6DcLLBNGUQhq3t4aogAwAfJSoiEE-l2S39n5Q_-dE,1198
-cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/utils.py,sha256=q2Fo-x2lAgn0_lXeWXqWvTyhVt9AqR3QbNZT4xdyIck,2586
-cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/data_processing/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/data_processing/data_splitting.py,sha256=NHbO_BYm3hY4vJYwlX0jsNKyrjjvlY9tSxQF5y4W6Mg,1261
-cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/data_processing/data_transform.py,sha256=R5NZZOyKmZvXF-nTNApphPlss5BiVU-RJzqdolxS27o,8725
-cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/data_processing/post_transform.py,sha256=rV1UMosPARCwx-D8OhiZnvJ96FILXI5mSK1QhdYGX9c,4316
+cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/config/data-processing-config.yaml,sha256=hqLI8VTSuGOdHl9gqe69Jt1Gu1rtW9RZyz0RVPwgmN0,1930
+cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/config/dataset-config.yaml,sha256=aDqrqL1-TKmNlPF6qTjskeqaGWBQKaiMk37ChUZTr0s,210
+cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/config/training-config.yaml,sha256=7CHvavp9PKnPh-mfV2PrLm0bOiZhUgdS-Iza1PFDdak,1212
+cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/__init__.py,sha256=ljkv__OCuorZfyDAkwk8b-FGHQhlVRUSImc-YBinCIU,1001
+cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/run.py,sha256=hi_2ZUFjHf49e0cadaRvn1lDZMxjmOg4TSfPavlvRvI,11593
+cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/utils.py,sha256=0ERRAhQWSgl8hZdpcrw3nPxLtIDUpdntmZFzJG17Kjw,2698
+cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/data/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/data/data_splitting.py,sha256=hfKub9E-tk8w-biacBbMu2r8EQUpcBozzwHtjg22arU,1252
+cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/data/data_transform.py,sha256=-wZ6xTYF7ALscaausik9OqK8hPwNevjNqqorzlw1kuw,8626
+cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/data/post_transform.py,sha256=w20ynVg3cddsSjze_skop2DmVr1pIc47ZjxeyCQWyD4,4306
+cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/data/process.py,sha256=SMffsTpqv6TrRhRpg65sSwe39QyZ-kpgaN4q921dr08,3195
+cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/model/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/model/model.py,sha256=oeqOZqctmOdmYF8bzBSE0mjjAnhkJwmM7smu7htb91c,4847
+cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/model/predictor.py,sha256=43qtHN6o3Tcmr-yFxcMHOTugjdTaRLf_gLBkrylyjtA,2777
+cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/model/trainer.py,sha256=DzNqQRxbn97aCtA7LEUoxt-yMGIKdptPSk5orwhUZys,6683
+cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/model/ray/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/model/ray/model.py,sha256=4bu5eJ_klZ-LwrI8zTr_tTXPj6zCRI97EM6LHkqyPr0,5006
+cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/model/ray/trainer.py,sha256=tCsn6q4KP4I_jsIl_EtsXkRp2G4Oc8w0VVxQzChQ6Lw,3908
+cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/model/spark/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+cloudtik/runtime/ai/modeling/classical_ml/classification_and_regression/xgboost/modeling/model/spark/trainer.py,sha256=6B6DcLLBNGUQhq3t4aogAwAfJSoiEE-l2S39n5Q_-dE,1198
 cloudtik/runtime/ai/modeling/graph_modeling/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/config/tabular2graph.yaml,sha256=iVr2XsczqcilzCBDl_UT-po-L2JrwMD3lsvsFoNufos,1045
-cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/build_graph.py,sha256=XODBT6UGX_SXMUBIhTLgFhI8aWy29CYc7c9Wzltg5SA,6775
-cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/launch.py,sha256=zhMmZVtT_jszsrWnNBmqUwJD7eggw4CPyhrRemYECfE,31974
-cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/map_embeddings.py,sha256=M_2Onxwq3JKbVtOhcbbfz-OaMXKpoXBrEtAMQyuatf4,5011
-cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/map_embeddings_single.py,sha256=IABFI8U3vR73jA4MSpB8I-rvdDDERLO_k8cmNdHaPNc,4400
-cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/partition_graph.py,sha256=LJDUI6ad0i-CMNOfIal2KRYD40KVPbKWSeJX9SIcfYw,3124
-cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/process_data.py,sha256=DKoEdEu3xeGuOOxkZ_793qiQ3A1txEBHFS8bc_Xt7oc,4620
-cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/run.py,sha256=TgWdJYMtHYqxyjanLMjEfVL9r1dpzQ_zPKavre2XyVg,11651
-cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/train_graph_sage.py,sha256=4OAXkaTBUfvfYkKQupW3pC5BzxEnlBBxrwBORTMIVpQ,19752
-cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/train_graph_sage_single.py,sha256=9Ff5-hDS-LGpvvdmMCzMaCF43-GYO8Jt-z_PrbdH9Yo,13642
-cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/utils.py,sha256=DMlhs74SAsd1VkmvvKIYM3cVE_2fkoEX4HjF62dj620,469
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/config/tabular2graph.yaml,sha256=BYVpzqFepyUnwTLiZon196UYnXc0C7wu2hW0Yqx-0Kw,1618
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/__init__.py,sha256=qUVDV9rqDfxszjAu7kdmfygI7yDdDhif-T2oxptMAL8,1620
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/build_graph.py,sha256=EDeIu2RXQxLqPAcxarujHjPWnFy706ja4WEcRo-Hz0c,10245
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/embeddings.py,sha256=_Euo1DCOaBC0vYWWlpwuOEJQCxwcuJXWtwcflMIIByc,7008
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/launch.py,sha256=jFEcO0AIIcfEVyo5bn2h2lAhTxlk-96sTnUGc_ZdS0g,33297
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/partition_graph.py,sha256=SOYqYAkPVglWL5-8SLLdWKcbwO1-HB7GdyQQjnARHyw,2813
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/run.py,sha256=Bt2mWAZiOm8ojbzjGSKfuCkt8z9_EbquZyZ0Zg4x6xU,23233
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/tokenizer.py,sha256=VUQ628gm38fGkLgS0yyYRFtGaddTozo3oXuIX78UAkw,3400
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/utils.py,sha256=aItDoGs2pjJ-UzksVyE_5vjCSk2L8yTOoFiLDoFMB_g,1560
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/data/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/data/process.py,sha256=ZDRL_rHyROtCCcu852Ct1Osm_cX81R7lxWL8S0PgrJo,4133
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/utils.py,sha256=pBOZ0c5mTlFCUiotHmgOnciE8TLZr4E717OIZARoM38,2346
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/model.py,sha256=SEcneBQeU6GscbDhHbEc3Ay7GLwS0ZEQcWF2s-cBrmQ,9669
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/predict.py,sha256=FcRIXkIF7gyqEDsfifmLL1ByhZYfRnylFNChUsimTP4,2949
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/predictor.py,sha256=eNRCPAcq6OSr63sKu1ue9YISLmTBOiTVQLs9dTcTAdI,1538
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/train.py,sha256=cPuiNW06y4g3SvRTi6g0a0qwvsZ7Aalsu2crf7Gayb4,6718
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/trainer.py,sha256=4SL7OqUGW0tn_dHda0z86litZEqKEUfRnCrPx-4pvfE,9602
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/utils.py,sha256=hN0P9iF2K_AGe-60iAYPCUiW8Xnidfdl0hdlrVh79HQ,4456
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/distributed/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/distributed/model.py,sha256=6LTU6aHdFdP6BLmCwfmdm5edIiV4gGajKPsQG5KZmiI,3947
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/distributed/train.py,sha256=iivmliinAtZklHDBeFhdxhi-nx0XldFoLb9m-_lK4Mo,7358
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/distributed/trainer.py,sha256=i5UCdvq45BXhGVc4BNltLk679BvcPFDNLC-m9c33qwk,14232
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/distributed/utils.py,sha256=sJvGg8oLG_ANBpoSAl65BvPT46boYgaTlYRca3EUGYI,3089
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/inductive/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/inductive/model.py,sha256=BvxuXy9QcxHQwbyZub724rhXUxl5SQ3hTLMutgdMnUY,1811
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/inductive/distributed/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/inductive/distributed/model.py,sha256=7WvYzl19cVPFVWQJq_qLOu_ABQGJ6495lWV2Iz15AoA,1442
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/transductive/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/transductive/model.py,sha256=k0vKqj1lXQF7bMSRhrCsFyKjhNheQBxNMnANEoOrvTQ,1822
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/transductive/distributed/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/heterogeneous/transductive/distributed/model.py,sha256=QzjcvGO2QfWwX4juPQe0rH9xQ8ocRSFRXbKagKDSKvo,1113
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/model.py,sha256=HtINmcRhw6VSmj371qvteNGC0Sby-4EStr1m4-6I-4M,7668
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/predict.py,sha256=Cf4hVOIKXdMrTEaDuFJHkkZbV1C0La2hbtBTBTo0bHw,2981
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/predictor.py,sha256=Z10gfTbqEJhqVFJxvV2cXIv50b5W1nqGDyy0kZ8cbV4,1399
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/train.py,sha256=ZwrTh7rbLJ4WOudyD-fKW0UmTCONf55cmQQ2xB6Y8zw,5995
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/trainer.py,sha256=SvZKDyBVvaqDMyHiEwSHT6JMyJ0yKHEPxleaI6ih5v8,9226
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/utils.py,sha256=fUskhK3vJljSNsEkTlH3uLAebgPPaZyb_Urge1XjQAc,4620
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/distributed/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/distributed/model.py,sha256=bKwH3PxidSKL9zNlIUKDxDBn-xS3oVu8x6yWH63qi-w,3547
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/distributed/train.py,sha256=Mz4ZmInj7Y8Dsmp61YvIoH2uCsSUn39TbkmOPHWoYKA,6607
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/distributed/trainer.py,sha256=Xh--FTcWUQLiROaLNKOF2B1idNPjd1S-ce2qBvItA8M,14868
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/distributed/utils.py,sha256=m8PJMfkZgpY9tcUFaWCTVQiQMG4Nxq0Vi5R81AGKfnw,1626
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/inductive/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/inductive/model.py,sha256=1rKUYtLGcEwh_IC-Qd1Hw2mt8mg-WsHmss4eFjVoEbQ,2096
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/inductive/distributed/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/inductive/distributed/model.py,sha256=gZdfhzWW6SOnFLXtJvj54cxStOjcZhcWheuNxS2_fNk,1456
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/transductive/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/transductive/model.py,sha256=8Z54utvfuLThO7oKtTQEg99l0jXU4798sv3-9v1Wyng,1509
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/transductive/distributed/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+cloudtik/runtime/ai/modeling/graph_modeling/graph_sage/modeling/model/homogeneous/transductive/distributed/model.py,sha256=Z6N_9KNDPp7sB_3ygwB8DOxaO3rlsVFD4Fs86TVYR9Q,1087
 cloudtik/runtime/ai/modeling/transfer_learning/__init__.py,sha256=dkTaUfI6ZIK-QI8v5kzOthC8bqWuVh6tIeLty9EnRoE,48
 cloudtik/runtime/ai/modeling/transfer_learning/dataset.py,sha256=cGa08PKGdmCjIZZidwqHcqCXR-T7zjjdLWXDZGUvZ8Q,2054
 cloudtik/runtime/ai/modeling/transfer_learning/dataset_factory.py,sha256=HOpmYtPyuBk5Cd3cuMGnMCtYfTwFf9lxyJ9uTUpIxEc,8559
-cloudtik/runtime/ai/modeling/transfer_learning/model.py,sha256=I9620GlwEH4tAvhIy55vi88DK4LLZwGNkoq41pN80VQ,6851
-cloudtik/runtime/ai/modeling/transfer_learning/model_factory.py,sha256=FtSk70vIipF6QUgdhhs1Ixav5WDSoZApcfwXYFmszXw,11082
+cloudtik/runtime/ai/modeling/transfer_learning/model.py,sha256=FfPb2HcC7FfDReeb8pSmj06tXjv8xVpPlZnxF-QqXYo,7221
+cloudtik/runtime/ai/modeling/transfer_learning/model_factory.py,sha256=qczzBMaBX1PvvvGO3gkNG70C1Oz2MOYlmojrT87Xi04,11102
 cloudtik/runtime/ai/modeling/transfer_learning/common/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-cloudtik/runtime/ai/modeling/transfer_learning/common/utils.py,sha256=GqJAri3QIphaBGlzz7y0f5aWIllQN35bQHmA0PA2Va0,5954
+cloudtik/runtime/ai/modeling/transfer_learning/common/utils.py,sha256=rQeU2tUJLg6SbM9zGiCcWixdzLd5fyQzlq_BPAIF02E,6542
 cloudtik/runtime/ai/modeling/transfer_learning/common/downloader/README.md,sha256=od8oKZJ-Aha-AgVfzc7jiaNPj2oxknXhj-3Md_k7Gm0,1747
 cloudtik/runtime/ai/modeling/transfer_learning/common/downloader/__init__.py,sha256=QDQiPRTiUDZPTSjMysdbsBpic-cFNMuUsIYYx4uc370,674
-cloudtik/runtime/ai/modeling/transfer_learning/common/downloader/datasets.py,sha256=e6kd-zDps8bQ2ge3TSTHcJavMNoHnNo-oxVJeD5HR90,5363
+cloudtik/runtime/ai/modeling/transfer_learning/common/downloader/datasets.py,sha256=GsBYVs6R3Hwv6hne6kC3mqH01ZWoTYq4D4uCArRuJQs,5604
 cloudtik/runtime/ai/modeling/transfer_learning/common/downloader/models.py,sha256=O7ScUaYMB757GUG_AiJsN_jyo4OfCbIOgM-oSYBOZiw,4675
 cloudtik/runtime/ai/modeling/transfer_learning/common/downloader/sources.py,sha256=xhfkpr1aPlGhe9HPL6nygL6r1LO-LfGOH9gye1g-oXo,3008
-cloudtik/runtime/ai/modeling/transfer_learning/common/pytorch/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+cloudtik/runtime/ai/modeling/transfer_learning/common/pytorch/__init__.py,sha256=T1oXKy8rOviYANKjTiVHniOLUAmUqdizxYA1WbMLRHk,155
 cloudtik/runtime/ai/modeling/transfer_learning/common/pytorch/dataset.py,sha256=kC2lrfSjKS4cu8-BhTj72UZecxVILec8-WjwMt51hDs,10198
-cloudtik/runtime/ai/modeling/transfer_learning/common/pytorch/model.py,sha256=F75IUdYbuBZ_4rBSkpP3zqD9XzXeM50I0OSEivMkP_8,8408
+cloudtik/runtime/ai/modeling/transfer_learning/common/pytorch/model.py,sha256=EO0u8lMJu5oLUedGLo_PRMgkvgJzgxDIVMX3nKExOos,9489
+cloudtik/runtime/ai/modeling/transfer_learning/common/pytorch/train.py,sha256=tzIcIw4uNL5CUk9RXkzrFOUNx22mSq8qhAKwEcNysSE,3275
+cloudtik/runtime/ai/modeling/transfer_learning/common/pytorch/trainer.py,sha256=vRX66yivOSqHuCSMveO7krmOU8qmflfp4fn6GTbcsZE,8844
 cloudtik/runtime/ai/modeling/transfer_learning/common/pytorch/hugging_face/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/runtime/ai/modeling/transfer_learning/common/pytorch/hugging_face/dataset.py,sha256=SGDjRey3zjK14KikTUS7s9PeTs3IUUuMXBlERlpQb3U,13173
-cloudtik/runtime/ai/modeling/transfer_learning/common/pytorch/hugging_face/model.py,sha256=Ls2xum6FO0EF-M3CqEsJxBteV2dv8rD2uUofe_lZviA,3168
-cloudtik/runtime/ai/modeling/transfer_learning/common/tensorflow/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+cloudtik/runtime/ai/modeling/transfer_learning/common/pytorch/hugging_face/model.py,sha256=SByGyU2cz-4GUQx6ii1f0plCrQyHBMKZh-D5lW8UMbk,1859
+cloudtik/runtime/ai/modeling/transfer_learning/common/tensorflow/__init__.py,sha256=T1oXKy8rOviYANKjTiVHniOLUAmUqdizxYA1WbMLRHk,155
 cloudtik/runtime/ai/modeling/transfer_learning/common/tensorflow/dataset.py,sha256=XLa-kyhQpvnum9ckDmcTwCExQGRTSkpUcfk6oiVZfuw,4958
-cloudtik/runtime/ai/modeling/transfer_learning/common/tensorflow/model.py,sha256=JRnfGZK8dxT12WbNnrqVpw6BTzxn_3idn408-CtWWoY,12893
+cloudtik/runtime/ai/modeling/transfer_learning/common/tensorflow/model.py,sha256=hXE5BydKFL_VoqkSzdNiqHD9XOM88t4_fikbrWpl6do,10400
+cloudtik/runtime/ai/modeling/transfer_learning/common/tensorflow/train.py,sha256=sH2x1jXTAu5MG7365tNwflZwsxa2KH1tqLlPdewhjSE,3103
+cloudtik/runtime/ai/modeling/transfer_learning/common/tensorflow/trainer.py,sha256=DjR8OdyG9mamU73qwmif8FoW7yO9Z8gCVmzXrNhoLLs,6731
 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/__init__.py,sha256=QDQiPRTiUDZPTSjMysdbsBpic-cFNMuUsIYYx4uc370,674
 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/image_anomaly_detection_dataset.py,sha256=AmXaDoUeFgHMYCZEgwZ7zT9_YXtwMp8q6FtQM5PYgjk,25689
-cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/image_anomaly_detection_model.py,sha256=VVBZMcfYYBZWHFPzWqsinA3NxatAgWKtQKKtGQeg2D8,29347
+cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/image_anomaly_detection_dataset.py,sha256=a9jpzX9RTW78h_zJNIrSOPXrDh7hvLND57UozEXSj0g,25668
+cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/image_anomaly_detection_model.py,sha256=O-qPKIZFPqUs2pVGKfJZfEnzI3lOVpSVGU-IWNhzcA4,29689
 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/utils.py,sha256=LcX1mpX9sIugLEKkDZSGtsCQy2DUz55TecA7-UbOtw0,5769
 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/cutpaste/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/cutpaste/cutpaste.py,sha256=BQVmuJvkAXAamYgKmSBO2Y3m5xBaYoqhPX5rYNIVEOo,6977
 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/cutpaste/model.py,sha256=0AnkbKhFiJyPQgs2RC1PIedZe5M_EOEXo2arP3A03Ng,2390
 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/simsiam/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/simsiam/builder.py,sha256=YBSUGQsfAPbFWHhPEO9NPHJpi-f2w1fyYN6CnNrLBAA,2535
 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/simsiam/loader.py,sha256=s1qM2vh85ZtsRFof2YkvwGScQfq70h2NzfvDLrifeo8,1380
 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/torchvision/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/torchvision/image_anomaly_detection_model.py,sha256=fe4d-tu7l6yZW8Ht8_Oj9THNaT3xtV-b0xm1vQchVYs,2131
 cloudtik/runtime/ai/modeling/transfer_learning/image_anomaly_detection/pytorch/torchvision/image_anomaly_detection_models.json,sha256=uEpr3y1jtAu3y6W3ZHUSoWPlRcwQRh7DRq3fD7RgBJU,865
 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/image_classification_dataset.py,sha256=0r3tyhEQSnR3RGhPoswL4jJfFL4EYgT1ZzJydqbDnhs,1223
-cloudtik/runtime/ai/modeling/transfer_learning/image_classification/image_classification_model.py,sha256=WxtEENJ77lJ4G1EkblYxN36FWESr2IkLDrfOGZMEwaY,3320
+cloudtik/runtime/ai/modeling/transfer_learning/image_classification/image_classification_model.py,sha256=SzK2XBqPXy_84N5sk1b8hYfYpxIIvnfCgHGEalECUgQ,3337
 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/image_classification_template.yaml,sha256=ebvVGH0etNO6PuxBtSJ1tAkbJfSR7hlfRy5V1BVTbtY,3044
 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/pytorch/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/pytorch/image_classification_dataset.py,sha256=9Y7KACkn_8kJPjLkucEJjSmNTqnm3qXA_5LfusBPP0Y,6393
-cloudtik/runtime/ai/modeling/transfer_learning/image_classification/pytorch/image_classification_model.py,sha256=hBVwgeUK90FyHT-23hkxLhncmfwvX6tRJUq4dHlNACY,36293
+cloudtik/runtime/ai/modeling/transfer_learning/image_classification/pytorch/image_classification_model.py,sha256=NHzZ5-ksHkzGISHgJyRE078mnpRuf06_NFJA9URT9GE,33503
 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/pytorch/pytorch_hub/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/pytorch/pytorch_hub/image_classification_model.py,sha256=ySXlh5ndQPsLKqJvoZ7s-ZZKqT0GjylRHpsVuD1F53k,2444
 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/pytorch/pytorch_hub/image_classification_models.json,sha256=oROMaXYqHaiUb4UXxBaS_qVOoEYGo6oVx2ytHVSJ2fY,4894
 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/pytorch/torchvision/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/pytorch/torchvision/image_classification_dataset.py,sha256=QPyx9xvtSKgH1u9Us6uKj-S0qVge2I-GxcXCmLyLz-Q,5925
-cloudtik/runtime/ai/modeling/transfer_learning/image_classification/pytorch/torchvision/image_classification_model.py,sha256=Nw-mJXI_c6VFaH7Y9QX5qu7kQ_JyojQunkd8GufB-VM,14721
+cloudtik/runtime/ai/modeling/transfer_learning/image_classification/pytorch/torchvision/image_classification_model.py,sha256=aAh_znq88qOrmZXpTPWrJTJ45D2t6fJBKxiIUYfh6iQ,15058
 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/pytorch/torchvision/image_classification_models.json,sha256=8dSCiQwhSi16SNM2PJT0tY4mMK4h9HaL0k5iQpX8njc,10856
 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/image_classification_dataset.py,sha256=-NfFXno6vQfYbsp62won-roeRk7565wCY0jbDfoglpM,10274
-cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/image_classification_model.py,sha256=GyD1KsI28NiyX0leZHoBlDGbnnIeClMa--48K2cLbgk,30894
+cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/image_classification_model.py,sha256=gQNlb-KVGfV4aubHTIAf40Abgz3y-_in76zDsXWFxyc,29790
 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/keras/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/keras/image_classification_model.py,sha256=_lT-J0ciGUXbCp3ixhmZ3LEyOLQe6I8oztVXA4I-wKk,3351
 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/keras/image_classification_models.json,sha256=2AlkKvwLEsqtQhwoSs-E9HJGahmckijfGS4UMFebDgA,4762
 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/tf_hub/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/tf_hub/image_classification_model.py,sha256=Imx2TbATY8TWdjeMuPNvWO3GmOgaWBGKyyd2PRGy2NU,12300
+cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/tf_hub/image_classification_model.py,sha256=KUXTnGVGoJcLtR6eTrLJcVzDfDek5m1GpwpI4DSK7B4,13137
 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/tf_hub/image_classification_models.json,sha256=PYJz0VHyr-j4-nwpADu5D-Ik7VktfgAQnF3lb3hcs-w,6362
 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/tfds/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/runtime/ai/modeling/transfer_learning/image_classification/tensorflow/tfds/image_classification_dataset.py,sha256=3tQU0O72hMDcgoLXgST1Fo4lnT6nSi_zs_l43Z2uF0o,6852
 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/__init__.py,sha256=oWqFUgZceN7bQKjG8CC6VdjEoboTkF5LN4AG_95lEu4,674
 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/text_classification_dataset.py,sha256=o1Oaq2UVKZXGr8zhMgaFNqzv7yvp6-jD8WWtsThNefw,2146
-cloudtik/runtime/ai/modeling/transfer_learning/text_classification/text_classification_model.py,sha256=X1cIx64ZOedQ1aBYoerC6je1ueutv51l7cO6fxstdO8,3196
+cloudtik/runtime/ai/modeling/transfer_learning/text_classification/text_classification_model.py,sha256=QVCvEzJXQkKCtWDIk5Pl5R0cxdigDnI5vRlOvH9Cs1A,2458
 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/text_classification_template.yaml,sha256=q_CgrtmYrDnxaltVI03I1CmoeDJIM1zcSTEUY-vK-Iw,216
 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/pytorch/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/pytorch/text_classification_dataset.py,sha256=xEClyghpAH4tz0ZEZEdkffPFu60jwqzAhdeL_t5jLro,8957
 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/pytorch/hugging_face/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-cloudtik/runtime/ai/modeling/transfer_learning/text_classification/pytorch/hugging_face/text_classification_dataset.py,sha256=g5-ww0cOX_umsn4PjQiCzfbJh704c0SXdFxk8OG9X-c,6389
+cloudtik/runtime/ai/modeling/transfer_learning/text_classification/pytorch/hugging_face/text_classification_dataset.py,sha256=6oVKjH-4yIOWvzxSwf29acEQf99rSuLZBFDjgGyKlwY,6527
 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/pytorch/hugging_face/text_classification_datasets.json,sha256=4ELTq0leNFhYgW22uDOavsrhOdfbl4vG9Qc1TRG3Tao,1969
-cloudtik/runtime/ai/modeling/transfer_learning/text_classification/pytorch/hugging_face/text_classification_model.py,sha256=37e0dwQrnpWSFrois7Iy6-x6NzjFfu4BKHQVV7lW6Xs,43796
+cloudtik/runtime/ai/modeling/transfer_learning/text_classification/pytorch/hugging_face/text_classification_model.py,sha256=SyRuIjLdLyvNMxN7R4P15TPVA8v-kFiPAxktgn-YG_w,37457
 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/pytorch/hugging_face/text_classification_models.json,sha256=gTFGG44tlz0h5VyPcNhENo0k8oF7ZW4kRCk3mKSt2BY,933
 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/tensorflow/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/tensorflow/text_classification_dataset.py,sha256=syp3pbBxpIYL8cQiBzWfhXFL5mrlHzKgpzem3jRYezo,8713
-cloudtik/runtime/ai/modeling/transfer_learning/text_classification/tensorflow/text_classification_model.py,sha256=tCBESrkPdASld--gU8Wd7p5lFLM_RRp5CtxyRdxVqRs,28242
+cloudtik/runtime/ai/modeling/transfer_learning/text_classification/tensorflow/text_classification_model.py,sha256=WMw6Oo9wwqytOIm9cr0zvKxtXlmph4MrWA1S0-1MmoA,26653
 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/tensorflow/tf_hub/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-cloudtik/runtime/ai/modeling/transfer_learning/text_classification/tensorflow/tf_hub/text_classification_model.py,sha256=vJil88Dwg3NYrNWBZddXpNUaV_2UUUMMlNffqVRYCLk,12625
+cloudtik/runtime/ai/modeling/transfer_learning/text_classification/tensorflow/tf_hub/text_classification_model.py,sha256=R3VFG13j_0ONIEfk1GHcay0ZPCrdW_KA2Vsq_0EeFuw,13454
 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/tensorflow/tf_hub/text_classification_models.json,sha256=yVc-FiCNRsCeyJhJVx9wNLiSrBvRgLmZf5ORE5HF7Ow,14019
 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/tensorflow/tfds/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/tensorflow/tfds/text_classification_dataset.py,sha256=GoZtBx7s0fb9F4Ybkwfd-I5idgQQqjqhshSJhIgX6Wo,5203
 cloudtik/runtime/ai/modeling/transfer_learning/text_classification/tensorflow/tfds/text_classification_datasets.json,sha256=yX-xopIUirxKqDLJZmQbOxTEefNANatg1fJzcP7_8P0,529
-cloudtik/runtime/ai/runner/__init__.py,sha256=WUDdx0qc0SGary-n1wE8yjiw8-IcVu-hFi3Tal32AN4,7892
-cloudtik/runtime/ai/runner/distributed_training_launcher.py,sha256=tOndggRHQo8d3qGFPEU_uZc_pYZw2FqkzUpXynEKxLI,3369
-cloudtik/runtime/ai/runner/horovod_training_launcher.py,sha256=6OJQUrlNUgiZdQgYIwrr1HaojhMP5-YNuzPP8cwJqDs,1665
-cloudtik/runtime/ai/runner/launch.py,sha256=hMt0B1HMpQJy-IORuX8zJXs_6XNCpv3FaozwdKmDG2A,18296
-cloudtik/runtime/ai/runner/launcher.py,sha256=S0u8Usu_08u_XuWA--aD3-NXQcFMcsBSlGAJPdCVSY0,2078
+cloudtik/runtime/ai/runner/__init__.py,sha256=XkFR23Vx-KJj1wTDQT6PlL2JWcAEc3__Q60rTVRs0es,8310
+cloudtik/runtime/ai/runner/distributed_training_launcher.py,sha256=9k01OSMxZiyUoSflZU9FOHHz0Ty1YodBhVXn8YwXVms,1767
+cloudtik/runtime/ai/runner/horovod_training_launcher.py,sha256=ZiNstHUiEhHZlqc-4l-gUUAwuAGtzlVFi0GXct8zXeQ,1404
+cloudtik/runtime/ai/runner/launch.py,sha256=xVmBf8ZLb_W0TYMGyhmvienZF8E9d_QihJmiNqOa-xY,21795
+cloudtik/runtime/ai/runner/launcher.py,sha256=AZ6x1pPz3qyAZkjTrUo-MT99q99KLZDYeRikab7BIYk,2610
 cloudtik/runtime/ai/runner/cpu/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-cloudtik/runtime/ai/runner/cpu/default_training_launcher.py,sha256=sKK96iPwi2aEJd1l6ooQ7yA7qjJLmyI6KCAHXPUcXSA,5283
-cloudtik/runtime/ai/runner/cpu/launcher.py,sha256=nOb0uZIMBxWgTgx1kWN8kSWdp5_KOSjdY4KxyCCq8tM,5924
-cloudtik/runtime/ai/runner/cpu/multi_instance_launcher.py,sha256=Q4CfZHHmCD8aCSJidH8oC9hzip0FnHw-TynS290LzaU,11516
-cloudtik/runtime/ai/runner/cpu/optimized_training_launcher.py,sha256=AofPa3a_gjBtfyXR39e0nToraSz0991UM_UMKyHngsE,4168
+cloudtik/runtime/ai/runner/cpu/default_training_launcher.py,sha256=V9_yVhbQR7Oylx1pOQGnwY-CRpOW65nGvslAG3j4Ku8,5544
+cloudtik/runtime/ai/runner/cpu/launcher.py,sha256=GkE0ztfA6mshUAcEBW3ES8JCkajf4q-XgxGrQoXEuRM,5950
+cloudtik/runtime/ai/runner/cpu/multi_instance_launcher.py,sha256=XecF9uqZv-d3eds-GEgPZWie3y6hxiMbsG8fywiHnwc,11650
+cloudtik/runtime/ai/runner/cpu/optimized_training_launcher.py,sha256=hOt7enyZvk7dDO8oN2EGLZQJd6gvtk01hyuCDYLQUq8,4198
 cloudtik/runtime/ai/runner/cpu/platform_utils.py,sha256=fbYCLN0dFX1gn95N4aqZ7F56C9U3wi4F4dT72HEt410,18934
 cloudtik/runtime/ai/runner/cpu/utils.py,sha256=52ndCLmfNwkBEeteHnPB-HgLcej2DjmYKZR-YZxyVss,5195
 cloudtik/runtime/ai/runner/util/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+cloudtik/runtime/ai/runner/util/distributor.py,sha256=ISjgvYOCDNk0O2tMQGHPt5J7XaSpP7rs03GuAZNAQJY,11304
 cloudtik/runtime/ai/runner/util/safe_shell_exec.py,sha256=kpiQ26oRkNt2JymBYJ0daa0vmdtUh9GuHECx5sBEowM,9817
 cloudtik/runtime/ai/runner/util/threads.py,sha256=oLmEbRY3rNfIILnv0WV0HwwPyMLAxDFcfIiP5QtGs3s,5686
 cloudtik/runtime/ai/runner/util/tiny_shell_exec.py,sha256=tPMrGXCW_R0Escwfl1Xs7KOopQu4CmsfbMvs32kCnho,1406
-cloudtik/runtime/ai/runner/util/utils.py,sha256=K3ZKdqwLl5vZ-ogXUuwxOx5sKFskv4gbNzxBEtD9gIs,4029
+cloudtik/runtime/ai/runner/util/utils.py,sha256=JI2DLLXerUBkUbW6jr8y-NILeS9dg1R2jUfXoNfcW5A,4142
 cloudtik/runtime/ai/scripts/cloudtik-rsh.sh,sha256=Auy-AjZkxkZHglUrfJ5n2RGxh50yrlcgykZieuxg5mw,100
 cloudtik/runtime/ai/scripts/configure.sh,sha256=cZioxd71Z6A635gSVFjm2fzVJe-Pll2LKrv7DG22Ms4,7180
-cloudtik/runtime/ai/scripts/install.sh,sha256=Jhq1DkJTbPxaT-K3LV863bTmSwcqA6AJNVW3RzejebY,7538
+cloudtik/runtime/ai/scripts/install.sh,sha256=cXpwDtQ6w7I56Tz6K2NZFOkWxW6hvNQZD2sdH-XhXaI,7695
 cloudtik/runtime/ai/scripts/services.sh,sha256=_d7K0kv9PbeoCc-bIBgsVbmTxIQMqx1diWZ4RZH8Rig,3359
+cloudtik/runtime/ai/util/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+cloudtik/runtime/ai/util/utils.py,sha256=g_hZ-6QsTkQWCtth9Fur2WuZp3k1ZHFcE1g4Ii7hvKI,2353
 cloudtik/runtime/common/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/runtime/common/runtime_base.py,sha256=hs1evBLQ8NAPSLu6PVBMdI50P_Vjw79J3whZtt_ZrqY,1512
 cloudtik/runtime/common/utils.py,sha256=1iOluOh375IihRmqWk3wdySTPQ6SQKqiCuzwtxvdgvY,2331
-cloudtik/runtime/common/conf/hadoop/core-site.xml,sha256=rmCIKIz4CxCVSWxHkoiA_oinnIiwfUQ6bj139siYre0,1192
-cloudtik/runtime/common/conf/hadoop/aliyun/core-site.xml,sha256=wsbV52HUxJlQbXz5sk9V88y76gpXY1YFWKXSmfDL8Bk,1680
-cloudtik/runtime/common/conf/hadoop/aws/core-site.xml,sha256=bOD3twLkvYcO4jIJ1A-h7MYi2GgNO2mBsv312fUVpII,1843
-cloudtik/runtime/common/conf/hadoop/azure/core-site.xml,sha256=yUcsezO_bWYapRzNT3pSfhnrhWxVB7o44p93r42eDJU,1681
-cloudtik/runtime/common/conf/hadoop/gcp/core-site.xml,sha256=IgM-mjHKsXTcynIlCsWVf9zl64P4npu9RDXLUaQ3XHY,2593
-cloudtik/runtime/common/conf/hadoop/huaweicloud/core-site.xml,sha256=PNZEBsvrQANp6yiP1MxMwEuWuVzmk7_Z78jgbosoDQU,1515
+cloudtik/runtime/common/conf/hadoop/core-site.xml,sha256=SCx8vUpR5HMR6rmrxo6a_ookcVHjUxQ9CwCr_9FD2Gk,984
+cloudtik/runtime/common/conf/hadoop/aliyun/core-site.xml,sha256=ylzjc4yacQ6RaD1gN8f-B76p7rWRWRvtfrTmC1zWZxo,1472
+cloudtik/runtime/common/conf/hadoop/aws/core-site.xml,sha256=3N5fot7pDsbJlWEKreHb-Nt39p1zhkdCoUCRJzCgt-g,1635
+cloudtik/runtime/common/conf/hadoop/azure/core-site.xml,sha256=xsHKlsVJyN8gPdt5dpe5Zmg1ydDrWPCcKSDnyg0JiJw,1473
+cloudtik/runtime/common/conf/hadoop/gcp/core-site.xml,sha256=CTEufOaoNN8FrAPXUqHT2P6o4qPCwR-9cQZ61_X-lnA,2385
+cloudtik/runtime/common/conf/hadoop/huaweicloud/core-site.xml,sha256=mS3SekNgGZaGEHSyaHLbz3QzFjLol4tCxjLonluS6ps,1307
 cloudtik/runtime/common/scripts/api-credential.sh,sha256=JKZH0pgizPFTijYbXSwRB5Q29mnYQgRqTwdgEk0giVY,1254
-cloudtik/runtime/common/scripts/cloud-storage-fuse.sh,sha256=LBbZm5zTIF62gREeQUj-ifN1i_mU5VuLtHHxoN-n0LI,11396
-cloudtik/runtime/common/scripts/hadoop-cloud-credential.sh,sha256=AWfaxe_et0wkW6eLDEz85o42bod4fKd3-m2F2RLJO_o,10842
-cloudtik/runtime/common/scripts/hadoop-install.sh,sha256=gFuM5l-RfQlMj4c8smeOy8zJmzpMWtPWD76_kzXHres,2300
+cloudtik/runtime/common/scripts/cloud-storage-fuse.sh,sha256=tnzS7TNxJs66arqkk4tNtdio05pUhqfQ2nROSWfYVDc,16126
+cloudtik/runtime/common/scripts/hadoop-cloud-credential.sh,sha256=cCd-uJs8PNthTGf1zymJpMunwv2ppNPXFkUo_Mipmr4,10843
+cloudtik/runtime/common/scripts/hadoop-install.sh,sha256=YEz1BA6eTBRbxvzf-1QVE_fo4C5Dt3NiBYhNNijj7RQ,2404
 cloudtik/runtime/common/scripts/jdk-install.sh,sha256=xH6cXkFOUFxGbOlXjClf0SQLpnuRgsTwLOqjGKDtpAo,1044
-cloudtik/runtime/common/scripts/util-functions.sh,sha256=vyrUB_b2HqA6_DJyvArqE5Obr8e1CFPF4oVe8urRqeg,1667
+cloudtik/runtime/common/scripts/util-functions.sh,sha256=8lMjlsjXxLhAH5oZ-IZFSxoaAlcYZhmB50cjitlbTYo,2466
 cloudtik/runtime/flink/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-cloudtik/runtime/flink/api.py,sha256=lJiE-lmp6u2NVlOpR1unAkIZtej0wMkTSS8ha16HieY,2303
+cloudtik/runtime/flink/api.py,sha256=MDLClUKilXuCCiCHPJtPBsPazlpfqjMo5wdnTgvdB4c,2506
 cloudtik/runtime/flink/runtime.py,sha256=K4aPhJg-pBhRNVrkD8tM3LEFEHwLqJ2hJrRVtoYPht4,3352
 cloudtik/runtime/flink/scaling_policy.py,sha256=D-42sLJlVKc-57etlS8HD92s9eH6jKn_SJGGVu5Ehdw,15513
 cloudtik/runtime/flink/scripts.py,sha256=SeUo9pFLsPhjsZKmDaHAwNXTuqh3qWFlDvvqh7uVahM,2829
-cloudtik/runtime/flink/utils.py,sha256=ktrG9Eh_GXPmhxzjXFT83dR_y4UQxUjcKMPLJzIH6hM,15664
+cloudtik/runtime/flink/utils.py,sha256=64q3XIovDUltiL-2wOCIk5bum2UEhHk0Po_54ZmpQgI,16408
 cloudtik/runtime/flink/conf/flink/flink-conf.yaml,sha256=da-v8u8vJtn0tonEUYUihSkWMzkReoV3ipLFJE-vG-g,620
 cloudtik/runtime/flink/conf/hadoop/yarn-site.xml,sha256=CDP9ZZQq5k8a6tNfdLQwGwtToO7j1hDM1X3dSpoMG4g,2772
 cloudtik/runtime/flink/config/commands.yaml,sha256=e84cSUi1sDV6psN7kxfqbVQVdCtBh2oWVbVNABjVQEc,995
 cloudtik/runtime/flink/config/defaults.yaml,sha256=5vY5u9K47w2bOdkthw4OAokTI9sr8rcL9kXHrgpsUz8,25
 cloudtik/runtime/flink/scripts/configure.py,sha256=ZdUesPVq5MGuLhnAA1t3xYwWeEF_Eoy6_wYPbufY-D0,500
-cloudtik/runtime/flink/scripts/configure.sh,sha256=r5_gxfBv1TosxFZe3sdNVmnmWegKGQwXJRshDr7RwqA,15805
-cloudtik/runtime/flink/scripts/install.sh,sha256=HV6UcXi-lOi9LiAFtxQ6NI0hS3M4YFV-abMRvH6qDss,5882
-cloudtik/runtime/flink/scripts/services.sh,sha256=XGUxLXLsWUWmU8vYMUmXwhUVUvYGxaSw7VS3nEZrw2M,1948
+cloudtik/runtime/flink/scripts/configure.sh,sha256=Vz84JxXSGc2Vi5mWIJ2gY9IeDlYplInAX8Yb-xcvk-8,15514
+cloudtik/runtime/flink/scripts/install.sh,sha256=MzaqUimSQ-eRR8gMzd0n6PaSCX_daeZYdEnBKvoiAvI,5882
+cloudtik/runtime/flink/scripts/services.sh,sha256=CVgw-F_BBiVAcHZ8pPBzvcd9cp3aoGtt227fRAFw2JE,2263
 cloudtik/runtime/ganglia/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/runtime/ganglia/runtime.py,sha256=RCAVED2lpZH4Jkmbvp1W9buZjosZzgvF4F7LAtZRvmY,1212
 cloudtik/runtime/ganglia/utils.py,sha256=YozXDG8XIcS9hYPkK0rVDuPehadRp9GO-0T2sEPHyCc,942
 cloudtik/runtime/ganglia/conf/gmetad.conf,sha256=Qwcn15JJ_5xKHOGBF2W0Y9bk-2C0yGrt5D_fufw2JNw,8052
 cloudtik/runtime/ganglia/conf/gmond.conf,sha256=vvJXW971QHzuOeMtVuj4EU0z1V1i1_qsmMhE36NZfn4,7792
 cloudtik/runtime/ganglia/conf/gmond.node.conf,sha256=zaZmmfcu_pH0PFag1I_vxzpM4PM3PGJE1gBJcbMB-FM,8006
 cloudtik/runtime/ganglia/conf/conf.d/diskstat.pyconf,sha256=SrfRWz-vLbAMPxYqoAUkKSG1YGTNWMknhbfYI_n-emg,255
 cloudtik/runtime/ganglia/config/commands.yaml,sha256=DYuCzHl2NYFvhmMsUTr1iPl69idwRanSm_BdJQw2tAI,1015
 cloudtik/runtime/ganglia/config/defaults.yaml,sha256=5vY5u9K47w2bOdkthw4OAokTI9sr8rcL9kXHrgpsUz8,25
 cloudtik/runtime/ganglia/scripts/configure.sh,sha256=H89-wjkTP72kErkUwUfi4fZLb9T6EiamQuRsOv1swIw,4135
-cloudtik/runtime/ganglia/scripts/install.sh,sha256=o3UxZZ4WQnKlsqk8IfxfrQFUgljFopKze3Y_xRbZS2Q,1820
+cloudtik/runtime/ganglia/scripts/install.sh,sha256=80aW6wIR9M2Es_hV2H3vdWu5o_gy1svMtzjtUFpcjL4,1794
 cloudtik/runtime/ganglia/scripts/services.sh,sha256=CyIuqLNmaT4g7P9wUxx7DILogn1LxXT-SJUbvsgYBKQ,893
 cloudtik/runtime/hdfs/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/runtime/hdfs/runtime.py,sha256=WEowUOdegRnGKVwPMdHos04xSYTYbVzk_pD7gDfMc3g,2139
 cloudtik/runtime/hdfs/utils.py,sha256=Ybw56Zhs_hAIMYDg5oJ8s94p52QoqnIpfDcvQSs4-1E,2557
-cloudtik/runtime/hdfs/conf/hadoop/hdfs-site.xml,sha256=k7wl-sHC3fBAxIlIvzhYL87YJ6LtTp4n0GxKWa4vL9Y,2073
+cloudtik/runtime/hdfs/conf/hadoop/core-site.xml,sha256=7WlWiVLgd8qj9EPCj_eYMLi8UNxQS5Naht--QgSIync,1228
+cloudtik/runtime/hdfs/conf/hadoop/hdfs-site.xml,sha256=ZAdNaB_QncsBx4qwh5DQdLEbvMV5fKE8vcNDJbPdaOs,2406
 cloudtik/runtime/hdfs/config/commands.yaml,sha256=R_MLC7d09He1BeZtC2RT8OtkmU6epNIM366dQJYb-3c,985
 cloudtik/runtime/hdfs/config/defaults.yaml,sha256=5vY5u9K47w2bOdkthw4OAokTI9sr8rcL9kXHrgpsUz8,25
-cloudtik/runtime/hdfs/scripts/configure.sh,sha256=dCabo8OG_4qfMbAEk_OrfEdDC5vA7KTPACkt9yjz8VI,3034
-cloudtik/runtime/hdfs/scripts/install.sh,sha256=YzNrlU35r1fioBsE6D4VdMndw3EDUbCOcDRryj8WDvg,573
-cloudtik/runtime/hdfs/scripts/services.sh,sha256=8SouTu-ZHBJ2R0B_7L0N4L-XObLmh9E46wWu3uF7ifE,911
+cloudtik/runtime/hdfs/scripts/configure.sh,sha256=VbxX_zT43R0byDfK-wrdM9eaOYtMV0EgTKOS86bWlIc,3521
+cloudtik/runtime/hdfs/scripts/install.sh,sha256=b6FxsqCBNRdF0s_psM4bk7_KAqj-iG7EorMQhWm43oM,573
+cloudtik/runtime/hdfs/scripts/services.sh,sha256=CHjPKh4Ukp-yQr1-scH29k0-QdG6KZGgoCoezmTDxtQ,987
 cloudtik/runtime/kafka/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/runtime/kafka/runtime.py,sha256=xdyZYJUDkwpS5ZxuFCvhurMoTAcIYa00i2wG2hnPZCk,2426
 cloudtik/runtime/kafka/utils.py,sha256=7R_-4IJ_o1PBcMGehjJOu3LDXFbvljgdwPLr5ix8C80,4091
 cloudtik/runtime/kafka/conf/kafka/server.properties,sha256=SSFhEAKL7SDPtfOB6HTCNk6PbpucP5B7-ta3HSYsD_M,6883
 cloudtik/runtime/kafka/config/commands.yaml,sha256=bUUMSQ53K17oK2zMPHviR64d0DXW3I1H2Mb5r2Bu-8Q,995
 cloudtik/runtime/kafka/config/defaults.yaml,sha256=5vY5u9K47w2bOdkthw4OAokTI9sr8rcL9kXHrgpsUz8,25
 cloudtik/runtime/kafka/scripts/configure-kafka.sh,sha256=89Gy0BA9NB-Ew1iONS9NXLATDDt3bzyeUjLvEOnlP7c,2960
@@ -419,19 +477,19 @@
 cloudtik/runtime/metastore/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/runtime/metastore/runtime.py,sha256=we2B8qD7o9qNeIJkcjzoctOTBFcbCakHXPvk58eF3UQ,2165
 cloudtik/runtime/metastore/utils.py,sha256=bcmV--exxDbufoEL0GTPe7J4aQ2R613OGdhQIDYM7XA,2211
 cloudtik/runtime/metastore/conf/hive/metastore-site.xml,sha256=KRuZLj-gwwNCBCXePMuM8wFIEJEFauUYaaSkhacaqEI,2191
 cloudtik/runtime/metastore/config/commands.yaml,sha256=gGjQrBSRr5z2yvJJOxoBUkGWQvd6r2EoefYFE8tHn9s,818
 cloudtik/runtime/metastore/config/defaults.yaml,sha256=5vY5u9K47w2bOdkthw4OAokTI9sr8rcL9kXHrgpsUz8,25
 cloudtik/runtime/metastore/scripts/configure.sh,sha256=nlT-Dfmr1uDxU86gdQAwho5u-K_6c5Eg_H-uRvRtmQk,4248
-cloudtik/runtime/metastore/scripts/install.sh,sha256=cZDk_KA9t8UpFkfQwcQytIhrqXQNxeWPOLshH860fO4,1725
+cloudtik/runtime/metastore/scripts/install.sh,sha256=He2qYDu-I2xenyUml1z-KZ0HEthB1ZOxSb51DxavEV0,1725
 cloudtik/runtime/metastore/scripts/services.sh,sha256=pOX1JqDOVHAdZwldQazJF7I0Qo9CEAdw5W79DzzDoys,1099
 cloudtik/runtime/presto/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/runtime/presto/runtime.py,sha256=db0c1WwnADmMG5FHbSFpGxmheqZI_LimIDeXB6keRIU,2806
-cloudtik/runtime/presto/utils.py,sha256=0cTDsQxQVn_kXmbd56hjRIDdscfhx89pXawB4uP4IUY,6525
+cloudtik/runtime/presto/utils.py,sha256=dJSxbeHRCe42x79H_LegP_EQCMbKmM-c2A7I2DH4_Kw,6614
 cloudtik/runtime/presto/conf/presto/config.properties,sha256=vWIkmC_H-rDU5MYfUa9MNWVKBR6GCU6aQxEPv4Yol2g,384
 cloudtik/runtime/presto/conf/presto/config.worker.properties,sha256=izJn_WbdYZJmiRvsd-dG8Lf5uq-d4R6VzxxKTB6T-uc,250
 cloudtik/runtime/presto/conf/presto/jvm.config,sha256=ROZZze0hegr9SzHCKNekh8dzlt0vRSNDNc_-XK21QeE,220
 cloudtik/runtime/presto/conf/presto/node.properties,sha256=GCh28mp4p3HITAnSvZhQcCVfl01hYioDQMVO2rILR3c,90
 cloudtik/runtime/presto/conf/presto/catalog/gcs.key-file.json,sha256=lgzwaulRPIrIGfpJox6kmEWOKNuUweky9LEVvI-wDhw,374
 cloudtik/runtime/presto/conf/presto/catalog/hive-azure-core-site.xml,sha256=CTO6-aoE3gcOkHlX_SOvMzZJqrvrgCqJDAnViCYeS9Y,956
 cloudtik/runtime/presto/conf/presto/catalog/hive.config.properties,sha256=EkT0KKOfsK4xUSeCm4as452OCGfmTQWTldqOcraUCak,87
@@ -450,35 +508,36 @@
 cloudtik/runtime/ray/utils.py,sha256=RQz7CJQ3610_C-8W5oDnZOhWkCXiaTE6jDrzAjetyWM,3020
 cloudtik/runtime/ray/config/commands.yaml,sha256=m2yg25ADk3XgcuOJitWinYXrso8mwq8MrDMZRb1m_g0,975
 cloudtik/runtime/ray/config/defaults.yaml,sha256=5vY5u9K47w2bOdkthw4OAokTI9sr8rcL9kXHrgpsUz8,25
 cloudtik/runtime/ray/scripts/configure.sh,sha256=rRqoys6wc6n34wc_5X0giixI6XFeQ2eqZ9IEyYX4n1U,834
 cloudtik/runtime/ray/scripts/install.sh,sha256=kt7Q9OZ1oWCzUCTzAobKDr0ZswXnpI3ene5PKWXvpx0,498
 cloudtik/runtime/ray/scripts/services.sh,sha256=Zv0dLbMO71IqUQmKKCBmRSn1PpIYQAbkFPnSheO54M8,881
 cloudtik/runtime/spark/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-cloudtik/runtime/spark/api.py,sha256=7qF7WPj3sLdCb9dy1yMdIMBI8LFdfoZJ3kUw1ZuoMAo,2339
+cloudtik/runtime/spark/api.py,sha256=dX5zqDOP3BxnGWzTUs6RV0iKgR18B4jNk8gKw9CKD9k,2542
 cloudtik/runtime/spark/job_waiter.py,sha256=wG28gKtoX3BlE89k4uv1_u9PmeW1S0ML_ghuAxAOZyE,1761
 cloudtik/runtime/spark/runtime.py,sha256=af43cpScJtCWugxh2eniwr4tRWkqxVVdhJvDBP9JH40,3615
 cloudtik/runtime/spark/scaling_policy.py,sha256=E3mXoi6KZl30K7liRME9onzunmdNk9_-HTuNFRBuQZs,15691
 cloudtik/runtime/spark/scripts.py,sha256=OvIqg55Uz8nUPdJb6QZf0BipJce4BZSB-ANSBCViwDg,2861
-cloudtik/runtime/spark/utils.py,sha256=_gDz7ZkyN3H8c-q0L7jyl1BkGdMuRiZmJGPOOqPVsZc,17757
+cloudtik/runtime/spark/utils.py,sha256=yqQ3eatHmR2hjXvwBuxWERXfpz5bBV2OQjv0VprSFf4,18816
+cloudtik/runtime/spark/conf/hadoop/hdfs-site.xml,sha256=1Lup4LVqkEtqOWXVGB6VlMCSbNDIv2K5MEdB48GuB_4,1281
 cloudtik/runtime/spark/conf/hadoop/yarn-site.xml,sha256=HOfmzMsIr3hjScCim7kaj1QlGYxN7tBCedWtNHnh9NM,2960
 cloudtik/runtime/spark/conf/spark/spark-defaults.conf,sha256=Esp4KGvswj7itS__FbUPEvHaforjpU6sni033bBnuDo,1345
 cloudtik/runtime/spark/config/commands.yaml,sha256=FRyqGAPa5m4TfdZ2n5EgSuB5HGaM0E9W2sx-bWzGvuY,995
 cloudtik/runtime/spark/config/defaults.yaml,sha256=5vY5u9K47w2bOdkthw4OAokTI9sr8rcL9kXHrgpsUz8,25
 cloudtik/runtime/spark/scripts/configure.py,sha256=rIqVo1DKkHIjqNXAyOddgFPoCfZwvwlIztZyArX7snQ,500
-cloudtik/runtime/spark/scripts/configure.sh,sha256=5mb0OSLz_B-RCkrVnjskn2kAnLXfU9twe5Hs_0tkR3k,15597
-cloudtik/runtime/spark/scripts/install.sh,sha256=Bi4A9wuqdvKKDSA3E0y3sTfu9dyRmDlyUItxWBLIITs,7226
-cloudtik/runtime/spark/scripts/services.sh,sha256=ZImiBtu6E6c1su5LLEfyXjXaVV7gct5qLlCC2vap5y4,1951
+cloudtik/runtime/spark/scripts/configure.sh,sha256=_xyMXc97oNeWQzOaZdV4leQ4h5gpCzH4Y96s9iqYihM,17679
+cloudtik/runtime/spark/scripts/install.sh,sha256=T5McBIUpo73R_mJIsOryDVvfCzWId5TviUIg1KB3uIQ,7226
+cloudtik/runtime/spark/scripts/services.sh,sha256=DvOQ9sBsn9kH8qCuUl0Kn-tGVV9BWWVRaX4NIt_pS9U,2291
 cloudtik/runtime/sshserver/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/runtime/sshserver/runtime.py,sha256=aPyv_wn5B0rLMIMrgkeR4iZt4Fm4WzArThpYudjssfg,1543
 cloudtik/runtime/sshserver/utils.py,sha256=xPWpuOgXXf8SyuQR6eEEKvOae3znoTYcdnSCuGX_w5o,2493
 cloudtik/runtime/sshserver/conf/sshd_config,sha256=THdRJLF0iA4IklLwcUrFavbdyqwU3OSYeGMYyqe2jfQ,3287
 cloudtik/runtime/sshserver/config/commands.yaml,sha256=fMSiOQDgwqpKOFLgcWOnh8-ayb87vkheb5VMkfMSmAk,1035
 cloudtik/runtime/sshserver/config/defaults.yaml,sha256=5vY5u9K47w2bOdkthw4OAokTI9sr8rcL9kXHrgpsUz8,25
-cloudtik/runtime/sshserver/scripts/configure.sh,sha256=D752IRb6XNUF2R9KX72efuEr77mQMj0Bktlpsis7Gfo,1742
+cloudtik/runtime/sshserver/scripts/configure.sh,sha256=bAvnl69w6cRdFOwil9FaZsHmKE5Vh94-N6-Dgl5uoYE,2005
 cloudtik/runtime/sshserver/scripts/install.sh,sha256=mjnynh0S8yfVh6l8g0CkdDS4BEPReZdzxkU6xW6D6Hk,489
 cloudtik/runtime/sshserver/scripts/services.sh,sha256=8ZIhdK-TlXN7eg2moSdUV3Pc0I4Mt1xY7gOVie2gMN8,1088
 cloudtik/runtime/trino/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/runtime/trino/runtime.py,sha256=pAorxWfmCrxWkJcz9wShegz1yG1OnXkrBXk6MyfhlSo,2803
 cloudtik/runtime/trino/utils.py,sha256=pNU9rg1FtOlXJ_Cnk2fTjsyVxKMeny-2Egi9eJ5xmag,6127
 cloudtik/runtime/trino/conf/trino/config.properties,sha256=EpMkY2XbGtZnQcVGGrdcHD_u30J8v8myc_DjNaJ3yUk,286
 cloudtik/runtime/trino/conf/trino/config.worker.properties,sha256=fB1w5ZQdc3ujttI5rZ6F5aCOR2YNRWIkeKwcx9tBkPY,182
@@ -490,32 +549,32 @@
 cloudtik/runtime/trino/conf/trino/catalog/hive.gcs.properties,sha256=wZa5p47mI7EfHEbh1nCrBekZjISX8eTRmNFamQ2EtwE,225
 cloudtik/runtime/trino/conf/trino/catalog/hive.properties,sha256=xXzQBGeDUuAmGvBNwahXOkdDTyU0cy8xu2YRKNvtmPE,62
 cloudtik/runtime/trino/conf/trino/catalog/hive.s3.properties,sha256=VoVvZGfjW8wsvL8gWywbeakH5qG0OHGereS4niHquio,142
 cloudtik/runtime/trino/config/commands.yaml,sha256=0_pGcTxIRjSmd7wvwdFQ6X4tiWcKV8qQ9pRgF7NPA_o,995
 cloudtik/runtime/trino/config/defaults.yaml,sha256=tjc1zyeFAu-RvRgU60HeoxrOEgykKtrCJP5wHUJgXfQ,218
 cloudtik/runtime/trino/scripts/configure.py,sha256=JKES5UcEn51DRolXb8uvqajqyM_7ikwKfS6C_9lB-c4,731
 cloudtik/runtime/trino/scripts/configure.sh,sha256=nLpJP0uIshNdYaj8HTzqbddIFG-hONB8q8BVWTb3tu4,10275
-cloudtik/runtime/trino/scripts/install.sh,sha256=2TmPc_eLFhtg-mZ0unyciwoeOZb1_9XcEARKILmrg9w,1931
+cloudtik/runtime/trino/scripts/install.sh,sha256=jUSzs32higu_Ng2t61q9VEP4UJIHyCy3nPa4jDeiM5E,1931
 cloudtik/runtime/trino/scripts/services.sh,sha256=USSBm9Zr86xGDsWVsEKohWmDYp-viLOHtVtuf6zOuWk,673
 cloudtik/runtime/zookeeper/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 cloudtik/runtime/zookeeper/runtime.py,sha256=NoRRVv5RKdUS54ITJHVIm6KqJF6y3Xrt_tgReyj_Ocs,2555
 cloudtik/runtime/zookeeper/utils.py,sha256=6Wxc0NBUCCKAO6uQmAumGUfttFeVj4D9fbvgPl_j580,4568
 cloudtik/runtime/zookeeper/conf/zookeeper/myid,sha256=1PWf0Q-vCWZgeUwVo756HjsXIaRc8_zWdgUV2CuM-J0,18
 cloudtik/runtime/zookeeper/conf/zookeeper/zoo.cfg,sha256=0TM7tcgoimnQOOuBAM2zTjCOSaNFg1LN-UOSm70BaVM,1155
 cloudtik/runtime/zookeeper/config/commands.yaml,sha256=CON9gpiF0mHaUW2Dr3MKoEdHa2HEMbcGBcX8PSk0lsk,1035
 cloudtik/runtime/zookeeper/config/defaults.yaml,sha256=5vY5u9K47w2bOdkthw4OAokTI9sr8rcL9kXHrgpsUz8,25
 cloudtik/runtime/zookeeper/scripts/configure.py,sha256=QZhaXQuckd8ExBL0OXqaevmSbj0i2mXjCNX081ZS67g,704
 cloudtik/runtime/zookeeper/scripts/configure.sh,sha256=Eh-W24Oi8gU81_jZWUs0ipifTTvL80fhs3HAKlWBdNg,2154
 cloudtik/runtime/zookeeper/scripts/install.sh,sha256=rDn7ncOz6SCD3ouqENNYaS33RGf4iZweaHbTL4n8MKI,1206
 cloudtik/runtime/zookeeper/scripts/services.sh,sha256=Oa-fz82nkigM-Y41nxjSYojHm-6bIqCYtA--1cflqUw,918
 cloudtik/scripts/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-cloudtik/scripts/head_scripts.py,sha256=RkU6uYt_GBj-VeinUm3r4A1Ktw0ZIUat8pqqh7bXqLk,23568
-cloudtik/scripts/node_scripts.py,sha256=u4iSzLML5ms7fU72oeNU38fNLsSb6vb4F64LJvE0HFY,17442
+cloudtik/scripts/head_scripts.py,sha256=7hN4WAYhz6gq9a8mCJ4A2iCcBWXDTR0W2AUaosCfWz8,24872
+cloudtik/scripts/node_scripts.py,sha256=GN2-LJW5A2HD5GSIyLQTLReohLpX3s7alqZ_aJaNPhs,17137
 cloudtik/scripts/runtime_scripts.py,sha256=_VBjWtRlxrp7PeR4U-KuO_huMspOSJ4V7EB-ojZx_Gk,7781
-cloudtik/scripts/scripts.py,sha256=9MM1vYILL7rxE4_9jZMJknFzamZ4SStyV3IMJEGnG54,37561
+cloudtik/scripts/scripts.py,sha256=fpd3AkRj9ajB6x0sR4tfgzXLL4IvrUCW4pYJG9TcMQI,41089
 cloudtik/scripts/utils.py,sha256=9J9mj5GpgXtdQngCroZoqF2t2j1fDCrX3_mfoC7u1UI,701
 cloudtik/scripts/workspace.py,sha256=ulr5CYCpQMrDaUbNUeSlgiKpIt9DbroLR2LjmJo6zGA,7205
 cloudtik/templates/aliyun/large-highmem.yaml,sha256=0q5EtGA05vdoZSdoA8VTOiw_x7SwTvM9Vo4KGPowCn4,249
 cloudtik/templates/aliyun/large.yaml,sha256=5jjWkk-IQ0FAtDDJzf883T0Uhr7fo_1kttNfgeNklg0,889
 cloudtik/templates/aliyun/medium-highmem.yaml,sha256=WEdXNI_bJMKYQgKARWulFZak5QhDOs0u2jk5_64Sulw,250
 cloudtik/templates/aliyun/medium.yaml,sha256=XrINUlbfExQvsSJnqr_S4TmekJLoCD9fvapRJ2kNBIU,771
 cloudtik/templates/aliyun/small-highmem.yaml,sha256=oVayi0PU1AbIBXCpkpSPcv2zxWtukYMoTORT0AjJ-yo,247
@@ -707,26 +766,28 @@
 cloudtik/tests/integration/constants.py,sha256=ooqIKfuLG-LtLZPI0kqHBOaBlUC7qJRCqRZRvy1kDLk,2103
 cloudtik/tests/integration/test_aliyun_cluster.py,sha256=RxthqUUBPvFVPsaHspgJRXl02T4cxRsZoxS7idoGGkY,1485
 cloudtik/tests/integration/test_aws_cluster.py,sha256=ZPKmKHv-sjihWIdV89zhvgeRCDIseyMHCDrsUMdbyDs,1425
 cloudtik/tests/integration/test_azure_cluster.py,sha256=SPvymDylPvikahukciBvXPCGxnoBOqNYbFgbx7S9_Nc,1451
 cloudtik/tests/integration/test_gcp_cluster.py,sha256=lGpLBnaju2WG4XZcLWsH75qTioBe6oxPwxrlWmlU2v4,1425
 cloudtik/tests/integration/test_huaweicloud_cluster.py,sha256=qqBru8HOBmzNtf2oJhhxSDdXaQoweCQqJj_Ecf4N9nU,1548
 cloudtik/tests/unit/__init__.py,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
-cloudtik/tests/unit/test_cloudtik.py,sha256=MkoWkg4Hfk6ryE7l4SGTEO3vOg-gWSJDN2RxIKsUw3I,65497
+cloudtik/tests/unit/test_cloudtik.py,sha256=NzP594xYog701wn6U9Vlq06R5pEHukdUxmXHtnkcELs,66777
 cloudtik/tests/unit/aws/__init__.py,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
 cloudtik/tests/unit/aws/conftest.py,sha256=N3JfHVjkTr2rO2_Y88GVIUuO10oKyZUu1m8UW3eTvDk,1024
 cloudtik/tests/unit/aws/test_aws_batch_tag_update.py,sha256=UvugUqurOaVhdjZxI4y2rqobWdNd2fAi_tRfj1awb9A,2285
 cloudtik/tests/unit/core/__init__.py,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
 cloudtik/tests/unit/core/test_crypto.py,sha256=ChQ7XE3JviCxHJAhlfpZgVtUH0heO_zQ35aEmVK6o7I,1079
 cloudtik/tests/unit/core/test_job_waiter.py,sha256=BWEWcpObja81_6nTybquQvK3MdpxQRm8aKiDbFdf1gE,1481
 cloudtik/tests/unit/core/test_provider.py,sha256=LZTRz1p_kUYDivuVXE0845abJAwGUQUIA37hxksbPL0,2340
 cloudtik/tests/unit/core/test_scaling_policy.py,sha256=TCIQ3XPpohC3WOHkCXj9YYwOGm6ck9FhSS1sLeN46Yg,17545
 cloudtik/tests/unit/core/test_utils.py,sha256=kqKHnq4yuVkfxZF7I-8oiGJY09KcfchvUJYwyMy9MP4,13036
 cloudtik/tests/unit/core/state/__init__.py,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
 cloudtik/tests/unit/core/state/test_state_table.py,sha256=GcuQ4sFADF3yCPMMB5iHe80AYylwkKswH1PnZh1hVug,3334
 cloudtik/tests/unit/gcp/__init__.py,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
 cloudtik/tests/unit/gcp/test_gcp_node_provider.py,sha256=dhLIM6Qg6PaadMAKoHozHstNRJZ5WqQhvATR64po994,3635
-cloudtik-1.1.0.dist-info/METADATA,sha256=dRm_hue5skSwIgFNpaedaVg4PVZg1M3N9IgymTaw2Bk,22509
-cloudtik-1.1.0.dist-info/WHEEL,sha256=3oXbtF4vYwmQyf0LQIagKtl0VO3gP86556qKiLb0bDc,103
-cloudtik-1.1.0.dist-info/entry_points.txt,sha256=4R1fgN77EEQMX8H0zPUHciRKC2gGhYzqx78NK8j7iL4,392
-cloudtik-1.1.0.dist-info/top_level.txt,sha256=dm0w93LRuyfFbwf-0PUkzBpqdvKv6rrtSxuuJfuAh58,9
-cloudtik-1.1.0.dist-info/RECORD,,
+cloudtik/tests/unit/runtime/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+cloudtik/tests/unit/runtime/test_ai_utils.py,sha256=klMjodw3HdvSUO9uGkSTmeHw2tEQV_6gdJtF4drQyqI,4892
+cloudtik-1.2.0.dist-info/METADATA,sha256=fuPknr4pWS5nX-nvUNe4EBap5msdHc-5gaPecMgaNJ4,22665
+cloudtik-1.2.0.dist-info/WHEEL,sha256=3oXbtF4vYwmQyf0LQIagKtl0VO3gP86556qKiLb0bDc,103
+cloudtik-1.2.0.dist-info/entry_points.txt,sha256=4R1fgN77EEQMX8H0zPUHciRKC2gGhYzqx78NK8j7iL4,392
+cloudtik-1.2.0.dist-info/top_level.txt,sha256=dm0w93LRuyfFbwf-0PUkzBpqdvKv6rrtSxuuJfuAh58,9
+cloudtik-1.2.0.dist-info/RECORD,,
```

